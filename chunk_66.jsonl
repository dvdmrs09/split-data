{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nassign a horizontal view to current graph", "response": "def _set_view(self):\n        \"\"\"Assign a horizontal view to current graph\"\"\"\n        if self.logarithmic:\n            view_class = HorizontalLogView\n        else:\n            view_class = HorizontalView\n\n        self.view = view_class(\n            self.width - self.margin_box.x, self.height - self.margin_box.y,\n            self._box\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _bar(self, serie, parent, x0, x1, y, i, zero, secondary=False):\n        x, y = self.view((x0, y))\n        x1, _ = self.view((x1, y))\n        width = x1 - x\n        height = self.view.y(zero) - y\n        series_margin = width * self._series_margin\n        x += series_margin\n        width -= 2 * series_margin\n\n        r = serie.rounded_bars * 1 if serie.rounded_bars else 0\n        alter(\n            self.svg.transposable_node(\n                parent,\n                'rect',\n                x=x,\n                y=y,\n                rx=r,\n                ry=r,\n                width=width,\n                height=height,\n                class_='rect reactive tooltip-trigger'\n            ), serie.metadata.get(i)\n        )\n        return x, y, width, height", "response": "Internal bar drawing function."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef bar(self, serie, rescale=False):\n        serie_node = self.svg.serie(serie)\n        bars = self.svg.node(serie_node['plot'], class_=\"histbars\")\n        points = serie.points\n\n        for i, (y, x0, x1) in enumerate(points):\n            if None in (x0, x1, y) or (self.logarithmic and y <= 0):\n                continue\n            metadata = serie.metadata.get(i)\n\n            bar = decorate(\n                self.svg, self.svg.node(bars, class_='histbar'), metadata\n            )\n            val = self._format(serie, i)\n\n            bounds = self._bar(\n                serie, bar, x0, x1, y, i, self.zero, secondary=rescale\n            )\n            self._tooltip_and_print_values(\n                serie_node, serie, bar, i, val, metadata, *bounds\n            )", "response": "Draw a bar graph for a serie"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _compute(self):\n        if self.xvals:\n            xmin = min(self.xvals)\n            xmax = max(self.xvals)\n            xrng = (xmax - xmin)\n        else:\n            xrng = None\n\n        if self.yvals:\n            ymin = min(min(self.yvals), self.zero)\n            ymax = max(max(self.yvals), self.zero)\n            yrng = (ymax - ymin)\n        else:\n            yrng = None\n\n        for serie in self.all_series:\n            serie.points = serie.values\n\n        if xrng:\n            self._box.xmin, self._box.xmax = xmin, xmax\n        if yrng:\n            self._box.ymin, self._box.ymax = ymin, ymax\n\n        if self.range and self.range[0] is not None:\n            self._box.ymin = self.range[0]\n\n        if self.range and self.range[1] is not None:\n            self._box.ymax = self.range[1]", "response": "Compute x max and x scale and set labels"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmakes a serie slice", "response": "def slice(self, serie, start_angle, total):\n        \"\"\"Make a serie slice\"\"\"\n        serie_node = self.svg.serie(serie)\n        dual = self._len > 1 and not self._order == 1\n\n        slices = self.svg.node(serie_node['plot'], class_=\"slices\")\n        serie_angle = 0\n        original_start_angle = start_angle\n        if self.half_pie:\n            center = ((self.width - self.margin_box.x) / 2.,\n                      (self.height - self.margin_box.y) / 1.25)\n        else:\n            center = ((self.width - self.margin_box.x) / 2.,\n                      (self.height - self.margin_box.y) / 2.)\n\n        radius = min(center)\n        for i, val in enumerate(serie.values):\n            perc = val / total\n            if self.half_pie:\n                angle = 2 * pi * perc / 2\n            else:\n                angle = 2 * pi * perc\n            serie_angle += angle\n            val = self._format(serie, i)\n            metadata = serie.metadata.get(i)\n            slice_ = decorate(\n                self.svg, self.svg.node(slices, class_=\"slice\"), metadata\n            )\n            if dual:\n                small_radius = radius * .9\n                big_radius = radius\n            else:\n                big_radius = radius * .9\n                small_radius = radius * serie.inner_radius\n\n            alter(\n                self.svg.slice(\n                    serie_node, slice_, big_radius, small_radius, angle,\n                    start_angle, center, val, i, metadata\n                ), metadata\n            )\n            start_angle += angle\n\n        if dual:\n            val = self._serie_format(serie, sum(serie.values))\n            self.svg.slice(\n                serie_node, self.svg.node(slices,\n                                          class_=\"big_slice\"), radius * .9, 0,\n                serie_angle, original_start_angle, center, val, i, metadata\n            )\n        return serie_angle"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _plot(self):\n        total = sum(map(sum, map(lambda x: x.values, self.series)))\n        if total == 0:\n            return\n        if self.half_pie:\n            current_angle = 3 * pi / 2\n        else:\n            current_angle = 0\n\n        for index, serie in enumerate(self.series):\n            angle = self.slice(serie, current_angle, total)\n            current_angle += angle", "response": "Draw all the serie slices"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _set_view(self):\n        if self.logarithmic:\n            if self._dual:\n                view_class = XYLogView\n            else:\n                view_class = LogView\n        else:\n            view_class = ReverseView if self.inverse_y_axis else View\n\n        self.view = view_class(\n            self.width - self.margin_box.x, self.height - self.margin_box.y,\n            self._box\n        )", "response": "Assign a view to current graph"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmaking the x axis labels and guides", "response": "def _x_axis(self):\n        \"\"\"Make the x axis: labels and guides\"\"\"\n        if not self._x_labels or not self.show_x_labels:\n            return\n        axis = self.svg.node(\n            self.nodes['plot'],\n            class_=\"axis x%s\" % (' always_show' if self.show_x_guides else '')\n        )\n        truncation = self.truncate_label\n        if not truncation:\n            if self.x_label_rotation or len(self._x_labels) <= 1:\n                truncation = 25\n            else:\n                first_label_position = self.view.x(self._x_labels[0][1]) or 0\n                last_label_position = self.view.x(self._x_labels[-1][1]) or 0\n                available_space = (last_label_position - first_label_position\n                                   ) / len(self._x_labels) - 1\n\n                truncation = reverse_text_len(\n                    available_space, self.style.label_font_size\n                )\n                truncation = max(truncation, 1)\n\n        lastlabel = self._x_labels[-1][0]\n        if 0 not in [label[1] for label in self._x_labels]:\n            self.svg.node(\n                axis,\n                'path',\n                d='M%f %f v%f' % (0, 0, self.view.height),\n                class_='line'\n            )\n            lastlabel = None\n\n        for label, position in self._x_labels:\n            if self.horizontal:\n                major = position in self._x_labels_major\n            else:\n                major = label in self._x_labels_major\n            if not (self.show_minor_x_labels or major):\n                continue\n            guides = self.svg.node(axis, class_='guides')\n            x = self.view.x(position)\n            if x is None:\n                continue\n            y = self.view.height + 5\n            last_guide = (self._y_2nd_labels and label == lastlabel)\n            self.svg.node(\n                guides,\n                'path',\n                d='M%f %f v%f' % (x or 0, 0, self.view.height),\n                class_='%s%s%sline' % (\n                    'axis ' if label == \"0\" else '', 'major '\n                    if major else '', 'guide '\n                    if position != 0 and not last_guide else ''\n                )\n            )\n            y += .5 * self.style.label_font_size + 5\n            text = self.svg.node(\n                guides, 'text', x=x, y=y, class_='major' if major else ''\n            )\n\n            text.text = truncate(label, truncation)\n            if text.text != label:\n                self.svg.node(guides, 'title').text = label\n            elif self._dual:\n                self.svg.node(\n                    guides,\n                    'title',\n                ).text = self._x_format(position)\n\n            if self.x_label_rotation:\n                text.attrib['transform'] = \"rotate(%d %f %f)\" % (\n                    self.x_label_rotation, x, y\n                )\n                if self.x_label_rotation >= 180:\n                    text.attrib['class'] = ' '.join((\n                        text.attrib['class']\n                        and text.attrib['class'].split(' ') or []\n                    ) + ['backwards'])\n\n        if self._y_2nd_labels and 0 not in [label[1]\n                                            for label in self._x_labels]:\n            self.svg.node(\n                axis,\n                'path',\n                d='M%f %f v%f' % (self.view.width, 0, self.view.height),\n                class_='line'\n            )\n\n        if self._x_2nd_labels:\n            secondary_ax = self.svg.node(\n                self.nodes['plot'],\n                class_=\"axis x x2%s\" %\n                (' always_show' if self.show_x_guides else '')\n            )\n            for label, position in self._x_2nd_labels:\n                major = label in self._x_labels_major\n                if not (self.show_minor_x_labels or major):\n                    continue\n                # it is needed, to have the same structure as primary axis\n                guides = self.svg.node(secondary_ax, class_='guides')\n                x = self.view.x(position)\n                y = -5\n                text = self.svg.node(\n                    guides, 'text', x=x, y=y, class_='major' if major else ''\n                )\n                text.text = label\n                if self.x_label_rotation:\n                    text.attrib['transform'] = \"rotate(%d %f %f)\" % (\n                        -self.x_label_rotation, x, y\n                    )\n                    if self.x_label_rotation >= 180:\n                        text.attrib['class'] = ' '.join((\n                            text.attrib['class']\n                            and text.attrib['class'].split(' ') or []\n                        ) + ['backwards'])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _y_axis(self):\n        if not self._y_labels or not self.show_y_labels:\n            return\n\n        axis = self.svg.node(\n            self.nodes['plot'],\n            class_=\"axis y%s\" % (' always_show' if self.show_y_guides else '')\n        )\n\n        if (0 not in [label[1] for label in self._y_labels]\n                and self.show_y_guides):\n            self.svg.node(\n                axis,\n                'path',\n                d='M%f %f h%f' % (\n                    0, 0 if self.inverse_y_axis else self.view.height,\n                    self.view.width\n                ),\n                class_='line'\n            )\n\n        for label, position in self._y_labels:\n            if self.horizontal:\n                major = label in self._y_labels_major\n            else:\n                major = position in self._y_labels_major\n\n            if not (self.show_minor_y_labels or major):\n                continue\n            guides = self.svg.node(\n                axis,\n                class_='%sguides' %\n                ('logarithmic ' if self.logarithmic else '')\n            )\n            x = -5\n            y = self.view.y(position)\n            if not y:\n                continue\n            if self.show_y_guides:\n                self.svg.node(\n                    guides,\n                    'path',\n                    d='M%f %f h%f' % (0, y, self.view.width),\n                    class_='%s%s%sline' % (\n                        'axis ' if label == \"0\" else '', 'major '\n                        if major else '', 'guide ' if position != 0 else ''\n                    )\n                )\n            text = self.svg.node(\n                guides,\n                'text',\n                x=x,\n                y=y + .35 * self.style.label_font_size,\n                class_='major' if major else ''\n            )\n\n            text.text = label\n\n            if self.y_label_rotation:\n                text.attrib['transform'] = \"rotate(%d %f %f)\" % (\n                    self.y_label_rotation, x, y\n                )\n                if 90 < self.y_label_rotation < 270:\n                    text.attrib['class'] = ' '.join((\n                        text.attrib['class']\n                        and text.attrib['class'].split(' ') or []\n                    ) + ['backwards'])\n            self.svg.node(\n                guides,\n                'title',\n            ).text = self._y_format(position)\n\n        if self._y_2nd_labels:\n            secondary_ax = self.svg.node(self.nodes['plot'], class_=\"axis y2\")\n            for label, position in self._y_2nd_labels:\n                major = position in self._y_labels_major\n                if not (self.show_minor_y_labels or major):\n                    continue\n                # it is needed, to have the same structure as primary axis\n                guides = self.svg.node(secondary_ax, class_='guides')\n                x = self.view.width + 5\n                y = self.view.y(position)\n                text = self.svg.node(\n                    guides,\n                    'text',\n                    x=x,\n                    y=y + .35 * self.style.label_font_size,\n                    class_='major' if major else ''\n                )\n                text.text = label\n                if self.y_label_rotation:\n                    text.attrib['transform'] = \"rotate(%d %f %f)\" % (\n                        self.y_label_rotation, x, y\n                    )\n                    if 90 < self.y_label_rotation < 270:\n                        text.attrib['class'] = ' '.join((\n                            text.attrib['class']\n                            and text.attrib['class'].split(' ') or []\n                        ) + ['backwards'])", "response": "Make the y axis labels and guides."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmaking the legend box.", "response": "def _legend(self):\n        \"\"\"Make the legend box\"\"\"\n        if not self.show_legend:\n            return\n        truncation = self.truncate_legend\n        if self.legend_at_bottom:\n            x = self.margin_box.left + self.spacing\n            y = (\n                self.margin_box.top + self.view.height + self._x_title_height +\n                self._x_labels_height + self.spacing\n            )\n            cols = self.legend_at_bottom_columns or ceil(sqrt(self._order)\n                                                         ) or 1\n\n            if not truncation:\n                available_space = self.view.width / cols - (\n                    self.legend_box_size + 5\n                )\n                truncation = reverse_text_len(\n                    available_space, self.style.legend_font_size\n                )\n        else:\n            x = self.spacing\n            y = self.margin_box.top + self.spacing\n            cols = 1\n            if not truncation:\n                truncation = 15\n\n        legends = self.svg.node(\n            self.nodes['graph'],\n            class_='legends',\n            transform='translate(%d, %d)' % (x, y)\n        )\n\n        h = max(self.legend_box_size, self.style.legend_font_size)\n        x_step = self.view.width / cols\n        if self.legend_at_bottom:\n            secondary_legends = legends  # svg node is the same\n        else:\n\n            # draw secondary axis on right\n            x = self.margin_box.left + self.view.width + self.spacing\n            if self._y_2nd_labels:\n                h, w = get_texts_box(\n                    cut(self._y_2nd_labels), self.style.label_font_size\n                )\n                x += self.spacing + max(\n                    w * abs(cos(rad(self.y_label_rotation))), h\n                )\n\n            y = self.margin_box.top + self.spacing\n\n            secondary_legends = self.svg.node(\n                self.nodes['graph'],\n                class_='legends',\n                transform='translate(%d, %d)' % (x, y)\n            )\n\n        serie_number = -1\n        i = 0\n\n        for titles, is_secondary in ((self._legends, False),\n                                     (self._secondary_legends, True)):\n            if not self.legend_at_bottom and is_secondary:\n                i = 0\n\n            for title in titles:\n                serie_number += 1\n                if title is None:\n                    continue\n                col = i % cols\n                row = i // cols\n\n                legend = self.svg.node(\n                    secondary_legends if is_secondary else legends,\n                    class_='legend reactive activate-serie',\n                    id=\"activate-serie-%d\" % serie_number\n                )\n                self.svg.node(\n                    legend,\n                    'rect',\n                    x=col * x_step,\n                    y=1.5 * row * h + (\n                        self.style.legend_font_size - self.legend_box_size\n                        if self.style.legend_font_size > self.legend_box_size\n                        else 0\n                    ) / 2,\n                    width=self.legend_box_size,\n                    height=self.legend_box_size,\n                    class_=\"color-%d reactive\" % serie_number\n                )\n\n                if isinstance(title, dict):\n                    node = decorate(self.svg, legend, title)\n                    title = title['title']\n                else:\n                    node = legend\n\n                truncated = truncate(title, truncation)\n                self.svg.node(\n                    node,\n                    'text',\n                    x=col * x_step + self.legend_box_size + 5,\n                    y=1.5 * row * h + .5 * h + .3 * self.style.legend_font_size\n                ).text = truncated\n\n                if truncated != title:\n                    self.svg.node(legend, 'title').text = title\n\n                i += 1"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmakes the X - Axis title", "response": "def _make_x_title(self):\n        \"\"\"Make the X-Axis title\"\"\"\n        y = (self.height - self.margin_box.bottom + self._x_labels_height)\n        if self._x_title:\n            for i, title_line in enumerate(self._x_title, 1):\n                text = self.svg.node(\n                    self.nodes['title'],\n                    'text',\n                    class_='title',\n                    x=self.margin_box.left + self.view.width / 2,\n                    y=y + i * (self.style.title_font_size + self.spacing)\n                )\n                text.text = title_line"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmaking the Y - Axis title", "response": "def _make_y_title(self):\n        \"\"\"Make the Y-Axis title\"\"\"\n        if self._y_title:\n            yc = self.margin_box.top + self.view.height / 2\n            for i, title_line in enumerate(self._y_title, 1):\n                text = self.svg.node(\n                    self.nodes['title'],\n                    'text',\n                    class_='title',\n                    x=self._legend_at_left_width,\n                    y=i * (self.style.title_font_size + self.spacing) + yc\n                )\n                text.attrib['transform'] = \"rotate(%d %f %f)\" % (\n                    -90, self._legend_at_left_width, yc\n                )\n                text.text = title_line"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _tooltip_data(self, node, value, x, y, classes=None, xlabel=None):\n        self.svg.node(node, 'desc', class_=\"value\").text = value\n        if classes is None:\n            classes = []\n            if x > self.view.width / 2:\n                classes.append('left')\n            if y > self.view.height / 2:\n                classes.append('top')\n            classes = ' '.join(classes)\n\n        self.svg.node(node, 'desc', class_=\"x \" + classes).text = to_str(x)\n        self.svg.node(node, 'desc', class_=\"y \" + classes).text = to_str(y)\n        if xlabel:\n            self.svg.node(node, 'desc', class_=\"x_label\").text = to_str(xlabel)", "response": "Insert in desc tags informations for the javascript tooltip"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites the print value.", "response": "def _static_value(\n            self,\n            serie_node,\n            value,\n            x,\n            y,\n            metadata,\n            align_text='left',\n            classes=None\n    ):\n        \"\"\"Write the print value\"\"\"\n        label = metadata and metadata.get('label')\n        classes = classes and [classes] or []\n\n        if self.print_labels and label:\n            label_cls = classes + ['label']\n            if self.print_values:\n                y -= self.style.value_font_size / 2\n            self.svg.node(\n                serie_node['text_overlay'],\n                'text',\n                class_=' '.join(label_cls),\n                x=x,\n                y=y + self.style.value_font_size / 3\n            ).text = label\n            y += self.style.value_font_size\n\n        if self.print_values or self.dynamic_print_values:\n            val_cls = classes + ['value']\n            if self.dynamic_print_values:\n                val_cls.append('showable')\n\n            self.svg.node(\n                serie_node['text_overlay'],\n                'text',\n                class_=' '.join(val_cls),\n                x=x,\n                y=y + self.style.value_font_size / 3,\n                attrib={\n                    'text-anchor': align_text\n                }\n            ).text = value if self.print_zeroes or value != '0' else ''"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _points(self, x_pos):\n        for serie in self.all_series:\n            serie.points = [(x_pos[i], v) for i, v in enumerate(serie.values)]\n            if serie.points and self.interpolate:\n                serie.interpolated = self._interpolate(x_pos, serie.values)\n            else:\n                serie.interpolated = []", "response": "Convert given data values into drawable points and interpolated points if interpolate option is specified."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _compute_secondary(self):\n        # secondary y axis support\n        if self.secondary_series and self._y_labels:\n            y_pos = list(zip(*self._y_labels))[1]\n            if self.include_x_axis:\n                ymin = min(self._secondary_min, 0)\n                ymax = max(self._secondary_max, 0)\n            else:\n                ymin = self._secondary_min\n                ymax = self._secondary_max\n            steps = len(y_pos)\n            left_range = abs(y_pos[-1] - y_pos[0])\n            right_range = abs(ymax - ymin) or 1\n            scale = right_range / ((steps - 1) or 1)\n            self._y_2nd_labels = [(self._y_format(ymin + i * scale), pos)\n                                  for i, pos in enumerate(y_pos)]\n\n            self._scale = left_range / right_range\n            self._scale_diff = y_pos[0]\n            self._scale_min_2nd = ymin", "response": "Compute secondary axis min max and label positions"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_x_label(self, i):\n        if not self.x_labels or not self._x_labels or len(self._x_labels) <= i:\n            return\n        return self._x_labels[i][0]", "response": "Convenience function to get the x_label of a value index"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nformat the nth value for the serie", "response": "def _format(self, serie, i):\n        \"\"\"Format the nth value for the serie\"\"\"\n        value = serie.values[i]\n        metadata = serie.metadata.get(i)\n\n        kwargs = {'chart': self, 'serie': serie, 'index': i}\n        formatter = ((metadata and metadata.get('formatter'))\n                     or serie.formatter or self.formatter\n                     or self._value_format)\n        kwargs = filter_kwargs(formatter, kwargs)\n        return formatter(value, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _serie_format(self, serie, value):\n\n        kwargs = {'chart': self, 'serie': serie, 'index': None}\n        formatter = (serie.formatter or self.formatter or self._value_format)\n        kwargs = filter_kwargs(formatter, kwargs)\n        return formatter(value, **kwargs)", "response": "Format an independent value for the serie"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _compute_margin(self):\n        self._legend_at_left_width = 0\n        for series_group in (self.series, self.secondary_series):\n            if self.show_legend and series_group:\n                h, w = get_texts_box(\n                    map(\n                        lambda x: truncate(x, self.truncate_legend or 15), [\n                            serie.title['title']\n                            if isinstance(serie.title, dict) else serie.title\n                            or '' for serie in series_group\n                        ]\n                    ), self.style.legend_font_size\n                )\n                if self.legend_at_bottom:\n                    h_max = max(h, self.legend_box_size)\n                    cols = (\n                        self._order // self.legend_at_bottom_columns\n                        if self.legend_at_bottom_columns else\n                        ceil(sqrt(self._order)) or 1\n                    )\n                    self.margin_box.bottom += self.spacing + h_max * round(\n                        cols - 1\n                    ) * 1.5 + h_max\n                else:\n                    if series_group is self.series:\n                        legend_width = self.spacing + w + self.legend_box_size\n                        self.margin_box.left += legend_width\n                        self._legend_at_left_width += legend_width\n                    else:\n                        self.margin_box.right += (\n                            self.spacing + w + self.legend_box_size\n                        )\n\n        self._x_labels_height = 0\n        if (self._x_labels or self._x_2nd_labels) and self.show_x_labels:\n            for xlabels in (self._x_labels, self._x_2nd_labels):\n                if xlabels:\n                    h, w = get_texts_box(\n                        map(\n                            lambda x: truncate(x, self.truncate_label or 25),\n                            cut(xlabels)\n                        ), self.style.label_font_size\n                    )\n                    self._x_labels_height = self.spacing + max(\n                        w * abs(sin(rad(self.x_label_rotation))), h\n                    )\n                    if xlabels is self._x_labels:\n                        self.margin_box.bottom += self._x_labels_height\n                    else:\n                        self.margin_box.top += self._x_labels_height\n                    if self.x_label_rotation:\n                        if self.x_label_rotation % 180 < 90:\n                            self.margin_box.right = max(\n                                w * abs(cos(rad(self.x_label_rotation))),\n                                self.margin_box.right\n                            )\n                        else:\n                            self.margin_box.left = max(\n                                w * abs(cos(rad(self.x_label_rotation))),\n                                self.margin_box.left\n                            )\n\n        if self.show_y_labels:\n            for ylabels in (self._y_labels, self._y_2nd_labels):\n                if ylabels:\n                    h, w = get_texts_box(\n                        cut(ylabels), self.style.label_font_size\n                    )\n                    if ylabels is self._y_labels:\n                        self.margin_box.left += self.spacing + max(\n                            w * abs(cos(rad(self.y_label_rotation))), h\n                        )\n                    else:\n                        self.margin_box.right += self.spacing + max(\n                            w * abs(cos(rad(self.y_label_rotation))), h\n                        )\n\n        self._title = split_title(\n            self.title, self.width, self.style.title_font_size\n        )\n\n        if self.title:\n            h, _ = get_text_box(self._title[0], self.style.title_font_size)\n            self.margin_box.top += len(self._title) * (self.spacing + h)\n\n        self._x_title = split_title(\n            self.x_title, self.width - self.margin_box.x,\n            self.style.title_font_size\n        )\n\n        self._x_title_height = 0\n        if self._x_title:\n            h, _ = get_text_box(self._x_title[0], self.style.title_font_size)\n            height = len(self._x_title) * (self.spacing + h)\n            self.margin_box.bottom += height\n            self._x_title_height = height + self.spacing\n\n        self._y_title = split_title(\n            self.y_title, self.height - self.margin_box.y,\n            self.style.title_font_size\n        )\n\n        self._y_title_height = 0\n        if self._y_title:\n            h, _ = get_text_box(self._y_title[0], self.style.title_font_size)\n            height = len(self._y_title) * (self.spacing + h)\n            self.margin_box.left += height\n            self._y_title_height = height + self.spacing\n\n        # Inner margin\n        if self.print_values_position == 'top':\n            gh = self.height - self.margin_box.y\n            alpha = 1.1 * (self.style.value_font_size / gh) * self._box.height\n            if self._max and self._max > 0:\n                self._box.ymax += alpha\n            if self._min and self._min < 0:\n                self._box.ymin -= alpha", "response": "Compute graph margins from set texts"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the values of the series.", "response": "def _values(self):\n        \"\"\"Getter for series values (flattened)\"\"\"\n        return [\n            val for serie in self.series for val in serie.values\n            if val is not None\n        ]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _secondary_values(self):\n        return [\n            val for serie in self.secondary_series for val in serie.values\n            if val is not None\n        ]", "response": "Get the values of the secondary series."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _secondary_min(self):\n        return (\n            self.secondary_range[0]\n            if (self.secondary_range\n                and self.secondary_range[0] is not None) else\n            (min(self._secondary_values) if self._secondary_values else None)\n        )", "response": "Getter for the minimum series value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _min(self):\n        return (\n            self.range[0] if (self.range and self.range[0] is not None) else\n            (min(self._values) if self._values else None)\n        )", "response": "Getter for the minimum series value"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the maximum value of the current set of values", "response": "def _max(self):\n        \"\"\"Getter for the maximum series value\"\"\"\n        return (\n            self.range[1] if (self.range and self.range[1] is not None) else\n            (max(self._values) if self._values else None)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _secondary_max(self):\n        return (\n            self.secondary_range[1]\n            if (self.secondary_range\n                and self.secondary_range[1] is not None) else\n            (max(self._secondary_values) if self._secondary_values else None)\n        )", "response": "Gets the maximum value of the secondary entries."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _draw(self):\n        self._compute()\n        self._compute_x_labels()\n        self._compute_x_labels_major()\n        self._compute_y_labels()\n        self._compute_y_labels_major()\n        self._compute_secondary()\n        self._post_compute()\n        self._compute_margin()\n        self._decorate()\n        if self.series and self._has_data() and self._values:\n            self._plot()\n        else:\n            self.svg.draw_no_data()", "response": "Draw all the things"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if there is any data in the series", "response": "def _has_data(self):\n        \"\"\"Check if there is any data\"\"\"\n        return any([\n            len([\n                v for a in (s[0] if is_list_like(s) else [s])\n                for v in (a if is_list_like(a) else [a]) if v is not None\n            ]) for s in self.raw_series\n        ])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _bar(self, serie, parent, x, y, i, zero, secondary=False):\n        width = (self.view.x(1) - self.view.x(0)) / self._len\n        x, y = self.view((x, y))\n        series_margin = width * self._series_margin\n        x += series_margin\n        width -= 2 * series_margin\n        width /= self._order\n        if self.horizontal:\n            serie_index = self._order - serie.index - 1\n        else:\n            serie_index = serie.index\n        x += serie_index * width\n\n        serie_margin = width * self._serie_margin\n        x += serie_margin\n        width -= 2 * serie_margin\n        height = self.view.y(zero) - y\n        r = serie.rounded_bars * 1 if serie.rounded_bars else 0\n        alter(\n            self.svg.transposable_node(\n                parent,\n                'rect',\n                x=x,\n                y=y,\n                rx=r,\n                ry=r,\n                width=width,\n                height=height,\n                class_='rect reactive tooltip-trigger'\n            ), serie.metadata.get(i)\n        )\n        return x, y, width, height", "response": "Internal bar drawing function."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef bar(self, serie, rescale=False):\n        serie_node = self.svg.serie(serie)\n        bars = self.svg.node(serie_node['plot'], class_=\"bars\")\n        if rescale and self.secondary_series:\n            points = self._rescale(serie.points)\n        else:\n            points = serie.points\n\n        for i, (x, y) in enumerate(points):\n            if None in (x, y) or (self.logarithmic and y <= 0):\n                continue\n            metadata = serie.metadata.get(i)\n            val = self._format(serie, i)\n\n            bar = decorate(\n                self.svg, self.svg.node(bars, class_='bar'), metadata\n            )\n\n            x_, y_, width, height = self._bar(\n                serie, bar, x, y, i, self.zero, secondary=rescale\n            )\n\n            self._confidence_interval(\n                serie_node['overlay'], x_ + width / 2, y_, serie.values[i],\n                metadata\n            )\n\n            self._tooltip_and_print_values(\n                serie_node, serie, bar, i, val, metadata, x_, y_, width, height\n            )", "response": "Draw a bar graph for a serie"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _compute(self):\n        if self._min:\n            self._box.ymin = min(self._min, self.zero)\n        if self._max:\n            self._box.ymax = max(self._max, self.zero)\n        self._x_pos = [\n            x / self._len for x in range(self._len + 1)\n        ] if self._len > 1 else [0, 1]  # Center if only one value\n\n        self._points(self._x_pos)\n\n        self._x_pos = [(i + .5) / self._len for i in range(self._len)]", "response": "Compute y min max and y scale and set labels"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nformat value for dual value display.", "response": "def _value_format(self, value):\n        \"\"\"Format value for dual value display.\"\"\"\n        return super(Funnel, self)._value_format(value and abs(value))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndraw a funnel slice", "response": "def funnel(self, serie):\n        \"\"\"Draw a funnel slice\"\"\"\n        serie_node = self.svg.serie(serie)\n        fmt = lambda x: '%f %f' % x\n        for i, poly in enumerate(serie.points):\n            metadata = serie.metadata.get(i)\n            val = self._format(serie, i)\n\n            funnels = decorate(\n                self.svg, self.svg.node(serie_node['plot'], class_=\"funnels\"),\n                metadata\n            )\n\n            alter(\n                self.svg.node(\n                    funnels,\n                    'polygon',\n                    points=' '.join(map(fmt, map(self.view, poly))),\n                    class_='funnel reactive tooltip-trigger'\n                ), metadata\n            )\n\n            # Poly center from label\n            x, y = self.view((\n                self._center(self._x_pos[serie.index]),\n                sum([point[1] for point in poly]) / len(poly)\n            ))\n            self._tooltip_data(\n                funnels, val, x, y, 'centered', self._get_x_label(serie.index)\n            )\n            self._static_value(serie_node, val, x, y, metadata)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _compute(self):\n        self._x_pos = [\n            (x + 1) / self._order for x in range(self._order)\n        ] if self._order != 1 else [.5]  # Center if only one value\n\n        previous = [[self.zero, self.zero] for i in range(self._len)]\n        for i, serie in enumerate(self.series):\n            y_height = -sum(serie.safe_values) / 2\n            all_x_pos = [0] + self._x_pos\n            serie.points = []\n            for j, value in enumerate(serie.values):\n                poly = []\n                poly.append((all_x_pos[i], previous[j][0]))\n                poly.append((all_x_pos[i], previous[j][1]))\n                previous[j][0] = y_height\n                y_height = previous[j][1] = y_height + value\n                poly.append((all_x_pos[i + 1], previous[j][1]))\n                poly.append((all_x_pos[i + 1], previous[j][0]))\n                serie.points.append(poly)\n\n        val_max = max(list(map(sum, cut(self.series, 'values'))) + [self.zero])\n        self._box.ymin = -val_max\n        self._box.ymax = val_max\n\n        if self.range and self.range[0] is not None:\n            self._box.ymin = self.range[0]\n\n        if self.range and self.range[1] is not None:\n            self._box.ymax = self.range[1]", "response": "Compute y min max and y scale and set labels"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef swap(self):\n        self.xmin, self.ymin = self.ymin, self.xmin\n        self.xmax, self.ymax = self.ymax, self.xmax", "response": "Swap the min and max values of the current set of resources."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fix(self, with_margin=True):\n        if not self.width:\n            self.xmax = self.xmin + 1\n        if not self.height:\n            self.ymin /= 2\n            self.ymax += self.ymin\n        xmargin = self.margin * self.width\n        self.xmin -= xmargin\n        self.xmax += xmargin\n        if with_margin:\n            ymargin = self.margin * self.height\n            self.ymin -= ymargin\n            self.ymax += ymargin", "response": "Correct box when no values and take margin in account"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef x(self, x):\n        if x is None:\n            return None\n        if self._force_vertical:\n            return super(HorizontalView, self).x(x)\n        return super(HorizontalView, self).y(x)", "response": "Project x as y"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef y(self, y):\n        if y is None:\n            return None\n        if self._force_vertical:\n            return super(HorizontalView, self).y(y)\n        return super(HorizontalView, self).x(y)", "response": "Project y as x"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef x(self, x):\n        if x is None:\n            return None\n        if self._force_vertical:\n            return super(HorizontalLogView, self).x(x)\n        return super(XLogView, self).y(x)", "response": "Project x as y"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef y(self, y):\n        if y is None:\n            return None\n        if self._force_vertical:\n            return super(XLogView, self).y(y)\n        return super(HorizontalLogView, self).x(y)", "response": "Project y as x"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a date into a datetime", "response": "def date_to_datetime(x):\n    \"\"\"Convert a date into a datetime\"\"\"\n    if not isinstance(x, datetime) and isinstance(x, date):\n        return datetime.combine(x, time())\n    return x"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert a time into a datetime", "response": "def time_to_datetime(x):\n    \"\"\"Convert a time into a datetime\"\"\"\n    if isinstance(x, time):\n        return datetime.combine(date(1970, 1, 1), x)\n    return x"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef time_to_seconds(x):\n    if isinstance(x, time):\n        return ((((x.hour * 60) + x.minute) * 60 + x.second) * 10**6 +\n                x.microsecond) / 10**6\n\n    if is_str(x):\n        return x\n    # Clamp to valid time\n    return x and max(0, min(x, 24 * 3600 - 10**-6))", "response": "Convert a time in a seconds sum"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef seconds_to_time(x):\n    t = int(x * 10**6)\n    ms = t % 10**6\n    t = t // 10**6\n    s = t % 60\n    t = t // 60\n    m = t % 60\n    t = t // 60\n    h = t\n    return time(h, m, s, ms)", "response": "Convert a number of second into a time"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the value formatter for this graph", "response": "def _x_format(self):\n        \"\"\"Return the value formatter for this graph\"\"\"\n\n        def datetime_to_str(x):\n            dt = datetime.utcfromtimestamp(x)\n            return self.x_value_formatter(dt)\n\n        return datetime_to_str"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _x_format(self):\n\n        def date_to_str(x):\n            d = datetime.utcfromtimestamp(x).date()\n            return self.x_value_formatter(d)\n\n        return date_to_str", "response": "Return the value formatter for this graph"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _x_format(self):\n\n        def date_to_str(x):\n            t = seconds_to_time(x)\n            return self.x_value_formatter(t)\n\n        return date_to_str", "response": "Return the value formatter for this graph"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the value formatter for this graph", "response": "def _x_format(self):\n        \"\"\"Return the value formatter for this graph\"\"\"\n\n        def timedelta_to_str(x):\n            td = timedelta(seconds=x)\n            return self.x_value_formatter(td)\n\n        return timedelta_to_str"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _set_view(self):\n        if self.logarithmic:\n            view_class = PolarThetaLogView\n        else:\n            view_class = PolarThetaView\n\n        self.view = view_class(\n            self.width - self.margin_box.x, self.height - self.margin_box.y,\n            self._box\n        )", "response": "Assign a view to current graph"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef needle(self, serie):\n        serie_node = self.svg.serie(serie)\n        for i, theta in enumerate(serie.values):\n            if theta is None:\n                continue\n\n            def point(x, y):\n                return '%f %f' % self.view((x, y))\n\n            val = self._format(serie, i)\n            metadata = serie.metadata.get(i)\n            gauges = decorate(\n                self.svg, self.svg.node(serie_node['plot'], class_=\"dots\"),\n                metadata\n            )\n\n            tolerance = 1.15\n\n            if theta < self._min:\n                theta = self._min * tolerance\n\n            if theta > self._max:\n                theta = self._max * tolerance\n\n            w = (self._box._tmax - self._box._tmin + self.view.aperture) / 4\n\n            if self.logarithmic:\n                w = min(w, self._min - self._min * 10**-10)\n\n            alter(\n                self.svg.node(\n                    gauges,\n                    'path',\n                    d='M %s L %s A %s 1 0 1 %s Z' % (\n                        point(.85, theta),\n                        point(self.needle_width, theta - w),\n                        '%f %f' % (self.needle_width, self.needle_width),\n                        point(self.needle_width, theta + w),\n                    ),\n                    class_='line reactive tooltip-trigger'\n                ), metadata\n            )\n\n            x, y = self.view((.75, theta))\n            self._tooltip_data(gauges, val, x, y, xlabel=self._get_x_label(i))\n            self._static_value(serie_node, val, x, y, metadata)", "response": "Draw a needle for each value in serie."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _y_axis(self, draw_axes=True):\n        axis = self.svg.node(self.nodes['plot'], class_=\"axis x gauge\")\n\n        for i, (label, theta) in enumerate(self._y_labels):\n            guides = self.svg.node(axis, class_='guides')\n\n            self.svg.line(\n                guides, [self.view((.95, theta)),\n                         self.view((1, theta))],\n                close=True,\n                class_='line'\n            )\n\n            self.svg.line(\n                guides, [self.view((0, theta)),\n                         self.view((.95, theta))],\n                close=True,\n                class_='guide line %s' %\n                ('major' if i in (0, len(self._y_labels) - 1) else '')\n            )\n\n            x, y = self.view((.9, theta))\n            self.svg.node(guides, 'text', x=x, y=y).text = label\n\n            self.svg.node(\n                guides,\n                'title',\n            ).text = self._y_format(theta)", "response": "Override y axis to plot a polar axis"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\noverriding x axis to put a center circle in center", "response": "def _x_axis(self, draw_axes=True):\n        \"\"\"Override x axis to put a center circle in center\"\"\"\n        axis = self.svg.node(self.nodes['plot'], class_=\"axis y gauge\")\n        x, y = self.view((0, 0))\n        self.svg.node(axis, 'circle', cx=x, cy=y, r=4)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes y min max and y scale and set labels", "response": "def _compute(self):\n        \"\"\"Compute y min and max and y scale and set labels\"\"\"\n        self.min_ = self._min or 0\n        self.max_ = self._max or 0\n        if self.max_ - self.min_ == 0:\n            self.min_ -= 1\n            self.max_ += 1\n\n        self._box.set_polar_box(0, 1, self.min_, self.max_)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the css color list for the current locale.", "response": "def get_colors(self, prefix, len_):\n        \"\"\"Get the css color list\"\"\"\n\n        def color(tupl):\n            \"\"\"Make a color css\"\"\"\n            return ((\n                '%s.color-{0}, %s.color-{0} a:visited {{\\n'\n                '  stroke: {1};\\n'\n                '  fill: {1};\\n'\n                '}}\\n'\n            ) % (prefix, prefix)).format(*tupl)\n\n        def value_color(tupl):\n            \"\"\"Make a value color css\"\"\"\n            return ((\n                '%s .text-overlay .color-{0} text {{\\n'\n                '  fill: {1};\\n'\n                '}}\\n'\n            ) % (prefix, )).format(*tupl)\n\n        def ci_color(tupl):\n            \"\"\"Make a value color css\"\"\"\n            if not tupl[1]:\n                return ''\n            return (('%s .color-{0} .ci {{\\n'\n                     '  stroke: {1};\\n'\n                     '}}\\n') % (prefix, )).format(*tupl)\n\n        if len(self.colors) < len_:\n            missing = len_ - len(self.colors)\n            cycles = 1 + missing // len(self.colors)\n            colors = []\n            for i in range(0, cycles + 1):\n                for color_ in self.colors:\n                    colors.append(darken(color_, 33 * i / cycles))\n                    if len(colors) >= len_:\n                        break\n                else:\n                    continue\n                break\n        else:\n            colors = self.colors[:len_]\n\n        # Auto compute foreground value color when color is missing\n        value_colors = []\n        for i in range(len_):\n            if i < len(self.value_colors) and self.value_colors[i] is not None:\n                value_colors.append(self.value_colors[i])\n            else:\n                value_colors.append(\n                    'white' if is_foreground_light(colors[i]) else 'black'\n                )\n\n        return '\\n'.join(\n            chain(\n                map(color, enumerate(colors)),\n                map(value_color, enumerate(value_colors)),\n                map(ci_color, enumerate(self.ci_colors))\n            )\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_dict(self):\n        config = {}\n        for attr in dir(self):\n            if not attr.startswith('_'):\n                value = getattr(self, attr)\n                if not hasattr(value, '__call__'):\n                    config[attr] = value\n        return config", "response": "Convert instance to a serializable mapping."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ninterpolates x y using a quadratic algorithm.", "response": "def quadratic_interpolate(x, y, precision=250, **kwargs):\n    \"\"\"\n    Interpolate x, y using a quadratic algorithm\n    https://en.wikipedia.org/wiki/Spline_(mathematics)\n    \"\"\"\n    n = len(x) - 1\n    delta_x = [x2 - x1 for x1, x2 in zip(x, x[1:])]\n    delta_y = [y2 - y1 for y1, y2 in zip(y, y[1:])]\n    slope = [delta_y[i] / delta_x[i] if delta_x[i] else 1 for i in range(n)]\n\n    # Quadratic spline: a + bx + cx\u00b2\n    a = y\n    b = [0] * (n + 1)\n    c = [0] * (n + 1)\n\n    for i in range(1, n):\n        b[i] = 2 * slope[i - 1] - b[i - 1]\n\n    c = [(slope[i] - b[i]) / delta_x[i] if delta_x[i] else 0 for i in range(n)]\n\n    for i in range(n + 1):\n        yield x[i], a[i]\n        if i == n or delta_x[i] == 0:\n            continue\n        for s in range(1, precision):\n            X = s * delta_x[i] / precision\n            X2 = X * X\n            yield x[i] + X, a[i] + b[i] * X + c[i] * X2"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cubic_interpolate(x, y, precision=250, **kwargs):\n    n = len(x) - 1\n    # Spline equation is a + bx + cx\u00b2 + dx\u00b3\n    # ie: Spline part i equation is a[i] + b[i]x + c[i]x\u00b2 + d[i]x\u00b3\n    a = y\n    b = [0] * (n + 1)\n    c = [0] * (n + 1)\n    d = [0] * (n + 1)\n    m = [0] * (n + 1)\n    z = [0] * (n + 1)\n\n    h = [x2 - x1 for x1, x2 in zip(x, x[1:])]\n    k = [a2 - a1 for a1, a2 in zip(a, a[1:])]\n    g = [k[i] / h[i] if h[i] else 1 for i in range(n)]\n\n    for i in range(1, n):\n        j = i - 1\n        l = 1 / (2 * (x[i + 1] - x[j]) - h[j] * m[j]) if x[i + 1] - x[j] else 0\n        m[i] = h[i] * l\n        z[i] = (3 * (g[i] - g[j]) - h[j] * z[j]) * l\n\n    for j in reversed(range(n)):\n        if h[j] == 0:\n            continue\n        c[j] = z[j] - (m[j] * c[j + 1])\n        b[j] = g[j] - (h[j] * (c[j + 1] + 2 * c[j])) / 3\n        d[j] = (c[j + 1] - c[j]) / (3 * h[j])\n\n    for i in range(n + 1):\n        yield x[i], a[i]\n        if i == n or h[i] == 0:\n            continue\n        for s in range(1, precision):\n            X = s * h[i] / precision\n            X2 = X * X\n            X3 = X2 * X\n            yield x[i] + X, a[i] + b[i] * X + c[i] * X2 + d[i] * X3", "response": "Interpolate x y using a cubic algorithm\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef hermite_interpolate(\n        x, y, precision=250, type='cardinal', c=None, b=None, t=None\n):\n    \"\"\"\n    Interpolate x, y using the hermite method.\n    See https://en.wikipedia.org/wiki/Cubic_Hermite_spline\n\n    This interpolation is configurable and contain 4 subtypes:\n      * Catmull Rom\n      * Finite Difference\n      * Cardinal\n      * Kochanek Bartels\n\n    The cardinal subtype is customizable with a parameter:\n      * c: tension (0, 1)\n\n    This last type is also customizable using 3 parameters:\n      * c: continuity (-1, 1)\n      * b: bias       (-1, 1)\n      * t: tension    (-1, 1)\n\n    \"\"\"\n    n = len(x) - 1\n    m = [1] * (n + 1)\n    w = [1] * (n + 1)\n    delta_x = [x2 - x1 for x1, x2 in zip(x, x[1:])]\n    if type == 'catmull_rom':\n        type = 'cardinal'\n        c = 0\n    if type == 'finite_difference':\n        for i in range(1, n):\n            m[i] = w[i] = .5 * ((y[i + 1] - y[i]) / (x[i + 1] - x[i]) +\n                                (y[i] - y[i - 1]) / (x[i] - x[i - 1])\n                                ) if x[i + 1] - x[i] and x[i] - x[i - 1] else 0\n\n    elif type == 'kochanek_bartels':\n        c = c or 0\n        b = b or 0\n        t = t or 0\n        for i in range(1, n):\n            m[i] = .5 * ((1 - t) * (1 + b) * (1 + c) * (y[i] - y[i - 1]) +\n                         (1 - t) * (1 - b) * (1 - c) * (y[i + 1] - y[i]))\n            w[i] = .5 * ((1 - t) * (1 + b) * (1 - c) * (y[i] - y[i - 1]) +\n                         (1 - t) * (1 - b) * (1 + c) * (y[i + 1] - y[i]))\n\n    if type == 'cardinal':\n        c = c or 0\n        for i in range(1, n):\n            m[i] = w[i] = (1 - c) * (y[i + 1] - y[i - 1]) / (\n                x[i + 1] - x[i - 1]\n            ) if x[i + 1] - x[i - 1] else 0\n\n    def p(i, x_):\n        t = (x_ - x[i]) / delta_x[i]\n        t2 = t * t\n        t3 = t2 * t\n\n        h00 = 2 * t3 - 3 * t2 + 1\n        h10 = t3 - 2 * t2 + t\n        h01 = -2 * t3 + 3 * t2\n        h11 = t3 - t2\n\n        return (\n            h00 * y[i] + h10 * m[i] * delta_x[i] + h01 * y[i + 1] +\n            h11 * w[i + 1] * delta_x[i]\n        )\n\n    for i in range(n + 1):\n        yield x[i], y[i]\n        if i == n or delta_x[i] == 0:\n            continue\n        for s in range(1, precision):\n            X = x[i] + s * delta_x[i] / precision\n            yield X, p(i, X)", "response": "Interpolate x y using the hermite method."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninterpolate x y using Lagrange polynomials", "response": "def lagrange_interpolate(x, y, precision=250, **kwargs):\n    \"\"\"\n    Interpolate x, y using Lagrange polynomials\n    https://en.wikipedia.org/wiki/Lagrange_polynomial\n    \"\"\"\n    n = len(x) - 1\n    delta_x = [x2 - x1 for x1, x2 in zip(x, x[1:])]\n    for i in range(n + 1):\n        yield x[i], y[i]\n        if i == n or delta_x[i] == 0:\n            continue\n\n        for s in range(1, precision):\n            X = x[i] + s * delta_x[i] / precision\n            s = 0\n            for k in range(n + 1):\n                p = 1\n                for m in range(n + 1):\n                    if m == k:\n                        continue\n                    if x[k] - x[m]:\n                        p *= (X - x[m]) / (x[k] - x[m])\n                s += y[k] * p\n            yield X, s"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _value_format(self, value):\n        return super(VerticalPyramid, self)._value_format(value and abs(value))", "response": "Format value for dual value display."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nseparating values between odd and even series stacked", "response": "def _get_separated_values(self, secondary=False):\n        \"\"\"Separate values between odd and even series stacked\"\"\"\n        series = self.secondary_series if secondary else self.series\n        positive_vals = map(\n            sum,\n            zip(\n                *[\n                    serie.safe_values for index, serie in enumerate(series)\n                    if index % 2\n                ]\n            )\n        )\n        negative_vals = map(\n            sum,\n            zip(\n                *[\n                    serie.safe_values for index, serie in enumerate(series)\n                    if not index % 2\n                ]\n            )\n        )\n        return list(positive_vals), list(negative_vals)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing Y min and max", "response": "def _compute_box(self, positive_vals, negative_vals):\n        \"\"\"Compute Y min and max\"\"\"\n        max_ = max(\n            max(positive_vals or [self.zero]),\n            max(negative_vals or [self.zero])\n        )\n\n        if self.range and self.range[0] is not None:\n            self._box.ymin = self.range[0]\n        else:\n            self._box.ymin = -max_\n\n        if self.range and self.range[1] is not None:\n            self._box.ymax = self.range[1]\n        else:\n            self._box.ymax = max_"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _pre_compute_secondary(self, positive_vals, negative_vals):\n        self._secondary_max = max(max(positive_vals), max(negative_vals))\n        self._secondary_min = -self._secondary_max", "response": "Compute secondary y min and max"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef prepare_values(self, raw, offset=0):\n        from pygal.graph.map import BaseMap\n        from pygal import Histogram\n\n        if self.zero == 0 and isinstance(self, BaseMap):\n            self.zero = 1\n\n        if self.x_label_rotation:\n            self.x_label_rotation %= 360\n\n        if self.y_label_rotation:\n            self.y_label_rotation %= 360\n\n        for key in ('x_labels', 'y_labels'):\n            if getattr(self, key):\n                setattr(self, key, list(getattr(self, key)))\n        if not raw:\n            return\n\n        adapters = list(self._adapters) or [lambda x: x]\n        if self.logarithmic:\n            for fun in not_zero, positive:\n                if fun in adapters:\n                    adapters.remove(fun)\n            adapters = adapters + [positive, not_zero]\n        adapters = adapters + [decimal_to_float]\n\n        self._adapt = reduce(compose, adapters) if not self.strict else ident\n        self._x_adapt = reduce(\n            compose, self._x_adapters\n        ) if not self.strict and getattr(self, '_x_adapters', None) else ident\n\n        series = []\n\n        raw = [(\n            list(raw_values) if not isinstance(raw_values, dict) else\n            raw_values, serie_config_kwargs\n        ) for raw_values, serie_config_kwargs in raw]\n\n        width = max([len(values)\n                     for values, _ in raw] + [len(self.x_labels or [])])\n\n        for raw_values, serie_config_kwargs in raw:\n            metadata = {}\n            values = []\n            if isinstance(raw_values, dict):\n                if isinstance(self, BaseMap):\n                    raw_values = list(raw_values.items())\n                else:\n                    value_list = [None] * width\n                    for k, v in raw_values.items():\n                        if k in (self.x_labels or []):\n                            value_list[self.x_labels.index(k)] = v\n                    raw_values = value_list\n\n            for index, raw_value in enumerate(raw_values + (\n                (width - len(raw_values)) * [None]  # aligning values\n                    if len(raw_values) < width else [])):\n                if isinstance(raw_value, dict):\n                    raw_value = dict(raw_value)\n                    value = raw_value.pop('value', None)\n                    metadata[index] = raw_value\n                else:\n                    value = raw_value\n\n                # Fix this by doing this in charts class methods\n                if isinstance(self, Histogram):\n                    if value is None:\n                        value = (None, None, None)\n                    elif not is_list_like(value):\n                        value = (value, self.zero, self.zero)\n                    elif len(value) == 2:\n                        value = (1, value[0], value[1])\n                    value = list(map(self._adapt, value))\n                elif self._dual:\n                    if value is None:\n                        value = (None, None)\n                    elif not is_list_like(value):\n                        value = (value, self.zero)\n                    if self._x_adapt:\n                        value = (\n                            self._x_adapt(value[0]), self._adapt(value[1])\n                        )\n                    if isinstance(self, BaseMap):\n                        value = (self._adapt(value[0]), value[1])\n                    else:\n                        value = list(map(self._adapt, value))\n                else:\n                    value = self._adapt(value)\n\n                values.append(value)\n            serie_config = SerieConfig()\n            serie_config(\n                **dict((k, v) for k, v in self.state.__dict__.items()\n                       if k in dir(serie_config))\n            )\n            serie_config(**serie_config_kwargs)\n            series.append(\n                Serie(offset + len(series), values, serie_config, metadata)\n            )\n        return series", "response": "Prepare the values to start with sane values"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset up the transient state prior rendering", "response": "def setup(self, **kwargs):\n        \"\"\"Set up the transient state prior rendering\"\"\"\n        # Keep labels in case of map\n        if getattr(self, 'x_labels', None) is not None:\n            self.x_labels = list(self.x_labels)\n        if getattr(self, 'y_labels', None) is not None:\n            self.y_labels = list(self.y_labels)\n        self.state = State(self, **kwargs)\n        if isinstance(self.style, type):\n            self.style = self.style()\n        self.series = self.prepare_values([\n            rs for rs in self.raw_series if not rs[1].get('secondary')\n        ]) or []\n        self.secondary_series = self.prepare_values([\n            rs for rs in self.raw_series if rs[1].get('secondary')\n        ], len(self.series)) or []\n        self.horizontal = getattr(self, 'horizontal', False)\n        self.svg = Svg(self)\n        self._x_labels = None\n        self._y_labels = None\n        self._x_2nd_labels = None\n        self._y_2nd_labels = None\n        self.nodes = {}\n        self.margin_box = Margin(\n            self.margin_top or self.margin, self.margin_right or self.margin,\n            self.margin_bottom or self.margin, self.margin_left or self.margin\n        )\n        self._box = Box()\n        self.view = None\n        if self.logarithmic and self.zero == 0:\n            # Explicit min to avoid interpolation dependency\n            positive_values = list(\n                filter(\n                    lambda x: x > 0, [\n                        val[1] or 1 if self._dual else val\n                        for serie in self.series for val in serie.safe_values\n                    ]\n                )\n            )\n\n            self.zero = min(positive_values or (1, )) or 1\n        if self._len < 3:\n            self.interpolate = None\n        self._draw()\n        self.svg.pre_render()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating the pygal test web app", "response": "def create_app():\n    \"\"\"Creates the pygal test web app\"\"\"\n\n    app = Flask(__name__)\n\n    @app.before_request\n    def before_request():\n        if request.args.get('etree'):\n            etree.to_etree()\n        elif request.args.get('lxml'):\n            etree.to_lxml()\n\n    def _random(data, order):\n        max = 10**order\n        min = 10**random.randrange(0, order)\n\n        series = []\n        for i in range(random.randrange(1, 10)):\n            values = [(\n                random_value((-max, min)[random.randrange(0, 2)], max),\n                random_value((-max, min)[random.randrange(0, 2)], max)\n            ) for i in range(data)]\n            series.append((random_label(), values, {}))\n        return series\n\n    def _random_series(type, data, order):\n        max = 10**order\n        min = 10**random.randrange(0, order)\n        with_secondary = bool(random.randint(0, 1))\n        series = []\n        for i in range(random.randrange(1, 10)):\n            if type == 'Pie':\n                values = random_value(min, max)\n            elif type == 'XY':\n                values = [(\n                    random_value((-max, min)[random.randrange(0, 2)], max),\n                    random_value((-max, min)[random.randrange(0, 2)], max)\n                ) for i in range(data)]\n            else:\n                values = [\n                    random_value((-max, min)[random.randrange(1, 2)], max)\n                    for i in range(data)\n                ]\n            config = {\n                'secondary': with_secondary and bool(random.randint(0, 1))\n            }\n            series.append((random_label(), values, config))\n        return series\n\n    from .tests import get_test_routes\n    links = get_test_routes(app)\n\n    @app.route(\"/\")\n    def index():\n        return render_template(\n            'index.jinja2',\n            styles=styles,\n            parametric_styles=parametric_styles,\n            parametric_colors=(\n                '#ff5995', '#b6e354', '#feed6c', '#8cedff', '#9e6ffe'\n            ),\n            links=links,\n            charts_name=pygal.CHARTS_NAMES\n        )\n\n    @app.route(\"/svg/<type>/<series>/<config>\")\n    def svg(type, series, config):\n        graph = get(type)(pickle.loads(b64decode(str(config))))\n        for title, values, serie_config in pickle.loads(b64decode(\n                str(series))):\n            graph.add(title, values, **serie_config)\n        return graph.render_response()\n\n    @app.route(\"/table/<type>/<series>/<config>\")\n    def table(type, series, config):\n        graph = get(type)(pickle.loads(b64decode(str(config))))\n        for title, values, serie_config in pickle.loads(b64decode(\n                str(series))):\n            graph.add(title, values, **serie_config)\n        return graph.render_table()\n\n    @app.route(\"/sparkline/<style>\")\n    @app.route(\"/sparkline/parameric/<style>/<color>\")\n    def sparkline(style, color=None):\n        if color is None:\n            style = styles[style]\n        else:\n            style = parametric_styles[style](color)\n\n        line = pygal.Line(style=style, pretty_print=True)\n        line.add('_', [random.randrange(0, 10) for _ in range(25)])\n        return Response(\n            line.render_sparkline(height=40), mimetype='image/svg+xml'\n        )\n\n    @app.route(\"/with/table/<type>\")\n    def with_table(type):\n        chart = pygal.StackedBar(\n            disable_xml_declaration=True, x_label_rotation=35\n        )\n        chart.title = (\n            'What Linux distro do you primarily use'\n            ' on your server computers? (Desktop'\n            ' users vs Server Users)'\n        )\n\n        if type == 'series':\n            chart.add('Debian', [1775, 82])\n            chart.add('Ubuntu', [1515, 80])\n            chart.add('CentOS', [807, 60])\n            chart.add('Arch Linux', [549, 12])\n            chart.add('Red Hat Enterprise Linux', [247, 10])\n            chart.add('Gentoo', [129, 7])\n            chart.add('Fedora', [91, 6])\n            chart.add('Amazon Linux', [60, 0])\n            chart.add('OpenSUSE', [58, 0])\n            chart.add('Slackware', [50, 3])\n            chart.add('Xubuntu', [38, 1])\n            chart.add('Rasbian', [33, 4])\n            chart.add('SUSE Linux Enterprise Server', [33, 1])\n            chart.add('Linux Mint', [30, 4])\n            chart.add('Scientific Linux', [32, 0])\n            chart.add('Other', [187, 5])\n\n        elif type == 'labels':\n            chart.x_labels = [\n                'Debian', 'Ubuntu', 'CentOS', 'Arch Linux',\n                'Red Hat Enterprise Linux', 'Gentoo', 'Fedora', 'Amazon Linux',\n                'OpenSUSE', 'Slackware', 'Xubuntu', 'Rasbian',\n                'SUSE Linux Enterprise Server', 'Linux Mint',\n                'Scientific Linux', 'Other'\n            ]\n            chart.add(\n                'Desktop Users', [\n                    1775, 1515, 807, 549, 247, 129, 91, 60, 58, 50, 38, 33, 33,\n                    30, 32, 187\n                ]\n            )\n            chart.add(\n                'Server Users',\n                [82, 80, 60, 12, 10, 7, 6, 0, 0, 3, 1, 4, 1, 4, 0, 5]\n            )\n\n        return render_template('table.jinja2', chart=chart)\n\n    @app.route(\"/all\")\n    @app.route(\"/all/<style>\")\n    @app.route(\"/all/<style>/<color>\")\n    @app.route(\"/all/<style>/<color>/<base_style>\")\n    @app.route(\"/all/interpolate=<interpolate>\")\n    def all(style='default', color=None, interpolate=None, base_style=None):\n        width, height = 600, 400\n        data = random.randrange(1, 10)\n        order = random.randrange(1, 10)\n        if color is None:\n            style = styles[style]\n        else:\n            style = parametric_styles[style](\n                color, base_style=styles[base_style or 'default']\n            )\n\n        xy_series = _random(data, order)\n        other_series = []\n        for title, values, config in xy_series:\n            other_series.append((title, cut(values, 1), config))\n        xy_series = b64encode(pickle.dumps(xy_series))\n        other_series = b64encode(pickle.dumps(other_series))\n        config = Config()\n        config.width = width\n        config.height = height\n        config.fill = bool(random.randrange(0, 2))\n        config.interpolate = interpolate\n        config.style = style\n        svgs = []\n        for chart in pygal.CHARTS:\n            type = '.'.join((chart.__module__, chart.__name__))\n            if chart._dual:\n                config.x_labels = None\n            else:\n                config.x_labels = [random_label() for i in range(data)]\n            svgs.append({\n                'type': type,\n                'series': xy_series if chart._dual else other_series,\n                'config': b64encode(pickle.dumps(config))\n            })\n\n        return render_template(\n            'svgs.jinja2', svgs=svgs, width=width, height=height\n        )\n\n    @app.route(\"/rotation\")\n    def rotation():\n        width, height = 375, 245\n        config = Config()\n        config.width = width\n        config.height = height\n        config.fill = True\n        config.style = styles['neon']\n        data = random.randrange(1, 10)\n        order = random.randrange(1, 10)\n        series = b64encode(pickle.dumps(_random_series(type, data, order)))\n        labels = [random_label() for i in range(data)]\n        svgs = []\n        config.show_legend = bool(random.randrange(0, 2))\n        for angle in range(0, 370, 10):\n            config.title = \"%d rotation\" % angle\n            config.x_labels = labels\n            config.x_label_rotation = angle\n            config.y_label_rotation = angle\n            svgs.append({\n                'type': 'pygal.Bar',\n                'series': series,\n                'config': b64encode(pickle.dumps(config))\n            })\n\n        return render_template(\n            'svgs.jinja2', svgs=svgs, width=width, height=height\n        )\n\n    @app.route(\"/interpolation\")\n    def interpolation():\n        width, height = 600, 400\n        config = Config()\n        config.width = width\n        config.height = height\n        config.fill = True\n        config.style = styles['neon']\n        data = random.randrange(1, 10)\n        order = random.randrange(1, 10)\n        series = b64encode(pickle.dumps(_random_series(type, data, order)))\n        svgs = []\n        for interpolation in 'quadratic', 'cubic', 'lagrange', 'trigonometric':\n            config.title = \"%s interpolation\" % interpolation\n            config.interpolate = interpolation\n            svgs.append({\n                'type': 'pygal.StackedLine',\n                'series': series,\n                'config': b64encode(pickle.dumps(config))\n            })\n\n        for params in [{'type': 'catmull_rom'}, {'type': 'finite_difference'},\n                       {'type': 'cardinal',\n                        'c': .25}, {'type': 'cardinal',\n                                    'c': .5}, {'type': 'cardinal', 'c': .75},\n                       {'type': 'cardinal',\n                        'c': 1.5}, {'type': 'cardinal',\n                                    'c': 2}, {'type': 'cardinal', 'c': 5},\n                       {'type': 'kochanek_bartels', 'b': 1, 'c': 1,\n                        't': 1}, {'type': 'kochanek_bartels', 'b': -1, 'c': 1,\n                                  't': 1}, {'type': 'kochanek_bartels', 'b': 1,\n                                            'c': -1, 't': 1},\n                       {'type': 'kochanek_bartels', 'b': 1, 'c': 1, 't': -1}, {\n                           'type': 'kochanek_bartels', 'b': -1, 'c': 1, 't': -1\n                       }, {'type': 'kochanek_bartels', 'b': -1, 'c': -1,\n                           't': 1}, {'type': 'kochanek_bartels', 'b': -1,\n                                     'c': -1, 't': -1}]:\n            config.title = \"Hermite interpolation with params %r\" % params\n            config.interpolate = 'hermite'\n            config.interpolation_parameters = params\n            svgs.append({\n                'type': 'pygal.StackedLine',\n                'series': series,\n                'config': b64encode(pickle.dumps(config))\n            })\n\n        return render_template(\n            'svgs.jinja2', svgs=svgs, width=width, height=height\n        )\n\n    @app.route(\"/raw_svgs/\")\n    def raw_svgs():\n        svgs = []\n        for color in styles['neon'].colors:\n            chart = pygal.Pie(\n                style=parametric_styles['rotate'](color),\n                width=400,\n                height=300\n            )\n            chart.title = color\n            chart.disable_xml_declaration = True\n            chart.explicit_size = True\n            chart.js = ['http://l:2343/2.0.x/pygal-tooltips.js']\n            for i in range(6):\n                chart.add(str(i), 2**i)\n            svgs.append(chart.render())\n        return render_template('raw_svgs.jinja2', svgs=svgs)\n\n    return app"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rgb_to_hsl(r, g, b):\n    r = r or 0\n    g = g or 0\n    b = b or 0\n    r /= 255\n    g /= 255\n    b /= 255\n    max_ = max((r, g, b))\n    min_ = min((r, g, b))\n    d = max_ - min_\n\n    if not d:\n        h = 0\n    elif r is max_:\n        h = 60 * (g - b) / d\n    elif g is max_:\n        h = 60 * (b - r) / d + 120\n    else:\n        h = 60 * (r - g) / d + 240\n\n    l = .5 * (max_ + min_)\n    if not d:\n        s = 0\n    elif l < 0.5:\n        s = .5 * d / l\n    else:\n        s = .5 * d / (1 - l)\n    return tuple(map(normalize_float, (h % 360, s * 100, l * 100)))", "response": "Convert a color in r g b to a color in h s l"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef hsl_to_rgb(h, s, l):\n    h /= 360\n    s /= 100\n    l /= 100\n\n    m2 = l * (s + 1) if l <= .5 else l + s - l * s\n    m1 = 2 * l - m2\n\n    def h_to_rgb(h):\n        h = h % 1\n        if 6 * h < 1:\n            return m1 + 6 * h * (m2 - m1)\n        if 2 * h < 1:\n            return m2\n        if 3 * h < 2:\n            return m1 + 6 * (2 / 3 - h) * (m2 - m1)\n        return m1\n\n    r, g, b = map(\n        lambda x: round(x * 255), map(h_to_rgb, (h + 1 / 3, h, h - 1 / 3))\n    )\n\n    return r, g, b", "response": "Convert a color in h s l to a color in r g b"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntakes any css color definition and give back a tuple containing the r g b a values along with a type which can be rgb rgba rrggbb rgb rgba rrggbbaa rgb rgba rrggbb rgb rgba rrggbbaa rgb rgba rrggbb rgb rgba rrggbb rgb rgba rrggbbaa rgb rgba rrggbb rgb rgba rrggbb rgb rgba r g b rgb rgba r b a type", "response": "def parse_color(color):\n    \"\"\"Take any css color definition and give back a tuple containing the\n    r, g, b, a values along with a type which can be: #rgb, #rgba, #rrggbb,\n    #rrggbbaa, rgb, rgba\n    \"\"\"\n    r = g = b = a = type = None\n    if color.startswith('#'):\n        color = color[1:]\n        if len(color) == 3:\n            type = '#rgb'\n            color = color + 'f'\n        if len(color) == 4:\n            type = type or '#rgba'\n            color = ''.join([c * 2 for c in color])\n        if len(color) == 6:\n            type = type or '#rrggbb'\n            color = color + 'ff'\n        assert len(color) == 8\n        type = type or '#rrggbbaa'\n        r, g, b, a = [\n            int(''.join(c), 16) for c in zip(color[::2], color[1::2])\n        ]\n        a /= 255\n    elif color.startswith('rgb('):\n        type = 'rgb'\n        color = color[4:-1]\n        r, g, b, a = [int(c) for c in color.split(',')] + [1]\n    elif color.startswith('rgba('):\n        type = 'rgba'\n        color = color[5:-1]\n        r, g, b, a = [int(c) for c in color.split(',')[:-1]\n                      ] + [float(color.split(',')[-1])]\n    return r, g, b, a, type"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a css color string for the specified color values.", "response": "def unparse_color(r, g, b, a, type):\n    \"\"\"\n    Take the r, g, b, a color values and give back\n    a type css color string. This is the inverse function of parse_color\n    \"\"\"\n    if type == '#rgb':\n        # Don't lose precision on rgb shortcut\n        if r % 17 == 0 and g % 17 == 0 and b % 17 == 0:\n            return '#%x%x%x' % (int(r / 17), int(g / 17), int(b / 17))\n        type = '#rrggbb'\n\n    if type == '#rgba':\n        if r % 17 == 0 and g % 17 == 0 and b % 17 == 0:\n            return '#%x%x%x%x' % (\n                int(r / 17), int(g / 17), int(b / 17), int(a * 15)\n            )\n        type = '#rrggbbaa'\n\n    if type == '#rrggbb':\n        return '#%02x%02x%02x' % (r, g, b)\n\n    if type == '#rrggbbaa':\n        return '#%02x%02x%02x%02x' % (r, g, b, int(a * 255))\n\n    if type == 'rgb':\n        return 'rgb(%d, %d, %d)' % (r, g, b)\n\n    if type == 'rgba':\n        return 'rgba(%d, %d, %d, %g)' % (r, g, b, a)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadjusts an attribute of color by a percent", "response": "def adjust(color, attribute, percent):\n    \"\"\"Adjust an attribute of color by a percent\"\"\"\n    r, g, b, a, type = parse_color(color)\n    r, g, b = hsl_to_rgb(*_adjust(rgb_to_hsl(r, g, b), attribute, percent))\n    return unparse_color(r, g, b, a, type)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmatch term glob against short - name.", "response": "def do_list(lookup, term):\n    \"\"\"Matches term glob against short-name.\"\"\"\n\n    space = lookup.keys()\n    matches = fnmatch.filter(space, term)\n\n    return [(m, translate(lookup, m)) for m in matches]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmatches term glob against short - name keywords and categories.", "response": "def do_find(lookup, term):\n    \"\"\"Matches term glob against short-name, keywords and categories.\"\"\"\n\n    space = defaultdict(list)\n\n    for name in lookup.keys():\n        space[name].append(name)\n\n    try:\n        iter_lookup = lookup.iteritems()  # Python 2\n    except AttributeError:\n        iter_lookup = lookup.items()  # Python 3\n\n    for name, definition in iter_lookup:\n        for keyword in definition['keywords']:\n            space[keyword].append(name)\n        space[definition['category']].append(name)\n\n    matches = fnmatch.filter(space.keys(), term)\n\n    results = set()\n    for match in matches:\n        results.update(space[match])\n\n    return [(r, translate(lookup, r)) for r in results]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef compile(expr, params=None):\n    from ibis.sql.alchemy import to_sqlalchemy\n\n    return to_sqlalchemy(expr, dialect.make_context(params=params))", "response": "Compile the expression into SQLAlchemy"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the unique names in an AST.", "response": "def find_names(node):\n    \"\"\"Return the unique :class:`ast.Name` instances in an AST.\n\n    Parameters\n    ----------\n    node : ast.AST\n\n    Returns\n    -------\n    unique_names : List[ast.Name]\n\n    Examples\n    --------\n    >>> import ast\n    >>> node = ast.parse('a + b')\n    >>> names = find_names(node)\n    >>> names  # doctest: +ELLIPSIS\n    [<_ast.Name object at 0x...>, <_ast.Name object at 0x...>]\n    >>> names[0].id\n    'a'\n    >>> names[1].id\n    'b'\n    \"\"\"\n    return list(\n        toolz.unique(\n            filter(None, NameFinder().find(node)),\n            key=lambda node: (node.id, type(node.ctx)),\n        )\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef put_tarfile(\n        self,\n        hdfs_path,\n        local_path,\n        compression='gzip',\n        verbose=None,\n        overwrite=False,\n    ):\n        \"\"\"\n        Write contents of tar archive to HDFS directly without having to\n        decompress it locally first\n\n        Parameters\n        ----------\n        hdfs_path : string\n        local_path : string\n        compression : {'gzip', 'bz2', None}\n        overwrite : boolean, default False\n        verbose : boolean, default None (global default)\n        \"\"\"\n        import tarfile\n\n        modes = {None: 'r', 'gzip': 'r:gz', 'bz2': 'r:bz2'}\n\n        if compression not in modes:\n            raise ValueError(\n                'Invalid compression type {0}'.format(compression)\n            )\n        mode = modes[compression]\n\n        tf = tarfile.open(local_path, mode=mode)\n        for info in tf:\n            if not info.isfile():\n                continue\n\n            buf = tf.extractfile(info)\n            abspath = posixpath.join(hdfs_path, info.path)\n            self.put(abspath, buf, verbose=verbose, overwrite=overwrite)", "response": "Writes contents of tar archive to HDFS directly without having to to\n        decompress it locally first"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef delete(self, hdfs_path, recursive=False):\n        return self.client.delete(hdfs_path, recursive=recursive)", "response": "Delete a file located at hdfs_path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _warn_if_deprecated(key):\n\n    d = _get_deprecated_option(key)\n    if d:\n        if d.msg:\n            print(d.msg)\n            warnings.warn(d.msg, DeprecationWarning)\n        else:\n            msg = \"'%s' is deprecated\" % key\n            if d.removal_ver:\n                msg += ' and will be removed in %s' % d.removal_ver\n            if d.rkey:\n                msg += \", please use '%s' instead.\" % d.rkey\n            else:\n                msg += ', please refrain from using it.'\n\n            warnings.warn(msg, DeprecationWarning)\n        return True\n    return False", "response": "Checks if key is a deprecated option and prints a warning."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbuild a formatted description of a registered option and prints it.", "response": "def _build_option_description(k):\n    \"\"\"Builds a formatted description of a registered option and prints it.\"\"\"\n    o = _get_registered_option(k)\n    d = _get_deprecated_option(k)\n\n    buf = ['{} '.format(k)]\n\n    if o.doc:\n        doc = '\\n'.join(o.doc.strip().splitlines())\n    else:\n        doc = 'No description available.'\n\n    buf.append(doc)\n\n    if o:\n        buf.append(\n            '\\n    [default: {}] [currently: {}]'.format(\n                o.defval, _get_option(k, True)\n            )\n        )\n\n    if d:\n        buf.append(\n            '\\n    (Deprecated{})'.format(\n                ', use `{}` instead.'.format(d.rkey) if d.rkey else ''\n            )\n        )\n\n    buf.append('\\n\\n')\n    return ''.join(buf)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pp_options_list(keys, width=80, _print=False):\n\n    from textwrap import wrap\n    from itertools import groupby\n\n    def pp(name, ks):\n        pfx = '- ' + name + '.[' if name else ''\n        ls = wrap(\n            ', '.join(ks),\n            width,\n            initial_indent=pfx,\n            subsequent_indent='  ',\n            break_long_words=False,\n        )\n        if ls and ls[-1] and name:\n            ls[-1] = ls[-1] + ']'\n        return ls\n\n    ls = []\n    singles = [x for x in sorted(keys) if x.find('.') < 0]\n    if singles:\n        ls += pp('', singles)\n    keys = [x for x in keys if x.find('.') >= 0]\n\n    for k, g in groupby(sorted(keys), lambda x: x[: x.rfind('.')]):\n        ks = [x[len(k) + 1 :] for x in list(g)]\n        ls += pp(k, ks)\n    s = '\\n'.join(ls)\n    if _print:\n        print(s)\n    else:\n        return s", "response": "Builds a concise listing of available options grouped by prefix"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a function that checks that the input value has the given type.", "response": "def is_type_factory(_type):\n    \"\"\"\n    Parameters\n    ----------\n    `_type` - a type to be compared against (e.g. type(x) == `_type`)\n    Returns\n    -------\n    validator - a function of a single argument x , which returns the\n                True if type(x) is equal to `_type`\n    \"\"\"\n\n    def inner(x):\n        if type(x) != _type:\n            raise ValueError(\"Value must have type '%s'\" % str(_type))\n\n    return inner"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a function that checks if x is an instance of _type.", "response": "def is_instance_factory(_type):\n    \"\"\"\n    Parameters\n    ----------\n    `_type` - the type to be checked against\n    Returns\n    -------\n    validator - a function of a single argument x , which returns the\n                True if x is an instance of `_type`\n    \"\"\"\n    if isinstance(_type, (tuple, list)):\n        _type = tuple(_type)\n        type_repr = \"|\".join(map(str, _type))\n    else:\n        type_repr = \"'%s'\" % _type\n\n    def inner(x):\n        if not isinstance(x, _type):\n            raise ValueError(\"Value must be an instance of %s\" % type_repr)\n\n    return inner"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck whether two objects left and right are equal.", "response": "def all_equal(left, right, cache=None):\n    \"\"\"Check whether two objects `left` and `right` are equal.\n\n    Parameters\n    ----------\n    left : Union[object, Expr, Node]\n    right : Union[object, Expr, Node]\n    cache : Optional[Dict[Tuple[Node, Node], bool]]\n        A dictionary indicating whether two Nodes are equal\n    \"\"\"\n    if cache is None:\n        cache = {}\n\n    if util.is_iterable(left):\n        # check that left and right are equal length iterables and that all\n        # of their elements are equal\n        return (\n            util.is_iterable(right)\n            and len(left) == len(right)\n            and all(\n                itertools.starmap(\n                    functools.partial(all_equal, cache=cache), zip(left, right)\n                )\n            )\n        )\n\n    if hasattr(left, 'equals'):\n        return left.equals(right, cache=cache)\n    return left == right"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a new instance of the object with the specified result expression.", "response": "def else_(self, result_expr):\n        \"\"\"\n        Specify\n\n        Returns\n        -------\n        builder : CaseBuilder\n        \"\"\"\n        kwargs = {\n            slot: getattr(self, slot)\n            for slot in self.__slots__\n            if slot != 'default'\n        }\n\n        result_expr = ir.as_value_expr(result_expr)\n        kwargs['default'] = result_expr\n        # Maintain immutability\n        return type(self)(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a new case - result pair to the set of cases and results.", "response": "def when(self, case_expr, result_expr):\n        \"\"\"\n        Add a new case-result pair.\n\n        Parameters\n        ----------\n        case : Expr\n          Expression to equality-compare with base expression. Must be\n          comparable with the base.\n        result : Expr\n          Value when the case predicate evaluates to true.\n\n        Returns\n        -------\n        builder : CaseBuilder\n        \"\"\"\n        case_expr = ir.as_value_expr(case_expr)\n        result_expr = ir.as_value_expr(result_expr)\n\n        if not rlz.comparable(self.base, case_expr):\n            raise TypeError(\n                'Base expression and passed case are not ' 'comparable'\n            )\n\n        cases = list(self.cases)\n        cases.append(case_expr)\n\n        results = list(self.results)\n        results.append(result_expr)\n\n        # Maintain immutability\n        return type(self)(self.base, cases, results, self.default)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds a new case - result pair to the set of cases and results.", "response": "def when(self, case_expr, result_expr):\n        \"\"\"\n        Add a new case-result pair.\n\n        Parameters\n        ----------\n        case : Expr\n          Expression to equality-compare with base expression. Must be\n          comparable with the base.\n        result : Expr\n          Value when the case predicate evaluates to true.\n\n        Returns\n        -------\n        builder : CaseBuilder\n        \"\"\"\n        case_expr = ir.as_value_expr(case_expr)\n        result_expr = ir.as_value_expr(result_expr)\n\n        if not isinstance(case_expr, ir.BooleanValue):\n            raise TypeError(case_expr)\n\n        cases = list(self.cases)\n        cases.append(case_expr)\n\n        results = list(self.results)\n        results.append(result_expr)\n\n        # Maintain immutability\n        return type(self)(cases, results, self.default)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef validate(self, value=_undefined, name=None):\n        if self.optional:\n            if value is _undefined or value is None:\n                if self.default is None:\n                    return None\n                elif util.is_function(self.default):\n                    value = self.default()\n                else:\n                    value = self.default\n        elif value is _undefined:\n            if name is not None:\n                name_msg = \"argument `{}`\".format(name)\n            else:\n                name_msg = \"unnamed argument\"\n            raise TypeError(\"Missing required value for {}\".format(name_msg))\n\n        return self.validator(value)", "response": "Validate the value of the parameter."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a table expression that references a particular table in the specified database.", "response": "def table(self, name, database=None):\n        \"\"\"\n        Create a table expression that references a particular table in the\n        database\n\n        Parameters\n        ----------\n        name : string\n        database : string, optional\n\n        Returns\n        -------\n        table : TableExpr\n        \"\"\"\n        qualified_name = self._fully_qualified_name(name, database)\n        schema = self._get_table_schema(qualified_name)\n        node = self.table_class(qualified_name, schema, self)\n        return self.table_expr_class(node)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a Database object for a given database name.", "response": "def database(self, name=None):\n        \"\"\"\n        Create a Database object for a given database name that can be used for\n        exploring and manipulating the objects (tables, functions, views, etc.)\n        inside\n\n        Parameters\n        ----------\n        name : string\n          Name of database\n\n        Returns\n        -------\n        database : Database\n        \"\"\"\n        # TODO: validate existence of database\n        if name is None:\n            name = self.current_database\n        return self.database_class(name, self)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a SQL query to an Ibis table expression", "response": "def sql(self, query):\n        \"\"\"\n        Convert a SQL query to an Ibis table expression\n\n        Parameters\n        ----------\n\n        Returns\n        -------\n        table : TableExpr\n        \"\"\"\n        # Get the schema by adding a LIMIT 0 on to the end of the query. If\n        # there is already a limit in the query, we find and remove it\n        limited_query = 'SELECT * FROM ({}) t0 LIMIT 0'.format(query)\n        schema = self._get_schema_using_query(limited_query)\n        return ops.SQLQueryResult(query, schema, self).to_expr()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nexecute a given query string and return the result set.", "response": "def raw_sql(self, query, results=False):\n        \"\"\"\n        Execute a given query string. Could have unexpected results if the\n        query modifies the behavior of the session in a way unknown to Ibis; be\n        careful.\n\n        Parameters\n        ----------\n        query : string\n          DML or DDL statement\n        results : boolean, default False\n          Pass True if the query as a result set\n\n        Returns\n        -------\n        cur : ImpalaCursor if results=True, None otherwise\n          You must call cur.release() after you are finished using the cursor.\n        \"\"\"\n        return self._execute(query, results=results)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncompiles and execute an Ibis expression and return the result in - memory in the appropriate object type.", "response": "def execute(self, expr, params=None, limit='default', **kwargs):\n        \"\"\"\n        Compile and execute Ibis expression using this backend client\n        interface, returning results in-memory in the appropriate object type\n\n        Parameters\n        ----------\n        expr : Expr\n        limit : int, default None\n          For expressions yielding result yets; retrieve at most this number of\n          values/rows. Overrides any limit already set on the expression.\n        params : not yet implemented\n\n        Returns\n        -------\n        output : input type dependent\n          Table expressions: pandas.DataFrame\n          Array expressions: pandas.Series\n          Scalar expressions: Python scalar value\n        \"\"\"\n        query_ast = self._build_ast_ensure_limit(expr, limit, params=params)\n        result = self._execute_query(query_ast, **kwargs)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef compile(self, expr, params=None, limit=None):\n        query_ast = self._build_ast_ensure_limit(expr, limit, params=params)\n        return query_ast.compile()", "response": "Translate expression to one or more queries according to backend target\n        Returns ------- single query or list of queries\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef explain(self, expr, params=None):\n        if isinstance(expr, ir.Expr):\n            context = self.dialect.make_context(params=params)\n            query_ast = self._build_ast(expr, context)\n            if len(query_ast.queries) > 1:\n                raise Exception('Multi-query expression')\n\n            query = query_ast.queries[0].compile()\n        else:\n            query = expr\n\n        statement = 'EXPLAIN {0}'.format(query)\n\n        with self._execute(statement, results=True) as cur:\n            result = self._get_list(cur)\n\n        return 'Query:\\n{0}\\n\\n{1}'.format(\n            util.indent(query, 2), '\\n'.join(result)\n        )", "response": "Query for and return the query plan associated with the indicated\n        expression or SQL query."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef drop(self, force=False):\n        self.client.drop_database(self.name, force=force)", "response": "Drop the database\n\n        Parameters\n        ----------\n        drop : boolean, default False\n          Drop any objects if they exist, and do not fail if the databaes does\n          not exist"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a table expression referencing a table in this database.", "response": "def table(self, name):\n        \"\"\"\n        Return a table expression referencing a table in this database\n\n        Returns\n        -------\n        table : TableExpr\n        \"\"\"\n        qualified_name = self._qualify(name)\n        return self.client.table(qualified_name, self.name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load(data, udf, data_dir, overwrite):\n    con = make_ibis_client(ENV)\n\n    # validate our environment before performing possibly expensive operations\n    if not can_write_to_hdfs(con):\n        raise IbisError('Failed to write to HDFS; check your settings')\n    if udf and not can_build_udfs():\n        raise IbisError('Build environment does not support building UDFs')\n\n    # load the data files\n    if data:\n        load_impala_data(con, str(data_dir), overwrite)\n    else:\n        logger.info('Skipping Ibis test data load (--no-data)')\n\n    # build and upload the UDFs\n    if udf:\n        already_loaded = is_udf_loaded(con)\n        logger.info('Attempting to build and load test UDFs')\n        if already_loaded and not overwrite:\n            logger.info('UDFs already loaded and not overwriting; moving on')\n        else:\n            if already_loaded:\n                logger.info('UDFs already loaded; attempting to overwrite')\n            logger.info('Building UDFs')\n            build_udfs()\n            logger.info('Uploading UDFs')\n            upload_udfs(con)\n    else:\n        logger.info('Skipping UDF build/load (--no-udf)')", "response": "Load Ibis test data and build and upload UDFs"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cleanup(test_data, udfs, tmp_data, tmp_db):\n    con = make_ibis_client(ENV)\n\n    if udfs:\n        # this comes before test_data bc the latter clobbers this too\n        con.hdfs.rmdir(os.path.join(ENV.test_data_dir, 'udf'))\n\n    if test_data:\n        con.drop_database(ENV.test_data_db, force=True)\n        con.hdfs.rmdir(ENV.test_data_dir)\n\n    if tmp_data:\n        con.hdfs.rmdir(ENV.tmp_dir)\n\n    if tmp_db:\n        con.drop_database(ENV.tmp_db, force=True)", "response": "Cleanup Ibis test data and UDFs"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsubstituting subexpressions in expr with expression to expression mapping substitutions.", "response": "def sub_for(expr, substitutions):\n    \"\"\"Substitute subexpressions in `expr` with expression to expression\n    mapping `substitutions`.\n\n    Parameters\n    ----------\n    expr : ibis.expr.types.Expr\n        An Ibis expression\n    substitutions : List[Tuple[ibis.expr.types.Expr, ibis.expr.types.Expr]]\n        A mapping from expression to expression. If any subexpression of `expr`\n        is equal to any of the keys in `substitutions`, the value for that key\n        will replace the corresponding expression in `expr`.\n\n    Returns\n    -------\n    ibis.expr.types.Expr\n        An Ibis expression\n    \"\"\"\n    mapping = {k.op(): v for k, v in substitutions}\n    substitutor = Substitutor()\n    return substitutor.substitute(expr, mapping)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding every occurrence of a TableExpr in expr.", "response": "def find_immediate_parent_tables(expr):\n    \"\"\"Find every first occurrence of a :class:`ibis.expr.types.TableExpr`\n    object in `expr`.\n\n    Parameters\n    ----------\n    expr : ir.Expr\n\n    Yields\n    ------\n    e : ir.Expr\n\n    Notes\n    -----\n    This function does not traverse into TableExpr objects. This means that the\n    underlying PhysicalTable of a Selection will not be yielded, for example.\n\n    Examples\n    --------\n    >>> import ibis, toolz\n    >>> t = ibis.table([('a', 'int64')], name='t')\n    >>> expr = t.mutate(foo=t.a + 1)\n    >>> result = list(find_immediate_parent_tables(expr))\n    >>> len(result)\n    1\n    >>> result[0]  # doctest: +NORMALIZE_WHITESPACE\n    ref_0\n    UnboundTable[table]\n      name: t\n      schema:\n        a : int64\n    Selection[table]\n      table:\n        Table: ref_0\n      selections:\n        Table: ref_0\n        foo = Add[int64*]\n          left:\n            a = Column[int64*] 'a' from table\n              ref_0\n          right:\n            Literal[int8]\n              1\n    \"\"\"\n\n    def finder(expr):\n        if isinstance(expr, ir.TableExpr):\n            return lin.halt, expr\n        else:\n            return lin.proceed, None\n\n    return lin.traverse(finder, expr)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn True if expr contains a reduction.", "response": "def has_reduction(expr):\n    \"\"\"Does `expr` contain a reduction?\n\n    Parameters\n    ----------\n    expr : ibis.expr.types.Expr\n        An ibis expression\n\n    Returns\n    -------\n    truth_value : bool\n        Whether or not there's at least one reduction in `expr`\n\n    Notes\n    -----\n    The ``isinstance(op, ops.TableNode)`` check in this function implies\n    that we only examine every non-table expression that precedes the first\n    table expression.\n    \"\"\"\n\n    def fn(expr):\n        op = expr.op()\n        if isinstance(op, ops.TableNode):  # don't go below any table nodes\n            return lin.halt, None\n        if isinstance(op, ops.Reduction):\n            return lin.halt, True\n        return lin.proceed, None\n\n    reduction_status = lin.traverse(fn, expr)\n    return any(reduction_status)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find_source_table(expr):\n\n    def finder(expr):\n        if isinstance(expr, ir.TableExpr):\n            return lin.halt, expr\n        else:\n            return lin.proceed, None\n\n    first_tables = lin.traverse(finder, expr.op().flat_args())\n    options = list(toolz.unique(first_tables, key=methodcaller('op')))\n\n    if len(options) > 1:\n        raise NotImplementedError('More than one base table not implemented')\n\n    return options[0]", "response": "Find the first table expression observed for each argument that the expression depends on."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef flatten_predicate(expr):\n\n    def predicate(expr):\n        if isinstance(expr.op(), ops.And):\n            return lin.proceed, None\n        else:\n            return lin.halt, expr\n\n    return list(lin.traverse(predicate, expr, type=ir.BooleanColumn))", "response": "Yield the expressions corresponding to the And nodes of expr."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_reduction(expr):\n\n    def has_reduction(op):\n        if getattr(op, '_reduction', False):\n            return True\n\n        for arg in op.args:\n            if isinstance(arg, ir.ScalarExpr) and has_reduction(arg.op()):\n                return True\n\n        return False\n\n    return has_reduction(expr.op() if isinstance(expr, ir.Expr) else expr)", "response": "Checks whether an expression is a reduction or not."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsubstitute expressions with other expressions.", "response": "def _substitute(self, expr, mapping):\n        \"\"\"Substitute expressions with other expressions.\n\n        Parameters\n        ----------\n        expr : ibis.expr.types.Expr\n        mapping : Mapping[ibis.expr.operations.Node, ibis.expr.types.Expr]\n\n        Returns\n        -------\n        ibis.expr.types.Expr\n        \"\"\"\n        node = expr.op()\n        try:\n            return mapping[node]\n        except KeyError:\n            if node.blocks():\n                return expr\n\n            new_args = list(node.args)\n            unchanged = True\n            for i, arg in enumerate(new_args):\n                if isinstance(arg, ir.Expr):\n                    new_arg = self.substitute(arg, mapping)\n                    unchanged = unchanged and new_arg is arg\n                    new_args[i] = new_arg\n            if unchanged:\n                return expr\n            try:\n                new_node = type(node)(*new_args)\n            except IbisTypeError:\n                return expr\n\n            try:\n                name = expr.get_name()\n            except ExpressionError:\n                name = None\n            return expr._factory(new_node, name=name)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a NULL scalar", "response": "def null():\n    \"\"\"Create a NULL/NA scalar\"\"\"\n    import ibis.expr.operations as ops\n\n    global _NULL\n    if _NULL is None:\n        _NULL = ops.NullLiteral().to_expr()\n\n    return _NULL"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef literal(value, type=None):\n    import ibis.expr.datatypes as dt\n    import ibis.expr.operations as ops\n\n    if hasattr(value, 'op') and isinstance(value.op(), ops.Literal):\n        return value\n\n    try:\n        inferred_dtype = dt.infer(value)\n    except com.InputTypeError:\n        has_inferred = False\n    else:\n        has_inferred = True\n\n    if type is None:\n        has_explicit = False\n    else:\n        has_explicit = True\n        explicit_dtype = dt.dtype(type)\n\n    if has_explicit and has_inferred:\n        try:\n            # ensure type correctness: check that the inferred dtype is\n            # implicitly castable to the explicitly given dtype and value\n            dtype = inferred_dtype.cast(explicit_dtype, value=value)\n        except com.IbisTypeError:\n            raise TypeError(\n                'Value {!r} cannot be safely coerced to {}'.format(value, type)\n            )\n    elif has_explicit:\n        dtype = explicit_dtype\n    elif has_inferred:\n        dtype = inferred_dtype\n    else:\n        raise TypeError(\n            'The datatype of value {!r} cannot be inferred, try '\n            'passing it explicitly with the `type` keyword.'.format(value)\n        )\n\n    if dtype is dt.null:\n        return null().cast(dtype)\n    else:\n        return ops.Literal(value, dtype=dtype).to_expr()", "response": "Create a scalar expression from a Python value."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sequence(values):\n    import ibis.expr.operations as ops\n\n    return ops.ValueList(values).to_expr()", "response": "Returns a sequence representation of a list of Python values."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef param(type):\n    import ibis.expr.datatypes as dt\n    import ibis.expr.operations as ops\n\n    return ops.ScalarParameter(dt.dtype(type)).to_expr()", "response": "Create a new parameter of a particular type that is defined just before the execution of the current sequence."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef visualize(self, format='svg'):\n        import ibis.expr.visualize as viz\n\n        path = viz.draw(viz.to_graph(self), format=format)\n        webbrowser.open('file://{}'.format(os.path.abspath(path)))", "response": "Visualize an expression in the browser as an SVG image."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef execute(self, limit='default', params=None, **kwargs):\n        from ibis.client import execute\n\n        return execute(self, limit=limit, params=params, **kwargs)", "response": "Execute this expression against the backend and return the result."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompile the sequence of keys into a sequence of values.", "response": "def compile(self, limit=None, params=None):\n        \"\"\"\n        Compile expression to whatever execution target, to verify\n\n        Returns\n        -------\n        compiled : value or list\n           query representation or list thereof\n        \"\"\"\n        from ibis.client import compile\n\n        return compile(self, limit=limit, params=params)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the concatenation of this expression lists and the others expression lists.", "response": "def concat(self, *others):\n        \"\"\"\n        Concatenate expression lists\n\n        Returns\n        -------\n        combined : ExprList\n        \"\"\"\n        import ibis.expr.operations as ops\n\n        exprs = list(self.exprs())\n        for o in others:\n            if not isinstance(o, ExprList):\n                raise TypeError(o)\n            exprs.extend(o.exprs())\n        return ops.ExpressionList(exprs).to_expr()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\npromoting this column expression to a table projection", "response": "def to_projection(self):\n        \"\"\"\n        Promote this column expression to a table projection\n        \"\"\"\n        roots = self._root_tables()\n        if len(roots) > 1:\n            raise com.RelationError(\n                'Cannot convert array expression '\n                'involving multiple base table references '\n                'to a projection'\n            )\n\n        table = TableExpr(roots[0])\n        return table.projection([self])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_column(self, name):\n        import ibis.expr.operations as ops\n\n        ref = ops.TableColumn(name, self)\n        return ref.to_expr()", "response": "Get a reference to a single column from the table\n       "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef group_by(self, by=None, **additional_grouping_expressions):\n        from ibis.expr.groupby import GroupedTableExpr\n\n        return GroupedTableExpr(self, by, **additional_grouping_expressions)", "response": "Create an intermediate grouped table expression that groups the table by the given values."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_aggregation(\n        self, metric_name=None, parent_table=None, backup_metric_name=None\n    ):\n        \"\"\"\n        Convert the TopK operation to a table aggregation\n        \"\"\"\n        op = self.op()\n\n        arg_table = find_base_table(op.arg)\n\n        by = op.by\n        if not isinstance(by, Expr):\n            by = by(arg_table)\n            by_table = arg_table\n        else:\n            by_table = find_base_table(op.by)\n\n        if metric_name is None:\n            if by.get_name() == op.arg.get_name():\n                by = by.name(backup_metric_name)\n        else:\n            by = by.name(metric_name)\n\n        if arg_table.equals(by_table):\n            agg = arg_table.aggregate(by, by=[op.arg])\n        elif parent_table is not None:\n            agg = parent_table.aggregate(by, by=[op.arg])\n        else:\n            raise com.IbisError(\n                'Cross-table TopK; must provide a parent ' 'joined table'\n            )\n\n        return agg.sort_by([(by.get_name(), False)]).limit(op.k)", "response": "Convert the TopK operation to a table aggregation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef index(self):\n        import ibis.expr.operations as ops\n\n        return ops.DayOfWeekIndex(self.op().arg).to_expr()", "response": "Get the index of the day of the week."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the name of the day of the week.", "response": "def full_name(self):\n        \"\"\"Get the name of the day of the week.\n\n        Returns\n        -------\n        StringValue\n            The name of the day of the week\n        \"\"\"\n        import ibis.expr.operations as ops\n\n        return ops.DayOfWeekName(self.op().arg).to_expr()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nindenting lines by spaces spaces.", "response": "def indent(lines, spaces=4):\n    \"\"\"Indent `lines` by `spaces` spaces.\n\n    Parameters\n    ----------\n    lines : Union[str, List[str]]\n        A string or list of strings to indent\n    spaces : int\n        The number of spaces to indent `lines`\n\n    Returns\n    -------\n    indented_lines : str\n    \"\"\"\n    if isinstance(lines, str):\n        text = [lines]\n    text = '\\n'.join(lines)\n    return textwrap.indent(text, ' ' * spaces)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a semicolon to the result of a visit_* call.", "response": "def semicolon(f):\n    \"\"\"Add a semicolon to the result of a visit_* call.\n\n    Parameters\n    ----------\n    f : callable\n    \"\"\"\n\n    @functools.wraps(f)\n    def wrapper(*args, **kwargs):\n        return f(*args, **kwargs) + ';'\n\n    return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nassigning symbols to local variables.", "response": "def local_scope(self):\n        \"\"\"Assign symbols to local variables.\n        \"\"\"\n        self.scope = self.scope.new_child()\n        try:\n            yield self.scope\n        finally:\n            self.scope = self.scope.parents"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef visit_ListComp(self, node):\n        try:\n            generator, = node.generators\n        except ValueError:\n            raise NotImplementedError(\n                'Only single loop comprehensions are allowed'\n            )\n\n        names = find_names(generator.target)\n        argslist = [ast.arg(arg=name.id, annotation=None) for name in names]\n        if len(names) <= 1:\n            signature = ast.arguments(\n                args=argslist,\n                vararg=None,\n                kwonlyargs=[],\n                kw_defaults=[],\n                kwarg=None,\n                defaults=[],\n            )\n        else:\n            signature = ast.List(elts=argslist, ctx=ast.Load())\n\n        array = generator.iter\n        lam_sig = functools.partial(ast.Lambda, args=signature)\n\n        filters = generator.ifs\n        if filters:\n            filt = ast.BoolOp(op=ast.And(), values=filters)\n            # array.filter\n            method = ast.Attribute(value=array, attr='filter', ctx=ast.Load())\n            # array.filter(func)\n            array = ast.Call(\n                func=method, args=[lam_sig(body=filt)], keywords=[]\n            )\n\n        method = ast.Attribute(value=array, attr='map', ctx=ast.Load())\n        mapped = ast.Call(\n            func=method, args=[lam_sig(body=node.elt)], keywords=[]\n        )\n        result = self.visit(mapped)\n        return result", "response": "Generate a curried lambda function\n            for list comprehensions."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting an ibis dtype to the pandas alternative", "response": "def ibis_dtype_to_pandas(ibis_dtype):\n    \"\"\"Convert ibis dtype to the pandas / numpy alternative\"\"\"\n    assert isinstance(ibis_dtype, dt.DataType)\n\n    if isinstance(ibis_dtype, dt.Timestamp) and ibis_dtype.timezone:\n        return DatetimeTZDtype('ns', ibis_dtype.timezone)\n    elif isinstance(ibis_dtype, dt.Interval):\n        return np.dtype('timedelta64[{}]'.format(ibis_dtype.unit))\n    elif isinstance(ibis_dtype, dt.Category):\n        return CategoricalDtype()\n    elif type(ibis_dtype) in _ibis_dtypes:\n        return _ibis_dtypes[type(ibis_dtype)]\n    else:\n        return np.dtype(np.object_)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting obj to the timezone timezone.", "response": "def convert_timezone(obj, timezone):\n    \"\"\"Convert `obj` to the timezone `timezone`.\n\n    Parameters\n    ----------\n    obj : datetime.date or datetime.datetime\n\n    Returns\n    -------\n    type(obj)\n    \"\"\"\n    if timezone is None:\n        return obj.replace(tzinfo=None)\n    return pytz.timezone(timezone).localize(obj)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ibis_schema_apply_to(schema, df):\n\n    for column, dtype in schema.items():\n        pandas_dtype = dtype.to_pandas()\n        col = df[column]\n        col_dtype = col.dtype\n\n        try:\n            not_equal = pandas_dtype != col_dtype\n        except TypeError:\n            # ugh, we can't compare dtypes coming from pandas, assume not equal\n            not_equal = True\n\n        if not_equal or dtype == dt.string:\n            df[column] = convert(col_dtype, dtype, col)\n\n    return df", "response": "Applies the Ibis schema to a pandas DataFrame df."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads the data from a pandas. DataFrame into the internal dictionary.", "response": "def load_data(self, table_name, obj, **kwargs):\n        \"\"\"\n        Parameters\n        ----------\n        table_name : string\n        obj: pandas.DataFrame\n        \"\"\"\n        # kwargs is a catch all for any options required by other backends.\n        self.dictionary[table_name] = pd.DataFrame(obj)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef database(self, name=None):\n        if name == self.current_database or (\n            name is None and name != self.current_database\n        ):\n            return self.database_class(self.current_database, self)\n        else:\n            url = self.con.url\n            client_class = type(self)\n            new_client = client_class(\n                host=url.host,\n                user=url.username,\n                port=url.port,\n                password=url.password,\n                database=name,\n            )\n            return self.database_class(name, new_client)", "response": "Connect to a database called name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a table expression that references a particular a table containing a particular schema.", "response": "def table(self, name, database=None, schema=None):\n        \"\"\"Create a table expression that references a particular a table\n        called `name` in a MySQL database called `database`.\n\n        Parameters\n        ----------\n        name : str\n            The name of the table to retrieve.\n        database : str, optional\n            The database in which the table referred to by `name` resides. If\n            ``None`` then the ``current_database`` is used.\n        schema : str, optional\n            The schema in which the table resides.  If ``None`` then the\n            `public` schema is assumed.\n\n        Returns\n        -------\n        table : TableExpr\n            A table expression.\n        \"\"\"\n        if database is not None and database != self.current_database:\n            return self.database(name=database).table(name=name, schema=schema)\n        else:\n            alch_table = self._get_sqla_table(name, schema=schema)\n            node = self.table_class(alch_table, self, self._schemas.get(name))\n            return self.table_expr_class(node)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _generate_tokens(pat: GenericAny, text: str) -> Iterator[Token]:\n    rules = _TYPE_RULES\n    keys = _TYPE_KEYS\n    groupindex = pat.groupindex\n    scanner = pat.scanner(text)\n    for m in iter(scanner.match, None):\n        lastgroup = m.lastgroup\n        func = rules[keys[groupindex[lastgroup] - 1]]\n        if func is not None:\n            yield func(m.group(lastgroup))", "response": "Generate a sequence of tokens from text that match pat"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef infer_struct(value: Mapping[str, GenericAny]) -> Struct:\n    if not value:\n        raise TypeError('Empty struct type not supported')\n    return Struct(list(value.keys()), list(map(infer, value.values())))", "response": "Infer the : class ~ibis. expr. datatypes. Struct type of value."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninfers the : class ~ibis. expr. datatypes. Map type of value.", "response": "def infer_map(value: Mapping[GenericAny, GenericAny]) -> Map:\n    \"\"\"Infer the :class:`~ibis.expr.datatypes.Map` type of `value`.\"\"\"\n    if not value:\n        return Map(null, null)\n    return Map(\n        highest_precedence(map(infer, value.keys())),\n        highest_precedence(map(infer, value.values())),\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninfers the : class ~ibis. expr. datatypes. Array type of values.", "response": "def infer_list(values: List[GenericAny]) -> Array:\n    \"\"\"Infer the :class:`~ibis.expr.datatypes.Array` type of `values`.\"\"\"\n    if not values:\n        return Array(null)\n    return Array(highest_precedence(map(infer, values)))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef infer_set(values: GenericSet) -> Set:\n    if not values:\n        return Set(null)\n    return Set(highest_precedence(map(infer, values)))", "response": "Infer the set type of values."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nattempts to implicitly cast from source dtype to target dtype", "response": "def cast(\n    source: Union[DataType, str], target: Union[DataType, str], **kwargs\n) -> DataType:\n    \"\"\"Attempts to implicitly cast from source dtype to target dtype\"\"\"\n    source, result_target = dtype(source), dtype(target)\n\n    if not castable(source, result_target, **kwargs):\n        raise com.IbisTypeError(\n            'Datatype {} cannot be implicitly '\n            'casted to {}'.format(source, result_target)\n        )\n    return result_target"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef type(self) -> DataType:\n        if self._accept(Tokens.PRIMITIVE):\n            assert self.tok is not None\n            return self.tok.value\n\n        elif self._accept(Tokens.TIMESTAMP):\n            if self._accept(Tokens.LPAREN):\n                self._expect(Tokens.STRARG)\n                assert self.tok is not None\n                timezone = self.tok.value[1:-1]  # remove surrounding quotes\n                self._expect(Tokens.RPAREN)\n                return Timestamp(timezone=timezone)\n            return timestamp\n\n        elif self._accept(Tokens.TIME):\n            return Time()\n\n        elif self._accept(Tokens.INTERVAL):\n            if self._accept(Tokens.LBRACKET):\n                self._expect(Tokens.PRIMITIVE)\n                assert self.tok is not None\n                value_type = self.tok.value\n                self._expect(Tokens.RBRACKET)\n            else:\n                value_type = int32\n\n            if self._accept(Tokens.LPAREN):\n                self._expect(Tokens.STRARG)\n                assert self.tok is not None\n                unit = self.tok.value[1:-1]  # remove surrounding quotes\n                self._expect(Tokens.RPAREN)\n            else:\n                unit = 's'\n\n            return Interval(unit, value_type)\n\n        elif self._accept(Tokens.DECIMAL):\n            if self._accept(Tokens.LPAREN):\n                self._expect(Tokens.INTEGER)\n                assert self.tok is not None\n                precision = self.tok.value\n\n                self._expect(Tokens.COMMA)\n\n                self._expect(Tokens.INTEGER)\n                scale = self.tok.value\n\n                self._expect(Tokens.RPAREN)\n            else:\n                precision = 9\n                scale = 0\n            return Decimal(precision, scale)\n\n        elif self._accept(Tokens.VARCHAR) or self._accept(Tokens.CHAR):\n            # VARCHAR, VARCHAR(n), CHAR, and CHAR(n) all parse as STRING\n            if self._accept(Tokens.LPAREN):\n                self._expect(Tokens.INTEGER)\n                self._expect(Tokens.RPAREN)\n                return string\n            return string\n\n        elif self._accept(Tokens.ARRAY):\n            self._expect(Tokens.LBRACKET)\n\n            value_type = self.type()\n\n            self._expect(Tokens.RBRACKET)\n            return Array(value_type)\n\n        elif self._accept(Tokens.SET):\n            self._expect(Tokens.LBRACKET)\n\n            value_type = self.type()\n\n            self._expect(Tokens.RBRACKET)\n            return Set(value_type)\n\n        elif self._accept(Tokens.MAP):\n            self._expect(Tokens.LBRACKET)\n\n            self._expect(Tokens.PRIMITIVE)\n            assert self.tok is not None\n            key_type = self.tok.value\n\n            self._expect(Tokens.COMMA)\n\n            value_type = self.type()\n\n            self._expect(Tokens.RBRACKET)\n\n            return Map(key_type, value_type)\n\n        elif self._accept(Tokens.STRUCT):\n            self._expect(Tokens.LBRACKET)\n\n            self._expect(Tokens.FIELD)\n            assert self.tok is not None\n            names = [self.tok.value]\n\n            self._expect(Tokens.COLON)\n\n            types = [self.type()]\n\n            while self._accept(Tokens.COMMA):\n                self._expect(Tokens.FIELD)\n                names.append(self.tok.value)\n\n                self._expect(Tokens.COLON)\n                types.append(self.type())\n\n            self._expect(Tokens.RBRACKET)\n            return Struct(names, types)\n\n        # geo spatial data type\n        elif self._accept(Tokens.POINT):\n            geotype = None\n            srid = None\n\n            if self._accept(Tokens.SEMICOLON):\n                self._expect(Tokens.INTEGER)\n                assert self.tok is not None\n                srid = self.tok.value\n\n            if self._accept(Tokens.COLON):\n                if self._accept(Tokens.GEOGRAPHY):\n                    geotype = 'geography'\n                elif self._accept(Tokens.GEOMETRY):\n                    geotype = 'geometry'\n\n            return Point(geotype=geotype, srid=srid)\n\n        elif self._accept(Tokens.LINESTRING):\n            geotype = None\n            srid = None\n\n            if self._accept(Tokens.SEMICOLON):\n                self._expect(Tokens.INTEGER)\n                assert self.tok is not None\n                srid = self.tok.value\n\n            if self._accept(Tokens.COLON):\n                if self._accept(Tokens.GEOGRAPHY):\n                    geotype = 'geography'\n                elif self._accept(Tokens.GEOMETRY):\n                    geotype = 'geometry'\n\n            return LineString(geotype=geotype, srid=srid)\n\n        elif self._accept(Tokens.POLYGON):\n            geotype = None\n            srid = None\n\n            if self._accept(Tokens.SEMICOLON):\n                self._expect(Tokens.INTEGER)\n                assert self.tok is not None\n                srid = self.tok.value\n\n            if self._accept(Tokens.COLON):\n                if self._accept(Tokens.GEOGRAPHY):\n                    geotype = 'geography'\n                elif self._accept(Tokens.GEOMETRY):\n                    geotype = 'geometry'\n\n            return Polygon(geotype=geotype, srid=srid)\n\n        elif self._accept(Tokens.MULTIPOLYGON):\n            geotype = None\n            srid = None\n\n            if self._accept(Tokens.SEMICOLON):\n                self._expect(Tokens.INTEGER)\n                assert self.tok is not None\n                srid = self.tok.value\n\n            if self._accept(Tokens.COLON):\n                if self._accept(Tokens.GEOGRAPHY):\n                    geotype = 'geography'\n                elif self._accept(Tokens.GEOMETRY):\n                    geotype = 'geometry'\n\n            return MultiPolygon(geotype=geotype, srid=srid)\n\n        else:\n            raise SyntaxError('Type cannot be parsed: {}'.format(self.text))", "response": "Return a list of the types of the current locale."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef verify(expr, params=None):\n    try:\n        compile(expr, params=params)\n        return True\n    except com.TranslationError:\n        return False", "response": "Determine if expression can be translated to execute on\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a MapDClient for use with Ibis", "response": "def connect(\n    uri=None,\n    user=None,\n    password=None,\n    host=None,\n    port=9091,\n    database=None,\n    protocol='binary',\n    execution_type=EXECUTION_TYPE_CURSOR,\n):\n    \"\"\"Create a MapDClient for use with Ibis\n\n    Parameters could be\n\n    :param uri: str\n    :param user: str\n    :param password: str\n    :param host: str\n    :param port: int\n    :param database: str\n    :param protocol: str\n    :param execution_type: int\n    Returns\n    -------\n    MapDClient\n\n    \"\"\"\n    client = MapDClient(\n        uri=uri,\n        user=user,\n        password=password,\n        host=host,\n        port=port,\n        database=database,\n        protocol=protocol,\n        execution_type=execution_type,\n    )\n\n    if options.default_backend is None:\n        options.default_backend = client\n\n    return client"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_udf_node(name, fields):\n    definition = next(_udf_name_cache[name])\n    external_name = '{}_{:d}'.format(name, definition)\n    return type(external_name, (BigQueryUDFNode,), fields)", "response": "Create a new UDF node type."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef udf(input_type, output_type, strict=True, libraries=None):\n    '''Define a UDF for BigQuery\n\n    Parameters\n    ----------\n    input_type : List[DataType]\n    output_type : DataType\n    strict : bool\n        Whether or not to put a ``'use strict';`` string at the beginning of\n        the UDF. Setting to ``False`` is probably a bad idea.\n    libraries : List[str]\n        A list of Google Cloud Storage URIs containing to JavaScript source\n        code. Note that any symbols (functions, classes, variables, etc.) that\n        are exposed in these JavaScript files will be visible inside the UDF.\n\n    Returns\n    -------\n    wrapper : Callable\n        The wrapped function\n\n    Notes\n    -----\n    ``INT64`` is not supported as an argument type or a return type, as per\n    `the BigQuery documentation\n    <https://cloud.google.com/bigquery/docs/reference/standard-sql/user-defined-functions#sql-type-encodings-in-javascript>`_.\n\n    Examples\n    --------\n    >>> from ibis.bigquery import udf\n    >>> import ibis.expr.datatypes as dt\n    >>> @udf(input_type=[dt.double], output_type=dt.double)\n    ... def add_one(x):\n    ...     return x + 1\n    >>> print(add_one.js)\n    CREATE TEMPORARY FUNCTION add_one_0(x FLOAT64)\n    RETURNS FLOAT64\n    LANGUAGE js AS \"\"\"\n    'use strict';\n    function add_one(x) {\n        return (x + 1);\n    }\n    return add_one(x);\n    \"\"\";\n    >>> @udf(input_type=[dt.double, dt.double],\n    ...      output_type=dt.Array(dt.double))\n    ... def my_range(start, stop):\n    ...     def gen(start, stop):\n    ...         curr = start\n    ...         while curr < stop:\n    ...             yield curr\n    ...             curr += 1\n    ...     result = []\n    ...     for value in gen(start, stop):\n    ...         result.append(value)\n    ...     return result\n    >>> print(my_range.js)\n    CREATE TEMPORARY FUNCTION my_range_0(start FLOAT64, stop FLOAT64)\n    RETURNS ARRAY<FLOAT64>\n    LANGUAGE js AS \"\"\"\n    'use strict';\n    function my_range(start, stop) {\n        function* gen(start, stop) {\n            let curr = start;\n            while ((curr < stop)) {\n                yield curr;\n                curr += 1;\n            }\n        }\n        let result = [];\n        for (let value of gen(start, stop)) {\n            result.push(value);\n        }\n        return result;\n    }\n    return my_range(start, stop);\n    \"\"\";\n    >>> @udf(\n    ...     input_type=[dt.double, dt.double],\n    ...     output_type=dt.Struct.from_tuples([\n    ...         ('width', 'double'), ('height', 'double')\n    ...     ])\n    ... )\n    ... def my_rectangle(width, height):\n    ...     class Rectangle:\n    ...         def __init__(self, width, height):\n    ...             self.width = width\n    ...             self.height = height\n    ...\n    ...         @property\n    ...         def area(self):\n    ...             return self.width * self.height\n    ...\n    ...         def perimeter(self):\n    ...             return 2 * (self.width + self.height)\n    ...\n    ...     return Rectangle(width, height)\n    >>> print(my_rectangle.js)\n    CREATE TEMPORARY FUNCTION my_rectangle_0(width FLOAT64, height FLOAT64)\n    RETURNS STRUCT<width FLOAT64, height FLOAT64>\n    LANGUAGE js AS \"\"\"\n    'use strict';\n    function my_rectangle(width, height) {\n        class Rectangle {\n            constructor(width, height) {\n                this.width = width;\n                this.height = height;\n            }\n            get area() {\n                return (this.width * this.height);\n            }\n            perimeter() {\n                return (2 * (this.width + this.height));\n            }\n        }\n        return (new Rectangle(width, height));\n    }\n    return my_rectangle(width, height);\n    \"\"\";\n    '''\n    if libraries is None:\n        libraries = []\n\n    def wrapper(f):\n        if not callable(f):\n            raise TypeError('f must be callable, got {}'.format(f))\n\n        signature = inspect.signature(f)\n        parameter_names = signature.parameters.keys()\n\n        udf_node_fields = collections.OrderedDict(\n            [\n                (name, Arg(rlz.value(type)))\n                for name, type in zip(parameter_names, input_type)\n            ]\n            + [\n                (\n                    'output_type',\n                    lambda self, output_type=output_type: rlz.shape_like(\n                        self.args, dtype=output_type\n                    ),\n                ),\n                ('__slots__', ('js',)),\n            ]\n        )\n\n        udf_node = create_udf_node(f.__name__, udf_node_fields)\n\n        @compiles(udf_node)\n        def compiles_udf_node(t, expr):\n            return '{}({})'.format(\n                udf_node.__name__, ', '.join(map(t.translate, expr.op().args))\n            )\n\n        type_translation_context = UDFContext()\n        return_type = ibis_type_to_bigquery_type(\n            dt.dtype(output_type), type_translation_context\n        )\n        bigquery_signature = ', '.join(\n            '{name} {type}'.format(\n                name=name,\n                type=ibis_type_to_bigquery_type(\n                    dt.dtype(type), type_translation_context\n                ),\n            )\n            for name, type in zip(parameter_names, input_type)\n        )\n        source = PythonToJavaScriptTranslator(f).compile()\n        js = '''\\\nCREATE TEMPORARY FUNCTION {external_name}({signature})\nRETURNS {return_type}\nLANGUAGE js AS \"\"\"\n{strict}{source}\nreturn {internal_name}({args});\n\"\"\"{libraries};'''.format(\n            external_name=udf_node.__name__,\n            internal_name=f.__name__,\n            return_type=return_type,\n            source=source,\n            signature=bigquery_signature,\n            strict=repr('use strict') + ';\\n' if strict else '',\n            args=', '.join(parameter_names),\n            libraries=(\n                '\\nOPTIONS (\\n    library={}\\n)'.format(repr(list(libraries)))\n                if libraries\n                else ''\n            ),\n        )\n\n        @functools.wraps(f)\n        def wrapped(*args, **kwargs):\n            node = udf_node(*args, **kwargs)\n            node.js = js\n            return node.to_expr()\n\n        wrapped.__signature__ = signature\n        wrapped.js = js\n        return wrapped\n\n    return wrapper", "response": "Define a UDF for BigQuery\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef execute_cross_join(op, left, right, **kwargs):\n    # generate a unique name for the temporary join key\n    key = \"cross_join_{}\".format(ibis.util.guid())\n    join_key = {key: True}\n    new_left = left.assign(**join_key)\n    new_right = right.assign(**join_key)\n\n    # inner/outer doesn't matter because every row matches every other row\n    result = pd.merge(\n        new_left,\n        new_right,\n        how='inner',\n        on=key,\n        copy=False,\n        suffixes=constants.JOIN_SUFFIXES,\n    )\n\n    # remove the generated key\n    del result[key]\n\n    return result", "response": "Execute a cross join in pandas."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmerge a pull request.", "response": "def merge_pr(\n    pr_num: int,\n    base_ref: str,\n    target_ref: str,\n    commit_title: str,\n    body: str,\n    pr_repo_desc: str,\n    original_head: str,\n    remote: str,\n    merge_method: str,\n    github_user: str,\n    password: str,\n) -> None:\n    \"\"\"Merge a pull request.\"\"\"\n    git_log = git[\n        \"log\",\n        \"{remote}/{target_ref}..{base_ref}\".format(\n            remote=remote, target_ref=target_ref, base_ref=base_ref\n        ),\n    ]\n\n    commit_authors = git_log[\"--pretty=format:%an <%ae>\"]().splitlines()\n    author_count = collections.Counter(commit_authors)\n    distinct_authors = [author for author, _ in author_count.most_common()]\n    commits = git_log[\"--pretty=format:%h [%an] %s\"]().splitlines()\n\n    merge_message_pieces = []\n    if body:\n        merge_message_pieces.append(\"\\n\".join(textwrap.wrap(body)))\n    merge_message_pieces.extend(map(\"Author: {}\".format, distinct_authors))\n\n    # The string \"Closes #{pull_request_number:d}\" is required for GitHub to\n    # correctly close the PR\n    merge_message_pieces.append(\n        (\n            \"\\nCloses #{pr_num:d} from {pr_repo_desc} and squashes the \"\n            \"following commits:\\n\"\n        ).format(pr_num=pr_num, pr_repo_desc=pr_repo_desc)\n    )\n    merge_message_pieces += commits\n\n    commit_message = \"\\n\".join(merge_message_pieces)\n    # PUT /repos/:owner/:repo/pulls/:number/merge\n    resp = requests.put(\n        \"{GITHUB_API_BASE}/pulls/{pr_num:d}/merge\".format(\n            GITHUB_API_BASE=GITHUB_API_BASE, pr_num=pr_num\n        ),\n        json=dict(\n            commit_title=commit_title,\n            commit_message=commit_message,\n            merge_method=merge_method,\n        ),\n        auth=(github_user, password),\n    )\n    resp.raise_for_status()\n    if resp.status_code == 200:\n        resp_json = resp.json()\n        merged = resp_json[\"merged\"]\n        assert merged is True, merged\n        click.echo(\n            \"Pull request #{pr_num:d} successfully merged.\".format(\n                pr_num=pr_num\n            )\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef execute_series_lead_lag_timedelta(\n    op, data, offset, default, aggcontext=None, **kwargs\n):\n    \"\"\"An implementation of shifting a column relative to another one that is\n    in units of time rather than rows.\n    \"\"\"\n    # lagging adds time (delayed), leading subtracts time (moved up)\n    func = operator.add if isinstance(op, ops.Lag) else operator.sub\n    group_by = aggcontext.group_by\n    order_by = aggcontext.order_by\n\n    # get the parent object from which `data` originated\n    parent = aggcontext.parent\n\n    # get the DataFrame from the parent object, handling the DataFrameGroupBy\n    # case\n    parent_df = getattr(parent, 'obj', parent)\n\n    # index our parent df by grouping and ordering keys\n    indexed_original_df = parent_df.set_index(group_by + order_by)\n\n    # perform the time shift\n    adjusted_parent_df = parent_df.assign(\n        **{k: func(parent_df[k], offset) for k in order_by}\n    )\n\n    # index the parent *after* adjustment\n    adjusted_indexed_parent = adjusted_parent_df.set_index(group_by + order_by)\n\n    # get the column we care about\n    result = adjusted_indexed_parent[getattr(data, 'obj', data).name]\n\n    # reindex the shifted data by the original frame's index\n    result = result.reindex(indexed_original_df.index)\n\n    # add a default if necessary\n    return post_lead_lag(result, default)", "response": "An implementation of shifting a column relative to another one that is\n    in units of time rather than rows."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load_data(self, df):\n        stmt = ddl.LoadData(self._qualified_name, df)\n        return self._execute(stmt)", "response": "Wraps the LOAD DATA DDL statement. Loads data into an MapD table from pandas. DataFrame or pyarrow. Table\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef rename(self, new_name, database=None):\n        m = ddl.fully_qualified_re.match(new_name)\n        if not m and database is None:\n            database = self._database\n\n        statement = ddl.RenameTable(\n            self._qualified_name, new_name, new_database=database\n        )\n\n        self._client._execute(statement)\n\n        op = self.op().change_name(statement.new_qualified_name)\n        return type(self)(op)", "response": "Rename the MapD table."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef alter(self, tbl_properties=None):\n\n        def _run_ddl(**kwds):\n            stmt = ddl.AlterTable(self._qualified_name, **kwds)\n            return self._execute(stmt)\n\n        return self._alter_table_helper(\n            _run_ddl, tbl_properties=tbl_properties\n        )", "response": "Execute an Alter Table statement and return the new table properties."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_database(self, name, owner=None):\n        statement = ddl.CreateDatabase(name, owner=owner)\n        self._execute(statement)", "response": "Create a new MapD database with the specified name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndrop an existing MapD database.", "response": "def drop_database(self, name, force=False):\n        \"\"\"\n        Drop an MapD database\n\n        Parameters\n        ----------\n        name : string\n          Database name\n        force : boolean, default False\n          If False and there are any tables in this database, raises an\n          IntegrityError\n        \"\"\"\n        tables = []\n\n        if not force or self.database(name):\n            tables = self.list_tables(database=name)\n\n        if not force and len(tables):\n            raise com.IntegrityError(\n                'Database {0} must be empty before being dropped, or set '\n                'force=True'.format(name)\n            )\n        statement = ddl.DropDatabase(name)\n        self._execute(statement)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a new user in MapD.", "response": "def create_user(self, name, password, is_super=False):\n        \"\"\"\n        Create a new MapD user\n\n        Parameters\n        ----------\n        name : string\n          User name\n        password : string\n          Password\n        is_super : bool\n          if user is a superuser\n        \"\"\"\n        statement = ddl.CreateUser(\n            name=name, password=password, is_super=is_super\n        )\n        self._execute(statement)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef alter_user(\n        self, name, password=None, is_super=None, insert_access=None\n    ):\n        \"\"\"\n        Alter MapD user parameters\n\n        Parameters\n        ----------\n        name : string\n          User name\n        password : string\n          Password\n        is_super : bool\n          If user is a superuser\n        insert_access : string\n          If users need to insert records to a database they do not own,\n          use insert_access property to give them the required privileges.\n        \"\"\"\n        statement = ddl.AlterUser(\n            name=name,\n            password=password,\n            is_super=is_super,\n            insert_access=insert_access,\n        )\n        self._execute(statement)", "response": "Alter MapD user parameters\n\n        Parameters\n        ----------\n        name : string\n          User name\n        password : string\n          Password\n        is_super : bool\n          If user is a superuser\n        insert_access : string\n          If users need to insert records to a database they do not own,\n          use insert_access property to give them the required privileges."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndrop an MapD user from the database.", "response": "def drop_user(self, name):\n        \"\"\"\n        Drop an MapD user\n\n        Parameters\n        ----------\n        name : string\n          Database name\n        \"\"\"\n        statement = ddl.DropUser(name)\n        self._execute(statement)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_view(self, name, expr, database=None):\n        ast = self._build_ast(expr, MapDDialect.make_context())\n        select = ast.queries[0]\n        statement = ddl.CreateView(name, select, database=database)\n        self._execute(statement)", "response": "Create an MapD view from a table expression"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef drop_view(self, name, database=None):\n        statement = ddl.DropView(name, database=database)\n        self._execute(statement, False)", "response": "Drop an MapD view."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new table in MapD using an Ibis table expression.", "response": "def create_table(\n        self, table_name, obj=None, schema=None, database=None, max_rows=None\n    ):\n        \"\"\"\n        Create a new table in MapD using an Ibis table expression.\n\n        Parameters\n        ----------\n        table_name : string\n        obj : TableExpr or pandas.DataFrame, optional\n          If passed, creates table from select statement results\n        schema : ibis.Schema, optional\n          Mutually exclusive with expr, creates an empty table with a\n          particular schema\n        database : string, default None (optional)\n        max_rows : int, Default None\n          Set the maximum number of rows allowed in a table to create a capped\n          collection. When this limit is reached, the oldest fragment is\n          removed. Default = 2^62.\n\n        Examples\n        --------\n        >>> con.create_table('new_table_name', table_expr)  # doctest: +SKIP\n        \"\"\"\n        _database = self.db_name\n        self.set_database(database)\n\n        if obj is not None:\n            if isinstance(obj, pd.DataFrame):\n                raise NotImplementedError(\n                    'Pandas Data Frame input not implemented.'\n                )\n            else:\n                to_insert = obj\n            ast = self._build_ast(to_insert, MapDDialect.make_context())\n            select = ast.queries[0]\n\n            statement = ddl.CTAS(table_name, select, database=database)\n        elif schema is not None:\n            statement = ddl.CreateTableWithSchema(\n                table_name, schema, database=database, max_rows=max_rows\n            )\n        else:\n            raise com.IbisError('Must pass expr or schema')\n\n        result = self._execute(statement, False)\n\n        self.set_database(_database)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndropping an MapD table", "response": "def drop_table(self, table_name, database=None, force=False):\n        \"\"\"\n        Drop an MapD table\n\n        Parameters\n        ----------\n        table_name : string\n        database : string, default None (optional)\n        force : boolean, default False\n          Database may throw exception if table does not exist\n\n        Examples\n        --------\n        >>> table = 'my_table'\n        >>> db = 'operations'\n        >>> con.drop_table(table, database=db, force=True)  # doctest: +SKIP\n        \"\"\"\n        _database = self.db_name\n        self.set_database(database)\n\n        statement = ddl.DropTable(\n            table_name, database=database, must_exist=not force\n        )\n        self._execute(statement, False)\n        self.set_database(_database)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef truncate_table(self, table_name, database=None):\n        statement = ddl.TruncateTable(table_name, database=database)\n        self._execute(statement, False)", "response": "Deletes all rows from table_name but do not drop an existing table_name"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nattempts to drop a relation that may be a view or table.", "response": "def drop_table_or_view(self, name, database=None, force=False):\n        \"\"\"\n        Attempt to drop a relation that may be a view or table\n        \"\"\"\n        try:\n            self.drop_table(name, database=database)\n        except Exception as e:\n            try:\n                self.drop_view(name, database=database)\n            except Exception:\n                raise e"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconnects to a database called name.", "response": "def database(self, name=None):\n        \"\"\"Connect to a database called `name`.\n\n        Parameters\n        ----------\n        name : str, optional\n            The name of the database to connect to. If ``None``, return\n            the database named ``self.current_database``.\n\n        Returns\n        -------\n        db : Database\n            An :class:`ibis.client.Database` instance.\n\n        Notes\n        -----\n        This creates a new connection if `name` is both not ``None`` and not\n        equal to the current database.\n        \"\"\"\n        if name == self.current_database or name is None:\n            return self.database_class(self.current_database, self)\n        else:\n            client_class = type(self)\n            new_client = client_class(\n                uri=self.uri,\n                user=self.user,\n                password=self.password,\n                host=self.host,\n                port=self.port,\n                database=name,\n                protocol=self.protocol,\n                execution_type=self.execution_type,\n            )\n            return self.database_class(name, new_client)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_data(self, table_name, obj, database=None, **kwargs):\n        _database = self.db_name\n        self.set_database(database)\n        self.con.load_table(table_name, obj, **kwargs)\n        self.set_database(_database)", "response": "Wraps the LOAD DATA DDL statement. Loads data into an MapD table by using the specified table name and object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef exists_table(self, name, database=None):\n        return bool(self.list_tables(like=name, database=database))", "response": "Determines if the indicated table or view exists in the specified database."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_schema(self, table_name, database=None):\n        col_names = []\n        col_types = []\n\n        for col in self.con.get_table_details(table_name):\n            col_names.append(col.name)\n            col_types.append(MapDDataType.parse(col.type))\n\n        return sch.schema(\n            [\n                (col.name, MapDDataType.parse(col.type))\n                for col in self.con.get_table_details(table_name)\n            ]\n        )", "response": "Returns a Schema object for the indicated table and database."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef connect(\n    project_id: Optional[str] = None,\n    dataset_id: Optional[str] = None,\n    credentials: Optional[google.auth.credentials.Credentials] = None,\n) -> BigQueryClient:\n    \"\"\"Create a BigQueryClient for use with Ibis.\n\n    Parameters\n    ----------\n    project_id : str\n        A BigQuery project id.\n    dataset_id : str\n        A dataset id that lives inside of the project indicated by\n        `project_id`.\n    credentials : google.auth.credentials.Credentials\n\n    Returns\n    -------\n    BigQueryClient\n\n    \"\"\"\n    if credentials is None:\n        credentials_cache = pydata_google_auth.cache.ReadWriteCredentialsCache(\n            filename=\"ibis.json\"\n        )\n        credentials, project_id = pydata_google_auth.default(\n            SCOPES,\n            client_id=CLIENT_ID,\n            client_secret=CLIENT_SECRET,\n            credentials_cache=credentials_cache,\n        )\n\n    return BigQueryClient(\n        project_id, dataset_id=dataset_id, credentials=credentials\n    )", "response": "Create a BigQueryClient for use with Ibis."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef udf(f):\n\n    @functools.wraps(f)\n    def wrapper(*args):\n        if any(arg is None for arg in args):\n            return None\n        return f(*args)\n\n    _SQLITE_UDF_REGISTRY.add(wrapper)\n    return wrapper", "response": "Create a SQLite scalar UDF from f."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _ibis_sqlite_regex_extract(string, pattern, index):\n    result = re.search(pattern, string)\n    if result is not None and 0 <= index <= (result.lastindex or -1):\n        return result.group(index)\n    return None", "response": "Extract match of regular expression pattern from string at index."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _register_function(func, con):\n    nargs = number_of_arguments(func)\n    con.connection.connection.create_function(func.__name__, nargs, func)", "response": "Register a Python callable with a SQLite connection con."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nregister a Python class that performs aggregation in SQLite.", "response": "def _register_aggregate(agg, con):\n    \"\"\"Register a Python class that performs aggregation in SQLite.\n\n    Parameters\n    ----------\n    agg : type\n    con : sqlalchemy.Connection\n    \"\"\"\n    nargs = number_of_arguments(agg.step) - 1  # because self\n    con.connection.connection.create_aggregate(agg.__name__, nargs, agg)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconnects another SQLite database file to a specific database.", "response": "def attach(self, name, path, create=False):\n        \"\"\"Connect another SQLite database file\n\n        Parameters\n        ----------\n        name : string\n            Database name within SQLite\n        path : string\n            Path to sqlite3 file\n        create : boolean, optional\n            If file does not exist, create file if True otherwise raise an\n            Exception\n        \"\"\"\n        if not os.path.exists(path) and not create:\n            raise com.IbisError('File {!r} does not exist'.format(path))\n\n        self.raw_sql(\n            \"ATTACH DATABASE {path!r} AS {name}\".format(\n                path=path,\n                name=self.con.dialect.identifier_preparer.quote(name),\n            )\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef table(self, name, database=None):\n        alch_table = self._get_sqla_table(name, schema=database)\n        node = self.table_class(alch_table, self)\n        return self.table_expr_class(node)", "response": "Create a table expression that references a particular table in the specified database."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconnects to HDFS. Parameters ---------- host : str Host name of the HDFS NameNode port : int NameNode's WebHDFS port protocol : str, The protocol used to communicate with HDFS. The only valid value is ``'webhdfs'``. use_https : bool Connect to WebHDFS with HTTPS, otherwise plain HTTP. For secure authentication, the default for this is True, otherwise False. auth_mechanism : str Set to NOSASL or PLAIN for non-secure clusters. Set to GSSAPI or LDAP for Kerberos-secured clusters. verify : bool Set to :data:`False` to turn off verifying SSL certificates. session : Optional[requests.Session] A custom :class:`requests.Session` object. Notes ----- Other keywords are forwarded to HDFS library classes. Returns ------- WebHDFS", "response": "def hdfs_connect(\n    host='localhost',\n    port=50070,\n    protocol='webhdfs',\n    use_https='default',\n    auth_mechanism='NOSASL',\n    verify=True,\n    session=None,\n    **kwds\n):\n    \"\"\"Connect to HDFS.\n\n    Parameters\n    ----------\n    host : str\n        Host name of the HDFS NameNode\n    port : int\n        NameNode's WebHDFS port\n    protocol : str,\n        The protocol used to communicate with HDFS. The only valid value is\n        ``'webhdfs'``.\n    use_https : bool\n        Connect to WebHDFS with HTTPS, otherwise plain HTTP. For secure\n        authentication, the default for this is True, otherwise False.\n    auth_mechanism : str\n        Set to NOSASL or PLAIN for non-secure clusters.\n        Set to GSSAPI or LDAP for Kerberos-secured clusters.\n    verify : bool\n        Set to :data:`False` to turn off verifying SSL certificates.\n    session : Optional[requests.Session]\n        A custom :class:`requests.Session` object.\n\n    Notes\n    -----\n    Other keywords are forwarded to HDFS library classes.\n\n    Returns\n    -------\n    WebHDFS\n\n    \"\"\"\n    import requests\n\n    if session is None:\n        session = requests.Session()\n    session.verify = verify\n    if auth_mechanism in ('GSSAPI', 'LDAP'):\n        if use_https == 'default':\n            prefix = 'https'\n        else:\n            prefix = 'https' if use_https else 'http'\n        try:\n            import requests_kerberos  # noqa: F401\n        except ImportError:\n            raise IbisError(\n                \"Unable to import requests-kerberos, which is required for \"\n                \"Kerberos HDFS support. Install it by executing `pip install \"\n                \"requests-kerberos` or `pip install hdfs[kerberos]`.\"\n            )\n        from hdfs.ext.kerberos import KerberosClient\n\n        # note SSL\n        url = '{0}://{1}:{2}'.format(prefix, host, port)\n        kwds.setdefault('mutual_auth', 'OPTIONAL')\n        hdfs_client = KerberosClient(url, session=session, **kwds)\n    else:\n        if use_https == 'default':\n            prefix = 'http'\n        else:\n            prefix = 'https' if use_https else 'http'\n        from hdfs.client import InsecureClient\n\n        url = '{}://{}:{}'.format(prefix, host, port)\n        hdfs_client = InsecureClient(url, session=session, **kwds)\n    return WebHDFS(hdfs_client)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nam this really that useful? Repeat an array like a Python list using modular arithmetic, scalar subqueries, and PostgreSQL's ARRAY function. This is inefficient if PostgreSQL allocates memory for the entire sequence and the output column. A quick glance at PostgreSQL's C code shows the sequence is evaluated stepwise, which suggests that it's roughly constant memory for the sequence generation.", "response": "def _array_repeat(t, expr):\n    \"\"\"Is this really that useful?\n\n    Repeat an array like a Python list using modular arithmetic,\n    scalar subqueries, and PostgreSQL's ARRAY function.\n\n    This is inefficient if PostgreSQL allocates memory for the entire sequence\n    and the output column. A quick glance at PostgreSQL's C code shows the\n    sequence is evaluated stepwise, which suggests that it's roughly constant\n    memory for the sequence generation.\n    \"\"\"\n    raw, times = map(t.translate, expr.op().args)\n\n    # SQLAlchemy uses our column's table in the FROM clause. We need a simpler\n    # expression to workaround this.\n    array = sa.column(raw.name, type_=raw.type)\n\n    # We still need to prefix the table name to the column name in the final\n    # query, so make sure the column knows its origin\n    array.table = raw.table\n\n    array_length = _cardinality(array)\n\n    # sequence from 1 to the total number of elements desired in steps of 1.\n    # the call to greatest isn't necessary, but it provides clearer intent\n    # rather than depending on the implicit postgres generate_series behavior\n    start = step = 1\n    stop = sa.func.greatest(times, 0) * array_length\n    series = sa.func.generate_series(start, stop, step).alias()\n    series_column = sa.column(series.name, type_=sa.INTEGER)\n\n    # if our current index modulo the array's length\n    # is a multiple of the array's length, then the index is the array's length\n    index_expression = series_column % array_length\n    index = sa.func.coalesce(sa.func.nullif(index_expression, 0), array_length)\n\n    # tie it all together in a scalar subquery and collapse that into an ARRAY\n    selected = sa.select([array[index]]).select_from(series)\n    return sa.func.array(selected.as_scalar())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _replace_interval_with_scalar(expr):\n    try:\n        expr_op = expr.op()\n    except AttributeError:\n        expr_op = None\n\n    if not isinstance(expr, (dt.Interval, ir.IntervalValue)):\n        # Literal expressions have op method but native types do not.\n        if isinstance(expr_op, ops.Literal):\n            return expr_op.value\n        else:\n            return expr\n    elif isinstance(expr, dt.Interval):\n        try:\n            microseconds = _map_interval_to_microseconds[expr.unit]\n            return microseconds\n        except KeyError:\n            raise ValueError(\n                \"Expected preceding values of week(), \"\n                + \"day(), hour(), minute(), second(), millisecond(), \"\n                + \"microseconds(), nanoseconds(); got {}\".format(expr)\n            )\n    elif expr_op.args and isinstance(expr, ir.IntervalValue):\n        if len(expr_op.args) > 2:\n            raise com.NotImplementedError(\n                \"'preceding' argument cannot be parsed.\"\n            )\n        left_arg = _replace_interval_with_scalar(expr_op.args[0])\n        right_arg = _replace_interval_with_scalar(expr_op.args[1])\n        method = _map_interval_op_to_op[type(expr_op)]\n        return method(left_arg, right_arg)", "response": "Replaces the Interval and IntervalValue components of an expression with a comparable scalar expression."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef roots(expr, types=(ops.PhysicalTable,)):\n    stack = [\n        arg.to_expr()\n        for arg in reversed(expr.op().root_tables())\n        if isinstance(arg, types)\n    ]\n\n    def extender(op):\n        return reversed(\n            list(\n                itertools.chain.from_iterable(\n                    arg.op().root_tables()\n                    for arg in op.flat_args()\n                    if isinstance(arg, types)\n                )\n            )\n        )\n\n    return _search_for_nodes(stack, extender, types)", "response": "Yields every node of a particular type on which an expression depends on."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget relevant arguments for a given operation.", "response": "def _get_args(op, name):\n    \"\"\"Hack to get relevant arguments for lineage computation.\n\n    We need a better way to determine the relevant arguments of an expression.\n    \"\"\"\n    # Could use multipledispatch here to avoid the pasta\n    if isinstance(op, ops.Selection):\n        assert name is not None, 'name is None'\n        result = op.selections\n\n        # if Selection.selections is always columnar, could use an\n        # OrderedDict to prevent scanning the whole thing\n        return [col for col in result if col._name == name]\n    elif isinstance(op, ops.Aggregation):\n        assert name is not None, 'name is None'\n        return [\n            col\n            for col in itertools.chain(op.by, op.metrics)\n            if col._name == name\n        ]\n    else:\n        return op.args"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef lineage(expr, container=Stack):\n    if not isinstance(expr, ir.ColumnExpr):\n        raise TypeError('Input expression must be an instance of ColumnExpr')\n\n    c = container([(expr, expr._name)])\n\n    seen = set()\n\n    # while we haven't visited everything\n    while c:\n        node, name = c.get()\n\n        if node not in seen:\n            seen.add(node)\n            yield node\n\n        # add our dependencies to the container if they match our name\n        # and are ibis expressions\n        c.extend(\n            (arg, getattr(arg, '_name', name))\n            for arg in c.visitor(_get_args(node.op(), name))\n            if isinstance(arg, ir.Expr)\n        )", "response": "Yields the path of the expression tree that comprises a column\n    expression."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef adjoin(space: int, *lists: Sequence[str]) -> str:\n    lengths = [max(map(len, x)) + space for x in lists[:-1]]\n\n    # not the last one\n    lengths.append(max(map(len, lists[-1])))\n    max_len = max(map(len, lists))\n    chains = (\n        itertools.chain(\n            (x.ljust(length) for x in lst),\n            itertools.repeat(' ' * length, max_len - len(lst)),\n        )\n        for lst, length in zip(lists, lengths)\n    )\n    return '\\n'.join(map(''.join, zip(*chains)))", "response": "Glue together two sets of strings using space."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns True if the difference between a and b is less than eps.", "response": "def approx_equal(a: Real, b: Real, eps: Real):\n    \"\"\"Return whether the difference between `a` and `b` is less than `eps`.\n\n    Parameters\n    ----------\n    a\n    b\n    eps\n\n    Returns\n    -------\n    bool\n\n    \"\"\"\n    assert abs(a - b) < eps"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef safe_index(elements: Sequence[T], value: T):\n    try:\n        return elements.index(value)\n    except ValueError:\n        return -1", "response": "Find the location of value in elements return - 1 if value is not found instead of raising ValueError."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_iterable(o: Any) -> bool:\n    return not isinstance(o, (str, bytes)) and isinstance(\n        o, collections.abc.Iterable\n    )", "response": "Return whether o is iterable and not a string or bytes."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts value to units of to.", "response": "def convert_unit(value, unit, to):\n    \"\"\"Convert `value`, is assumed to be in units of `unit`, to units of `to`.\n\n    Parameters\n    ----------\n    value : Union[numbers.Real, ibis.expr.types.NumericValue]\n\n    Returns\n    -------\n    Union[numbers.Integral, ibis.expr.types.NumericValue]\n\n    Examples\n    --------\n    >>> one_second = 1000\n    >>> x = convert_unit(one_second, 'ms', 's')\n    >>> x\n    1\n    >>> one_second = 1\n    >>> x = convert_unit(one_second, 's', 'ms')\n    >>> x\n    1000\n    >>> x = convert_unit(one_second, 's', 's')\n    >>> x\n    1\n    >>> x = convert_unit(one_second, 's', 'M')\n    Traceback (most recent call last):\n        ...\n    ValueError: Cannot convert to or from variable length interval\n\n    \"\"\"\n    # Don't do anything if from and to units are equivalent\n    if unit == to:\n        return value\n\n    units = ('W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns')\n    factors = (7, 24, 60, 60, 1000, 1000, 1000)\n\n    monthly_units = ('Y', 'Q', 'M')\n    monthly_factors = (4, 3)\n\n    try:\n        i, j = units.index(unit), units.index(to)\n    except ValueError:\n        try:\n            i, j = monthly_units.index(unit), monthly_units.index(to)\n            factors = monthly_factors\n        except ValueError:\n            raise ValueError(\n                'Cannot convert to or from variable length interval'\n            )\n\n    factor = functools.reduce(operator.mul, factors[min(i, j) : max(i, j)], 1)\n    assert factor > 1\n\n    if i < j:\n        return value * factor\n\n    assert i > j\n    return value // factor"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef consume(iterator: Iterator[T], n: Optional[int] = None) -> None:\n    # Use functions that consume iterators at C speed.\n    if n is None:\n        # feed the entire iterator into a zero-length deque\n        collections.deque(iterator, maxlen=0)\n    else:\n        # advance to the empty slice starting at position n\n        next(itertools.islice(iterator, n, n), None)", "response": "Advance the iterator n - steps ahead."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef arguments_from_signature(signature, *args, **kwargs):\n    bound = signature.bind_partial(*args)\n    meta_kwargs = toolz.merge({'kwargs': kwargs}, kwargs)\n    remaining_parameters = signature.parameters.keys() - bound.arguments.keys()\n    new_kwargs = {\n        k: meta_kwargs[k]\n        for k in remaining_parameters\n        if k in signature.parameters\n        if signature.parameters[k].kind\n        in {\n            Parameter.KEYWORD_ONLY,\n            Parameter.POSITIONAL_OR_KEYWORD,\n            Parameter.VAR_KEYWORD,\n        }\n    }\n    return args, new_kwargs", "response": "Validate signature against args and kwargs and return the kwargs asked for in the signature."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing the appropriate signature for a UDF or UDAF.", "response": "def udf_signature(input_type, pin, klass):\n    \"\"\"Compute the appropriate signature for a\n    :class:`~ibis.expr.operations.Node` from a list of input types\n    `input_type`.\n\n    Parameters\n    ----------\n    input_type : List[ibis.expr.datatypes.DataType]\n        A list of :class:`~ibis.expr.datatypes.DataType` instances representing\n        the signature of a UDF/UDAF.\n    pin : Optional[int]\n        If this is not None, pin the `pin`-th argument type to `klass`\n    klass : Union[Type[pd.Series], Type[SeriesGroupBy]]\n        The pandas object that every argument type should contain\n\n    Returns\n    -------\n    Tuple[Type]\n        A tuple of types appropriate for use in a multiple dispatch signature.\n\n    Examples\n    --------\n    >>> from pprint import pprint\n    >>> import pandas as pd\n    >>> from pandas.core.groupby import SeriesGroupBy\n    >>> import ibis.expr.datatypes as dt\n    >>> input_type = [dt.string, dt.double]\n    >>> sig = udf_signature(input_type, pin=None, klass=pd.Series)\n    >>> pprint(sig)  # doctest: +ELLIPSIS\n    ((<class '...Series'>, <... '...str...'>, <... 'NoneType'>),\n     (<class '...Series'>,\n      <... 'float'>,\n      <... 'numpy.floating'>,\n      <... 'NoneType'>))\n    >>> not_nullable_types = [\n    ...     dt.String(nullable=False), dt.Double(nullable=False)]\n    >>> sig = udf_signature(not_nullable_types, pin=None, klass=pd.Series)\n    >>> pprint(sig)  # doctest: +ELLIPSIS\n    ((<class '...Series'>, <... '...str...'>),\n     (<class '...Series'>,\n      <... 'float'>,\n      <... 'numpy.floating'>))\n    >>> sig0 = udf_signature(input_type, pin=0, klass=SeriesGroupBy)\n    >>> sig1 = udf_signature(input_type, pin=1, klass=SeriesGroupBy)\n    >>> pprint(sig0)  # doctest: +ELLIPSIS\n    (<class '...SeriesGroupBy'>,\n     (<class '...SeriesGroupBy'>,\n      <... 'float'>,\n      <... 'numpy.floating'>,\n      <... 'NoneType'>))\n    >>> pprint(sig1)  # doctest: +ELLIPSIS\n    ((<class '...SeriesGroupBy'>,\n      <... '...str...'>,\n      <... 'NoneType'>),\n     <class '...SeriesGroupBy'>)\n    \"\"\"\n    nargs = len(input_type)\n\n    if not nargs:\n        return ()\n\n    if nargs == 1:\n        r, = input_type\n        result = (klass,) + rule_to_python_type(r) + nullable(r)\n        return (result,)\n\n    return tuple(\n        klass\n        if pin is not None and pin == i\n        else ((klass,) + rule_to_python_type(r) + nullable(r))\n        for i, r in enumerate(input_type)\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parameter_count(funcsig):\n    return sum(\n        param.kind in {param.POSITIONAL_OR_KEYWORD, param.POSITIONAL_ONLY}\n        for param in funcsig.parameters.values()\n        if param.default is Parameter.empty\n    )", "response": "Get the number of positional - or - keyword or position - only parameters in a\n    function signature."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking that the signature of a function is correct.", "response": "def valid_function_signature(input_type, func):\n    \"\"\"Check that the declared number of inputs (the length of `input_type`)\n    and the number of inputs to `func` are equal.\n\n    Parameters\n    ----------\n    input_type : List[DataType]\n    func : callable\n\n    Returns\n    -------\n    inspect.Signature\n    \"\"\"\n    funcsig = signature(func)\n    declared_parameter_count = len(input_type)\n    function_parameter_count = parameter_count(funcsig)\n\n    if declared_parameter_count != function_parameter_count:\n        raise TypeError(\n            'Function signature {!r} has {:d} parameters, '\n            'input_type has {:d}. These must match'.format(\n                func.__name__,\n                function_parameter_count,\n                declared_parameter_count,\n            )\n        )\n    return funcsig"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndefine a UDF function that operates element wise on a Pandas Series.", "response": "def elementwise(input_type, output_type):\n        \"\"\"Define a UDF (user-defined function) that operates element wise on a\n        Pandas Series.\n\n        Parameters\n        ----------\n        input_type : List[ibis.expr.datatypes.DataType]\n            A list of the types found in :mod:`~ibis.expr.datatypes`. The\n            length of this list must match the number of arguments to the\n            function. Variadic arguments are not yet supported.\n        output_type : ibis.expr.datatypes.DataType\n            The return type of the function.\n\n        Examples\n        --------\n        >>> import ibis\n        >>> import ibis.expr.datatypes as dt\n        >>> from ibis.pandas.udf import udf\n        >>> @udf.elementwise(input_type=[dt.string], output_type=dt.int64)\n        ... def my_string_length(series):\n        ...     return series.str.len() * 2\n        \"\"\"\n\n        def wrapper(func):\n            # validate that the input_type argument and the function signature\n            # match\n            funcsig = valid_function_signature(input_type, func)\n\n            # generate a new custom node\n            UDFNode = type(\n                func.__name__,\n                (ops.ValueOp,),\n                {\n                    'signature': sig.TypeSignature.from_dtypes(input_type),\n                    'output_type': output_type.column_type,\n                },\n            )\n\n            # definitions\n            # Define an execution rule for a simple elementwise Series\n            # function\n            @execute_node.register(\n                UDFNode, *udf_signature(input_type, pin=None, klass=pd.Series)\n            )\n            @execute_node.register(\n                UDFNode,\n                *(\n                    rule_to_python_type(argtype) + nullable(argtype)\n                    for argtype in input_type\n                ),\n            )\n            def execute_udf_node(op, *args, **kwargs):\n                args, kwargs = arguments_from_signature(\n                    funcsig, *args, **kwargs\n                )\n                return func(*args, **kwargs)\n\n            # Define an execution rule for elementwise operations on a\n            # grouped Series\n            nargs = len(input_type)\n            group_by_signatures = [\n                udf_signature(input_type, pin=pin, klass=SeriesGroupBy)\n                for pin in range(nargs)\n            ]\n\n            @toolz.compose(\n                *(\n                    execute_node.register(UDFNode, *types)\n                    for types in group_by_signatures\n                )\n            )\n            def execute_udf_node_groupby(op, *args, **kwargs):\n                groupers = [\n                    grouper\n                    for grouper in (\n                        getattr(arg, 'grouper', None) for arg in args\n                    )\n                    if grouper is not None\n                ]\n\n                # all grouping keys must be identical\n                assert all(groupers[0] == grouper for grouper in groupers[1:])\n\n                # we're performing a scalar operation on grouped column, so\n                # perform the operation directly on the underlying Series\n                # and regroup after it's finished\n                arguments = [getattr(arg, 'obj', arg) for arg in args]\n                groupings = groupers[0].groupings\n                args, kwargs = arguments_from_signature(\n                    signature(func), *arguments, **kwargs\n                )\n                return func(*args, **kwargs).groupby(groupings)\n\n            @functools.wraps(func)\n            def wrapped(*args):\n                return UDFNode(*args).to_expr()\n\n            return wrapped\n\n        return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef reduction(input_type, output_type):\n        return udf._grouped(\n            input_type,\n            output_type,\n            base_class=ops.Reduction,\n            output_type_method=operator.attrgetter('scalar_type'),\n        )", "response": "Define a user - defined reduction function that takes N pandas Series\n            or scalar values as inputs and produces one row of output."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndefines an analytic user - defined function that takes Niso - formatted Series or scalar values as inputs and produces N rows of output.", "response": "def analytic(input_type, output_type):\n        \"\"\"Define an *analytic* user-defined function that takes N\n        pandas Series or scalar values as inputs and produces N rows of output.\n\n        Parameters\n        ----------\n        input_type : List[ibis.expr.datatypes.DataType]\n            A list of the types found in :mod:`~ibis.expr.datatypes`. The\n            length of this list must match the number of arguments to the\n            function. Variadic arguments are not yet supported.\n        output_type : ibis.expr.datatypes.DataType\n            The return type of the function.\n\n        Examples\n        --------\n        >>> import ibis\n        >>> import ibis.expr.datatypes as dt\n        >>> from ibis.pandas.udf import udf\n        >>> @udf.analytic(input_type=[dt.double], output_type=dt.double)\n        ... def zscore(series):  # note the use of aggregate functions\n        ...     return (series - series.mean()) / series.std()\n        \"\"\"\n        return udf._grouped(\n            input_type,\n            output_type,\n            base_class=ops.AnalyticOp,\n            output_type_method=operator.attrgetter('column_type'),\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndefines a user - defined function that is applied per group.", "response": "def _grouped(input_type, output_type, base_class, output_type_method):\n        \"\"\"Define a user-defined function that is applied per group.\n\n        Parameters\n        ----------\n        input_type : List[ibis.expr.datatypes.DataType]\n            A list of the types found in :mod:`~ibis.expr.datatypes`. The\n            length of this list must match the number of arguments to the\n            function. Variadic arguments are not yet supported.\n        output_type : ibis.expr.datatypes.DataType\n            The return type of the function.\n        base_class : Type[T]\n            The base class of the generated Node\n        output_type_method : Callable\n            A callable that determines the method to call to get the expression\n            type of the UDF\n\n        See Also\n        --------\n        ibis.pandas.udf.reduction\n        ibis.pandas.udf.analytic\n        \"\"\"\n\n        def wrapper(func):\n            funcsig = valid_function_signature(input_type, func)\n\n            UDAFNode = type(\n                func.__name__,\n                (base_class,),\n                {\n                    'signature': sig.TypeSignature.from_dtypes(input_type),\n                    'output_type': output_type_method(output_type),\n                },\n            )\n\n            # An execution rule for a simple aggregate node\n            @execute_node.register(\n                UDAFNode, *udf_signature(input_type, pin=None, klass=pd.Series)\n            )\n            def execute_udaf_node(op, *args, **kwargs):\n                args, kwargs = arguments_from_signature(\n                    funcsig, *args, **kwargs\n                )\n                return func(*args, **kwargs)\n\n            # An execution rule for a grouped aggregation node. This\n            # includes aggregates applied over a window.\n            nargs = len(input_type)\n            group_by_signatures = [\n                udf_signature(input_type, pin=pin, klass=SeriesGroupBy)\n                for pin in range(nargs)\n            ]\n\n            @toolz.compose(\n                *(\n                    execute_node.register(UDAFNode, *types)\n                    for types in group_by_signatures\n                )\n            )\n            def execute_udaf_node_groupby(op, *args, **kwargs):\n                # construct a generator that yields the next group of data\n                # for every argument excluding the first (pandas performs\n                # the iteration for the first argument) for each argument\n                # that is a SeriesGroupBy.\n                #\n                # If the argument is not a SeriesGroupBy then keep\n                # repeating it until all groups are exhausted.\n                aggcontext = kwargs.pop('aggcontext', None)\n                assert aggcontext is not None, 'aggcontext is None'\n                iters = (\n                    (data for _, data in arg)\n                    if isinstance(arg, SeriesGroupBy)\n                    else itertools.repeat(arg)\n                    for arg in args[1:]\n                )\n                funcsig = signature(func)\n\n                def aggregator(first, *rest, **kwargs):\n                    # map(next, *rest) gets the inputs for the next group\n                    # TODO: might be inefficient to do this on every call\n                    args, kwargs = arguments_from_signature(\n                        funcsig, first, *map(next, rest), **kwargs\n                    )\n                    return func(*args, **kwargs)\n\n                result = aggcontext.agg(args[0], aggregator, *iters, **kwargs)\n                return result\n\n            @functools.wraps(func)\n            def wrapped(*args):\n                return UDAFNode(*args).to_expr()\n\n            return wrapped\n\n        return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretrieves an ibis schema from a SQLAlchemy table.", "response": "def schema_from_table(table, schema=None):\n    \"\"\"Retrieve an ibis schema from a SQLAlchemy ``Table``.\n\n    Parameters\n    ----------\n    table : sa.Table\n\n    Returns\n    -------\n    schema : ibis.expr.datatypes.Schema\n        An ibis schema corresponding to the types of the columns in `table`.\n    \"\"\"\n    schema = schema if schema is not None else {}\n    pairs = []\n    for name, column in table.columns.items():\n        if name in schema:\n            dtype = dt.dtype(schema[name])\n        else:\n            dtype = dt.dtype(\n                getattr(table.bind, 'dialect', SQLAlchemyDialect()),\n                column.type,\n                nullable=column.nullable,\n            )\n        pairs.append((name, dtype))\n    return sch.schema(pairs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef invalidates_reflection_cache(f):\n\n    @functools.wraps(f)\n    def wrapped(self, *args, **kwargs):\n        result = f(self, *args, **kwargs)\n\n        # only invalidate the cache after we've succesfully called the wrapped\n        # function\n        self._reflection_cache_is_dirty = True\n        return result\n\n    return wrapped", "response": "Decorator that invalidates the SQLAlchemy reflection cache if f performs an operation that mutates database or table metadata such as CREATE TABLE or DROP TABLE etc."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a table expression referencing a table in this schema.", "response": "def table(self, name):\n        \"\"\"\n        Return a table expression referencing a table in this schema\n\n        Returns\n        -------\n        table : TableExpr\n        \"\"\"\n        qualified_name = self._qualify(name)\n        return self.database.table(qualified_name, self.name)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef list_tables(self, like=None, database=None, schema=None):\n        inspector = self.inspector\n        names = inspector.get_table_names(schema=schema)\n        names.extend(inspector.get_view_names(schema=schema))\n        if like is not None:\n            names = [x for x in names if like in x]\n        return sorted(names)", "response": "List tables in the current database or indicated schema."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef convert_to_database_compatible_value(value):\n    if pd.isnull(value):\n        return None\n    if isinstance(value, pd.Timestamp):\n        return value.to_pydatetime()\n    try:\n        return value.item()\n    except AttributeError:\n        return value", "response": "Convert a value to a database compatible value."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks whether value matches pattern.", "response": "def matches(value, pattern):\n    \"\"\"Check whether `value` matches `pattern`.\n\n    Parameters\n    ----------\n    value : ast.AST\n    pattern : ast.AST\n\n    Returns\n    -------\n    matched : bool\n    \"\"\"\n    # types must match exactly\n    if type(value) != type(pattern):\n        return False\n\n    # primitive value, such as None, True, False etc\n    if not isinstance(value, ast.AST) and not isinstance(pattern, ast.AST):\n        return value == pattern\n\n    fields = [\n        (field, getattr(pattern, field))\n        for field in pattern._fields\n        if hasattr(pattern, field)\n    ]\n    for field_name, field_value in fields:\n        if not matches(getattr(value, field_name), field_value):\n            return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef highest_precedence_dtype(exprs):\n    if not exprs:\n        raise ValueError('Must pass at least one expression')\n\n    return dt.highest_precedence(expr.type() for expr in exprs)", "response": "Returns the highest precedence type from the passed expressions"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns whether source ir type is implicitly castable to target ir type", "response": "def castable(source, target):\n    \"\"\"Return whether source ir type is implicitly castable to target\n\n    Based on the underlying datatypes and the value in case of Literals\n    \"\"\"\n    op = source.op()\n    value = getattr(op, 'value', None)\n    return dt.castable(source.type(), target.type(), value=value)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncast source to target type.", "response": "def cast(source, target):\n    \"\"\"Currently Literal to *Scalar implicit casts are allowed\"\"\"\n    import ibis.expr.operations as ops  # TODO: don't use ops here\n\n    if not castable(source, target):\n        raise com.IbisTypeError('Source is not castable to target type!')\n\n    # currently it prevents column -> scalar implicit castings\n    # however the datatypes are matching\n    op = source.op()\n    if not isinstance(op, ops.Literal):\n        raise com.IbisTypeError('Only able to implicitly cast literals!')\n\n    out_type = target.type().scalar_type()\n    return out_type(op)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef one_of(inners, arg):\n    for inner in inners:\n        with suppress(com.IbisTypeError, ValueError):\n            return inner(arg)\n\n    rules_formatted = ', '.join(map(repr, inners))\n    raise com.IbisTypeError(\n        'Arg passes neither of the following rules: {}'.format(rules_formatted)\n    )", "response": "At least one of the validators must pass arg"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef instance_of(klass, arg):\n    if not isinstance(arg, klass):\n        raise com.IbisTypeError(\n            'Given argument with type {} is not an instance of {}'.format(\n                type(arg), klass\n            )\n        )\n    return arg", "response": "Require that a value is an instance of a particular Python type."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nvalidating that the given argument is a Value with a particular datatype", "response": "def value(dtype, arg):\n    \"\"\"Validates that the given argument is a Value with a particular datatype\n\n    Parameters\n    ----------\n    dtype : DataType subclass or DataType instance\n    arg : python literal or an ibis expression\n      If a python literal is given the validator tries to coerce it to an ibis\n      literal.\n\n    Returns\n    -------\n    arg : AnyValue\n      An ibis value expression with the specified datatype\n    \"\"\"\n    if not isinstance(arg, ir.Expr):\n        # coerce python literal to ibis literal\n        arg = ir.literal(arg)\n\n    if not isinstance(arg, ir.AnyValue):\n        raise com.IbisTypeError(\n            'Given argument with type {} is not a value '\n            'expression'.format(type(arg))\n        )\n\n    # retrieve literal values for implicit cast check\n    value = getattr(arg.op(), 'value', None)\n\n    if isinstance(dtype, type) and isinstance(arg.type(), dtype):\n        # dtype class has been specified like dt.Interval or dt.Decimal\n        return arg\n    elif dt.castable(arg.type(), dt.dtype(dtype), value=value):\n        # dtype instance or string has been specified and arg's dtype is\n        # implicitly castable to it, like dt.int8 is castable to dt.int64\n        return arg\n    else:\n        raise com.IbisTypeError(\n            'Given argument with datatype {} is not '\n            'subtype of {} nor implicitly castable to '\n            'it'.format(arg.type(), dtype)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwrapping a function object that creates a UDA object.", "response": "def wrap_uda(\n    hdfs_file,\n    inputs,\n    output,\n    update_fn,\n    init_fn=None,\n    merge_fn=None,\n    finalize_fn=None,\n    serialize_fn=None,\n    close_fn=None,\n    name=None,\n):\n    \"\"\"\n    Creates a callable aggregation function object. Must be created in Impala\n    to be used\n\n    Parameters\n    ----------\n    hdfs_file: .so file that contains relevant UDA\n    inputs: list of strings denoting ibis datatypes\n    output: string denoting ibis datatype\n    update_fn: string\n      Library symbol name for update function\n    init_fn: string, optional\n      Library symbol name for initialization function\n    merge_fn: string, optional\n      Library symbol name for merge function\n    finalize_fn: string, optional\n      Library symbol name for finalize function\n    serialize_fn : string, optional\n      Library symbol name for serialize UDA API function. Not required for all\n      UDAs; see documentation for more.\n    close_fn : string, optional\n    name: string, optional\n      Used internally to track function\n\n    Returns\n    -------\n    container : UDA object\n    \"\"\"\n    func = ImpalaUDA(\n        inputs,\n        output,\n        update_fn,\n        init_fn,\n        merge_fn,\n        finalize_fn,\n        serialize_fn=serialize_fn,\n        name=name,\n        lib_path=hdfs_file,\n    )\n    return func"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwrapping a UDF function object in Impala to be", "response": "def wrap_udf(hdfs_file, inputs, output, so_symbol, name=None):\n    \"\"\"\n    Creates a callable scalar function object. Must be created in Impala to be\n    used\n\n    Parameters\n    ----------\n    hdfs_file: .so file that contains relevant UDF\n    inputs: list of strings or sig.TypeSignature\n      Input types to UDF\n    output: string\n      Ibis data type\n    so_symbol: string, C++ function name for relevant UDF\n    name: string (optional). Used internally to track function\n\n    Returns\n    -------\n    container : UDF object\n    \"\"\"\n    func = ImpalaUDF(inputs, output, so_symbol, name=name, lib_path=hdfs_file)\n    return func"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_operation(op, func_name, db):\n    full_name = '{0}.{1}'.format(db, func_name)\n    # TODO\n    # if op.input_type is rlz.listof:\n    #     translator = comp.varargs(full_name)\n    # else:\n    arity = len(op.signature)\n    translator = comp.fixed_arity(full_name, arity)\n\n    comp._operation_registry[op] = translator", "response": "Registers the given operation within the Ibis SQL translation toolchain."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates an ImpalaClient for use with Ibis.", "response": "def connect(\n    host='localhost',\n    port=21050,\n    database='default',\n    timeout=45,\n    use_ssl=False,\n    ca_cert=None,\n    user=None,\n    password=None,\n    auth_mechanism='NOSASL',\n    kerberos_service_name='impala',\n    pool_size=8,\n    hdfs_client=None,\n):\n    \"\"\"Create an ImpalaClient for use with Ibis.\n\n    Parameters\n    ----------\n    host : str, optional\n        Host name of the impalad or HiveServer2 in Hive\n    port : int, optional\n        Impala's HiveServer2 port\n    database : str, optional\n        Default database when obtaining new cursors\n    timeout : int, optional\n        Connection timeout in seconds when communicating with HiveServer2\n    use_ssl : bool, optional\n        Use SSL when connecting to HiveServer2\n    ca_cert : str, optional\n        Local path to 3rd party CA certificate or copy of server certificate\n        for self-signed certificates. If SSL is enabled, but this argument is\n        ``None``, then certificate validation is skipped.\n    user : str, optional\n        LDAP user to authenticate\n    password : str, optional\n        LDAP password to authenticate\n    auth_mechanism : str, optional\n        {'NOSASL' <- default, 'PLAIN', 'GSSAPI', 'LDAP'}.\n        Use NOSASL for non-secured Impala connections.  Use PLAIN for\n        non-secured Hive clusters.  Use LDAP for LDAP authenticated\n        connections.  Use GSSAPI for Kerberos-secured clusters.\n    kerberos_service_name : str, optional\n        Specify particular impalad service principal.\n\n    Examples\n    --------\n    >>> import ibis\n    >>> import os\n    >>> hdfs_host = os.environ.get('IBIS_TEST_NN_HOST', 'localhost')\n    >>> hdfs_port = int(os.environ.get('IBIS_TEST_NN_PORT', 50070))\n    >>> impala_host = os.environ.get('IBIS_TEST_IMPALA_HOST', 'localhost')\n    >>> impala_port = int(os.environ.get('IBIS_TEST_IMPALA_PORT', 21050))\n    >>> hdfs = ibis.hdfs_connect(host=hdfs_host, port=hdfs_port)\n    >>> hdfs  # doctest: +ELLIPSIS\n    <ibis.filesystems.WebHDFS object at 0x...>\n    >>> client = ibis.impala.connect(\n    ...     host=impala_host,\n    ...     port=impala_port,\n    ...     hdfs_client=hdfs,\n    ... )\n    >>> client  # doctest: +ELLIPSIS\n    <ibis.impala.client.ImpalaClient object at 0x...>\n\n    Returns\n    -------\n    ImpalaClient\n    \"\"\"\n    params = {\n        'host': host,\n        'port': port,\n        'database': database,\n        'timeout': timeout,\n        'use_ssl': use_ssl,\n        'ca_cert': ca_cert,\n        'user': user,\n        'password': password,\n        'auth_mechanism': auth_mechanism,\n        'kerberos_service_name': kerberos_service_name,\n    }\n\n    con = ImpalaConnection(pool_size=pool_size, **params)\n    try:\n        client = ImpalaClient(con, hdfs_client=hdfs_client)\n    except Exception:\n        con.close()\n        raise\n    else:\n        if options.default_backend is None:\n            options.default_backend = client\n\n    return client"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nexecuting an expression expr with data provided in scope.", "response": "def execute_with_scope(expr, scope, aggcontext=None, clients=None, **kwargs):\n    \"\"\"Execute an expression `expr`, with data provided in `scope`.\n\n    Parameters\n    ----------\n    expr : ibis.expr.types.Expr\n        The expression to execute.\n    scope : collections.Mapping\n        A dictionary mapping :class:`~ibis.expr.operations.Node` subclass\n        instances to concrete data such as a pandas DataFrame.\n    aggcontext : Optional[ibis.pandas.aggcontext.AggregationContext]\n\n    Returns\n    -------\n    result : scalar, pd.Series, pd.DataFrame\n    \"\"\"\n    op = expr.op()\n\n    # Call pre_execute, to allow clients to intercept the expression before\n    # computing anything *and* before associating leaf nodes with data. This\n    # allows clients to provide their own data for each leaf.\n    if clients is None:\n        clients = list(find_backends(expr))\n\n    if aggcontext is None:\n        aggcontext = agg_ctx.Summarize()\n\n    pre_executed_scope = pre_execute(\n        op, *clients, scope=scope, aggcontext=aggcontext, **kwargs\n    )\n    new_scope = toolz.merge(scope, pre_executed_scope)\n    result = execute_until_in_scope(\n        expr,\n        new_scope,\n        aggcontext=aggcontext,\n        clients=clients,\n        # XXX: we *explicitly* pass in scope and not new_scope here so that\n        # post_execute sees the scope of execute_with_scope, not the scope of\n        # execute_until_in_scope\n        post_execute_=functools.partial(\n            post_execute,\n            scope=scope,\n            aggcontext=aggcontext,\n            clients=clients,\n            **kwargs,\n        ),\n        **kwargs,\n    )\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nexecuting until our op is in scope.", "response": "def execute_until_in_scope(\n    expr, scope, aggcontext=None, clients=None, post_execute_=None, **kwargs\n):\n    \"\"\"Execute until our op is in `scope`.\n\n    Parameters\n    ----------\n    expr : ibis.expr.types.Expr\n    scope : Mapping\n    aggcontext : Optional[AggregationContext]\n    clients : List[ibis.client.Client]\n    kwargs : Mapping\n    \"\"\"\n    # these should never be None\n    assert aggcontext is not None, 'aggcontext is None'\n    assert clients is not None, 'clients is None'\n    assert post_execute_ is not None, 'post_execute_ is None'\n\n    # base case: our op has been computed (or is a leaf data node), so\n    # return the corresponding value\n    op = expr.op()\n    if op in scope:\n        return scope[op]\n\n    new_scope = execute_bottom_up(\n        expr,\n        scope,\n        aggcontext=aggcontext,\n        post_execute_=post_execute_,\n        clients=clients,\n        **kwargs,\n    )\n    new_scope = toolz.merge(\n        new_scope, pre_execute(op, *clients, scope=scope, **kwargs)\n    )\n    return execute_until_in_scope(\n        expr,\n        new_scope,\n        aggcontext=aggcontext,\n        clients=clients,\n        post_execute_=post_execute_,\n        **kwargs,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef execute_bottom_up(\n    expr, scope, aggcontext=None, post_execute_=None, clients=None, **kwargs\n):\n    \"\"\"Execute `expr` bottom-up.\n\n    Parameters\n    ----------\n    expr : ibis.expr.types.Expr\n    scope : Mapping[ibis.expr.operations.Node, object]\n    aggcontext : Optional[ibis.pandas.aggcontext.AggregationContext]\n    kwargs : Dict[str, object]\n\n    Returns\n    -------\n    result : Mapping[\n        ibis.expr.operations.Node,\n        Union[pandas.Series, pandas.DataFrame, scalar_types]\n    ]\n        A mapping from node to the computed result of that Node\n    \"\"\"\n    assert post_execute_ is not None, 'post_execute_ is None'\n    op = expr.op()\n\n    # if we're in scope then return the scope, this will then be passed back\n    # into execute_bottom_up, which will then terminate\n    if op in scope:\n        return scope\n    elif isinstance(op, ops.Literal):\n        # special case literals to avoid the overhead of dispatching\n        # execute_node\n        return {\n            op: execute_literal(\n                op, op.value, expr.type(), aggcontext=aggcontext, **kwargs\n            )\n        }\n\n    # figure out what arguments we're able to compute on based on the\n    # expressions inputs. things like expressions, None, and scalar types are\n    # computable whereas ``list``s are not\n    computable_args = [arg for arg in op.inputs if is_computable_input(arg)]\n\n    # recursively compute each node's arguments until we've changed type\n    scopes = [\n        execute_bottom_up(\n            arg,\n            scope,\n            aggcontext=aggcontext,\n            post_execute_=post_execute_,\n            clients=clients,\n            **kwargs,\n        )\n        if hasattr(arg, 'op')\n        else {arg: arg}\n        for arg in computable_args\n    ]\n\n    # if we're unable to find data then raise an exception\n    if not scopes:\n        raise com.UnboundExpressionError(\n            'Unable to find data for expression:\\n{}'.format(repr(expr))\n        )\n\n    # there should be exactly one dictionary per computable argument\n    assert len(computable_args) == len(scopes)\n\n    new_scope = toolz.merge(scopes)\n\n    # pass our computed arguments to this node's execute_node implementation\n    data = [\n        new_scope[arg.op()] if hasattr(arg, 'op') else arg\n        for arg in computable_args\n    ]\n    result = execute_node(\n        op,\n        *data,\n        scope=scope,\n        aggcontext=aggcontext,\n        clients=clients,\n        **kwargs,\n    )\n    computed = post_execute_(op, result)\n    return {op: computed}", "response": "Execute expr and return a new node with the computed result."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nexecuting an expression against data that is bound to it.", "response": "def main_execute(expr, params=None, scope=None, aggcontext=None, **kwargs):\n    \"\"\"Execute an expression against data that are bound to it. If no data\n    are bound, raise an Exception.\n\n    Parameters\n    ----------\n    expr : ibis.expr.types.Expr\n        The expression to execute\n    params : Mapping[ibis.expr.types.Expr, object]\n        The data that an unbound parameter in `expr` maps to\n    scope : Mapping[ibis.expr.operations.Node, object]\n        Additional scope, mapping ibis operations to data\n    aggcontext : Optional[ibis.pandas.aggcontext.AggregationContext]\n        An object indicating how to compute aggregations. For example,\n        a rolling mean needs to be computed differently than the mean of a\n        column.\n    kwargs : Dict[str, object]\n        Additional arguments that can potentially be used by individual node\n        execution\n\n    Returns\n    -------\n    result : Union[\n        pandas.Series, pandas.DataFrame, ibis.pandas.core.simple_types\n    ]\n\n    Raises\n    ------\n    ValueError\n        * If no data are bound to the input expression\n    \"\"\"\n    if scope is None:\n        scope = {}\n\n    if params is None:\n        params = {}\n\n    # TODO: make expresions hashable so that we can get rid of these .op()\n    # calls everywhere\n    params = {k.op() if hasattr(k, 'op') else k: v for k, v in params.items()}\n\n    new_scope = toolz.merge(scope, params)\n    return execute_with_scope(expr, new_scope, aggcontext=aggcontext, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nexecuting an expression against data that is bound to it and reset the index of the result.", "response": "def execute_and_reset(\n    expr, params=None, scope=None, aggcontext=None, **kwargs\n):\n    \"\"\"Execute an expression against data that are bound to it. If no data\n    are bound, raise an Exception.\n\n    Notes\n    -----\n    The difference between this function and :func:`~ibis.pandas.core.execute`\n    is that this function resets the index of the result, if the result has\n    an index.\n\n    Parameters\n    ----------\n    expr : ibis.expr.types.Expr\n        The expression to execute\n    params : Mapping[ibis.expr.types.Expr, object]\n        The data that an unbound parameter in `expr` maps to\n    scope : Mapping[ibis.expr.operations.Node, object]\n        Additional scope, mapping ibis operations to data\n    aggcontext : Optional[ibis.pandas.aggcontext.AggregationContext]\n        An object indicating how to compute aggregations. For example,\n        a rolling mean needs to be computed differently than the mean of a\n        column.\n    kwargs : Dict[str, object]\n        Additional arguments that can potentially be used by individual node\n        execution\n\n    Returns\n    -------\n    result : Union[\n        pandas.Series, pandas.DataFrame, ibis.pandas.core.simple_types\n    ]\n\n    Raises\n    ------\n    ValueError\n        * If no data are bound to the input expression\n    \"\"\"\n    result = execute(\n        expr, params=params, scope=scope, aggcontext=aggcontext, **kwargs\n    )\n    if isinstance(result, pd.DataFrame):\n        schema = expr.schema()\n        df = result.reset_index()\n        return df.loc[:, schema.names]\n    elif isinstance(result, pd.Series):\n        return result.reset_index(drop=True)\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconnect to a Kudu client and return a new instance of this class.", "response": "def connect(\n        self,\n        host_or_hosts,\n        port_or_ports=7051,\n        rpc_timeout=None,\n        admin_timeout=None,\n    ):\n        \"\"\"\n        Pass-through connection interface to the Kudu client\n\n        Parameters\n        ----------\n        host_or_hosts : string or list of strings\n          If you have multiple Kudu masters for HA, pass a list\n        port_or_ports : int or list of int, default 7051\n          If you pass multiple host names, pass multiple ports\n        rpc_timeout : kudu.TimeDelta\n          See Kudu client documentation for details\n        admin_timeout : kudu.TimeDelta\n          See Kudu client documentation for details\n\n        Returns\n        -------\n        None\n        \"\"\"\n        self.client = kudu.connect(\n            host_or_hosts,\n            port_or_ports,\n            rpc_timeout_ms=rpc_timeout,\n            admin_timeout_ms=admin_timeout,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_table(\n        self,\n        impala_name,\n        kudu_name,\n        primary_keys=None,\n        obj=None,\n        schema=None,\n        database=None,\n        external=False,\n        force=False,\n    ):\n        \"\"\"\n        Create an Kudu-backed table in the connected Impala cluster. For\n        non-external tables, this will create a Kudu table with a compatible\n        storage schema.\n\n        This function is patterned after the ImpalaClient.create_table function\n        designed for physical filesystems (like HDFS).\n\n        Parameters\n        ----------\n        impala_name : string\n          Name of the created Impala table\n        kudu_name : string\n          Name of hte backing Kudu table. Will be created if external=False\n        primary_keys : list of column names\n          List of\n        obj : TableExpr or pandas.DataFrame, optional\n          If passed, creates table from select statement results\n        schema : ibis.Schema, optional\n          Mutually exclusive with expr, creates an empty table with a\n          particular schema\n        database : string, default None (optional)\n        external : boolean, default False\n          If False, a new Kudu table will be created. Otherwise, the Kudu table\n          must already exist.\n        \"\"\"\n        self._check_connected()\n\n        if not external and (primary_keys is None or len(primary_keys) == 0):\n            raise ValueError(\n                'Must specify primary keys when DDL creates a '\n                'new Kudu table'\n            )\n\n        if obj is not None:\n            if external:\n                raise ValueError(\n                    'Cannot create an external Kudu-Impala table '\n                    'from an expression or DataFrame'\n                )\n\n            if isinstance(obj, pd.DataFrame):\n                from ibis.impala.pandas_interop import write_temp_dataframe\n\n                writer, to_insert = write_temp_dataframe(\n                    self.impala_client, obj\n                )\n            else:\n                to_insert = obj\n            # XXX: exposing a lot of internals\n            ast = self.impala_client._build_ast(to_insert)\n            select = ast.queries[0]\n\n            stmt = CTASKudu(\n                impala_name,\n                kudu_name,\n                self.client.master_addrs,\n                select,\n                primary_keys,\n                database=database,\n            )\n        else:\n            if external:\n                ktable = self.client.table(kudu_name)\n                kschema = ktable.schema\n                schema = schema_kudu_to_ibis(kschema)\n                primary_keys = kschema.primary_keys()\n            elif schema is None:\n                raise ValueError(\n                    'Must specify schema for new empty ' 'Kudu-backed table'\n                )\n\n            stmt = CreateTableKudu(\n                impala_name,\n                kudu_name,\n                self.client.master_addrs,\n                schema,\n                primary_keys,\n                external=external,\n                database=database,\n                can_exist=False,\n            )\n\n        self.impala_client._execute(stmt)", "response": "Creates a new table in the connected Impala cluster."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef table(\n        self, kudu_name, name=None, database=None, persist=False, external=True\n    ):\n        \"\"\"\n        Convenience to expose an existing Kudu table (using CREATE TABLE) as an\n        Impala table. To create a new table both in the Hive Metastore with\n        storage in Kudu, use create_table.\n\n        Note: all tables created are EXTERNAL for now. Creates a temporary\n        table (like parquet_file and others) unless persist=True.\n\n        If you create a persistent table you can thereafter use it like any\n        other Impala table.\n\n        Parameters\n        ----------\n        kudu_name : string\n          The name of the table in the Kudu cluster\n        name : string, optional\n          Name of the created table in Impala / Hive Metastore. Randomly\n          generated if not specified.\n        database : string, optional\n          Database to create the table in. Uses the temp db if not provided\n        persist : boolean, default False\n          If True, do not drop the table upon Ibis garbage collection /\n          interpreter shutdown. Be careful using this in conjunction with the\n          `external` option.\n        external : boolean, default True\n          If True, create the Impala table as EXTERNAL so the Kudu data is not\n          deleted when the Impala table is dropped\n\n        Returns\n        -------\n        parquet_table : ImpalaTable\n        \"\"\"\n        # Law of demeter, but OK for now because internal class coupling\n        name, database = self.impala_client._get_concrete_table_path(\n            name, database, persist=persist\n        )\n        self.create_table(name, kudu_name, database=database, external=True)\n        return self.impala_client._wrap_new_table(name, database, persist)", "response": "Creates a new Impala table in the Impala Hive Metastore with the specified name and database."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef compile(expr, params=None):\n    from ibis.clickhouse.compiler import to_sql\n\n    return to_sql(expr, dialect.make_context(params=params))", "response": "Returns a SQL expression that can be used to compile the expression."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a ClickhouseClient for use with Ibis.", "response": "def connect(\n    host='localhost',\n    port=9000,\n    database='default',\n    user='default',\n    password='',\n    client_name='ibis',\n    compression=_default_compression,\n):\n    \"\"\"Create an ClickhouseClient for use with Ibis.\n\n    Parameters\n    ----------\n    host : str, optional\n        Host name of the clickhouse server\n    port : int, optional\n        Clickhouse server's  port\n    database : str, optional\n        Default database when executing queries\n    user : str, optional\n        User to authenticate with\n    password : str, optional\n        Password to authenticate with\n    client_name: str, optional\n        This will appear in clickhouse server logs\n    compression: str, optional\n        Weather or not to use compression.\n        Default is lz4 if installed else False.\n        Possible choices: lz4, lz4hc, quicklz, zstd, True, False\n        True is equivalent to 'lz4'.\n\n    Examples\n    --------\n    >>> import ibis\n    >>> import os\n    >>> clickhouse_host = os.environ.get('IBIS_TEST_CLICKHOUSE_HOST',\n    ...                                  'localhost')\n    >>> clickhouse_port = int(os.environ.get('IBIS_TEST_CLICKHOUSE_PORT',\n    ...                                      9000))\n    >>> client = ibis.clickhouse.connect(\n    ...     host=clickhouse_host,\n    ...     port=clickhouse_port\n    ... )\n    >>> client  # doctest: +ELLIPSIS\n    <ibis.clickhouse.client.ClickhouseClient object at 0x...>\n\n    Returns\n    -------\n    ClickhouseClient\n    \"\"\"\n    client = ClickhouseClient(\n        host,\n        port=port,\n        database=database,\n        user=user,\n        password=password,\n        client_name=client_name,\n        compression=compression,\n    )\n    if options.default_backend is None:\n        options.default_backend = client\n\n    return client"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef list_tables(self, like=None, database=None):\n        statement = 'SHOW TABLES'\n        if database:\n            statement += \" FROM `{0}`\".format(database)\n        if like:\n            m = fully_qualified_re.match(like)\n            if m:\n                database, quoted, unquoted = m.groups()\n                like = quoted or unquoted\n                return self.list_tables(like=like, database=database)\n            statement += \" LIKE '{0}'\".format(like)\n\n        data, _, _ = self.raw_sql(statement, results=True)\n        return data[0]", "response": "List tables in the current database. Like the SHOW TABLES command in the clickhouse - shell."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef list_databases(self, like=None):\n        statement = 'SELECT name FROM system.databases'\n        if like:\n            statement += \" WHERE name LIKE '{0}'\".format(like)\n\n        data, _, _ = self.raw_sql(statement, results=True)\n        return data[0]", "response": "List databases in Clickhouse cluster."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a Schema object for the indicated table and database.", "response": "def get_schema(self, table_name, database=None):\n        \"\"\"\n        Return a Schema object for the indicated table and database\n\n        Parameters\n        ----------\n        table_name : string\n          May be fully qualified\n        database : string, default None\n\n        Returns\n        -------\n        schema : ibis Schema\n        \"\"\"\n        qualified_name = self._fully_qualified_name(table_name, database)\n        query = 'DESC {0}'.format(qualified_name)\n        data, _, _ = self.raw_sql(query, results=True)\n\n        colnames, coltypes = data[:2]\n        coltypes = list(map(ClickhouseDataType.parse, coltypes))\n\n        return sch.schema(colnames, coltypes)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndetermine if the indicated table or view exists.", "response": "def exists_table(self, name, database=None):\n        \"\"\"\n        Determine if the indicated table or view exists\n\n        Parameters\n        ----------\n        name : string\n        database : string, default None\n\n        Returns\n        -------\n        if_exists : boolean\n        \"\"\"\n        return len(self.list_tables(like=name, database=database)) > 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef remap_overlapping_column_names(table_op, root_table, data_columns):\n    if not isinstance(table_op, ops.Join):\n        return None\n\n    left_root, right_root = ops.distinct_roots(table_op.left, table_op.right)\n    suffixes = {\n        left_root: constants.LEFT_JOIN_SUFFIX,\n        right_root: constants.RIGHT_JOIN_SUFFIX,\n    }\n    column_names = [\n        ({name, name + suffixes[root_table]} & data_columns, name)\n        for name in root_table.schema.names\n    ]\n    mapping = OrderedDict(\n        (first(col_name), final_name)\n        for col_name, final_name in column_names\n        if col_name\n    )\n    return mapping", "response": "Returns an OrderedDict mapping possibly - suffixed column names to column names without suffixes."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes the predicates for a table operation.", "response": "def _compute_predicates(table_op, predicates, data, scope, **kwargs):\n    \"\"\"Compute the predicates for a table operation.\n\n    Parameters\n    ----------\n    table_op : TableNode\n    predicates : List[ir.ColumnExpr]\n    data : pd.DataFrame\n    scope : dict\n    kwargs : dict\n\n    Returns\n    -------\n    computed_predicate : pd.Series[bool]\n\n    Notes\n    -----\n    This handles the cases where the predicates are computed columns, in\n    addition to the simple case of named columns coming directly from the input\n    table.\n    \"\"\"\n    for predicate in predicates:\n        # Map each root table of the predicate to the data so that we compute\n        # predicates on the result instead of any left or right tables if the\n        # Selection is on a Join. Project data to only inlude columns from\n        # the root table.\n        root_tables = predicate.op().root_tables()\n\n        # handle suffixes\n        additional_scope = {}\n        data_columns = frozenset(data.columns)\n\n        for root_table in root_tables:\n            mapping = remap_overlapping_column_names(\n                table_op, root_table, data_columns\n            )\n            if mapping is not None:\n                new_data = data.loc[:, mapping.keys()].rename(columns=mapping)\n            else:\n                new_data = data\n            additional_scope[root_table] = new_data\n\n        new_scope = toolz.merge(scope, additional_scope)\n        yield execute(predicate, scope=new_scope, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nextracts all union queries from table.", "response": "def flatten_union(table):\n    \"\"\"Extract all union queries from `table`.\n\n    Parameters\n    ----------\n    table : TableExpr\n\n    Returns\n    -------\n    Iterable[Union[TableExpr, bool]]\n    \"\"\"\n    op = table.op()\n    if isinstance(op, ops.Union):\n        return toolz.concatv(\n            flatten_union(op.left), [op.distinct], flatten_union(op.right)\n        )\n    return [table]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_result(self):\n        translated = self.translate(self.expr)\n        if self._needs_name(self.expr):\n            # TODO: this could fail in various ways\n            name = self.expr.get_name()\n            translated = self.name(translated, name)\n        return translated", "response": "Build compiled SQL expression from the bottom up and return as a string"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a string representation of the current query.", "response": "def compile(self):\n        \"\"\"\n        This method isn't yet idempotent; calling multiple times may yield\n        unexpected results\n        \"\"\"\n        # Can't tell if this is a hack or not. Revisit later\n        self.context.set_query(self)\n\n        # If any subqueries, translate them and add to beginning of query as\n        # part of the WITH section\n        with_frag = self.format_subqueries()\n\n        # SELECT\n        select_frag = self.format_select_set()\n\n        # FROM, JOIN, UNION\n        from_frag = self.format_table_set()\n\n        # WHERE\n        where_frag = self.format_where()\n\n        # GROUP BY and HAVING\n        groupby_frag = self.format_group_by()\n\n        # ORDER BY\n        order_frag = self.format_order_by()\n\n        # LIMIT\n        limit_frag = self.format_limit()\n\n        # Glue together the query fragments and return\n        query = '\\n'.join(\n            filter(\n                None,\n                [\n                    with_frag,\n                    select_frag,\n                    from_frag,\n                    where_frag,\n                    groupby_frag,\n                    order_frag,\n                    limit_frag,\n                ],\n            )\n        )\n        return query"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning an unbound Ibis table for creating expressions.", "response": "def table(schema, name=None):\n    \"\"\"\n    Create an unbound Ibis table for creating expressions. Cannot be executed\n    without being bound to some physical table.\n\n    Useful for testing\n\n    Parameters\n    ----------\n    schema : ibis Schema\n    name : string, default None\n      Name for table\n\n    Returns\n    -------\n    table : TableExpr\n    \"\"\"\n    if not isinstance(schema, Schema):\n        if isinstance(schema, dict):\n            schema = Schema.from_dict(schema)\n        else:\n            schema = Schema.from_tuples(schema)\n\n    node = ops.UnboundTable(schema, name=name)\n    return node.to_expr()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef desc(expr):\n    if not isinstance(expr, Expr):\n        return ops.DeferredSortKey(expr, ascending=False)\n    else:\n        return ops.SortKey(expr, ascending=False).to_expr()", "response": "Create a sort key that is used in sort_by"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef timestamp(value, timezone=None):\n    if isinstance(value, str):\n        try:\n            value = pd.Timestamp(value, tz=timezone)\n        except pd.errors.OutOfBoundsDatetime:\n            value = dateutil.parser.parse(value)\n    if isinstance(value, numbers.Integral):\n        raise TypeError(\n            (\n                \"Passing an integer to ibis.timestamp is not supported. Use \"\n                \"ibis.literal({value:d}).to_timestamp() to create a timestamp \"\n                \"expression from an integer.\"\n            ).format(value=value)\n        )\n    return literal(value, type=dt.Timestamp(timezone=timezone))", "response": "Returns a timestamp literal if value is likely coercible to a timestampScalar"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef date(value):\n    if isinstance(value, str):\n        value = to_date(value)\n    return literal(value, type=dt.date)", "response": "Returns a date literal if value is likely coercible to a date"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef time(value):\n    if isinstance(value, str):\n        value = to_time(value)\n    return literal(value, type=dt.time)", "response": "Returns a time literal if value is likely coercible to a time\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns an IntervalScalar object for the given value.", "response": "def interval(\n    value=None,\n    unit='s',\n    years=None,\n    quarters=None,\n    months=None,\n    weeks=None,\n    days=None,\n    hours=None,\n    minutes=None,\n    seconds=None,\n    milliseconds=None,\n    microseconds=None,\n    nanoseconds=None,\n):\n    \"\"\"\n    Returns an interval literal\n\n    Parameters\n    ----------\n    value : int or datetime.timedelta, default None\n    years : int, default None\n    quarters : int, default None\n    months : int, default None\n    days : int, default None\n    weeks : int, default None\n    hours : int, default None\n    minutes : int, default None\n    seconds : int, default None\n    milliseconds : int, default None\n    microseconds : int, default None\n    nanoseconds : int, default None\n\n    Returns\n    --------\n    result : IntervalScalar\n    \"\"\"\n    if value is not None:\n        if isinstance(value, datetime.timedelta):\n            unit = 's'\n            value = int(value.total_seconds())\n        elif not isinstance(value, int):\n            raise ValueError('Interval value must be an integer')\n    else:\n        kwds = [\n            ('Y', years),\n            ('Q', quarters),\n            ('M', months),\n            ('W', weeks),\n            ('D', days),\n            ('h', hours),\n            ('m', minutes),\n            ('s', seconds),\n            ('ms', milliseconds),\n            ('us', microseconds),\n            ('ns', nanoseconds),\n        ]\n        defined_units = [(k, v) for k, v in kwds if v is not None]\n\n        if len(defined_units) != 1:\n            raise ValueError('Exactly one argument is required')\n\n        unit, value = defined_units[0]\n\n    value_type = literal(value).type()\n    type = dt.Interval(unit, value_type)\n\n    return literal(value, type=type).op().to_expr()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef negate(arg):\n    op = arg.op()\n    if hasattr(op, 'negate'):\n        result = op.negate()\n    else:\n        result = ops.Negate(arg)\n\n    return result.to_expr()", "response": "Returns a negated version of the argument"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef count(expr, where=None):\n    op = expr.op()\n    if isinstance(op, ops.DistinctColumn):\n        result = ops.CountDistinct(op.args[0], where).to_expr()\n    else:\n        result = ops.Count(expr, where).to_expr()\n\n    return result.name('count')", "response": "Compute cardinality of a sequence size of a sequence expression."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconcatenate values using the indicated separator by default.", "response": "def group_concat(arg, sep=',', where=None):\n    \"\"\"\n    Concatenate values using the indicated separator (comma by default) to\n    produce a string\n\n    Parameters\n    ----------\n    arg : array expression\n    sep : string, default ','\n    where : bool, default None\n\n    Returns\n    -------\n    concatenated : string scalar\n    \"\"\"\n    return ops.GroupConcat(arg, sep, where).to_expr()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef arbitrary(arg, where=None, how=None):\n    return ops.Arbitrary(arg, how, where).to_expr()", "response": "Returns an arbitrary column of arbitrary values."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef where(boolean_expr, true_expr, false_null_expr):\n    op = ops.Where(boolean_expr, true_expr, false_null_expr)\n    return op.to_expr()", "response": "Returns a ternary expression that evaluates X then Y where X = True and Y = False or NULL."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nturn an aggregation or full - sample analytic operation into a windowed operation.", "response": "def over(expr, window):\n    \"\"\"\n    Turn an aggregation or full-sample analytic operation into a windowed\n    operation. See ibis.window for more details on window configuration\n\n    Parameters\n    ----------\n    expr : value expression\n    window : ibis.Window\n\n    Returns\n    -------\n    expr : type of input\n    \"\"\"\n    prior_op = expr.op()\n\n    if isinstance(prior_op, ops.WindowOp):\n        op = prior_op.over(window)\n    else:\n        op = ops.WindowOp(expr, window)\n\n    result = op.to_expr()\n\n    try:\n        name = expr.get_name()\n    except com.ExpressionError:\n        pass\n    else:\n        result = result.name(name)\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing a frequency table for this value expression", "response": "def value_counts(arg, metric_name='count'):\n    \"\"\"\n    Compute a frequency table for this value expression\n\n    Parameters\n    ----------\n\n    Returns\n    -------\n    counts : TableExpr\n      Aggregated table\n    \"\"\"\n    base = ir.find_base_table(arg)\n    metric = base.count().name(metric_name)\n\n    try:\n        arg.get_name()\n    except com.ExpressionError:\n        arg = arg.name('unnamed')\n\n    return base.group_by(arg).aggregate(metric)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef between(arg, lower, upper):\n    lower = as_value_expr(lower)\n    upper = as_value_expr(upper)\n\n    op = ops.Between(arg, lower, upper)\n    return op.to_expr()", "response": "Check if the input expr falls between lower and upper bounds."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck whether the value expression is contained within the specified list of values.", "response": "def isin(arg, values):\n    \"\"\"\n    Check whether the value expression is contained within the indicated\n    list of values.\n\n    Parameters\n    ----------\n    values : list, tuple, or array expression\n      The values can be scalar or array-like. Each of them must be\n      comparable with the calling expression, or None (NULL).\n\n    Examples\n    --------\n    >>> import ibis\n    >>> table = ibis.table([('string_col', 'string')])\n    >>> table2 = ibis.table([('other_string_col', 'string')])\n    >>> expr = table.string_col.isin(['foo', 'bar', 'baz'])\n    >>> expr2 = table.string_col.isin(table2.other_string_col)\n\n    Returns\n    -------\n    contains : BooleanValue\n    \"\"\"\n    op = ops.Contains(arg, values)\n    return op.to_expr()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef notin(arg, values):\n    op = ops.NotContains(arg, values)\n    return op.to_expr()", "response": "Like isin but checks whether this expression s value(s ) are not contained in the passed values."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef substitute(arg, value, replacement=None, else_=None):\n    expr = arg.case()\n    if isinstance(value, dict):\n        for k, v in sorted(value.items()):\n            expr = expr.when(k, v)\n    else:\n        expr = expr.when(value, replacement)\n\n    if else_ is not None:\n        expr = expr.else_(else_)\n    else:\n        expr = expr.else_(arg)\n\n    return expr.end()", "response": "Substitute one or more values in a value expression"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cases(arg, case_result_pairs, default=None):\n    builder = arg.case()\n    for case, result in case_result_pairs:\n        builder = builder.when(case, result)\n    if default is not None:\n        builder = builder.else_(default)\n    return builder.end()", "response": "Create a case expression in one shot."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a sequence of k topk elements in a sequence.", "response": "def topk(arg, k, by=None):\n    \"\"\"\n    Returns\n    -------\n    topk : TopK filter expression\n    \"\"\"\n    op = ops.TopK(arg, k, by=by)\n    return op.to_expr()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _generic_summary(arg, exact_nunique=False, prefix=None):\n    metrics = [arg.count(), arg.isnull().sum().name('nulls')]\n\n    if exact_nunique:\n        unique_metric = arg.nunique().name('uniques')\n    else:\n        unique_metric = arg.approx_nunique().name('uniques')\n\n    metrics.append(unique_metric)\n    return _wrap_summary_metrics(metrics, prefix)", "response": "Generic summary function for the input value expression."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes a set of summary metrics from the input numeric value expression", "response": "def _numeric_summary(arg, exact_nunique=False, prefix=None):\n    \"\"\"\n    Compute a set of summary metrics from the input numeric value expression\n\n    Parameters\n    ----------\n    arg : numeric value expression\n    exact_nunique : boolean, default False\n    prefix : string, default None\n      String prefix for metric names\n\n    Returns\n    -------\n    summary : (count, # nulls, min, max, sum, mean, nunique)\n    \"\"\"\n    metrics = [\n        arg.count(),\n        arg.isnull().sum().name('nulls'),\n        arg.min(),\n        arg.max(),\n        arg.sum(),\n        arg.mean(),\n    ]\n\n    if exact_nunique:\n        unique_metric = arg.nunique().name('nunique')\n    else:\n        unique_metric = arg.approx_nunique().name('approx_nunique')\n\n    metrics.append(unique_metric)\n    return _wrap_summary_metrics(metrics, prefix)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the value of the rounded to the specified number of decimal places.", "response": "def round(arg, digits=None):\n    \"\"\"\n    Round values either to integer or indicated number of decimal places.\n\n    Returns\n    -------\n    rounded : type depending on digits argument\n      digits None or 0\n        decimal types: decimal\n        other numeric types: bigint\n      digits nonzero\n        decimal types: decimal\n        other numeric types: double\n    \"\"\"\n    op = ops.Round(arg, digits)\n    return op.to_expr()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef log(arg, base=None):\n    op = ops.Log(arg, base)\n    return op.to_expr()", "response": "Returns the logarithm of the input argument."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef clip(arg, lower=None, upper=None):\n    if lower is None and upper is None:\n        raise ValueError(\"at least one of lower and \" \"upper must be provided\")\n\n    op = ops.Clip(arg, lower, upper)\n    return op.to_expr()", "response": "Trim values at input threshold ( s."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef quantile(arg, quantile, interpolation='linear'):\n    if isinstance(quantile, collections.abc.Sequence):\n        op = ops.MultiQuantile(arg, quantile, interpolation)\n    else:\n        op = ops.Quantile(arg, quantile, interpolation)\n    return op.to_expr()", "response": "Computes the value at the given quantile"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _integer_to_timestamp(arg, unit='s'):\n    op = ops.TimestampFromUNIX(arg, unit)\n    return op.to_expr()", "response": "Convert an integer UNIX timestamp to a timestamp expression"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts an integer interval to an interval expression", "response": "def _integer_to_interval(arg, unit='s'):\n    \"\"\"\n    Convert integer interval with the same inner type\n\n    Parameters\n    ----------\n    unit : {'Y', 'M', 'W', 'D', 'h', 'm', s', 'ms', 'us', 'ns'}\n\n    Returns\n    -------\n    interval : interval value expression\n    \"\"\"\n    op = ops.IntervalFromInteger(arg, unit)\n    return op.to_expr()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef convert_base(arg, from_base, to_base):\n    return ops.BaseConvert(arg, from_base, to_base).to_expr()", "response": "Convert number from one base to another."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes the standard deviation of a numeric array.", "response": "def std(arg, where=None, how='sample'):\n    \"\"\"\n    Compute standard deviation of numeric array\n\n    Parameters\n    ----------\n    how : {'sample', 'pop'}, default 'sample'\n\n    Returns\n    -------\n    stdev : double scalar\n    \"\"\"\n    expr = ops.StandardDev(arg, how, where).to_expr()\n    expr = expr.name('std')\n    return expr"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef variance(arg, where=None, how='sample'):\n    expr = ops.Variance(arg, how, where).to_expr()\n    expr = expr.name('var')\n    return expr", "response": "Compute the variance of a numeric array."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef correlation(left, right, where=None, how='sample'):\n    expr = ops.Correlation(left, right, how, where).to_expr()\n    return expr", "response": "Compute correlation of two numeric array"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef covariance(left, right, where=None, how='sample'):\n    expr = ops.Covariance(left, right, how, where).to_expr()\n    return expr", "response": "Compute covariance of two numeric array"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the area of a geo spatial data", "response": "def geo_area(arg, use_spheroid=None):\n    \"\"\"\n    Compute area of a geo spatial data\n\n    Parameters\n    ----------\n    arg : geometry or geography\n    use_spheroid:  default None\n\n    Returns\n    -------\n    area : double scalar\n    \"\"\"\n    op = ops.GeoArea(arg, use_spheroid)\n    return op.to_expr()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef geo_contains(left, right):\n    op = ops.GeoContains(left, right)\n    return op.to_expr()", "response": "Check if the first geometry contains the second one"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute distance between two geo spatial data", "response": "def geo_distance(left, right, use_spheroid=None):\n    \"\"\"\n    Compute distance between two geo spatial data\n\n    Parameters\n    ----------\n    left : geometry or geography\n    right : geometry or geography\n    use_spheroid : default None\n\n    Returns\n    -------\n    distance : double scalar\n    \"\"\"\n    op = ops.GeoDistance(left, right, use_spheroid)\n    return op.to_expr()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute the length of a geo spatial data", "response": "def geo_length(arg, use_spheroid=None):\n    \"\"\"\n    Compute length of a geo spatial data\n\n    Parameters\n    ----------\n    arg : geometry or geography\n    use_spheroid : default None\n\n    Returns\n    -------\n    length : double scalar\n    \"\"\"\n    op = ops.GeoLength(arg, use_spheroid)\n    return op.to_expr()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the perimeter of a geo spatial data", "response": "def geo_perimeter(arg, use_spheroid=None):\n    \"\"\"\n    Compute perimeter of a geo spatial data\n\n    Parameters\n    ----------\n    arg : geometry or geography\n    use_spheroid : default None\n\n    Returns\n    -------\n    perimeter : double scalar\n    \"\"\"\n    op = ops.GeoPerimeter(arg, use_spheroid)\n    return op.to_expr()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef geo_max_distance(left, right):\n    op = ops.GeoMaxDistance(left, right)\n    return op.to_expr()", "response": "Returns the 2 - dimensional maximum distance between two geometries in\n    projected units."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the Nth point in a single linestring in a single linestring.", "response": "def geo_point_n(arg, n):\n    \"\"\"Return the Nth point in a single linestring in the geometry.\n    Negative values are counted backwards from the end of the LineString,\n    so that -1 is the last point. Returns NULL if there is no linestring in\n    the geometry\n\n    Parameters\n    ----------\n    arg : geometry\n    n : integer\n\n    Returns\n    -------\n    PointN : geometry scalar\n    \"\"\"\n    op = ops.GeoPointN(arg, n)\n    return op.to_expr()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ifelse(arg, true_expr, false_expr):\n    # Result will be the result of promotion of true/false exprs. These\n    # might be conflicting types; same type resolution as case expressions\n    # must be used.\n    case = ops.SearchedCaseBuilder()\n    return case.when(arg, true_expr).else_(false_expr).end()", "response": "Returns a ternary expression that applies a boolean expression to the given boolean expression."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _string_substr(self, start, length=None):\n    op = ops.Substring(self, start, length)\n    return op.to_expr()", "response": "Returns a sequence of substrings from the beginning of each string in the sequence."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntranslates a string from one set of characters to another.", "response": "def _translate(self, from_str, to_str):\n    \"\"\"\n    Returns string with set of 'from' characters replaced\n    by set of 'to' characters.\n    from_str[x] is replaced by to_str[x].\n    To avoid unexpected behavior, from_str should be\n    shorter than to_string.\n\n    Parameters\n    ----------\n    from_str : string\n    to_str : string\n\n    Examples\n    --------\n    >>> import ibis\n    >>> table = ibis.table([('string_col', 'string')])\n    >>> expr = table.string_col.translate('a', 'b')\n    >>> expr = table.string_col.translate('a', 'bc')\n\n    Returns\n    -------\n    translated : string\n    \"\"\"\n    return ops.Translate(self, from_str, to_str).to_expr()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _string_find(self, substr, start=None, end=None):\n    if end is not None:\n        raise NotImplementedError\n    return ops.StringFind(self, substr, start, end).to_expr()", "response": "Returns the position of first occurence of substring optionally after a particular position."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _lpad(self, length, pad=' '):\n    return ops.LPad(self, length, pad).to_expr()", "response": "Returns a new string with given length by truncating on right and padding on left."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _rpad(self, length, pad=' '):\n    return ops.RPad(self, length, pad).to_expr()", "response": "Returns a new string with given length by truncating on right."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _string_like(self, patterns):\n    return functools.reduce(\n        operator.or_,\n        (\n            ops.StringSQLLike(self, pattern).to_expr()\n            for pattern in util.promote_list(patterns)\n        ),\n    )", "response": "Returns a fuzzy matching function equivalent to the SQL LIKE directive."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _string_ilike(self, patterns):\n    return functools.reduce(\n        operator.or_,\n        (\n            ops.StringSQLILike(self, pattern).to_expr()\n            for pattern in util.promote_list(patterns)\n        ),\n    )", "response": "Returns a fuzzy matching function equivalent to the SQL LIKE directive."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn specified index 0 indexed from string based on regex pattern given", "response": "def regex_extract(arg, pattern, index):\n    \"\"\"\n    Returns specified index, 0 indexed, from string based on regex pattern\n    given\n\n    Parameters\n    ----------\n    pattern : string (regular expression string)\n    index : int, 0 indexed\n\n    Returns\n    -------\n    extracted : string\n    \"\"\"\n    return ops.RegexExtract(arg, pattern, index).to_expr()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreplace match found by regex with replacement string.", "response": "def regex_replace(arg, pattern, replacement):\n    \"\"\"\n    Replaces match found by regex with replacement string.\n    Replacement string can also be a regex\n\n    Parameters\n    ----------\n    pattern : string (regular expression string)\n    replacement : string (can be regular expression string)\n\n    Examples\n    --------\n    >>> import ibis\n    >>> table = ibis.table([('strings', 'string')])\n    >>> result = table.strings.replace('(b+)', r'<\\1>')  # 'aaabbbaa' becomes 'aaa<bbb>aaa'  # noqa: E501\n\n    Returns\n    -------\n    modified : string\n    \"\"\"\n    return ops.RegexReplace(arg, pattern, replacement).to_expr()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a new string that replaces each exactly occurrence of pattern with given replacement .", "response": "def _string_replace(arg, pattern, replacement):\n    \"\"\"\n    Replaces each exactly occurrence of pattern with given replacement\n    string. Like Python built-in str.replace\n\n    Parameters\n    ----------\n    pattern : string\n    replacement : string\n\n    Examples\n    --------\n    >>> import ibis\n    >>> table = ibis.table([('strings', 'string')])\n    >>> result = table.strings.replace('aaa', 'foo')  # 'aaabbbaaa' becomes 'foobbbfoo'  # noqa: E501\n\n    Returns\n    -------\n    replaced : string\n    \"\"\"\n    return ops.StringReplace(arg, pattern, replacement).to_expr()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses a string and returns a timestamp.", "response": "def to_timestamp(arg, format_str, timezone=None):\n    \"\"\"\n    Parses a string and returns a timestamp.\n\n    Parameters\n    ----------\n    format_str : A format string potentially of the type '%Y-%m-%d'\n    timezone : An optional string indicating the timezone,\n        i.e. 'America/New_York'\n\n    Examples\n    --------\n    >>> import ibis\n    >>> date_as_str = ibis.literal('20170206')\n    >>> result = date_as_str.to_timestamp('%Y%m%d')\n\n    Returns\n    -------\n    parsed : TimestampValue\n    \"\"\"\n    return ops.StringToTimestamp(arg, format_str, timezone).to_expr()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_url(arg, extract, key=None):\n    return ops.ParseURL(arg, extract, key).to_expr()", "response": "Parse a URL into a sequence of expressions"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nslicing or index array at index.", "response": "def _array_slice(array, index):\n    \"\"\"Slice or index `array` at `index`.\n\n    Parameters\n    ----------\n    index : int or ibis.expr.types.IntegerValue or slice\n\n    Returns\n    -------\n    sliced_array : ibis.expr.types.ValueExpr\n        If `index` is an ``int`` or :class:`~ibis.expr.types.IntegerValue` then\n        the return type is the element type of `array`. If `index` is a\n        ``slice`` then the return type is the same type as the input.\n    \"\"\"\n    if isinstance(index, slice):\n        start = index.start\n        stop = index.stop\n        if (start is not None and start < 0) or (\n            stop is not None and stop < 0\n        ):\n            raise ValueError('negative slicing not yet supported')\n\n        step = index.step\n\n        if step is not None and step != 1:\n            raise NotImplementedError('step can only be 1')\n\n        op = ops.ArraySlice(array, start if start is not None else 0, stop)\n    else:\n        op = ops.ArrayIndex(array, index)\n    return op.to_expr()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the value of the key or the default if the key does not exist.", "response": "def get(expr, key, default=None):\n    \"\"\"\n    Return the mapped value for this key, or the default\n    if the key does not exist\n\n    Parameters\n    ----------\n    key : any\n    default : any\n    \"\"\"\n    return ops.MapValueOrDefaultForKey(expr, key, default).to_expr()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _struct_get_field(expr, field_name):\n    return ops.StructField(expr, field_name).to_expr().name(field_name)", "response": "Returns the field_name field from the Struct expression expr."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef between_time(arg, lower, upper, timezone=None):\n\n    if isinstance(arg.op(), ops.Time):\n        # Here we pull out the first argument to the underlying Time operation\n        # which is by definition (in _timestamp_value_methods) a\n        # TimestampValue. We do this so that we can potentially specialize the\n        # \"between time\" operation for timestamp_value_expr.time().between().\n        # A similar mechanism is triggered when creating expressions like\n        # t.column.distinct().count(), which is turned into t.column.nunique().\n        arg = arg.op().args[0]\n        if timezone is not None:\n            arg = arg.cast(dt.Timestamp(timezone=timezone))\n        op = ops.BetweenTime(arg, lower, upper)\n    else:\n        op = ops.Between(arg, lower, upper)\n\n    return op.to_expr()", "response": "Check if the input expr falls between lower and upper bounds."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef join(left, right, predicates=(), how='inner'):\n    klass = _join_classes[how.lower()]\n    if isinstance(predicates, Expr):\n        predicates = _L.flatten_predicate(predicates)\n\n    op = klass(left, right, predicates)\n    return op.to_expr()", "response": "Perform a relational join between two tables."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef asof_join(left, right, predicates=(), by=(), tolerance=None):\n    return ops.AsOfJoin(left, right, predicates, by, tolerance).to_expr()", "response": "Perform an asof join between two tables."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cross_join(*tables, **kwargs):\n    # TODO(phillipc): Implement prefix keyword argument\n    op = ops.CrossJoin(*tables, **kwargs)\n    return op.to_expr()", "response": "Perform a cross join on a list of tables."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _table_info(self, buf=None):\n    metrics = [self.count().name('nrows')]\n    for col in self.columns:\n        metrics.append(self[col].count().name(col))\n\n    metrics = self.aggregate(metrics).execute().loc[0]\n\n    names = ['Column', '------'] + self.columns\n    types = ['Type', '----'] + [repr(x) for x in self.schema().types]\n    counts = ['Non-null #', '----------'] + [str(x) for x in metrics[1:]]\n    col_metrics = util.adjoin(2, names, types, counts)\n    result = 'Table rows: {}\\n\\n{}'.format(metrics[0], col_metrics)\n\n    print(result, file=buf)", "response": "Similar to pandas DataFrame. info. Show column names types and nullCounts. Output to buf by default."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _table_set_column(table, name, expr):\n    expr = table._ensure_expr(expr)\n\n    if expr._name != name:\n        expr = expr.name(name)\n\n    if name not in table:\n        raise KeyError('{0} is not in the table'.format(name))\n\n    # TODO: This assumes that projection is required; may be backend-dependent\n    proj_exprs = []\n    for key in table.columns:\n        if key == name:\n            proj_exprs.append(expr)\n        else:\n            proj_exprs.append(table[key])\n\n    return table.projection(proj_exprs)", "response": "Returns a new table expression with the given column name replaced with expr."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef filter(table, predicates):\n    resolved_predicates = _resolve_predicates(table, predicates)\n    return _L.apply_filter(table, resolved_predicates)", "response": "Returns a filtered version of the table based on boolean expressions."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef aggregate(table, metrics=None, by=None, having=None, **kwds):\n    if metrics is None:\n        metrics = []\n\n    for k, v in sorted(kwds.items()):\n        v = table._ensure_expr(v)\n        metrics.append(v.name(k))\n\n    op = table.op().aggregate(table, metrics, by=by, having=having)\n    return op.to_expr()", "response": "Aggregate a table with a given set of reductions with grouping\n    expressions and post - aggregation filters."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _table_limit(table, n, offset=0):\n    op = ops.Limit(table, n, offset=offset)\n    return op.to_expr()", "response": "Returns a TableExpr that is limited to the first n rows in a table."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsorts table by the indicated column expressions and sort orders", "response": "def _table_sort_by(table, sort_exprs):\n    \"\"\"\n    Sort table by the indicated column expressions and sort orders\n    (ascending/descending)\n\n    Parameters\n    ----------\n    sort_exprs : sorting expressions\n      Must be one of:\n        - Column name or expression\n        - Sort key, e.g. desc(col)\n        - (column name, True (ascending) / False (descending))\n\n    Examples\n    --------\n    >>> import ibis\n    >>> t = ibis.table([('a', 'int64'), ('b', 'string')])\n    >>> ab_sorted = t.sort_by([('a', True), ('b', False)])\n\n    Returns\n    -------\n    sorted : TableExpr\n    \"\"\"\n    result = table.op().sort_by(table, sort_exprs)\n    return result.to_expr()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the table set union of two tables.", "response": "def _table_union(left, right, distinct=False):\n    \"\"\"\n    Form the table set union of two table expressions having identical\n    schemas.\n\n    Parameters\n    ----------\n    right : TableExpr\n    distinct : boolean, default False\n        Only union distinct rows not occurring in the calling table (this\n        can be very expensive, be careful)\n\n    Returns\n    -------\n    union : TableExpr\n    \"\"\"\n    op = ops.Union(left, right, distinct=distinct)\n    return op.to_expr()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nforces schema resolution for a joined table", "response": "def _table_materialize(table):\n    \"\"\"\n    Force schema resolution for a joined table, selecting all fields from\n    all tables.\n    \"\"\"\n    if table._is_materialized():\n        return table\n\n    op = ops.MaterializedJoin(table)\n    return op.to_expr()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef projection(table, exprs):\n    import ibis.expr.analysis as L\n\n    if isinstance(exprs, (Expr, str)):\n        exprs = [exprs]\n\n    projector = L.Projector(table, exprs)\n\n    op = projector.get_result()\n    return op.to_expr()", "response": "Compute new table expression with the indicated column expressions from the given table."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrelabeling a table expression by applying substitutions to the table column names.", "response": "def _table_relabel(table, substitutions, replacements=None):\n    \"\"\"\n    Change table column names, otherwise leaving table unaltered\n\n    Parameters\n    ----------\n    substitutions\n\n    Returns\n    -------\n    relabeled : TableExpr\n    \"\"\"\n    if replacements is not None:\n        raise NotImplementedError\n\n    observed = set()\n\n    exprs = []\n    for c in table.columns:\n        expr = table[c]\n        if c in substitutions:\n            expr = expr.name(substitutions[c])\n            observed.add(c)\n        exprs.append(expr)\n\n    for c in substitutions:\n        if c not in observed:\n            raise KeyError('{0!r} is not an existing column'.format(c))\n\n    return table.projection(exprs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprevent optimization from happening below expr.", "response": "def prevent_rewrite(expr, client=None):\n    \"\"\"Prevent optimization from happening below `expr`.\n\n    Parameters\n    ----------\n    expr : ir.TableExpr\n        Any table expression whose optimization you want to prevent\n    client : ibis.client.Client, optional, default None\n        A client to use to create the SQLQueryResult operation. This is useful\n        if you're compiling an expression that derives from an\n        :class:`~ibis.expr.operations.UnboundTable` operation.\n\n    Returns\n    -------\n    sql_query_result : ir.TableExpr\n    \"\"\"\n    if client is None:\n        client, = ibis.client.find_backends(expr)\n    query = client.compile(expr)\n    return ops.SQLQueryResult(query, expr.schema(), client).to_expr()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _flatten_subclass_tree(cls):\n    subclasses = frozenset(cls.__subclasses__())\n    children = frozenset(toolz.concat(map(_flatten_subclass_tree, subclasses)))\n    return frozenset({cls}) | subclasses | children", "response": "Return the set of all child classes of cls."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts BigQuery field to an ibis type.", "response": "def bigquery_field_to_ibis_dtype(field):\n    \"\"\"Convert BigQuery `field` to an ibis type.\"\"\"\n    typ = field.field_type\n    if typ == 'RECORD':\n        fields = field.fields\n        assert fields, 'RECORD fields are empty'\n        names = [el.name for el in fields]\n        ibis_types = list(map(dt.dtype, fields))\n        ibis_type = dt.Struct(names, ibis_types)\n    else:\n        ibis_type = _LEGACY_TO_STANDARD.get(typ, typ)\n        ibis_type = _DTYPE_TO_IBIS_TYPE.get(ibis_type, ibis_type)\n    if field.mode == 'REPEATED':\n        ibis_type = dt.Array(ibis_type)\n    return ibis_type"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef bigquery_schema(table):\n    fields = OrderedDict((el.name, dt.dtype(el)) for el in table.schema)\n    partition_info = table._properties.get('timePartitioning', None)\n\n    # We have a partitioned table\n    if partition_info is not None:\n        partition_field = partition_info.get('field', NATIVE_PARTITION_COL)\n\n        # Only add a new column if it's not already a column in the schema\n        fields.setdefault(partition_field, dt.timestamp)\n    return sch.schema(fields)", "response": "Infer the schema of a BigQuery table object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding all ScalarParameter instances in the sequence.", "response": "def _find_scalar_parameter(expr):\n    \"\"\"Find all :class:`~ibis.expr.types.ScalarParameter` instances.\n\n    Parameters\n    ----------\n    expr : ibis.expr.types.Expr\n\n    Returns\n    -------\n    Tuple[bool, object]\n        The operation and the parent expresssion's resolved name.\n\n    \"\"\"\n    op = expr.op()\n\n    if isinstance(op, ops.ScalarParameter):\n        result = op, expr.get_name()\n    else:\n        result = None\n    return lin.proceed, result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing the billing project data project and dataset if available.", "response": "def parse_project_and_dataset(\n    project: str, dataset: Optional[str] = None\n) -> Tuple[str, str, Optional[str]]:\n    \"\"\"Compute the billing project, data project, and dataset if available.\n\n    This function figure out the project id under which queries will run versus\n    the project of where the data live as well as what dataset to use.\n\n    Parameters\n    ----------\n    project : str\n        A project name\n    dataset : Optional[str]\n        A ``<project>.<dataset>`` string or just a dataset name\n\n    Examples\n    --------\n    >>> data_project, billing_project, dataset = parse_project_and_dataset(\n    ...     'ibis-gbq',\n    ...     'foo-bar.my_dataset'\n    ... )\n    >>> data_project\n    'foo-bar'\n    >>> billing_project\n    'ibis-gbq'\n    >>> dataset\n    'my_dataset'\n    >>> data_project, billing_project, dataset = parse_project_and_dataset(\n    ...     'ibis-gbq',\n    ...     'my_dataset'\n    ... )\n    >>> data_project\n    'ibis-gbq'\n    >>> billing_project\n    'ibis-gbq'\n    >>> dataset\n    'my_dataset'\n    >>> data_project, billing_project, dataset = parse_project_and_dataset(\n    ...     'ibis-gbq'\n    ... )\n    >>> data_project\n    'ibis-gbq'\n    >>> print(dataset)\n    None\n\n    \"\"\"\n    try:\n        data_project, dataset = dataset.split('.')\n    except (ValueError, AttributeError):\n        billing_project = data_project = project\n    else:\n        billing_project = project\n\n    return data_project, billing_project, dataset"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef columns(self):\n        result = self.query.result()\n        return [field.name for field in result.schema]", "response": "Return the columns of the result set."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef description(self):\n        result = self.query.result()\n        return [field for field in result.schema]", "response": "Get the fields of the result set s schema."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef connect(\n    host='localhost',\n    user=None,\n    password=None,\n    port=3306,\n    database=None,\n    url=None,\n    driver='pymysql',\n):\n\n    \"\"\"Create an Ibis client located at `user`:`password`@`host`:`port`\n    connected to a MySQL database named `database`.\n\n    Parameters\n    ----------\n    host : string, default 'localhost'\n    user : string, default None\n    password : string, default None\n    port : string or integer, default 3306\n    database : string, default None\n    url : string, default None\n        Complete SQLAlchemy connection string. If passed, the other connection\n        arguments are ignored.\n    driver : string, default 'pymysql'\n\n    Returns\n    -------\n    MySQLClient\n\n    Examples\n    --------\n    >>> import os\n    >>> import getpass\n    >>> host = os.environ.get('IBIS_TEST_MYSQL_HOST', 'localhost')\n    >>> user = os.environ.get('IBIS_TEST_MYSQL_USER', getpass.getuser())\n    >>> password = os.environ.get('IBIS_TEST_MYSQL_PASSWORD')\n    >>> database = os.environ.get('IBIS_TEST_MYSQL_DATABASE',\n    ...                           'ibis_testing')\n    >>> con = connect(\n    ...     database=database,\n    ...     host=host,\n    ...     user=user,\n    ...     password=password\n    ... )\n    >>> con.list_tables()  # doctest: +ELLIPSIS\n    [...]\n    >>> t = con.table('functional_alltypes')\n    >>> t\n    MySQLTable[table]\n      name: functional_alltypes\n      schema:\n        index : int64\n        Unnamed: 0 : int64\n        id : int32\n        bool_col : int8\n        tinyint_col : int8\n        smallint_col : int16\n        int_col : int32\n        bigint_col : int64\n        float_col : float32\n        double_col : float64\n        date_string_col : string\n        string_col : string\n        timestamp_col : timestamp\n        year : int32\n        month : int32\n    \"\"\"\n    return MySQLClient(\n        host=host,\n        user=user,\n        password=password,\n        port=port,\n        database=database,\n        url=url,\n        driver=driver,\n    )", "response": "Create an Ibis client from a MySQL connection."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef execute_cast_simple_literal_to_timestamp(op, data, type, **kwargs):\n    return pd.Timestamp(data, tz=type.timezone)", "response": "Cast integer and strings to timestamps"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef execute_cast_timestamp_to_timestamp(op, data, type, **kwargs):\n    input_timezone = data.tz\n    target_timezone = type.timezone\n\n    if input_timezone == target_timezone:\n        return data\n\n    if input_timezone is None or target_timezone is None:\n        return data.tz_localize(target_timezone)\n\n    return data.tz_convert(target_timezone)", "response": "Cast timestamps to other timestamps including timezone if necessary"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef wrap_case_result(raw, expr):\n    raw_1d = np.atleast_1d(raw)\n    if np.any(pd.isnull(raw_1d)):\n        result = pd.Series(raw_1d)\n    else:\n        result = pd.Series(\n            raw_1d, dtype=constants.IBIS_TYPE_TO_PANDAS_TYPE[expr.type()]\n        )\n    if result.size == 1 and isinstance(expr, ir.ScalarExpr):\n        return result.item()\n    return result", "response": "Wrap a CASE statement result in a Series and handle returning scalars."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a window clause for use with window functions.", "response": "def window(preceding=None, following=None, group_by=None, order_by=None):\n    \"\"\"Create a window clause for use with window functions.\n\n    This ROW window clause aggregates adjacent rows based on differences in row\n    number.\n\n    All window frames / ranges are inclusive.\n\n    Parameters\n    ----------\n    preceding : int, tuple, or None, default None\n        Specify None for unbounded, 0 to include current row tuple for\n        off-center window\n    following : int, tuple, or None, default None\n        Specify None for unbounded, 0 to include current row tuple for\n        off-center window\n    group_by : expressions, default None\n        Either specify here or with TableExpr.group_by\n    order_by : expressions, default None\n        For analytic functions requiring an ordering, specify here, or let Ibis\n        determine the default ordering (for functions like rank)\n\n    Returns\n    -------\n    Window\n\n    \"\"\"\n    return Window(\n        preceding=preceding,\n        following=following,\n        group_by=group_by,\n        order_by=order_by,\n        how='rows',\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a range - based window clause for use with window functions.", "response": "def range_window(preceding=None, following=None, group_by=None, order_by=None):\n    \"\"\"Create a range-based window clause for use with window functions.\n\n    This RANGE window clause aggregates rows based upon differences in the\n    value of the order-by expression.\n\n    All window frames / ranges are inclusive.\n\n    Parameters\n    ----------\n    preceding : int, tuple, or None, default None\n        Specify None for unbounded, 0 to include current row tuple for\n        off-center window\n    following : int, tuple, or None, default None\n        Specify None for unbounded, 0 to include current row tuple for\n        off-center window\n    group_by : expressions, default None\n        Either specify here or with TableExpr.group_by\n    order_by : expressions, default None\n        For analytic functions requiring an ordering, specify here, or let Ibis\n        determine the default ordering (for functions like rank)\n\n    Returns\n    -------\n    Window\n\n    \"\"\"\n    return Window(\n        preceding=preceding,\n        following=following,\n        group_by=group_by,\n        order_by=order_by,\n        how='range',\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a cumulative window for use with aggregate window functions.", "response": "def cumulative_window(group_by=None, order_by=None):\n    \"\"\"Create a cumulative window for use with aggregate window functions.\n\n    All window frames / ranges are inclusive.\n\n    Parameters\n    ----------\n    group_by : expressions, default None\n        Either specify here or with TableExpr.group_by\n    order_by : expressions, default None\n        For analytic functions requiring an ordering, specify here, or let Ibis\n        determine the default ordering (for functions like rank)\n\n    Returns\n    -------\n    Window\n\n    \"\"\"\n    return Window(\n        preceding=None, following=0, group_by=group_by, order_by=order_by\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef trailing_window(rows, group_by=None, order_by=None):\n    return Window(\n        preceding=rows, following=0, group_by=group_by, order_by=order_by\n    )", "response": "Create a trailing window for use with aggregate window functions."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a trailing time window for use with aggregate window functions.", "response": "def trailing_range_window(preceding, order_by, group_by=None):\n    \"\"\"Create a trailing time window for use with aggregate window functions.\n\n    Parameters\n    ----------\n    preceding : float or expression of intervals, i.e.\n        ibis.interval(days=1) + ibis.interval(hours=5)\n    order_by : expressions, default None\n        For analytic functions requiring an ordering, specify here, or let Ibis\n        determine the default ordering (for functions like rank)\n    group_by : expressions, default None\n        Either specify here or with TableExpr.group_by\n\n    Returns\n    -------\n    Window\n\n    \"\"\"\n    return Window(\n        preceding=preceding,\n        following=0,\n        group_by=group_by,\n        order_by=order_by,\n        how='range',\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndispatching to ImpalaClient. create_table.", "response": "def create_table(self, table_name, obj=None, **kwargs):\n        \"\"\"\n        Dispatch to ImpalaClient.create_table. See that function's docstring\n        for more\n        \"\"\"\n        return self.client.create_table(\n            table_name, obj=obj, database=self.name, **kwargs\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nclose all open Impyla sessions and connection pool.", "response": "def close(self):\n        \"\"\"\n        Close all open Impyla sessions\n        \"\"\"\n        for impyla_connection in self._connections:\n            impyla_connection.close()\n\n        self._connections.clear()\n        self.connection_pool.clear()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninvoke Impala COMPUTE STATS command to compute column table and partition statistics.", "response": "def compute_stats(self, incremental=False):\n        \"\"\"\n        Invoke Impala COMPUTE STATS command to compute column, table, and\n        partition statistics.\n\n        See also ImpalaClient.compute_stats\n        \"\"\"\n        return self._client.compute_stats(\n            self._qualified_name, incremental=incremental\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef insert(\n        self,\n        obj=None,\n        overwrite=False,\n        partition=None,\n        values=None,\n        validate=True,\n    ):\n        \"\"\"\n        Insert into Impala table. Wraps ImpalaClient.insert\n\n        Parameters\n        ----------\n        obj : TableExpr or pandas DataFrame\n        overwrite : boolean, default False\n          If True, will replace existing contents of table\n        partition : list or dict, optional\n          For partitioned tables, indicate the partition that's being inserted\n          into, either with an ordered list of partition keys or a dict of\n          partition field name to value. For example for the partition\n          (year=2007, month=7), this can be either (2007, 7) or {'year': 2007,\n          'month': 7}.\n        validate : boolean, default True\n          If True, do more rigorous validation that schema of table being\n          inserted is compatible with the existing table\n\n        Examples\n        --------\n        >>> t.insert(table_expr)  # doctest: +SKIP\n\n        # Completely overwrite contents\n        >>> t.insert(table_expr, overwrite=True)  # doctest: +SKIP\n        \"\"\"\n        if isinstance(obj, pd.DataFrame):\n            from ibis.impala.pandas_interop import write_temp_dataframe\n\n            writer, expr = write_temp_dataframe(self._client, obj)\n        else:\n            expr = obj\n\n        if values is not None:\n            raise NotImplementedError\n\n        if validate:\n            existing_schema = self.schema()\n            insert_schema = expr.schema()\n            if not insert_schema.equals(existing_schema):\n                _validate_compatible(insert_schema, existing_schema)\n\n        if partition is not None:\n            partition_schema = self.partition_schema()\n            partition_schema_names = frozenset(partition_schema.names)\n            expr = expr.projection(\n                [\n                    column\n                    for column in expr.columns\n                    if column not in partition_schema_names\n                ]\n            )\n        else:\n            partition_schema = None\n\n        ast = build_ast(expr, ImpalaDialect.make_context())\n        select = ast.queries[0]\n        statement = ddl.InsertSelect(\n            self._qualified_name,\n            select,\n            partition=partition,\n            partition_schema=partition_schema,\n            overwrite=overwrite,\n        )\n        return self._execute(statement)", "response": "Inserts a new entry into the Impala table."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrap the LOAD DATA DDL statement. Loads data into an Impala table by physically moving data files.", "response": "def load_data(self, path, overwrite=False, partition=None):\n        \"\"\"\n        Wraps the LOAD DATA DDL statement. Loads data into an Impala table by\n        physically moving data files.\n\n        Parameters\n        ----------\n        path : string\n        overwrite : boolean, default False\n          Overwrite the existing data in the entire table or indicated\n          partition\n        partition : dict, optional\n          If specified, the partition must already exist\n\n        Returns\n        -------\n        query : ImpalaQuery\n        \"\"\"\n        if partition is not None:\n            partition_schema = self.partition_schema()\n        else:\n            partition_schema = None\n\n        stmt = ddl.LoadData(\n            self._qualified_name,\n            path,\n            partition=partition,\n            partition_schema=partition_schema,\n        )\n\n        return self._execute(stmt)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef partition_schema(self):\n        schema = self.schema()\n        name_to_type = dict(zip(schema.names, schema.types))\n\n        result = self.partitions()\n\n        partition_fields = []\n        for x in result.columns:\n            if x not in name_to_type:\n                break\n            partition_fields.append((x, name_to_type[x]))\n\n        pnames, ptypes = zip(*partition_fields)\n        return sch.Schema(pnames, ptypes)", "response": "Returns the schema for the partitioned tables return the names and types for the partition columns\n        partition columns\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a new table partition to the table.", "response": "def add_partition(self, spec, location=None):\n        \"\"\"\n        Add a new table partition, creating any new directories in HDFS if\n        necessary.\n\n        Partition parameters can be set in a single DDL statement, or you can\n        use alter_partition to set them after the fact.\n\n        Returns\n        -------\n        None (for now)\n        \"\"\"\n        part_schema = self.partition_schema()\n        stmt = ddl.AddPartition(\n            self._qualified_name, spec, part_schema, location=location\n        )\n        return self._execute(stmt)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nchange setting and parameters of the table.", "response": "def alter(\n        self,\n        location=None,\n        format=None,\n        tbl_properties=None,\n        serde_properties=None,\n    ):\n        \"\"\"\n        Change setting and parameters of the table.\n\n        Parameters\n        ----------\n        location : string, optional\n          For partitioned tables, you may want the alter_partition function\n        format : string, optional\n        tbl_properties : dict, optional\n        serde_properties : dict, optional\n\n        Returns\n        -------\n        None (for now)\n        \"\"\"\n\n        def _run_ddl(**kwds):\n            stmt = ddl.AlterTable(self._qualified_name, **kwds)\n            return self._execute(stmt)\n\n        return self._alter_table_helper(\n            _run_ddl,\n            location=location,\n            format=format,\n            tbl_properties=tbl_properties,\n            serde_properties=serde_properties,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\naltering the partition of the current object.", "response": "def alter_partition(\n        self,\n        spec,\n        location=None,\n        format=None,\n        tbl_properties=None,\n        serde_properties=None,\n    ):\n        \"\"\"\n        Change setting and parameters of an existing partition\n\n        Parameters\n        ----------\n        spec : dict or list\n          The partition keys for the partition being modified\n        location : string, optional\n        format : string, optional\n        tbl_properties : dict, optional\n        serde_properties : dict, optional\n\n        Returns\n        -------\n        None (for now)\n        \"\"\"\n        part_schema = self.partition_schema()\n\n        def _run_ddl(**kwds):\n            stmt = ddl.AlterPartition(\n                self._qualified_name, spec, part_schema, **kwds\n            )\n            return self._execute(stmt)\n\n        return self._alter_table_helper(\n            _run_ddl,\n            location=location,\n            format=format,\n            tbl_properties=tbl_properties,\n            serde_properties=serde_properties,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef drop_partition(self, spec):\n        part_schema = self.partition_schema()\n        stmt = ddl.DropPartition(self._qualified_name, spec, part_schema)\n        return self._execute(stmt)", "response": "Drop an existing table partition."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef close(self):\n        for obj in self._temp_objects:\n            try:\n                obj.drop()\n            except HS2Error:\n                pass\n\n        self.con.close()", "response": "Closes Impala connection and drop any temporary objects that are not in use."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef list_tables(self, like=None, database=None):\n        statement = 'SHOW TABLES'\n        if database:\n            statement += ' IN {0}'.format(database)\n        if like:\n            m = ddl.fully_qualified_re.match(like)\n            if m:\n                database, quoted, unquoted = m.groups()\n                like = quoted or unquoted\n                return self.list_tables(like=like, database=database)\n            statement += \" LIKE '{0}'\".format(like)\n\n        with self._execute(statement, results=True) as cur:\n            result = self._get_list(cur)\n\n        return result", "response": "List tables in the current database."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a new Impala database with the given name and optionally path.", "response": "def create_database(self, name, path=None, force=False):\n        \"\"\"\n        Create a new Impala database\n\n        Parameters\n        ----------\n        name : string\n          Database name\n        path : string, default None\n          HDFS path where to store the database data; otherwise uses Impala\n          default\n        \"\"\"\n        if path:\n            # explicit mkdir ensures the user own the dir rather than impala,\n            # which is easier for manual cleanup, if necessary\n            self.hdfs.mkdir(path)\n        statement = ddl.CreateDatabase(name, path=path, can_exist=force)\n        return self._execute(statement)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef drop_database(self, name, force=False):\n        if not force or self.exists_database(name):\n            tables = self.list_tables(database=name)\n            udfs = self.list_udfs(database=name)\n            udas = self.list_udas(database=name)\n        else:\n            tables = []\n            udfs = []\n            udas = []\n        if force:\n            for table in tables:\n                self.log('Dropping {0}'.format('{0}.{1}'.format(name, table)))\n                self.drop_table_or_view(table, database=name)\n            for func in udfs:\n                self.log(\n                    'Dropping function {0}({1})'.format(func.name, func.inputs)\n                )\n                self.drop_udf(\n                    func.name,\n                    input_types=func.inputs,\n                    database=name,\n                    force=True,\n                )\n            for func in udas:\n                self.log(\n                    'Dropping aggregate function {0}({1})'.format(\n                        func.name, func.inputs\n                    )\n                )\n                self.drop_uda(\n                    func.name,\n                    input_types=func.inputs,\n                    database=name,\n                    force=True,\n                )\n        else:\n            if len(tables) > 0 or len(udfs) > 0 or len(udas) > 0:\n                raise com.IntegrityError(\n                    'Database {0} must be empty before '\n                    'being dropped, or set '\n                    'force=True'.format(name)\n                )\n        statement = ddl.DropDatabase(name, must_exist=not force)\n        return self._execute(statement)", "response": "Drop an Impala database."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlist the databases in the Impala cluster.", "response": "def list_databases(self, like=None):\n        \"\"\"\n        List databases in the Impala cluster. Like the SHOW DATABASES command\n        in the impala-shell.\n\n        Parameters\n        ----------\n        like : string, default None\n          e.g. 'foo*' to match all tables starting with 'foo'\n\n        Returns\n        -------\n        databases : list of strings\n        \"\"\"\n        statement = 'SHOW DATABASES'\n        if like:\n            statement += \" LIKE '{0}'\".format(like)\n\n        with self._execute(statement, results=True) as cur:\n            results = self._get_list(cur)\n\n        return results"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_schema(self, table_name, database=None):\n        qualified_name = self._fully_qualified_name(table_name, database)\n        query = 'DESCRIBE {}'.format(qualified_name)\n\n        # only pull out the first two columns which are names and types\n        pairs = [row[:2] for row in self.con.fetchall(query)]\n\n        names, types = zip(*pairs)\n        ibis_types = [udf.parse_type(type.lower()) for type in types]\n        names = [name.lower() for name in names]\n\n        return sch.Schema(names, ibis_types)", "response": "Returns a Schema object for the indicated table and database."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_options(self):\n        query = 'SET'\n        return dict(row[:2] for row in self.con.fetchall(query))", "response": "Return current query options for the Impala session"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the compression codec for the log entry.", "response": "def set_compression_codec(self, codec):\n        \"\"\"\n        Parameters\n        \"\"\"\n        if codec is None:\n            codec = 'none'\n        else:\n            codec = codec.lower()\n\n        if codec not in ('none', 'gzip', 'snappy'):\n            raise ValueError('Unknown codec: {0}'.format(codec))\n\n        self.set_options({'COMPRESSION_CODEC': codec})"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_table(\n        self,\n        table_name,\n        obj=None,\n        schema=None,\n        database=None,\n        external=False,\n        force=False,\n        # HDFS options\n        format='parquet',\n        location=None,\n        partition=None,\n        like_parquet=None,\n    ):\n        \"\"\"\n        Create a new table in Impala using an Ibis table expression. This is\n        currently designed for tables whose data is stored in HDFS (or\n        eventually other filesystems).\n\n        Parameters\n        ----------\n        table_name : string\n        obj : TableExpr or pandas.DataFrame, optional\n          If passed, creates table from select statement results\n        schema : ibis.Schema, optional\n          Mutually exclusive with expr, creates an empty table with a\n          particular schema\n        database : string, default None (optional)\n        force : boolean, default False\n          Do not create table if table with indicated name already exists\n        external : boolean, default False\n          Create an external table; Impala will not delete the underlying data\n          when the table is dropped\n        format : {'parquet'}\n        location : string, default None\n          Specify the directory location where Impala reads and writes files\n          for the table\n        partition : list of strings\n          Must pass a schema to use this. Cannot partition from an expression\n          (create-table-as-select)\n        like_parquet : string (HDFS path), optional\n          Can specify in lieu of a schema\n\n        Examples\n        --------\n        >>> con.create_table('new_table_name', table_expr)  # doctest: +SKIP\n        \"\"\"\n        if like_parquet is not None:\n            raise NotImplementedError\n\n        if obj is not None:\n            if isinstance(obj, pd.DataFrame):\n                from ibis.impala.pandas_interop import write_temp_dataframe\n\n                writer, to_insert = write_temp_dataframe(self, obj)\n            else:\n                to_insert = obj\n            ast = self._build_ast(to_insert, ImpalaDialect.make_context())\n            select = ast.queries[0]\n\n            statement = ddl.CTAS(\n                table_name,\n                select,\n                database=database,\n                can_exist=force,\n                format=format,\n                external=external,\n                partition=partition,\n                path=location,\n            )\n        elif schema is not None:\n            statement = ddl.CreateTableWithSchema(\n                table_name,\n                schema,\n                database=database,\n                format=format,\n                can_exist=force,\n                external=external,\n                path=location,\n                partition=partition,\n            )\n        else:\n            raise com.IbisError('Must pass expr or schema')\n\n        return self._execute(statement)", "response": "Create a new table in Impala using an Ibis table expression."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef avro_file(\n        self,\n        hdfs_dir,\n        avro_schema,\n        name=None,\n        database=None,\n        external=True,\n        persist=False,\n    ):\n        \"\"\"\n        Create a (possibly temporary) table to read a collection of Avro data.\n\n        Parameters\n        ----------\n        hdfs_dir : string\n          Absolute HDFS path to directory containing avro files\n        avro_schema : dict\n          The Avro schema for the data as a Python dict\n        name : string, default None\n        database : string, default None\n        external : boolean, default True\n        persist : boolean, default False\n\n        Returns\n        -------\n        avro_table : ImpalaTable\n        \"\"\"\n        name, database = self._get_concrete_table_path(\n            name, database, persist=persist\n        )\n\n        stmt = ddl.CreateTableAvro(\n            name, hdfs_dir, avro_schema, database=database, external=external\n        )\n        self._execute(stmt)\n        return self._wrap_new_table(name, database, persist)", "response": "Create a temporary table containing the avro data for the specified entry."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef delimited_file(\n        self,\n        hdfs_dir,\n        schema,\n        name=None,\n        database=None,\n        delimiter=',',\n        na_rep=None,\n        escapechar=None,\n        lineterminator=None,\n        external=True,\n        persist=False,\n    ):\n        \"\"\"\n        Interpret delimited text files (CSV / TSV / etc.) as an Ibis table. See\n        `parquet_file` for more exposition on what happens under the hood.\n\n        Parameters\n        ----------\n        hdfs_dir : string\n          HDFS directory name containing delimited text files\n        schema : ibis Schema\n        name : string, default None\n          Name for temporary or persistent table; otherwise random one\n          generated\n        database : string\n          Database to create the (possibly temporary) table in\n        delimiter : length-1 string, default ','\n          Pass None if there is no delimiter\n        escapechar : length-1 string\n          Character used to escape special characters\n        lineterminator : length-1 string\n          Character used to delimit lines\n        external : boolean, default True\n          Create table as EXTERNAL (data will not be deleted on drop). Not that\n          if persist=False and external=False, whatever data you reference will\n          be deleted\n        persist : boolean, default False\n          If True, do not delete the table upon garbage collection of ibis\n          table object\n\n        Returns\n        -------\n        delimited_table : ImpalaTable\n        \"\"\"\n        name, database = self._get_concrete_table_path(\n            name, database, persist=persist\n        )\n\n        stmt = ddl.CreateTableDelimited(\n            name,\n            hdfs_dir,\n            schema,\n            database=database,\n            delimiter=delimiter,\n            external=external,\n            na_rep=na_rep,\n            lineterminator=lineterminator,\n            escapechar=escapechar,\n        )\n        self._execute(stmt)\n        return self._wrap_new_table(name, database, persist)", "response": "Interpret delimited text files as an Ibis table."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmakes indicated parquet file in HDFS available as an Ibis table. The table created can be optionally named and persisted, otherwise a unique name will be generated. Temporarily, for any non-persistent external table created by Ibis we will attempt to drop it when the underlying object is garbage collected (or the Python interpreter shuts down normally). Parameters ---------- hdfs_dir : string Path in HDFS schema : ibis Schema If no schema provided, and neither of the like_* argument is passed, one will be inferred from one of the parquet files in the directory. like_file : string Absolute path to Parquet file in HDFS to use for schema definitions. An alternative to having to supply an explicit schema like_table : string Fully scoped and escaped string to an Impala table whose schema we will use for the newly created table. name : string, optional random unique name generated otherwise database : string, optional Database to create the (possibly temporary) table in external : boolean, default True If a table is external, the referenced data will not be deleted when the table is dropped in Impala. Otherwise (external=False) Impala takes ownership of the Parquet file. persist : boolean, default False Do not drop the table upon Ibis garbage collection / interpreter shutdown Returns ------- parquet_table : ImpalaTable", "response": "def parquet_file(\n        self,\n        hdfs_dir,\n        schema=None,\n        name=None,\n        database=None,\n        external=True,\n        like_file=None,\n        like_table=None,\n        persist=False,\n    ):\n        \"\"\"\n        Make indicated parquet file in HDFS available as an Ibis table.\n\n        The table created can be optionally named and persisted, otherwise a\n        unique name will be generated. Temporarily, for any non-persistent\n        external table created by Ibis we will attempt to drop it when the\n        underlying object is garbage collected (or the Python interpreter shuts\n        down normally).\n\n        Parameters\n        ----------\n        hdfs_dir : string\n          Path in HDFS\n        schema : ibis Schema\n          If no schema provided, and neither of the like_* argument is passed,\n          one will be inferred from one of the parquet files in the directory.\n        like_file : string\n          Absolute path to Parquet file in HDFS to use for schema\n          definitions. An alternative to having to supply an explicit schema\n        like_table : string\n          Fully scoped and escaped string to an Impala table whose schema we\n          will use for the newly created table.\n        name : string, optional\n          random unique name generated otherwise\n        database : string, optional\n          Database to create the (possibly temporary) table in\n        external : boolean, default True\n          If a table is external, the referenced data will not be deleted when\n          the table is dropped in Impala. Otherwise (external=False) Impala\n          takes ownership of the Parquet file.\n        persist : boolean, default False\n          Do not drop the table upon Ibis garbage collection / interpreter\n          shutdown\n\n        Returns\n        -------\n        parquet_table : ImpalaTable\n        \"\"\"\n        name, database = self._get_concrete_table_path(\n            name, database, persist=persist\n        )\n\n        # If no schema provided, need to find some absolute path to a file in\n        # the HDFS directory\n        if like_file is None and like_table is None and schema is None:\n            file_name = self.hdfs._find_any_file(hdfs_dir)\n            like_file = pjoin(hdfs_dir, file_name)\n\n        stmt = ddl.CreateTableParquet(\n            name,\n            hdfs_dir,\n            schema=schema,\n            database=database,\n            example_file=like_file,\n            example_table=like_table,\n            external=external,\n            can_exist=False,\n        )\n        self._execute(stmt)\n        return self._wrap_new_table(name, database, persist)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef insert(\n        self,\n        table_name,\n        obj=None,\n        database=None,\n        overwrite=False,\n        partition=None,\n        values=None,\n        validate=True,\n    ):\n        \"\"\"\n        Insert into existing table.\n\n        See ImpalaTable.insert for other parameters.\n\n        Parameters\n        ----------\n        table_name : string\n        database : string, default None\n\n        Examples\n        --------\n        >>> table = 'my_table'\n        >>> con.insert(table, table_expr)  # doctest: +SKIP\n\n        # Completely overwrite contents\n        >>> con.insert(table, table_expr, overwrite=True)  # doctest: +SKIP\n        \"\"\"\n        table = self.table(table_name, database=database)\n        return table.insert(\n            obj=obj,\n            overwrite=overwrite,\n            partition=partition,\n            values=values,\n            validate=validate,\n        )", "response": "Insert into an existing table."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load_data(\n        self, table_name, path, database=None, overwrite=False, partition=None\n    ):\n        \"\"\"\n        Wraps the LOAD DATA DDL statement. Loads data into an Impala table by\n        physically moving data files.\n\n        Parameters\n        ----------\n        table_name : string\n        database : string, default None (optional)\n        \"\"\"\n        table = self.table(table_name, database=database)\n        return table.load_data(path, overwrite=overwrite, partition=partition)", "response": "A wrapper for the LOAD DATA DDL statement that loads data into Impala table by the given table name and path."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndropping an Impala table", "response": "def drop_table(self, table_name, database=None, force=False):\n        \"\"\"\n        Drop an Impala table\n\n        Parameters\n        ----------\n        table_name : string\n        database : string, default None (optional)\n        force : boolean, default False\n          Database may throw exception if table does not exist\n\n        Examples\n        --------\n        >>> table = 'my_table'\n        >>> db = 'operations'\n        >>> con.drop_table(table, database=db, force=True)  # doctest: +SKIP\n        \"\"\"\n        statement = ddl.DropTable(\n            table_name, database=database, must_exist=not force\n        )\n        self._execute(statement)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncaches a table in the given pool.", "response": "def cache_table(self, table_name, database=None, pool='default'):\n        \"\"\"\n        Caches a table in cluster memory in the given pool.\n\n        Parameters\n        ----------\n        table_name : string\n        database : string default None (optional)\n        pool : string, default 'default'\n           The name of the pool in which to cache the table\n\n        Examples\n        --------\n        >>> table = 'my_table'\n        >>> db = 'operations'\n        >>> pool = 'op_4GB_pool'\n        >>> con.cache_table('my_table', database=db, pool=pool)  # noqa: E501 # doctest: +SKIP\n        \"\"\"\n        statement = ddl.CacheTable(table_name, database=database, pool=pool)\n        self._execute(statement)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a function within Impala.", "response": "def create_function(self, func, name=None, database=None):\n        \"\"\"\n        Creates a function within Impala\n\n        Parameters\n        ----------\n        func : ImpalaUDF or ImpalaUDA\n          Created with wrap_udf or wrap_uda\n        name : string (optional)\n        database : string (optional)\n        \"\"\"\n        if name is None:\n            name = func.name\n        database = database or self.current_database\n\n        if isinstance(func, udf.ImpalaUDF):\n            stmt = ddl.CreateUDF(func, name=name, database=database)\n        elif isinstance(func, udf.ImpalaUDA):\n            stmt = ddl.CreateUDA(func, name=name, database=database)\n        else:\n            raise TypeError(func)\n        self._execute(stmt)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndropping a UDF from the database.", "response": "def drop_udf(\n        self,\n        name,\n        input_types=None,\n        database=None,\n        force=False,\n        aggregate=False,\n    ):\n        \"\"\"\n        Drops a UDF\n        If only name is given, this will search\n        for the relevant UDF and drop it.\n        To delete an overloaded UDF, give only a name and force=True\n\n        Parameters\n        ----------\n        name : string\n        input_types : list of strings (optional)\n        force : boolean, default False Must be set to true to\n                drop overloaded UDFs\n        database : string, default None\n        aggregate : boolean, default False\n        \"\"\"\n        if not input_types:\n            if not database:\n                database = self.current_database\n            result = self.list_udfs(database=database, like=name)\n            if len(result) > 1:\n                if force:\n                    for func in result:\n                        self._drop_single_function(\n                            func.name,\n                            func.inputs,\n                            database=database,\n                            aggregate=aggregate,\n                        )\n                    return\n                else:\n                    raise Exception(\n                        \"More than one function \"\n                        + \"with {0} found.\".format(name)\n                        + \"Please specify force=True\"\n                    )\n            elif len(result) == 1:\n                func = result.pop()\n                self._drop_single_function(\n                    func.name,\n                    func.inputs,\n                    database=database,\n                    aggregate=aggregate,\n                )\n                return\n            else:\n                raise Exception(\"No function found with name {0}\".format(name))\n        self._drop_single_function(\n            name, input_types, database=database, aggregate=aggregate\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef drop_uda(self, name, input_types=None, database=None, force=False):\n        return self.drop_udf(\n            name, input_types=input_types, database=database, force=force\n        )", "response": "Drop aggregate function. See drop_udf for more information on the\n            parameters."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef list_udfs(self, database=None, like=None):\n        if not database:\n            database = self.current_database\n        statement = ddl.ListFunction(database, like=like, aggregate=False)\n        with self._execute(statement, results=True) as cur:\n            result = self._get_udfs(cur, udf.ImpalaUDF)\n        return result", "response": "Lists all UDFs associated with given database."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nlisting all UDAFs associated with a given database.", "response": "def list_udas(self, database=None, like=None):\n        \"\"\"\n        Lists all UDAFs associated with a given database\n\n        Parameters\n        ----------\n        database : string\n        like : string for searching (optional)\n        \"\"\"\n        if not database:\n            database = self.current_database\n        statement = ddl.ListFunction(database, like=like, aggregate=True)\n        with self._execute(statement, results=True) as cur:\n            result = self._get_udfs(cur, udf.ImpalaUDA)\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if a given UDF exists within a specified database.", "response": "def exists_udf(self, name, database=None):\n        \"\"\"\n        Checks if a given UDF exists within a specified database\n\n        Parameters\n        ----------\n        name : string, UDF name\n        database : string, database name\n\n        Returns\n        -------\n        if_exists : boolean\n        \"\"\"\n        return len(self.list_udfs(database=database, like=name)) > 0"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef exists_uda(self, name, database=None):\n        return len(self.list_udas(database=database, like=name)) > 0", "response": "Checks if a given UDAF exists within a specified database."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nissue COMPUTE STATS command for a given table.", "response": "def compute_stats(self, name, database=None, incremental=False):\n        \"\"\"\n        Issue COMPUTE STATS command for a given table\n\n        Parameters\n        ----------\n        name : string\n          Can be fully qualified (with database name)\n        database : string, optional\n        incremental : boolean, default False\n          If True, issue COMPUTE INCREMENTAL STATS\n        \"\"\"\n        maybe_inc = 'INCREMENTAL ' if incremental else ''\n        cmd = 'COMPUTE {0}STATS'.format(maybe_inc)\n\n        stmt = self._table_command(cmd, name, database=database)\n        self._execute(stmt)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nissuing INVALIDATE METADATA command optionally only applying to a particular table.", "response": "def invalidate_metadata(self, name=None, database=None):\n        \"\"\"\n        Issue INVALIDATE METADATA command, optionally only applying to a\n        particular table. See Impala documentation.\n\n        Parameters\n        ----------\n        name : string, optional\n          Table name. Can be fully qualified (with database)\n        database : string, optional\n        \"\"\"\n        stmt = 'INVALIDATE METADATA'\n        if name is not None:\n            stmt = self._table_command(stmt, name, database=database)\n        self._execute(stmt)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef refresh(self, name, database=None):\n        # TODO(wesm): can this statement be cancelled?\n        stmt = self._table_command('REFRESH', name, database=database)\n        self._execute(stmt)", "response": "Reloads the HDFS block location metadata for a table."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef describe_formatted(self, name, database=None):\n        from ibis.impala.metadata import parse_metadata\n\n        stmt = self._table_command(\n            'DESCRIBE FORMATTED', name, database=database\n        )\n        query = ImpalaQuery(self, stmt)\n        result = query.execute()\n\n        # Leave formatting to pandas\n        for c in result.columns:\n            result[c] = result[c].str.strip()\n\n        return parse_metadata(result)", "response": "Returns a dictionary of formatted metadata for the given table name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving the list of files in a table.", "response": "def show_files(self, name, database=None):\n        \"\"\"\n        Retrieve results of SHOW FILES command for a table. See Impala\n        documentation for more.\n\n        Parameters\n        ----------\n        name : string\n          Table name. Can be fully qualified (with database)\n        database : string, optional\n        \"\"\"\n        stmt = self._table_command('SHOW FILES IN', name, database=database)\n        return self._exec_statement(stmt)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the stats for the specified table.", "response": "def table_stats(self, name, database=None):\n        \"\"\"\n        Return results of SHOW TABLE STATS for indicated table. See also\n        ImpalaTable.stats\n        \"\"\"\n        stmt = self._table_command('SHOW TABLE STATS', name, database=database)\n        return self._exec_statement(stmt)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef column_stats(self, name, database=None):\n        stmt = self._table_command(\n            'SHOW COLUMN STATS', name, database=database\n        )\n        return self._exec_statement(stmt)", "response": "Return the column stats for the specified table."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write_dataframe(self, df, path, format='csv'):\n        from ibis.impala.pandas_interop import DataFrameWriter\n\n        writer = DataFrameWriter(self, df)\n        return writer.write_csv(path)", "response": "Writes a pandas DataFrame to a specified file path in the specified format"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds a post - aggregation result filter to the grouped table.", "response": "def having(self, expr):\n        \"\"\"\n        Add a post-aggregation result filter (like the having argument in\n        `aggregate`), for composability with the group_by API\n\n        Parameters\n        ----------\n        expr : ibis.expr.types.Expr\n\n        Returns\n        -------\n        grouped : GroupedTableExpr\n        \"\"\"\n        exprs = util.promote_list(expr)\n        new_having = self._having + exprs\n        return GroupedTableExpr(\n            self.table,\n            self.by,\n            having=new_having,\n            order_by=self._order_by,\n            window=self._window,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef mutate(self, exprs=None, **kwds):\n        if exprs is None:\n            exprs = []\n        else:\n            exprs = util.promote_list(exprs)\n\n        kwd_names = list(kwds.keys())\n        kwd_values = list(kwds.values())\n        kwd_values = self.table._resolve(kwd_values)\n\n        for k, v in sorted(zip(kwd_names, kwd_values)):\n            exprs.append(v.name(k))\n\n        return self.projection([self.table] + exprs)", "response": "Returns a table expression that is mutated with the given exprs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlike mutate but do not include existing table columns", "response": "def projection(self, exprs):\n        \"\"\"\n        Like mutate, but do not include existing table columns\n        \"\"\"\n        w = self._get_window()\n        windowed_exprs = []\n        exprs = self.table._resolve(exprs)\n        for expr in exprs:\n            expr = L.windowize_function(expr, w=w)\n            windowed_exprs.append(expr)\n        return self.table.projection(windowed_exprs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a new expression that is applied to downstream analytic expressions that are grouped over this one.", "response": "def over(self, window):\n        \"\"\"\n        Add a window clause to be applied to downstream analytic expressions\n        \"\"\"\n        return GroupedTableExpr(\n            self.table,\n            self.by,\n            having=self._having,\n            order_by=self._order_by,\n            window=window,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the count of the rows in the grouped table.", "response": "def count(self, metric_name='count'):\n        \"\"\"\n        Convenience function for computing the group sizes (number of rows per\n        group) given a grouped table.\n\n        Parameters\n        ----------\n        metric_name : string, default 'count'\n          Name to use for the row count metric\n\n        Returns\n        -------\n        aggregated : TableExpr\n          The aggregated table\n        \"\"\"\n        metric = self.table.count().name(metric_name)\n        return self.table.aggregate([metric], by=self.by, having=self._having)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef bucket(\n    arg,\n    buckets,\n    closed='left',\n    close_extreme=True,\n    include_under=False,\n    include_over=False,\n):\n    \"\"\"\n    Compute a discrete binning of a numeric array\n\n    Parameters\n    ----------\n    arg : numeric array expression\n    buckets : list\n    closed : {'left', 'right'}, default 'left'\n      Which side of each interval is closed. For example\n      buckets = [0, 100, 200]\n      closed = 'left': 100 falls in 2nd bucket\n      closed = 'right': 100 falls in 1st bucket\n    close_extreme : boolean, default True\n\n    Returns\n    -------\n    bucketed : coded value expression\n    \"\"\"\n    op = Bucket(\n        arg,\n        buckets,\n        closed=closed,\n        close_extreme=close_extreme,\n        include_under=include_under,\n        include_over=include_over,\n    )\n    return op.to_expr()", "response": "Compute a discrete binning of a numeric array."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing a histogram of the given number of bins.", "response": "def histogram(\n    arg, nbins=None, binwidth=None, base=None, closed='left', aux_hash=None\n):\n    \"\"\"\n    Compute a histogram with fixed width bins\n\n    Parameters\n    ----------\n    arg : numeric array expression\n    nbins : int, default None\n      If supplied, will be used to compute the binwidth\n    binwidth : number, default None\n      If not supplied, computed from the data (actual max and min values)\n    base : number, default None\n    closed : {'left', 'right'}, default 'left'\n      Which side of each interval is closed\n\n    Returns\n    -------\n    histogrammed : coded value expression\n    \"\"\"\n    op = Histogram(\n        arg, nbins, binwidth, base, closed=closed, aux_hash=aux_hash\n    )\n    return op.to_expr()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a string expression that represents the label of the given categories.", "response": "def category_label(arg, labels, nulls=None):\n    \"\"\"\n    Format a known number of categories as strings\n\n    Parameters\n    ----------\n    labels : list of string\n    nulls : string, optional\n      How to label any null values among the categories\n\n    Returns\n    -------\n    string_categories : string value expression\n    \"\"\"\n    op = CategoryLabel(arg, labels, nulls)\n    return op.to_expr()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a chroot for third - party isolated pex.", "response": "def isolated():\n  \"\"\"Returns a chroot for third_party isolated from the ``sys.path``.\n\n  PEX will typically be installed in site-packages flat alongside many other distributions; as such,\n  adding the location of the pex distribution to the ``sys.path`` will typically expose many other\n  distributions. An isolated chroot can be used as a ``sys.path`` entry to effect only the exposure\n  of pex.\n\n  :return: The path of the chroot.\n  :rtype: str\n  \"\"\"\n  global _ISOLATED\n  if _ISOLATED is None:\n    from pex import vendor\n    from pex.common import safe_mkdtemp, Chroot\n\n    chroot = Chroot(safe_mkdtemp())\n    with _tracer().timed('Isolating pex in {}'.format(chroot)):\n      pex_path = os.path.join(vendor.VendorSpec.ROOT, 'pex')\n      for root, _, files in os.walk(pex_path):\n        for f in files:\n          if not f.endswith('.pyc'):\n            abs_file_path = os.path.join(root, f)\n            relpath = os.path.relpath(abs_file_path, pex_path)\n            chroot.copy(abs_file_path, os.path.join('pex', relpath), label='pex')\n\n    _ISOLATED = chroot\n  return _ISOLATED.path()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef install(root=None, expose=None):\n  VendorImporter.install_vendored(prefix=import_prefix(), root=root, expose=expose)", "response": "Installs the default vendor importer for PEX vendored code."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nexpose vendored code in isolated chroots.", "response": "def expose(dists):\n  \"\"\"Exposes vendored code in isolated chroots.\n\n  Any vendored distributions listed in ``dists`` will be unpacked to individual chroots for addition\n  to the ``sys.path``; ie: ``expose(['setuptools', 'wheel'])`` will unpack these vendored\n  distributions and yield the two chroot paths they were unpacked to.\n\n  :param dists: A list of vendored distribution names to expose.\n  :type dists: list of str\n  :raise: :class:`ValueError` if any distributions to expose cannot be found.\n  :returns: An iterator of exposed vendored distribution chroot paths.\n  \"\"\"\n  from pex.common import safe_delete\n\n  for path in VendorImporter.expose(dists, root=isolated()):\n    safe_delete(os.path.join(path, '__init__.py'))\n    yield path"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef install_vendored(cls, prefix, root=None, expose=None):\n    from pex import vendor\n\n    root = cls._abs_root(root)\n    vendored_path_items = [spec.relpath for spec in vendor.iter_vendor_specs()]\n\n    installed = list(cls._iter_installed_vendor_importers(prefix, root, vendored_path_items))\n    assert len(installed) <= 1, (\n      'Unexpected extra importers installed for vendored code:\\n\\t{}'\n        .format('\\n\\t'.join(map(str, installed)))\n    )\n    if installed:\n      vendor_importer = installed[0]\n    else:\n      # Install all vendored code for pex internal access to it through the vendor import `prefix`.\n      vendor_importer = cls.install(uninstallable=True,\n                                    prefix=prefix,\n                                    path_items=vendored_path_items,\n                                    root=root)\n\n    if expose:\n      # But only expose the bits needed.\n      exposed_paths = []\n      for path in cls.expose(expose, root):\n        sys.path.insert(0, path)\n        exposed_paths.append(os.path.relpath(path, root))\n\n      vendor_importer._expose(exposed_paths)", "response": "Install an importer for all vendored code with the given prefix."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninstall an importer for modules found under path_items at the given import prefix.", "response": "def install(cls, uninstallable, prefix, path_items, root=None, warning=None):\n    \"\"\"Install an importer for modules found under ``path_items`` at the given import ``prefix``.\n\n    :param bool uninstallable: ``True`` if the installed importer should be uninstalled and any\n                               imports it performed be un-imported when ``uninstall`` is called.\n    :param str prefix: The import prefix the installed importer will be responsible for.\n    :param path_items: The paths relative to ``root`` containing modules to expose for import under\n                       ``prefix``.\n    :param str root: The root path of the distribution containing the vendored code. NB: This is the\n                     the path to the pex code, which serves as the root under which code is vendored\n                     at ``pex/vendor/_vendored``.\n    :param str warning: An optional warning to emit if any imports are made through the installed\n                        importer.\n    :return:\n    \"\"\"\n    root = cls._abs_root(root)\n    importables = tuple(cls._iter_importables(root=root, path_items=path_items, prefix=prefix))\n    vendor_importer = cls(root=root,\n                          importables=importables,\n                          uninstallable=uninstallable,\n                          warning=warning)\n    sys.meta_path.insert(0, vendor_importer)\n    _tracer().log('Installed {}'.format(vendor_importer), V=3)\n    return vendor_importer"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef wheel_dist_name(self):\n        components = (safer_name(self.distribution.get_name()),\n                      safer_version(self.distribution.get_version()))\n        if self.build_number:\n            components += (self.build_number,)\n        return '-'.join(components)", "response": "Return the full name of the wheel distribution with - replaced with _"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef egg2dist(self, egginfo_path, distinfo_path):\n        def adios(p):\n            \"\"\"Appropriately delete directory, file or link.\"\"\"\n            if os.path.exists(p) and not os.path.islink(p) and os.path.isdir(p):\n                shutil.rmtree(p)\n            elif os.path.exists(p):\n                os.unlink(p)\n\n        adios(distinfo_path)\n\n        if not os.path.exists(egginfo_path):\n            # There is no egg-info. This is probably because the egg-info\n            # file/directory is not named matching the distribution name used\n            # to name the archive file. Check for this case and report\n            # accordingly.\n            import glob\n            pat = os.path.join(os.path.dirname(egginfo_path), '*.egg-info')\n            possible = glob.glob(pat)\n            err = \"Egg metadata expected at %s but not found\" % (egginfo_path,)\n            if possible:\n                alt = os.path.basename(possible[0])\n                err += \" (%s found - possible misnamed archive file?)\" % (alt,)\n\n            raise ValueError(err)\n\n        if os.path.isfile(egginfo_path):\n            # .egg-info is a single file\n            pkginfo_path = egginfo_path\n            pkg_info = pkginfo_to_metadata(egginfo_path, egginfo_path)\n            os.mkdir(distinfo_path)\n        else:\n            # .egg-info is a directory\n            pkginfo_path = os.path.join(egginfo_path, 'PKG-INFO')\n            pkg_info = pkginfo_to_metadata(egginfo_path, pkginfo_path)\n\n            # ignore common egg metadata that is useless to wheel\n            shutil.copytree(egginfo_path, distinfo_path,\n                            ignore=lambda x, y: {'PKG-INFO', 'requires.txt', 'SOURCES.txt',\n                                                 'not-zip-safe'}\n                            )\n\n            # delete dependency_links if it is only whitespace\n            dependency_links_path = os.path.join(distinfo_path, 'dependency_links.txt')\n            with open(dependency_links_path, 'r') as dependency_links_file:\n                dependency_links = dependency_links_file.read().strip()\n            if not dependency_links:\n                adios(dependency_links_path)\n\n        write_pkg_info(os.path.join(distinfo_path, 'METADATA'), pkg_info)\n\n        # XXX heuristically copy any LICENSE/LICENSE.txt?\n        license = self.license_file()\n        if license:\n            license_filename = 'LICENSE.txt'\n            shutil.copy(license, os.path.join(distinfo_path, license_filename))\n\n        adios(egginfo_path)", "response": "Convert an egg - info directory into a. dist - info directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates a public / private key pair.", "response": "def keygen(get_keyring=get_keyring):\n    \"\"\"Generate a public/private key pair.\"\"\"\n    warn_signatures()\n    WheelKeys, keyring = get_keyring()\n\n    ed25519ll = signatures.get_ed25519ll()\n\n    wk = WheelKeys().load()\n\n    keypair = ed25519ll.crypto_sign_keypair()\n    vk = native(urlsafe_b64encode(keypair.vk))\n    sk = native(urlsafe_b64encode(keypair.sk))\n    kr = keyring.get_keyring()\n    kr.set_password(\"wheel\", vk, sk)\n    print(\"Created Ed25519 keypair with vk={}\".format(vk))\n    print(\"in {!r}\".format(kr))\n\n    sk2 = kr.get_password('wheel', vk)\n    if sk2 != sk:\n        raise WheelError(\"Keyring is broken. Could not retrieve secret key.\")\n\n    print(\"Trusting {} to sign and verify all packages.\".format(vk))\n    wk.add_signer('+', vk)\n    wk.trust('+', vk)\n    wk.save()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sign(wheelfile, replace=False, get_keyring=get_keyring):\n    warn_signatures()\n    WheelKeys, keyring = get_keyring()\n\n    ed25519ll = signatures.get_ed25519ll()\n\n    wf = WheelFile(wheelfile, append=True)\n    wk = WheelKeys().load()\n\n    name = wf.parsed_filename.group('name')\n    sign_with = wk.signers(name)[0]\n    print(\"Signing {} with {}\".format(name, sign_with[1]))\n\n    vk = sign_with[1]\n    kr = keyring.get_keyring()\n    sk = kr.get_password('wheel', vk)\n    keypair = ed25519ll.Keypair(urlsafe_b64decode(binary(vk)),\n                                urlsafe_b64decode(binary(sk)))\n\n    record_name = wf.distinfo_name + '/RECORD'\n    sig_name = wf.distinfo_name + '/RECORD.jws'\n    if sig_name in wf.zipfile.namelist():\n        raise WheelError(\"Wheel is already signed.\")\n    record_data = wf.zipfile.read(record_name)\n    payload = {\"hash\": \"sha256=\" + native(urlsafe_b64encode(hashlib.sha256(record_data).digest()))}\n    sig = signatures.sign(payload, keypair)\n    wf.zipfile.writestr(sig_name, json.dumps(sig, sort_keys=True))\n    wf.zipfile.close()", "response": "Sign a wheelfile with ed25519ll"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef verify(wheelfile):\n    warn_signatures()\n    wf = WheelFile(wheelfile)\n    sig_name = wf.distinfo_name + '/RECORD.jws'\n    try:\n        sig = json.loads(native(wf.zipfile.open(sig_name).read()))\n    except KeyError:\n        raise WheelError('The wheel is not signed (RECORD.jws not found at end of the archive).')\n\n    verified = signatures.verify(sig)\n    print(\"Signatures are internally consistent.\", file=sys.stderr)\n    print(json.dumps(verified, indent=2))", "response": "Verify a wheel.\n\n    The signature will be verified for internal consistency ONLY and printed.\n    Wheel's own unpack/install commands verify the manifest against the\n    signature and file contents."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef install_scripts(distributions):\n    try:\n        if \"__PEX_UNVENDORED__\" in __import__(\"os\").environ:\n          from setuptools.command import easy_install  # vendor:skip\n        else:\n          from pex.third_party.setuptools.command import easy_install\n\n        if \"__PEX_UNVENDORED__\" in __import__(\"os\").environ:\n          import pkg_resources  # vendor:skip\n        else:\n          import pex.third_party.pkg_resources as pkg_resources\n\n    except ImportError:\n        raise RuntimeError(\"'wheel install_scripts' needs setuptools.\")\n\n    for dist in distributions:\n        pkg_resources_dist = pkg_resources.get_distribution(dist)\n        install = get_install_command(dist)\n        command = easy_install.easy_install(install.distribution)\n        command.args = ['wheel']  # dummy argument\n        command.finalize_options()\n        command.install_egg_scripts(pkg_resources_dist)", "response": "Installs the entry_points console_scripts for the named distributions."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_rc(cls, rc=None):\n    ret_vars = {}\n    rc_locations = ['/etc/pexrc',\n                    '~/.pexrc',\n                    os.path.join(os.path.dirname(sys.argv[0]), '.pexrc')]\n    if rc:\n      rc_locations.append(rc)\n    for filename in rc_locations:\n      try:\n        with open(os.path.expanduser(filename)) as fh:\n          rc_items = map(cls._get_kv, fh)\n          ret_vars.update(dict(filter(None, rc_items)))\n      except IOError:\n        continue\n    return ret_vars", "response": "Read pex runtime configuration variables from a pexrc file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef patch(self, **kw):\n    old_environ = self._environ\n    self._environ = self._environ.copy()\n    self._environ.update(kw)\n    yield\n    self._environ = old_environ", "response": "Update the environment for the duration of a context."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the distutils install paths for the named dist.", "response": "def get_install_paths(name):\n    \"\"\"\n    Return the (distutils) install paths for the named dist.\n\n    A dict with ('purelib', 'platlib', 'headers', 'scripts', 'data') keys.\n    \"\"\"\n    paths = {}\n\n    i = get_install_command(name)\n\n    for key in install.SCHEME_KEYS:\n        paths[key] = getattr(i, 'install_' + key)\n\n    # pip uses a similar path as an alternative to the system's (read-only)\n    # include directory:\n    if hasattr(sys, 'real_prefix'):  # virtualenv\n        paths['headers'] = os.path.join(sys.prefix,\n                                        'include',\n                                        'site',\n                                        'python' + sys.version[:3],\n                                        name)\n\n    return paths"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef named_temporary_file(*args, **kwargs):\n  assert 'delete' not in kwargs\n  kwargs['delete'] = False\n  fp = tempfile.NamedTemporaryFile(*args, **kwargs)\n  try:\n    with fp:\n      yield fp\n  finally:\n    os.remove(fp.name)", "response": "A context manager that creates a temporary file and returns it."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngives a. pth file extract and yield all inner paths without honoring imports. This shadows the python s site. py behavior which is invoked at interpreter startup.", "response": "def iter_pth_paths(filename):\n  \"\"\"Given a .pth file, extract and yield all inner paths without honoring imports. This shadows\n  python's site.py behavior, which is invoked at interpreter startup.\"\"\"\n  try:\n    f = open(filename, 'rU')  # noqa\n  except IOError:\n    return\n\n  dirname = os.path.dirname(filename)\n  known_paths = set()\n\n  with f:\n    for line in f:\n      line = line.rstrip()\n      if not line or line.startswith('#'):\n        continue\n      elif line.startswith(('import ', 'import\\t')):\n        try:\n          exec_function(line, globals_map={})\n          continue\n        except Exception:\n          # NB: import lines are routinely abused with extra code appended using `;` so the class of\n          # exceptions that might be raised in broader than ImportError. As such we cacth broadly\n          # here.\n\n          # Defer error handling to the higher level site.py logic invoked at startup.\n          return\n      else:\n        extras_dir, extras_dir_case_insensitive = makepath(dirname, line)\n        if extras_dir_case_insensitive not in known_paths and os.path.exists(extras_dir):\n          yield extras_dir\n          known_paths.add(extras_dir_case_insensitive)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmerges paths into a single path delimited by colons and split on colons to return", "response": "def merge_split(*paths):\n  \"\"\"Merge paths into a single path delimited by colons and split on colons to return\n  a list of paths.\n\n  :param paths: a variable length list of path strings\n  :return: a list of paths from the merged path list split by colons\n  \"\"\"\n  filtered_paths = filter(None, paths)\n  return [p for p in ':'.join(filtered_paths).split(':') if p]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nyielding the filename and stream for files identified as data in the distribution", "response": "def walk_data(cls, dist, path='/'):\n    \"\"\"Yields filename, stream for files identified as data in the distribution\"\"\"\n    for rel_fn in filter(None, dist.resource_listdir(path)):\n      full_fn = os.path.join(path, rel_fn)\n      if dist.resource_isdir(full_fn):\n        for fn, stream in cls.walk_data(dist, full_fn):\n          yield fn, stream\n      else:\n        yield full_fn[1:], dist.get_resource_stream(dist._provider, full_fn)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn whether or not a distribution is zip - safe.", "response": "def zipsafe(dist):\n    \"\"\"Returns whether or not we determine a distribution is zip-safe.\"\"\"\n    # zip-safety is only an attribute of eggs.  wheels are considered never\n    # zip safe per implications of PEP 427.\n    if hasattr(dist, 'egg_info') and dist.egg_info.endswith('EGG-INFO'):\n      egg_metadata = dist.metadata_listdir('')\n      return 'zip-safe' in egg_metadata and 'native_libs.txt' not in egg_metadata\n    else:\n      return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef access_zipped_assets(cls, static_module_name, static_path, dir_location=None):\n\n    # asset_path is initially a module name that's the same as the static_path, but will be\n    # changed to walk the directory tree\n    def walk_zipped_assets(static_module_name, static_path, asset_path, temp_dir):\n      for asset in resource_listdir(static_module_name, asset_path):\n        asset_target = os.path.normpath(\n            os.path.join(os.path.relpath(asset_path, static_path), asset))\n        if resource_isdir(static_module_name, os.path.join(asset_path, asset)):\n          safe_mkdir(os.path.join(temp_dir, asset_target))\n          walk_zipped_assets(static_module_name, static_path, os.path.join(asset_path, asset),\n            temp_dir)\n        else:\n          with open(os.path.join(temp_dir, asset_target), 'wb') as fp:\n            path = os.path.join(static_path, asset_target)\n            file_data = resource_string(static_module_name, path)\n            fp.write(file_data)\n\n    if dir_location is None:\n      temp_dir = safe_mkdtemp()\n    else:\n      temp_dir = dir_location\n\n    walk_zipped_assets(static_module_name, static_path, static_path, temp_dir)\n\n    return temp_dir", "response": "Create a copy of static resource files from within a directory."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a single or multiple distribution from a path.", "response": "def distribution_from_path(cls, path, name=None):\n    \"\"\"Return a distribution from a path.\n\n    If name is provided, find the distribution.  If none is found matching the name,\n    return None.  If name is not provided and there is unambiguously a single\n    distribution, return that distribution otherwise None.\n    \"\"\"\n    # Monkeypatch pkg_resources finders should it not already be so.\n    register_finders()\n    if name is None:\n      distributions = set(find_distributions(path))\n      if len(distributions) == 1:\n        return distributions.pop()\n    else:\n      for dist in find_distributions(path):\n        if dist.project_name == name:\n          return dist"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates the digest of a single file in memory - efficient manner.", "response": "def update_hash(cls, filelike, digest):\n    \"\"\"Update the digest of a single file in a memory-efficient manner.\"\"\"\n    block_size = digest.block_size * 1024\n    for chunk in iter(lambda: filelike.read(block_size), b''):\n      digest.update(chunk)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the digest of a single file in a memory - efficient manner.", "response": "def hash(cls, path, digest=None, hasher=sha1):\n    \"\"\"Return the digest of a single file in a memory-efficient manner.\"\"\"\n    if digest is None:\n      digest = hasher()\n    with open(path, 'rb') as fh:\n      cls.update_hash(fh, digest)\n    return digest.hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the hash of the contents of a zipfile comparable with a cls. dir_hash.", "response": "def zip_hash(cls, zf, prefix=''):\n    \"\"\"Return the hash of the contents of a zipfile, comparable with a cls.dir_hash.\"\"\"\n    prefix_length = len(prefix)\n    names = sorted(name[prefix_length:] for name in zf.namelist()\n        if name.startswith(prefix) and not name.endswith('.pyc') and not name.endswith('/'))\n    def stream_factory(name):\n      return zf.open(prefix + name)\n    return cls._compute_hash(names, stream_factory)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pex_hash(cls, d):\n    names = sorted(f for f in cls._iter_files(d) if not (f.endswith('.pyc') or f.startswith('.')))\n    def stream_factory(name):\n      return open(os.path.join(d, name), 'rb')  # noqa: T802\n    return cls._compute_hash(names, stream_factory)", "response": "Return a reproducible hash of the contents of a directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef distribution_compatible(dist, supported_tags=None):\n  if supported_tags is None:\n    supported_tags = get_supported()\n  package = Package.from_href(dist.location)\n  if not package:\n    return False\n  return package.compatible(supported_tags)", "response": "Returns True if the given distribution is compatible with the given interpreter or platform combination."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nregistering a concrete implementation of a Package to be recognized by pex.", "response": "def register(cls, package_type):\n    \"\"\"Register a concrete implementation of a Package to be recognized by pex.\"\"\"\n    if not issubclass(package_type, cls):\n      raise TypeError('package_type must be a subclass of Package.')\n    cls._REGISTRY.add(package_type)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts from a url to a Package object.", "response": "def from_href(cls, href, **kw):\n    \"\"\"Convert from a url to Package.\n\n    :param href: The url to parse\n    :type href: string\n    :returns: A Package object if a valid concrete implementation exists, otherwise None.\n    \"\"\"\n    package = cls._HREF_TO_PACKAGE_CACHE.get(href)\n    if package is not None:\n      return package\n    link_href = Link.wrap(href)\n    for package_type in cls._REGISTRY:\n      try:\n        package = package_type(link_href.url, **kw)\n        break\n      except package_type.InvalidPackage:\n        continue\n    if package is not None:\n      cls._HREF_TO_PACKAGE_CACHE.store(href, package)\n    return package"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef satisfies(self, requirement, allow_prereleases=None):\n    requirement = maybe_requirement(requirement)\n    link_name = safe_name(self.name).lower()\n    if link_name != requirement.key:\n      return False\n\n    # NB: If we upgrade to setuptools>=34 the SpecifierSet used here (requirement.specifier) will\n    # come from a non-vendored `packaging` package and pex's bootstrap code in `PEXBuilder` will\n    # need an update.\n    return requirement.specifier.contains(self.raw_version, prereleases=allow_prereleases)", "response": "Determines whether this package satisfies the given requirement."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _add_finder(importer, finder):\n\n  existing_finder = _get_finder(importer)\n\n  if not existing_finder:\n    pkg_resources.register_finder(importer, finder)\n  else:\n    pkg_resources.register_finder(importer, ChainedFinder.of(existing_finder, finder))", "response": "Register a new pkg_resources path finder that does not replace the existing finder."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nremoves an existing finder from pkg_resources.", "response": "def _remove_finder(importer, finder):\n  \"\"\"Remove an existing finder from pkg_resources.\"\"\"\n\n  existing_finder = _get_finder(importer)\n\n  if not existing_finder:\n    return\n\n  if isinstance(existing_finder, ChainedFinder):\n    try:\n      existing_finder.finders.remove(finder)\n    except ValueError:\n      return\n    if len(existing_finder.finders) == 1:\n      pkg_resources.register_finder(importer, existing_finder.finders[0])\n    elif len(existing_finder.finders) == 0:\n      pkg_resources.register_finder(importer, pkg_resources.find_nothing)\n  else:\n    pkg_resources.register_finder(importer, pkg_resources.find_nothing)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef register_finders():\n\n  # If the previous finder is set, then we've already monkeypatched, so skip.\n  global __PREVIOUS_FINDER\n  if __PREVIOUS_FINDER:\n    return\n\n  # save previous finder so that it can be restored\n  previous_finder = _get_finder(zipimport.zipimporter)\n  assert previous_finder, 'This appears to be using an incompatible setuptools.'\n\n  # Enable finding zipped wheels.\n  pkg_resources.register_finder(\n      zipimport.zipimporter, ChainedFinder.of(pkg_resources.find_eggs_in_zip, find_wheels_in_zip))\n\n  # append the wheel finder\n  _add_finder(pkgutil.ImpImporter, find_wheels_on_path)\n\n  if importlib_machinery is not None:\n    _add_finder(importlib_machinery.FileFinder, find_wheels_on_path)\n\n  __PREVIOUS_FINDER = previous_finder", "response": "Register finders necessary for PEX to function properly."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning location content of script in distribution or None if not there.", "response": "def get_script_from_egg(name, dist):\n  \"\"\"Returns location, content of script in distribution or (None, None) if not there.\"\"\"\n  if dist.metadata_isdir('scripts') and name in dist.metadata_listdir('scripts'):\n    return (\n        os.path.join(dist.egg_info, 'scripts', name),\n        dist.get_metadata('scripts/%s' % name).replace('\\r\\n', '\\n').replace('\\r', '\\n'))\n  return None, None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef patched_packing_env(env):\n  old_env = pkg_resources.packaging.markers.default_environment\n  new_env = lambda: env\n  pkg_resources._vendor.packaging.markers.default_environment = new_env\n  try:\n    yield\n  finally:\n    pkg_resources._vendor.packaging.markers.default_environment = old_env", "response": "Monkey patch packaging. markers. default_environment to match env"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef platform_to_tags(platform, interpreter):\n  if platform.count('-') >= 3:\n    tags = platform.rsplit('-', 3)\n  else:\n    tags = [platform, interpreter.identity.impl_ver,\n            interpreter.identity.abbr_impl, interpreter.identity.abi_tag]\n  tags[0] = tags[0].replace('.', '_').replace('-', '_')\n  return tags", "response": "Splits a platform like linux_x86_64 - 36 - cp - cp36m into its components."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nresolve all of the requirements in the given iterable of Requirement - like things.", "response": "def resolve(requirements,\n            fetchers=None,\n            interpreter=None,\n            platform=None,\n            context=None,\n            precedence=None,\n            cache=None,\n            cache_ttl=None,\n            allow_prereleases=None,\n            use_manylinux=None):\n  \"\"\"Produce all distributions needed to (recursively) meet `requirements`\n\n  :param requirements: An iterator of Requirement-like things, either\n    :class:`pkg_resources.Requirement` objects or requirement strings.\n  :keyword fetchers: (optional) A list of :class:`Fetcher` objects for locating packages.  If\n    unspecified, the default is to look for packages on PyPI.\n  :keyword interpreter: (optional) A :class:`PythonInterpreter` object to use for building\n    distributions and for testing distribution compatibility.\n  :keyword versions: (optional) a list of string versions, of the form [\"33\", \"32\"],\n    or None. The first version will be assumed to support our ABI.\n  :keyword platform: (optional) specify the exact platform you want valid\n    tags for, or None. If None, use the local system platform.\n  :keyword impl: (optional) specify the exact implementation you want valid\n    tags for, or None. If None, use the local interpreter impl.\n  :keyword abi: (optional) specify the exact abi you want valid\n    tags for, or None. If None, use the local interpreter abi.\n  :keyword context: (optional) A :class:`Context` object to use for network access.  If\n    unspecified, the resolver will attempt to use the best available network context.\n  :keyword precedence: (optional) An ordered list of allowable :class:`Package` classes\n    to be used for producing distributions.  For example, if precedence is supplied as\n    ``(WheelPackage, SourcePackage)``, wheels will be preferred over building from source, and\n    eggs will not be used at all.  If ``(WheelPackage, EggPackage)`` is suppplied, both wheels and\n    eggs will be used, but the resolver will not resort to building anything from source.\n  :keyword cache: (optional) A directory to use to cache distributions locally.\n  :keyword cache_ttl: (optional integer in seconds) If specified, consider non-exact matches when\n    resolving requirements.  For example, if ``setuptools==2.2`` is specified and setuptools 2.2 is\n    available in the cache, it will always be used.  However, if a non-exact requirement such as\n    ``setuptools>=2,<3`` is specified and there exists a setuptools distribution newer than\n    cache_ttl seconds that satisfies the requirement, then it will be used.  If the distribution\n    is older than cache_ttl seconds, it will be ignored.  If ``cache_ttl`` is not specified,\n    resolving inexact requirements will always result in making network calls through the\n    ``context``.\n  :keyword allow_prereleases: (optional) Include pre-release and development versions.  If\n    unspecified only stable versions will be resolved, unless explicitly included.\n  :keyword use_manylinux: (optional) Whether or not to use manylinux for linux resolves.\n  :returns: List of :class:`ResolvedDistribution` instances meeting ``requirements``.\n  :raises Unsatisfiable: If ``requirements`` is not transitively satisfiable.\n  :raises Untranslateable: If no compatible distributions could be acquired for\n    a particular requirement.\n\n  This method improves upon the setuptools dependency resolution algorithm by maintaining sets of\n  all compatible distributions encountered for each requirement rather than the single best\n  distribution encountered for each requirement.  This prevents situations where ``tornado`` and\n  ``tornado==2.0`` could be treated as incompatible with each other because the \"best\n  distribution\" when encountering ``tornado`` was tornado 3.0.  Instead, ``resolve`` maintains the\n  set of compatible distributions for each requirement as it is encountered, and iteratively filters\n  the set.  If the set of distributions ever becomes empty, then ``Unsatisfiable`` is raised.\n\n  .. versionchanged:: 0.8\n    A number of keywords were added to make requirement resolution slightly easier to configure.\n    The optional ``obtainer`` keyword was replaced by ``fetchers``, ``translator``, ``context``,\n    ``threads``, ``precedence``, ``cache`` and ``cache_ttl``, also all optional keywords.\n\n  .. versionchanged:: 1.0\n    The ``translator`` and ``threads`` keywords have been removed.  The choice of threading\n    policy is now implicit.  The choice of translation policy is dictated by ``precedence``\n    directly.\n\n  .. versionchanged:: 1.0\n    ``resolver`` is now just a wrapper around the :class:`Resolver` and :class:`CachingResolver`\n    classes.\n\n  .. versionchanged:: 1.5.0\n    The ``pkg_blacklist``  has been removed and the return type changed to a list of\n    :class:`ResolvedDistribution`.\n  \"\"\"\n\n  builder = ResolverOptionsBuilder(fetchers=fetchers,\n                                   allow_prereleases=allow_prereleases,\n                                   use_manylinux=use_manylinux,\n                                   precedence=precedence,\n                                   context=context)\n\n  if cache:\n    resolver = CachingResolver(cache,\n                               cache_ttl,\n                               allow_prereleases=allow_prereleases,\n                               use_manylinux=use_manylinux,\n                               interpreter=interpreter,\n                               platform=platform)\n  else:\n    resolver = Resolver(allow_prereleases=allow_prereleases,\n                        use_manylinux=use_manylinux,\n                        interpreter=interpreter,\n                        platform=platform)\n\n  return resolver.resolve(resolvables_from_iterable(requirements, builder, interpreter=interpreter))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef merge(self, resolvable, packages, parent=None):\n    self.__tuples.append(_ResolvedPackages(resolvable, OrderedSet(packages),\n                                           parent, resolvable.is_constraint))\n    self._check()", "response": "Add a resolvable and its resolved packages."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the set of compatible packages given a resolvable name.", "response": "def get(self, name):\n    \"\"\"Get the set of compatible packages given a resolvable name.\"\"\"\n    resolvable, packages, parent, constraint_only = self._collapse().get(\n        self.normalize(name), _ResolvedPackages.empty())\n    return packages"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef replace_built(self, built_packages):\n    def map_packages(resolved_packages):\n      packages = OrderedSet(built_packages.get(p, p) for p in resolved_packages.packages)\n      return _ResolvedPackages(resolved_packages.resolvable, packages,\n                               resolved_packages.parent, resolved_packages.constraint_only)\n\n    return _ResolvableSet([map_packages(rp) for rp in self.__tuples])", "response": "Returns a copy of this set but with built packages replaced."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sign(payload, keypair):\n    get_ed25519ll()\n    #\n    header = {\n                \"alg\": ALG,\n                \"jwk\": {\n                    \"kty\": ALG,  # alg -> kty in jwk-08.\n                    \"vk\": native(urlsafe_b64encode(keypair.vk))\n                }\n             }\n\n    encoded_header = urlsafe_b64encode(binary(json.dumps(header, sort_keys=True)))\n    encoded_payload = urlsafe_b64encode(binary(json.dumps(payload, sort_keys=True)))\n    secured_input = b\".\".join((encoded_header, encoded_payload))\n    sig_msg = ed25519ll.crypto_sign(secured_input, keypair.sk)\n    signature = sig_msg[:ed25519ll.SIGNATUREBYTES]\n    encoded_signature = urlsafe_b64encode(signature)\n\n    return {\"recipients\":\n            [{\"header\": native(encoded_header),\n              \"signature\": native(encoded_signature)}],\n            \"payload\": native(encoded_payload)}", "response": "Return a JWS - JS format signature given a JSON - serializable payload and\naurant an Ed25519 keypair."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nverifies the signature of a JSON object.", "response": "def verify(jwsjs):\n    \"\"\"Return (decoded headers, payload) if all signatures in jwsjs are\n    consistent, else raise ValueError.\n\n    Caller must decide whether the keys are actually trusted.\"\"\"\n    get_ed25519ll()\n    # XXX forbid duplicate keys in JSON input using object_pairs_hook (2.7+)\n    recipients = jwsjs[\"recipients\"]\n    encoded_payload = binary(jwsjs[\"payload\"])\n    headers = []\n    for recipient in recipients:\n        assertTrue(len(recipient) == 2, \"Unknown recipient key {0}\".format(recipient))\n        h = binary(recipient[\"header\"])\n        s = binary(recipient[\"signature\"])\n        header = json.loads(native(urlsafe_b64decode(h)))\n        assertTrue(header[\"alg\"] == ALG,\n                   \"Unexpected algorithm {0}\".format(header[\"alg\"]))\n        if \"alg\" in header[\"jwk\"] and \"kty\" not in header[\"jwk\"]:\n            header[\"jwk\"][\"kty\"] = header[\"jwk\"][\"alg\"]  # b/w for JWK < -08\n        assertTrue(header[\"jwk\"][\"kty\"] == ALG,  # true for Ed25519\n                   \"Unexpected key type {0}\".format(header[\"jwk\"][\"kty\"]))\n        vk = urlsafe_b64decode(binary(header[\"jwk\"][\"vk\"]))\n        secured_input = b\".\".join((h, encoded_payload))\n        sig = urlsafe_b64decode(s)\n        sig_msg = sig+secured_input\n        verified_input = native(ed25519ll.crypto_sign_open(sig_msg, vk))\n        verified_header, verified_payload = verified_input.split('.')\n        verified_header = binary(verified_header)\n        decoded_header = native(urlsafe_b64decode(verified_header))\n        headers.append(json.loads(decoded_header))\n\n    verified_payload = binary(verified_payload)\n\n    # only return header, payload that have passed through the crypto library.\n    payload = json.loads(native(urlsafe_b64decode(verified_payload)))\n\n    return headers, payload"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load_internal_cache(cls, pex, pex_info):\n    internal_cache = os.path.join(pex, pex_info.internal_cache)\n    with TRACER.timed('Searching dependency cache: %s' % internal_cache, V=2):\n      if os.path.isdir(pex):\n        for dist in find_distributions(internal_cache):\n          yield dist\n      else:\n        for dist in itertools.chain(*cls.write_zipped_internal_cache(pex, pex_info)):\n          yield dist", "response": "Possibly cache out the internal cache."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read(self, link):\n    with contextlib.closing(self.open(link)) as fp:\n      return fp.read()", "response": "Return the binary content associated with the link."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfetching the binary content associated with the link and write to a file.", "response": "def fetch(self, link, into=None):\n    \"\"\"Fetch the binary content associated with the link and write to a file.\n\n    :param link: The :class:`Link` to fetch.\n    :keyword into: If specified, write into the directory ``into``.  If ``None``, creates a new\n      temporary directory that persists for the duration of the interpreter.\n    \"\"\"\n    target = os.path.join(into or safe_mkdtemp(), link.filename)\n\n    if os.path.exists(target):\n      # Assume that if the local file already exists, it is safe to use.\n      return target\n\n    with TRACER.timed('Fetching %s' % link.url, V=2):\n      target_tmp = '%s.%s' % (target, uuid.uuid4())\n      with contextlib.closing(self.open(link)) as in_fp:\n        with safe_open(target_tmp, 'wb') as out_fp:\n          shutil.copyfileobj(in_fp, out_fp)\n\n    os.rename(target_tmp, target)\n    return target"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndetects the hashing algorithm from the fragment in the link if any.", "response": "def detect_algorithm(cls, link):\n    \"\"\"Detect the hashing algorithm from the fragment in the link, if any.\"\"\"\n    if any(link.fragment.startswith('%s=' % algorithm) for algorithm in HASHLIB_ALGORITHMS):\n      algorithm, value = link.fragment.split('=', 2)\n      try:\n        return hashlib.new(algorithm), value\n      except ValueError:  # unsupported algorithm\n        return None, None\n    return None, None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef convert_requirements(requirements):\n    for req in requirements:\n        parsed_requirement = pkg_resources.Requirement.parse(req)\n        spec = requires_to_requires_dist(parsed_requirement)\n        extras = \",\".join(parsed_requirement.extras)\n        if extras:\n            extras = \"[%s]\" % extras\n        yield (parsed_requirement.project_name + extras + spec)", "response": "Yields Requires - Dist strings for parsed requirements strings."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nyielding a list of tuples that can be used to install a new requirement.", "response": "def generate_requirements(extras_require):\n    \"\"\"\n    Convert requirements from a setup()-style dictionary to ('Requires-Dist', 'requirement')\n    and ('Provides-Extra', 'extra') tuples.\n\n    extras_require is a dictionary of {extra: [requirements]} as passed to setup(),\n    using the empty extra {'': [requirements]} to hold install_requires.\n    \"\"\"\n    for extra, depends in extras_require.items():\n        condition = ''\n        if extra and ':' in extra:  # setuptools extra:condition syntax\n            extra, condition = extra.split(':', 1)\n            extra = pkg_resources.safe_extra(extra)\n        if extra:\n            yield ('Provides-Extra', extra)\n            if condition:\n                condition = \"(\" + condition + \") and \"\n            condition += \"extra == '%s'\" % extra\n        if condition:\n            condition = '; ' + condition\n        for new_req in convert_requirements(depends):\n            yield ('Requires-Dist', new_req + condition)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pkginfo_to_metadata(egg_info_path, pkginfo_path):\n    pkg_info = read_pkg_info(pkginfo_path)\n    pkg_info.replace_header('Metadata-Version', '2.1')\n    requires_path = os.path.join(egg_info_path, 'requires.txt')\n    if os.path.exists(requires_path):\n        with open(requires_path) as requires_file:\n            requires = requires_file.read()\n        for extra, reqs in sorted(pkg_resources.split_sections(requires),\n                                  key=lambda x: x[0] or ''):\n            for item in generate_requirements({extra: reqs}):\n                pkg_info[item[0]] = item[1]\n\n    description = pkg_info['Description']\n    if description:\n        pkg_info.set_payload(dedent_description(pkg_info))\n        del pkg_info['Description']\n\n    return pkg_info", "response": "Convert. egg - info directory with PKG - INFO to the Metadata 2. 1 format\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncoax Unicode out of an email Message", "response": "def pkginfo_unicode(pkg_info, field):\n    \"\"\"Hack to coax Unicode out of an email Message() - Python 3.3+\"\"\"\n    text = pkg_info[field]\n    field = field.lower()\n    if not isinstance(text, str):\n        if not hasattr(pkg_info, 'raw_items'):  # Python 3.2\n            return str(text)\n        for item in pkg_info.raw_items():\n            if item[0].lower() == field:\n                text = item[1].encode('ascii', 'surrogateescape') \\\n                    .decode('utf-8')\n                break\n\n    return text"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef trust(self, scope, vk):\n        self.data['verifiers'].append({'scope': scope, 'vk': vk})\n        return self", "response": "Start trusting a particular key for given scope."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nstopping trusting a particular key for given scope.", "response": "def untrust(self, scope, vk):\n        \"\"\"Stop trusting a particular key for given scope.\"\"\"\n        self.data['verifiers'].remove({'scope': scope, 'vk': vk})\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning list of trusted key for given scope.", "response": "def trusted(self, scope=None):\n        \"\"\"Return list of [(scope, trusted key), ...] for given scope.\"\"\"\n        trust = [(x['scope'], x['vk']) for x in self.data['verifiers']\n                 if x['scope'] in (scope, '+')]\n        trust.sort(key=lambda x: x[0])\n        trust.reverse()\n        return trust"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning list of signing key(s) in the given scope.", "response": "def signers(self, scope):\n        \"\"\"Return list of signing key(s).\"\"\"\n        sign = [(x['scope'], x['vk']) for x in self.data['signers'] if x['scope'] in (scope, '+')]\n        sign.sort(key=lambda x: x[0])\n        sign.reverse()\n        return sign"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_abi_tag():\n    soabi = get_config_var('SOABI')\n    impl = get_abbr_impl()\n    if not soabi and impl in {'cp', 'pp'} and hasattr(sys, 'maxunicode'):\n        d = ''\n        m = ''\n        u = ''\n        if get_flag('Py_DEBUG',\n                    lambda: hasattr(sys, 'gettotalrefcount'),\n                    warn=(impl == 'cp')):\n            d = 'd'\n        if get_flag('WITH_PYMALLOC',\n                    lambda: impl == 'cp',\n                    warn=(impl == 'cp')):\n            m = 'm'\n\n        # NB: Modified from ~ `from .extern import six; six.PY2`\n        PY2 = sys.version_info[0] == 2\n        if (get_flag('Py_UNICODE_SIZE',\n                     lambda: sys.maxunicode == 0x10ffff,\n                     expected=4,\n                     warn=(impl == 'cp' and PY2)) and PY2):\n            u = 'u'\n\n        abi = '%s%s%s%s%s' % (impl, get_impl_ver(), d, m, u)\n    elif soabi and soabi.startswith('cpython-'):\n        abi = 'cp' + soabi.split('-')[1]\n    elif soabi:\n        abi = soabi.replace('.', '_').replace('-', '_')\n    else:\n        abi = None\n    return abi", "response": "Return the ABI tag based on the SOABI or emulate SOABI."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nclones this PEX environment into a new PEXBuilder.", "response": "def clone(self, into=None):\n    \"\"\"Clone this PEX environment into a new PEXBuilder.\n\n    :keyword into: (optional) An optional destination directory to clone this PEXBuilder into.  If\n      not specified, a temporary directory will be created.\n\n    Clones PEXBuilder into a new location.  This is useful if the PEXBuilder has been frozen and\n    rendered immutable.\n\n    .. versionchanged:: 0.8\n      The temporary directory created when ``into`` is not specified is now garbage collected on\n      interpreter exit.\n    \"\"\"\n    chroot_clone = self._chroot.clone(into=into)\n    clone = self.__class__(\n      chroot=chroot_clone,\n      interpreter=self._interpreter,\n      pex_info=self._pex_info.copy(),\n      preamble=self._preamble,\n      copy=self._copy)\n    clone.set_shebang(self._shebang)\n    clone._distributions = self._distributions.copy()\n    return clone"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_source(self, filename, env_filename):\n    self._ensure_unfrozen('Adding source')\n    self._copy_or_link(filename, env_filename, \"source\")", "response": "Add a source file to the PEX environment."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_resource(self, filename, env_filename):\n    self._ensure_unfrozen('Adding a resource')\n    self._copy_or_link(filename, env_filename, \"resource\")", "response": "Add a resource to the PEX environment."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the executable for this environment.", "response": "def set_executable(self, filename, env_filename=None):\n    \"\"\"Set the executable for this environment.\n\n    :param filename: The file that should be executed within the PEX environment when the PEX is\n      invoked.\n    :keyword env_filename: (optional) The name that the executable file should be stored as within\n      the PEX.  By default this will be the base name of the given filename.\n\n    The entry point of the PEX may also be specified via ``PEXBuilder.set_entry_point``.\n    \"\"\"\n    self._ensure_unfrozen('Setting the executable')\n    if self._pex_info.script:\n      raise self.InvalidExecutableSpecification('Cannot set both entry point and script of PEX!')\n    if env_filename is None:\n      env_filename = os.path.basename(filename)\n    if self._chroot.get(\"executable\"):\n      raise self.InvalidExecutableSpecification(\n          \"Setting executable on a PEXBuilder that already has one!\")\n    self._copy_or_link(filename, env_filename, \"executable\")\n    entry_point = env_filename\n    entry_point = entry_point.replace(os.path.sep, '.')\n    self._pex_info.entry_point = entry_point.rpartition('.')[0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_script(self, script):\n\n    # check if 'script' is a console_script\n    dist, entry_point = get_entry_point_from_console_script(script, self._distributions)\n    if entry_point:\n      self.set_entry_point(entry_point)\n      TRACER.log('Set entrypoint to console_script %r in %r' % (entry_point, dist))\n      return\n\n    # check if 'script' is an ordinary script\n    dist, _, _ = get_script_from_distributions(script, self._distributions)\n    if dist:\n      if self._pex_info.entry_point:\n        raise self.InvalidExecutableSpecification('Cannot set both entry point and script of PEX!')\n      self._pex_info.script = script\n      TRACER.log('Set entrypoint to script %r in %r' % (script, dist))\n      return\n\n    raise self.InvalidExecutableSpecification(\n        'Could not find script %r in any distribution %s within PEX!' % (\n            script, ', '.join(str(d) for d in self._distributions)))", "response": "Sets the entry point of this PEX environment based upon a distribution script."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_installer_paths(self, base):\n    return {\n      'purelib': base,\n      'headers': os.path.join(base, 'headers'),\n      'scripts': os.path.join(base, 'bin'),\n      'platlib': base,\n      'data': base\n    }", "response": "Set up an overrides dict for WheelFile. install that installs the contents\n    of a wheel into its own base in the pex dependencies cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_distribution(self, dist, dist_name=None):\n    self._ensure_unfrozen('Adding a distribution')\n    dist_name = dist_name or os.path.basename(dist.location)\n    self._distributions.add(dist)\n\n    if os.path.isdir(dist.location):\n      dist_hash = self._add_dist_dir(dist.location, dist_name)\n    else:\n      dist_hash = self._add_dist_zip(dist.location, dist_name)\n\n    # add dependency key so that it can rapidly be retrieved from cache\n    self._pex_info.add_distribution(dist_name, dist_hash)", "response": "Adds a distribution to the environment."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a distribution by its location on disk.", "response": "def add_dist_location(self, dist, name=None):\n    \"\"\"Add a distribution by its location on disk.\n\n    :param dist: The path to the distribution to add.\n    :keyword name: (optional) The name of the distribution, should the dist directory alone be\n      ambiguous.  Packages contained within site-packages directories may require specifying\n      ``name``.\n    :raises PEXBuilder.InvalidDistribution: When the path does not contain a matching distribution.\n\n    PEX supports packed and unpacked .whl and .egg distributions, as well as any distribution\n    supported by setuptools/pkg_resources.\n    \"\"\"\n    self._ensure_unfrozen('Adding a distribution')\n    bdist = DistributionHelper.distribution_from_path(dist)\n    if bdist is None:\n      raise self.InvalidDistribution('Could not find distribution at %s' % dist)\n    self.add_distribution(bdist)\n    self.add_requirement(bdist.as_requirement())"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef freeze(self, bytecode_compile=True):\n    self._ensure_unfrozen('Freezing the environment')\n    self._prepare_code_hash()\n    self._prepare_manifest()\n    self._prepare_bootstrap()\n    self._prepare_main()\n    if bytecode_compile:\n      self._precompile_source()\n    self._frozen = True", "response": "Freeze the PEX.\n\n    :param bytecode_compile: If True, precompile .py files into .pyc files when freezing code.\n\n    Freezing the PEX writes all the necessary metadata and environment bootstrapping code.  It may\n    only be called once and renders the PEXBuilder immutable."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef build(self, filename, bytecode_compile=True):\n    if not self._frozen:\n      self.freeze(bytecode_compile=bytecode_compile)\n    try:\n      os.unlink(filename + '~')\n      self._logger.warn('Previous binary unexpectedly exists, cleaning: %s' % (filename + '~'))\n    except OSError:\n      # The expectation is that the file does not exist, so continue\n      pass\n    if os.path.dirname(filename):\n      safe_mkdir(os.path.dirname(filename))\n    with open(filename + '~', 'ab') as pexfile:\n      assert os.path.getsize(pexfile.name) == 0\n      pexfile.write(to_bytes('%s\\n' % self._shebang))\n    self._chroot.zip(filename + '~', mode='a')\n    if os.path.exists(filename):\n      os.unlink(filename)\n    os.rename(filename + '~', filename)\n    chmod_plus_x(filename)", "response": "Package the PEX into a zipfile."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmerges a new PEX_PATH definition into the existing one.", "response": "def merge_pex_path(self, pex_path):\n    \"\"\"Merges a new PEX_PATH definition into the existing one (if any).\n    :param string pex_path: The PEX_PATH to merge.\n    \"\"\"\n    if not pex_path:\n      return\n    self.pex_path = ':'.join(merge_split(self.pex_path, pex_path))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngive an iterable of resolvable - like objects return a list of Resolvable objects.", "response": "def resolvables_from_iterable(iterable, builder, interpreter=None):\n  \"\"\"Given an iterable of resolvable-like objects, return list of Resolvable objects.\n\n  :param iterable: An iterable of :class:`Resolvable`, :class:`Requirement`, :class:`Package`,\n      or `str` to map into an iterable of :class:`Resolvable` objects.\n  :returns: A list of :class:`Resolvable` objects.\n  \"\"\"\n\n  def translate(obj):\n    if isinstance(obj, Resolvable):\n      return obj\n    elif isinstance(obj, Requirement):\n      return ResolvableRequirement(obj, builder.build(obj.key))\n    elif isinstance(obj, Package):\n      return ResolvablePackage(obj, builder.build(obj.name))\n    elif isinstance(obj, compatibility_string):\n      return Resolvable.get(obj, builder, interpreter=interpreter)\n    else:\n      raise ValueError('Do not know how to resolve %s' % type(obj))\n  return list(map(translate, iterable))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get(cls, resolvable_string, options_builder=None, interpreter=None):\n    options_builder = options_builder or ResolverOptionsBuilder()\n    for resolvable_impl in cls._REGISTRY:\n      try:\n        return resolvable_impl.from_string(resolvable_string,\n                                           options_builder,\n                                           interpreter=interpreter)\n      except cls.InvalidRequirement:\n        continue\n    raise cls.InvalidRequirement('Unknown requirement type: %s' % resolvable_string)", "response": "Get a : class:`Resolvable` from a string."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngive some filters yield any interpreter that matches at least one of them.", "response": "def matched_interpreters(interpreters, constraints):\n  \"\"\"Given some filters, yield any interpreter that matches at least one of them.\n\n  :param interpreters: a list of PythonInterpreter objects for filtering\n  :param constraints: A sequence of strings that constrain the interpreter compatibility for this\n    pex. Each string uses the Requirement-style format, e.g. 'CPython>=3' or '>=2.7,<3' for\n    requirements agnostic to interpreter class. Multiple requirement strings may be combined\n    into a list to OR the constraints, such as ['CPython>=2.7,<3', 'CPython>=3.4'].\n  :return interpreter: returns a generator that yields compatible interpreters\n  \"\"\"\n  for interpreter in interpreters:\n    if any(interpreter.identity.matches(filt) for filt in constraints):\n      TRACER.log(\"Constraints on interpreters: %s, Matching Interpreter: %s\"\n                 % (constraints, interpreter.binary), V=3)\n      yield interpreter"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nuse parse_version from pkg_resources or distutils as available.", "response": "def parse_version(version):\n    \"\"\"Use parse_version from pkg_resources or distutils as available.\"\"\"\n    global parse_version\n    try:\n        if \"__PEX_UNVENDORED__\" in __import__(\"os\").environ:\n          from pkg_resources import parse_version  # vendor:skip\n        else:\n          from pex.third_party.pkg_resources import parse_version\n\n    except ImportError:\n        from distutils.version import LooseVersion as parse_version\n    return parse_version(version)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nyield the set of tags that are compatible with the same version as the current wheel file.", "response": "def tags(self):\n        \"\"\"A wheel file is compatible with the Cartesian product of the\n        period-delimited tags in its filename.\n        To choose a wheel file among several candidates having the same\n        distribution version 'ver', an installer ranks each triple of\n        (pyver, abi, plat) that its Python installation can run, sorting\n        the wheels by the best-ranked tag it supports and then by their\n        arity which is just len(list(compatibility_tags)).\n        \"\"\"\n        tags = self.parsed_filename.groupdict()\n        for pyver in tags['pyver'].split('.'):\n            for abi in tags['abi'].split('.'):\n                for plat in tags['plat'].split('.'):\n                    yield (pyver, abi, plat)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef compatibility_rank(self, supported):\n        preferences = []\n        for tag in self.compatibility_tags:\n            try:\n                preferences.append(supported.index(tag))\n            # Tag not present\n            except ValueError:\n                pass\n        if len(preferences):\n            return (min(preferences), self.arity)\n        return (_big_number, 0)", "response": "Rank the wheel against the supported tags. Smaller ranks are more\n            compatible!"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef open(self, name_or_info, mode=\"r\", pwd=None):\n        # A non-monkey-patched version would contain most of zipfile.py\n        ef = super(VerifyingZipFile, self).open(name_or_info, mode, pwd)\n        if isinstance(name_or_info, zipfile.ZipInfo):\n            name = name_or_info.filename\n        else:\n            name = name_or_info\n\n        if name in self._expected_hashes and self._expected_hashes[name] is not None:\n            expected_hash = self._expected_hashes[name]\n            try:\n                _update_crc_orig = ef._update_crc\n            except AttributeError:\n                warnings.warn('Need ZipExtFile._update_crc to implement '\n                              'file hash verification (in Python >= 2.7)')\n                return ef\n            running_hash = self._hash_algorithm()\n            if hasattr(ef, '_eof'):  # py33\n                def _update_crc(data):\n                    _update_crc_orig(data)\n                    running_hash.update(data)\n                    if ef._eof and running_hash.digest() != expected_hash:\n                        raise BadWheelFile(\"Bad hash for file %r\" % ef.name)\n            else:\n                def _update_crc(data, eof=None):\n                    _update_crc_orig(data, eof=eof)\n                    running_hash.update(data)\n                    if eof and running_hash.digest() != expected_hash:\n                        raise BadWheelFile(\"Bad hash for file %r\" % ef.name)\n            ef._update_crc = _update_crc\n        elif self.strict and name not in self._expected_hashes:\n            raise BadWheelFile(\"No expected hash for file %r\" % ef.name)\n        return ef", "response": "Return file - like object for name."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntruncate the last entry off this ZIP archive.", "response": "def pop(self):\n        \"\"\"Truncate the last file off this zipfile.\n        Assumes infolist() is in the same order as the files (true for\n        ordinary zip files created by Python)\"\"\"\n        if not self.fp:\n            raise RuntimeError(\n                  \"Attempt to pop from ZIP archive that was already closed\")\n        last = self.infolist().pop()\n        del self.NameToInfo[last.filename]\n        self.fp.seek(last.header_offset, os.SEEK_SET)\n        self.fp.truncate()\n        self._didModify = True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngiving a Requirement check if this interpreter matches.", "response": "def matches(self, requirement):\n    \"\"\"Given a Requirement, check if this interpreter matches.\"\"\"\n    try:\n      requirement = self.parse_requirement(requirement, self._interpreter)\n    except ValueError as e:\n      raise self.UnknownRequirement(str(e))\n    return self.distribution in requirement"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pkg_resources_env(self, platform_str):\n    os_name = ''\n    platform_machine = ''\n    platform_release = ''\n    platform_system = ''\n    platform_version = ''\n    sys_platform = ''\n\n    if 'win' in platform_str:\n      os_name = 'nt'\n      platform_machine = 'AMD64' if '64' in platform_str else 'x86'\n      platform_system = 'Windows'\n      sys_platform = 'win32'\n    elif 'linux' in platform_str:\n      os_name = 'posix'\n      platform_machine = 'x86_64' if '64' in platform_str else 'i686'\n      platform_system = 'Linux'\n      sys_platform = 'linux2' if self._version[0] == 2 else 'linux'\n    elif 'macosx' in platform_str:\n      os_name = 'posix'\n      platform_str = platform_str.replace('.', '_')\n      platform_machine = platform_str.split('_', 3)[-1]\n      # Darwin version are macOS version + 4\n      platform_release = '{}.0.0'.format(int(platform_str.split('_')[2]) + 4)\n      platform_system = 'Darwin'\n      platform_version = 'Darwin Kernel Version {}'.format(platform_release)\n      sys_platform = 'darwin'\n\n    return {\n      'implementation_name': self.interpreter.lower(),\n      'implementation_version': self.version_str,\n      'os_name': os_name,\n      'platform_machine': platform_machine,\n      'platform_release': platform_release,\n      'platform_system': platform_system,\n      'platform_version': platform_version,\n      'python_full_version': self.version_str,\n      'platform_python_implementation': self.interpreter,\n      'python_version': self.version_str[:3],\n      'sys_platform': sys_platform,\n    }", "response": "Returns a dict that can be used in place of packaging. default_environment."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_env(cls, hashbang):\n    paths = os.getenv('PATH', '').split(':')\n    for path in paths:\n      for fn in cls.expand_path(path):\n        basefile = os.path.basename(fn)\n        if hashbang == basefile:\n          try:\n            return cls.from_binary(fn)\n          except Exception as e:\n            TRACER.log('Could not identify %s: %s' % (fn, e))", "response": "Resolve a PythonInterpreter as / usr / bin / env would."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating an interpreter from the given binary.", "response": "def from_binary(cls, binary):\n    \"\"\"Create an interpreter from the given `binary`.\n\n    :param str binary: The path to the python interpreter binary.\n    :return: an interpreter created from the given `binary` with only the specified\n             extras.\n    :rtype: :class:`PythonInterpreter`\n    \"\"\"\n    if binary not in cls.CACHE:\n      if binary == sys.executable:\n        cls.CACHE[binary] = cls._from_binary_internal()\n      else:\n        cls.CACHE[binary] = cls._from_binary_external(binary)\n    return cls.CACHE[binary]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef find(cls, paths):\n    pythons = []\n    for path in paths:\n      for fn in cls.expand_path(path):\n        basefile = os.path.basename(fn)\n        if cls._matches_binary_name(basefile):\n          try:\n            pythons.append(cls.from_binary(fn))\n          except Exception as e:\n            TRACER.log('Could not identify %s: %s' % (fn, e))\n            continue\n    return pythons", "response": "Given a list of files or directories try to detect python interpreters amongst them."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef filter(cls, pythons):\n    good = []\n\n    MAJOR, MINOR, SUBMINOR = range(3)\n    def version_filter(version):\n      return (version[MAJOR] == 2 and version[MINOR] >= 7 or\n              version[MAJOR] == 3 and version[MINOR] >= 4)\n\n    all_versions = set(interpreter.identity.version for interpreter in pythons)\n    good_versions = filter(version_filter, all_versions)\n\n    for version in good_versions:\n      # For each candidate, use the latest version we find on the filesystem.\n      candidates = defaultdict(list)\n      for interp in pythons:\n        if interp.identity.version == version:\n          candidates[interp.identity.interpreter].append(interp)\n      for interp_class in candidates:\n        candidates[interp_class].sort(\n            key=lambda interp: os.path.getmtime(interp.binary), reverse=True)\n        good.append(candidates[interp_class].pop(0))\n\n    return good", "response": "Given a map of python interpreters filter out duplicate versions and versions we would prefer not to use."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef open_process(cls, cmd, **kwargs):\n    assert len(cmd) > 0, 'cannot execute an empty command!'\n\n    try:\n      return subprocess.Popen(cmd, **kwargs)\n    except (IOError, OSError) as e:\n      if e.errno == errno.ENOENT:\n        raise cls.ExecutableNotFound(cmd, e)\n      else:\n        raise cls.ExecutionError(repr(e), cmd, e)", "response": "Opens a process via subprocess. Popen."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nexecutes a command via subprocess. Popen and returns the stdio.", "response": "def execute(cls, cmd, stdin_payload=None, **kwargs):\n    \"\"\"Execute a command via subprocess.Popen and returns the stdio.\n\n    :param string|list cmd: A list or string representing the command to run.\n    :param string stdin_payload: A string representing the stdin payload, if any, to send.\n    :param **kwargs: Additional kwargs to pass through to subprocess.Popen.\n    :return: A tuple of strings representing (stdout, stderr), pre-decoded for utf-8.\n    :raises: `Executor.ExecutableNotFound` when the executable requested to run does not exist.\n             `Executor.NonZeroExit` when the execution fails with a non-zero exit code.\n    \"\"\"\n    process = cls.open_process(cmd=cmd,\n                               stdin=subprocess.PIPE,\n                               stdout=subprocess.PIPE,\n                               stderr=subprocess.PIPE,\n                               **kwargs)\n    stdout_raw, stderr_raw = process.communicate(input=stdin_payload)\n    # N.B. In cases where `stdout` or `stderr` is passed as parameters, these can be None.\n    stdout = stdout_raw.decode('utf-8') if stdout_raw is not None else stdout_raw\n    stderr = stderr_raw.decode('utf-8') if stderr_raw is not None else stderr_raw\n\n    if process.returncode != 0:\n      raise cls.NonZeroExit(cmd, process.returncode, stdout, stderr)\n\n    return stdout, stderr"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse(self):\n        for section_name, section_options in self.sections.items():\n\n            method_postfix = ''\n            if section_name:  # [section.option] variant\n                method_postfix = '_%s' % section_name\n\n            section_parser_method = getattr(\n                self,\n                # Dots in section names are tranlsated into dunderscores.\n                ('parse_section%s' % method_postfix).replace('.', '__'),\n                None)\n\n            if section_parser_method is None:\n                raise DistutilsOptionError(\n                    'Unsupported distribution option section: [%s.%s]' % (\n                        self.section_prefix, section_name))\n\n            section_parser_method(section_options)", "response": "Parses configuration file items from one or more related sections."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_supported_for_any_abi(version=None, platform=None, impl=None, force_manylinux=False):\n  unique_tags = {\n    tag for abi in _gen_all_abis(impl, version)\n    for tag in _get_supported(version=version,\n                              platform=platform,\n                              impl=impl,\n                              abi=abi,\n                              force_manylinux=force_manylinux)\n  }\n  return list(unique_tags)", "response": "Generates supported tags for unspecified ABI types to support more intuitive cross - platform\n     resolution."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef supported_tags(self, interpreter=None, force_manylinux=True):\n    if interpreter and not self.is_extended:\n      # N.B. If we don't get an extended platform specifier, we generate\n      # all possible ABI permutations to mimic earlier pex version\n      # behavior and make cross-platform resolution more intuitive.\n      return _get_supported_for_any_abi(\n        platform=self.platform,\n        impl=interpreter.identity.abbr_impl,\n        version=interpreter.identity.impl_ver,\n        force_manylinux=force_manylinux\n      )\n    else:\n      return _get_supported(\n        platform=self.platform,\n        impl=self.impl,\n        version=self.version,\n        abi=self.abi,\n        force_manylinux=force_manylinux\n      )", "response": "Returns a list of supported PEP425 tags for the current platform."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntranslating a SourcePackage to a binary distribution.", "response": "def translate(self, package, into=None):\n    \"\"\"From a SourcePackage, translate to a binary distribution.\"\"\"\n    if not isinstance(package, SourcePackage):\n      return None\n    if not package.local:\n      raise ValueError('SourceTranslator cannot translate remote packages.')\n\n    installer = None\n    version = self._interpreter.version\n    unpack_path = Archiver.unpack(package.local_path)\n    into = into or safe_mkdtemp()\n\n    try:\n      if self._use_2to3 and version >= (3,):\n        with TRACER.timed('Translating 2->3 %s' % package.name):\n          self.run_2to3(unpack_path)\n      installer = self._installer_impl(unpack_path, interpreter=self._interpreter)\n      with TRACER.timed('Packaging %s' % package.name):\n        try:\n          dist_path = installer.bdist()\n        except self._installer_impl.InstallFailure as e:\n          TRACER.log('Failed to install package at %s: %s' % (unpack_path, e))\n          return None\n        target_path = os.path.join(into, os.path.basename(dist_path))\n        safe_copy(dist_path, target_path)\n        target_package = Package.from_href(target_path)\n        if not target_package:\n          TRACER.log('Target path %s does not look like a Package.' % target_path)\n          return None\n        if not target_package.compatible(self._supported_tags):\n          TRACER.log('Target package %s is not compatible with %s' % (\n              target_package, self._supported_tags))\n          return None\n        return DistributionHelper.distribution_from_path(target_path)\n    except Exception as e:\n      TRACER.log('Failed to translate %s' % package)\n      TRACER.log(traceback.format_exc())\n    finally:\n      if installer:\n        installer.cleanup()\n      if unpack_path:\n        safe_rmtree(unpack_path)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntranslate a binary package to a local binary distribution.", "response": "def translate(self, package, into=None):\n    \"\"\"From a binary package, translate to a local binary distribution.\"\"\"\n    if not package.local:\n      raise ValueError('BinaryTranslator cannot translate remote packages.')\n    if not isinstance(package, self._package_type):\n      return None\n    if not package.compatible(self._supported_tags):\n      TRACER.log('Target package %s is not compatible with %s' % (\n          package, self._supported_tags))\n      return None\n    into = into or safe_mkdtemp()\n    target_path = os.path.join(into, package.filename)\n    safe_copy(package.local_path, target_path)\n    return DistributionHelper.distribution_from_path(target_path)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn an interpreter configured with vendored distributions as extras.", "response": "def setup_interpreter(distributions, interpreter=None):\n  \"\"\"Return an interpreter configured with vendored distributions as extras.\n\n  Any distributions that are present in the vendored set will be added to the interpreter as extras.\n\n  :param distributions: The names of distributions to setup the interpreter with.\n  :type distributions: list of str\n  :param interpreter: An optional interpreter to configure. If ``None``, the current interpreter is\n                      used.\n  :type interpreter: :class:`pex.interpreter.PythonInterpreter`\n  :return: An bare interpreter configured with vendored extras.\n  :rtype: :class:`pex.interpreter.PythonInterpreter`\n  \"\"\"\n  from pex.interpreter import PythonInterpreter\n\n  interpreter = interpreter or PythonInterpreter.get()\n  for dist in _vendored_dists(OrderedSet(distributions)):\n    interpreter = interpreter.with_extra(dist.key, dist.version, dist.location)\n  return interpreter"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef vendor_runtime(chroot, dest_basedir, label, root_module_names):\n  vendor_module_names = {root_module_name: False for root_module_name in root_module_names}\n  for spec in iter_vendor_specs():\n    for root, dirs, files in os.walk(spec.target_dir):\n      if root == spec.target_dir:\n        dirs[:] = [pkg_name for pkg_name in dirs if pkg_name in vendor_module_names]\n        files[:] = [mod_name for mod_name in files if mod_name[:-3] in vendor_module_names]\n        vendored_names = dirs + files\n        if vendored_names:\n          pkg_path = ''\n          for pkg in spec.relpath.split(os.sep):\n            pkg_path = os.path.join(pkg_path, pkg)\n            pkg_file = os.path.join(pkg_path, '__init__.py')\n            src = os.path.join(VendorSpec.ROOT, pkg_file)\n            dest = os.path.join(dest_basedir, pkg_file)\n            if os.path.exists(src):\n              chroot.copy(src, dest, label)\n            else:\n              # We delete `pex/vendor/_vendored/<dist>/__init__.py` when isolating third_party.\n              chroot.touch(dest, label)\n          for name in vendored_names:\n            vendor_module_names[name] = True\n            TRACER.log('Vendoring {} from {} @ {}'.format(name, spec, spec.target_dir), V=3)\n\n      for filename in files:\n        if not filename.endswith('.pyc'):  # Sources and data only.\n          src = os.path.join(root, filename)\n          dest = os.path.join(dest_basedir, spec.relpath, os.path.relpath(src, spec.target_dir))\n          chroot.copy(src, dest, label)\n\n  if not all(vendor_module_names.values()):\n    raise ValueError('Failed to extract {module_names} from:\\n\\t{specs}'.format(\n      module_names=', '.join(module\n                             for module, written in vendor_module_names.items() if not written),\n      specs='\\n\\t'.join('{} @ {}'.format(spec, spec.target_dir) for spec in iter_vendor_specs())))", "response": "Adds vendored code fileset to a chroot."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_packages(self):\n    for index, _ in enumerate(self._subpath_components):\n      relpath = _PACKAGE_COMPONENTS + self._subpath_components[:index + 1] + ['__init__.py']\n      touch(os.path.join(self.ROOT, *relpath))", "response": "Create missing packages in the current directory."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef unescape(s):\n  s = s.replace(\"&lt;\", \"<\")\n  s = s.replace(\"&gt;\", \">\")\n  # this has to be last:\n  s = s.replace(\"&amp;\", \"&\")\n  return s", "response": "Unescapes html. Taken from Python s Moin escapingHtml."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn rel = links that should be scraped skipping obviously data links", "response": "def rel_links(cls, page):\n    \"\"\"return rel= links that should be scraped, skipping obviously data links.\"\"\"\n    for match in cls.REL_RE.finditer(page):\n      href, rel = match.group(0), match.group(1)\n      if rel not in cls.REL_TYPES:\n        continue\n      href_match = cls.HREF_RE.search(href)\n      if href_match:\n        href = cls.href_match_to_url(href_match)\n        parsed_href = urlparse(href)\n        if any(parsed_href.path.endswith(ext) for ext in cls.REL_SKIP_EXTENSIONS):\n          continue\n        yield href"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn all links on a page including potentially rel = links", "response": "def links(cls, page):\n    \"\"\"return all links on a page, including potentially rel= links.\"\"\"\n    for match in cls.HREF_RE.finditer(page):\n      yield cls.href_match_to_url(match)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\narchiving all files under base_dir and name it like base_name.", "response": "def archive_wheelfile(base_name, base_dir):\n    \"\"\"Archive all files under `base_dir` in a whl file and name it like\n    `base_name`.\n    \"\"\"\n    olddir = os.path.abspath(os.curdir)\n    base_name = os.path.abspath(base_name)\n    try:\n        os.chdir(base_dir)\n        return make_wheelfile_inner(base_name)\n    finally:\n        os.chdir(olddir)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef make_wheelfile_inner(base_name, base_dir='.'):\n\n    zip_filename = base_name + \".whl\"\n\n    log.info(\"creating '%s' and adding '%s' to it\", zip_filename, base_dir)\n\n    # Some applications need reproducible .whl files, but they can't do this\n    # without forcing the timestamp of the individual ZipInfo objects.  See\n    # issue #143.\n    timestamp = os.environ.get('SOURCE_DATE_EPOCH')\n    if timestamp is None:\n        date_time = None\n    else:\n        date_time = time.gmtime(int(timestamp))[0:6]\n\n    score = {'WHEEL': 1, 'METADATA': 2, 'RECORD': 3}\n\n    def writefile(path, date_time):\n        st = os.stat(path)\n        if date_time is None:\n            mtime = time.gmtime(st.st_mtime)\n            date_time = mtime[0:6]\n        zinfo = zipfile.ZipInfo(path, date_time)\n        zinfo.external_attr = st.st_mode << 16\n        zinfo.compress_type = zipfile.ZIP_DEFLATED\n        with open(path, 'rb') as fp:\n            zip.writestr(zinfo, fp.read())\n        log.info(\"adding '%s'\" % path)\n\n    with zipfile.ZipFile(zip_filename, \"w\", compression=zipfile.ZIP_DEFLATED,\n                         allowZip64=True) as zip:\n        deferred = []\n        for dirpath, dirnames, filenames in os.walk(base_dir):\n            # Sort the directory names so that `os.walk` will walk them in a\n            # defined order on the next iteration.\n            dirnames.sort()\n            for name in sorted(filenames):\n                path = os.path.normpath(os.path.join(dirpath, name))\n\n                if os.path.isfile(path):\n                    if dirpath.endswith('.dist-info'):\n                        deferred.append((score.get(name, 0), path))\n                    else:\n                        writefile(path, date_time)\n\n        deferred.sort()\n        for score, path in deferred:\n            writefile(path, date_time)\n\n    return zip_filename", "response": "Create a whl file from all the files under base_dir and add it to the archive."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef wrap(cls, url):\n    if isinstance(url, cls):\n      return url\n    elif isinstance(url, compatible_string):\n      return cls(url)\n    else:\n      raise ValueError('url must be either a string or Link.')", "response": "Given a string - like or Link object return a Link object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef wrap_iterable(cls, url_or_urls):\n    try:\n      return [cls.wrap(url_or_urls)]\n    except ValueError:\n      pass\n    if isinstance(url_or_urls, Iterable):\n      return [cls.wrap(url) for url in url_or_urls]\n    raise ValueError('url_or_urls must be string/Link or iterable of strings/Links')", "response": "Given a string or iterable of strings or Links return an iterable of Link objects."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a Link wrapping the local filename.", "response": "def from_filename(cls, filename):\n    \"\"\"Return a :class:`Link` wrapping the local filename.\"\"\"\n    result = cls._FROM_FILENAME_CACHE.get(filename)\n    if result is None:\n      result = cls(cls._normalize(filename))\n      cls._FROM_FILENAME_CACHE.store(filename, result)\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef join(self, href):\n    return self.wrap(urlparse.urljoin(self.url, href))", "response": "Given a string - like path relative to this link return the absolute url."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of resolvable objects from a requirements. txt file.", "response": "def requirements_from_file(filename, builder=None, interpreter=None):\n  \"\"\"Return a list of :class:`Resolvable` objects from a requirements.txt file.\n\n  :param filename: The filename of the requirements.txt\n  :keyword builder: (optional) The ResolverOptionsBuilder from which we should inherit\n    default resolver options.\n  :type builder: :class:`ResolverOptionsBuilder`\n  \"\"\"\n\n  relpath = os.path.dirname(filename)\n  with open(filename, 'r') as fp:\n    return requirements_from_lines(fp.readlines(),\n                                   builder=builder,\n                                   relpath=relpath,\n                                   interpreter=interpreter)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nopening a urllib2 request handling HTTP authentication", "response": "def open_with_auth(url, opener=urllib.request.urlopen):\n    \"\"\"Open a urllib2 request, handling HTTP authentication\"\"\"\n\n    scheme, netloc, path, params, query, frag = urllib.parse.urlparse(url)\n\n    # Double scheme does not raise on Mac OS X as revealed by a\n    # failing test. We would expect \"nonnumeric port\". Refs #20.\n    if netloc.endswith(':'):\n        raise http_client.InvalidURL(\"nonnumeric port: ''\")\n\n    if scheme in ('http', 'https'):\n        auth, host = urllib.parse.splituser(netloc)\n    else:\n        auth = None\n\n    if not auth:\n        cred = PyPIConfig().find_credential(url)\n        if cred:\n            auth = str(cred)\n            info = cred.username, url\n            log.info('Authenticating as %s for %s (from .pypirc)', *info)\n\n    if auth:\n        auth = \"Basic \" + _encode_auth(auth)\n        parts = scheme, host, path, params, query, frag\n        new_url = urllib.parse.urlunparse(parts)\n        request = urllib.request.Request(new_url)\n        request.add_header(\"Authorization\", auth)\n    else:\n        request = urllib.request.Request(url)\n\n    request.add_header('User-Agent', user_agent)\n    fp = opener(request)\n\n    if auth:\n        # Put authentication info back into request URL if same host,\n        # so that links found on the page will work\n        s2, h2, path2, param2, query2, frag2 = urllib.parse.urlparse(fp.url)\n        if s2 == scheme and h2 == host:\n            parts = s2, netloc, path2, param2, query2, frag2\n            fp.url = urllib.parse.urlunparse(parts)\n\n    return fp"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_abbr_impl():\n    impl = platform.python_implementation()\n    if impl == 'PyPy':\n        return 'pp'\n    elif impl == 'Jython':\n        return 'jy'\n    elif impl == 'IronPython':\n        return 'ip'\n    elif impl == 'CPython':\n        return 'cp'\n\n    raise LookupError('Unknown Python implementation: ' + impl)", "response": "Return abbreviated implementation name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the flag value of the needed config variable.", "response": "def get_flag(var, fallback, expected=True, warn=True):\n    \"\"\"Use a fallback method for determining SOABI flags if the needed config\n    var is unset or unavailable.\"\"\"\n    val = get_config_var(var)\n    if val is None:\n        if warn:\n            warnings.warn(\"Config variable '{0}' is unset, Python ABI tag may \"\n                          \"be incorrect\".format(var), RuntimeWarning, 2)\n        return fallback()\n    return val == expected"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_platform():\n    # XXX remove distutils dependency\n    result = distutils.util.get_platform().replace('.', '_').replace('-', '_')\n    if result == \"linux_x86_64\" and sys.maxsize == 2147483647:\n        # pip pull request #3497\n        result = \"linux_i686\"\n    return result", "response": "Return our platform name win32 linux_x86_64 linux_i686"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_supported(versions=None, supplied_platform=None):\n    supported = []\n\n    # Versions must be given with respect to the preference\n    if versions is None:\n        versions = []\n        version_info = get_impl_version_info()\n        major = version_info[:-1]\n        # Support all previous minor Python versions.\n        for minor in range(version_info[-1], -1, -1):\n            versions.append(''.join(map(str, major + (minor,))))\n\n    impl = get_abbr_impl()\n\n    abis = []\n\n    abi = get_abi_tag()\n    if abi:\n        abis[0:0] = [abi]\n\n    abi3s = set()\n    import imp\n    for suffix in imp.get_suffixes():\n        if suffix[0].startswith('.abi'):\n            abi3s.add(suffix[0].split('.', 2)[1])\n\n    abis.extend(sorted(list(abi3s)))\n\n    abis.append('none')\n\n    platforms = []\n    if supplied_platform:\n        platforms.append(supplied_platform)\n    platforms.append(get_platform())\n\n    # Current version, current API (built specifically for our Python):\n    for abi in abis:\n        for arch in platforms:\n            supported.append(('%s%s' % (impl, versions[0]), abi, arch))\n\n    # abi3 modules compatible with older version of Python\n    for version in versions[1:]:\n        # abi3 was introduced in Python 3.2\n        if version in ('31', '30'):\n            break\n        for abi in abi3s:   # empty set if not Python 3\n            for arch in platforms:\n                supported.append((\"%s%s\" % (impl, version), abi, arch))\n\n    # No abi / arch, but requires our implementation:\n    for i, version in enumerate(versions):\n        supported.append(('%s%s' % (impl, version), 'none', 'any'))\n        if i == 0:\n            # Tagged specifically as being cross-version compatible\n            # (with just the major version specified)\n            supported.append(('%s%s' % (impl, versions[0][0]), 'none', 'any'))\n\n    # Major Python version + platform; e.g. binaries not using the Python API\n    supported.append(('py%s' % (versions[0][0]), 'none', arch))\n\n    # No abi / arch, generic Python\n    for i, version in enumerate(versions):\n        supported.append(('py%s' % (version,), 'none', 'any'))\n        if i == 0:\n            supported.append(('py%s' % (version[0]), 'none', 'any'))\n\n    return supported", "response": "Return a list of supported tags for each version specified in versions."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef open_zip(path, *args, **kwargs):\n  with contextlib.closing(PermPreservingZipFile(path, *args, **kwargs)) as zip:\n    yield zip", "response": "A contextmanager for zip files. Passes through positional and kwargs to zipfile. ZipFile."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef safe_mkdir(directory, clean=False):\n  if clean:\n    safe_rmtree(directory)\n  try:\n    os.makedirs(directory)\n  except OSError as e:\n    if e.errno != errno.EEXIST:\n      raise", "response": "Safely create a directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef safe_open(filename, *args, **kwargs):\n  safe_mkdir(os.path.dirname(filename))\n  return open(filename, *args, **kwargs)", "response": "Safely open a file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef safe_delete(filename):\n  try:\n    os.unlink(filename)\n  except OSError as e:\n    if e.errno != errno.ENOENT:\n      raise", "response": "Delete a file safely."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndeletes a directory if it s present.", "response": "def safe_rmtree(directory):\n  \"\"\"Delete a directory if it's present. If it's not present, no-op.\"\"\"\n  if os.path.exists(directory):\n    shutil.rmtree(directory, True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nrenames src to dest using os. rename.", "response": "def rename_if_empty(src, dest, allowable_errors=(errno.EEXIST, errno.ENOTEMPTY)):\n  \"\"\"Rename `src` to `dest` using `os.rename()`.\n\n  If an `OSError` with errno in `allowable_errors` is encountered during the rename, the `dest`\n  dir is left unchanged and the `src` directory will simply be removed.\n  \"\"\"\n  try:\n    os.rename(src, dest)\n  except OSError as e:\n    if e.errno in allowable_errors:\n      safe_rmtree(src)\n    else:\n      raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef touch(file, times=None):\n  if times:\n    if len(times) > 2:\n      raise ValueError('times must either be a tuple of (atime, mtime) or else a single time value '\n                       'to use for both.')\n\n    if len(times) == 1:\n      times = (times, times)\n\n  with safe_open(file, 'a'):\n    os.utime(file, times)", "response": "Equivalent of unix touch path."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nclone this chroot. :keyword into: (optional) An optional destination directory to clone the Chroot into. If not specified, a temporary directory will be created. .. versionchanged:: 0.8 The temporary directory created when ``into`` is not specified is now garbage collected on interpreter exit.", "response": "def clone(self, into=None):\n    \"\"\"Clone this chroot.\n\n    :keyword into: (optional) An optional destination directory to clone the\n      Chroot into.  If not specified, a temporary directory will be created.\n\n    .. versionchanged:: 0.8\n      The temporary directory created when ``into`` is not specified is now garbage collected on\n      interpreter exit.\n    \"\"\"\n    into = into or safe_mkdtemp()\n    new_chroot = Chroot(into)\n    for label, fileset in self.filesets.items():\n      for fn in fileset:\n        new_chroot.link(os.path.join(self.chroot, fn), fn, label=label)\n    return new_chroot"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef copy(self, src, dst, label=None):\n    dst = self._normalize(dst)\n    self._tag(dst, label)\n    self._ensure_parent(dst)\n    shutil.copy(src, os.path.join(self.chroot, dst))", "response": "Copy src to dst."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef link(self, src, dst, label=None):\n    dst = self._normalize(dst)\n    self._tag(dst, label)\n    self._ensure_parent(dst)\n    abs_src = src\n    abs_dst = os.path.join(self.chroot, dst)\n    safe_copy(abs_src, abs_dst, overwrite=False)", "response": "Hard link src to dst with optional label."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrite data to dst with optional label.", "response": "def write(self, data, dst, label=None, mode='wb'):\n    \"\"\"Write data to ``chroot/dst`` with optional label.\n\n    Has similar exceptional cases as ``Chroot.copy``\n    \"\"\"\n    dst = self._normalize(dst)\n    self._tag(dst, label)\n    self._ensure_parent(dst)\n    with open(os.path.join(self.chroot, dst), mode) as wp:\n      wp.write(data)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nperforming touch on dst with optional label.", "response": "def touch(self, dst, label=None):\n    \"\"\"Perform 'touch' on ``chroot/dst`` with optional label.\n\n    Has similar exceptional cases as Chroot.copy\n    \"\"\"\n    dst = self._normalize(dst)\n    self._tag(dst, label)\n    touch(os.path.join(self.chroot, dst))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting all files in the chroot.", "response": "def files(self):\n    \"\"\"Get all files in the chroot.\"\"\"\n    all_files = set()\n    for label in self.filesets:\n      all_files.update(self.filesets[label])\n    return all_files"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngive an iterable of directory paths yield paths to all. pth files within.", "response": "def _scan_pth_files(dir_paths):\n    \"\"\"Given an iterable of directory paths, yield paths to all .pth files within.\"\"\"\n    for dir_path in dir_paths:\n      if not os.path.exists(dir_path):\n        continue\n\n      pth_filenames = (f for f in os.listdir(dir_path) if f.endswith('.pth'))\n      for pth_filename in pth_filenames:\n        yield os.path.join(dir_path, pth_filename)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef minimum_sys_modules(cls, site_libs, modules=None):\n\n    modules = modules or sys.modules\n    new_modules = {}\n\n    for module_name, module in modules.items():\n      # builtins can stay\n      if not hasattr(module, '__path__'):\n        new_modules[module_name] = module\n        continue\n\n      # Unexpected objects, e.g. PEP 420 namespace packages, should just be dropped.\n      if not isinstance(module.__path__, list):\n        TRACER.log('Dropping %s' % (module_name,), V=3)\n        continue\n\n      # Pop off site-impacting __path__ elements in-place.\n      for k in reversed(range(len(module.__path__))):\n        if cls._tainted_path(module.__path__[k], site_libs):\n          TRACER.log('Scrubbing %s.__path__: %s' % (module_name, module.__path__[k]), V=3)\n          module.__path__.pop(k)\n\n      # It still contains path elements not in site packages, so it can stay in sys.modules\n      if module.__path__:\n        new_modules[module_name] = module\n\n    return new_modules", "response": "Given a set of site - packages paths return a set of sys. modules that are not in site - packages."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the minimum sys necessary to run this interpreter a la python - S.", "response": "def minimum_sys(cls, inherit_path):\n    \"\"\"Return the minimum sys necessary to run this interpreter, a la python -S.\n\n    :returns: (sys.path, sys.path_importer_cache, sys.modules) tuple of a\n      bare python installation.\n    \"\"\"\n    site_libs = set(cls.site_libs())\n    for site_lib in site_libs:\n      TRACER.log('Found site-library: %s' % site_lib)\n    for extras_path in cls._extras_paths():\n      TRACER.log('Found site extra: %s' % extras_path)\n      site_libs.add(extras_path)\n    site_libs = set(os.path.normpath(path) for path in site_libs)\n\n    sys_path, sys_path_importer_cache = cls.minimum_sys_path(site_libs, inherit_path)\n    sys_modules = cls.minimum_sys_modules(site_libs)\n\n    return sys_path, sys_path_importer_cache, sys_modules"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\npatching pkg_resources given a new working set.", "response": "def patch_pkg_resources(cls, working_set):\n    \"\"\"Patch pkg_resources given a new working set.\"\"\"\n    pkg_resources.working_set = working_set\n    pkg_resources.require = working_set.require\n    pkg_resources.iter_entry_points = working_set.iter_entry_points\n    pkg_resources.run_script = pkg_resources.run_main = working_set.run_script\n    pkg_resources.add_activation_listener = working_set.subscribe"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npatches sys with all site scrubbed.", "response": "def patch_sys(self, inherit_path):\n    \"\"\"Patch sys with all site scrubbed.\"\"\"\n    def patch_dict(old_value, new_value):\n      old_value.clear()\n      old_value.update(new_value)\n\n    def patch_all(path, path_importer_cache, modules):\n      sys.path[:] = path\n      patch_dict(sys.path_importer_cache, path_importer_cache)\n      patch_dict(sys.modules, modules)\n\n    new_sys_path, new_sys_path_importer_cache, new_sys_modules = self.minimum_sys(inherit_path)\n\n    new_sys_path.extend(merge_split(self._pex_info.pex_path, self._vars.PEX_PATH))\n\n    patch_all(new_sys_path, new_sys_path_importer_cache, new_sys_modules)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef execute(self):\n    teardown_verbosity = self._vars.PEX_TEARDOWN_VERBOSE\n    try:\n      pex_inherit_path = self._vars.PEX_INHERIT_PATH\n      if pex_inherit_path == \"false\":\n        pex_inherit_path = self._pex_info.inherit_path\n      self.patch_sys(pex_inherit_path)\n      working_set = self._activate()\n      self.patch_pkg_resources(working_set)\n      exit_code = self._wrap_coverage(self._wrap_profiling, self._execute)\n      if exit_code:\n        sys.exit(exit_code)\n    except Exception:\n      # Allow the current sys.excepthook to handle this app exception before we tear things down in\n      # finally, then reraise so that the exit status is reflected correctly.\n      sys.excepthook(*sys.exc_info())\n      raise\n    except SystemExit as se:\n      # Print a SystemExit error message, avoiding a traceback in python3.\n      # This must happen here, as sys.stderr is about to be torn down\n      if not isinstance(se.code, int) and se.code is not None:\n        print(se.code, file=sys.stderr)\n      raise\n    finally:\n      # squash all exceptions on interpreter teardown -- the primary type here are\n      # atexit handlers failing to run because of things such as:\n      #   http://stackoverflow.com/questions/2572172/referencing-other-modules-in-atexit\n      if not teardown_verbosity:\n        sys.stderr.flush()\n        sys.stderr = DevNull()\n        sys.excepthook = lambda *a, **kw: None", "response": "Execute the PEX.\n\n    This function makes assumptions that it is the last function called by\n    the interpreter."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run(self, args=(), with_chroot=False, blocking=True, setsid=False, **kwargs):\n    self.clean_environment()\n\n    cmdline = self.cmdline(args)\n    TRACER.log('PEX.run invoking %s' % ' '.join(cmdline))\n    process = Executor.open_process(cmdline,\n                                    cwd=self._pex if with_chroot else os.getcwd(),\n                                    preexec_fn=os.setsid if setsid else None,\n                                    stdin=kwargs.pop('stdin', None),\n                                    stdout=kwargs.pop('stdout', None),\n                                    stderr=kwargs.pop('stderr', None),\n                                    **kwargs)\n    return process.wait() if blocking else process", "response": "Run the PythonEnvironment in an interpreter in a subprocess."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncompiles the given python source files using this compiler s interpreter.", "response": "def compile(self, root, relpaths):\n    \"\"\"Compiles the given python source files using this compiler's interpreter.\n\n    :param string root: The root path all the source files are found under.\n    :param list relpaths: The realtive paths from the `root` of the source files to compile.\n    :returns: A list of relative paths of the compiled bytecode files.\n    :raises: A :class:`Compiler.Error` if there was a problem bytecode compiling any of the files.\n    \"\"\"\n    with named_temporary_file() as fp:\n      fp.write(to_bytes(_COMPILER_MAIN % {'root': root, 'relpaths': relpaths}, encoding='utf-8'))\n      fp.flush()\n\n      try:\n        out, _ = Executor.execute([self._interpreter.binary, '-sE', fp.name])\n      except Executor.NonZeroExit as e:\n        raise self.CompilationFailure(\n          'encountered %r during bytecode compilation.\\nstderr was:\\n%s\\n' % (e, e.stderr)\n        )\n\n      return out.splitlines()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef crypto_sign_keypair(seed=None):\n    if seed is None:\n        seed = os.urandom(PUBLICKEYBYTES)\n    else:\n        warnings.warn(\"ed25519ll should choose random seed.\",\n                      RuntimeWarning)\n    if len(seed) != 32:\n        raise ValueError(\"seed must be 32 random bytes or None.\")\n    skbytes = seed\n    vkbytes = djbec.publickey(skbytes)\n    return Keypair(vkbytes, skbytes+vkbytes)", "response": "Return a verifying secret key from a given seed or os. urandom ( 32 )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef crypto_sign(msg, sk):\n    if len(sk) != SECRETKEYBYTES:\n        raise ValueError(\"Bad signing key length %d\" % len(sk))\n    vkbytes = sk[PUBLICKEYBYTES:]\n    skbytes = sk[:PUBLICKEYBYTES]\n    sig = djbec.signature(msg, skbytes, vkbytes)\n    return sig + msg", "response": "Return signature + message given message and secret key."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns message given signature + message and the verifying key.", "response": "def crypto_sign_open(signed, vk):\n    \"\"\"Return message given signature+message and the verifying key.\"\"\"\n    if len(vk) != PUBLICKEYBYTES:\n        raise ValueError(\"Bad verifying key length %d\" % len(vk))\n    rc = djbec.checkvalid(signed[:SIGNATUREBYTES], signed[SIGNATUREBYTES:], vk)\n    if not rc:\n        raise ValueError(\"rc != True\", rc)\n    return signed[SIGNATUREBYTES:]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlocate the active PEX bootstrap.", "response": "def locate(cls):\n    \"\"\"Locates the active PEX bootstrap.\n\n    :rtype: :class:`Bootstrap`\n    \"\"\"\n    if cls._INSTANCE is None:\n      bootstrap_path = __file__\n      module_import_path = __name__.split('.')\n\n      # For example, our __file__ might be requests.pex/.bootstrap/pex/bootstrap.pyc and our import\n      # path pex.bootstrap; so we walk back through all the module components of our import path to\n      # find the base sys.path entry where we were found (requests.pex/.bootstrap in this example).\n      for _ in module_import_path:\n        bootstrap_path = os.path.dirname(bootstrap_path)\n\n      cls._INSTANCE = cls(sys_path_entry=bootstrap_path)\n    return cls._INSTANCE"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndemotes the bootstrap code to the end of the sys. path so it is found last.", "response": "def demote(self):\n    \"\"\"Demote the bootstrap code to the end of the `sys.path` so it is found last.\n\n    :return: The list of un-imported bootstrap modules.\n    :rtype: list of :class:`types.ModuleType`\n    \"\"\"\n    import sys  # Grab a hold of `sys` early since we'll be un-importing our module in this process.\n\n    unimported_modules = []\n    for name, module in reversed(sorted(sys.modules.items())):\n      if self.imported_from_bootstrap(module):\n        unimported_modules.append(sys.modules.pop(name))\n\n    sys.path[:] = [path for path in sys.path if os.path.realpath(path) != self._realpath]\n    sys.path.append(self._sys_path_entry)\n\n    return unimported_modules"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning True if the given module object was imported from bootstrap code.", "response": "def imported_from_bootstrap(self, module):\n    \"\"\"Return ``True`` if the given ``module`` object was imported from bootstrap code.\n\n    :param module: The module to check the provenance of.\n    :type module: :class:`types.ModuleType`\n    :rtype: bool\n    \"\"\"\n\n    # A vendored module.\n    path = getattr(module, '__file__', None)\n    if path and os.path.realpath(path).startswith(self._realpath):\n      return True\n\n    # A vendored package.\n    path = getattr(module, '__path__', None)\n    if path and any(os.path.realpath(path_item).startswith(self._realpath)\n                    for path_item in path):\n      return True\n\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_compatible_interpreters(pex_python_path=None, compatibility_constraints=None):\n  if pex_python_path:\n    interpreters = []\n    for binary in pex_python_path.split(os.pathsep):\n      try:\n        interpreters.append(PythonInterpreter.from_binary(binary))\n      except Executor.ExecutionError:\n        print(\"Python interpreter %s in PEX_PYTHON_PATH failed to load properly.\" % binary,\n          file=sys.stderr)\n    if not interpreters:\n      die('PEX_PYTHON_PATH was defined, but no valid interpreters could be identified. Exiting.')\n  else:\n    # We may have been invoked with a specific interpreter not on the $PATH, make sure our\n    # sys.executable is included as a candidate in this case.\n    interpreters = OrderedSet([PythonInterpreter.get()])\n\n    # Add all qualifying interpreters found in $PATH.\n    interpreters.update(PythonInterpreter.all())\n\n  return list(\n    matched_interpreters(interpreters, compatibility_constraints)\n    if compatibility_constraints\n    else interpreters\n  )", "response": "Find all compatible interpreters on the system within the supplied constraints and use\n     PEX_PYTHON_PATH if it is set."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef maybe_reexec_pex(compatibility_constraints):\n  if os.environ.pop('SHOULD_EXIT_BOOTSTRAP_REEXEC', None):\n    # We've already been here and selected an interpreter. Continue to execution.\n    return\n\n  target = None\n  with TRACER.timed('Selecting runtime interpreter based on pexrc', V=3):\n    if ENV.PEX_PYTHON and not ENV.PEX_PYTHON_PATH:\n      # preserve PEX_PYTHON re-exec for backwards compatibility\n      # TODO: Kill this off completely in favor of PEX_PYTHON_PATH\n      # https://github.com/pantsbuild/pex/issues/431\n      target = _select_pex_python_interpreter(ENV.PEX_PYTHON,\n                                              compatibility_constraints=compatibility_constraints)\n    elif ENV.PEX_PYTHON_PATH:\n      target = _select_interpreter(pex_python_path=ENV.PEX_PYTHON_PATH,\n                                   compatibility_constraints=compatibility_constraints)\n\n    elif compatibility_constraints:\n      # Apply constraints to target using regular PATH\n      target = _select_interpreter(compatibility_constraints=compatibility_constraints)\n\n  if target and os.path.realpath(target) != os.path.realpath(sys.executable):\n    cmdline = [target] + sys.argv\n    TRACER.log('Re-executing: cmdline=\"%s\", sys.executable=\"%s\", PEX_PYTHON=\"%s\", '\n               'PEX_PYTHON_PATH=\"%s\", COMPATIBILITY_CONSTRAINTS=\"%s\"'\n               % (cmdline, sys.executable, ENV.PEX_PYTHON, ENV.PEX_PYTHON_PATH,\n                  compatibility_constraints))\n    ENV.delete('PEX_PYTHON')\n    ENV.delete('PEX_PYTHON_PATH')\n    os.environ['SHOULD_EXIT_BOOTSTRAP_REEXEC'] = '1'\n    os.execve(target, cmdline, ENV.copy())", "response": "Try to re - execute the interpreter specified by the PEX - INFO\n."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef bootstrap_pex_env(entry_point):\n  pex_info = _bootstrap(entry_point)\n\n  from .environment import PEXEnvironment\n  PEXEnvironment(entry_point, pex_info).activate()", "response": "Bootstrap the current runtime environment using a given pex."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating options so that defaults are user relative to specified pex_root.", "response": "def make_relative_to_root(path):\n  \"\"\"Update options so that defaults are user relative to specified pex_root.\"\"\"\n  return os.path.normpath(path.format(pex_root=ENV.PEX_ROOT))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef dump(type, exc):\n        try:\n            return pickle.dumps(type), pickle.dumps(exc)\n        except Exception:\n            # get UnpickleableException inside the sandbox\n            if \"__PEX_UNVENDORED__\" in __import__(\"os\").environ:\n              from setuptools.sandbox import UnpickleableException as cls  # vendor:skip\n            else:\n              from pex.third_party.setuptools.sandbox import UnpickleableException as cls\n\n            return cls.dump(cls, cls(repr(exc)))", "response": "Dump a type and exception into a string."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses the contents of the wininst file and the egg - info directory.", "response": "def parse_info(wininfo_name, egginfo_name):\n    \"\"\"Extract metadata from filenames.\n\n    Extracts the 4 metadataitems needed (name, version, pyversion, arch) from\n    the installer filename and the name of the egg-info directory embedded in\n    the zipfile (if any).\n\n    The egginfo filename has the format::\n\n        name-ver(-pyver)(-arch).egg-info\n\n    The installer filename has the format::\n\n        name-ver.arch(-pyver).exe\n\n    Some things to note:\n\n    1. The installer filename is not definitive. An installer can be renamed\n       and work perfectly well as an installer. So more reliable data should\n       be used whenever possible.\n    2. The egg-info data should be preferred for the name and version, because\n       these come straight from the distutils metadata, and are mandatory.\n    3. The pyver from the egg-info data should be ignored, as it is\n       constructed from the version of Python used to build the installer,\n       which is irrelevant - the installer filename is correct here (even to\n       the point that when it's not there, any version is implied).\n    4. The architecture must be taken from the installer filename, as it is\n       not included in the egg-info data.\n    5. Architecture-neutral installers still have an architecture because the\n       installer format itself (being executable) is architecture-specific. We\n       should therefore ignore the architecture if the content is pure-python.\n    \"\"\"\n\n    egginfo = None\n    if egginfo_name:\n        egginfo = egg_info_re.search(egginfo_name)\n        if not egginfo:\n            raise ValueError(\"Egg info filename %s is not valid\" % (egginfo_name,))\n\n    # Parse the wininst filename\n    # 1. Distribution name (up to the first '-')\n    w_name, sep, rest = wininfo_name.partition('-')\n    if not sep:\n        raise ValueError(\"Installer filename %s is not valid\" % (wininfo_name,))\n\n    # Strip '.exe'\n    rest = rest[:-4]\n    # 2. Python version (from the last '-', must start with 'py')\n    rest2, sep, w_pyver = rest.rpartition('-')\n    if sep and w_pyver.startswith('py'):\n        rest = rest2\n        w_pyver = w_pyver.replace('.', '')\n    else:\n        # Not version specific - use py2.py3. While it is possible that\n        # pure-Python code is not compatible with both Python 2 and 3, there\n        # is no way of knowing from the wininst format, so we assume the best\n        # here (the user can always manually rename the wheel to be more\n        # restrictive if needed).\n        w_pyver = 'py2.py3'\n    # 3. Version and architecture\n    w_ver, sep, w_arch = rest.rpartition('.')\n    if not sep:\n        raise ValueError(\"Installer filename %s is not valid\" % (wininfo_name,))\n\n    if egginfo:\n        w_name = egginfo.group('name')\n        w_ver = egginfo.group('ver')\n\n    return {'name': w_name, 'ver': w_ver, 'arch': w_arch, 'pyver': w_pyver}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting the raw value to decimal representation.", "response": "def stripe_to_db(self, data):\n\t\t\"\"\"Convert the raw value to decimal representation.\"\"\"\n\t\tval = data.get(self.name)\n\n\t\t# Note: 0 is a possible return value, which is 'falseish'\n\t\tif val is not None:\n\t\t\treturn val / decimal.Decimal(\"100\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef stripe_to_db(self, data):\n\t\tval = data.get(self.name)\n\n\t\t# Note: 0 is a possible return value, which is 'falseish'\n\t\tif val is not None:\n\t\t\treturn convert_tstamp(val)", "response": "Convert the raw timestamp value to a DateTime representation."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a WebhookEventTrigger object from a Django request object.", "response": "def from_request(cls, request):\n\t\t\"\"\"\n\t\tCreate, validate and process a WebhookEventTrigger given a Django\n\t\trequest object.\n\n\t\tThe process is three-fold:\n\t\t1. Create a WebhookEventTrigger object from a Django request.\n\t\t2. Validate the WebhookEventTrigger as a Stripe event using the API.\n\t\t3. If valid, process it into an Event object (and child resource).\n\t\t\"\"\"\n\n\t\theaders = fix_django_headers(request.META)\n\t\tassert headers\n\t\ttry:\n\t\t\tbody = request.body.decode(request.encoding or \"utf-8\")\n\t\texcept Exception:\n\t\t\tbody = \"(error decoding body)\"\n\n\t\tip = request.META.get(\"REMOTE_ADDR\")\n\t\tif ip is None:\n\t\t\twarnings.warn(\n\t\t\t\t\"Could not determine remote IP (missing REMOTE_ADDR). \"\n\t\t\t\t\"This is likely an issue with your wsgi/server setup.\"\n\t\t\t)\n\t\t\tip = \"0.0.0.0\"\n\t\tobj = cls.objects.create(headers=headers, body=body, remote_ip=ip)\n\n\t\ttry:\n\t\t\tobj.valid = obj.validate()\n\t\t\tif obj.valid:\n\t\t\t\tif djstripe_settings.WEBHOOK_EVENT_CALLBACK:\n\t\t\t\t\t# If WEBHOOK_EVENT_CALLBACK, pass it for processing\n\t\t\t\t\tdjstripe_settings.WEBHOOK_EVENT_CALLBACK(obj)\n\t\t\t\telse:\n\t\t\t\t\t# Process the item (do not save it, it'll get saved below)\n\t\t\t\t\tobj.process(save=False)\n\t\texcept Exception as e:\n\t\t\tmax_length = WebhookEventTrigger._meta.get_field(\"exception\").max_length\n\t\t\tobj.exception = str(e)[:max_length]\n\t\t\tobj.traceback = format_exc()\n\n\t\t\t# Send the exception as the webhook_processing_error signal\n\t\t\twebhook_processing_error.send(\n\t\t\t\tsender=WebhookEventTrigger, exception=e, data=getattr(e, \"http_body\", \"\")\n\t\t\t)\n\n\t\t\t# re-raise the exception so Django sees it\n\t\t\traise e\n\t\tfinally:\n\t\t\tobj.save()\n\n\t\treturn obj"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef validate(self, api_key=None):\n\n\t\tlocal_data = self.json_body\n\t\tif \"id\" not in local_data or \"livemode\" not in local_data:\n\t\t\treturn False\n\n\t\tif self.is_test_event:\n\t\t\tlogger.info(\"Test webhook received: {}\".format(local_data))\n\t\t\treturn False\n\n\t\tif djstripe_settings.WEBHOOK_VALIDATION is None:\n\t\t\t# validation disabled\n\t\t\treturn True\n\t\telif (\n\t\t\tdjstripe_settings.WEBHOOK_VALIDATION == \"verify_signature\"\n\t\t\tand djstripe_settings.WEBHOOK_SECRET\n\t\t):\n\t\t\ttry:\n\t\t\t\tstripe.WebhookSignature.verify_header(\n\t\t\t\t\tself.body,\n\t\t\t\t\tself.headers.get(\"stripe-signature\"),\n\t\t\t\t\tdjstripe_settings.WEBHOOK_SECRET,\n\t\t\t\t\tdjstripe_settings.WEBHOOK_TOLERANCE,\n\t\t\t\t)\n\t\t\texcept stripe.error.SignatureVerificationError:\n\t\t\t\treturn False\n\t\t\telse:\n\t\t\t\treturn True\n\n\t\tlivemode = local_data[\"livemode\"]\n\t\tapi_key = api_key or djstripe_settings.get_default_api_key(livemode)\n\n\t\t# Retrieve the event using the api_version specified in itself\n\t\twith stripe_temporary_api_version(local_data[\"api_version\"], validate=False):\n\t\t\tremote_data = Event.stripe_class.retrieve(id=local_data[\"id\"], api_key=api_key)\n\n\t\treturn local_data[\"data\"] == remote_data[\"data\"]", "response": "This function validates the Event message."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef remove(self):\n\n\t\t# First, wipe default source on all customers that use this card.\n\t\tCustomer.objects.filter(default_source=self.id).update(default_source=None)\n\n\t\ttry:\n\t\t\tself._api_delete()\n\t\texcept InvalidRequestError as exc:\n\t\t\tif \"No such source:\" in str(exc) or \"No such customer:\" in str(exc):\n\t\t\t\t# The exception was thrown because the stripe customer or card was already\n\t\t\t\t# deleted on the stripe side, ignore the exception\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\t# The exception was raised for another reason, re-raise it\n\t\t\t\traise\n\n\t\tself.delete()", "response": "Removes a card from this customer s account."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a single use token that wraps the details of a credit card.", "response": "def create_token(\n\t\tcls,\n\t\tnumber,\n\t\texp_month,\n\t\texp_year,\n\t\tcvc,\n\t\tapi_key=djstripe_settings.STRIPE_SECRET_KEY,\n\t\t**kwargs\n\t):\n\t\t\"\"\"\n\t\tCreates a single use token that wraps the details of a credit card. This token can be used in\n\t\tplace of a credit card dictionary with any API method. These tokens can only be used once: by\n\t\tcreating a new charge object, or attaching them to a customer.\n\t\t(Source: https://stripe.com/docs/api/python#create_card_token)\n\n\t\t:param exp_month: The card's expiration month.\n\t\t:type exp_month: Two digit int\n\t\t:param exp_year: The card's expiration year.\n\t\t:type exp_year: Two or Four digit int\n\t\t:param number: The card number\n\t\t:type number: string without any separators (no spaces)\n\t\t:param cvc: Card security code.\n\t\t:type cvc: string\n\t\t\"\"\"\n\n\t\tcard = {\"number\": number, \"exp_month\": exp_month, \"exp_year\": exp_year, \"cvc\": cvc}\n\t\tcard.update(kwargs)\n\n\t\treturn stripe.Token.create(api_key=api_key, card=card)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndetach the source from its customer.", "response": "def detach(self):\n\t\t\"\"\"\n\t\tDetach the source from its customer.\n\t\t\"\"\"\n\n\t\t# First, wipe default source on all customers that use this.\n\t\tCustomer.objects.filter(default_source=self.id).update(default_source=None)\n\n\t\ttry:\n\t\t\t# TODO - we could use the return value of sync_from_stripe_data\n\t\t\t#  or call its internals - self._sync/_attach_objects_hook etc here\n\t\t\t#  to update `self` at this point?\n\t\t\tself.sync_from_stripe_data(self.api_retrieve().detach())\n\t\t\treturn True\n\t\texcept (InvalidRequestError, NotImplementedError):\n\t\t\t# The source was already detached. Resyncing.\n\t\t\t# NotImplementedError is an artifact of stripe-python<2.0\n\t\t\t# https://github.com/stripe/stripe-python/issues/376\n\t\t\tself.sync_from_stripe_data(self.api_retrieve())\n\t\t\treturn False"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef queryset(self, request, queryset):\n\t\tfilter_args = {self._filter_arg_key: None}\n\n\t\tif self.value() == \"yes\":\n\t\t\treturn queryset.exclude(**filter_args)\n\t\tif self.value() == \"no\":\n\t\t\treturn queryset.filter(**filter_args)", "response": "Returns the filtered queryset based on the value provided in the query string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of tuples.", "response": "def lookups(self, request, model_admin):\n\t\t\"\"\"\n\t\tReturn a list of tuples.\n\n\t\tThe first element in each tuple is the coded value for the option that will\n\t\tappear in the URL query. The second element is the\n\t\thuman-readable name for the option that will appear\n\t\tin the right sidebar.\n\t\tsource: https://docs.djangoproject.com/en/1.10/ref/contrib/admin/#django.contrib.admin.ModelAdmin.list_filter\n\t\t\"\"\"\n\t\tstatuses = [\n\t\t\t[x, x.replace(\"_\", \" \").title()]\n\t\t\tfor x in models.Subscription.objects.values_list(\"status\", flat=True).distinct()\n\t\t]\n\t\tstatuses.append([\"none\", \"No Subscription\"])\n\t\treturn statuses"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the filtered queryset based on the status of the current object.", "response": "def queryset(self, request, queryset):\n\t\t\"\"\"\n\t\tReturn the filtered queryset based on the value provided in the query string.\n\n\t\tsource: https://docs.djangoproject.com/en/1.10/ref/contrib/admin/#django.contrib.admin.ModelAdmin.list_filter\n\t\t\"\"\"\n\t\tif self.value() is None:\n\t\t\treturn queryset.all()\n\t\telse:\n\t\t\treturn queryset.filter(subscriptions__status=self.value()).distinct()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef handler(*event_types):\n\n\tdef decorator(func):\n\t\tfor event_type in event_types:\n\t\t\tregistrations[event_type].append(func)\n\t\treturn func\n\n\treturn decorator", "response": "A decorator that registers a function as a webhook handler."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef handler_all(func=None):\n\tif not func:\n\t\treturn functools.partial(handler_all)\n\n\tregistrations_global.append(func)\n\n\treturn func", "response": "Decorator that registers a function as a webhook handler for ALL webhook events."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncall all registered handlers for the provided event type and sub - type.", "response": "def call_handlers(event):\n\t\"\"\"\n\tInvoke all handlers for the provided event type/sub-type.\n\n\tThe handlers are invoked in the following order:\n\n\t1. Global handlers\n\t2. Event type handlers\n\t3. Event sub-type handlers\n\n\tHandlers within each group are invoked in order of registration.\n\n\t:param event: The event model object.\n\t:type event: ``djstripe.models.Event``\n\t\"\"\"\n\tchain = [registrations_global]\n\n\t# Build up a list of handlers with each qualified part of the event\n\t# category and verb.  For example, \"customer.subscription.created\" creates:\n\t#   1. \"customer\"\n\t#   2. \"customer.subscription\"\n\t#   3. \"customer.subscription.created\"\n\tfor index, _ in enumerate(event.parts):\n\t\tqualified_event_type = \".\".join(event.parts[: (index + 1)])\n\t\tchain.append(registrations[qualified_event_type])\n\n\tfor handler_func in itertools.chain(*chain):\n\t\thandler_func(event=event)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks the user has configured API live and test keys correctly.", "response": "def check_stripe_api_key(app_configs=None, **kwargs):\n\t\"\"\"Check the user has configured API live/test keys correctly.\"\"\"\n\tfrom . import settings as djstripe_settings\n\n\tmessages = []\n\n\tif not djstripe_settings.STRIPE_SECRET_KEY:\n\t\tmsg = \"Could not find a Stripe API key.\"\n\t\thint = \"Add STRIPE_TEST_SECRET_KEY and STRIPE_LIVE_SECRET_KEY to your settings.\"\n\t\tmessages.append(checks.Critical(msg, hint=hint, id=\"djstripe.C001\"))\n\telif djstripe_settings.STRIPE_LIVE_MODE:\n\t\tif not djstripe_settings.LIVE_API_KEY.startswith((\"sk_live_\", \"rk_live_\")):\n\t\t\tmsg = \"Bad Stripe live API key.\"\n\t\t\thint = 'STRIPE_LIVE_SECRET_KEY should start with \"sk_live_\"'\n\t\t\tmessages.append(checks.Critical(msg, hint=hint, id=\"djstripe.C002\"))\n\telse:\n\t\tif not djstripe_settings.TEST_API_KEY.startswith((\"sk_test_\", \"rk_test_\")):\n\t\t\tmsg = \"Bad Stripe test API key.\"\n\t\t\thint = 'STRIPE_TEST_SECRET_KEY should start with \"sk_test_\"'\n\t\t\tmessages.append(checks.Critical(msg, hint=hint, id=\"djstripe.C003\"))\n\n\treturn messages"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_stripe_api_version(app_configs=None, **kwargs):\n\tfrom . import settings as djstripe_settings\n\n\tmessages = []\n\tdefault_version = djstripe_settings.DEFAULT_STRIPE_API_VERSION\n\tversion = djstripe_settings.get_stripe_api_version()\n\n\tif not validate_stripe_api_version(version):\n\t\tmsg = \"Invalid Stripe API version: {}\".format(version)\n\t\thint = \"STRIPE_API_VERSION should be formatted as: YYYY-MM-DD\"\n\t\tmessages.append(checks.Critical(msg, hint=hint, id=\"djstripe.C004\"))\n\n\tif version != default_version:\n\t\tmsg = (\n\t\t\t\"The Stripe API version has a non-default value of '{}'. \"\n\t\t\t\"Non-default versions are not explicitly supported, and may \"\n\t\t\t\"cause compatibility issues.\".format(version)\n\t\t)\n\t\thint = \"Use the dj-stripe default for Stripe API version: {}\".format(default_version)\n\t\tmessages.append(checks.Warning(msg, hint=hint, id=\"djstripe.W001\"))\n\n\treturn messages", "response": "Check the user has configured API version correctly."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking that the DJSTRIPE_USE_NATIVE_JSONFIELD is not set unless Postgres is in use.", "response": "def check_native_jsonfield_postgres_engine(app_configs=None, **kwargs):\n\t\"\"\"\n\tCheck that the DJSTRIPE_USE_NATIVE_JSONFIELD isn't set unless Postgres is in use.\n\t\"\"\"\n\tfrom . import settings as djstripe_settings\n\n\tmessages = []\n\terror_msg = \"DJSTRIPE_USE_NATIVE_JSONFIELD is not compatible with engine {engine} for database {name}\"\n\n\tif djstripe_settings.USE_NATIVE_JSONFIELD:\n\t\tfor db_name, db_config in settings.DATABASES.items():\n\t\t\t# Hi there.\n\t\t\t# You may be reading this because you are using Postgres, but\n\t\t\t# dj-stripe is not detecting that correctly. For example, maybe you\n\t\t\t# are using multiple databases with different engines, or you have\n\t\t\t# your own backend. As long as you are certain you can support jsonb,\n\t\t\t# you can use the SILENCED_SYSTEM_CHECKS setting to ignore this check.\n\t\t\tengine = db_config.get(\"ENGINE\", \"\")\n\t\t\tif \"postgresql\" not in engine and \"postgis\" not in engine:\n\t\t\t\tmessages.append(\n\t\t\t\t\tchecks.Critical(\n\t\t\t\t\t\terror_msg.format(name=repr(db_name), engine=repr(engine)),\n\t\t\t\t\t\thint=\"Switch to Postgres, or unset DJSTRIPE_USE_NATIVE_JSONFIELD\",\n\t\t\t\t\t\tid=\"djstripe.C005\",\n\t\t\t\t\t)\n\t\t\t\t)\n\n\treturn messages"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_stripe_api_host(app_configs=None, **kwargs):\n\tfrom django.conf import settings\n\n\tmessages = []\n\n\tif not settings.DEBUG and hasattr(settings, \"STRIPE_API_HOST\"):\n\t\tmessages.append(\n\t\t\tchecks.Warning(\n\t\t\t\t\"STRIPE_API_HOST should not be set in production! This is most likely unintended.\",\n\t\t\t\thint=\"Remove STRIPE_API_HOST from your Django settings.\",\n\t\t\t\tid=\"djstripe.W002\",\n\t\t\t)\n\t\t)\n\n\treturn messages", "response": "Check that STRIPE_API_HOST is not being used in production."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_webhook_secret(app_configs=None, **kwargs):\n\tfrom . import settings as djstripe_settings\n\n\tmessages = []\n\n\tsecret = djstripe_settings.WEBHOOK_SECRET\n\tif secret and not secret.startswith(\"whsec_\"):\n\t\tmessages.append(\n\t\t\tchecks.Warning(\n\t\t\t\t\"DJSTRIPE_WEBHOOK_SECRET does not look valid\",\n\t\t\t\thint=\"It should start with whsec_...\",\n\t\t\t\tid=\"djstripe.W003\",\n\t\t\t)\n\t\t)\n\n\treturn messages", "response": "Check that DJSTRIPE_WEBHOOK_SECRET looks correct."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_webhook_validation(app_configs=None, **kwargs):\n\tfrom . import settings as djstripe_settings\n\n\tmessages = []\n\n\tvalidation_options = (\"verify_signature\", \"retrieve_event\")\n\n\tif djstripe_settings.WEBHOOK_VALIDATION is None:\n\t\tmessages.append(\n\t\t\tchecks.Warning(\n\t\t\t\t\"Webhook validation is disabled, this is a security risk if the webhook view is enabled\",\n\t\t\t\thint=\"Set DJSTRIPE_WEBHOOK_VALIDATION to one of {}\".format(\n\t\t\t\t\t\", \".join(validation_options)\n\t\t\t\t),\n\t\t\t\tid=\"djstripe.W004\",\n\t\t\t)\n\t\t)\n\telif djstripe_settings.WEBHOOK_VALIDATION == \"verify_signature\":\n\t\tif not djstripe_settings.WEBHOOK_SECRET:\n\t\t\tmessages.append(\n\t\t\t\tchecks.Critical(\n\t\t\t\t\t\"DJSTRIPE_WEBHOOK_VALIDATION='verify_signature' but DJSTRIPE_WEBHOOK_SECRET is not set\",\n\t\t\t\t\thint=\"Set DJSTRIPE_WEBHOOK_SECRET or set DJSTRIPE_WEBHOOK_VALIDATION='retrieve_event'\",\n\t\t\t\t\tid=\"djstripe.C006\",\n\t\t\t\t)\n\t\t\t)\n\telif djstripe_settings.WEBHOOK_VALIDATION not in validation_options:\n\t\tmessages.append(\n\t\t\tchecks.Critical(\n\t\t\t\t\"DJSTRIPE_WEBHOOK_VALIDATION is invalid\",\n\t\t\t\thint=\"Set DJSTRIPE_WEBHOOK_VALIDATION to one of {} or None\".format(\n\t\t\t\t\t\", \".join(validation_options)\n\t\t\t\t),\n\t\t\t\tid=\"djstripe.C007\",\n\t\t\t)\n\t\t)\n\n\treturn messages", "response": "Check that the DJSTRIPE_WEBHOOK_VALIDATION is valid and return a list of messages."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking that DJSTRIPE_SUBSCRIBER_CUSTOMER_KEY fits in metadata.", "response": "def check_subscriber_key_length(app_configs=None, **kwargs):\n\t\"\"\"\n\tCheck that DJSTRIPE_SUBSCRIBER_CUSTOMER_KEY fits in metadata.\n\n\tDocs: https://stripe.com/docs/api#metadata\n\t\"\"\"\n\tfrom . import settings as djstripe_settings\n\n\tmessages = []\n\n\tkey = djstripe_settings.SUBSCRIBER_CUSTOMER_KEY\n\tkey_size = len(str(key))\n\tif key and key_size > 40:\n\t\tmessages.append(\n\t\t\tchecks.Error(\n\t\t\t\t\"DJSTRIPE_SUBSCRIBER_CUSTOMER_KEY must be no more than 40 characters long\",\n\t\t\t\thint=\"Current value: %r (%i characters)\" % (key, key_size),\n\t\t\t\tid=\"djstripe.E001\",\n\t\t\t)\n\t\t)\n\n\treturn messages"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef handle(self, *args, **options):\n\t\tfor plan_data in Plan.api_list():\n\t\t\tplan = Plan.sync_from_stripe_data(plan_data)\n\t\t\tprint(\"Synchronized plan {0}\".format(plan.id))", "response": "Call sync_from_stripe_data for each plan returned by api_list."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fix_django_headers(meta):\n\tret = {}\n\tfor k, v in meta.items():\n\t\tif k.startswith(\"HTTP_\"):\n\t\t\tk = k[len(\"HTTP_\") :]\n\t\telif k not in (\"CONTENT_LENGTH\", \"CONTENT_TYPE\"):\n\t\t\t# Skip CGI garbage\n\t\t\tcontinue\n\n\t\tret[k.lower().replace(\"_\", \"-\")] = v\n\n\treturn ret", "response": "Fix django - specific headers"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef subscriber_has_active_subscription(subscriber, plan=None):\n\tif isinstance(subscriber, AnonymousUser):\n\t\traise ImproperlyConfigured(ANONYMOUS_USER_ERROR_MSG)\n\n\tif isinstance(subscriber, get_user_model()):\n\t\tif subscriber.is_superuser or subscriber.is_staff:\n\t\t\treturn True\n\tfrom .models import Customer\n\n\tcustomer, created = Customer.get_or_create(subscriber)\n\tif created or not customer.has_active_subscription(plan):\n\t\treturn False\n\treturn True", "response": "check if a subscriber has an active subscription"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_supported_currency_choices(api_key):\n\timport stripe\n\n\tstripe.api_key = api_key\n\n\taccount = stripe.Account.retrieve()\n\tsupported_payment_currencies = stripe.CountrySpec.retrieve(account[\"country\"])[\n\t\t\"supported_payment_currencies\"\n\t]\n\n\treturn [(currency, currency.upper()) for currency in supported_payment_currencies]", "response": "Get a list of all supported currencies and their upper cased names."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts a Stripe API timestamp response to a native datetime.", "response": "def convert_tstamp(response):\n\t\"\"\"\n\tConvert a Stripe API timestamp response (unix epoch) to a native datetime.\n\n\t:rtype: datetime\n\t\"\"\"\n\tif response is None:\n\t\t# Allow passing None to convert_tstamp()\n\t\treturn response\n\n\t# Overrides the set timezone to UTC - I think...\n\ttz = timezone.utc if settings.USE_TZ else None\n\n\treturn datetime.datetime.fromtimestamp(response, tz)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_context_data(self, **kwargs):\n\t\tcontext = super().get_context_data(**kwargs)\n\t\tcontext.update(\n\t\t\t{\n\t\t\t\t\"STRIPE_PUBLIC_KEY\": djstripe_settings.STRIPE_PUBLIC_KEY,\n\t\t\t\t\"plans\": Plan.objects.all(),\n\t\t\t}\n\t\t)\n\t\treturn context", "response": "Inject STRIPE_PUBLIC_KEY and plans into context_data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_context_data(self, *args, **kwargs):\n\t\tcontext = super().get_context_data(**kwargs)\n\t\tcontext[\"is_plans_plural\"] = Plan.objects.count() > 1\n\t\tcontext[\"customer\"], _created = Customer.get_or_create(\n\t\t\tsubscriber=djstripe_settings.subscriber_request_callback(self.request)\n\t\t)\n\t\tcontext[\"subscription\"] = context[\"customer\"].subscription\n\t\treturn context", "response": "Inject is_plans_plural and customer into context_data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef upcoming(\n\t\tcls,\n\t\tapi_key=djstripe_settings.STRIPE_SECRET_KEY,\n\t\tcustomer=None,\n\t\tcoupon=None,\n\t\tsubscription=None,\n\t\tsubscription_plan=None,\n\t\tsubscription_prorate=None,\n\t\tsubscription_proration_date=None,\n\t\tsubscription_quantity=None,\n\t\tsubscription_trial_end=None,\n\t\t**kwargs\n\t):\n\t\t\"\"\"\n\t\tGets the upcoming preview invoice (singular) for a customer.\n\n\t\tAt any time, you can preview the upcoming\n\t\tinvoice for a customer. This will show you all the charges that are\n\t\tpending, including subscription renewal charges, invoice item charges,\n\t\tetc. It will also show you any discount that is applicable to the\n\t\tcustomer. (Source: https://stripe.com/docs/api#upcoming_invoice)\n\n\t\t.. important:: Note that when you are viewing an upcoming invoice, you are simply viewing a preview.\n\n\t\t:param customer: The identifier of the customer whose upcoming invoice \\\n\t\tyou'd like to retrieve.\n\t\t:type customer: Customer or string (customer ID)\n\t\t:param coupon: The code of the coupon to apply.\n\t\t:type coupon: str\n\t\t:param subscription: The identifier of the subscription to retrieve an \\\n\t\tinvoice for.\n\t\t:type subscription: Subscription or string (subscription ID)\n\t\t:param subscription_plan: If set, the invoice returned will preview \\\n\t\tupdating the subscription given to this plan, or creating a new \\\n\t\tsubscription to this plan if no subscription is given.\n\t\t:type subscription_plan: Plan or string (plan ID)\n\t\t:param subscription_prorate: If previewing an update to a subscription, \\\n\t\tthis decides whether the preview will show the result of applying \\\n\t\tprorations or not.\n\t\t:type subscription_prorate: bool\n\t\t:param subscription_proration_date: If previewing an update to a \\\n\t\tsubscription, and doing proration, subscription_proration_date forces \\\n\t\tthe proration to be calculated as though the update was done at the \\\n\t\tspecified time.\n\t\t:type subscription_proration_date: datetime\n\t\t:param subscription_quantity: If provided, the invoice returned will \\\n\t\tpreview updating or creating a subscription with that quantity.\n\t\t:type subscription_quantity: int\n\t\t:param subscription_trial_end: If provided, the invoice returned will \\\n\t\tpreview updating or creating a subscription with that trial end.\n\t\t:type subscription_trial_end: datetime\n\t\t:returns: The upcoming preview invoice.\n\t\t:rtype: UpcomingInvoice\n\t\t\"\"\"\n\n\t\t# Convert Customer to id\n\t\tif customer is not None and isinstance(customer, StripeModel):\n\t\t\tcustomer = customer.id\n\n\t\t# Convert Subscription to id\n\t\tif subscription is not None and isinstance(subscription, StripeModel):\n\t\t\tsubscription = subscription.id\n\n\t\t# Convert Plan to id\n\t\tif subscription_plan is not None and isinstance(subscription_plan, StripeModel):\n\t\t\tsubscription_plan = subscription_plan.id\n\n\t\ttry:\n\t\t\tupcoming_stripe_invoice = cls.stripe_class.upcoming(\n\t\t\t\tapi_key=api_key,\n\t\t\t\tcustomer=customer,\n\t\t\t\tcoupon=coupon,\n\t\t\t\tsubscription=subscription,\n\t\t\t\tsubscription_plan=subscription_plan,\n\t\t\t\tsubscription_prorate=subscription_prorate,\n\t\t\t\tsubscription_proration_date=subscription_proration_date,\n\t\t\t\tsubscription_quantity=subscription_quantity,\n\t\t\t\tsubscription_trial_end=subscription_trial_end,\n\t\t\t\t**kwargs\n\t\t\t)\n\t\texcept InvalidRequestError as exc:\n\t\t\tif str(exc) != \"Nothing to invoice for customer\":\n\t\t\t\traise\n\t\t\treturn\n\n\t\t# Workaround for \"id\" being missing (upcoming invoices don't persist).\n\t\tupcoming_stripe_invoice[\"id\"] = \"upcoming\"\n\n\t\treturn UpcomingInvoice._create_from_stripe_object(upcoming_stripe_invoice, save=False)", "response": "This method returns the upcoming preview invoice for a customer."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef retry(self):\n\n\t\tif not self.paid and not self.forgiven and not self.closed:\n\t\t\tstripe_invoice = self.api_retrieve()\n\t\t\tupdated_stripe_invoice = (\n\t\t\t\tstripe_invoice.pay()\n\t\t\t)  # pay() throws an exception if the charge is not successful.\n\t\t\ttype(self).sync_from_stripe_data(updated_stripe_invoice)\n\t\t\treturn True\n\t\treturn False", "response": "Retry payment on this invoice if it isn t paid closed or forgiven."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the status of this invoice.", "response": "def status(self):\n\t\t\"\"\" Attempts to label this invoice with a status. Note that an invoice can be more than one of the choices.\n\t\t\tWe just set a priority on which status appears.\n\t\t\"\"\"\n\n\t\tif self.paid:\n\t\t\treturn self.STATUS_PAID\n\t\tif self.forgiven:\n\t\t\treturn self.STATUS_FORGIVEN\n\t\tif self.closed:\n\t\t\treturn self.STATUS_CLOSED\n\t\treturn self.STATUS_OPEN"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef plan(self):\n\n\t\tfor invoiceitem in self.invoiceitems.all():\n\t\t\tif invoiceitem.plan:\n\t\t\t\treturn invoiceitem.plan\n\n\t\tif self.subscription:\n\t\t\treturn self.subscription.plan", "response": "Gets the associated plan for this invoice."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting or create a Plan.", "response": "def get_or_create(cls, **kwargs):\n\t\t\"\"\" Get or create a Plan.\"\"\"\n\n\t\ttry:\n\t\t\treturn Plan.objects.get(id=kwargs[\"id\"]), False\n\t\texcept Plan.DoesNotExist:\n\t\t\treturn cls.create(**kwargs), True"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update_name(self):\n\n\t\tp = self.api_retrieve()\n\t\tp.name = self.name\n\t\tp.save()\n\n\t\tself.save()", "response": "Update the name of the Plan in Stripe and in the DB."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update(\n\t\tself,\n\t\tplan=None,\n\t\tapplication_fee_percent=None,\n\t\tbilling_cycle_anchor=None,\n\t\tcoupon=None,\n\t\tprorate=djstripe_settings.PRORATION_POLICY,\n\t\tproration_date=None,\n\t\tmetadata=None,\n\t\tquantity=None,\n\t\ttax_percent=None,\n\t\ttrial_end=None,\n\t):\n\t\t\"\"\"\n\t\tSee `Customer.subscribe() <#djstripe.models.Customer.subscribe>`__\n\n\t\t:param plan: The plan to which to subscribe the customer.\n\t\t:type plan: Plan or string (plan ID)\n\t\t:param application_fee_percent:\n\t\t:type application_fee_percent:\n\t\t:param billing_cycle_anchor:\n\t\t:type billing_cycle_anchor:\n\t\t:param coupon:\n\t\t:type coupon:\n\t\t:param prorate: Whether or not to prorate when switching plans. Default is True.\n\t\t:type prorate: boolean\n\t\t:param proration_date:\n\t\t\tIf set, the proration will be calculated as though the subscription was updated at the\n\t\t\tgiven time. This can be used to apply exactly the same proration that was previewed\n\t\t\twith upcoming invoice endpoint. It can also be used to implement custom proration\n\t\t\tlogic, such as prorating by day instead of by second, by providing the time that you\n\t\t\twish to use for proration calculations.\n\t\t:type proration_date: datetime\n\t\t:param metadata:\n\t\t:type metadata:\n\t\t:param quantity:\n\t\t:type quantity:\n\t\t:param tax_percent:\n\t\t:type tax_percent:\n\t\t:param trial_end:\n\t\t:type trial_end:\n\n\t\t.. note:: The default value for ``prorate`` is the DJSTRIPE_PRORATION_POLICY setting.\n\n\t\t.. important:: Updating a subscription by changing the plan or quantity creates a new ``Subscription`` in \\\n\t\tStripe (and dj-stripe).\n\t\t\"\"\"\n\n\t\t# Convert Plan to id\n\t\tif plan is not None and isinstance(plan, StripeModel):\n\t\t\tplan = plan.id\n\n\t\tkwargs = deepcopy(locals())\n\t\tdel kwargs[\"self\"]\n\n\t\tstripe_subscription = self.api_retrieve()\n\n\t\tfor kwarg, value in kwargs.items():\n\t\t\tif value is not None:\n\t\t\t\tsetattr(stripe_subscription, kwarg, value)\n\n\t\treturn Subscription.sync_from_stripe_data(stripe_subscription.save())", "response": "Update the subscription with the given parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef extend(self, delta):\n\n\t\tif delta.total_seconds() < 0:\n\t\t\traise ValueError(\"delta must be a positive timedelta.\")\n\n\t\tif self.trial_end is not None and self.trial_end > timezone.now():\n\t\t\tperiod_end = self.trial_end\n\t\telse:\n\t\t\tperiod_end = self.current_period_end\n\n\t\tperiod_end += delta\n\n\t\treturn self.update(prorate=False, trial_end=period_end)", "response": "Extend this subscription by the provided delta."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cancel(self, at_period_end=djstripe_settings.CANCELLATION_AT_PERIOD_END):\n\n\t\t# If plan has trial days and customer cancels before\n\t\t# trial period ends, then end subscription now,\n\t\t# i.e. at_period_end=False\n\t\tif self.trial_end and self.trial_end > timezone.now():\n\t\t\tat_period_end = False\n\n\t\tif at_period_end:\n\t\t\tstripe_subscription = self.api_retrieve()\n\t\t\tstripe_subscription.cancel_at_period_end = True\n\t\t\tstripe_subscription.save()\n\t\telse:\n\t\t\ttry:\n\t\t\t\tstripe_subscription = self._api_delete()\n\t\t\texcept InvalidRequestError as exc:\n\t\t\t\tif \"No such subscription:\" in str(exc):\n\t\t\t\t\t# cancel() works by deleting the subscription. The object still\n\t\t\t\t\t# exists in Stripe however, and can still be retrieved.\n\t\t\t\t\t# If the subscription was already canceled (status=canceled),\n\t\t\t\t\t# that api_retrieve() call will fail with \"No such subscription\".\n\t\t\t\t\t# However, this may also happen if the subscription legitimately\n\t\t\t\t\t# does not exist, in which case the following line will re-raise.\n\t\t\t\t\tstripe_subscription = self.api_retrieve()\n\t\t\t\telse:\n\t\t\t\t\traise\n\n\t\treturn Subscription.sync_from_stripe_data(stripe_subscription)", "response": "Cancels this subscription. If you set the at_period_end parameter to true, the subscription will remain active\n\t\tuntil the end of the period, at which point it will be canceled and not renewed. By default, the subscription\n\t\tis terminated immediately. In either case, the customer will not be charged again for the subscription. Note,\n\t\thowever, that any pending invoice items that you've created will still be charged for at the end of the period\n\t\tunless manually deleted. If you've set the subscription to cancel at period end, any pending prorations will\n\t\talso be left in place and collected at the end of the period, but if the subscription is set to cancel\n\t\timmediately, pending prorations will be removed.\n\n\t\tBy default, all unpaid invoices for the customer will be closed upon subscription cancellation. We do this in\n\t\torder to prevent unexpected payment retries once the customer has canceled a subscription. However, you can\n\t\treopen the invoices manually after subscription cancellation to have us proceed with automatic retries, or you\n\t\tcould even re-attempt payment yourself on all unpaid invoices before allowing the customer to cancel the\n\t\tsubscription at all.\n\n\t\t:param at_period_end: A flag that if set to true will delay the cancellation of the subscription until the end\n\t\t\tof the current period. Default is False.\n\t\t:type at_period_end: boolean\n\n\t\t.. important:: If a subscription is cancelled during a trial period, the ``at_period_end`` flag will be \\\n\t\toverridden to False so that the trial ends immediately and the customer's card isn't charged."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef reactivate(self):\n\t\tstripe_subscription = self.api_retrieve()\n\t\tstripe_subscription.plan = self.plan.id\n\t\tstripe_subscription.cancel_at_period_end = False\n\n\t\treturn Subscription.sync_from_stripe_data(stripe_subscription.save())", "response": "Reactivates this subscription.\n\n\t\tIf a customer's subscription is canceled with ``at_period_end`` set to True and it has not yet reached the end\n\t\tof the billing period, it can be reactivated. Subscriptions canceled immediately cannot be reactivated.\n\t\t(Source: https://stripe.com/docs/subscriptions/canceling-pausing)\n\n\t\t.. warning:: Reactivating a fully canceled Subscription will fail silently. Be sure to check the returned \\\n\t\tSubscription's status."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_period_current(self):\n\n\t\treturn self.current_period_end > timezone.now() or (\n\t\t\tself.trial_end and self.trial_end > timezone.now()\n\t\t)", "response": "Returns True if this subscription s period is current false otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn True if the status of the subscription is temporarily current.", "response": "def is_status_temporarily_current(self):\n\t\t\"\"\"\n\t\tA status is temporarily current when the subscription is canceled with the ``at_period_end`` flag.\n\t\tThe subscription is still active, but is technically canceled and we're just waiting for it to run out.\n\n\t\tYou could use this method to give customers limited service after they've canceled. For example, a video\n\t\ton demand service could only allow customers to download their libraries and do nothing else when their\n\t\tsubscription is temporarily current.\n\t\t\"\"\"\n\n\t\treturn (\n\t\t\tself.canceled_at and self.start < self.canceled_at and self.cancel_at_period_end\n\t\t)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef handle(self, *args, **options):\n\t\tfor subscriber in get_subscriber_model().objects.filter(djstripe_customers=None):\n\t\t\t# use get_or_create in case of race conditions on large subscriber bases\n\t\t\tCustomer.get_or_create(subscriber=subscriber)\n\t\t\tprint(\"Created subscriber for {0}\".format(subscriber.email))", "response": "Create Customer objects for Subscribers without Customer objects associated."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the customer s valid subscriptions.", "response": "def get(self, request, **kwargs):\n\t\t\"\"\"\n\t\tReturn the customer's valid subscriptions.\n\n\t\tReturns with status code 200.\n\t\t\"\"\"\n\t\tcustomer, _created = Customer.get_or_create(\n\t\t\tsubscriber=subscriber_request_callback(self.request)\n\t\t)\n\n\t\tserializer = SubscriptionSerializer(customer.subscription)\n\t\treturn Response(serializer.data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef post(self, request, **kwargs):\n\t\tserializer = CreateSubscriptionSerializer(data=request.data)\n\n\t\tif serializer.is_valid():\n\t\t\ttry:\n\t\t\t\tcustomer, _created = Customer.get_or_create(\n\t\t\t\t\tsubscriber=subscriber_request_callback(self.request)\n\t\t\t\t)\n\t\t\t\tcustomer.add_card(serializer.data[\"stripe_token\"])\n\t\t\t\tcharge_immediately = serializer.data.get(\"charge_immediately\")\n\t\t\t\tif charge_immediately is None:\n\t\t\t\t\tcharge_immediately = True\n\n\t\t\t\tcustomer.subscribe(serializer.data[\"plan\"], charge_immediately)\n\t\t\t\treturn Response(serializer.data, status=status.HTTP_201_CREATED)\n\t\t\texcept Exception:\n\t\t\t\t# TODO: Better error messages\n\t\t\t\treturn Response(\n\t\t\t\t\t\"Something went wrong processing the payment.\", status=status.HTTP_400_BAD_REQUEST\n\t\t\t\t)\n\n\t\treturn Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST)", "response": "Create a new subscription for the user."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delete(self, request, **kwargs):\n\t\ttry:\n\t\t\tcustomer, _created = Customer.get_or_create(\n\t\t\t\tsubscriber=subscriber_request_callback(self.request)\n\t\t\t)\n\t\t\tcustomer.subscription.cancel(at_period_end=CANCELLATION_AT_PERIOD_END)\n\n\t\t\treturn Response(status=status.HTTP_204_NO_CONTENT)\n\t\texcept Exception:\n\t\t\treturn Response(\n\t\t\t\t\"Something went wrong cancelling the subscription.\",\n\t\t\t\tstatus=status.HTTP_400_BAD_REQUEST,\n\t\t\t)", "response": "Cancel the customers current subscription."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef stripe_temporary_api_version(version, validate=True):\n\told_version = djstripe_settings.get_stripe_api_version()\n\n\ttry:\n\t\tdjstripe_settings.set_stripe_api_version(version, validate=validate)\n\t\tyield\n\tfinally:\n\t\t# Validation is bypassed since we're restoring a previous value.\n\t\tdjstripe_settings.set_stripe_api_version(old_version, validate=False)", "response": "Temporarily replace the global api_version used in stripe API calls with the given value."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef postgres_migration_prep(apps, schema_editor):\n\n\tAccount = apps.get_model(\"djstripe\", \"Account\")\n\tBankAccount = apps.get_model(\"djstripe\", \"BankAccount\")\n\tCard = apps.get_model(\"djstripe\", \"Card\")\n\tCharge = apps.get_model(\"djstripe\", \"Charge\")\n\tCustomer = apps.get_model(\"djstripe\", \"Customer\")\n\tEvent = apps.get_model(\"djstripe\", \"Event\")\n\tInvoice = apps.get_model(\"djstripe\", \"Invoice\")\n\tPayout = apps.get_model(\"djstripe\", \"Payout\")\n\tPlan = apps.get_model(\"djstripe\", \"Plan\")\n\tProduct = apps.get_model(\"djstripe\", \"Product\")\n\tRefund = apps.get_model(\"djstripe\", \"Refund\")\n\tSource = apps.get_model(\"djstripe\", \"Source\")\n\tTransfer = apps.get_model(\"djstripe\", \"Transfer\")\n\n\tmodel_fields = [\n\t\t(\n\t\t\tAccount,\n\t\t\t(\n\t\t\t\t\"business_name\",\n\t\t\t\t\"business_primary_color\",\n\t\t\t\t\"business_url\",\n\t\t\t\t\"payout_statement_descriptor\",\n\t\t\t\t\"product_description\",\n\t\t\t\t\"support_url\",\n\t\t\t),\n\t\t),\n\t\t(BankAccount, (\"account_holder_name\",)),\n\t\t(\n\t\t\tCard,\n\t\t\t(\n\t\t\t\t\"address_city\",\n\t\t\t\t\"address_country\",\n\t\t\t\t\"address_line1\",\n\t\t\t\t\"address_line1_check\",\n\t\t\t\t\"address_line2\",\n\t\t\t\t\"address_state\",\n\t\t\t\t\"address_zip\",\n\t\t\t\t\"address_zip_check\",\n\t\t\t\t\"country\",\n\t\t\t\t\"cvc_check\",\n\t\t\t\t\"dynamic_last4\",\n\t\t\t\t\"fingerprint\",\n\t\t\t\t\"name\",\n\t\t\t\t\"tokenization_method\",\n\t\t\t),\n\t\t),\n\t\t(\n\t\t\tCharge,\n\t\t\t(\n\t\t\t\t\"failure_code\",\n\t\t\t\t\"failure_message\",\n\t\t\t\t\"receipt_email\",\n\t\t\t\t\"receipt_number\",\n\t\t\t\t\"statement_descriptor\",\n\t\t\t\t\"transfer_group\",\n\t\t\t),\n\t\t),\n\t\t(Customer, (\"business_vat_id\", \"currency\", \"email\")),\n\t\t(Event, (\"idempotency_key\", \"request_id\")),\n\t\t(Invoice, (\"hosted_invoice_url\", \"invoice_pdf\", \"number\", \"statement_descriptor\")),\n\t\t(Payout, (\"failure_code\", \"failure_message\", \"statement_descriptor\")),\n\t\t(Plan, (\"aggregate_usage\", \"billing_scheme\", \"nickname\")),\n\t\t(Product, (\"caption\", \"statement_descriptor\", \"unit_label\")),\n\t\t(Refund, (\"failure_reason\", \"reason\", \"receipt_number\")),\n\t\t(Source, (\"currency\", \"statement_descriptor\")),\n\t\t(Transfer, (\"transfer_group\",)),\n\t]\n\n\tfor model, fields in model_fields:\n\t\tfor field in fields:\n\t\t\tfilter_param = {\"{}__isnull\".format(field): True}\n\t\t\tupdate_param = {field: \"\"}\n\t\t\tmodel.objects.filter(**filter_param).update(**update_param)", "response": "Prepare the database for migration 0003."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef customer_webhook_handler(event):\n\tif event.customer:\n\t\t# As customers are tied to local users, djstripe will not create\n\t\t# customers that do not already exist locally.\n\t\t_handle_crud_like_event(\n\t\t\ttarget_cls=models.Customer, event=event, crud_exact=True, crud_valid=True\n\t\t)", "response": "Handle updates to customer objects.\n\n\tFirst determines the crud_type and then handles the event if a customer exists locally.\n\tAs customers are tied to local users, djstripe will not create customers that\n\tdo not already exist locally.\n\n\tDocs and an example customer webhook response: https://stripe.com/docs/api#customer_object"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nhandling customer discount events.", "response": "def customer_discount_webhook_handler(event):\n\t\"\"\"Handle updates to customer discount objects.\n\n\tDocs: https://stripe.com/docs/api#discounts\n\n\tBecause there is no concept of a \"Discount\" model in dj-stripe (due to the\n\tlack of a stripe id on them), this is a little different to the other\n\thandlers.\n\t\"\"\"\n\n\tcrud_type = CrudType.determine(event=event)\n\tdiscount_data = event.data.get(\"object\", {})\n\tcoupon_data = discount_data.get(\"coupon\", {})\n\tcustomer = event.customer\n\n\tif crud_type.created or crud_type.updated:\n\t\tcoupon, _ = _handle_crud_like_event(\n\t\t\ttarget_cls=models.Coupon, event=event, data=coupon_data, id=coupon_data.get(\"id\")\n\t\t)\n\t\tcoupon_start = discount_data.get(\"start\")\n\t\tcoupon_end = discount_data.get(\"end\")\n\telse:\n\t\tcoupon = None\n\t\tcoupon_start = None\n\t\tcoupon_end = None\n\n\tcustomer.coupon = coupon\n\tcustomer.coupon_start = convert_tstamp(coupon_start)\n\tcustomer.coupon_end = convert_tstamp(coupon_end)\n\tcustomer.save()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef customer_source_webhook_handler(event):\n\tcustomer_data = event.data.get(\"object\", {})\n\tsource_type = customer_data.get(\"object\", {})\n\n\t# TODO: handle other types of sources (https://stripe.com/docs/api#customer_object-sources)\n\tif source_type == SourceType.card:\n\t\tif event.verb.endswith(\"deleted\") and customer_data:\n\t\t\t# On customer.source.deleted, we do not delete the object, we merely unlink it.\n\t\t\t# customer = Customer.objects.get(id=customer_data[\"id\"])\n\t\t\t# NOTE: for now, customer.sources still points to Card\n\t\t\t# Also, https://github.com/dj-stripe/dj-stripe/issues/576\n\t\t\tmodels.Card.objects.filter(id=customer_data.get(\"id\", \"\")).delete()\n\t\t\tmodels.DjstripePaymentMethod.objects.filter(id=customer_data.get(\"id\", \"\")).delete()\n\t\telse:\n\t\t\t_handle_crud_like_event(target_cls=models.Card, event=event)", "response": "Handle customer payment - source events."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nhandling updates to other object events.", "response": "def other_object_webhook_handler(event):\n\t\"\"\"Handle updates to transfer, charge, invoice, invoiceitem, plan, product and source objects.\n\n\tDocs for:\n\t- charge: https://stripe.com/docs/api#charges\n\t- coupon: https://stripe.com/docs/api#coupons\n\t- invoice: https://stripe.com/docs/api#invoices\n\t- invoiceitem: https://stripe.com/docs/api#invoiceitems\n\t- plan: https://stripe.com/docs/api#plans\n\t- product: https://stripe.com/docs/api#products\n\t- source: https://stripe.com/docs/api#sources\n\t\"\"\"\n\n\tif event.parts[:2] == [\"charge\", \"dispute\"]:\n\t\t# Do not attempt to handle charge.dispute.* events.\n\t\t# We do not have a Dispute model yet.\n\t\ttarget_cls = models.Dispute\n\telse:\n\t\ttarget_cls = {\n\t\t\t\"charge\": models.Charge,\n\t\t\t\"coupon\": models.Coupon,\n\t\t\t\"invoice\": models.Invoice,\n\t\t\t\"invoiceitem\": models.InvoiceItem,\n\t\t\t\"plan\": models.Plan,\n\t\t\t\"product\": models.Product,\n\t\t\t\"transfer\": models.Transfer,\n\t\t\t\"source\": models.Source,\n\t\t}.get(event.category)\n\n\t_handle_crud_like_event(target_cls=target_cls, event=event)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndetermine if the event verb is a crud_type event.", "response": "def determine(cls, event, verb=None, exact=False):\n\t\t\"\"\"\n\t\tDetermine if the event verb is a crud_type (without the 'R') event.\n\n\t\t:param verb: The event verb to examine.\n\t\t:type verb: string (``str``/`unicode``)\n\t\t:param exact: If True, match crud_type to event verb string exactly.\n\t\t:param type: ``bool``\n\t\t:returns: The CrudType state object.\n\t\t:rtype: ``CrudType``\n\t\t\"\"\"\n\t\tverb = verb or event.verb\n\n\t\tdef check(crud_type_event):\n\t\t\tif exact:\n\t\t\t\treturn verb == crud_type_event\n\t\t\telse:\n\t\t\t\treturn verb.endswith(crud_type_event)\n\n\t\tcreated = updated = deleted = False\n\n\t\tif check(\"updated\"):\n\t\t\tupdated = True\n\t\telif check(\"created\"):\n\t\t\tcreated = True\n\t\telif check(\"deleted\"):\n\t\t\tdeleted = True\n\n\t\treturn cls(created=created, updated=updated, deleted=deleted)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if the request is matching the rule.", "response": "def is_matching_rule(self, request):\n\t\t\"\"\"Check according to the rules defined in the class docstring.\"\"\"\n\t\t# First, if in DEBUG mode and with django-debug-toolbar, we skip\n\t\t#   this entire process.\n\t\tif settings.DEBUG and request.path.startswith(\"/__debug__\"):\n\t\t\treturn True\n\n\t\t# Second we check against matches\n\t\tmatch = resolve(request.path, getattr(request, \"urlconf\", settings.ROOT_URLCONF))\n\t\tif \"({0})\".format(match.app_name) in EXEMPT:\n\t\t\treturn True\n\n\t\tif \"[{0}]\".format(match.namespace) in EXEMPT:\n\t\t\treturn True\n\n\t\tif \"{0}:{1}\".format(match.namespace, match.url_name) in EXEMPT:\n\t\t\treturn True\n\n\t\tif match.url_name in EXEMPT:\n\t\t\treturn True\n\n\t\t# Third, we check wildcards:\n\t\tfor exempt in [x for x in EXEMPT if x.startswith(\"fn:\")]:\n\t\t\texempt = exempt.replace(\"fn:\", \"\")\n\t\t\tif fnmatch.fnmatch(request.path, exempt):\n\t\t\t\treturn True\n\n\t\treturn False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nredirecting to the subscribe page if the user lacks an active subscription.", "response": "def check_subscription(self, request):\n\t\t\"\"\"Redirect to the subscribe page if the user lacks an active subscription.\"\"\"\n\t\tsubscriber = subscriber_request_callback(request)\n\n\t\tif not subscriber_has_active_subscription(subscriber):\n\t\t\tif not SUBSCRIPTION_REDIRECT:\n\t\t\t\traise ImproperlyConfigured(\"DJSTRIPE_SUBSCRIPTION_REDIRECT is not set.\")\n\t\t\treturn redirect(SUBSCRIPTION_REDIRECT)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns Subscriptions not in trial status between a certain time range.", "response": "def started_during(self, year, month):\n\t\t\"\"\"Return Subscriptions not in trial status between a certain time range.\"\"\"\n\t\treturn self.exclude(status=\"trialing\").filter(start__year=year, start__month=month)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef canceled_during(self, year, month):\n\t\treturn self.canceled().filter(canceled_at__year=year, canceled_at__month=month)", "response": "Return Subscriptions canceled during a certain time range."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn started_during Subscriptions with plan counts annotated.", "response": "def started_plan_summary_for(self, year, month):\n\t\t\"\"\"Return started_during Subscriptions with plan counts annotated.\"\"\"\n\t\treturn (\n\t\t\tself.started_during(year, month)\n\t\t\t.values(\"plan\")\n\t\t\t.order_by()\n\t\t\t.annotate(count=models.Count(\"plan\"))\n\t\t)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn active Subscriptions with plan counts annotated.", "response": "def active_plan_summary(self):\n\t\t\"\"\"Return active Subscriptions with plan counts annotated.\"\"\"\n\t\treturn self.active().values(\"plan\").order_by().annotate(count=models.Count(\"plan\"))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning Subscriptions canceled within a time range with plan counts annotated.", "response": "def canceled_plan_summary_for(self, year, month):\n\t\t\"\"\"Return Subscriptions canceled within a time range with plan counts annotated.\"\"\"\n\t\treturn (\n\t\t\tself.canceled_during(year, month)\n\t\t\t.values(\"plan\")\n\t\t\t.order_by()\n\t\t\t.annotate(count=models.Count(\"plan\"))\n\t\t)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn number of canceled Subscriptions divided by active Subscriptions.", "response": "def churn(self):\n\t\t\"\"\"Return number of canceled Subscriptions divided by active Subscriptions.\"\"\"\n\t\tcanceled = self.canceled().count()\n\t\tactive = self.active().count()\n\t\treturn decimal.Decimal(str(canceled)) / decimal.Decimal(str(active))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef during(self, year, month):\n\t\treturn self.filter(created__year=year, created__month=month)", "response": "Return Transfers between a certain time range."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef paid_totals_for(self, year, month):\n\t\treturn self.during(year, month).aggregate(total_amount=models.Sum(\"amount\"))", "response": "Return paid Transfers during a certain year month with total amounts annotated."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef paid_totals_for(self, year, month):\n\t\treturn (\n\t\t\tself.during(year, month)\n\t\t\t.filter(paid=True)\n\t\t\t.aggregate(\n\t\t\t\ttotal_amount=models.Sum(\"amount\"), total_refunded=models.Sum(\"amount_refunded\")\n\t\t\t)\n\t\t)", "response": "Return paid Charges during a certain year month with total amount fee and refunded annotated."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_stripe_dashboard_url(self):\n\t\tif not self.stripe_dashboard_item_name or not self.id:\n\t\t\treturn \"\"\n\t\telse:\n\t\t\treturn \"{base_url}{item}/{id}\".format(\n\t\t\t\tbase_url=self._get_base_stripe_dashboard_url(),\n\t\t\t\titem=self.stripe_dashboard_item_name,\n\t\t\t\tid=self.id,\n\t\t\t)", "response": "Get the stripe dashboard url for this object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncall the stripe API s retrieve operation for this model.", "response": "def api_retrieve(self, api_key=None):\n\t\t\"\"\"\n\t\tCall the stripe API's retrieve operation for this model.\n\n\t\t:param api_key: The api key to use for this request. Defaults to settings.STRIPE_SECRET_KEY.\n\t\t:type api_key: string\n\t\t\"\"\"\n\t\tapi_key = api_key or self.default_api_key\n\n\t\treturn self.stripe_class.retrieve(\n\t\t\tid=self.id, api_key=api_key, expand=self.expand_fields\n\t\t)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalling the stripe API s list operation for this model.", "response": "def api_list(cls, api_key=djstripe_settings.STRIPE_SECRET_KEY, **kwargs):\n\t\t\"\"\"\n\t\tCall the stripe API's list operation for this model.\n\n\t\t:param api_key: The api key to use for this request. Defaults to djstripe_settings.STRIPE_SECRET_KEY.\n\t\t:type api_key: string\n\n\t\tSee Stripe documentation for accepted kwargs for each object.\n\n\t\t:returns: an iterator over all items in the query\n\t\t\"\"\"\n\n\t\treturn cls.stripe_class.list(api_key=api_key, **kwargs).auto_paging_iter()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _api_create(cls, api_key=djstripe_settings.STRIPE_SECRET_KEY, **kwargs):\n\n\t\treturn cls.stripe_class.create(api_key=api_key, **kwargs)", "response": "Call the stripe API s create operation for this model."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _api_delete(self, api_key=None, **kwargs):\n\t\tapi_key = api_key or self.default_api_key\n\n\t\treturn self.api_retrieve(api_key=api_key).delete(**kwargs)", "response": "Call the stripe API s delete operation for this model"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nextracts stripe id from stripe field data", "response": "def _id_from_data(cls, data):\n\t\t\"\"\"\n\t\tExtract stripe id from stripe field data\n\t\t:param data:\n\t\t:return:\n\t\t\"\"\"\n\n\t\tif isinstance(data, str):\n\t\t\t# data like \"sub_6lsC8pt7IcFpjA\"\n\t\t\tid_ = data\n\t\telif data:\n\t\t\t# data like {\"id\": sub_6lsC8pt7IcFpjA\", ...}\n\t\t\tid_ = data.get(\"id\")\n\t\telse:\n\t\t\tid_ = None\n\n\t\treturn id_"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget called by this object's create and sync methods just after save. Use this to populate fields after the model is saved. :param cls: The target class for the instantiated object. :param data: The data dictionary received from the Stripe API. :type data: dict", "response": "def _attach_objects_post_save_hook(self, cls, data, pending_relations=None):\n\t\t\"\"\"\n\t\tGets called by this object's create and sync methods just after save.\n\t\tUse this to populate fields after the model is saved.\n\n\t\t:param cls: The target class for the instantiated object.\n\t\t:param data: The data dictionary received from the Stripe API.\n\t\t:type data: dict\n\t\t\"\"\"\n\n\t\tunprocessed_pending_relations = []\n\t\tif pending_relations is not None:\n\t\t\tfor post_save_relation in pending_relations:\n\t\t\t\tobject_id, field, id_ = post_save_relation\n\n\t\t\t\tif self.id == id_:\n\t\t\t\t\t# the target instance now exists\n\t\t\t\t\ttarget = field.model.objects.get(id=object_id)\n\t\t\t\t\tsetattr(target, field.name, self)\n\t\t\t\t\ttarget.save()\n\n\t\t\t\t\tif django.VERSION < (2, 1):\n\t\t\t\t\t\t# refresh_from_db doesn't clear related objects cache on django<2.1\n\t\t\t\t\t\t# instead manually clear the instance cache so refresh_from_db will reload it\n\t\t\t\t\t\tfor field in self._meta.concrete_fields:\n\t\t\t\t\t\t\tif field.is_relation and field.is_cached(self):\n\t\t\t\t\t\t\t\tfield.delete_cached_value(self)\n\n\t\t\t\t\t# reload so that indirect relations back to this object - eg self.charge.invoice = self are set\n\t\t\t\t\t# TODO - reverse the field reference here to avoid hitting the DB?\n\t\t\t\t\tself.refresh_from_db()\n\t\t\t\telse:\n\t\t\t\t\tunprocessed_pending_relations.append(post_save_relation)\n\n\t\t\tif len(pending_relations) != len(unprocessed_pending_relations):\n\t\t\t\t# replace in place so passed in list is updated in calling method\n\t\t\t\tpending_relations[:] = unprocessed_pending_relations"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _create_from_stripe_object(\n\t\tcls, data, current_ids=None, pending_relations=None, save=True\n\t):\n\t\t\"\"\"\n\t\tInstantiates a model instance using the provided data object received\n\t\tfrom Stripe, and saves it to the database if specified.\n\n\t\t:param data: The data dictionary received from the Stripe API.\n\t\t:type data: dict\n\t\t:param current_ids: stripe ids of objects that are currently being processed\n\t\t:type current_ids: set\n\t\t:param pending_relations: list of tuples of relations to be attached post-save\n\t\t:type pending_relations: list\n\t\t:param save: If True, the object is saved after instantiation.\n\t\t:type save: bool\n\t\t:returns: The instantiated object.\n\t\t\"\"\"\n\n\t\tinstance = cls(\n\t\t\t**cls._stripe_object_to_record(\n\t\t\t\tdata, current_ids=current_ids, pending_relations=pending_relations\n\t\t\t)\n\t\t)\n\t\tinstance._attach_objects_hook(cls, data)\n\n\t\tif save:\n\t\t\tinstance.save(force_insert=True)\n\n\t\tinstance._attach_objects_post_save_hook(\n\t\t\tcls, data, pending_relations=pending_relations\n\t\t)\n\n\t\treturn instance", "response": "Create a new object from the provided data object received from Stripe API."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving InvoiceItems for an invoice. If the invoice item doesn't exist already then it is created. If the invoice is an upcoming invoice that doesn't persist to the database (i.e. ephemeral) then the invoice items are also not saved. :param target_cls: The target class to instantiate per invoice item. :type target_cls: ``InvoiceItem`` :param data: The data dictionary received from the Stripe API. :type data: dict :param invoice: The invoice object that should hold the invoice items. :type invoice: ``djstripe.models.Invoice``", "response": "def _stripe_object_to_invoice_items(cls, target_cls, data, invoice):\n\t\t\"\"\"\n\t\tRetrieves InvoiceItems for an invoice.\n\n\t\tIf the invoice item doesn't exist already then it is created.\n\n\t\tIf the invoice is an upcoming invoice that doesn't persist to the\n\t\tdatabase (i.e. ephemeral) then the invoice items are also not saved.\n\n\t\t:param target_cls: The target class to instantiate per invoice item.\n\t\t:type target_cls: ``InvoiceItem``\n\t\t:param data: The data dictionary received from the Stripe API.\n\t\t:type data: dict\n\t\t:param invoice: The invoice object that should hold the invoice items.\n\t\t:type invoice: ``djstripe.models.Invoice``\n\t\t\"\"\"\n\n\t\tlines = data.get(\"lines\")\n\t\tif not lines:\n\t\t\treturn []\n\n\t\tinvoiceitems = []\n\t\tfor line in lines.get(\"data\", []):\n\t\t\tif invoice.id:\n\t\t\t\tsave = True\n\t\t\t\tline.setdefault(\"invoice\", invoice.id)\n\n\t\t\t\tif line.get(\"type\") == \"subscription\":\n\t\t\t\t\t# Lines for subscriptions need to be keyed based on invoice and\n\t\t\t\t\t# subscription, because their id is *just* the subscription\n\t\t\t\t\t# when received from Stripe. This means that future updates to\n\t\t\t\t\t# a subscription will change previously saved invoices - Doing\n\t\t\t\t\t# the composite key avoids this.\n\t\t\t\t\tif not line[\"id\"].startswith(invoice.id):\n\t\t\t\t\t\tline[\"id\"] = \"{invoice_id}-{subscription_id}\".format(\n\t\t\t\t\t\t\tinvoice_id=invoice.id, subscription_id=line[\"id\"]\n\t\t\t\t\t\t)\n\t\t\telse:\n\t\t\t\t# Don't save invoice items for ephemeral invoices\n\t\t\t\tsave = False\n\n\t\t\tline.setdefault(\"customer\", invoice.customer.id)\n\t\t\tline.setdefault(\"date\", int(dateformat.format(invoice.date, \"U\")))\n\n\t\t\titem, _ = target_cls._get_or_create_from_stripe_object(\n\t\t\t\tline, refetch=False, save=save\n\t\t\t)\n\t\t\tinvoiceitems.append(item)\n\n\t\treturn invoiceitems"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _stripe_object_to_subscription_items(cls, target_cls, data, subscription):\n\n\t\titems = data.get(\"items\")\n\t\tif not items:\n\t\t\treturn []\n\n\t\tsubscriptionitems = []\n\t\tfor item_data in items.get(\"data\", []):\n\t\t\titem, _ = target_cls._get_or_create_from_stripe_object(item_data, refetch=False)\n\t\t\tsubscriptionitems.append(item)\n\n\t\treturn subscriptionitems", "response": "Helper function to convert a Stripe object to a list of SubscriptionItems."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving Refunds for a charge :param target_cls: The target class to instantiate per invoice item. :type target_cls: ``Refund`` :param data: The data dictionary received from the Stripe API. :type data: dict :param charge: The charge object that refunds are for. :type invoice: ``djstripe.models.Refund`` :return:", "response": "def _stripe_object_to_refunds(cls, target_cls, data, charge):\n\t\t\"\"\"\n\t\tRetrieves Refunds for a charge\n\t\t:param target_cls: The target class to instantiate per invoice item.\n\t\t:type target_cls: ``Refund``\n\t\t:param data: The data dictionary received from the Stripe API.\n\t\t:type data: dict\n\t\t:param charge: The charge object that refunds are for.\n\t\t:type invoice: ``djstripe.models.Refund``\n\t\t:return:\n\t\t\"\"\"\n\n\t\trefunds = data.get(\"refunds\")\n\t\tif not refunds:\n\t\t\treturn []\n\n\t\trefund_objs = []\n\t\tfor refund_data in refunds.get(\"data\", []):\n\t\t\titem, _ = target_cls._get_or_create_from_stripe_object(refund_data, refetch=False)\n\t\t\trefund_objs.append(item)\n\n\t\treturn refund_objs"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sync_from_stripe_data(cls, data, field_name=\"id\"):\n\t\tcurrent_ids = set()\n\n\t\tif data.get(field_name, None):\n\t\t\t# stop nested objects from trying to retrieve this object before initial sync is complete\n\t\t\tcurrent_ids.add(cls._id_from_data(data.get(field_name)))\n\n\t\tinstance, created = cls._get_or_create_from_stripe_object(\n\t\t\tdata, field_name=field_name, current_ids=current_ids\n\t\t)\n\n\t\tif not created:\n\t\t\tinstance._sync(cls._stripe_object_to_record(data))\n\t\t\tinstance._attach_objects_hook(cls, data)\n\t\t\tinstance.save()\n\t\t\tinstance._attach_objects_post_save_hook(cls, data)\n\n\t\treturn instance", "response": "Syncs this object from the stripe data provided."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _calculate_refund_amount(self, amount=None):\n\t\teligible_to_refund = self.amount - (self.amount_refunded or 0)\n\t\tif amount:\n\t\t\tamount_to_refund = min(eligible_to_refund, amount)\n\t\telse:\n\t\t\tamount_to_refund = eligible_to_refund\n\t\treturn int(amount_to_refund * 100)", "response": "Calculate the amount that can be refunded in CENTS\n\t"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef refund(self, amount=None, reason=None):\n\t\tcharge_obj = self.api_retrieve().refund(\n\t\t\tamount=self._calculate_refund_amount(amount=amount), reason=reason\n\t\t)\n\t\treturn self.__class__.sync_from_stripe_data(charge_obj)", "response": "Initiate a refund. If amount is not provided, then this will be a full refund.\n\n\t\t:param amount: A positive decimal amount representing how much of this charge\n\t\t\tto refund. Can only refund up to the unrefunded amount remaining of the charge.\n\t\t:trye amount: Decimal\n\t\t:param reason: String indicating the reason for the refund. If set, possible values\n\t\t\tare ``duplicate``, ``fraudulent``, and ``requested_by_customer``. Specifying\n\t\t\t``fraudulent`` as the reason when you believe the charge to be fraudulent will\n\t\t\thelp Stripe improve their fraud detection algorithms.\n\n\t\t:return: Stripe charge object\n\t\t:rtype: dict"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncapturing the payment of an existing uncaptured charge.", "response": "def capture(self):\n\t\t\"\"\"\n\t\tCapture the payment of an existing, uncaptured, charge.\n\t\tThis is the second half of the two-step payment flow, where first you\n\t\tcreated a charge with the capture option set to False.\n\n\t\tSee https://stripe.com/docs/api#capture_charge\n\t\t\"\"\"\n\n\t\tcaptured_charge = self.api_retrieve().capture()\n\t\treturn self.__class__.sync_from_stripe_data(captured_charge)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_or_create(cls, subscriber, livemode=djstripe_settings.STRIPE_LIVE_MODE):\n\n\t\ttry:\n\t\t\treturn Customer.objects.get(subscriber=subscriber, livemode=livemode), False\n\t\texcept Customer.DoesNotExist:\n\t\t\taction = \"create:{}\".format(subscriber.pk)\n\t\t\tidempotency_key = djstripe_settings.get_idempotency_key(\"customer\", action, livemode)\n\t\t\treturn cls.create(subscriber, idempotency_key=idempotency_key), True", "response": "Get or create a dj - stripe customer."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsubscribes this customer to a plan. :param plan: The plan to which to subscribe the customer. :type plan: Plan or string (plan ID) :param application_fee_percent: This represents the percentage of the subscription invoice subtotal that will be transferred to the application owner's Stripe account. The request must be made with an OAuth key in order to set an application fee percentage. :type application_fee_percent: Decimal. Precision is 2; anything more will be ignored. A positive decimal between 1 and 100. :param coupon: The code of the coupon to apply to this subscription. A coupon applied to a subscription will only affect invoices created for that particular subscription. :type coupon: string :param quantity: The quantity applied to this subscription. Default is 1. :type quantity: integer :param metadata: A set of key/value pairs useful for storing additional information. :type metadata: dict :param tax_percent: This represents the percentage of the subscription invoice subtotal that will be calculated and added as tax to the final amount each billing period. :type tax_percent: Decimal. Precision is 2; anything more will be ignored. A positive decimal between 1 and 100. :param billing_cycle_anchor: A future timestamp to anchor the subscription\u2019s billing cycle. This is used to determine the date of the first full invoice, and, for plans with month or year intervals, the day of the month for subsequent invoices. :type billing_cycle_anchor: datetime :param trial_end: The end datetime of the trial period the customer will get before being charged for the first time. If set, this will override the default trial period of the plan the customer is being subscribed to. The special value ``now`` can be provided to end the customer's trial immediately. :type trial_end: datetime :param charge_immediately: Whether or not to charge for the subscription upon creation. If False, an invoice will be created at the end of this period. :type charge_immediately: boolean :param trial_from_plan: Indicates if a plan\u2019s trial_period_days should be applied to the subscription. Setting trial_end per subscription is preferred, and this defaults to false. Setting this flag to true together with trial_end is not allowed. :type trial_from_plan: boolean :param trial_period_days: Integer representing the number of trial period days before the customer is charged for the first time. This will always overwrite any trials that might apply via a subscribed plan. :type trial_period_days: integer .. Notes: .. ``charge_immediately`` is only available on ``Customer.subscribe()`` .. if you're using ``Customer.subscribe()`` instead of ``Customer.subscribe()``, ``plan`` \\ can only be a string", "response": "def subscribe(\n\t\tself,\n\t\tplan,\n\t\tcharge_immediately=True,\n\t\tapplication_fee_percent=None,\n\t\tcoupon=None,\n\t\tquantity=None,\n\t\tmetadata=None,\n\t\ttax_percent=None,\n\t\tbilling_cycle_anchor=None,\n\t\ttrial_end=None,\n\t\ttrial_from_plan=None,\n\t\ttrial_period_days=None,\n\t):\n\t\t\"\"\"\n\t\tSubscribes this customer to a plan.\n\n\t\t:param plan: The plan to which to subscribe the customer.\n\t\t:type plan: Plan or string (plan ID)\n\t\t:param application_fee_percent: This represents the percentage of the subscription invoice subtotal\n\t\t\tthat will be transferred to the application owner's Stripe account.\n\t\t\tThe request must be made with an OAuth key in order to set an\n\t\t\tapplication fee percentage.\n\t\t:type application_fee_percent: Decimal. Precision is 2; anything more will be ignored. A positive\n\t\t\tdecimal between 1 and 100.\n\t\t:param coupon: The code of the coupon to apply to this subscription. A coupon applied to a subscription\n\t\t\twill only affect invoices created for that particular subscription.\n\t\t:type coupon: string\n\t\t:param quantity: The quantity applied to this subscription. Default is 1.\n\t\t:type quantity: integer\n\t\t:param metadata: A set of key/value pairs useful for storing additional information.\n\t\t:type metadata: dict\n\t\t:param tax_percent: This represents the percentage of the subscription invoice subtotal that will\n\t\t\tbe calculated and added as tax to the final amount each billing period.\n\t\t:type tax_percent: Decimal. Precision is 2; anything more will be ignored. A positive decimal\n\t\t\tbetween 1 and 100.\n\t\t:param billing_cycle_anchor: A future timestamp to anchor the subscription\u2019s billing cycle.\n\t\t\tThis is used to determine the date of the first full invoice, and,\n\t\t\tfor plans with month or year intervals, the day of the month for\n\t\t\tsubsequent invoices.\n\t\t:type billing_cycle_anchor: datetime\n\t\t:param trial_end: The end datetime of the trial period the customer will get before being charged for\n\t\t\tthe first time. If set, this will override the default trial period of the plan the\n\t\t\tcustomer is being subscribed to. The special value ``now`` can be provided to end\n\t\t\tthe customer's trial immediately.\n\t\t:type trial_end: datetime\n\t\t:param charge_immediately: Whether or not to charge for the subscription upon creation. If False, an\n\t\t\tinvoice will be created at the end of this period.\n\t\t:type charge_immediately: boolean\n\t\t:param trial_from_plan: Indicates if a plan\u2019s trial_period_days should be applied to the subscription.\n\t\t\tSetting trial_end per subscription is preferred, and this defaults to false.\n\t\t\tSetting this flag to true together with trial_end is not allowed.\n\t\t:type trial_from_plan: boolean\n\t\t:param trial_period_days: Integer representing the number of trial period days before the customer is\n\t\t\tcharged for the first time. This will always overwrite any trials that might\n\t\t\tapply via a subscribed plan.\n\t\t:type trial_period_days: integer\n\n\t\t.. Notes:\n\t\t.. ``charge_immediately`` is only available on ``Customer.subscribe()``\n\t\t.. if you're using ``Customer.subscribe()`` instead of ``Customer.subscribe()``, ``plan`` \\\n\t\tcan only be a string\n\t\t\"\"\"\n\t\tfrom .billing import Subscription\n\n\t\t# Convert Plan to id\n\t\tif isinstance(plan, StripeModel):\n\t\t\tplan = plan.id\n\n\t\tstripe_subscription = Subscription._api_create(\n\t\t\tplan=plan,\n\t\t\tcustomer=self.id,\n\t\t\tapplication_fee_percent=application_fee_percent,\n\t\t\tcoupon=coupon,\n\t\t\tquantity=quantity,\n\t\t\tmetadata=metadata,\n\t\t\tbilling_cycle_anchor=billing_cycle_anchor,\n\t\t\ttax_percent=tax_percent,\n\t\t\ttrial_end=trial_end,\n\t\t\ttrial_from_plan=trial_from_plan,\n\t\t\ttrial_period_days=trial_period_days,\n\t\t)\n\n\t\tif charge_immediately:\n\t\t\tself.send_invoice()\n\n\t\treturn Subscription.sync_from_stripe_data(stripe_subscription)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a charge for this customer. Parameters not implemented: * **receipt_email** - Since this is a charge on a customer, the customer's email address is used. :param amount: The amount to charge. :type amount: Decimal. Precision is 2; anything more will be ignored. :param currency: 3-letter ISO code for currency :type currency: string :param application_fee: A fee that will be applied to the charge and transfered to the platform owner's account. :type application_fee: Decimal. Precision is 2; anything more will be ignored. :param capture: Whether or not to immediately capture the charge. When false, the charge issues an authorization (or pre-authorization), and will need to be captured later. Uncaptured charges expire in 7 days. Default is True :type capture: bool :param description: An arbitrary string. :type description: string :param destination: An account to make the charge on behalf of. :type destination: Account :param metadata: A set of key/value pairs useful for storing additional information. :type metadata: dict :param shipping: Shipping information for the charge. :type shipping: dict :param source: The source to use for this charge. Must be a source attributed to this customer. If None, the customer's default source is used. Can be either the id of the source or the source object itself. :type source: string, Source :param statement_descriptor: An arbitrary string to be displayed on the customer's credit card statement. :type statement_descriptor: string", "response": "def charge(\n\t\tself,\n\t\tamount,\n\t\tcurrency=None,\n\t\tapplication_fee=None,\n\t\tcapture=None,\n\t\tdescription=None,\n\t\tdestination=None,\n\t\tmetadata=None,\n\t\tshipping=None,\n\t\tsource=None,\n\t\tstatement_descriptor=None,\n\t\tidempotency_key=None,\n\t):\n\t\t\"\"\"\n\t\tCreates a charge for this customer.\n\n\t\tParameters not implemented:\n\n\t\t* **receipt_email** - Since this is a charge on a customer, the customer's email address is used.\n\n\n\t\t:param amount: The amount to charge.\n\t\t:type amount: Decimal. Precision is 2; anything more will be ignored.\n\t\t:param currency: 3-letter ISO code for currency\n\t\t:type currency: string\n\t\t:param application_fee: A fee that will be applied to the charge and transfered to the platform owner's\n\t\t\taccount.\n\t\t:type application_fee: Decimal. Precision is 2; anything more will be ignored.\n\t\t:param capture: Whether or not to immediately capture the charge. When false, the charge issues an\n\t\t\tauthorization (or pre-authorization), and will need to be captured later. Uncaptured\n\t\t\tcharges expire in 7 days. Default is True\n\t\t:type capture: bool\n\t\t:param description: An arbitrary string.\n\t\t:type description: string\n\t\t:param destination: An account to make the charge on behalf of.\n\t\t:type destination: Account\n\t\t:param metadata: A set of key/value pairs useful for storing additional information.\n\t\t:type metadata: dict\n\t\t:param shipping: Shipping information for the charge.\n\t\t:type shipping: dict\n\t\t:param source: The source to use for this charge. Must be a source attributed to this customer. If None,\n\t\t\tthe customer's default source is used. Can be either the id of the source or the source object\n\t\t\titself.\n\t\t:type source: string, Source\n\t\t:param statement_descriptor: An arbitrary string to be displayed on the customer's credit card statement.\n\t\t:type statement_descriptor: string\n\t\t\"\"\"\n\n\t\tif not isinstance(amount, decimal.Decimal):\n\t\t\traise ValueError(\"You must supply a decimal value representing dollars.\")\n\n\t\t# TODO: better default detection (should charge in customer default)\n\t\tcurrency = currency or \"usd\"\n\n\t\t# Convert Source to id\n\t\tif source and isinstance(source, StripeModel):\n\t\t\tsource = source.id\n\n\t\tstripe_charge = Charge._api_create(\n\t\t\tamount=int(amount * 100),  # Convert dollars into cents\n\t\t\tcurrency=currency,\n\t\t\tapplication_fee=int(application_fee * 100)\n\t\t\tif application_fee\n\t\t\telse None,  # Convert dollars into cents\n\t\t\tcapture=capture,\n\t\t\tdescription=description,\n\t\t\tdestination=destination,\n\t\t\tmetadata=metadata,\n\t\t\tshipping=shipping,\n\t\t\tcustomer=self.id,\n\t\t\tsource=source,\n\t\t\tstatement_descriptor=statement_descriptor,\n\t\t\tidempotency_key=idempotency_key,\n\t\t)\n\n\t\treturn Charge.sync_from_stripe_data(stripe_charge)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds an arbitrary charge or credit to the customer's upcoming invoice. Different than creating a charge. Charges are separate bills that get processed immediately. Invoice items are appended to the customer's next invoice. This is extremely useful when adding surcharges to subscriptions. :param amount: The amount to charge. :type amount: Decimal. Precision is 2; anything more will be ignored. :param currency: 3-letter ISO code for currency :type currency: string :param description: An arbitrary string. :type description: string :param discountable: Controls whether discounts apply to this invoice item. Defaults to False for prorations or negative invoice items, and True for all other invoice items. :type discountable: boolean :param invoice: An existing invoice to add this invoice item to. When left blank, the invoice item will be added to the next upcoming scheduled invoice. Use this when adding invoice items in response to an ``invoice.created`` webhook. You cannot add an invoice item to an invoice that has already been paid, attempted or closed. :type invoice: Invoice or string (invoice ID) :param metadata: A set of key/value pairs useful for storing additional information. :type metadata: dict :param subscription: A subscription to add this invoice item to. When left blank, the invoice item will be be added to the next upcoming scheduled invoice. When set, scheduled invoices for subscriptions other than the specified subscription will ignore the invoice item. Use this when you want to express that an invoice item has been accrued within the context of a particular subscription. :type subscription: Subscription or string (subscription ID) .. Notes: .. if you're using ``Customer.add_invoice_item()`` instead of ``Customer.add_invoice_item()``, \\ ``invoice`` and ``subscriptions`` can only be strings", "response": "def add_invoice_item(\n\t\tself,\n\t\tamount,\n\t\tcurrency,\n\t\tdescription=None,\n\t\tdiscountable=None,\n\t\tinvoice=None,\n\t\tmetadata=None,\n\t\tsubscription=None,\n\t):\n\t\t\"\"\"\n\t\tAdds an arbitrary charge or credit to the customer's upcoming invoice.\n\t\tDifferent than creating a charge. Charges are separate bills that get\n\t\tprocessed immediately. Invoice items are appended to the customer's next\n\t\tinvoice. This is extremely useful when adding surcharges to subscriptions.\n\n\t\t:param amount: The amount to charge.\n\t\t:type amount: Decimal. Precision is 2; anything more will be ignored.\n\t\t:param currency: 3-letter ISO code for currency\n\t\t:type currency: string\n\t\t:param description: An arbitrary string.\n\t\t:type description: string\n\t\t:param discountable: Controls whether discounts apply to this invoice item. Defaults to False for\n\t\t\tprorations or negative invoice items, and True for all other invoice items.\n\t\t:type discountable: boolean\n\t\t:param invoice: An existing invoice to add this invoice item to. When left blank, the invoice\n\t\t\titem will be added to the next upcoming scheduled invoice. Use this when adding\n\t\t\tinvoice items in response to an ``invoice.created`` webhook. You cannot add an invoice\n\t\t\titem to an invoice that has already been paid, attempted or closed.\n\t\t:type invoice: Invoice or string (invoice ID)\n\t\t:param metadata: A set of key/value pairs useful for storing additional information.\n\t\t:type metadata: dict\n\t\t:param subscription: A subscription to add this invoice item to. When left blank, the invoice\n\t\t\titem will be be added to the next upcoming scheduled invoice. When set,\n\t\t\tscheduled invoices for subscriptions other than the specified subscription\n\t\t\twill ignore the invoice item. Use this when you want to express that an\n\t\t\tinvoice item has been accrued within the context of a particular subscription.\n\t\t:type subscription: Subscription or string (subscription ID)\n\n\t\t.. Notes:\n\t\t.. if you're using ``Customer.add_invoice_item()`` instead of ``Customer.add_invoice_item()``, \\\n\t\t``invoice`` and ``subscriptions`` can only be strings\n\t\t\"\"\"\n\t\tfrom .billing import InvoiceItem\n\n\t\tif not isinstance(amount, decimal.Decimal):\n\t\t\traise ValueError(\"You must supply a decimal value representing dollars.\")\n\n\t\t# Convert Invoice to id\n\t\tif invoice is not None and isinstance(invoice, StripeModel):\n\t\t\tinvoice = invoice.id\n\n\t\t# Convert Subscription to id\n\t\tif subscription is not None and isinstance(subscription, StripeModel):\n\t\t\tsubscription = subscription.id\n\n\t\tstripe_invoiceitem = InvoiceItem._api_create(\n\t\t\tamount=int(amount * 100),  # Convert dollars into cents\n\t\t\tcurrency=currency,\n\t\t\tcustomer=self.id,\n\t\t\tdescription=description,\n\t\t\tdiscountable=discountable,\n\t\t\tinvoice=invoice,\n\t\t\tmetadata=metadata,\n\t\t\tsubscription=subscription,\n\t\t)\n\n\t\treturn InvoiceItem.sync_from_stripe_data(stripe_invoiceitem)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_card(self, source, set_default=True):\n\t\tfrom .payment_methods import DjstripePaymentMethod\n\n\t\tstripe_customer = self.api_retrieve()\n\t\tnew_stripe_payment_method = stripe_customer.sources.create(source=source)\n\n\t\tif set_default:\n\t\t\tstripe_customer.default_source = new_stripe_payment_method[\"id\"]\n\t\t\tstripe_customer.save()\n\n\t\tnew_payment_method = DjstripePaymentMethod.from_stripe_object(\n\t\t\tnew_stripe_payment_method\n\t\t)\n\n\t\t# Change the default source\n\t\tif set_default:\n\t\t\tself.default_source = new_payment_method\n\t\t\tself.save()\n\n\t\treturn new_payment_method.resolve()", "response": "Adds a card to this customer s account."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef has_active_subscription(self, plan=None):\n\n\t\tif plan is None:\n\t\t\tvalid_subscriptions = self._get_valid_subscriptions()\n\n\t\t\tif len(valid_subscriptions) == 0:\n\t\t\t\treturn False\n\t\t\telif len(valid_subscriptions) == 1:\n\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\traise TypeError(\n\t\t\t\t\t\"plan cannot be None if more than one valid subscription exists for this customer.\"\n\t\t\t\t)\n\n\t\telse:\n\t\t\t# Convert Plan to id\n\t\t\tif isinstance(plan, StripeModel):\n\t\t\t\tplan = plan.id\n\n\t\t\treturn any(\n\t\t\t\t[\n\t\t\t\t\tsubscription.is_valid()\n\t\t\t\t\tfor subscription in self.subscriptions.filter(plan__id=plan)\n\t\t\t\t]\n\t\t\t)", "response": "Checks to see if this customer has an active subscription to the given plan."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef active_subscriptions(self):\n\t\treturn self.subscriptions.filter(\n\t\t\tstatus=enums.SubscriptionStatus.active, current_period_end__gt=timezone.now()\n\t\t)", "response": "Returns active subscriptions (subscriptions with an active status that end in the future)."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting this customer s subscription", "response": "def subscription(self):\n\t\t\"\"\"\n\t\tShortcut to get this customer's subscription.\n\n\t\t:returns: None if the customer has no subscriptions, the subscription if\n\t\t\tthe customer has a subscription.\n\t\t:raises MultipleSubscriptionException: Raised if the customer has multiple subscriptions.\n\t\t\tIn this case, use ``Customer.subscriptions`` instead.\n\t\t\"\"\"\n\n\t\tsubscriptions = self.valid_subscriptions\n\n\t\tif subscriptions.count() > 1:\n\t\t\traise MultipleSubscriptionException(\n\t\t\t\t\"This customer has multiple subscriptions. Use Customer.subscriptions \"\n\t\t\t\t\"to access them.\"\n\t\t\t)\n\t\telse:\n\t\t\treturn subscriptions.first()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsend the customer s latest invoice.", "response": "def send_invoice(self):\n\t\t\"\"\"\n\t\tPay and send the customer's latest invoice.\n\n\t\t:returns: True if an invoice was able to be created and paid, False otherwise\n\t\t\t(typically if there was nothing to invoice).\n\t\t\"\"\"\n\t\tfrom .billing import Invoice\n\n\t\ttry:\n\t\t\tinvoice = Invoice._api_create(customer=self.id)\n\t\t\tinvoice.pay()\n\t\t\treturn True\n\t\texcept InvalidRequestError:  # TODO: Check this for a more specific error message.\n\t\t\treturn False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nattempting to retry collecting payment on the customer s unpaid invoices.", "response": "def retry_unpaid_invoices(self):\n\t\t\"\"\" Attempt to retry collecting payment on the customer's unpaid invoices.\"\"\"\n\n\t\tself._sync_invoices()\n\t\tfor invoice in self.invoices.filter(paid=False, closed=False):\n\t\t\ttry:\n\t\t\t\tinvoice.retry()  # Always retry unpaid invoices\n\t\t\texcept InvalidRequestError as exc:\n\t\t\t\tif str(exc) != \"Invoice is already paid\":\n\t\t\t\t\traise"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a coupon to a Customer.", "response": "def add_coupon(self, coupon, idempotency_key=None):\n\t\t\"\"\"\n\t\tAdd a coupon to a Customer.\n\n\t\tThe coupon can be a Coupon object, or a valid Stripe Coupon ID.\n\t\t\"\"\"\n\t\tif isinstance(coupon, StripeModel):\n\t\t\tcoupon = coupon.id\n\n\t\tstripe_customer = self.api_retrieve()\n\t\tstripe_customer[\"coupon\"] = coupon\n\t\tstripe_customer.save(idempotency_key=idempotency_key)\n\t\treturn self.__class__.sync_from_stripe_data(stripe_customer)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef upcoming_invoice(self, **kwargs):\n\t\tfrom .billing import Invoice\n\n\t\tkwargs[\"customer\"] = self\n\t\treturn Invoice.upcoming(**kwargs)", "response": "Gets the upcoming preview invoice for this customer."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef invoke_webhook_handlers(self):\n\n\t\twebhooks.call_handlers(event=self)\n\n\t\tsignal = WEBHOOK_SIGNALS.get(self.type)\n\t\tif signal:\n\t\t\treturn signal.send(sender=Event, event=self)", "response": "Invoke any webhook handlers that have been registered for this event."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_callback_function(setting_name, default=None):\n\tfunc = getattr(settings, setting_name, None)\n\tif not func:\n\t\treturn default\n\n\tif callable(func):\n\t\treturn func\n\n\tif isinstance(func, str):\n\t\tfunc = import_string(func)\n\n\tif not callable(func):\n\t\traise ImproperlyConfigured(\"{name} must be callable.\".format(name=setting_name))\n\n\treturn func", "response": "Get a callback function based on a setting name."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_subscriber_model():\n\tmodel_name = get_subscriber_model_string()\n\n\t# Attempt a Django 1.7 app lookup\n\ttry:\n\t\tsubscriber_model = django_apps.get_model(model_name)\n\texcept ValueError:\n\t\traise ImproperlyConfigured(\n\t\t\t\"DJSTRIPE_SUBSCRIBER_MODEL must be of the form 'app_label.model_name'.\"\n\t\t)\n\texcept LookupError:\n\t\traise ImproperlyConfigured(\n\t\t\t\"DJSTRIPE_SUBSCRIBER_MODEL refers to model '{model}' \"\n\t\t\t\"that has not been installed.\".format(model=model_name)\n\t\t)\n\n\tif (\n\t\t\"email\" not in [field_.name for field_ in subscriber_model._meta.get_fields()]\n\t) and not hasattr(subscriber_model, \"email\"):\n\t\traise ImproperlyConfigured(\"DJSTRIPE_SUBSCRIBER_MODEL must have an email attribute.\")\n\n\tif model_name != settings.AUTH_USER_MODEL:\n\t\t# Custom user model detected. Make sure the callback is configured.\n\t\tfunc = get_callback_function(\"DJSTRIPE_SUBSCRIBER_MODEL_REQUEST_CALLBACK\")\n\t\tif not func:\n\t\t\traise ImproperlyConfigured(\n\t\t\t\t\"DJSTRIPE_SUBSCRIBER_MODEL_REQUEST_CALLBACK must be implemented \"\n\t\t\t\t\"if a DJSTRIPE_SUBSCRIBER_MODEL is defined.\"\n\t\t\t)\n\n\treturn subscriber_model", "response": "Get the subscriber model that is active in this project."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the API version to use for Stripe requests.", "response": "def set_stripe_api_version(version=None, validate=True):\n\t\"\"\"\n\tSet the desired API version to use for Stripe requests.\n\n\t:param version: The version to set for the Stripe API.\n\t:type version: ``str``\n\t:param validate: If True validate the value for the specified version).\n\t:type validate: ``bool``\n\t\"\"\"\n\tversion = version or get_stripe_api_version()\n\n\tif validate:\n\t\tvalid = validate_stripe_api_version(version)\n\t\tif not valid:\n\t\t\traise ValueError(\"Bad stripe API version: {}\".format(version))\n\n\tstripe.api_version = version"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef handle(self, *args, **options):\n\t\tqs = get_subscriber_model().objects.filter(djstripe_customers__isnull=True)\n\t\tcount = 0\n\t\ttotal = qs.count()\n\t\tfor subscriber in qs:\n\t\t\tcount += 1\n\t\t\tperc = int(round(100 * (float(count) / float(total))))\n\t\t\tprint(\n\t\t\t\t\"[{0}/{1} {2}%] Syncing {3} [{4}]\".format(\n\t\t\t\t\tcount, total, perc, subscriber.email, subscriber.pk\n\t\t\t\t)\n\t\t\t)\n\t\t\tsync_subscriber(subscriber)", "response": "Call sync_subscriber on Subscribers without customers associated to them."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nretrieve the status of the current process.", "response": "def _GetStatus(self):\n    \"\"\"Retrieves status information.\n\n    Returns:\n      dict[str, object]: status attributes, indexed by name.\n    \"\"\"\n    if self._analysis_mediator:\n      number_of_produced_event_tags = (\n          self._analysis_mediator.number_of_produced_event_tags)\n      number_of_produced_reports = (\n          self._analysis_mediator.number_of_produced_analysis_reports)\n    else:\n      number_of_produced_event_tags = None\n      number_of_produced_reports = None\n\n    if self._process_information:\n      used_memory = self._process_information.GetUsedMemory() or 0\n    else:\n      used_memory = 0\n\n    if self._memory_profiler:\n      self._memory_profiler.Sample('main', used_memory)\n\n    status = {\n        'display_name': '',\n        'identifier': self._name,\n        'number_of_consumed_event_tags': None,\n        'number_of_consumed_events': self._number_of_consumed_events,\n        'number_of_consumed_reports': None,\n        'number_of_consumed_sources': None,\n        'number_of_consumed_warnings': None,\n        'number_of_produced_event_tags': number_of_produced_event_tags,\n        'number_of_produced_events': None,\n        'number_of_produced_reports': number_of_produced_reports,\n        'number_of_produced_sources': None,\n        'number_of_produced_warnings': None,\n        'processing_status': self._status,\n        'task_identifier': None,\n        'used_memory': used_memory}\n\n    if self._status in (\n        definitions.STATUS_INDICATOR_ABORTED,\n        definitions.STATUS_INDICATOR_COMPLETED):\n      self._foreman_status_wait_event.set()\n\n    return status"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _Main(self):\n    self._StartProfiling(self._processing_configuration.profiling)\n\n    if self._serializers_profiler:\n      self._storage_writer.SetSerializersProfiler(self._serializers_profiler)\n\n    if self._storage_profiler:\n      self._storage_writer.SetStorageProfiler(self._storage_profiler)\n\n    logger.debug('Analysis plugin: {0!s} (PID: {1:d}) started'.format(\n        self._name, self._pid))\n\n    # Creating the threading event in the constructor will cause a pickle\n    # error on Windows when an analysis process is created.\n    self._foreman_status_wait_event = threading.Event()\n    self._status = definitions.STATUS_INDICATOR_ANALYZING\n\n    task = tasks.Task()\n    # TODO: temporary solution.\n    task.identifier = self._analysis_plugin.plugin_name\n\n    self._task = task\n\n    storage_writer = self._storage_writer.CreateTaskStorage(task)\n\n    if self._serializers_profiler:\n      storage_writer.SetSerializersProfiler(self._serializers_profiler)\n\n    if self._storage_profiler:\n      storage_writer.SetStorageProfiler(self._storage_profiler)\n\n    storage_writer.Open()\n\n    self._analysis_mediator = analysis_mediator.AnalysisMediator(\n        storage_writer, self._knowledge_base, data_location=self._data_location)\n\n    # TODO: set event_filter_expression in mediator.\n\n    storage_writer.WriteTaskStart()\n\n    try:\n      logger.debug(\n          '{0!s} (PID: {1:d}) started monitoring event queue.'.format(\n              self._name, self._pid))\n\n      while not self._abort:\n        try:\n          event = self._event_queue.PopItem()\n\n        except (errors.QueueClose, errors.QueueEmpty) as exception:\n          logger.debug('ConsumeItems exiting with exception {0:s}.'.format(\n              type(exception)))\n          break\n\n        if isinstance(event, plaso_queue.QueueAbort):\n          logger.debug('ConsumeItems exiting, dequeued QueueAbort object.')\n          break\n\n        self._ProcessEvent(self._analysis_mediator, event)\n\n        self._number_of_consumed_events += 1\n\n        if self._guppy_memory_profiler:\n          self._guppy_memory_profiler.Sample()\n\n      logger.debug(\n          '{0!s} (PID: {1:d}) stopped monitoring event queue.'.format(\n              self._name, self._pid))\n\n      if not self._abort:\n        self._status = definitions.STATUS_INDICATOR_REPORTING\n\n        self._analysis_mediator.ProduceAnalysisReport(self._analysis_plugin)\n\n    # All exceptions need to be caught here to prevent the process\n    # from being killed by an uncaught exception.\n    except Exception as exception:  # pylint: disable=broad-except\n      logger.warning(\n          'Unhandled exception in process: {0!s} (PID: {1:d}).'.format(\n              self._name, self._pid))\n      logger.exception(exception)\n\n      self._abort = True\n\n    finally:\n      storage_writer.WriteTaskCompletion(aborted=self._abort)\n\n      storage_writer.Close()\n\n      if self._serializers_profiler:\n        storage_writer.SetSerializersProfiler(None)\n\n      if self._storage_profiler:\n        storage_writer.SetStorageProfiler(None)\n\n    try:\n      self._storage_writer.FinalizeTaskStorage(task)\n    except IOError:\n      pass\n\n    if self._abort:\n      self._status = definitions.STATUS_INDICATOR_ABORTED\n    else:\n      self._status = definitions.STATUS_INDICATOR_COMPLETED\n\n    self._foreman_status_wait_event.wait(self._FOREMAN_STATUS_WAIT)\n\n    logger.debug('Analysis plugin: {0!s} (PID: {1:d}) stopped'.format(\n        self._name, self._pid))\n\n    if self._serializers_profiler:\n      self._storage_writer.SetSerializersProfiler(None)\n\n    if self._storage_profiler:\n      self._storage_writer.SetStorageProfiler(None)\n\n    self._StopProfiling()\n\n    self._analysis_mediator = None\n    self._foreman_status_wait_event = None\n    self._storage_writer = None\n    self._task = None\n\n    try:\n      self._event_queue.Close(abort=self._abort)\n    except errors.QueueAlreadyClosed:\n      logger.error('Queue for {0:s} was already closed.'.format(self.name))", "response": "The main loop of the main loop."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprocessing an event object.", "response": "def _ProcessEvent(self, mediator, event):\n    \"\"\"Processes an event.\n\n    Args:\n      mediator (AnalysisMediator): mediates interactions between\n          analysis plugins and other components, such as storage and dfvfs.\n      event (EventObject): event.\n    \"\"\"\n    try:\n      self._analysis_plugin.ExamineEvent(mediator, event)\n\n    except Exception as exception:  # pylint: disable=broad-except\n      self.SignalAbort()\n\n      # TODO: write analysis error.\n\n      if self._debug_output:\n        logger.warning('Unhandled exception while processing event object.')\n        logger.exception(exception)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsignals the process to abort.", "response": "def SignalAbort(self):\n    \"\"\"Signals the process to abort.\"\"\"\n    self._abort = True\n    if self._foreman_status_wait_event:\n      self._foreman_status_wait_event.set()\n    if self._analysis_mediator:\n      self._analysis_mediator.SignalAbort()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _GetISO8601String(self, structure):\n    time_zone_offset = structure.time_zone_offset\n\n    try:\n      time_zone_offset_hours = int(time_zone_offset[1:3], 10)\n      time_zone_offset_minutes = int(time_zone_offset[3:5], 10)\n    except (IndexError, TypeError, ValueError) as exception:\n      raise ValueError(\n          'unable to parse time zone offset with error: {0!s}.'.format(\n              exception))\n\n    try:\n      iso8601 = (\n          '{0:04d}-{1:02d}-{2:02d}T{3:02d}:{4:02d}:{5:02d}.{6:03d}'\n          '{7:s}{8:02d}:{9:02d}').format(\n              structure.year, structure.month, structure.day,\n              structure.hours, structure.minutes, structure.seconds,\n              structure.microseconds, time_zone_offset[0],\n              time_zone_offset_hours, time_zone_offset_minutes)\n    except ValueError as exception:\n      raise ValueError(\n          'unable to format date time string with error: {0!s}.'.format(\n              exception))\n\n    return iso8601", "response": "Retrieves an ISO 8601 date time string from the structure."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse a logline record structure and produces events.", "response": "def _ParseRecordLogline(self, parser_mediator, structure):\n    \"\"\"Parses a logline record structure and produces events.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      structure (pyparsing.ParseResults): structure of tokens derived from\n          a line of a text file.\n    \"\"\"\n    date_time = dfdatetime_time_elements.TimeElementsInMilliseconds()\n\n    try:\n      datetime_iso8601 = self._GetISO8601String(structure.date_time)\n      date_time.CopyFromStringISO8601(datetime_iso8601)\n    except ValueError:\n      parser_mediator.ProduceExtractionWarning(\n          'invalid date time value: {0!s}'.format(structure.date_time))\n      return\n\n    event_data = GoogleDriveSyncLogEventData()\n    event_data.log_level = structure.log_level\n    event_data.pid = structure.pid\n    event_data.thread = structure.thread\n    event_data.source_code = structure.source_code\n    # Replace newlines with spaces in structure.message to preserve output.\n    event_data.message = structure.message.replace('\\n', ' ')\n\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_ADDED)\n\n    parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef VerifyStructure(self, parser_mediator, lines):\n    try:\n      structure = self._GDS_LINE.parseString(lines)\n    except pyparsing.ParseException as exception:\n      logger.debug('Not a Google Drive Sync log file: {0!s}'.format(exception))\n      return False\n\n    date_time = dfdatetime_time_elements.TimeElementsInMilliseconds()\n\n    try:\n      datetime_iso8601 = self._GetISO8601String(structure.date_time)\n      date_time.CopyFromStringISO8601(datetime_iso8601)\n    except ValueError as exception:\n      logger.debug((\n          'Not a Google Drive Sync log file, invalid date/time: {0!s} '\n          'with error: {1!s}').format(structure.date_time, exception))\n      return False\n\n    return True", "response": "Verify that this file is a Google Drive Sync log file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretrieves an URL from a reference to an entry in the from_visit table.", "response": "def _GetUrl(self, url, cache, database):\n    \"\"\"Retrieves an URL from a reference to an entry in the from_visit table.\n\n    Args:\n      url (str): URL.\n      cache (SQLiteCache): cache.\n      database (SQLiteDatabase): database.\n\n    Returns:\n      str: URL or an empty string if no URL was found.\n    \"\"\"\n    if not url:\n      return ''\n\n    url_cache_results = cache.GetResults('url')\n    if not url_cache_results:\n      result_set = database.Query(self._URL_CACHE_QUERY)\n\n      cache.CacheQueryResults(result_set, 'url', 'id', ('url', 'title'))\n      url_cache_results = cache.GetResults('url')\n\n    reference_url, reference_title = url_cache_results.get(url, ['', ''])\n\n    if not reference_url:\n      return ''\n\n    return '{0:s} ({1:s})'.format(reference_url, reference_title)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving a visit source type based on the identifier.", "response": "def _GetVisitSource(self, visit_identifier, cache, database):\n    \"\"\"Retrieves a visit source type based on the identifier.\n\n    Args:\n      visit_identifier (str): identifier from the visits table for the\n          particular record.\n      cache (SQLiteCache): cache which contains cached results from querying\n          the visit_source table.\n      database (SQLiteDatabase): database.\n\n    Returns:\n      int: visit source type or None if no visit source type was found for\n          the identifier.\n    \"\"\"\n    sync_cache_results = cache.GetResults('sync')\n    if not sync_cache_results:\n      result_set = database.Query(self._SYNC_CACHE_QUERY)\n\n      cache.CacheQueryResults(result_set, 'sync', 'id', ('source',))\n      sync_cache_results = cache.GetResults('sync')\n\n    if sync_cache_results and visit_identifier:\n      results = sync_cache_results.get(visit_identifier, None)\n      if results:\n        return results[0]\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse a last visited row.", "response": "def ParseLastVisitedRow(\n      self, parser_mediator, query, row, cache=None, database=None,\n      **unused_kwargs):\n    \"\"\"Parses a last visited row.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      query (str): query that created the row.\n      row (sqlite3.Row): row.\n      cache (SQLiteCache): cache which contains cached results from querying\n          the visits and urls tables.\n      database (Optional[SQLiteDatabase]): database.\n    \"\"\"\n    query_hash = hash(query)\n\n    hidden = self._GetRowValue(query_hash, row, 'hidden')\n    transition = self._GetRowValue(query_hash, row, 'transition')\n\n    visit_identifier = self._GetRowValue(query_hash, row, 'visit_id')\n    from_visit = self._GetRowValue(query_hash, row, 'from_visit')\n\n    event_data = ChromeHistoryPageVisitedEventData()\n    event_data.from_visit = self._GetUrl(from_visit, cache, database)\n    event_data.offset = self._GetRowValue(query_hash, row, 'id')\n    event_data.query = query\n    event_data.page_transition_type = (\n        transition & self._PAGE_TRANSITION_CORE_MASK)\n    event_data.title = self._GetRowValue(query_hash, row, 'title')\n    event_data.typed_count = self._GetRowValue(query_hash, row, 'typed_count')\n    event_data.url = self._GetRowValue(query_hash, row, 'url')\n    event_data.url_hidden = hidden == '1'\n    event_data.visit_source = self._GetVisitSource(\n        visit_identifier, cache, database)\n\n    timestamp = self._GetRowValue(query_hash, row, 'visit_time')\n    date_time = dfdatetime_webkit_time.WebKitTime(timestamp=timestamp)\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_LAST_VISITED)\n    parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse a file downloaded row.", "response": "def ParseFileDownloadedRow(\n      self, parser_mediator, query, row, **unused_kwargs):\n    \"\"\"Parses a file downloaded row.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      query (str): query that created the row.\n      row (sqlite3.Row): row.\n    \"\"\"\n    query_hash = hash(query)\n\n    event_data = ChromeHistoryFileDownloadedEventData()\n    event_data.full_path = self._GetRowValue(query_hash, row, 'target_path')\n    event_data.offset = self._GetRowValue(query_hash, row, 'id')\n    event_data.query = query\n    event_data.received_bytes = self._GetRowValue(\n        query_hash, row, 'received_bytes')\n    event_data.total_bytes = self._GetRowValue(query_hash, row, 'total_bytes')\n    event_data.url = self._GetRowValue(query_hash, row, 'url')\n\n    timestamp = self._GetRowValue(query_hash, row, 'start_time')\n    date_time = dfdatetime_webkit_time.WebKitTime(timestamp=timestamp)\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_FILE_DOWNLOADED)\n    parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nencoding a string in the preferred encoding.", "response": "def _EncodeString(self, string):\n    \"\"\"Encodes a string in the preferred encoding.\n\n    Returns:\n      bytes: encoded string.\n    \"\"\"\n    try:\n      # Note that encode() will first convert string into a Unicode string\n      # if necessary.\n      encoded_string = string.encode(\n          self.preferred_encoding, errors=self._encode_errors)\n    except UnicodeEncodeError:\n      if self._encode_errors == 'strict':\n        logger.error(\n            'Unable to properly write output due to encoding error. '\n            'Switching to error tolerant encoding which can result in '\n            'non Basic Latin (C0) characters to be replaced with \"?\" or '\n            '\"\\\\ufffd\".')\n        self._encode_errors = 'replace'\n\n      encoded_string = string.encode(\n          self.preferred_encoding, errors=self._encode_errors)\n\n    return encoded_string"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nenforces a process memory limit.", "response": "def _EnforceProcessMemoryLimit(self, memory_limit):\n    \"\"\"Enforces a process memory limit.\n\n    Args:\n      memory_limit (int): maximum number of bytes the process is allowed\n          to allocate, where 0 represents no limit and None a default of\n          4 GiB.\n    \"\"\"\n    # Resource is not supported on Windows.\n    if resource:\n      if memory_limit is None:\n        memory_limit = 4 * 1024 * 1024 * 1024\n      elif memory_limit == 0:\n        memory_limit = resource.RLIM_INFINITY\n\n      resource.setrlimit(resource.RLIMIT_DATA, (memory_limit, memory_limit))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _ParseInformationalOptions(self, options):\n    self._debug_mode = getattr(options, 'debug', False)\n    self._quiet_mode = getattr(options, 'quiet', False)\n\n    if self._debug_mode and self._quiet_mode:\n      logger.warning(\n          'Cannot use debug and quiet mode at the same time, defaulting to '\n          'debug output.')", "response": "Parses the informational options."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse the log file options.", "response": "def _ParseLogFileOptions(self, options):\n    \"\"\"Parses the log file options.\n\n    Args:\n      options (argparse.Namespace): command line arguments.\n    \"\"\"\n    self._log_file = self.ParseStringOption(options, 'log_file')\n    if not self._log_file:\n      local_date_time = datetime.datetime.now()\n      self._log_file = (\n          '{0:s}-{1:04d}{2:02d}{3:02d}T{4:02d}{5:02d}{6:02d}.log.gz').format(\n              self.NAME, local_date_time.year, local_date_time.month,\n              local_date_time.day, local_date_time.hour, local_date_time.minute,\n              local_date_time.second)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing the timezone option.", "response": "def _ParseTimezoneOption(self, options):\n    \"\"\"Parses the timezone options.\n\n    Args:\n      options (argparse.Namespace): command line arguments.\n\n    Raises:\n      BadConfigOption: if the options are invalid.\n    \"\"\"\n    time_zone_string = self.ParseStringOption(options, 'timezone')\n    if isinstance(time_zone_string, py2to3.STRING_TYPES):\n      if time_zone_string.lower() == 'list':\n        self.list_timezones = True\n\n      elif time_zone_string:\n        try:\n          pytz.timezone(time_zone_string)\n        except pytz.UnknownTimeZoneError:\n          raise errors.BadConfigOption(\n              'Unknown time zone: {0:s}'.format(time_zone_string))\n\n        self._preferred_time_zone = time_zone_string"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprompts user for input.", "response": "def _PromptUserForInput(self, input_text):\n    \"\"\"Prompts user for an input.\n\n    Args:\n      input_text (str): text used for prompting the user for input.\n\n    Returns:\n      str: input read from the user.\n    \"\"\"\n    self._output_writer.Write('{0:s}: '.format(input_text))\n    return self._input_reader.Read()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding the basic options to the argument group.", "response": "def AddBasicOptions(self, argument_group):\n    \"\"\"Adds the basic options to the argument group.\n\n    Args:\n      argument_group (argparse._ArgumentGroup): argparse argument group.\n    \"\"\"\n    version_string = self.GetVersionInformation()\n\n    # We want a custom help message and not the default argparse one.\n    argument_group.add_argument(\n        '-h', '--help', action='help',\n        help='Show this help message and exit.')\n\n    argument_group.add_argument(\n        '--troubles', dest='show_troubleshooting', action='store_true',\n        default=False, help='Show troubleshooting information.')\n\n    argument_group.add_argument(\n        '-V', '--version', dest='version', action='version',\n        version=version_string, help='Show the version information.')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef AddInformationalOptions(self, argument_group):\n    argument_group.add_argument(\n        '-d', '--debug', dest='debug', action='store_true', default=False,\n        help='Enable debug output.')\n\n    argument_group.add_argument(\n        '-q', '--quiet', dest='quiet', action='store_true', default=False,\n        help='Disable informational output.')", "response": "Adds the informational options to the argument group."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef AddLogFileOptions(self, argument_group):\n    argument_group.add_argument(\n        '--logfile', '--log_file', '--log-file', action='store',\n        metavar='FILENAME', dest='log_file', type=str, default='', help=(\n            'Path of the file in which to store log messages, by default '\n            'this file will be named: \"{0:s}-YYYYMMDDThhmmss.log.gz\". Note '\n            'that the file will be gzip compressed if the extension is '\n            '\".gz\".').format(self.NAME))", "response": "Adds the log file option to the argument group."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding the time zone option to the argument group.", "response": "def AddTimeZoneOption(self, argument_group):\n    \"\"\"Adds the time zone option to the argument group.\n\n    Args:\n      argument_group (argparse._ArgumentGroup): argparse argument group.\n    \"\"\"\n    # Note the default here is None so we can determine if the time zone\n    # option was set.\n    argument_group.add_argument(\n        '-z', '--zone', '--timezone', dest='timezone', action='store',\n        type=str, default=None, help=(\n            'explicitly define the timezone. Typically the timezone is '\n            'determined automatically where possible otherwise it will '\n            'default to UTC. Use \"-z list\" to see a list of available '\n            'timezones.'))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving the command line arguments.", "response": "def GetCommandLineArguments(self):\n    \"\"\"Retrieves the command line arguments.\n\n    Returns:\n      str: command line arguments.\n    \"\"\"\n    command_line_arguments = sys.argv\n    if not command_line_arguments:\n      return ''\n\n    if isinstance(command_line_arguments[0], py2to3.BYTES_TYPE):\n      encoding = sys.stdin.encoding\n\n      # Note that sys.stdin.encoding can be None.\n      if not encoding:\n        encoding = self.preferred_encoding\n\n      try:\n        command_line_arguments = [\n            argument.decode(encoding) for argument in command_line_arguments]\n\n      except UnicodeDecodeError:\n        logger.error(\n            'Unable to properly read command line input due to encoding '\n            'error. Replacing non Basic Latin (C0) characters with \"?\" or '\n            '\"\\\\ufffd\".')\n\n        command_line_arguments = [\n            argument.decode(encoding, errors='replace')\n            for argument in command_line_arguments]\n\n    return ' '.join(command_line_arguments)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse a numeric option.", "response": "def ParseNumericOption(self, options, name, base=10, default_value=None):\n    \"\"\"Parses a numeric option.\n\n    If the option is not set the default value is returned.\n\n    Args:\n      options (argparse.Namespace): command line arguments.\n      name (str): name of the numeric option.\n      base (Optional[int]): base of the numeric value.\n      default_value (Optional[object]): default value.\n\n    Returns:\n      int: numeric value.\n\n    Raises:\n      BadConfigOption: if the options are invalid.\n    \"\"\"\n    numeric_value = getattr(options, name, None)\n    if not numeric_value:\n      return default_value\n\n    try:\n      return int(numeric_value, base)\n\n    except (TypeError, ValueError):\n      name = name.replace('_', ' ')\n      raise errors.BadConfigOption(\n          'Unsupported numeric value {0:s}: {1!s}.'.format(\n              name, numeric_value))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ParseStringOption(self, options, argument_name, default_value=None):\n    argument_value = getattr(options, argument_name, None)\n    if not argument_value:\n      return default_value\n\n    if isinstance(argument_value, py2to3.BYTES_TYPE):\n      encoding = sys.stdin.encoding\n\n      # Note that sys.stdin.encoding can be None.\n      if not encoding:\n        encoding = self.preferred_encoding\n\n      try:\n        argument_value = codecs.decode(argument_value, encoding)\n      except UnicodeDecodeError as exception:\n        raise errors.BadConfigOption((\n            'Unable to convert option: {0:s} to Unicode with error: '\n            '{1!s}.').format(argument_name, exception))\n\n    elif not isinstance(argument_value, py2to3.UNICODE_TYPE):\n      raise errors.BadConfigOption(\n          'Unsupported option: {0:s} string type required.'.format(\n              argument_name))\n\n    return argument_value", "response": "Parses a string command line argument."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprints a separator line.", "response": "def PrintSeparatorLine(self):\n    \"\"\"Prints a separator line.\"\"\"\n    self._output_writer.Write('-' * self._LINE_LENGTH)\n    self._output_writer.Write('\\n')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef Read(self):\n    encoded_string = self._file_object.readline()\n\n    if isinstance(encoded_string, py2to3.UNICODE_TYPE):\n      return encoded_string\n\n    try:\n      string = codecs.decode(encoded_string, self._encoding, self._errors)\n    except UnicodeDecodeError:\n      if self._errors == 'strict':\n        logger.error(\n            'Unable to properly read input due to encoding error. '\n            'Switching to error tolerant encoding which can result in '\n            'non Basic Latin (C0) characters to be replaced with \"?\" or '\n            '\"\\\\ufffd\".')\n        self._errors = 'replace'\n\n      string = codecs.decode(encoded_string, self._encoding, self._errors)\n\n    return string", "response": "Reads a string from the input."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwrites a string to the output.", "response": "def Write(self, string):\n    \"\"\"Writes a string to the output.\n\n    Args:\n      string (str): output.\n    \"\"\"\n    try:\n      # Note that encode() will first convert string into a Unicode string\n      # if necessary.\n      encoded_string = codecs.encode(string, self._encoding, self._errors)\n    except UnicodeEncodeError:\n      if self._errors == 'strict':\n        logger.error(\n            'Unable to properly write output due to encoding error. '\n            'Switching to error tolerant encoding which can result in '\n            'non Basic Latin (C0) characters to be replaced with \"?\" or '\n            '\"\\\\ufffd\".')\n        self._errors = 'replace'\n\n      encoded_string = codecs.encode(string, self._encoding, self._errors)\n\n    self._file_object.write(encoded_string)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites a string to the output.", "response": "def Write(self, string):\n    \"\"\"Writes a string to the output.\n\n    Args:\n      string (str): output.\n    \"\"\"\n    if sys.version_info[0] < 3:\n      super(StdoutOutputWriter, self).Write(string)\n    else:\n      # sys.stdout.write() on Python 3 by default will error if string is\n      # of type bytes.\n      sys.stdout.write(string)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _CheckPythonModule(\n    module_name, version_attribute_name, minimum_version,\n    is_required=True, maximum_version=None, verbose_output=True):\n  \"\"\"Checks the availability of a Python module.\n\n  Args:\n    module_name (str): name of the module.\n    version_attribute_name (str): name of the attribute that contains\n       the module version or method to retrieve the module version.\n    minimum_version (str): minimum required version.\n    is_required (Optional[bool]): True if the Python module is a required\n        dependency.\n    maximum_version (Optional[str]): maximum required version. Should only be\n        used if there is a later version that is not supported.\n    verbose_output (Optional[bool]): True if output should be verbose.\n\n  Returns:\n    bool: True if the Python module is available and conforms to\n        the minimum required version, False otherwise.\n  \"\"\"\n  module_object = _ImportPythonModule(module_name)\n  if not module_object:\n    if not is_required:\n      print('[OPTIONAL]\\tmissing: {0:s}.'.format(module_name))\n      return True\n\n    print('[FAILURE]\\tmissing: {0:s}.'.format(module_name))\n    return False\n\n  if not version_attribute_name or not minimum_version:\n    if verbose_output:\n      print('[OK]\\t\\t{0:s}'.format(module_name))\n    return True\n\n  module_version = None\n  if not version_attribute_name.endswith('()'):\n    module_version = getattr(module_object, version_attribute_name, None)\n  else:\n    version_method = getattr(module_object, version_attribute_name[:-2], None)\n    if version_method:\n      module_version = version_method()\n\n  if not module_version:\n    if not is_required:\n      print((\n          '[OPTIONAL]\\tunable to determine version information '\n          'for: {0:s}').format(module_name))\n      return True\n\n    print((\n        '[FAILURE]\\tunable to determine version information '\n        'for: {0:s}').format(module_name))\n    return False\n\n  # Make sure the module version is a string.\n  module_version = '{0!s}'.format(module_version)\n\n  # Split the version string and convert every digit into an integer.\n  # A string compare of both version strings will yield an incorrect result.\n  module_version_map = list(\n      map(int, _VERSION_SPLIT_REGEX.split(module_version)))\n  minimum_version_map = list(\n      map(int, _VERSION_SPLIT_REGEX.split(minimum_version)))\n\n  if module_version_map < minimum_version_map:\n    if not is_required:\n      print((\n          '[OPTIONAL]\\t{0:s} version: {1!s} is too old, {2!s} or later '\n          'required.').format(module_name, module_version, minimum_version))\n      return True\n\n    print((\n        '[FAILURE]\\t{0:s} version: {1!s} is too old, {2!s} or later '\n        'required.').format(module_name, module_version, minimum_version))\n    return False\n\n  if maximum_version:\n    maximum_version_map = list(\n        map(int, _VERSION_SPLIT_REGEX.split(maximum_version)))\n    if module_version_map > maximum_version_map:\n      if not is_required:\n        print((\n            '[OPTIONAL]\\t{0:s} version: {1!s} is too recent, {2!s} or earlier '\n            'required.').format(module_name, module_version, minimum_version))\n        return True\n\n      print((\n          '[FAILURE]\\t{0:s} version: {1!s} is too recent, {2!s} or earlier '\n          'required.').format(module_name, module_version, maximum_version))\n      return False\n\n  if verbose_output:\n    print('[OK]\\t\\t{0:s} version: {1!s}'.format(module_name, module_version))\n\n  return True", "response": "Checks the availability of a Python module."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks the availability of sqlite3.", "response": "def _CheckSQLite3(verbose_output=True):\n  \"\"\"Checks the availability of sqlite3.\n\n  Args:\n    verbose_output (Optional[bool]): True if output should be verbose.\n\n  Returns:\n    bool: True if the sqlite3 Python module is available, False otherwise.\n  \"\"\"\n  # On Windows sqlite3 can be provided by both pysqlite2.dbapi2 and\n  # sqlite3. sqlite3 is provided with the Python installation and\n  # pysqlite2.dbapi2 by the pysqlite2 Python module. Typically\n  # pysqlite2.dbapi2 would contain a newer version of sqlite3, hence\n  # we check for its presence first.\n  module_name = 'pysqlite2.dbapi2'\n  minimum_version = '3.7.8'\n\n  module_object = _ImportPythonModule(module_name)\n  if not module_object:\n    module_name = 'sqlite3'\n\n  module_object = _ImportPythonModule(module_name)\n  if not module_object:\n    print('[FAILURE]\\tmissing: {0:s}.'.format(module_name))\n    return False\n\n  module_version = getattr(module_object, 'sqlite_version', None)\n  if not module_version:\n    return False\n\n  # Split the version string and convert every digit into an integer.\n  # A string compare of both version strings will yield an incorrect result.\n  module_version_map = list(\n      map(int, _VERSION_SPLIT_REGEX.split(module_version)))\n  minimum_version_map = list(\n      map(int, _VERSION_SPLIT_REGEX.split(minimum_version)))\n\n  if module_version_map < minimum_version_map:\n    print((\n        '[FAILURE]\\t{0:s} version: {1!s} is too old, {2!s} or later '\n        'required.').format(module_name, module_version, minimum_version))\n    return False\n\n  if verbose_output:\n    print('[OK]\\t\\t{0:s} version: {1!s}'.format(module_name, module_version))\n\n  return True"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _ImportPythonModule(module_name):\n  try:\n    module_object = list(map(__import__, [module_name]))[0]\n  except ImportError:\n    return None\n\n  # If the module name contains dots get the upper most module object.\n  if '.' in module_name:\n    for submodule_name in module_name.split('.')[1:]:\n      module_object = getattr(module_object, submodule_name, None)\n\n  return module_object", "response": "Imports a Python module.\n Arguments : module_name ( str ) Returns a Python module object or None if the module cannot be imported."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef CheckDependencies(verbose_output=True):\n  print('Checking availability and versions of dependencies.')\n  check_result = True\n\n  for module_name, version_tuple in sorted(PYTHON_DEPENDENCIES.items()):\n    if not _CheckPythonModule(\n        module_name, version_tuple[0], version_tuple[1],\n        is_required=version_tuple[3], maximum_version=version_tuple[2],\n        verbose_output=verbose_output):\n      check_result = False\n\n  if not _CheckSQLite3(verbose_output=verbose_output):\n    check_result = False\n\n  if check_result and not verbose_output:\n    print('[OK]')\n\n  print('')\n  return check_result", "response": "Checks availability of the dependencies."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse the index table.", "response": "def _ParseIndexTable(self, file_object):\n    \"\"\"Parses the index table.\n\n    Args:\n      file_object (dfvfs.FileIO): a file-like object to parse.\n\n    Raises:\n      ParseError: if the index table cannot be read.\n    \"\"\"\n    cache_address_map = self._GetDataTypeMap('uint32le')\n    file_offset = file_object.get_offset()\n\n    cache_address_data = file_object.read(4)\n\n    while len(cache_address_data) == 4:\n      try:\n        value = self._ReadStructureFromByteStream(\n            cache_address_data, file_offset, cache_address_map)\n      except (ValueError, errors.ParseError) as exception:\n        raise errors.ParseError((\n            'Unable to map cache address at offset: 0x{0:08x} with error: '\n            '{1!s}').format(file_offset, exception))\n\n      if value:\n        cache_address = CacheAddress(value)\n        self.index_table.append(cache_address)\n\n      file_offset += 4\n\n      cache_address_data = file_object.read(4)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ParseFileObject(self, parser_mediator, file_object):\n    try:\n      self._ParseFileHeader(file_object)\n    except errors.ParseError as exception:\n      raise errors.ParseError(\n          'Unable to parse index file header with error: {0!s}'.format(\n              exception))\n    # Skip over the LRU data, which is 112 bytes in size.\n    file_object.seek(112, os.SEEK_CUR)\n    self._ParseIndexTable(file_object)", "response": "Parses a file - like object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _ParseFileHeader(self, file_object):\n    file_header_map = self._GetDataTypeMap(\n        'chrome_cache_data_block_file_header')\n\n    try:\n      file_header, _ = self._ReadStructureFromFileObject(\n          file_object, 0, file_header_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.ParseError(\n          'Unable to parse data block file header with error: {0!s}'.format(\n              exception))\n\n    if file_header.signature != self._FILE_SIGNATURE:\n      raise errors.ParseError('Unsupported data block file signature')\n\n    format_version = '{0:d}.{1:d}'.format(\n        file_header.major_version, file_header.minor_version)\n    if format_version not in ('2.0', '2.1'):\n      raise errors.ParseError(\n          'Unsupported data block file format version: {0:s}'.format(\n              format_version))\n\n    if file_header.block_size not in (256, 1024, 4096):\n      raise errors.ParseError(\n          'Unsupported data block file block size: {0:d}'.format(\n              file_header.block_size))", "response": "Parses the file header."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ParseCacheEntry(self, file_object, block_offset):\n    cache_entry_map = self._GetDataTypeMap('chrome_cache_entry')\n\n    try:\n      cache_entry, _ = self._ReadStructureFromFileObject(\n          file_object, block_offset, cache_entry_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.UnableToParseFile((\n          'Unable to parse cache entry at offset: 0x{0:08x} with error: '\n          '{1!s}').format(block_offset, exception))\n\n    cache_entry_object = CacheEntry()\n\n    cache_entry_object.hash = cache_entry.hash\n    cache_entry_object.next = CacheAddress(cache_entry.next_address)\n    cache_entry_object.rankings_node = CacheAddress(\n        cache_entry.rankings_node_address)\n    cache_entry_object.creation_time = cache_entry.creation_time\n\n    byte_array = cache_entry.key\n    byte_string = bytes(bytearray(byte_array))\n    cache_entry_object.key, _, _ = byte_string.partition(b'\\x00')\n\n    try:\n      cache_entry_object.original_url = cache_entry_object.key.decode('ascii')\n    except UnicodeDecodeError as exception:\n      raise errors.ParseError(\n          'Unable to decode original URL in key with error: {0!s}'.format(\n              exception))\n\n    return cache_entry_object", "response": "Parses a cache entry."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse Chrome Cache file entries.", "response": "def _ParseCacheEntries(self, parser_mediator, index_table, data_block_files):\n    \"\"\"Parses Chrome Cache file entries.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      index_table (list[CacheAddress]): the cache addresses which are stored in\n          the index file.\n      data_block_files (dict[str: file]): look up table for the data block\n          file-like object handles.\n    \"\"\"\n    # Parse the cache entries in the data block files.\n    for cache_address in index_table:\n      cache_address_chain_length = 0\n      while cache_address.value != 0:\n        if cache_address_chain_length >= 64:\n          parser_mediator.ProduceExtractionWarning(\n              'Maximum allowed cache address chain length reached.')\n          break\n\n        data_block_file_object = data_block_files.get(\n            cache_address.filename, None)\n        if not data_block_file_object:\n          message = 'Cache address: 0x{0:08x} missing data file.'.format(\n              cache_address.value)\n          parser_mediator.ProduceExtractionWarning(message)\n          break\n\n        try:\n          cache_entry = self._data_block_file_parser.ParseCacheEntry(\n              data_block_file_object, cache_address.block_offset)\n        except (IOError, errors.ParseError) as exception:\n          parser_mediator.ProduceExtractionWarning(\n              'Unable to parse cache entry with error: {0!s}'.format(\n                  exception))\n          break\n\n        event_data = ChromeCacheEntryEventData()\n        event_data.original_url = cache_entry.original_url\n\n        date_time = dfdatetime_webkit_time.WebKitTime(\n            timestamp=cache_entry.creation_time)\n        event = time_events.DateTimeValuesEvent(\n            date_time, definitions.TIME_DESCRIPTION_LAST_VISITED)\n        parser_mediator.ProduceEventWithEventData(event, event_data)\n\n        cache_address = cache_entry.next\n        cache_address_chain_length += 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _ParseIndexTable(\n      self, parser_mediator, file_system, file_entry, index_table):\n    \"\"\"Parses a Chrome Cache index table.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      file_system (dfvfs.FileSystem): file system.\n      file_entry (dfvfs.FileEntry): file entry.\n      index_table (list[CacheAddress]): the cache addresses which are stored in\n          the index file.\n    \"\"\"\n    # Build a lookup table for the data block files.\n    path_segments = file_system.SplitPath(file_entry.path_spec.location)\n\n    data_block_files = {}\n    for cache_address in index_table:\n      if cache_address.filename not in data_block_files:\n        # Remove the previous filename from the path segments list and\n        # add one of the data block files.\n        path_segments.pop()\n        path_segments.append(cache_address.filename)\n\n        # We need to pass only used arguments to the path specification\n        # factory otherwise it will raise.\n        kwargs = {}\n        if file_entry.path_spec.parent:\n          kwargs['parent'] = file_entry.path_spec.parent\n        kwargs['location'] = file_system.JoinPath(path_segments)\n\n        data_block_file_path_spec = path_spec_factory.Factory.NewPathSpec(\n            file_entry.path_spec.TYPE_INDICATOR, **kwargs)\n\n        try:\n          data_block_file_entry = path_spec_resolver.Resolver.OpenFileEntry(\n              data_block_file_path_spec)\n        except RuntimeError as exception:\n          message = (\n              'Unable to open data block file: {0:s} with error: '\n              '{1!s}'.format(kwargs['location'], exception))\n          parser_mediator.ProduceExtractionWarning(message)\n          data_block_file_entry = None\n\n        if not data_block_file_entry:\n          message = 'Missing data block file: {0:s}'.format(\n              cache_address.filename)\n          parser_mediator.ProduceExtractionWarning(message)\n          data_block_file_object = None\n\n        else:\n          data_block_file_object = data_block_file_entry.GetFileObject()\n\n          try:\n            self._data_block_file_parser.ParseFileObject(\n                parser_mediator, data_block_file_object)\n          except (IOError, errors.ParseError) as exception:\n            message = (\n                'Unable to parse data block file: {0:s} with error: '\n                '{1!s}').format(cache_address.filename, exception)\n            parser_mediator.ProduceExtractionWarning(message)\n            data_block_file_object.close()\n            data_block_file_object = None\n\n        data_block_files[cache_address.filename] = data_block_file_object\n\n    try:\n      self._ParseCacheEntries(\n          parser_mediator, index_table, data_block_files)\n    finally:\n      for data_block_file_object in iter(data_block_files.values()):\n        if data_block_file_object:\n          data_block_file_object.close()", "response": "Parses a Chrome Cache index table."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ParseFileEntry(self, parser_mediator, file_entry):\n    index_file_parser = ChromeCacheIndexFileParser()\n\n    file_object = file_entry.GetFileObject()\n    try:\n      index_file_parser.ParseFileObject(parser_mediator, file_object)\n    except (IOError, errors.ParseError) as exception:\n      file_object.close()\n\n      display_name = parser_mediator.GetDisplayName()\n      raise errors.UnableToParseFile(\n          '[{0:s}] unable to parse index file {1:s} with error: {2!s}'.format(\n              self.NAME, display_name, exception))\n\n    # TODO: create event based on index file creation time.\n\n    try:\n      file_system = file_entry.GetFileSystem()\n      self._ParseIndexTable(\n          parser_mediator, file_system, file_entry,\n          index_file_parser.index_table)\n    finally:\n      file_object.close()", "response": "Parses a Chrome Cache file entry."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing and validates the options.", "response": "def ParseOptions(cls, options, configuration_object):\n    \"\"\"Parses and validates options.\n\n    Args:\n      options (argparse.Namespace): parser options.\n      configuration_object (CLITool): object to be configured by the argument\n          helper.\n\n    Raises:\n      BadConfigObject: when the configuration object is of the wrong type.\n      BadConfigOption: when a configuration parameter fails validation.\n    \"\"\"\n    if not isinstance(configuration_object, tools.CLITool):\n      raise errors.BadConfigObject(\n          'Configuration object is not an instance of CLITool')\n\n    hashers = cls._ParseStringOption(\n        options, 'hashers', default_value=cls._DEFAULT_HASHER_STRING)\n\n    hasher_file_size_limit = cls._ParseNumericOption(\n        options, 'hasher_file_size_limit', default_value=0)\n\n    # TODO: validate hasher names.\n\n    if hasher_file_size_limit < 0:\n      raise errors.BadConfigOption(\n          'Invalid hasher file size limit value cannot be negative.')\n\n    setattr(configuration_object, '_hasher_names_string', hashers)\n    setattr(\n        configuration_object, '_hasher_file_size_limit', hasher_file_size_limit)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _FormatDateTime(self, event):\n    try:\n      datetime_object = datetime.datetime(\n          1970, 1, 1, 0, 0, 0, 0, tzinfo=pytz.UTC)\n      datetime_object += datetime.timedelta(microseconds=event.timestamp)\n      datetime_object.astimezone(self._output_mediator.timezone)\n\n      return datetime_object.replace(tzinfo=None)\n\n    except (OverflowError, ValueError) as exception:\n      self._ReportEventError(event, (\n          'unable to copy timestamp: {0!s} to a human readable date and time '\n          'with error: {1!s}. Defaulting to: \"ERROR\"').format(\n              event.timestamp, exception))\n      return 'ERROR'", "response": "Formats the date to a datetime object without timezone information."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _RemoveIllegalXMLCharacters(self, xml_string):\n    if not isinstance(xml_string, py2to3.STRING_TYPES):\n      return xml_string\n\n    return self._ILLEGAL_XML_RE.sub('\\ufffd', xml_string)", "response": "Removes illegal characters from the input XML."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nopen a new file and returns the file object.", "response": "def Open(self):\n    \"\"\"Creates a new workbook.\n\n    Raises:\n      IOError: if the specified output file already exists.\n      OSError: if the specified output file already exists.\n      ValueError: if the filename is not set.\n    \"\"\"\n    if not self._filename:\n      raise ValueError('Missing filename.')\n\n    if os.path.isfile(self._filename):\n      raise IOError((\n          'Unable to use an already existing file for output '\n          '[{0:s}]').format(self._filename))\n\n    options = {\n        'constant_memory': True,\n        'strings_to_urls': False,\n        'strings_to_formulas': False,\n        'default_date_format': self._timestamp_format}\n    self._workbook = xlsxwriter.Workbook(self._filename, options)\n    self._sheet = self._workbook.add_worksheet('Sheet')\n    self._current_row = 0"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwriting the body of an event object to the spreadsheet.", "response": "def WriteEventBody(self, event):\n    \"\"\"Writes the body of an event object to the spreadsheet.\n\n    Args:\n      event (EventObject): event.\n    \"\"\"\n    for field_name in self._fields:\n      if field_name == 'datetime':\n        output_value = self._FormatDateTime(event)\n      else:\n        output_value = self._dynamic_fields_helper.GetFormattedField(\n            event, field_name)\n\n      output_value = self._RemoveIllegalXMLCharacters(output_value)\n\n      # Auto adjust the column width based on the length of the output value.\n      column_index = self._fields.index(field_name)\n      self._column_widths.setdefault(column_index, 0)\n\n      if field_name == 'datetime':\n        column_width = min(\n            self._MAX_COLUMN_WIDTH, len(self._timestamp_format) + 2)\n      else:\n        column_width = min(self._MAX_COLUMN_WIDTH, len(output_value) + 2)\n\n      self._column_widths[column_index] = max(\n          self._MIN_COLUMN_WIDTH, self._column_widths[column_index],\n          column_width)\n      self._sheet.set_column(\n          column_index, column_index, self._column_widths[column_index])\n\n      if (field_name == 'datetime'\n          and isinstance(output_value, datetime.datetime)):\n        self._sheet.write_datetime(\n            self._current_row, column_index, output_value)\n      else:\n        self._sheet.write(self._current_row, column_index, output_value)\n\n    self._current_row += 1"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef WriteHeader(self):\n    self._column_widths = {}\n    bold = self._workbook.add_format({'bold': True})\n    bold.set_align('center')\n    for index, field_name in enumerate(self._fields):\n      self._sheet.write(self._current_row, index, field_name, bold)\n      self._column_widths[index] = len(field_name) + 2\n    self._current_row += 1\n    self._sheet.autofilter(0, len(self._fields) - 1, 0, 0)\n    self._sheet.freeze_panes(1, 0)", "response": "Writes the header to the spreadsheet."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nnormalizing date time parsed format to an ISO 8601 date time string.", "response": "def _GetISO8601String(self, structure):\n    \"\"\"Normalize date time parsed format to an ISO 8601 date time string.\n    The date and time values in Apache access log files are formatted as:\n    \"[18/Sep/2011:19:18:28 -0400]\".\n\n    Args:\n      structure (pyparsing.ParseResults): structure of tokens derived from a\n          line of a text file.\n\n    Returns:\n      str: ISO 8601 date time string.\n\n    Raises:\n      ValueError: if the structure cannot be converted into a date time string.\n    \"\"\"\n    time_offset = structure.time_offset\n    month = timelib.MONTH_DICT.get(structure.month.lower(), 0)\n\n    try:\n      time_offset_hours = int(time_offset[1:3], 10)\n      time_offset_minutes = int(time_offset[3:5], 10)\n    except (IndexError, TypeError, ValueError) as exception:\n      raise ValueError(\n          'unable to parse time zone offset with error: {0!s}.'.format(\n              exception))\n\n    try:\n      date_time_string = (\n          '{0:04d}-{1:02d}-{2:02d}T{3:02d}:{4:02d}:{5:02d}.000000'\n          '{6:s}{7:02d}:{8:02d}').format(\n              structure.year, month, structure.day, structure.hours,\n              structure.minutes, structure.seconds, time_offset[0],\n              time_offset_hours, time_offset_minutes)\n    except ValueError as exception:\n      raise ValueError(\n          'unable to format date time string with error: {0!s}.'.format(\n              exception))\n\n    return date_time_string"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing a matching entry. Args: parser_mediator (ParserMediator): mediates interactions between parsers and other components, such as storage and dfvfs. key (str): name of the parsed structure. structure (pyparsing.ParseResults): elements parsed from the file. Raises: ParseError: when the structure type is unknown.", "response": "def ParseRecord(self, parser_mediator, key, structure):\n    \"\"\"Parses a matching entry.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n        and other components, such as storage and dfvfs.\n      key (str): name of the parsed structure.\n      structure (pyparsing.ParseResults): elements parsed from the file.\n\n    Raises:\n      ParseError: when the structure type is unknown.\n    \"\"\"\n    if key not in self._SUPPORTED_KEYS:\n      raise errors.ParseError(\n          'Unable to parse record, unknown structure: {0:s}'.format(key))\n\n    date_time = dfdatetime_time_elements.TimeElements()\n\n    try:\n      iso_date_time = self._GetISO8601String(structure.date_time)\n      date_time.CopyFromStringISO8601(iso_date_time)\n    except ValueError:\n      parser_mediator.ProduceExtractionWarning(\n          'invalid date time value: {0!s}'.format(structure.date_time))\n      return\n\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_RECORDED)\n\n    event_data = ApacheAccessEventData()\n    event_data.ip_address = structure.ip_address\n    event_data.remote_name = structure.remote_name\n    event_data.user_name = structure.user_name\n    event_data.http_request = structure.http_request\n    event_data.http_response_code = structure.response_code\n    event_data.http_response_bytes = structure.response_bytes\n\n    if key == 'combined_log_format':\n      event_data.http_request_referer = structure.referer\n      event_data.http_request_user_agent = structure.user_agent\n\n    parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nverifying that this is an apache access log file.", "response": "def VerifyStructure(self, parser_mediator, line):\n    \"\"\"Verifies that this is an apache access log file.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n        and other components, such as storage and dfvfs.\n      line (str): line from the text file.\n\n    Returns:\n      bool: True if this is the correct parser, False otherwise.\n    \"\"\"\n    return max([parser.matches(line) for _, parser in self.LINE_STRUCTURES])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ParseAccountInformation(\n      self, parser_mediator, query, row, **unused_kwargs):\n    \"\"\"Parses account information.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      query (str): query that created the row.\n      row (sqlite3.Row): row with account information.\n    \"\"\"\n    query_hash = hash(query)\n\n    display_name = self._GetRowValue(query_hash, row, 'given_displayname')\n    fullname = self._GetRowValue(query_hash, row, 'fullname')\n\n    # TODO: Move this to the formatter, and ensure username is rendered\n    # properly when fullname and/or display_name is None.\n    username = '{0!s} <{1!s}>'.format(fullname, display_name)\n\n    event_data = SkypeAccountEventData()\n    event_data.country = self._GetRowValue(query_hash, row, 'country')\n    event_data.display_name = display_name\n    event_data.email = self._GetRowValue(query_hash, row, 'emails')\n    event_data.offset = self._GetRowValue(query_hash, row, 'id')\n    event_data.query = query\n    event_data.username = username\n\n    timestamp = self._GetRowValue(query_hash, row, 'profile_timestamp')\n    if timestamp:\n      date_time = dfdatetime_posix_time.PosixTime(timestamp=timestamp)\n      event = time_events.DateTimeValuesEvent(date_time, 'Profile Changed')\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    timestamp = self._GetRowValue(query_hash, row, 'authreq_timestamp')\n    if timestamp:\n      date_time = dfdatetime_posix_time.PosixTime(timestamp=timestamp)\n      event = time_events.DateTimeValuesEvent(\n          date_time, 'Authenticate Request')\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    timestamp = self._GetRowValue(query_hash, row, 'lastonline_timestamp')\n    if timestamp:\n      date_time = dfdatetime_posix_time.PosixTime(timestamp=timestamp)\n      event = time_events.DateTimeValuesEvent(date_time, 'Last Online')\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    timestamp = self._GetRowValue(query_hash, row, 'mood_timestamp')\n    if timestamp:\n      date_time = dfdatetime_posix_time.PosixTime(timestamp=timestamp)\n      event = time_events.DateTimeValuesEvent(date_time, 'Mood Event')\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    timestamp = self._GetRowValue(query_hash, row, 'sent_authrequest_time')\n    if timestamp:\n      date_time = dfdatetime_posix_time.PosixTime(timestamp=timestamp)\n      event = time_events.DateTimeValuesEvent(date_time, 'Auth Request Sent')\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    timestamp = self._GetRowValue(query_hash, row, 'lastused_timestamp')\n    if timestamp:\n      date_time = dfdatetime_posix_time.PosixTime(timestamp=timestamp)\n      event = time_events.DateTimeValuesEvent(date_time, 'Last Used')\n      parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Parses account information.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      query (str): query that created the row.\n      row (sqlite3.Row): row with account information."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ParseChat(self, parser_mediator, query, row, **unused_kwargs):\n    query_hash = hash(query)\n\n    participants = self._GetRowValue(query_hash, row, 'participants')\n    author = self._GetRowValue(query_hash, row, 'author')\n    dialog_partner = self._GetRowValue(query_hash, row, 'dialog_partner')\n    from_displayname = self._GetRowValue(query_hash, row, 'from_displayname')\n\n    accounts = []\n    participants = participants.split(' ')\n    for participant in participants:\n      if participant != author:\n        accounts.append(participant)\n\n    to_account = ', '.join(accounts)\n    if not to_account:\n      to_account = dialog_partner or 'Unknown User'\n\n    from_account = '{0:s} <{1:s}>'.format(from_displayname, author)\n\n    event_data = SkypeChatEventData()\n    event_data.from_account = from_account\n    event_data.query = query\n    event_data.text = self._GetRowValue(query_hash, row, 'body_xml')\n    event_data.title = self._GetRowValue(query_hash, row, 'title')\n    event_data.to_account = to_account\n\n    timestamp = self._GetRowValue(query_hash, row, 'timestamp')\n    if timestamp:\n      date_time = dfdatetime_posix_time.PosixTime(timestamp=timestamp)\n      event = time_events.DateTimeValuesEvent(date_time, 'Chat from Skype')\n      parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Parses a chat message."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses an SMS from Skype.", "response": "def ParseSMS(self, parser_mediator, query, row, **unused_kwargs):\n    \"\"\"Parses an SMS.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      query (str): query that created the row.\n      row (sqlite3.Row): row resulting from query.\n    \"\"\"\n    query_hash = hash(query)\n\n    phone_number = self._GetRowValue(query_hash, row, 'dstnum_sms')\n    if phone_number:\n      phone_number = phone_number.replace(' ', '')\n\n    event_data = SkypeSMSEventData()\n    event_data.number = phone_number\n    event_data.query = query\n    event_data.text = self._GetRowValue(query_hash, row, 'msg_sms')\n\n    timestamp = self._GetRowValue(query_hash, row, 'time_sms')\n    if timestamp:\n      date_time = dfdatetime_posix_time.PosixTime(timestamp=timestamp)\n      event = time_events.DateTimeValuesEvent(date_time, 'SMS from Skype')\n      parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ParseCall(self, parser_mediator, query, row, **unused_kwargs):\n    query_hash = hash(query)\n\n    guid = self._GetRowValue(query_hash, row, 'guid')\n    is_incoming = self._GetRowValue(query_hash, row, 'is_incoming')\n    videostatus = self._GetRowValue(query_hash, row, 'videostatus')\n\n    try:\n      aux = guid\n      if aux:\n        aux_list = aux.split('-')\n        src_aux = aux_list[0]\n        dst_aux = aux_list[1]\n      else:\n        src_aux = 'Unknown [no GUID]'\n        dst_aux = 'Unknown [no GUID]'\n    except IndexError:\n      src_aux = 'Unknown [{0:s}]'.format(guid)\n      dst_aux = 'Unknown [{0:s}]'.format(guid)\n\n    if is_incoming == '0':\n      user_start_call = True\n      source = src_aux\n\n      ip_address = self._GetRowValue(query_hash, row, 'ip_address')\n      if ip_address:\n        destination = '{0:s} <{1:s}>'.format(dst_aux, ip_address)\n      else:\n        destination = dst_aux\n    else:\n      user_start_call = False\n      source = src_aux\n      destination = dst_aux\n\n    call_identifier = self._GetRowValue(query_hash, row, 'id')\n\n    event_data = SkypeCallEventData()\n    event_data.dst_call = destination\n    event_data.offset = call_identifier\n    event_data.query = query\n    event_data.src_call = source\n    event_data.user_start_call = user_start_call\n    event_data.video_conference = videostatus == '3'\n\n    timestamp = self._GetRowValue(query_hash, row, 'try_call')\n    event_data.call_type = 'WAITING'\n    date_time = dfdatetime_posix_time.PosixTime(timestamp=timestamp)\n    event = time_events.DateTimeValuesEvent(date_time, 'Call from Skype')\n    parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    try:\n      timestamp = self._GetRowValue(query_hash, row, 'accept_call')\n      timestamp = int(timestamp)\n    except (ValueError, TypeError):\n      timestamp = None\n\n    if timestamp:\n      event_data.call_type = 'ACCEPTED'\n      date_time = dfdatetime_posix_time.PosixTime(timestamp=timestamp)\n      event = time_events.DateTimeValuesEvent(date_time, 'Call from Skype')\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n      try:\n        call_duration = self._GetRowValue(query_hash, row, 'call_duration')\n        call_duration = int(call_duration)\n      except (ValueError, TypeError):\n        parser_mediator.ProduceExtractionWarning(\n            'unable to determine when call: {0:s} was finished.'.format(\n                call_identifier))\n        call_duration = None\n\n      if call_duration:\n        timestamp += call_duration\n        event_data.call_type = 'FINISHED'\n        date_time = dfdatetime_posix_time.PosixTime(timestamp=timestamp)\n        event = time_events.DateTimeValuesEvent(date_time, 'Call from Skype')\n        parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Parses a call.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      query (str): query that created the row.\n      row (sqlite3.Row): row resulting from query.\n      query (Optional[str]): query."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse a file transfer.", "response": "def ParseFileTransfer(\n      self, parser_mediator, query, row, cache=None, database=None,\n      **unused_kwargs):\n    \"\"\"Parses a file transfer.\n\n    There is no direct relationship between who sends the file and\n    who accepts the file.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      query (str): query that created the row.\n      row (sqlite3.Row): row resulting from query.\n      cache (Optional[SQLiteCache]): cache.\n      database (Optional[SQLiteDatabase]): database.\n    \"\"\"\n    query_hash = hash(query)\n\n    source_dict = cache.GetResults('source')\n    if not source_dict:\n      results = database.Query(self.QUERY_SOURCE_FROM_TRANSFER)\n\n      cache.CacheQueryResults(\n          results, 'source', 'pk_id', ('skypeid', 'skypename'))\n      source_dict = cache.GetResults('source')\n\n    dest_dict = cache.GetResults('destination')\n    if not dest_dict:\n      results = database.Query(self.QUERY_DEST_FROM_TRANSFER)\n\n      cache.CacheQueryResults(\n          results, 'destination', 'parent_id', ('skypeid', 'skypename'))\n      dest_dict = cache.GetResults('destination')\n\n    source = 'Unknown'\n    destination = 'Unknown'\n\n    parent_id = self._GetRowValue(query_hash, row, 'parent_id')\n    partner_dispname = self._GetRowValue(query_hash, row, 'partner_dispname')\n    partner_handle = self._GetRowValue(query_hash, row, 'partner_handle')\n\n    if parent_id:\n      destination = '{0:s} <{1:s}>'.format(partner_handle, partner_dispname)\n      skype_id, skype_name = source_dict.get(parent_id, [None, None])\n      if skype_name:\n        source = '{0:s} <{1:s}>'.format(skype_id, skype_name)\n    else:\n      source = '{0:s} <{1:s}>'.format(partner_handle, partner_dispname)\n\n      pk_id = self._GetRowValue(query_hash, row, 'pk_id')\n      if pk_id:\n        skype_id, skype_name = dest_dict.get(pk_id, [None, None])\n        if skype_name:\n          destination = '{0:s} <{1:s}>'.format(skype_id, skype_name)\n\n    filename = self._GetRowValue(query_hash, row, 'filename')\n    filesize = self._GetRowValue(query_hash, row, 'filesize')\n\n    try:\n      file_size = int(filesize, 10)\n    except (ValueError, TypeError):\n      parser_mediator.ProduceExtractionWarning(\n          'unable to convert file size: {0!s} of file: {1:s}'.format(\n              filesize, filename))\n      file_size = 0\n\n    event_data = SkypeTransferFileEventData()\n    event_data.destination = destination\n    event_data.offset = self._GetRowValue(query_hash, row, 'id')\n    event_data.query = query\n    event_data.source = source\n    event_data.transferred_filename = filename\n    event_data.transferred_filepath = self._GetRowValue(\n        query_hash, row, 'filepath')\n    event_data.transferred_filesize = file_size\n\n    status = self._GetRowValue(query_hash, row, 'status')\n    starttime = self._GetRowValue(query_hash, row, 'starttime')\n\n    if status == 2:\n      if starttime:\n        event_data.action_type = 'SENDSOLICITUDE'\n\n        date_time = dfdatetime_posix_time.PosixTime(timestamp=starttime)\n        event = time_events.DateTimeValuesEvent(\n            date_time, 'File transfer from Skype')\n        parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    elif status == 8:\n      if starttime:\n        event_data.action_type = 'GETSOLICITUDE'\n\n        date_time = dfdatetime_posix_time.PosixTime(timestamp=starttime)\n        event = time_events.DateTimeValuesEvent(\n            date_time, 'File transfer from Skype')\n        parser_mediator.ProduceEventWithEventData(event, event_data)\n\n      accepttime = self._GetRowValue(query_hash, row, 'accepttime')\n      if accepttime:\n        event_data.action_type = 'ACCEPTED'\n\n        date_time = dfdatetime_posix_time.PosixTime(timestamp=accepttime)\n        event = time_events.DateTimeValuesEvent(\n            date_time, 'File transfer from Skype')\n        parser_mediator.ProduceEventWithEventData(event, event_data)\n\n      finishtime = self._GetRowValue(query_hash, row, 'finishtime')\n      if finishtime:\n        event_data.action_type = 'FINISHED'\n\n        date_time = dfdatetime_posix_time.PosixTime(timestamp=finishtime)\n        event = time_events.DateTimeValuesEvent(\n            date_time, 'File transfer from Skype')\n        parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving the amount of memory used by the process.", "response": "def GetUsedMemory(self):\n    \"\"\"Retrieves the amount of memory used by the process.\n\n    Returns:\n      int: amount of memory in bytes used by the process or None\n          if not available.\n    \"\"\"\n    try:\n      memory_info = self._process.memory_info()\n    except psutil.NoSuchProcess:\n      return None\n\n    # Psutil will return different memory information depending on what is\n    # available in that platform.\n    memory_data = getattr(memory_info, 'data', 0)\n    memory_shared = getattr(memory_info, 'shared', 0)\n\n    return memory_data + memory_shared"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef GetEntries(self, parser_mediator, match=None, **unused_kwargs):\n    if 'RememberedNetworks' not in match:\n      return\n\n    for wifi in match['RememberedNetworks']:\n      ssid = wifi.get('SSIDString', 'UNKNOWN_SSID')\n      security_type = wifi.get('SecurityType', 'UNKNOWN_SECURITY_TYPE')\n\n      event_data = plist_event.PlistTimeEventData()\n      event_data.desc = (\n          '[WiFi] Connected to network: <{0:s}> using security {1:s}').format(\n              ssid, security_type)\n      event_data.key = 'item'\n      event_data.root = '/RememberedNetworks'\n\n      datetime_value = wifi.get('LastConnected', None)\n      if datetime_value:\n        event = time_events.PythonDatetimeEvent(\n            datetime_value, definitions.TIME_DESCRIPTION_WRITTEN)\n\n      else:\n        date_time = dfdatetime_semantic_time.SemanticTime('Not set')\n        event = time_events.DateTimeValuesEvent(\n            date_time, definitions.TIME_DESCRIPTION_NOT_A_TIME)\n\n      parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Extracts relevant Airport entries."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts a specific value of the row to an integer.", "response": "def _GetIntegerValue(self, row, value_name):\n    \"\"\"Converts a specific value of the row to an integer.\n\n    Args:\n      row (dict[str, str]): fields of a single row, as specified in COLUMNS.\n      value_name (str): name of the value within the row.\n\n    Returns:\n      int: value or None if the value cannot be converted.\n    \"\"\"\n    value = row.get(value_name, None)\n    try:\n      return int(value, 10)\n    except (TypeError, ValueError):\n      return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses a single row of the log file and produces events.", "response": "def ParseRow(self, parser_mediator, row_offset, row):\n    \"\"\"Parses a line of the log file and produces events.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      row_offset (int): number of the corresponding line.\n      row (dict[str, str]): fields of a single row, as specified in COLUMNS.\n    \"\"\"\n    filename = row.get('name', None)\n    md5_hash = row.get('md5', None)\n    mode = row.get('mode_as_string', None)\n\n    inode_number = row.get('inode', None)\n    if '-' in inode_number:\n      inode_number, _, _ = inode_number.partition('-')\n\n    try:\n      inode_number = int(inode_number, 10)\n    except (TypeError, ValueError):\n      inode_number = None\n\n    data_size = self._GetIntegerValue(row, 'size')\n    user_uid = self._GetIntegerValue(row, 'uid')\n    user_gid = self._GetIntegerValue(row, 'gid')\n\n    event_data = MactimeEventData()\n    event_data.filename = filename\n    event_data.inode = inode_number\n    event_data.md5 = md5_hash\n    event_data.mode_as_string = mode\n    event_data.offset = row_offset\n    event_data.size = data_size\n    event_data.user_gid = user_gid\n\n    if user_uid is None:\n      event_data.user_sid = None\n    else:\n      # Note that the user_sid value is expected to be a string.\n      event_data.user_sid = '{0:d}'.format(user_uid)\n\n    for value_name, timestamp_description in iter(\n        self._TIMESTAMP_DESC_MAP.items()):\n      posix_time = self._GetIntegerValue(row, value_name)\n      # mactime will return 0 if the timestamp is not set.\n      if not posix_time:\n        continue\n\n      date_time = dfdatetime_posix_time.PosixTime(timestamp=posix_time)\n      event = time_events.DateTimeValuesEvent(date_time, timestamp_description)\n      parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef VerifyRow(self, parser_mediator, row):\n    # Sleuthkit version 3 format:\n    # MD5|name|inode|mode_as_string|UID|GID|size|atime|mtime|ctime|crtime\n    # 0|/lost+found|11|d/drwx------|0|0|12288|1337961350|1337961350|1337961350|0\n\n    if row['md5'] != '0' and not self._MD5_RE.match(row['md5']):\n      return False\n\n    # Check if the following columns contain a base 10 integer value if set.\n    for column_name in (\n        'uid', 'gid', 'size', 'atime', 'mtime', 'ctime', 'crtime'):\n      column_value = row.get(column_name, None)\n      if not column_value:\n        continue\n\n      try:\n        int(column_value, 10)\n      except (TypeError, ValueError):\n        return False\n\n    return True", "response": "Verifies if a line of the file is in the expected format."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ParseAutofillRow(\n      self, parser_mediator, query, row, **unused_kwargs):\n    \"\"\"Parses an autofill entry row.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      query (str): query that created the row.\n      row (sqlite3.Row): row.\n    \"\"\"\n    query_hash = hash(query)\n\n    event_data = ChromeAutofillEventData()\n    event_data.field_name = self._GetRowValue(query_hash, row, 'name')\n    event_data.value = self._GetRowValue(query_hash, row, 'value')\n    event_data.usage_count = self._GetRowValue(query_hash, row, 'count')\n    event_data.query = query\n\n    # Create one event for the first time an autofill entry was used\n    timestamp = self._GetRowValue(query_hash, row, 'date_created')\n    date_time = dfdatetime_posix_time.PosixTime(timestamp=timestamp)\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_CREATION)\n    parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    # If the autofill value has been used more than once, create another\n    # event for the most recent time it was used.\n    if event_data.usage_count > 1:\n      timestamp = self._GetRowValue(query_hash, row, 'date_last_used')\n      date_time = dfdatetime_posix_time.PosixTime(timestamp=timestamp)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_LAST_USED)\n      parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Parses an autofill entry row."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _AbortJoin(self, timeout=None):\n    for pid, process in iter(self._processes_per_pid.items()):\n      logger.debug('Waiting for process: {0:s} (PID: {1:d}).'.format(\n          process.name, pid))\n      process.join(timeout=timeout)\n      if not process.is_alive():\n        logger.debug('Process {0:s} (PID: {1:d}) stopped.'.format(\n            process.name, pid))", "response": "Aborts all processes in the process pool by joining with the parent process."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _AbortKill(self):\n    for pid, process in iter(self._processes_per_pid.items()):\n      if not process.is_alive():\n        continue\n\n      logger.warning('Killing process: {0:s} (PID: {1:d}).'.format(\n          process.name, pid))\n      self._KillProcess(pid)", "response": "Aborts all registered processes by sending a SIGKILL or equivalent."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _AbortTerminate(self):\n    for pid, process in iter(self._processes_per_pid.items()):\n      if not process.is_alive():\n        continue\n\n      logger.warning('Terminating process: {0:s} (PID: {1:d}).'.format(\n          process.name, pid))\n      process.terminate()", "response": "Aborts all registered processes by sending a SIGTERM or equivalent."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking the status of a worker process.", "response": "def _CheckStatusWorkerProcess(self, pid):\n    \"\"\"Checks the status of a worker process.\n\n    If a worker process is not responding the process is terminated and\n    a replacement process is started.\n\n    Args:\n      pid (int): process ID (PID) of a registered worker process.\n\n    Raises:\n      KeyError: if the process is not registered with the engine.\n    \"\"\"\n    # TODO: Refactor this method, simplify and separate concerns (monitoring\n    # vs management).\n    self._RaiseIfNotRegistered(pid)\n\n    process = self._processes_per_pid[pid]\n\n    process_status = self._QueryProcessStatus(process)\n    if process_status is None:\n      process_is_alive = False\n    else:\n      process_is_alive = True\n\n    process_information = self._process_information_per_pid[pid]\n    used_memory = process_information.GetUsedMemory() or 0\n\n    if self._worker_memory_limit and used_memory > self._worker_memory_limit:\n      logger.warning((\n          'Process: {0:s} (PID: {1:d}) killed because it exceeded the '\n          'memory limit: {2:d}.').format(\n              process.name, pid, self._worker_memory_limit))\n      self._KillProcess(pid)\n\n    if isinstance(process_status, dict):\n      self._rpc_errors_per_pid[pid] = 0\n      status_indicator = process_status.get('processing_status', None)\n\n    else:\n      rpc_errors = self._rpc_errors_per_pid.get(pid, 0) + 1\n      self._rpc_errors_per_pid[pid] = rpc_errors\n\n      if rpc_errors > self._MAXIMUM_RPC_ERRORS:\n        process_is_alive = False\n\n      if process_is_alive:\n        rpc_port = process.rpc_port.value\n        logger.warning((\n            'Unable to retrieve process: {0:s} (PID: {1:d}) status via '\n            'RPC socket: http://localhost:{2:d}').format(\n                process.name, pid, rpc_port))\n\n        processing_status_string = 'RPC error'\n        status_indicator = definitions.STATUS_INDICATOR_RUNNING\n      else:\n        processing_status_string = 'killed'\n        status_indicator = definitions.STATUS_INDICATOR_KILLED\n\n      process_status = {\n          'processing_status': processing_status_string}\n\n    self._UpdateProcessingStatus(pid, process_status, used_memory)\n\n    # _UpdateProcessingStatus can also change the status of the worker,\n    # So refresh the status if applicable.\n    for worker_status in self._processing_status.workers_status:\n      if worker_status.pid == pid:\n        status_indicator = worker_status.status\n        break\n\n    if status_indicator in definitions.ERROR_STATUS_INDICATORS:\n      logger.error((\n          'Process {0:s} (PID: {1:d}) is not functioning correctly. '\n          'Status code: {2!s}.').format(process.name, pid, status_indicator))\n\n      self._TerminateProcessByPid(pid)\n\n      replacement_process = None\n      for replacement_process_attempt in range(\n          self._MAXIMUM_REPLACEMENT_RETRIES):\n        logger.info((\n            'Attempt: {0:d} to start replacement worker process for '\n            '{1:s}').format(replacement_process_attempt + 1, process.name))\n\n        replacement_process = self._StartWorkerProcess(\n            process.name, self._storage_writer)\n        if replacement_process:\n          break\n\n        time.sleep(self._REPLACEMENT_WORKER_RETRY_DELAY)\n\n      if not replacement_process:\n        logger.error(\n            'Unable to create replacement worker process for: {0:s}'.format(\n                process.name))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _KillProcess(self, pid):\n    if sys.platform.startswith('win'):\n      process_terminate = 1\n      handle = ctypes.windll.kernel32.OpenProcess(\n          process_terminate, False, pid)\n      ctypes.windll.kernel32.TerminateProcess(handle, -1)\n      ctypes.windll.kernel32.CloseHandle(handle)\n\n    else:\n      try:\n        os.kill(pid, signal.SIGKILL)\n      except OSError as exception:\n        logger.error('Unable to kill process {0:d} with error: {1!s}'.format(\n            pid, exception))", "response": "Issues a SIGKILL or equivalent to the process."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _QueryProcessStatus(self, process):\n    process_is_alive = process.is_alive()\n    if process_is_alive:\n      rpc_client = self._rpc_clients_per_pid.get(process.pid, None)\n      process_status = rpc_client.CallFunction()\n    else:\n      process_status = None\n    return process_status", "response": "Queries a process to determine its status."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nregisters a process with the engine.", "response": "def _RegisterProcess(self, process):\n    \"\"\"Registers a process with the engine.\n\n    Args:\n      process (MultiProcessBaseProcess): process.\n\n    Raises:\n      KeyError: if the process is already registered with the engine.\n      ValueError: if the process is missing.\n    \"\"\"\n    if process is None:\n      raise ValueError('Missing process.')\n\n    if process.pid in self._processes_per_pid:\n      raise KeyError(\n          'Already managing process: {0!s} (PID: {1:d})'.format(\n              process.name, process.pid))\n\n    self._processes_per_pid[process.pid] = process"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _StartMonitoringProcess(self, process):\n    if process is None:\n      raise ValueError('Missing process.')\n\n    pid = process.pid\n\n    if pid in self._process_information_per_pid:\n      raise KeyError(\n          'Already monitoring process (PID: {0:d}).'.format(pid))\n\n    if pid in self._rpc_clients_per_pid:\n      raise KeyError(\n          'RPC client (PID: {0:d}) already exists'.format(pid))\n\n    rpc_client = plaso_xmlrpc.XMLProcessStatusRPCClient()\n\n    # Make sure that a worker process has started its RPC server.\n    # The RPC port will be 0 if no server is available.\n    rpc_port = process.rpc_port.value\n    time_waited_for_process = 0.0\n    while not rpc_port:\n      time.sleep(0.1)\n      rpc_port = process.rpc_port.value\n      time_waited_for_process += 0.1\n\n      if time_waited_for_process >= self._RPC_SERVER_TIMEOUT:\n        raise IOError(\n            'RPC client unable to determine server (PID: {0:d}) port.'.format(\n                pid))\n\n    hostname = 'localhost'\n\n    if not rpc_client.Open(hostname, rpc_port):\n      raise IOError((\n          'RPC client unable to connect to server (PID: {0:d}) '\n          'http://{1:s}:{2:d}').format(pid, hostname, rpc_port))\n\n    self._rpc_clients_per_pid[pid] = rpc_client\n    self._process_information_per_pid[pid] = process_info.ProcessInfo(pid)", "response": "Starts monitoring a process."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _StartStatusUpdateThread(self):\n    self._status_update_active = True\n    self._status_update_thread = threading.Thread(\n        name='Status update', target=self._StatusUpdateThreadMain)\n    self._status_update_thread.start()", "response": "Starts the status update thread."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nstops monitoring a process.", "response": "def _StopMonitoringProcess(self, process):\n    \"\"\"Stops monitoring a process.\n\n    Args:\n      process (MultiProcessBaseProcess): process.\n\n    Raises:\n      KeyError: if the process is not monitored.\n      ValueError: if the process is missing.\n    \"\"\"\n    if process is None:\n      raise ValueError('Missing process.')\n\n    pid = process.pid\n\n    self._RaiseIfNotMonitored(pid)\n\n    del self._process_information_per_pid[pid]\n\n    rpc_client = self._rpc_clients_per_pid.get(pid, None)\n    if rpc_client:\n      rpc_client.Close()\n      del self._rpc_clients_per_pid[pid]\n\n    if pid in self._rpc_errors_per_pid:\n      del self._rpc_errors_per_pid[pid]\n\n    logger.debug('Stopped monitoring process: {0:s} (PID: {1:d})'.format(\n        process.name, pid))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nstop monitoring all processes.", "response": "def _StopMonitoringProcesses(self):\n    \"\"\"Stops monitoring all processes.\"\"\"\n    # We need to make a copy of the list of pids since we are changing\n    # the dict in the loop.\n    for pid in list(self._process_information_per_pid.keys()):\n      self._RaiseIfNotRegistered(pid)\n      process = self._processes_per_pid[pid]\n\n      self._StopMonitoringProcess(process)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nstops the status update thread.", "response": "def _StopStatusUpdateThread(self):\n    \"\"\"Stops the status update thread.\"\"\"\n    self._status_update_active = False\n    if self._status_update_thread.isAlive():\n      self._status_update_thread.join()\n    self._status_update_thread = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _TerminateProcessByPid(self, pid):\n    self._RaiseIfNotRegistered(pid)\n\n    process = self._processes_per_pid[pid]\n\n    self._TerminateProcess(process)\n    self._StopMonitoringProcess(process)", "response": "Terminate a process that s monitored by the engine."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _TerminateProcess(self, process):\n    pid = process.pid\n    logger.warning('Terminating process: (PID: {0:d}).'.format(pid))\n    process.terminate()\n\n    # Wait for the process to exit.\n    process.join(timeout=self._PROCESS_JOIN_TIMEOUT)\n\n    if process.is_alive():\n      logger.warning('Killing process: (PID: {0:d}).'.format(pid))\n      self._KillProcess(pid)", "response": "Terminate a process.\n\n    Args:\n      process (MultiProcessBaseProcess): process to terminate."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd command line arguments to an argument group.", "response": "def AddArguments(cls, argument_group):\n    \"\"\"Adds command line arguments to an argument group.\n\n    This function takes an argument parser or an argument group object and adds\n    to it all the command line arguments this helper supports.\n\n    Args:\n      argument_group (argparse._ArgumentGroup|argparse.ArgumentParser):\n          argparse group.\n    \"\"\"\n    argument_group.add_argument(\n        '-o', '--output_format', '--output-format', metavar='FORMAT',\n        dest='output_format', default='dynamic', help=(\n            'The output format. Use \"-o list\" to see a list of available '\n            'output formats.'))\n\n    argument_group.add_argument(\n        '-w', '--write', metavar='OUTPUT_FILE', dest='write',\n        help='Output filename.')\n\n    # TODO: determine if this is repeated elsewhere and refactor this into\n    # a helper function.\n    arguments = sys.argv[1:]\n    argument_index = 0\n\n    if '-o' in arguments:\n      argument_index = arguments.index('-o') + 1\n    elif '--output_format' in arguments:\n      argument_index = arguments.index('--output_format') + 1\n    elif '--output-format' in arguments:\n      argument_index = arguments.index('--output-format') + 1\n\n    if 0 < argument_index < len(arguments):\n      names = [name.strip() for name in arguments[argument_index].split(',')]\n    else:\n      names = ['dynamic']\n\n    if names and names != ['list']:\n      manager.ArgumentHelperManager.AddCommandLineArguments(\n          argument_group, category='output', names=names)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing and validates the options.", "response": "def ParseOptions(cls, options, configuration_object):\n    \"\"\"Parses and validates options.\n\n    Args:\n      options (argparse.Namespace): parser options.\n      configuration_object (CLITool): object to be configured by the argument\n          helper.\n\n    Raises:\n      BadConfigObject: when the configuration object is of the wrong type.\n    \"\"\"\n    if not isinstance(configuration_object, tools.CLITool):\n      raise errors.BadConfigObject(\n          'Configuration object is not an instance of CLITool')\n\n    output_format = getattr(options, 'output_format', 'dynamic')\n    output_filename = getattr(options, 'write', None)\n\n    if output_format != 'list':\n      if not output_manager.OutputManager.HasOutputClass(output_format):\n        raise errors.BadConfigOption(\n            'Unsupported output format: {0:s}.'.format(output_format))\n\n    if output_manager.OutputManager.IsLinearOutputModule(output_format):\n      if not output_filename:\n        raise errors.BadConfigOption((\n            'Output format: {0:s} requires an output file').format(\n                output_format))\n\n      if os.path.exists(output_filename):\n        raise errors.BadConfigOption(\n            'Output file already exists: {0:s}.'.format(output_filename))\n\n    setattr(configuration_object, '_output_format', output_format)\n    setattr(configuration_object, '_output_filename', output_filename)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef PyParseRangeCheck(lower_bound, upper_bound):\n  # pylint: disable=unused-argument\n  def CheckRange(string, location, tokens):\n    \"\"\"Parse the arguments.\n\n    Args:\n      string (str): original string.\n      location (int): location in the string where the match was made\n      tokens (list[str]): tokens.\n    \"\"\"\n    try:\n      check_number = tokens[0]\n    except IndexError:\n      check_number = -1\n\n    if check_number < lower_bound:\n      raise pyparsing.ParseException(\n          'Value: {0:d} precedes lower bound: {1:d}'.format(\n              check_number, lower_bound))\n\n    if check_number > upper_bound:\n      raise pyparsing.ParseException(\n          'Value: {0:d} exceeds upper bound: {1:d}'.format(\n              check_number, upper_bound))\n\n  # Since callback methods for pyparsing need to accept certain parameters\n  # and there is no way to define conditions, like upper and lower bounds\n  # we need to return here a method that accepts those pyparsing parameters.\n  return CheckRange", "response": "A function that returns a function that checks that a number is within a defined range."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn an integer from a string.", "response": "def PyParseIntCast(string, location, tokens):\n  \"\"\"Return an integer from a string.\n\n  This is a pyparsing callback method that converts the matched\n  string into an integer.\n\n  The method modifies the content of the tokens list and converts\n  them all to an integer value.\n\n  Args:\n    string (str): original string.\n    location (int): location in the string where the match was made.\n    tokens (list[str]): extracted tokens, where the string to be converted\n        is stored.\n  \"\"\"\n  # Cast the regular tokens.\n  for index, token in enumerate(tokens):\n    try:\n      tokens[index] = int(token)\n    except ValueError:\n      logger.error('Unable to cast [{0:s}] to an int, setting to 0'.format(\n          token))\n      tokens[index] = 0\n\n  # We also need to cast the dictionary built tokens.\n  for key in tokens.keys():\n    try:\n      tokens[key] = int(tokens[key], 10)\n    except ValueError:\n      logger.error(\n          'Unable to cast [{0:s} = {1:d}] to an int, setting to 0'.format(\n              key, tokens[key]))\n      tokens[key] = 0"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a joined token from a list of tokens.", "response": "def PyParseJoinList(string, location, tokens):\n  \"\"\"Return a joined token from a list of tokens.\n\n  This is a callback method for pyparsing setParseAction that modifies\n  the returned token list to join all the elements in the list to a single\n  token.\n\n  Args:\n    string (str): original string.\n    location (int): location in the string where the match was made.\n    tokens (list[str]): extracted tokens, where the string to be converted\n        is stored.\n  \"\"\"\n  join_list = []\n  for token in tokens:\n    try:\n      join_list.append(str(token))\n    except UnicodeDecodeError:\n      join_list.append(repr(token))\n\n  tokens[0] = ''.join(join_list)\n  del tokens[1:]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _IsText(self, bytes_in, encoding=None):\n    # TODO: Improve speed and accuracy of this method.\n    # Start with the assumption we are dealing with text.\n    is_text = True\n\n    if isinstance(bytes_in, py2to3.UNICODE_TYPE):\n      return is_text\n\n    # Check if this is ASCII text string.\n    for value in bytes_in:\n      if py2to3.PY_2:\n        value = ord(value)\n      if not 31 < value < 128:\n        is_text = False\n        break\n\n    # We have an ASCII string.\n    if is_text:\n      return is_text\n\n    # Check if this is UTF-8\n    try:\n      bytes_in.decode('utf-8')\n      return True\n\n    except UnicodeDecodeError:\n      pass\n\n    if encoding:\n      try:\n        bytes_in.decode(encoding)\n        return True\n\n      except LookupError:\n        logger.error('Unsupported encoding: {0:s}'.format(encoding))\n      except UnicodeDecodeError:\n        pass\n\n    return False", "response": "Examine the bytes in and determine if they are indicative of text."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading a single line from a text file.", "response": "def _ReadLine(self, text_file_object, max_len=None, depth=0):\n    \"\"\"Reads a line from a text file.\n\n    Args:\n      text_file_object (dfvfs.TextFile): text file.\n      max_len (Optional[int]): maximum number of bytes a single line can take,\n          where None means all remaining bytes should be read.\n      depth (Optional[int]): number of new lines the parser encountered.\n\n    Returns:\n      str: single line read from the file-like object, or the maximum number of\n          characters, if max_len defined and line longer than the defined size.\n\n    Raises:\n      UnicodeDecodeError: if the text cannot be decoded using the specified\n          encoding.\n    \"\"\"\n    line = text_file_object.readline(size=max_len)\n\n    if not line:\n      return ''\n\n    if line in self._EMPTY_LINES:\n      if depth == self._MAXIMUM_DEPTH:\n        return ''\n\n      return self._ReadLine(text_file_object, max_len=max_len, depth=depth + 1)\n\n    return line.strip()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse a text file - like object using a pyparsing definition.", "response": "def ParseFileObject(self, parser_mediator, file_object):\n    \"\"\"Parses a text file-like object using a pyparsing definition.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      file_object (dfvfs.FileIO): file-like object.\n\n    Raises:\n      UnableToParseFile: when the file cannot be parsed.\n    \"\"\"\n    # TODO: self._line_structures is a work-around and this needs\n    # a structural fix.\n    if not self._line_structures:\n      raise errors.UnableToParseFile(\n          'Line structure undeclared, unable to proceed.')\n\n    encoding = self._ENCODING or parser_mediator.codepage\n    text_file_object = text_file.TextFile(file_object, encoding=encoding)\n\n    try:\n      line = self._ReadLine(text_file_object, max_len=self.MAX_LINE_LENGTH)\n    except UnicodeDecodeError:\n      raise errors.UnableToParseFile(\n          'Not a text file or encoding not supported.')\n\n    if not line:\n      raise errors.UnableToParseFile('Not a text file.')\n\n    if len(line) == self.MAX_LINE_LENGTH or len(\n        line) == self.MAX_LINE_LENGTH - 1:\n      logger.debug((\n          'Trying to read a line and reached the maximum allowed length of '\n          '{0:d}. The last few bytes of the line are: {1:s} [parser '\n          '{2:s}]').format(\n              self.MAX_LINE_LENGTH, repr(line[-10:]), self.NAME))\n\n    if not self._IsText(line):\n      raise errors.UnableToParseFile('Not a text file, unable to proceed.')\n\n    if not self.VerifyStructure(parser_mediator, line):\n      raise errors.UnableToParseFile('Wrong file structure.')\n\n    consecutive_line_failures = 0\n    index = None\n    # Set the offset to the beginning of the file.\n    self._current_offset = 0\n    # Read every line in the text file.\n    while line:\n      if parser_mediator.abort:\n        break\n      parsed_structure = None\n      use_key = None\n      # Try to parse the line using all the line structures.\n      for index, (key, structure) in enumerate(self._line_structures):\n        try:\n          parsed_structure = structure.parseString(line)\n        except pyparsing.ParseException:\n          pass\n        if parsed_structure:\n          use_key = key\n          break\n\n      if parsed_structure:\n        self.ParseRecord(parser_mediator, use_key, parsed_structure)\n        consecutive_line_failures = 0\n        if index is not None and index != 0:\n          key_structure = self._line_structures.pop(index)\n          self._line_structures.insert(0, key_structure)\n      else:\n        if len(line) > 80:\n          line = '{0:s}...'.format(line[:77])\n        parser_mediator.ProduceExtractionWarning(\n            'unable to parse log line: {0:s} at offset: {1:d}'.format(\n                repr(line), self._current_offset))\n        consecutive_line_failures += 1\n        if (consecutive_line_failures >\n            self.MAXIMUM_CONSECUTIVE_LINE_FAILURES):\n          raise errors.UnableToParseFile(\n              'more than {0:d} consecutive failures to parse lines.'.format(\n                  self.MAXIMUM_CONSECUTIVE_LINE_FAILURES))\n\n      self._current_offset = text_file_object.get_offset()\n\n      try:\n        line = self._ReadLine(text_file_object, max_len=self.MAX_LINE_LENGTH)\n      except UnicodeDecodeError:\n        parser_mediator.ProduceExtractionWarning(\n            'unable to read and decode log line at offset {0:d}'.format(\n                self._current_offset))\n        break"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _ReadLine(self, file_object):\n    if len(self._buffer) < self._buffer_size:\n      content = file_object.read(self._buffer_size)\n      content = content.decode(self._encoding)\n      self._buffer = ''.join([self._buffer, content])\n\n    line, new_line, self._buffer = self._buffer.partition('\\n')\n    if not line and not new_line:\n      line = self._buffer\n      self._buffer = ''\n\n    self._current_offset += len(line)\n\n    # Strip carriage returns from the text.\n    if line.endswith('\\r'):\n      line = line[:-len('\\r')]\n\n    if new_line:\n      line = ''.join([line, '\\n'])\n      self._current_offset += len('\\n')\n\n    return line", "response": "Reads a single line from the file - like object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading a line from the lines buffer.", "response": "def ReadLine(self, file_object):\n    \"\"\"Reads a line.\n\n    Args:\n      file_object (dfvfs.FileIO): file-like object.\n\n    Returns:\n      str: line read from the lines buffer.\n    \"\"\"\n    line, _, self.lines = self.lines.partition('\\n')\n    if not line:\n      self.ReadLines(file_object)\n      line, _, self.lines = self.lines.partition('\\n')\n\n    return line"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads lines into the lines buffer.", "response": "def ReadLines(self, file_object):\n    \"\"\"Reads lines into the lines buffer.\n\n    Args:\n      file_object (dfvfs.FileIO): file-like object.\n    \"\"\"\n    lines_size = len(self.lines)\n    if lines_size < self._buffer_size:\n      lines_size = self._buffer_size - lines_size\n      while lines_size > 0:\n        line = self._ReadLine(file_object)\n        if not line:\n          break\n\n        self.lines = ''.join([self.lines, line])\n        lines_size -= len(line)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef SkipAhead(self, file_object, number_of_characters):\n    lines_size = len(self.lines)\n    while number_of_characters >= lines_size:\n      number_of_characters -= lines_size\n\n      self.lines = ''\n      self.ReadLines(file_object)\n      lines_size = len(self.lines)\n      if lines_size == 0:\n        return\n\n    self.lines = self.lines[number_of_characters:]", "response": "Skips ahead a number of characters."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse a text file - like object using a pyparsing definition.", "response": "def ParseFileObject(self, parser_mediator, file_object):\n    \"\"\"Parses a text file-like object using a pyparsing definition.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      file_object (dfvfs.FileIO): file-like object.\n\n    Raises:\n      UnableToParseFile: when the file cannot be parsed.\n    \"\"\"\n    if not self.LINE_STRUCTURES:\n      raise errors.UnableToParseFile('Missing line structures.')\n\n    encoding = self._ENCODING or parser_mediator.codepage\n    text_reader = EncodedTextReader(\n        encoding, buffer_size=self.BUFFER_SIZE)\n\n    text_reader.Reset()\n\n    try:\n      text_reader.ReadLines(file_object)\n    except UnicodeDecodeError as exception:\n      raise errors.UnableToParseFile(\n          'Not a text file, with error: {0!s}'.format(exception))\n\n    if not self.VerifyStructure(parser_mediator, text_reader.lines):\n      raise errors.UnableToParseFile('Wrong file structure.')\n\n    # Using parseWithTabs() overrides Pyparsing's default replacement of tabs\n    # with spaces to SkipAhead() the correct number of bytes after a match.\n    for key, structure in self.LINE_STRUCTURES:\n      structure.parseWithTabs()\n\n\n    consecutive_line_failures = 0\n    # Read every line in the text file.\n    while text_reader.lines:\n      if parser_mediator.abort:\n        break\n\n      # Initialize pyparsing objects.\n      tokens = None\n      start = 0\n      end = 0\n\n      key = None\n\n      index = None\n\n      # Try to parse the line using all the line structures.\n      for index, (key, structure) in enumerate(self._line_structures):\n        try:\n          structure_generator = structure.scanString(\n              text_reader.lines, maxMatches=1)\n          parsed_structure = next(structure_generator, None)\n        except pyparsing.ParseException:\n          parsed_structure = None\n\n        if not parsed_structure:\n          continue\n\n        tokens, start, end = parsed_structure\n\n        # Only want to parse the structure if it starts\n        # at the beginning of the buffer.\n        if start == 0:\n          break\n\n      if tokens and start == 0:\n        # Move matching key, structure pair to the front of the list, so that\n        # structures that are more likely to match are tried first.\n        if index is not None and index != 0:\n          key_structure = self._line_structures.pop(index)\n          self._line_structures.insert(0, key_structure)\n\n        try:\n          self.ParseRecord(parser_mediator, key, tokens)\n          consecutive_line_failures = 0\n        except (errors.ParseError, errors.TimestampError) as exception:\n          parser_mediator.ProduceExtractionWarning(\n              'unable to parse record: {0:s} with error: {1!s}'.format(\n                  key, exception))\n\n        text_reader.SkipAhead(file_object, end)\n\n      else:\n        odd_line = text_reader.ReadLine(file_object)\n        if odd_line:\n          if len(odd_line) > 80:\n            odd_line = '{0:s}...'.format(odd_line[:77])\n          parser_mediator.ProduceExtractionWarning(\n              'unable to parse log line: {0:s}'.format(repr(odd_line)))\n          consecutive_line_failures += 1\n          if (consecutive_line_failures >\n              self.MAXIMUM_CONSECUTIVE_LINE_FAILURES):\n            raise errors.UnableToParseFile(\n                'more than {0:d} consecutive failures to parse lines.'.format(\n                    self.MAXIMUM_CONSECUTIVE_LINE_FAILURES))\n      try:\n        text_reader.ReadLines(file_object)\n      except UnicodeDecodeError as exception:\n        parser_mediator.ProduceExtractionWarning(\n            'unable to read lines with error: {0!s}'.format(exception))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nextracts uTorrent entries from the given bencoded data.", "response": "def GetEntries(self, parser_mediator, data=None, **unused_kwargs):\n    \"\"\"Extracts uTorrent active torrents.\n\n    This is the main parsing engine for the plugin. It determines if\n    the selected file is the proper file to parse and extracts current\n    running torrents.\n\n    interface.Process() checks for the given BENCODE_KEYS set, ensures\n    that it matches, and then passes the bencoded data to this function for\n    parsing. This plugin then parses the entire set of bencoded data to extract\n    the variable file-name keys to retrieve their values.\n\n    uTorrent creates a file, resume.dat, and a backup, resume.dat.old, to\n    for all active torrents. This is typically stored in the user's\n    application data folder.\n\n    These files, at a minimum, contain a '.fileguard' key and a dictionary\n    with a key name for a particular download with a '.torrent' file\n    extension.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      data (Optional[dict[str, object]]): bencode data values.\n    \"\"\"\n    # Walk through one of the torrent keys to ensure it's from a valid file.\n    for key, value in iter(data.items()):\n      if not '.torrent' in key:\n        continue\n\n      caption = value.get('caption')\n      path = value.get('path')\n      seedtime = value.get('seedtime')\n      if not caption or not path or seedtime < 0:\n        raise errors.WrongBencodePlugin(self.NAME)\n\n    for torrent, value in iter(data.items()):\n      if not '.torrent' in torrent:\n        continue\n\n      event_data = UTorrentEventData()\n      event_data.caption = value.get('caption', None)\n      event_data.path = value.get('path', None)\n\n      # Convert seconds to minutes.\n      seedtime = value.get('seedtime', None)\n      event_data.seedtime, _ = divmod(seedtime, 60)\n\n      # Create timeline events based on extracted values.\n      for event_key, event_value in iter(value.items()):\n        if event_key == 'added_on':\n          date_time = dfdatetime_posix_time.PosixTime(timestamp=event_value)\n          event = time_events.DateTimeValuesEvent(\n              date_time, definitions.TIME_DESCRIPTION_ADDED)\n          parser_mediator.ProduceEventWithEventData(event, event_data)\n\n        elif event_key == 'completed_on':\n          date_time = dfdatetime_posix_time.PosixTime(timestamp=event_value)\n          event = time_events.DateTimeValuesEvent(\n              date_time, definitions.TIME_DESCRIPTION_FILE_DOWNLOADED)\n          parser_mediator.ProduceEventWithEventData(event, event_data)\n\n        elif event_key == 'modtimes':\n          for modtime in event_value:\n            # Some values are stored as 0, skip those.\n            if not modtime:\n              continue\n\n            date_time = dfdatetime_posix_time.PosixTime(timestamp=modtime)\n            event = time_events.DateTimeValuesEvent(\n                date_time, definitions.TIME_DESCRIPTION_MODIFICATION)\n            parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nextract Safari history items from a list of entries.", "response": "def GetEntries(self, parser_mediator, match=None, **unused_kwargs):\n    \"\"\"Extracts Safari history items.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      match (Optional[dict[str: object]]): keys extracted from PLIST_KEYS.\n    \"\"\"\n    format_version = match.get('WebHistoryFileVersion', None)\n    if format_version != 1:\n      parser_mediator.ProduceExtractionWarning(\n          'unsupported Safari history version: {0!s}'.format(format_version))\n      return\n\n    if 'WebHistoryDates' not in match:\n      return\n\n    for history_entry in match.get('WebHistoryDates', {}):\n      last_visited_date = history_entry.get('lastVisitedDate', None)\n      if last_visited_date is None:\n        parser_mediator.ProduceExtractionWarning('missing last visited date')\n        continue\n\n      try:\n        # Last visited date is a string containing a floating point value.\n        timestamp = float(last_visited_date)\n      except (TypeError, ValueError):\n        parser_mediator.ProduceExtractionWarning(\n            'unable to convert last visited date {0:s}'.format(\n                last_visited_date))\n        continue\n\n      display_title = history_entry.get('displayTitle', None)\n\n      event_data = SafariHistoryEventData()\n      if display_title != event_data.title:\n        event_data.display_title = display_title\n      event_data.title = history_entry.get('title', None)\n      event_data.url = history_entry.get('', None)\n      event_data.visit_count = history_entry.get('visitCount', None)\n      event_data.was_http_non_get = history_entry.get(\n          'lastVisitWasHTTPNonGet', None)\n\n      # Convert the floating point value to an integer.\n      # TODO: add support for the fractional part of the floating point value.\n      timestamp = int(timestamp)\n      date_time = dfdatetime_cocoa_time.CocoaTime(timestamp=timestamp)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_LAST_VISITED)\n      parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef GetAnalyzersInformation(cls):\n    analyzer_information = []\n    for _, analyzer_class in cls.GetAnalyzers():\n      description = getattr(analyzer_class, 'DESCRIPTION', '')\n      analyzer_information.append((analyzer_class.NAME, description))\n\n    return analyzer_information", "response": "Retrieves the analyzers information."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nretrieving an instance of a specific analyzer.", "response": "def GetAnalyzerInstance(cls, analyzer_name):\n    \"\"\"Retrieves an instance of a specific analyzer.\n\n    Args:\n      analyzer_name (str): name of the analyzer to retrieve.\n\n    Returns:\n      BaseAnalyzer: analyzer instance.\n\n    Raises:\n      KeyError: if analyzer class is not set for the corresponding name.\n    \"\"\"\n    analyzer_name = analyzer_name.lower()\n    if analyzer_name not in cls._analyzer_classes:\n      raise KeyError(\n          'analyzer class not set for name: {0:s}.'.format(analyzer_name))\n\n    analyzer_class = cls._analyzer_classes[analyzer_name]\n    return analyzer_class()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef GetAnalyzerInstances(cls, analyzer_names):\n    analyzer_instances = []\n    for analyzer_name, analyzer_class in iter(cls.GetAnalyzers()):\n      if analyzer_name in analyzer_names:\n        analyzer_instances.append(analyzer_class())\n\n    return analyzer_instances", "response": "Retrieves instances for all the specified analyzers."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef GetAnalyzers(cls):\n    for analyzer_name, analyzer_class in iter(cls._analyzer_classes.items()):\n      yield analyzer_name, analyzer_class", "response": "Retrieves the registered analyzers."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ExamineEvent(self, mediator, event):\n    pathspec = getattr(event, 'pathspec', None)\n    if pathspec is None:\n      return\n    if self._paths_with_hashes.get(pathspec, None):\n      # We've already processed an event with this pathspec and extracted the\n      # hashes from it.\n      return\n    hash_attributes = {}\n    for attribute_name, attribute_value in event.GetAttributes():\n      if attribute_name.endswith('_hash'):\n        hash_attributes[attribute_name] = attribute_value\n    self._paths_with_hashes[pathspec] = hash_attributes", "response": "Analyzes an event and creates a new object containing the hashes as required."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate a string containing a pathspec and its hashes.", "response": "def _GeneratePathString(self, mediator, pathspec, hashes):\n    \"\"\"Generates a string containing a pathspec and its hashes.\n\n    Args:\n      mediator (AnalysisMediator): mediates interactions between analysis\n          plugins and other components, such as storage and dfvfs.\n      pathspec (dfvfs.Pathspec): the path specification) to generate a string\n          for.\n      hashes (dict[str, str]): mapping of hash attribute names to the value of\n          that hash for the path specification being processed.\n\n    Returns:\n      str: string of the form \"display_name: hash_type=hash_value\". For example,\n          \"OS:/path/spec: test_hash=4 other_hash=5\".\n    \"\"\"\n    display_name = mediator.GetDisplayNameForPathSpec(pathspec)\n    path_string = '{0:s}:'.format(display_name)\n    for hash_name, hash_value in sorted(hashes.items()):\n      path_string = '{0:s} {1:s}={2:s}'.format(\n          path_string, hash_name, hash_value)\n    return path_string"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef CompileReport(self, mediator):\n    lines_of_text = ['Listing file paths and hashes']\n    for pathspec, hashes in sorted(\n        self._paths_with_hashes.items(),\n        key=lambda tuple: tuple[0].comparable):\n\n      path_string = self._GeneratePathString(mediator, pathspec, hashes)\n      lines_of_text.append(path_string)\n\n    lines_of_text.append('')\n    report_text = '\\n'.join(lines_of_text)\n    return reports.AnalysisReport(plugin_name=self.NAME, text=report_text)", "response": "Compiles an analysis report."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse the event data form a variable - length data section.", "response": "def _ParseEventData(self, variable_length_section):\n    \"\"\"Parses the event data form a variable-length data section.\n\n    Args:\n      variable_length_section (job_variable_length_data_section): a\n          Windows Scheduled Task job variable-length data section.\n\n    Returns:\n      WinJobEventData: event data of the job file.\n    \"\"\"\n    event_data = WinJobEventData()\n    event_data.application = (\n        variable_length_section.application_name.rstrip('\\x00'))\n    event_data.comment = variable_length_section.comment.rstrip('\\x00')\n    event_data.parameters = (\n        variable_length_section.parameters.rstrip('\\x00'))\n    event_data.username = variable_length_section.author.rstrip('\\x00')\n    event_data.working_directory = (\n        variable_length_section.working_directory.rstrip('\\x00'))\n\n    return event_data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _ParseLastRunTime(self, parser_mediator, fixed_length_section):\n    systemtime_struct = fixed_length_section.last_run_time\n    system_time_tuple = (\n        systemtime_struct.year, systemtime_struct.month,\n        systemtime_struct.weekday, systemtime_struct.day_of_month,\n        systemtime_struct.hours, systemtime_struct.minutes,\n        systemtime_struct.seconds, systemtime_struct.milliseconds)\n\n    date_time = None\n    if system_time_tuple != self._EMPTY_SYSTEM_TIME_TUPLE:\n      try:\n        date_time = dfdatetime_systemtime.Systemtime(\n            system_time_tuple=system_time_tuple)\n      except ValueError:\n        parser_mediator.ProduceExtractionWarning(\n            'invalid last run time: {0!s}'.format(system_time_tuple))\n\n    return date_time", "response": "Parses the last run time from a fixed - length data section."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse the end time of a trigger.", "response": "def _ParseTriggerEndTime(self, parser_mediator, trigger):\n    \"\"\"Parses the end time from a trigger.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      trigger (job_trigger): a trigger.\n\n    Returns:\n      dfdatetime.DateTimeValues: last run date and time or None if not\n          available.\n    \"\"\"\n    time_elements_tuple = (\n        trigger.end_date.year, trigger.end_date.month,\n        trigger.end_date.day_of_month, 0, 0, 0)\n\n    date_time = None\n    if time_elements_tuple != (0, 0, 0, 0, 0, 0):\n      try:\n        date_time = dfdatetime_time_elements.TimeElements(\n            time_elements_tuple=time_elements_tuple)\n        date_time.is_local_time = True\n        # TODO: add functionality to dfdatetime to control precision.\n        date_time._precision = dfdatetime_definitions.PRECISION_1_DAY  # pylint: disable=protected-access\n      except ValueError:\n        parser_mediator.ProduceExtractionWarning(\n            'invalid trigger end time: {0!s}'.format(time_elements_tuple))\n\n    return date_time"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses the start time from a trigger.", "response": "def _ParseTriggerStartTime(self, parser_mediator, trigger):\n    \"\"\"Parses the start time from a trigger.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      trigger (job_trigger): a trigger.\n\n    Returns:\n      dfdatetime.DateTimeValues: last run date and time or None if not\n          available.\n    \"\"\"\n    time_elements_tuple = (\n        trigger.start_date.year, trigger.start_date.month,\n        trigger.start_date.day_of_month, trigger.start_time.hours,\n        trigger.start_time.minutes, 0)\n\n    date_time = None\n    if time_elements_tuple != (0, 0, 0, 0, 0, 0):\n      try:\n        date_time = dfdatetime_time_elements.TimeElements(\n            time_elements_tuple=time_elements_tuple)\n        date_time.is_local_time = True\n        # TODO: add functionality to dfdatetime to control precision.\n        date_time._precision = dfdatetime_definitions.PRECISION_1_MINUTE  # pylint: disable=protected-access\n      except ValueError:\n        parser_mediator.ProduceExtractionWarning(\n            'invalid trigger start time: {0!s}'.format(time_elements_tuple))\n\n    return date_time"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ParseFileObject(self, parser_mediator, file_object):\n    fixed_section_data_map = self._GetDataTypeMap(\n        'job_fixed_length_data_section')\n\n    try:\n      fixed_length_section, file_offset = self._ReadStructureFromFileObject(\n          file_object, 0, fixed_section_data_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.UnableToParseFile(\n          'Unable to parse fixed-length data section with error: {0!s}'.format(\n              exception))\n\n    if not fixed_length_section.product_version in self._PRODUCT_VERSIONS:\n      raise errors.UnableToParseFile(\n          'Unsupported product version in: 0x{0:04x}'.format(\n              fixed_length_section.product_version))\n\n    if not fixed_length_section.format_version == 1:\n      raise errors.UnableToParseFile(\n          'Unsupported format version in: {0:d}'.format(\n              fixed_length_section.format_version))\n\n    variable_section_data_map = self._GetDataTypeMap(\n        'job_variable_length_data_section')\n\n    try:\n      variable_length_section, data_size = self._ReadStructureFromFileObject(\n          file_object, file_offset, variable_section_data_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.UnableToParseFile((\n          'Unable to parse variable-length data section with error: '\n          '{0!s}').format(exception))\n\n    file_offset += data_size\n\n    event_data = self._ParseEventData(variable_length_section)\n\n    date_time = self._ParseLastRunTime(parser_mediator, fixed_length_section)\n    if date_time:\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_LAST_RUN)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    trigger_data_map = self._GetDataTypeMap('job_trigger')\n\n    for trigger_index in range(0, variable_length_section.number_of_triggers):\n      try:\n        trigger, data_size = self._ReadStructureFromFileObject(\n            file_object, file_offset, trigger_data_map)\n      except (ValueError, errors.ParseError) as exception:\n        raise errors.UnableToParseFile((\n            'Unable to parse trigger: {0:d} with error: {2!s}').format(\n                trigger_index, exception))\n\n      file_offset += data_size\n\n      event_data.trigger_type = trigger.trigger_type\n\n      date_time = self._ParseTriggerStartTime(parser_mediator, trigger)\n      if date_time:\n        event = time_events.DateTimeValuesEvent(\n            date_time, definitions.TIME_DESCRIPTION_SCHEDULED_TO_START,\n            time_zone=parser_mediator.timezone)\n        parser_mediator.ProduceEventWithEventData(event, event_data)\n\n      date_time = self._ParseTriggerEndTime(parser_mediator, trigger)\n      if date_time:\n        event = time_events.DateTimeValuesEvent(\n            date_time, definitions.TIME_DESCRIPTION_SCHEDULED_TO_START,\n            time_zone=parser_mediator.timezone)\n        parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Parses a Windows job file - like object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ParseOptions(cls, options, analysis_plugin):\n    if not isinstance(analysis_plugin, tagging.TaggingAnalysisPlugin):\n      raise errors.BadConfigObject(\n          'Analysis plugin is not an instance of TaggingAnalysisPlugin')\n\n    tagging_file = cls._ParseStringOption(options, 'tagging_file')\n    if not tagging_file:\n      raise errors.BadConfigOption(\n          'Tagging analysis plugin requires a tagging file.')\n\n    tagging_file_path = tagging_file\n    if not os.path.isfile(tagging_file_path):\n      # Check if the file exists in the data location path.\n      data_location = getattr(options, 'data_location', None)\n      if data_location:\n        tagging_file_path = os.path.join(data_location, tagging_file)\n\n    if not os.path.isfile(tagging_file_path):\n      raise errors.BadConfigOption(\n          'No such tagging file: {0:s}.'.format(tagging_file))\n\n    try:\n      analysis_plugin.SetAndLoadTagFile(tagging_file_path)\n\n    except UnicodeDecodeError:\n      raise errors.BadConfigOption(\n          'Invalid tagging file: {0:s} encoding must be UTF-8.'.format(\n              tagging_file))\n\n    except errors.TaggingFileError as exception:\n      raise errors.BadConfigOption(\n          'Unable to read tagging file: {0:s} with error: {1!s}'.format(\n              tagging_file, exception))", "response": "Parses and validates the options."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef Append(self, item):\n    if self._index >= self._size:\n      self._index = self._index % self._size\n\n    try:\n      self._list[self._index] = item\n    except IndexError:\n      self._list.append(item)\n    self._index += 1", "response": "Adds an item to the list."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef AddComment(self, comment):\n    if not comment:\n      return\n\n    if not self.comment:\n      self.comment = comment\n    else:\n      self.comment = ''.join([self.comment, comment])", "response": "Adds a comment to the event tag."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef AddLabel(self, label):\n    if not isinstance(label, py2to3.STRING_TYPES):\n      raise TypeError('label is not a string type. Is {0:s}'.format(\n          type(label)))\n    if not self._VALID_LABEL_REGEX.match(label):\n      raise ValueError((\n          'Unsupported label: \"{0:s}\". A label must only consist of '\n          'alphanumeric characters or underscores.').format(label))\n\n    if label not in self.labels:\n      self.labels.append(label)", "response": "Adds a label to the event tag."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef AddLabels(self, labels):\n    for label in labels:\n      if not self._VALID_LABEL_REGEX.match(label):\n        raise ValueError((\n            'Unsupported label: \"{0:s}\". A label must only consist of '\n            'alphanumeric characters or underscores.').format(label))\n\n    for label in labels:\n      if label not in self.labels:\n        self.labels.append(label)", "response": "Adds labels to the event tag."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef CopyToDict(self):\n    result_dict = {\n        'labels': self.labels\n    }\n    if self.comment:\n      result_dict['comment'] = self.comment\n\n    return result_dict", "response": "Copies the event tag to a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef CopyTextToLabel(cls, text, prefix=''):\n    text = '{0:s}{1:s}'.format(prefix, text)\n    return cls._INVALID_LABEL_CHARACTERS_REGEX.sub('_', text)", "response": "Copies a string to a label."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing a syslog message.", "response": "def ParseMessage(self, parser_mediator, key, date_time, tokens):\n    \"\"\"Produces an event from a syslog body that matched one of the grammars.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      key (str): name of the matching grammar.\n      date_time (dfdatetime.DateTimeValues): date and time values.\n      tokens (dict[str, str]): tokens derived from a syslog message based on\n          the defined grammar.\n\n    Raises:\n      ValueError: If an unknown key is provided.\n    \"\"\"\n    if key not in ('failed_connection', 'login', 'opened_connection'):\n      raise ValueError('Unknown grammar key: {0:s}'.format(key))\n\n    if key == 'login':\n      event_data = SSHLoginEventData()\n\n    elif key == 'failed_connection':\n      event_data = SSHFailedConnectionEventData()\n\n    elif key == 'opened_connection':\n      event_data = SSHOpenedConnectionEventData()\n\n    event_data.address = tokens.get('address', None)\n    event_data.authentication_method = tokens.get(\n        'authentication_method', None)\n    event_data.body = tokens.get('body', None)\n    event_data.fingerprint = tokens.get('fingerprint', None)\n    event_data.hostname = tokens.get('hostname', None)\n    # TODO: pass line number to offset or remove.\n    event_data.offset = 0\n    event_data.pid = tokens.get('pid', None)\n    event_data.protocol = tokens.get('protocol', None)\n    event_data.port = tokens.get('port', None)\n    event_data.reporter = tokens.get('reporter', None)\n    event_data.severity = tokens.get('severity', None)\n    event_data.username = tokens.get('username', None)\n\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_WRITTEN)\n    parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _ParseFiletime(self, byte_stream):\n    filetime_map = self._GetDataTypeMap('filetime')\n\n    try:\n      filetime = self._ReadStructureFromByteStream(\n          byte_stream, 0, filetime_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.ParseError(\n          'Unable to parse FILETIME value with error: {0!s}'.format(\n              exception))\n\n    if filetime == 0:\n      return None\n\n    try:\n      return dfdatetime_filetime.Filetime(timestamp=filetime)\n    except ValueError:\n      raise errors.ParseError(\n          'Invalid FILETIME value: 0x{0:08x}'.format(filetime))", "response": "Parses a FILETIME date and time value from a byte stream."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ExtractEvents(self, parser_mediator, registry_key, **kwargs):\n    shutdown_value = registry_key.GetValueByName('ShutdownTime')\n    if not shutdown_value:\n      return\n\n    try:\n      date_time = self._ParseFiletime(shutdown_value.data)\n    except errors.ParseError as exception:\n      parser_mediator.ProduceExtractionWarning(\n          'unable to determine shutdown timestamp with error: {0!s}'.format(\n              exception))\n      return\n\n    if not date_time:\n      date_time = dfdatetime_semantic_time.SemanticTime('Not set')\n\n    event_data = ShutdownWindowsRegistryEventData()\n    event_data.key_path = registry_key.path\n    event_data.offset = shutdown_value.offset\n    event_data.value_name = shutdown_value.name\n\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_LAST_SHUTDOWN)\n    parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Extracts events from a ShutdownTime Windows Registry value."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ParseFileObject(self, parser_mediator, file_object):\n    display_name = parser_mediator.GetDisplayName()\n    self.ParseFileLNKFile(parser_mediator, file_object, display_name)", "response": "Parses a Windows Shortcut file - like object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing a Windows Shortcut file - like object.", "response": "def ParseFileLNKFile(\n      self, parser_mediator, file_object, display_name):\n    \"\"\"Parses a Windows Shortcut (LNK) file-like object.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      file_object (dfvfs.FileIO): file-like object.\n      display_name (str): display name.\n    \"\"\"\n    lnk_file = pylnk.file()\n    lnk_file.set_ascii_codepage(parser_mediator.codepage)\n\n    try:\n      lnk_file.open_file_object(file_object)\n    except IOError as exception:\n      parser_mediator.ProduceExtractionWarning(\n          'unable to open file with error: {0!s}'.format(exception))\n      return\n\n    link_target = None\n    if lnk_file.link_target_identifier_data:\n      # TODO: change file_entry.name to display name once it is generated\n      # correctly.\n      display_name = parser_mediator.GetFilename()\n      shell_items_parser = shell_items.ShellItemsParser(display_name)\n      shell_items_parser.ParseByteStream(\n          parser_mediator, lnk_file.link_target_identifier_data,\n          codepage=parser_mediator.codepage)\n\n      link_target = shell_items_parser.CopyToPath()\n\n    event_data = WinLnkLinkEventData()\n    event_data.birth_droid_file_identifier = (\n        lnk_file.birth_droid_file_identifier)\n    event_data.birth_droid_volume_identifier = (\n        lnk_file.birth_droid_volume_identifier)\n    event_data.command_line_arguments = lnk_file.command_line_arguments\n    event_data.description = lnk_file.description\n    event_data.drive_serial_number = lnk_file.drive_serial_number\n    event_data.drive_type = lnk_file.drive_type\n    event_data.droid_file_identifier = lnk_file.droid_file_identifier\n    event_data.droid_volume_identifier = lnk_file.droid_volume_identifier\n    event_data.env_var_location = lnk_file.environment_variables_location\n    event_data.file_attribute_flags = lnk_file.file_attribute_flags\n    event_data.file_size = lnk_file.file_size\n    event_data.icon_location = lnk_file.icon_location\n    event_data.link_target = link_target\n    event_data.local_path = lnk_file.local_path\n    event_data.network_path = lnk_file.network_path\n    event_data.relative_path = lnk_file.relative_path\n    event_data.volume_label = lnk_file.volume_label\n    event_data.working_directory = lnk_file.working_directory\n\n    access_time = lnk_file.get_file_access_time_as_integer()\n    if access_time != 0:\n      date_time = dfdatetime_filetime.Filetime(timestamp=access_time)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_LAST_ACCESS)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    creation_time = lnk_file.get_file_creation_time_as_integer()\n    if creation_time != 0:\n      date_time = dfdatetime_filetime.Filetime(timestamp=creation_time)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_CREATION)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    modification_time = lnk_file.get_file_modification_time_as_integer()\n    if modification_time != 0:\n      date_time = dfdatetime_filetime.Filetime(timestamp=modification_time)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_MODIFICATION)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    if access_time == 0 and creation_time == 0 and modification_time == 0:\n      date_time = dfdatetime_semantic_time.SemanticTime('Not set')\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_NOT_A_TIME)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    if lnk_file.droid_file_identifier:\n      try:\n        self._ParseDistributedTrackingIdentifier(\n            parser_mediator, lnk_file.droid_file_identifier, display_name)\n      except (TypeError, ValueError) as exception:\n        parser_mediator.ProduceExtractionWarning(\n            'unable to read droid file identifier with error: {0!s}.'.format(\n                exception))\n\n    if lnk_file.birth_droid_file_identifier:\n      try:\n        self._ParseDistributedTrackingIdentifier(\n            parser_mediator, lnk_file.birth_droid_file_identifier, display_name)\n      except (TypeError, ValueError) as exception:\n        parser_mediator.ProduceExtractionWarning((\n            'unable to read birth droid file identifier with error: '\n            '{0!s}.').format(exception))\n\n    lnk_file.close()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndetermine if this is the right plugin for this cookie. Args: parser_mediator (ParserMediator): mediates interactions between parsers and other components, such as storage and dfvfs. cookie_name (str): the name of the cookie value. cookie_data (bytes): the cookie data, as a byte sequence. url (str): the full URL or path where the cookie was set. Raises: errors.WrongPlugin: If the cookie name differs from the one supplied in COOKIE_NAME. ValueError: If cookie_name or cookie_data are not set.", "response": "def Process(self, parser_mediator, cookie_name, cookie_data, url, **kwargs):\n    \"\"\"Determine if this is the right plugin for this cookie.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      cookie_name (str): the name of the cookie value.\n      cookie_data (bytes): the cookie data, as a byte sequence.\n      url (str): the full URL or path where the cookie was set.\n\n    Raises:\n      errors.WrongPlugin: If the cookie name differs from the one\n          supplied in COOKIE_NAME.\n      ValueError: If cookie_name or cookie_data are not set.\n    \"\"\"\n    if cookie_name is None or cookie_data is None:\n      raise ValueError('Cookie name or data are not set.')\n\n    if cookie_name != self.COOKIE_NAME:\n      raise errors.WrongPlugin(\n          'Not the correct cookie plugin for: {0:s} [{1:s}]'.format(\n              cookie_name, self.NAME))\n\n    # This will raise if unhandled keyword arguments are passed.\n    super(BaseCookiePlugin, self).Process(parser_mediator)\n\n    self.GetEntries(parser_mediator, cookie_data=cookie_data, url=url)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding command line arguments to an argument group.", "response": "def AddArguments(cls, argument_group):\n    \"\"\"Adds command line arguments to an argument group.\n\n    This function takes an argument parser or an argument group object and adds\n    to it all the command line arguments this helper supports.\n\n    Args:\n      argument_group (argparse._ArgumentGroup|argparse.ArgumentParser):\n          argparse group.\n    \"\"\"\n    argument_group.add_argument(\n        '--slice', metavar='DATE', dest='slice', type=str, default='',\n        action='store', help=(\n            'Create a time slice around a certain date. This parameter, if '\n            'defined will display all events that happened X minutes before '\n            'and after the defined date. X is controlled by the parameter '\n            '--slice_size but defaults to 5 minutes.'))\n\n    argument_group.add_argument(\n        '--slice_size', '--slice-size', dest='slice_size', type=int, default=5,\n        action='store', help=(\n            'Defines the slice size. In the case of a regular time slice it '\n            'defines the number of minutes the slice size should be. In the '\n            'case of the --slicer it determines the number of events before '\n            'and after a filter match has been made that will be included in '\n            'the result set. The default value is 5. See --slice or --slicer '\n            'for more details about this option.'))\n\n    argument_group.add_argument(\n        '--slicer', dest='slicer', action='store_true', default=False, help=(\n            'Create a time slice around every filter match. This parameter, '\n            'if defined will save all X events before and after a filter '\n            'match has been made. X is defined by the --slice_size '\n            'parameter.'))\n\n    argument_group.add_argument(\n        'filter', nargs='?', action='store', metavar='FILTER', default=None,\n        type=str, help=(\n            'A filter that can be used to filter the dataset before it '\n            'is written into storage. More information about the filters '\n            'and how to use them can be found here: {0:s}').format(\n                cls._DOCUMENTATION_URL))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ParseOptions(cls, options, configuration_object):\n    if not isinstance(configuration_object, tools.CLITool):\n      raise errors.BadConfigObject(\n          'Configuration object is not an instance of CLITool')\n\n    filter_expression = cls._ParseStringOption(options, 'filter')\n\n    filter_object = None\n    if filter_expression:\n      filter_object = event_filter.EventObjectFilter()\n\n      try:\n        filter_object.CompileFilter(filter_expression)\n      except errors.ParseError as exception:\n        raise errors.BadConfigOption((\n            'Unable to compile filter expression with error: '\n            '{0!s}').format(exception))\n\n    time_slice_event_time_string = getattr(options, 'slice', None)\n    time_slice_duration = getattr(options, 'slice_size', 5)\n    use_time_slicer = getattr(options, 'slicer', False)\n\n    # The slice and slicer cannot be set at the same time.\n    if time_slice_event_time_string and use_time_slicer:\n      raise errors.BadConfigOption(\n          'Time slice and slicer cannot be used at the same time.')\n\n    time_slice_event_timestamp = None\n    if time_slice_event_time_string:\n      # Note self._preferred_time_zone is None when not set but represents UTC.\n      preferred_time_zone = getattr(\n          configuration_object, '_preferred_time_zone', None) or 'UTC'\n      timezone = pytz.timezone(preferred_time_zone)\n      time_slice_event_timestamp = timelib.Timestamp.FromTimeString(\n          time_slice_event_time_string, timezone=timezone)\n      if time_slice_event_timestamp is None:\n        raise errors.BadConfigOption(\n            'Unsupported time slice event date and time: {0:s}'.format(\n                time_slice_event_time_string))\n\n    setattr(configuration_object, '_event_filter_expression', filter_expression)\n\n    if filter_object:\n      setattr(configuration_object, '_event_filter', filter_object)\n\n    setattr(configuration_object, '_use_time_slicer', use_time_slicer)\n\n    if time_slice_event_timestamp is not None or use_time_slicer:\n      # Note that time slicer uses the time slice to determine the duration.\n      # TODO: refactor TimeSlice to filters.\n      time_slice = time_slices.TimeSlice(\n          time_slice_event_timestamp, duration=time_slice_duration)\n      setattr(configuration_object, '_time_slice', time_slice)", "response": "Parses and validates the options."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _GetPluginData(self):\n    return_dict = {}\n\n    return_dict['Versions'] = [\n        ('plaso engine', plaso.__version__),\n        ('python', sys.version)]\n\n    hashers_information = hashers_manager.HashersManager.GetHashersInformation()\n    parsers_information = parsers_manager.ParsersManager.GetParsersInformation()\n    plugins_information = (\n        parsers_manager.ParsersManager.GetParserPluginsInformation())\n    presets_information = parsers_manager.ParsersManager.GetPresetsInformation()\n\n    return_dict['Hashers'] = hashers_information\n    return_dict['Parsers'] = parsers_information\n    return_dict['Parser Plugins'] = plugins_information\n    return_dict['Parser Presets'] = presets_information\n\n    return return_dict", "response": "Retrieves the version and various plugin information."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ParseArguments(self):\n    loggers.ConfigureLogging()\n\n    argument_parser = argparse.ArgumentParser(\n        description=self.DESCRIPTION, epilog=self.EPILOG, add_help=False,\n        formatter_class=argparse.RawDescriptionHelpFormatter)\n\n    self.AddBasicOptions(argument_parser)\n\n    helpers_manager.ArgumentHelperManager.AddCommandLineArguments(\n        argument_parser, names=['storage_file'])\n\n    data_location_group = argument_parser.add_argument_group(\n        'data location arguments')\n\n    argument_helper_names = ['artifact_definitions', 'data_location']\n    helpers_manager.ArgumentHelperManager.AddCommandLineArguments(\n        data_location_group, names=argument_helper_names)\n\n    extraction_group = argument_parser.add_argument_group(\n        'extraction arguments')\n\n    argument_helper_names = [\n        'artifact_filters', 'extraction', 'filter_file', 'hashers',\n        'parsers', 'yara_rules']\n    helpers_manager.ArgumentHelperManager.AddCommandLineArguments(\n        extraction_group, names=argument_helper_names)\n\n    self.AddStorageMediaImageOptions(extraction_group)\n    self.AddTimeZoneOption(extraction_group)\n    self.AddVSSProcessingOptions(extraction_group)\n    self.AddCredentialOptions(extraction_group)\n\n    info_group = argument_parser.add_argument_group('informational arguments')\n\n    self.AddInformationalOptions(info_group)\n\n    info_group.add_argument(\n        '--info', dest='show_info', action='store_true', default=False,\n        help='Print out information about supported plugins and parsers.')\n\n    info_group.add_argument(\n        '--use_markdown', '--use-markdown', dest='use_markdown',\n        action='store_true', default=False, help=(\n            'Output lists in Markdown format use in combination with '\n            '\"--hashers list\", \"--parsers list\" or \"--timezone list\"'))\n\n    info_group.add_argument(\n        '--no_dependencies_check', '--no-dependencies-check',\n        dest='dependencies_check', action='store_false', default=True,\n        help='Disable the dependencies check.')\n\n    self.AddLogFileOptions(info_group)\n\n    helpers_manager.ArgumentHelperManager.AddCommandLineArguments(\n        info_group, names=['status_view'])\n\n    output_group = argument_parser.add_argument_group('output arguments')\n\n    helpers_manager.ArgumentHelperManager.AddCommandLineArguments(\n        output_group, names=['text_prepend'])\n\n    processing_group = argument_parser.add_argument_group(\n        'processing arguments')\n\n    self.AddPerformanceOptions(processing_group)\n    self.AddProcessingOptions(processing_group)\n\n    processing_group.add_argument(\n        '--sigsegv_handler', '--sigsegv-handler', dest='sigsegv_handler',\n        action='store_true', default=False, help=(\n            'Enables the SIGSEGV handler. WARNING this functionality is '\n            'experimental and will a deadlock worker process if a real '\n            'segfault is caught, but not signal SIGSEGV. This functionality '\n            'is therefore primarily intended for debugging purposes'))\n\n    profiling_group = argument_parser.add_argument_group('profiling arguments')\n\n    helpers_manager.ArgumentHelperManager.AddCommandLineArguments(\n        profiling_group, names=['profiling'])\n\n    storage_group = argument_parser.add_argument_group('storage arguments')\n\n    helpers_manager.ArgumentHelperManager.AddCommandLineArguments(\n        storage_group, names=['storage_format'])\n\n    argument_parser.add_argument(\n        self._SOURCE_OPTION, action='store', metavar='SOURCE', nargs='?',\n        default=None, type=str, help=(\n            'Path to a source device, file or directory. If the source is '\n            'a supported storage media device or image file, archive file '\n            'or a directory, the files within are processed recursively.'))\n\n    try:\n      options = argument_parser.parse_args()\n    except UnicodeEncodeError:\n      # If we get here we are attempting to print help in a non-Unicode\n      # terminal.\n      self._output_writer.Write('\\n')\n      self._output_writer.Write(argument_parser.format_help())\n      return False\n\n    # Properly prepare the attributes according to local encoding.\n    if self.preferred_encoding == 'ascii':\n      logger.warning(\n          'The preferred encoding of your system is ASCII, which is not '\n          'optimal for the typically non-ASCII characters that need to be '\n          'parsed and processed. The tool will most likely crash and die, '\n          'perhaps in a way that may not be recoverable. A five second delay '\n          'is introduced to give you time to cancel the runtime and '\n          'reconfigure your preferred encoding, otherwise continue at own '\n          'risk.')\n      time.sleep(5)\n\n    if self._process_archives:\n      logger.warning(\n          'Scanning archive files currently can cause deadlock. Continue at '\n          'your own risk.')\n      time.sleep(5)\n\n    try:\n      self.ParseOptions(options)\n    except errors.BadConfigOption as exception:\n      self._output_writer.Write('ERROR: {0!s}\\n'.format(exception))\n      self._output_writer.Write('\\n')\n      self._output_writer.Write(argument_parser.format_usage())\n      return False\n\n    self._command_line_arguments = self.GetCommandLineArguments()\n\n    loggers.ConfigureLogging(\n        debug_output=self._debug_mode, filename=self._log_file,\n        quiet_mode=self._quiet_mode)\n\n    return True", "response": "Parses the command line arguments."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ParseOptions(self, options):\n    # The extraction options are dependent on the data location.\n    helpers_manager.ArgumentHelperManager.ParseOptions(\n        options, self, names=['data_location'])\n\n    self._ReadParserPresetsFromFile()\n\n    # Check the list options first otherwise required options will raise.\n    argument_helper_names = ['hashers', 'parsers', 'profiling']\n    helpers_manager.ArgumentHelperManager.ParseOptions(\n        options, self, names=argument_helper_names)\n\n    self._ParseTimezoneOption(options)\n\n    self.list_hashers = self._hasher_names_string == 'list'\n    self.list_parsers_and_plugins = self._parser_filter_expression == 'list'\n    self.list_profilers = self._profilers == 'list'\n\n    self.show_info = getattr(options, 'show_info', False)\n    self.show_troubleshooting = getattr(options, 'show_troubleshooting', False)\n\n    if getattr(options, 'use_markdown', False):\n      self._views_format_type = views.ViewsFactory.FORMAT_TYPE_MARKDOWN\n\n    self.dependencies_check = getattr(options, 'dependencies_check', True)\n\n    if (self.list_hashers or self.list_parsers_and_plugins or\n        self.list_profilers or self.list_timezones or self.show_info or\n        self.show_troubleshooting):\n      return\n\n    self._ParseInformationalOptions(options)\n\n    argument_helper_names = [\n        'artifact_definitions', 'artifact_filters', 'extraction',\n        'filter_file', 'status_view', 'storage_file', 'storage_format',\n        'text_prepend', 'yara_rules']\n    helpers_manager.ArgumentHelperManager.ParseOptions(\n        options, self, names=argument_helper_names)\n\n    self._ParseLogFileOptions(options)\n\n    self._ParseStorageMediaOptions(options)\n\n    self._ParsePerformanceOptions(options)\n    self._ParseProcessingOptions(options)\n\n    if not self._storage_file_path:\n      raise errors.BadConfigOption('Missing storage file option.')\n\n    serializer_format = getattr(\n        options, 'serializer_format', definitions.SERIALIZER_FORMAT_JSON)\n    if serializer_format not in definitions.SERIALIZER_FORMATS:\n      raise errors.BadConfigOption(\n          'Unsupported storage serializer format: {0:s}.'.format(\n              serializer_format))\n    self._storage_serializer_format = serializer_format\n\n    # TODO: where is this defined?\n    self._operating_system = getattr(options, 'os', None)\n\n    if self._operating_system:\n      self._mount_path = getattr(options, 'filename', None)\n\n    helpers_manager.ArgumentHelperManager.ParseOptions(\n        options, self, names=['status_view'])\n\n    self._enable_sigsegv_handler = getattr(options, 'sigsegv_handler', False)\n\n    self._EnforceProcessMemoryLimit(self._process_memory_limit)", "response": "Parses the options.\n\n    Args:\n      options (argparse.Namespace): command line arguments.\n\n    Raises:\n      BadConfigOption: if the options are invalid."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ExtractEventsFromSources(self):\n    self._CheckStorageFile(self._storage_file_path, warn_about_existing=True)\n\n    scan_context = self.ScanSource(self._source_path)\n    self._source_type = scan_context.source_type\n\n    self._status_view.SetMode(self._status_view_mode)\n    self._status_view.SetSourceInformation(\n        self._source_path, self._source_type,\n        artifact_filters=self._artifact_filters,\n        filter_file=self._filter_file)\n\n    status_update_callback = (\n        self._status_view.GetExtractionStatusUpdateCallback())\n\n    self._output_writer.Write('\\n')\n    self._status_view.PrintExtractionStatusHeader(None)\n    self._output_writer.Write('Processing started.\\n')\n\n    session = engine.BaseEngine.CreateSession(\n        artifact_filter_names=self._artifact_filters,\n        command_line_arguments=self._command_line_arguments,\n        debug_mode=self._debug_mode,\n        filter_file_path=self._filter_file,\n        preferred_encoding=self.preferred_encoding,\n        preferred_time_zone=self._preferred_time_zone,\n        preferred_year=self._preferred_year)\n\n    storage_writer = storage_factory.StorageFactory.CreateStorageWriter(\n        self._storage_format, session, self._storage_file_path)\n    if not storage_writer:\n      raise errors.BadConfigOption(\n          'Unsupported storage format: {0:s}'.format(self._storage_format))\n\n    single_process_mode = self._single_process_mode\n    if self._source_type == dfvfs_definitions.SOURCE_TYPE_FILE:\n      # No need to multi process a single file source.\n      single_process_mode = True\n\n    if single_process_mode:\n      extraction_engine = single_process_engine.SingleProcessEngine()\n    else:\n      extraction_engine = multi_process_engine.TaskMultiProcessEngine(\n          use_zeromq=self._use_zeromq)\n\n    # If the source is a directory or a storage media image\n    # run pre-processing.\n    if self._source_type in self._SOURCE_TYPES_TO_PREPROCESS:\n      self._PreprocessSources(extraction_engine)\n\n    configuration = self._CreateProcessingConfiguration(\n        extraction_engine.knowledge_base)\n\n    self._SetExtractionParsersAndPlugins(configuration, session)\n    self._SetExtractionPreferredTimeZone(extraction_engine.knowledge_base)\n\n    try:\n      filter_find_specs = extraction_engine.BuildFilterFindSpecs(\n          self._artifact_definitions_path, self._custom_artifacts_path,\n          extraction_engine.knowledge_base, self._artifact_filters,\n          self._filter_file)\n    except errors.InvalidFilter as exception:\n      raise errors.BadConfigOption(\n          'Unable to build filter specification: {0!s}'.format(exception))\n\n    processing_status = None\n    if single_process_mode:\n      logger.debug('Starting extraction in single process mode.')\n\n      processing_status = extraction_engine.ProcessSources(\n          self._source_path_specs, storage_writer, self._resolver_context,\n          configuration, filter_find_specs=filter_find_specs,\n          status_update_callback=status_update_callback)\n\n    else:\n      logger.debug('Starting extraction in multi process mode.')\n\n      processing_status = extraction_engine.ProcessSources(\n          session.identifier, self._source_path_specs, storage_writer,\n          configuration, enable_sigsegv_handler=self._enable_sigsegv_handler,\n          filter_find_specs=filter_find_specs,\n          number_of_worker_processes=self._number_of_extraction_workers,\n          status_update_callback=status_update_callback,\n          worker_memory_limit=self._worker_memory_limit)\n\n    self._status_view.PrintExtractionSummary(processing_status)", "response": "Processes the sources and extracts events."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ShowInfo(self):\n    self._output_writer.Write(\n        '{0:=^80s}\\n'.format(' log2timeline/plaso information '))\n\n    plugin_list = self._GetPluginData()\n    for header, data in plugin_list.items():\n      table_view = views.ViewsFactory.GetTableView(\n          self._views_format_type, column_names=['Name', 'Description'],\n          title=header)\n      for entry_header, entry_data in sorted(data):\n        table_view.AddRow([entry_header, entry_data])\n      table_view.Write(self._output_writer)", "response": "Shows information about available hashers parsers plugins etc."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse an Android usage - history file - like object.", "response": "def ParseFileObject(self, parser_mediator, file_object):\n    \"\"\"Parses an Android usage-history file-like object.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      file_object (dfvfs.FileIO): file-like object.\n\n    Raises:\n      UnableToParseFile: when the file cannot be parsed.\n    \"\"\"\n    data = file_object.read(self._HEADER_READ_SIZE)\n    if not data.startswith(b'<?xml'):\n      raise errors.UnableToParseFile(\n          'Not an Android usage history file [not XML]')\n\n    _, _, data = data.partition(b'\\n')\n    if not data.startswith(b'<usage-history'):\n      raise errors.UnableToParseFile(\n          'Not an Android usage history file [wrong XML root key]')\n\n    # The current offset of the file-like object needs to point at\n    # the start of the file for ElementTree to parse the XML data correctly.\n    file_object.seek(0, os.SEEK_SET)\n\n    xml = ElementTree.parse(file_object)\n    root_node = xml.getroot()\n\n    for application_node in root_node:\n      package_name = application_node.get('name', None)\n\n      for part_node in application_node.iter():\n        if part_node.tag != 'comp':\n          continue\n\n        last_resume_time = part_node.get('lrt', None)\n        if last_resume_time is None:\n          parser_mediator.ProduceExtractionWarning('missing last resume time.')\n          continue\n\n        try:\n          last_resume_time = int(last_resume_time, 10)\n        except ValueError:\n          parser_mediator.ProduceExtractionWarning(\n              'unsupported last resume time: {0:s}.'.format(last_resume_time))\n          continue\n\n        event_data = AndroidAppUsageEventData()\n        event_data.component = part_node.get('name', None)\n        event_data.package = package_name\n\n        date_time = dfdatetime_java_time.JavaTime(timestamp=last_resume_time)\n        event = time_events.DateTimeValuesEvent(\n            date_time, definitions.TIME_DESCRIPTION_LAST_RESUME)\n        parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ParseOptions(cls, options, configuration_object):\n    if not isinstance(configuration_object, tools.CLITool):\n      raise errors.BadConfigObject(\n          'Configuration object is not an instance of CLITool')\n\n    artifacts_path = getattr(options, 'artifact_definitions_path', None)\n\n    data_location = getattr(configuration_object, '_data_location', None)\n    if ((not artifacts_path or not os.path.exists(artifacts_path)) and\n        data_location):\n      artifacts_path = os.path.dirname(data_location)\n      artifacts_path = os.path.join(artifacts_path, 'artifacts')\n\n      if not os.path.exists(artifacts_path) and 'VIRTUAL_ENV' in os.environ:\n        artifacts_path = os.path.join(\n            os.environ['VIRTUAL_ENV'], 'share', 'artifacts')\n\n      if not os.path.exists(artifacts_path):\n        artifacts_path = os.path.join(sys.prefix, 'share', 'artifacts')\n      if not os.path.exists(artifacts_path):\n        artifacts_path = os.path.join(sys.prefix, 'local', 'share', 'artifacts')\n\n      if sys.prefix != '/usr':\n        if not os.path.exists(artifacts_path):\n          artifacts_path = os.path.join('/usr', 'share', 'artifacts')\n        if not os.path.exists(artifacts_path):\n          artifacts_path = os.path.join('/usr', 'local', 'share', 'artifacts')\n\n      if not os.path.exists(artifacts_path):\n        artifacts_path = None\n\n    if not artifacts_path or not os.path.exists(artifacts_path):\n      raise errors.BadConfigOption(\n          'Unable to determine path to artifact definitions.')\n\n    custom_artifacts_path = getattr(\n        options, 'custom_artifact_definitions_path', None)\n\n    if custom_artifacts_path and not os.path.isfile(custom_artifacts_path):\n      raise errors.BadConfigOption(\n          'No such artifacts filter file: {0:s}.'.format(custom_artifacts_path))\n\n    if custom_artifacts_path:\n      logger.info(\n          'Custom artifact filter file: {0:s}'.format(custom_artifacts_path))\n\n    registry = artifacts_registry.ArtifactDefinitionsRegistry()\n    reader = artifacts_reader.YamlArtifactsReader()\n\n    logger.info(\n        'Determined artifact definitions path: {0:s}'.format(artifacts_path))\n\n    try:\n      registry.ReadFromDirectory(reader, artifacts_path)\n\n    except (KeyError, artifacts_errors.FormatError) as exception:\n      raise errors.BadConfigOption((\n          'Unable to read artifact definitions from: {0:s} with error: '\n          '{1!s}').format(artifacts_path, exception))\n\n    for name in preprocessors_manager.PreprocessPluginsManager.GetNames():\n      if not registry.GetDefinitionByName(name):\n        raise errors.BadConfigOption(\n            'Missing required artifact definition: {0:s}'.format(name))\n\n    if custom_artifacts_path:\n      try:\n        registry.ReadFromFile(reader, custom_artifacts_path)\n\n      except (KeyError, artifacts_errors.FormatError) as exception:\n        raise errors.BadConfigOption((\n            'Unable to read artifact definitions from: {0:s} with error: '\n            '{1!s}').format(custom_artifacts_path, exception))\n\n    setattr(configuration_object, '_artifact_definitions_path', artifacts_path)\n    setattr(\n        configuration_object, '_custom_artifacts_path', custom_artifacts_path)", "response": "Parses and validates the options."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing and validates options.", "response": "def ParseOptions(cls, options, analysis_plugin):\n    \"\"\"Parses and validates options.\n\n    Args:\n      options (argparse.Namespace): parser options.\n      analysis_plugin (WindowsServicePlugin): analysis plugin to configure.\n\n    Raises:\n      BadConfigObject: when the output module object is of the wrong type.\n    \"\"\"\n    if not isinstance(\n        analysis_plugin, windows_services.WindowsServicesAnalysisPlugin):\n      raise errors.BadConfigObject((\n          'Analysis plugin is not an instance of '\n          'WindowsServicesAnalysisPlugin'))\n\n    output_format = cls._ParseStringOption(\n        options, 'windows_services_output', default_value=cls._DEFAULT_OUTPUT)\n    analysis_plugin.SetOutputFormat(output_format)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nopen a Windows Registry file - like object.", "response": "def Open(self, file_object, ascii_codepage='cp1252'):\n    \"\"\"Opens a Windows Registry file-like object.\n\n    Args:\n      file_object (dfvfs.FileIO): Windows Registry file-like object.\n      ascii_codepage (Optional[str]): ASCII string codepage.\n\n    Returns:\n      WinRegistryFile: Windows Registry file or None.\n    \"\"\"\n    registry_file = dfwinreg_regf.REGFWinRegistryFile(\n        ascii_codepage=ascii_codepage)\n\n    # We don't catch any IOErrors here since we want to produce a parse error\n    # from the parser if this happens.\n    registry_file.Open(file_object)\n\n    return registry_file"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _CanProcessKeyWithPlugin(self, registry_key, plugin):\n    for registry_key_filter in plugin.FILTERS:\n      # Skip filters that define key paths since they are already\n      # checked by the path filter.\n      if getattr(registry_key_filter, 'key_paths', []):\n        continue\n\n      if registry_key_filter.Match(registry_key):\n        return True\n\n    return False", "response": "Determines if a plugin can process a Windows Registry key or its values."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _ParseKeyWithPlugin(self, parser_mediator, registry_key, plugin):\n    try:\n      plugin.UpdateChainAndProcess(parser_mediator, registry_key)\n    except (IOError, dfwinreg_errors.WinRegistryValueError) as exception:\n      parser_mediator.ProduceExtractionWarning(\n          'in key: {0:s} error: {1!s}'.format(registry_key.path, exception))", "response": "Parses a Registry key with a specific plugin."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _NormalizeKeyPath(self, key_path):\n    normalized_key_path = key_path.lower()\n    # The Registry key path should start with:\n    # HKEY_LOCAL_MACHINE\\System\\ControlSet followed by 3 digits\n    # which makes 39 characters.\n    if (len(normalized_key_path) < 39 or\n        not normalized_key_path.startswith(self._CONTROL_SET_PREFIX)):\n      return normalized_key_path\n\n    # Key paths that contain ControlSet### must be normalized to\n    # CurrentControlSet.\n    return ''.join([\n        self._NORMALIZED_CONTROL_SET_PREFIX, normalized_key_path[39:]])", "response": "Normalizes a Windows Registry key path."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse a Registry key with a specific plugin.", "response": "def _ParseKey(self, parser_mediator, registry_key):\n    \"\"\"Parses the Registry key with a specific plugin.\n\n    Args:\n      parser_mediator (ParserMediator): parser mediator.\n      registry_key (dfwinreg.WinRegistryKey): Windwos Registry key.\n    \"\"\"\n    matching_plugin = None\n\n    normalized_key_path = self._NormalizeKeyPath(registry_key.path)\n    if self._path_filter.CheckPath(normalized_key_path):\n      matching_plugin = self._plugin_per_key_path[normalized_key_path]\n    else:\n      for plugin in self._plugins_without_key_paths:\n        if self._CanProcessKeyWithPlugin(registry_key, plugin):\n          matching_plugin = plugin\n          break\n\n    if not matching_plugin:\n      matching_plugin = self._default_plugin\n\n    if matching_plugin:\n      self._ParseKeyWithPlugin(parser_mediator, registry_key, matching_plugin)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _ParseRecurseKeys(self, parser_mediator, root_key):\n    for registry_key in root_key.RecurseKeys():\n      if parser_mediator.abort:\n        break\n\n      self._ParseKey(parser_mediator, registry_key)", "response": "Parses the Registry keys recursively."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _ParseKeysFromFindSpecs(self, parser_mediator, win_registry, find_specs):\n    searcher = dfwinreg_registry_searcher.WinRegistrySearcher(win_registry)\n    for registry_key_path in iter(searcher.Find(find_specs=find_specs)):\n      if parser_mediator.abort:\n        break\n\n      registry_key = searcher.GetKeyByPath(registry_key_path)\n      self._ParseKey(parser_mediator, registry_key)", "response": "Parses the Registry keys from FindSpecs."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ParseFileObject(self, parser_mediator, file_object):\n    win_registry_reader = FileObjectWinRegistryFileReader()\n\n    try:\n      registry_file = win_registry_reader.Open(file_object)\n    except IOError as exception:\n      parser_mediator.ProduceExtractionWarning(\n          'unable to open Windows Registry file with error: {0!s}'.format(\n              exception))\n      return\n\n    win_registry = dfwinreg_registry.WinRegistry()\n\n    key_path_prefix = win_registry.GetRegistryFileMapping(registry_file)\n    registry_file.SetKeyPathPrefix(key_path_prefix)\n    root_key = registry_file.GetRootKey()\n    if not root_key:\n      return\n\n    registry_find_specs = getattr(\n        parser_mediator.artifacts_filter_helper, 'registry_find_specs', None)\n\n    if not registry_find_specs:\n      try:\n        self._ParseRecurseKeys(parser_mediator, root_key)\n      except IOError as exception:\n        parser_mediator.ProduceExtractionWarning('{0!s}'.format(exception))\n\n    else:\n      artifacts_filter_helper = artifact_filters.ArtifactDefinitionsFilterHelper\n      if not artifacts_filter_helper.CheckKeyCompatibility(key_path_prefix):\n        logger.warning((\n            'Artifacts filters are not supported for Windows Registry file '\n            'with key path prefix: \"{0:s}\".').format(key_path_prefix))\n\n      else:\n        try:\n          win_registry.MapFile(key_path_prefix, registry_file)\n          self._ParseKeysFromFindSpecs(\n              parser_mediator, win_registry, registry_find_specs)\n        except IOError as exception:\n          parser_mediator.ProduceExtractionWarning('{0!s}'.format(exception))", "response": "Parses a Windows Registry file - like object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef CreateStorageReaderForFile(cls, path):\n    if sqlite_file.SQLiteStorageFile.CheckSupportedFormat(\n        path, check_readable_only=True):\n      return sqlite_reader.SQLiteStorageFileReader(path)\n\n    return None", "response": "Creates a storage reader based on the file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a storage writer.", "response": "def CreateStorageWriter(cls, storage_format, session, path):\n    \"\"\"Creates a storage writer.\n\n    Args:\n      session (Session): session the storage changes are part of.\n      path (str): path to the storage file.\n      storage_format (str): storage format.\n\n    Returns:\n      StorageWriter: a storage writer or None if the storage file cannot be\n          opened or the storage format is not supported.\n    \"\"\"\n    if storage_format == definitions.STORAGE_FORMAT_SQLITE:\n      return sqlite_writer.SQLiteStorageFileWriter(session, path)\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a storage writer based on the file.", "response": "def CreateStorageWriterForFile(cls, session, path):\n    \"\"\"Creates a storage writer based on the file.\n\n    Args:\n      session (Session): session the storage changes are part of.\n      path (str): path to the storage file.\n\n    Returns:\n      StorageWriter: a storage writer or None if the storage file cannot be\n          opened or the storage format is not supported.\n    \"\"\"\n    if sqlite_file.SQLiteStorageFile.CheckSupportedFormat(path):\n      return sqlite_writer.SQLiteStorageFileWriter(session, path)\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nopens the database and creates the required tables.", "response": "def Open(self):\n    \"\"\"Connects to the database and creates the required tables.\n\n    Raises:\n      IOError: if the 4n6time tables cannot be created or data not inserted\n          in the database.\n      OSError: if the 4n6time tables cannot be created or data not inserted\n          in the database.\n      ValueError: if no database name given.\n    \"\"\"\n    if not self._dbname:\n      raise ValueError('Missing database name.')\n\n    try:\n      if self._append:\n        self._connection = MySQLdb.connect(\n            self._host, self._user, self._password, self._dbname)\n        self._cursor = self._connection.cursor()\n\n      self._connection.set_character_set('utf8')\n      self._cursor.execute('SET NAMES utf8')\n      self._cursor.execute('SET CHARACTER SET utf8')\n      self._cursor.execute('SET character_set_connection=utf8')\n      self._cursor.execute('SET GLOBAL innodb_large_prefix=ON')\n      self._cursor.execute('SET GLOBAL innodb_file_format=barracuda')\n      self._cursor.execute('SET GLOBAL innodb_file_per_table=ON')\n      self._cursor.execute(\n          'CREATE DATABASE IF NOT EXISTS {0:s}'.format(self._dbname))\n      self._cursor.execute('USE {0:s}'.format(self._dbname))\n      # Create tables.\n      self._cursor.execute(self._CREATE_TABLE_QUERY)\n      if self._set_status:\n        self._set_status('Created table: log2timeline')\n\n      for field in self._META_FIELDS:\n        self._cursor.execute(\n            'CREATE TABLE IF NOT EXISTS l2t_{0}s ({0}s TEXT, frequency INT) '\n            'ENGINE=InnoDB ROW_FORMAT=COMPRESSED'.format(field))\n        if self._set_status:\n          self._set_status('Created table: l2t_{0:s}'.format(field))\n\n      self._cursor.execute(\n          'CREATE TABLE IF NOT EXISTS l2t_tags (tag TEXT) '\n          'ENGINE=InnoDB ROW_FORMAT=COMPRESSED')\n      if self._set_status:\n        self._set_status('Created table: l2t_tags')\n\n      self._cursor.execute(\n          'CREATE TABLE IF NOT EXISTS l2t_saved_query ('\n          'name TEXT, query TEXT) '\n          'ENGINE=InnoDB ROW_FORMAT=COMPRESSED')\n      if self._set_status:\n        self._set_status('Created table: l2t_saved_query')\n\n      self._cursor.execute(\n          'CREATE TABLE IF NOT EXISTS l2t_disk ('\n          'disk_type INT, mount_path TEXT, '\n          'dd_path TEXT, dd_offset TEXT, '\n          'storage_file TEXT, export_path TEXT) '\n          'ENGINE=InnoDB ROW_FORMAT=COMPRESSED')\n      self._cursor.execute(\n          'INSERT INTO l2t_disk ('\n          'disk_type, mount_path, dd_path, '\n          'dd_offset, storage_file, '\n          'export_path) VALUES '\n          '(0, \"\", \"\", \"\", \"\", \"\")')\n      if self._set_status:\n        self._set_status('Created table: l2t_disk')\n    except MySQLdb.Error as exception:\n      raise IOError('Unable to insert into database with error: {0!s}'.format(\n          exception))\n\n    self._count = 0"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the database credentials.", "response": "def SetCredentials(self, password=None, username=None):\n    \"\"\"Sets the database credentials.\n\n    Args:\n      password (Optional[str]): password to access the database.\n      username (Optional[str]): username to access the database.\n    \"\"\"\n    if password:\n      self._password = password\n    if username:\n      self._user = username"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the server information.", "response": "def SetServerInformation(self, server, port):\n    \"\"\"Sets the server information.\n\n    Args:\n      server (str): hostname or IP address of the database server.\n      port (int): port number of the database server.\n    \"\"\"\n    self._host = server\n    self._port = port"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrite the body of an event object to the output.", "response": "def WriteEventBody(self, event):\n    \"\"\"Writes the body of an event object to the output.\n\n    Args:\n      event (EventObject): event.\n    \"\"\"\n    if not hasattr(event, 'timestamp'):\n      return\n\n    row = self._GetSanitizedEventValues(event)\n    try:\n      self._cursor.execute(self._INSERT_QUERY, row)\n    except MySQLdb.Error as exception:\n      logger.warning(\n          'Unable to insert into database with error: {0!s}.'.format(\n              exception))\n\n    self._count += 1\n\n    # TODO: Experiment if committing the current transaction\n    # every 10000 inserts is the optimal approach.\n    if self._count % 10000 == 0:\n      self._connection.commit()\n      if self._set_status:\n        self._set_status('Inserting event: {0:d}'.format(self._count))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef AddRow(self, values):\n    if self._number_of_columns and len(values) != self._number_of_columns:\n      raise ValueError('Number of values is out of bounds.')\n\n    self._rows.append(values)\n\n    if not self._number_of_columns:\n      self._number_of_columns = len(values)", "response": "Adds a row of values to the internal list."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrite a header. Args: output_writer (OutputWriter): output writer.", "response": "def _WriteHeader(self, output_writer):\n    \"\"\"Writes a header.\n\n    Args:\n      output_writer (OutputWriter): output writer.\n    \"\"\"\n    header_string = ''\n    if self._title:\n      header_string = ' {0:s} '.format(self._title)\n\n    header_string = self._HEADER_FORMAT_STRING.format(header_string)\n    output_writer.Write(header_string)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _WriteRow(self, output_writer, values):\n    maximum_row_width = self._MAXIMUM_WIDTH - self._column_width - 3\n\n    # The format string of the first line of the column value.\n    primary_format_string = '{{0:>{0:d}s}} : {{1:s}}\\n'.format(\n        self._column_width)\n\n    # The format string of successive lines of the column value.\n    secondary_format_string = '{{0:<{0:d}s}}{{1:s}}\\n'.format(\n        self._column_width + 3)\n\n    if isinstance(values[1], py2to3.STRING_TYPES):\n      value_string = values[1]\n    else:\n      value_string = '{0!s}'.format(values[1])\n\n    if len(value_string) < maximum_row_width:\n      output_writer.Write(primary_format_string.format(\n          values[0], value_string))\n      return\n\n    # Split the column value in words.\n    words = value_string.split()\n\n    current = 0\n\n    lines = []\n    word_buffer = []\n    for word in words:\n      current += len(word) + 1\n      if current >= maximum_row_width:\n        current = len(word)\n        lines.append(' '.join(word_buffer))\n        word_buffer = [word]\n      else:\n        word_buffer.append(word)\n    lines.append(' '.join(word_buffer))\n\n    # Split the column value across multiple lines.\n    output_writer.Write(\n        primary_format_string.format(values[0], lines[0]))\n    for line in lines[1:]:\n      output_writer.Write(secondary_format_string.format('', line))", "response": "Writes a single row of values aligned to the column width."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef AddRow(self, values):\n    super(CLITableView, self).AddRow(values)\n\n    value_length = len(values[0])\n    if value_length > self._column_width:\n      self._column_width = value_length", "response": "Adds a row of values to the row list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwriting the table to the output writer.", "response": "def Write(self, output_writer):\n    \"\"\"Writes the table to the output writer.\n\n    Args:\n      output_writer (OutputWriter): output writer.\n\n    Raises:\n      RuntimeError: if the title exceeds the maximum width or\n          if the table has more than 2 columns or\n          if the column width is out of bounds.\n    \"\"\"\n    if self._title and len(self._title) > self._MAXIMUM_WIDTH:\n      raise RuntimeError('Title length out of bounds.')\n\n    if self._number_of_columns not in (0, 2):\n      raise RuntimeError('Unsupported number of columns: {0:d}.'.format(\n          self._number_of_columns))\n\n    if self._column_width < 0 or self._column_width >= self._MAXIMUM_WIDTH:\n      raise RuntimeError('Column width out of bounds.')\n\n    output_writer.Write('\\n')\n\n    self._WriteHeader(output_writer)\n\n    if self._columns:\n      self._WriteRow(output_writer, self._columns)\n      self._WriteSeparatorLine(output_writer)\n\n    for values in self._rows:\n      self._WriteRow(output_writer, values)\n\n    self._WriteSeparatorLine(output_writer)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a row of values to the internal list of the archive.", "response": "def AddRow(self, values):\n    \"\"\"Adds a row of values.\n\n    Args:\n      values (list[object]): values.\n\n    Raises:\n      ValueError: if the number of values is out of bounds.\n    \"\"\"\n    if self._number_of_columns and len(values) != self._number_of_columns:\n      raise ValueError('Number of values is out of bounds.')\n\n    if not self._column_sizes and self._columns:\n      self._column_sizes = [len(column) for column in self._columns]\n\n    value_strings = []\n    for value_index, value_string in enumerate(values):\n      if not isinstance(value_string, py2to3.UNICODE_TYPE):\n        value_string = '{0!s}'.format(value_string)\n      value_strings.append(value_string)\n\n      self._column_sizes[value_index] = max(\n          self._column_sizes[value_index], len(value_string))\n\n    self._rows.append(value_strings)\n\n    if not self._number_of_columns:\n      self._number_of_columns = len(value_strings)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwriting the table to the output writer.", "response": "def Write(self, output_writer):\n    \"\"\"Writes the table to the output writer.\n\n    Args:\n      output_writer (OutputWriter): output writer.\n    \"\"\"\n    if self._title:\n      output_writer.Write('### {0:s}\\n\\n'.format(self._title))\n\n    if not self._columns:\n      self._columns = ['' for _ in range(0, self._number_of_columns)]\n\n    output_writer.Write(' | '.join(self._columns))\n    output_writer.Write('\\n')\n\n    output_writer.Write(' | '.join(['---' for _ in self._columns]))\n    output_writer.Write('\\n')\n\n    for values in self._rows:\n      values = ['{0!s}'.format(value) for value in values]\n      output_writer.Write(' | '.join(values))\n      output_writer.Write('\\n')\n\n    output_writer.Write('\\n')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nretrieve a table view.", "response": "def GetTableView(cls, format_type, column_names=None, title=None):\n    \"\"\"Retrieves a table view.\n\n    Args:\n      format_type (str): table view format type.\n      column_names (Optional[list[str]]): column names.\n      title (Optional[str]): title.\n\n    Returns:\n      BaseTableView: table view.\n\n    Raises:\n      ValueError: if the format type is not supported.\n    \"\"\"\n    view_class = cls._TABLE_VIEW_FORMAT_CLASSES.get(format_type, None)\n    if not view_class:\n      raise ValueError('Unsupported format type: {0:s}'.format(format_type))\n\n    return view_class(column_names=column_names, title=title)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _GetDataTypeMap(self, name):\n    data_type_map = self._data_type_maps.get(name, None)\n    if not data_type_map:\n      data_type_map = self._fabric.CreateDataTypeMap(name)\n      self._data_type_maps[name] = data_type_map\n\n    return data_type_map", "response": "Retrieves a data type map defined by the definition file."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads a dtFabric definition file.", "response": "def _ReadDefinitionFile(self, filename):\n    \"\"\"Reads a dtFabric definition file.\n\n    Args:\n      filename (str): name of the dtFabric definition file.\n\n    Returns:\n      dtfabric.DataTypeFabric: data type fabric which contains the data format\n          data type maps of the data type definition, such as a structure, that\n          can be mapped onto binary data or None if no filename is provided.\n    \"\"\"\n    if not filename:\n      return None\n\n    path = os.path.join(self._DEFINITION_FILES_PATH, filename)\n    with open(path, 'rb') as file_object:\n      definition = file_object.read()\n\n    return dtfabric_fabric.DataTypeFabric(yaml_definition=definition)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread a structure from a byte stream.", "response": "def _ReadStructureFromByteStream(\n      self, byte_stream, file_offset, data_type_map, context=None):\n    \"\"\"Reads a structure from a byte stream.\n\n    Args:\n      byte_stream (bytes): byte stream.\n      file_offset (int): offset of the structure data relative to the start\n          of the file-like object.\n      data_type_map (dtfabric.DataTypeMap): data type map of the structure.\n      context (Optional[dtfabric.DataTypeMapContext]): data type map context.\n          The context is used within dtFabric to hold state about how to map\n          the data type definition onto the byte stream. In this class it is\n          used to determine the size of variable size data type definitions.\n\n    Returns:\n      object: structure values object.\n\n    Raises:\n      ParseError: if the structure cannot be read.\n      ValueError: if file-like object or data type map is missing.\n    \"\"\"\n    if not byte_stream:\n      raise ValueError('Missing byte stream.')\n\n    if not data_type_map:\n      raise ValueError('Missing data type map.')\n\n    try:\n      return data_type_map.MapByteStream(byte_stream, context=context)\n    except (dtfabric_errors.ByteStreamTooSmallError,\n            dtfabric_errors.MappingError) as exception:\n      raise errors.ParseError((\n          'Unable to map {0:s} data at offset: 0x{1:08x} with error: '\n          '{2!s}').format(data_type_map.name or '', file_offset, exception))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nruns sphinx - apidoc to auto - generate documentation.", "response": "def RunSphinxAPIDoc(_):\n  \"\"\"Runs sphinx-apidoc to auto-generate documentation.\"\"\"\n  current_directory = os.path.abspath(os.path.dirname(__file__))\n  module = os.path.join(current_directory, '..', 'plaso')\n  api_directory = os.path.join(current_directory, 'sources', 'api')\n  apidoc.main(['-o', api_directory, module, '--force'])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef setup(app):\n  # Triggers sphinx-apidoc to generate API documentation.\n  app.connect('builder-inited', RunSphinxAPIDoc)\n  app.add_config_value(\n      'recommonmark_config', {\n        'enable_auto_doc_ref': False},\n      True)\n\n  app.add_transform(AutoStructify)\n  app.add_transform(ProcessLink)", "response": "Called at Sphinx initialization."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_and_replace(self, node):\n    if isinstance(node, nodes.reference) and 'refuri' in node:\n      reference_uri = node['refuri']\n      if reference_uri.endswith('.md') and not reference_uri.startswith('http'):\n        reference_uri = reference_uri[:-3] + '.html'\n        node['refuri'] = reference_uri\n      else:\n        match = self.ANCHOR_REGEX.match(reference_uri)\n        if match:\n          node['refuri'] = '{0:s}.html#{1:s}'.format(\n              match.group('uri'), match.group('anchor'))\n    return node", "response": "Parses URIs containing. md and replaces them with their HTML page."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef traverse(self, node):\n    self.find_and_replace(node)\n\n    for c in node.children:\n      self.traverse(c)", "response": "Traverse the document tree rooted at node."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef AddArguments(cls, argument_group):\n    argument_group.add_argument(\n        '--fields', dest='fields', type=str, action='store',\n        default=cls._DEFAULT_FIELDS, help=(\n            'Defines which fields should be included in the output.'))\n    argument_group.add_argument(\n        '--additional_fields', dest='additional_fields', type=str,\n        action='store', default='', help=(\n            'Defines extra fields to be included in the output, in addition to'\n            ' the default fields, which are {0:s}.'.format(\n                cls._DEFAULT_FIELDS)))\n    argument_group.add_argument(\n        '--timestamp_format', dest='timestamp_format', type=str,\n        action='store', default=cls._DEFAULT_TIMESTAMP_FORMAT, help=(\n            'Set the timestamp format that will be used in the datetime'\n            'column of the XLSX spreadsheet.'))", "response": "Adds command line arguments to an argument group."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ParseOptions(cls, options, output_module):\n    if not isinstance(output_module, xlsx.XLSXOutputModule):\n      raise errors.BadConfigObject(\n          'Output module is not an instance of XLSXOutputModule')\n\n    fields = cls._ParseStringOption(\n        options, 'fields', default_value=cls._DEFAULT_FIELDS)\n\n    additional_fields = cls._ParseStringOption(options, 'additional_fields')\n\n    if additional_fields:\n      fields = '{0:s},{1:s}'.format(fields, additional_fields)\n\n    filename = getattr(options, 'write', None)\n    if not filename:\n      raise errors.BadConfigOption(\n          'Output filename was not provided use \"-w filename\" to specify.')\n\n    timestamp_format = cls._ParseStringOption(\n        options, 'timestamp_format',\n        default_value=cls._DEFAULT_TIMESTAMP_FORMAT)\n\n    output_module.SetFields([\n        field_name.strip() for field_name in fields.split(',')])\n    output_module.SetFilename(filename)\n    output_module.SetTimestampFormat(timestamp_format)", "response": "Parses and validates the options."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _ReadData(self, file_object, file_offset, data_size):\n    if not file_object:\n      raise ValueError('Missing file-like object.')\n\n    file_object.seek(file_offset, os.SEEK_SET)\n\n    read_error = ''\n\n    try:\n      data = file_object.read(data_size)\n\n      if len(data) != data_size:\n        read_error = 'missing data'\n\n    except IOError as exception:\n      read_error = '{0!s}'.format(exception)\n\n    if read_error:\n      raise errors.ParseError(\n          'Unable to read data at offset: 0x{0:08x} with error: {1:s}'.format(\n              file_offset, read_error))\n\n    return data", "response": "Reads the data from the file - like object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nextract the value in the strings table.", "response": "def _GetDictFromStringsTable(self, parser_mediator, table):\n    \"\"\"Build a dictionary of the value in the strings table.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      table (pyesedb.table): strings table.\n\n    Returns:\n      dict[str,object]: values per column name.\n    \"\"\"\n    if not table:\n      return {}\n\n    record_values = {}\n    for record in table.records:\n      if parser_mediator.abort:\n        break\n\n      if record.get_number_of_values() != 2:\n        continue\n\n      identification = self._GetRecordValue(record, 0)\n      filename = self._GetRecordValue(record, 1)\n\n      if not identification:\n        continue\n      record_values[identification] = filename\n\n    return record_values"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ParseNameSpace(\n      self, parser_mediator, cache=None, database=None, table=None,\n      **unused_kwargs):\n    \"\"\"Parses the namespace table.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      cache (Optional[ESEDBCache]): cache.\n      database (Optional[pyesedb.file]): ESE database.\n      table (Optional[pyesedb.table]): table.\n\n    Raises:\n      ValueError: if the database or table value is missing.\n    \"\"\"\n    if database is None:\n      raise ValueError('Missing database value.')\n\n    if table is None:\n      raise ValueError('Missing table value.')\n\n    strings = cache.GetResults('strings')\n    if not strings:\n      esedb_table = database.get_table_by_name('string')\n      strings = self._GetDictFromStringsTable(parser_mediator, esedb_table)\n      cache.StoreDictInCache('strings', strings)\n\n    for esedb_record in table.records:\n      if parser_mediator.abort:\n        break\n\n      record_values = self._GetRecordValues(\n          parser_mediator, table.name, esedb_record)\n\n      event_data = FileHistoryNamespaceEventData()\n      event_data.file_attribute = record_values.get('fileAttrib', None)\n      event_data.identifier = record_values.get('id', None)\n      event_data.parent_identifier = record_values.get('parentId', None)\n      event_data.usn_number = record_values.get('usn', None)\n      event_data.original_filename = strings.get(event_data.identifier, None)\n\n      created_timestamp = record_values.get('fileCreated')\n      if created_timestamp:\n        date_time = dfdatetime_filetime.Filetime(timestamp=created_timestamp)\n        event = time_events.DateTimeValuesEvent(\n            date_time, definitions.TIME_DESCRIPTION_CREATION)\n        parser_mediator.ProduceEventWithEventData(event, event_data)\n\n      modified_timestamp = record_values.get('fileModified')\n      if modified_timestamp:\n        date_time = dfdatetime_filetime.Filetime(timestamp=modified_timestamp)\n        event = time_events.DateTimeValuesEvent(\n            date_time, definitions.TIME_DESCRIPTION_MODIFICATION)\n        parser_mediator.ProduceEventWithEventData(event, event_data)\n\n      if not created_timestamp and not modified_timestamp:\n        date_time = dfdatetime_semantic_time.SemanticTime('Not set')\n        event = time_events.DateTimeValuesEvent(\n            date_time, definitions.TIME_DESCRIPTION_NOT_A_TIME)\n        parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Parses the namespace table."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nretrieve the display name for a path specification.", "response": "def GetDisplayNameForPathSpec(self, path_spec):\n    \"\"\"Retrieves the display name for a path specification.\n\n    Args:\n      path_spec (dfvfs.PathSpec): path specification.\n\n    Returns:\n      str: human readable version of the path specification.\n    \"\"\"\n    return path_helper.PathHelper.GetDisplayNameForPathSpec(\n        path_spec, mount_path=self._mount_path, text_prepend=self._text_prepend)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ProduceAnalysisReport(self, plugin):\n    analysis_report = plugin.CompileReport(self)\n    if not analysis_report:\n      return\n\n    analysis_report.time_compiled = timelib.Timestamp.GetNow()\n\n    plugin_name = getattr(analysis_report, 'plugin_name', plugin.plugin_name)\n    if plugin_name:\n      analysis_report.plugin_name = plugin_name\n\n    if self._event_filter_expression:\n      # TODO: rename filter string when refactoring the analysis reports.\n      analysis_report.filter_string = self._event_filter_expression\n\n    self._storage_writer.AddAnalysisReport(analysis_report)\n\n    self.number_of_produced_analysis_reports += 1\n    self.number_of_produced_event_tags = (\n        self._storage_writer.number_of_event_tags)\n\n    self.last_activity_timestamp = time.time()", "response": "Produces an analysis report."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nproduces an event tag.", "response": "def ProduceEventTag(self, event_tag):\n    \"\"\"Produces an event tag.\n\n    Args:\n      event_tag (EventTag): event tag.\n    \"\"\"\n    self._storage_writer.AddEventTag(event_tag)\n\n    self.number_of_produced_event_tags += 1\n\n    self.last_activity_timestamp = time.time()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef GetMessages(self, formatter_mediator, event):\n    event_values = event.CopyToDict()\n\n    # TODO: clean up the default formatter and add a test to make sure\n    # it is clear how it is intended to work.\n    text_pieces = []\n    for key, value in event_values.items():\n      if key in definitions.RESERVED_VARIABLE_NAMES:\n        continue\n      text_pieces.append('{0:s}: {1!s}'.format(key, value))\n\n    event_values['attribute_driven'] = ' '.join(text_pieces)\n    event_values['data_type'] = self.DATA_TYPE\n\n    return self._FormatMessages(\n        self.FORMAT_STRING, self.FORMAT_STRING_SHORT, event_values)", "response": "Determines the formatted message strings for an event object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds command line arguments to an argument group.", "response": "def AddArguments(cls, argument_group):\n    \"\"\"Adds command line arguments to an argument group.\n\n    This function takes an argument parser or an argument group object and adds\n    to it all the command line arguments this helper supports.\n\n    Args:\n      argument_group (argparse._ArgumentGroup|argparse.ArgumentParser):\n          argparse group.\n    \"\"\"\n    argument_group.add_argument(\n        '--language', metavar='LANGUAGE', dest='preferred_language',\n        default='en-US', type=str, help=(\n            'The preferred language identifier for Windows Event Log message '\n            'strings. Use \"--language list\" to see a list of available '\n            'language identifiers. Note that formatting will fall back on '\n            'en-US (LCID 0x0409) if the preferred language is not available '\n            'in the database of message string templates.'))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing and validates options.", "response": "def ParseOptions(cls, options, configuration_object):\n    \"\"\"Parses and validates options.\n\n    Args:\n      options (argparse.Namespace): parser options.\n      configuration_object (CLITool): object to be configured by the argument\n          helper.\n\n    Raises:\n      BadConfigObject: when the configuration object is of the wrong type.\n    \"\"\"\n    if not isinstance(configuration_object, tools.CLITool):\n      raise errors.BadConfigObject(\n          'Configuration object is not an instance of CLITool')\n\n    preferred_language = cls._ParseStringOption(\n        options, 'preferred_language', default_value='en-US')\n\n    setattr(configuration_object, '_preferred_language', preferred_language)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _GetEventIdentifiers(self, event):\n    attributes = []\n\n    attribute_string = 'data_type: {0:s}'.format(event.data_type)\n    attributes.append(attribute_string)\n\n    for attribute_name, attribute_value in sorted(event.GetAttributes()):\n      if attribute_name in self._IDENTIFIER_EXCLUDED_ATTRIBUTES:\n        continue\n\n      if not attribute_value:\n        continue\n\n      if attribute_name == 'pathspec':\n        attribute_value = attribute_value.comparable\n\n      elif isinstance(attribute_value, dict):\n        attribute_value = sorted(attribute_value.items())\n\n      elif isinstance(attribute_value, set):\n        attribute_value = sorted(list(attribute_value))\n\n      elif isinstance(attribute_value, py2to3.BYTES_TYPE):\n        attribute_value = repr(attribute_value)\n\n      try:\n        attribute_string = '{0:s}: {1!s}'.format(\n            attribute_name, attribute_value)\n      except UnicodeDecodeError:\n        logger.error('Failed to decode attribute {0:s}'.format(\n            attribute_name))\n      attributes.append(attribute_string)\n\n    # The 'atime', 'ctime', 'crtime', 'mtime' are included for backwards\n    # compatibility with the filestat parser.\n    if event.timestamp_desc in (\n        'atime', 'ctime', 'crtime', 'mtime',\n        definitions.TIME_DESCRIPTION_LAST_ACCESS,\n        definitions.TIME_DESCRIPTION_CHANGE,\n        definitions.TIME_DESCRIPTION_CREATION,\n        definitions.TIME_DESCRIPTION_MODIFICATION):\n      macb_group_identifier = ', '.join(attributes)\n    else:\n      macb_group_identifier = None\n\n    attributes.insert(0, event.timestamp_desc)\n    content_identifier = ', '.join(attributes)\n\n    return macb_group_identifier, content_identifier", "response": "Retrieves different identifiers of the event."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\npop an event from the heap.", "response": "def PopEvent(self):\n    \"\"\"Pops an event from the heap.\n\n    Returns:\n      tuple: containing:\n\n        str: identifier of the event MACB group or None if the event cannot\n            be grouped.\n        str: identifier of the event content.\n        EventObject: event.\n    \"\"\"\n    try:\n      macb_group_identifier, content_identifier, event = heapq.heappop(\n          self._heap)\n      if macb_group_identifier == '':\n        macb_group_identifier = None\n      return macb_group_identifier, content_identifier, event\n\n    except IndexError:\n      return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef PopEvents(self):\n    event = self.PopEvent()\n    while event:\n      yield event\n      event = self.PopEvent()", "response": "Pops events from the heap."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\npushes an event onto the heap.", "response": "def PushEvent(self, event):\n    \"\"\"Pushes an event onto the heap.\n\n    Args:\n      event (EventObject): event.\n    \"\"\"\n    macb_group_identifier, content_identifier = self._GetEventIdentifiers(event)\n\n    # We can ignore the timestamp here because the psort engine only stores\n    # events with the same timestamp in the event heap.\n    heap_values = (macb_group_identifier or '', content_identifier, event)\n    heapq.heappush(self._heap, heap_values)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nanalyzes events in a plaso storage.", "response": "def _AnalyzeEvents(self, storage_writer, analysis_plugins, event_filter=None):\n    \"\"\"Analyzes events in a plaso storage.\n\n    Args:\n      storage_writer (StorageWriter): storage writer.\n      analysis_plugins (dict[str, AnalysisPlugin]): analysis plugins that\n          should be run and their names.\n      event_filter (Optional[FilterObject]): event filter.\n\n    Returns:\n      collections.Counter: counter containing information about the events\n          processed and filtered.\n\n    Raises:\n      RuntimeError: if a non-recoverable situation is encountered.\n    \"\"\"\n    self._status = definitions.STATUS_INDICATOR_RUNNING\n    self._number_of_consumed_events = 0\n    self._number_of_consumed_reports = 0\n    self._number_of_consumed_sources = 0\n    self._number_of_consumed_warnings = 0\n    self._number_of_produced_events = 0\n    self._number_of_produced_reports = 0\n    self._number_of_produced_sources = 0\n    self._number_of_produced_warnings = 0\n\n    number_of_filtered_events = 0\n\n    logger.debug('Processing events.')\n\n    filter_limit = getattr(event_filter, 'limit', None)\n\n    for event in storage_writer.GetSortedEvents():\n      event_data_identifier = event.GetEventDataIdentifier()\n      if event_data_identifier:\n        event_data = storage_writer.GetEventDataByIdentifier(\n            event_data_identifier)\n        if event_data:\n          for attribute_name, attribute_value in event_data.GetAttributes():\n            setattr(event, attribute_name, attribute_value)\n\n      event_identifier = event.GetIdentifier()\n      event.tag = self._event_tag_index.GetEventTagByIdentifier(\n          storage_writer, event_identifier)\n\n      if event_filter:\n        filter_match = event_filter.Match(event)\n      else:\n        filter_match = None\n\n      # pylint: disable=singleton-comparison\n      if filter_match == False:\n        number_of_filtered_events += 1\n        continue\n\n      for event_queue in self._event_queues.values():\n        # TODO: Check for premature exit of analysis plugins.\n        event_queue.PushItem(event)\n\n      self._number_of_consumed_events += 1\n\n      if (event_filter and filter_limit and\n          filter_limit == self._number_of_consumed_events):\n        break\n\n    logger.debug('Finished pushing events to analysis plugins.')\n    # Signal that we have finished adding events.\n    for event_queue in self._event_queues.values():\n      event_queue.PushItem(plaso_queue.QueueAbort(), block=False)\n\n    logger.debug('Processing analysis plugin results.')\n\n    # TODO: use a task based approach.\n    plugin_names = [plugin_name for plugin_name in analysis_plugins.keys()]\n    while plugin_names:\n      for plugin_name in list(plugin_names):\n        if self._abort:\n          break\n\n        # TODO: temporary solution.\n        task = tasks.Task()\n        task.identifier = plugin_name\n\n        merge_ready = storage_writer.CheckTaskReadyForMerge(task)\n        if merge_ready:\n          storage_writer.PrepareMergeTaskStorage(task)\n          self._status = definitions.STATUS_INDICATOR_MERGING\n\n          event_queue = self._event_queues[plugin_name]\n          del self._event_queues[plugin_name]\n\n          event_queue.Close()\n\n          storage_merge_reader = storage_writer.StartMergeTaskStorage(task)\n\n          storage_merge_reader.MergeAttributeContainers(\n              callback=self._MergeEventTag)\n          # TODO: temporary solution.\n          plugin_names.remove(plugin_name)\n\n          self._status = definitions.STATUS_INDICATOR_RUNNING\n\n          self._number_of_produced_event_tags = (\n              storage_writer.number_of_event_tags)\n          self._number_of_produced_reports = (\n              storage_writer.number_of_analysis_reports)\n\n    try:\n      storage_writer.StopTaskStorage(abort=self._abort)\n    except (IOError, OSError) as exception:\n      logger.error('Unable to stop task storage with error: {0!s}'.format(\n          exception))\n\n    if self._abort:\n      logger.debug('Processing aborted.')\n    else:\n      logger.debug('Processing completed.')\n\n    events_counter = collections.Counter()\n    events_counter['Events filtered'] = number_of_filtered_events\n    events_counter['Events processed'] = self._number_of_consumed_events\n\n    return events_counter"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _CheckStatusAnalysisProcess(self, pid):\n    # TODO: Refactor this method, simplify and separate concerns (monitoring\n    # vs management).\n    self._RaiseIfNotRegistered(pid)\n\n    if pid in self._completed_analysis_processes:\n      status_indicator = definitions.STATUS_INDICATOR_COMPLETED\n      process_status = {\n          'processing_status': status_indicator}\n      used_memory = 0\n\n    else:\n      process = self._processes_per_pid[pid]\n\n      process_status = self._QueryProcessStatus(process)\n      if process_status is None:\n        process_is_alive = False\n      else:\n        process_is_alive = True\n\n      process_information = self._process_information_per_pid[pid]\n      used_memory = process_information.GetUsedMemory() or 0\n\n      if self._worker_memory_limit and used_memory > self._worker_memory_limit:\n        logger.warning((\n            'Process: {0:s} (PID: {1:d}) killed because it exceeded the '\n            'memory limit: {2:d}.').format(\n                process.name, pid, self._worker_memory_limit))\n        self._KillProcess(pid)\n\n      if isinstance(process_status, dict):\n        self._rpc_errors_per_pid[pid] = 0\n        status_indicator = process_status.get('processing_status', None)\n\n        if status_indicator == definitions.STATUS_INDICATOR_COMPLETED:\n          self._completed_analysis_processes.add(pid)\n\n      else:\n        rpc_errors = self._rpc_errors_per_pid.get(pid, 0) + 1\n        self._rpc_errors_per_pid[pid] = rpc_errors\n\n        if rpc_errors > self._MAXIMUM_RPC_ERRORS:\n          process_is_alive = False\n\n        if process_is_alive:\n          rpc_port = process.rpc_port.value\n          logger.warning((\n              'Unable to retrieve process: {0:s} (PID: {1:d}) status via '\n              'RPC socket: http://localhost:{2:d}').format(\n                  process.name, pid, rpc_port))\n\n          processing_status_string = 'RPC error'\n          status_indicator = definitions.STATUS_INDICATOR_RUNNING\n        else:\n          processing_status_string = 'killed'\n          status_indicator = definitions.STATUS_INDICATOR_KILLED\n\n        process_status = {\n            'processing_status': processing_status_string}\n\n    self._UpdateProcessingStatus(pid, process_status, used_memory)\n\n    if status_indicator in definitions.ERROR_STATUS_INDICATORS:\n      logger.error((\n          'Process {0:s} (PID: {1:d}) is not functioning correctly. '\n          'Status code: {2!s}.').format(\n              process.name, pid, status_indicator))\n\n      self._TerminateProcessByPid(pid)", "response": "Checks the status of an analysis process."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nexport an event using an output module.", "response": "def _ExportEvent(self, output_module, event, deduplicate_events=True):\n    \"\"\"Exports an event using an output module.\n\n    Args:\n      output_module (OutputModule): output module.\n      event (EventObject): event.\n      deduplicate_events (Optional[bool]): True if events should be\n          deduplicated.\n    \"\"\"\n    if event.timestamp != self._export_event_timestamp:\n      self._FlushExportBuffer(\n          output_module, deduplicate_events=deduplicate_events)\n      self._export_event_timestamp = event.timestamp\n\n    self._export_event_heap.PushEvent(event)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _ExportEvents(\n      self, storage_reader, output_module, deduplicate_events=True,\n      event_filter=None, time_slice=None, use_time_slicer=False):\n    \"\"\"Exports events using an output module.\n\n    Args:\n      storage_reader (StorageReader): storage reader.\n      output_module (OutputModule): output module.\n      deduplicate_events (Optional[bool]): True if events should be\n          deduplicated.\n      event_filter (Optional[FilterObject]): event filter.\n      time_slice (Optional[TimeRange]): time range that defines a time slice\n          to filter events.\n      use_time_slicer (Optional[bool]): True if the 'time slicer' should be\n          used. The 'time slicer' will provide a context of events around\n          an event of interest.\n    \"\"\"\n    self._status = definitions.STATUS_INDICATOR_EXPORTING\n\n    time_slice_buffer = None\n    time_slice_range = None\n\n    if time_slice:\n      if time_slice.event_timestamp is not None:\n        time_slice_range = storage_time_range.TimeRange(\n            time_slice.start_timestamp, time_slice.end_timestamp)\n\n      if use_time_slicer:\n        time_slice_buffer = bufferlib.CircularBuffer(time_slice.duration)\n\n    filter_limit = getattr(event_filter, 'limit', None)\n    forward_entries = 0\n\n    self._events_status.number_of_filtered_events = 0\n    self._events_status.number_of_events_from_time_slice = 0\n\n    for event in storage_reader.GetSortedEvents(time_range=time_slice_range):\n      event_data_identifier = event.GetEventDataIdentifier()\n      if event_data_identifier:\n        event_data = storage_reader.GetEventDataByIdentifier(\n            event_data_identifier)\n        if event_data:\n          for attribute_name, attribute_value in event_data.GetAttributes():\n            setattr(event, attribute_name, attribute_value)\n\n      event_identifier = event.GetIdentifier()\n      event.tag = self._event_tag_index.GetEventTagByIdentifier(\n          storage_reader, event_identifier)\n\n      if time_slice_range and event.timestamp != time_slice.event_timestamp:\n        self._events_status.number_of_events_from_time_slice += 1\n\n      if event_filter:\n        filter_match = event_filter.Match(event)\n      else:\n        filter_match = None\n\n      # pylint: disable=singleton-comparison\n      if filter_match == False:\n        if not time_slice_buffer:\n          self._events_status.number_of_filtered_events += 1\n\n        elif forward_entries == 0:\n          time_slice_buffer.Append(event)\n          self._events_status.number_of_filtered_events += 1\n\n        elif forward_entries <= time_slice_buffer.size:\n          self._ExportEvent(\n              output_module, event, deduplicate_events=deduplicate_events)\n          self._number_of_consumed_events += 1\n          self._events_status.number_of_events_from_time_slice += 1\n          forward_entries += 1\n\n        else:\n          # We reached the maximum size of the time slice and don't need to\n          # include other entries.\n          self._events_status.number_of_filtered_events += 1\n          forward_entries = 0\n\n      else:\n        # pylint: disable=singleton-comparison\n        if filter_match == True and time_slice_buffer:\n          # Empty the time slice buffer.\n          for event_in_buffer in time_slice_buffer.Flush():\n            self._ExportEvent(\n                output_module, event_in_buffer,\n                deduplicate_events=deduplicate_events)\n            self._number_of_consumed_events += 1\n            self._events_status.number_of_filtered_events += 1\n            self._events_status.number_of_events_from_time_slice += 1\n\n          forward_entries = 1\n\n        self._ExportEvent(\n            output_module, event, deduplicate_events=deduplicate_events)\n        self._number_of_consumed_events += 1\n\n        # pylint: disable=singleton-comparison\n        if (filter_match == True and filter_limit and\n            filter_limit == self._number_of_consumed_events):\n          break\n\n    self._FlushExportBuffer(output_module)", "response": "Exports events using an output module."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _FlushExportBuffer(self, output_module, deduplicate_events=True):\n    last_macb_group_identifier = None\n    last_content_identifier = None\n    macb_group = []\n\n    generator = self._export_event_heap.PopEvents()\n\n    for macb_group_identifier, content_identifier, event in generator:\n      if deduplicate_events and last_content_identifier == content_identifier:\n        self._events_status.number_of_duplicate_events += 1\n        continue\n\n      if macb_group_identifier is None:\n        if macb_group:\n          output_module.WriteEventMACBGroup(macb_group)\n          macb_group = []\n\n        output_module.WriteEvent(event)\n\n      else:\n        if (last_macb_group_identifier == macb_group_identifier or\n            not macb_group):\n          macb_group.append(event)\n\n        else:\n          output_module.WriteEventMACBGroup(macb_group)\n          macb_group = [event]\n\n        self._events_status.number_of_macb_grouped_events += 1\n\n      last_macb_group_identifier = macb_group_identifier\n      last_content_identifier = content_identifier\n\n    if macb_group:\n      output_module.WriteEventMACBGroup(macb_group)", "response": "Flushes buffered events and writes them to the output module."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmerge an event tag with the last stored event tag.", "response": "def _MergeEventTag(self, storage_writer, attribute_container):\n    \"\"\"Merges an event tag with the last stored event tag.\n\n    If there is an existing event the provided event tag is updated with\n    the contents of the existing one. After which the event tag index is\n    updated.\n\n    Args:\n      storage_writer (StorageWriter): storage writer.\n      attribute_container (AttributeContainer): container.\n    \"\"\"\n    if attribute_container.CONTAINER_TYPE != 'event_tag':\n      return\n\n    event_identifier = attribute_container.GetEventIdentifier()\n    if not event_identifier:\n      return\n\n    # Check if the event has already been tagged on a previous occasion,\n    # we need to append the event tag to the last stored one.\n    stored_event_tag = self._event_tag_index.GetEventTagByIdentifier(\n        storage_writer, event_identifier)\n    if stored_event_tag:\n      attribute_container.AddComment(stored_event_tag.comment)\n      attribute_container.AddLabels(stored_event_tag.labels)\n\n    self._event_tag_index.SetEventTag(attribute_container)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _StartAnalysisProcesses(self, storage_writer, analysis_plugins):\n    logger.info('Starting analysis plugins.')\n\n    for analysis_plugin in analysis_plugins.values():\n      self._analysis_plugins[analysis_plugin.NAME] = analysis_plugin\n\n      process = self._StartWorkerProcess(analysis_plugin.NAME, storage_writer)\n      if not process:\n        logger.error('Unable to create analysis process: {0:s}'.format(\n            analysis_plugin.NAME))\n\n    logger.info('Analysis plugins running')", "response": "Starts the analysis processes."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _StatusUpdateThreadMain(self):\n    while self._status_update_active:\n      # Make a local copy of the PIDs in case the dict is changed by\n      # the main thread.\n      for pid in list(self._process_information_per_pid.keys()):\n        self._CheckStatusAnalysisProcess(pid)\n\n      self._UpdateForemanProcessStatus()\n\n      if self._status_update_callback:\n        self._status_update_callback(self._processing_status)\n\n      time.sleep(self._STATUS_UPDATE_INTERVAL)", "response": "Main function of the status update thread."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nstop the analysis processes.", "response": "def _StopAnalysisProcesses(self, abort=False):\n    \"\"\"Stops the analysis processes.\n\n    Args:\n      abort (bool): True to indicated the stop is issued on abort.\n    \"\"\"\n    logger.debug('Stopping analysis processes.')\n    self._StopMonitoringProcesses()\n\n    # Note that multiprocessing.Queue is very sensitive regarding\n    # blocking on either a get or a put. So we try to prevent using\n    # any blocking behavior.\n\n    if abort:\n      # Signal all the processes to abort.\n      self._AbortTerminate()\n\n    if not self._use_zeromq:\n      logger.debug('Emptying queues.')\n      for event_queue in self._event_queues.values():\n        event_queue.Empty()\n\n    # Wake the processes to make sure that they are not blocking\n    # waiting for the queue new items.\n    for event_queue in self._event_queues.values():\n      event_queue.PushItem(plaso_queue.QueueAbort(), block=False)\n\n    # Try waiting for the processes to exit normally.\n    self._AbortJoin(timeout=self._PROCESS_JOIN_TIMEOUT)\n    for event_queue in self._event_queues.values():\n      event_queue.Close(abort=abort)\n\n    if abort:\n      # Kill any remaining processes.\n      self._AbortKill()\n    else:\n      # Check if the processes are still alive and terminate them if necessary.\n      self._AbortTerminate()\n      self._AbortJoin(timeout=self._PROCESS_JOIN_TIMEOUT)\n\n      for event_queue in self._event_queues.values():\n        event_queue.Close(abort=True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate the foreman process status.", "response": "def _UpdateForemanProcessStatus(self):\n    \"\"\"Update the foreman process status.\"\"\"\n    used_memory = self._process_information.GetUsedMemory() or 0\n\n    display_name = getattr(self._merge_task, 'identifier', '')\n\n    self._processing_status.UpdateForemanStatus(\n        self._name, self._status, self._pid, used_memory, display_name,\n        self._number_of_consumed_sources, self._number_of_produced_sources,\n        self._number_of_consumed_events, self._number_of_produced_events,\n        self._number_of_consumed_event_tags,\n        self._number_of_produced_event_tags,\n        self._number_of_consumed_warnings, self._number_of_produced_warnings,\n        self._number_of_consumed_reports, self._number_of_produced_reports)\n\n    self._processing_status.UpdateEventsStatus(self._events_status)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _UpdateProcessingStatus(self, pid, process_status, used_memory):\n    self._RaiseIfNotRegistered(pid)\n\n    if not process_status:\n      return\n\n    process = self._processes_per_pid[pid]\n\n    status_indicator = process_status.get('processing_status', None)\n\n    self._RaiseIfNotMonitored(pid)\n\n    display_name = process_status.get('display_name', '')\n\n    number_of_consumed_event_tags = process_status.get(\n        'number_of_consumed_event_tags', None)\n    number_of_produced_event_tags = process_status.get(\n        'number_of_produced_event_tags', None)\n\n    number_of_consumed_events = process_status.get(\n        'number_of_consumed_events', None)\n    number_of_produced_events = process_status.get(\n        'number_of_produced_events', None)\n\n    number_of_consumed_reports = process_status.get(\n        'number_of_consumed_reports', None)\n    number_of_produced_reports = process_status.get(\n        'number_of_produced_reports', None)\n\n    number_of_consumed_sources = process_status.get(\n        'number_of_consumed_sources', None)\n    number_of_produced_sources = process_status.get(\n        'number_of_produced_sources', None)\n\n    number_of_consumed_warnings = process_status.get(\n        'number_of_consumed_warnings', None)\n    number_of_produced_warnings = process_status.get(\n        'number_of_produced_warnings', None)\n\n    if status_indicator != definitions.STATUS_INDICATOR_IDLE:\n      last_activity_timestamp = process_status.get(\n          'last_activity_timestamp', 0.0)\n\n      if last_activity_timestamp:\n        last_activity_timestamp += self._PROCESS_WORKER_TIMEOUT\n\n        current_timestamp = time.time()\n        if current_timestamp > last_activity_timestamp:\n          logger.error((\n              'Process {0:s} (PID: {1:d}) has not reported activity within '\n              'the timeout period.').format(process.name, pid))\n          status_indicator = definitions.STATUS_INDICATOR_NOT_RESPONDING\n\n    self._processing_status.UpdateWorkerStatus(\n        process.name, status_indicator, pid, used_memory, display_name,\n        number_of_consumed_sources, number_of_produced_sources,\n        number_of_consumed_events, number_of_produced_events,\n        number_of_consumed_event_tags, number_of_produced_event_tags,\n        number_of_consumed_reports, number_of_produced_reports,\n        number_of_consumed_warnings, number_of_produced_warnings)", "response": "Updates the processing status of a worker process."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nstart a worker process.", "response": "def _StartWorkerProcess(self, process_name, storage_writer):\n    \"\"\"Creates, starts, monitors and registers a worker process.\n\n    Args:\n      process_name (str): process name.\n      storage_writer (StorageWriter): storage writer for a session storage used\n          to create task storage.\n\n    Returns:\n      MultiProcessWorkerProcess: extraction worker process or None on error.\n    \"\"\"\n    analysis_plugin = self._analysis_plugins.get(process_name, None)\n    if not analysis_plugin:\n      logger.error('Missing analysis plugin: {0:s}'.format(process_name))\n      return None\n\n    if self._use_zeromq:\n      queue_name = '{0:s} output event queue'.format(process_name)\n      output_event_queue = zeromq_queue.ZeroMQPushBindQueue(\n          name=queue_name, timeout_seconds=self._QUEUE_TIMEOUT)\n      # Open the queue so it can bind to a random port, and we can get the\n      # port number to use in the input queue.\n      output_event_queue.Open()\n\n    else:\n      output_event_queue = multi_process_queue.MultiProcessingQueue(\n          timeout=self._QUEUE_TIMEOUT)\n\n    self._event_queues[process_name] = output_event_queue\n\n    if self._use_zeromq:\n      queue_name = '{0:s} input event queue'.format(process_name)\n      input_event_queue = zeromq_queue.ZeroMQPullConnectQueue(\n          name=queue_name, delay_open=True, port=output_event_queue.port,\n          timeout_seconds=self._QUEUE_TIMEOUT)\n\n    else:\n      input_event_queue = output_event_queue\n\n    process = analysis_process.AnalysisProcess(\n        input_event_queue, storage_writer, self._knowledge_base,\n        analysis_plugin, self._processing_configuration,\n        data_location=self._data_location,\n        event_filter_expression=self._event_filter_expression,\n        name=process_name)\n\n    process.start()\n\n    logger.info('Started analysis plugin: {0:s} (PID: {1:d}).'.format(\n        process_name, process.pid))\n\n    try:\n      self._StartMonitoringProcess(process)\n    except (IOError, KeyError) as exception:\n      logger.error((\n          'Unable to monitor analysis plugin: {0:s} (PID: {1:d}) '\n          'with error: {2!s}').format(process_name, process.pid, exception))\n\n      process.terminate()\n      return None\n\n    self._RegisterProcess(process)\n    return process"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nanalyze the events in a plaso storage.", "response": "def AnalyzeEvents(\n      self, knowledge_base_object, storage_writer, data_location,\n      analysis_plugins, processing_configuration, event_filter=None,\n      event_filter_expression=None, status_update_callback=None,\n      worker_memory_limit=None):\n    \"\"\"Analyzes events in a plaso storage.\n\n    Args:\n      knowledge_base_object (KnowledgeBase): contains information from\n          the source data needed for processing.\n      storage_writer (StorageWriter): storage writer.\n      data_location (str): path to the location that data files should\n          be loaded from.\n      analysis_plugins (dict[str, AnalysisPlugin]): analysis plugins that\n          should be run and their names.\n      processing_configuration (ProcessingConfiguration): processing\n          configuration.\n      event_filter (Optional[FilterObject]): event filter.\n      event_filter_expression (Optional[str]): event filter expression.\n      status_update_callback (Optional[function]): callback function for status\n          updates.\n      worker_memory_limit (Optional[int]): maximum amount of memory a worker is\n          allowed to consume, where None represents the default memory limit\n          and 0 represents no limit.\n\n    Raises:\n      KeyboardInterrupt: if a keyboard interrupt was raised.\n    \"\"\"\n    if not analysis_plugins:\n      return\n\n    keyboard_interrupt = False\n\n    self._analysis_plugins = {}\n    self._data_location = data_location\n    self._event_filter_expression = event_filter_expression\n    self._events_status = processing_status.EventsStatus()\n    self._knowledge_base = knowledge_base_object\n    self._status_update_callback = status_update_callback\n    self._processing_configuration = processing_configuration\n\n    if worker_memory_limit is None:\n      self._worker_memory_limit = definitions.DEFAULT_WORKER_MEMORY_LIMIT\n    else:\n      self._worker_memory_limit = worker_memory_limit\n\n    self._StartProfiling(self._processing_configuration.profiling)\n\n    # Set up the storage writer before the analysis processes.\n    storage_writer.StartTaskStorage()\n\n    self._StartAnalysisProcesses(storage_writer, analysis_plugins)\n\n    # Start the status update thread after open of the storage writer\n    # so we don't have to clean up the thread if the open fails.\n    self._StartStatusUpdateThread()\n\n    try:\n      # Open the storage file after creating the worker processes otherwise\n      # the ZIP storage file will remain locked as long as the worker processes\n      # are alive.\n      storage_writer.Open()\n      storage_writer.ReadPreprocessingInformation(knowledge_base_object)\n      storage_writer.WriteSessionStart()\n\n      try:\n        self._AnalyzeEvents(\n            storage_writer, analysis_plugins, event_filter=event_filter)\n\n        self._status = definitions.STATUS_INDICATOR_FINALIZING\n\n      except KeyboardInterrupt:\n        keyboard_interrupt = True\n        self._abort = True\n\n        self._processing_status.aborted = True\n        if self._status_update_callback:\n          self._status_update_callback(self._processing_status)\n\n      finally:\n        storage_writer.WriteSessionCompletion(aborted=self._abort)\n\n        storage_writer.Close()\n\n    finally:\n      # Stop the status update thread after close of the storage writer\n      # so we include the storage sync to disk in the status updates.\n      self._StopStatusUpdateThread()\n\n    try:\n      self._StopAnalysisProcesses(abort=self._abort)\n\n    except KeyboardInterrupt:\n      keyboard_interrupt = True\n\n      self._AbortKill()\n\n      # The abort can leave the main process unresponsive\n      # due to incorrectly finalized IPC.\n      self._KillProcess(os.getpid())\n\n    self._StopProfiling()\n\n    # Reset values.\n    self._analysis_plugins = {}\n    self._data_location = None\n    self._event_filter_expression = None\n    self._knowledge_base = None\n    self._processing_configuration = None\n    self._status_update_callback = None\n    self._worker_memory_limit = definitions.DEFAULT_WORKER_MEMORY_LIMIT\n\n    if keyboard_interrupt:\n      raise KeyboardInterrupt\n\n    if keyboard_interrupt:\n      raise KeyboardInterrupt"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ExportEvents(\n      self, knowledge_base_object, storage_reader, output_module,\n      processing_configuration, deduplicate_events=True, event_filter=None,\n      status_update_callback=None, time_slice=None, use_time_slicer=False):\n    \"\"\"Exports events using an output module.\n\n    Args:\n      knowledge_base_object (KnowledgeBase): contains information from\n          the source data needed for processing.\n      storage_reader (StorageReader): storage reader.\n      output_module (OutputModule): output module.\n      processing_configuration (ProcessingConfiguration): processing\n          configuration.\n      deduplicate_events (Optional[bool]): True if events should be\n          deduplicated.\n      event_filter (Optional[FilterObject]): event filter.\n      status_update_callback (Optional[function]): callback function for status\n          updates.\n      time_slice (Optional[TimeSlice]): slice of time to output.\n      use_time_slicer (Optional[bool]): True if the 'time slicer' should be\n          used. The 'time slicer' will provide a context of events around\n          an event of interest.\n    \"\"\"\n    self._events_status = processing_status.EventsStatus()\n    self._processing_configuration = processing_configuration\n    self._status_update_callback = status_update_callback\n\n    storage_reader.ReadPreprocessingInformation(knowledge_base_object)\n\n    total_number_of_events = 0\n    for session in storage_reader.GetSessions():\n      total_number_of_events += session.parsers_counter['total']\n\n    self._events_status.total_number_of_events = total_number_of_events\n\n    output_module.Open()\n    output_module.WriteHeader()\n\n    self._StartStatusUpdateThread()\n\n    self._StartProfiling(self._processing_configuration.profiling)\n\n    try:\n      self._ExportEvents(\n          storage_reader, output_module, deduplicate_events=deduplicate_events,\n          event_filter=event_filter, time_slice=time_slice,\n          use_time_slicer=use_time_slicer)\n\n    finally:\n      # Stop the status update thread after close of the storage writer\n      # so we include the storage sync to disk in the status updates.\n      self._StopStatusUpdateThread()\n\n    output_module.WriteFooter()\n    output_module.Close()\n\n    self._StopProfiling()\n\n    self._UpdateForemanProcessStatus()\n\n    if self._status_update_callback:\n      self._status_update_callback(self._processing_status)\n\n    # Reset values.\n    self._status_update_callback = None\n    self._processing_configuration = None\n    self._events_status = None", "response": "Exports events from a KnowledgeBase object to an output module."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _GetLinkedPath(self, event):\n    if hasattr(event, 'local_path'):\n      return event.local_path\n\n    if hasattr(event, 'network_path'):\n      return event.network_path\n\n    if hasattr(event, 'relative_path'):\n      paths = []\n      if hasattr(event, 'working_directory'):\n        paths.append(event.working_directory)\n      paths.append(event.relative_path)\n\n      return '\\\\'.join(paths)\n\n    return 'Unknown'", "response": "Determines the linked path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef Main():\n  tool = image_export_tool.ImageExportTool()\n\n  if not tool.ParseArguments():\n    return False\n\n  if tool.list_signature_identifiers:\n    tool.ListSignatureIdentifiers()\n    return True\n\n  if not tool.has_filters:\n    logging.warning('No filter defined exporting all files.')\n\n  # TODO: print more status information like PrintOptions.\n  tool.PrintFilterCollection()\n\n  try:\n    tool.ProcessSources()\n\n  except (KeyboardInterrupt, errors.UserAbort):\n    logging.warning('Aborted by user.')\n    return False\n\n  except errors.BadConfigOption as exception:\n    logging.warning(exception)\n    return False\n\n  except errors.SourceScannerError as exception:\n    logging.warning((\n        'Unable to scan for a supported filesystem with error: {0!s}\\n'\n        'Most likely the image format is not supported by the '\n        'tool.').format(exception))\n    return False\n\n  return True", "response": "The main function.\n\n  Returns:\n    bool: True if successful or False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _DecodeURL(self, url):\n    if not url:\n      return ''\n\n    decoded_url = urlparse.unquote(url)\n    if isinstance(decoded_url, py2to3.BYTES_TYPE):\n      try:\n        decoded_url = decoded_url.decode('utf-8')\n      except UnicodeDecodeError as exception:\n        decoded_url = decoded_url.decode('utf-8', errors='replace')\n        logger.warning(\n            'Unable to decode URL: {0:s} with error: {1!s}'.format(\n                url, exception))\n\n    return decoded_url", "response": "Decodes the URL and replaces %XX to their corresponding characters."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nextracting a search query from a GMail search URL.", "response": "def _ExtractGMailSearchQuery(self, url):\n    \"\"\"Extracts a search query from a GMail search URL.\n\n    GMail: https://mail.google.com/mail/u/0/#search/query[/?]\n\n    Args:\n      url (str): URL.\n\n    Returns:\n      str: search query or None if no query was found.\n    \"\"\"\n    if 'search/' not in url:\n      return None\n\n    _, _, line = url.partition('search/')\n    line, _, _ = line.partition('/')\n    line, _, _ = line.partition('?')\n\n    return line.replace('+', ' ')"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nextract a search query from a Google Docs URL.", "response": "def _ExtractGoogleDocsSearchQuery(self, url):\n    \"\"\"Extracts a search query from a Google docs URL.\n\n    Google Docs: https://docs.google.com/.*/u/0/?q=query\n\n    Args:\n      url (str): URL.\n\n    Returns:\n      str: search query  or None if no query was found.\n    \"\"\"\n    if 'q=' not in url:\n      return None\n\n    line = self._GetBetweenQEqualsAndAmpersand(url)\n    if not line:\n      return None\n\n    return line.replace('+', ' ')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _ExtractGoogleSearchQuery(self, url):\n    if 'search' not in url or 'q=' not in url:\n      return None\n\n    line = self._GetBetweenQEqualsAndAmpersand(url)\n    if not line:\n      return None\n\n    return line.replace('+', ' ')", "response": "Extracts a search query from a Google URL."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _ExtractYahooSearchQuery(self, url):\n    if 'p=' not in url:\n      return None\n    _, _, line = url.partition('p=')\n    before_and, _, _ = line.partition('&')\n    if not before_and:\n      return None\n    yahoo_search_url = before_and.split()[0]\n\n    return yahoo_search_url.replace('+', ' ')", "response": "Extracts a search query from a Yahoo search URL."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nextract a search query from a Yandex search URL.", "response": "def _ExtractYandexSearchQuery(self, url):\n    \"\"\"Extracts a search query from a Yandex search URL.\n\n    Yandex: https://www.yandex.com/search/?text=query\n\n    Args:\n      url (str): URL.\n\n    Returns:\n      str: search query or None if no query was found.\n    \"\"\"\n    if 'text=' not in url:\n      return None\n    _, _, line = url.partition('text=')\n    before_and, _, _ = line.partition('&')\n    if not before_and:\n      return None\n    yandex_search_url = before_and.split()[0]\n\n    return yandex_search_url.replace('+', ' ')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _GetBetweenQEqualsAndAmpersand(self, url):\n    # Make sure we're analyzing the query part of the URL.\n    _, _, url = url.partition('?')\n    # Look for a key value pair named 'q'.\n    _, _, url = url.partition('q=')\n    if not url:\n      return ''\n\n    # Strip additional key value pairs.\n    url, _, _ = url.partition('&')\n    return url", "response": "Retrieves the substring between the substrings q = and &."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef CompileReport(self, mediator):\n    results = {}\n    for key, count in iter(self._counter.items()):\n      search_engine, _, search_term = key.partition(':')\n      results.setdefault(search_engine, {})\n      results[search_engine][search_term] = count\n\n    lines_of_text = []\n    for search_engine, terms in sorted(results.items()):\n      lines_of_text.append(' == ENGINE: {0:s} =='.format(search_engine))\n\n      for search_term, count in sorted(\n          terms.items(), key=lambda x: (x[1], x[0]), reverse=True):\n        lines_of_text.append('{0:d} {1:s}'.format(count, search_term))\n\n      # An empty string is added to have SetText create an empty line.\n      lines_of_text.append('')\n\n    lines_of_text.append('')\n    report_text = '\\n'.join(lines_of_text)\n    analysis_report = reports.AnalysisReport(\n        plugin_name=self.NAME, text=report_text)\n    analysis_report.report_array = self._search_term_timeline\n    analysis_report.report_dict = results\n    return analysis_report", "response": "Compiles an analysis report."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nanalyzing an event and returns a new event.", "response": "def ExamineEvent(self, mediator, event):\n    \"\"\"Analyzes an event.\n\n    Args:\n      mediator (AnalysisMediator): mediates interactions between\n          analysis plugins and other components, such as storage and dfvfs.\n      event (EventObject): event to examine.\n    \"\"\"\n    # This event requires an URL attribute.\n    url = getattr(event, 'url', None)\n    if not url:\n      return\n\n    # TODO: refactor this the source should be used in formatting only.\n    # Check if we are dealing with a web history event.\n    source, _ = formatters_manager.FormattersManager.GetSourceStrings(event)\n\n    if source != 'WEBHIST':\n      return\n\n    for engine, url_expression, method_name in self._URL_FILTERS:\n      callback_method = getattr(self, method_name, None)\n      if not callback_method:\n        logger.warning('Missing method: {0:s}'.format(callback_method))\n        continue\n\n      match = url_expression.search(url)\n      if not match:\n        continue\n\n      search_query = callback_method(url)\n      if not search_query:\n        logger.warning('Missing search query for URL: {0:s}'.format(url))\n        continue\n\n      search_query = self._DecodeURL(search_query)\n      if not search_query:\n        continue\n\n      event_tag = self._CreateEventTag(\n          event, self._EVENT_TAG_COMMENT, self._EVENT_TAG_LABELS)\n      mediator.ProduceEventTag(event_tag)\n\n      self._counter['{0:s}:{1:s}'.format(engine, search_query)] += 1\n\n      # Add the timeline format for each search term.\n      timestamp = getattr(event, 'timestamp', 0)\n      source = getattr(event, 'parser', 'N/A')\n      source = getattr(event, 'plugin', source)\n      self._search_term_timeline.append(\n          SEARCH_OBJECT(timestamp, source, engine, search_query))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ParseConversationRow(self, parser_mediator, query, row, **unused_kwargs):\n    query_hash = hash(query)\n\n    event_data = TangoAndroidConversationEventData()\n    event_data.conversation_identifier = self._GetRowValue(\n        query_hash, row, 'conv_id')\n\n    # TODO: payload is a base64 encoded binary blob, we need to find the\n    # structure to extract the relevant bits.\n    # event_data.payload = self._GetRowValue(query_hash, row, 'payload')\n\n    date_time = dfdatetime_semantic_time.NotSet()\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_NOT_A_TIME)\n    parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Parses a conversation row from the database."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ParseMessageRow(self, parser_mediator, query, row, **unused_kwargs):\n    query_hash = hash(query)\n\n    event_data = TangoAndroidMessageEventData()\n    event_data.message_identifier = self._GetRowValue(\n        query_hash, row, 'msg_id')\n\n    # TODO: payload is a base64 encoded binary blob, we need to find the\n    # structure to extract the relevant bits.\n    # event_data.payload = self._GetRowValue(query_hash, row, 'payload')\n\n    event_data.direction = self._GetRowValue(query_hash, row, 'direction')\n\n    timestamp = self._GetRowValue(query_hash, row, 'create_time')\n    if timestamp:\n      date_time = dfdatetime_java_time.JavaTime(timestamp=timestamp)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_CREATION)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    timestamp = self._GetRowValue(query_hash, row, 'send_time')\n    if timestamp:\n      date_time = dfdatetime_java_time.JavaTime(timestamp=timestamp)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_SENT)\n      parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Parses a message row from the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses a contact row from the database.", "response": "def ParseContactRow(self, parser_mediator, query, row, **unused_kwargs):\n    \"\"\"Parses a contact row from the database.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      query (str): query that created the row.\n      row (sqlite3.Row): row resulting from query.\n    \"\"\"\n    query_hash = hash(query)\n\n    event_data = TangoAndroidContactEventData()\n\n    first_name = self._GetRowValue(query_hash, row, 'first_name')\n    try:\n      decoded_text = base64_decode(first_name)\n      event_data.first_name = codecs.decode(decoded_text, 'utf-8')\n    except ValueError:\n      event_data.first_name = first_name\n      parser_mediator.ProduceExtractionWarning(\n          'unable to parse first name: {0:s}'.format(first_name))\n\n    last_name = self._GetRowValue(query_hash, row, 'last_name')\n    try:\n      decoded_text = base64_decode(last_name)\n      event_data.last_name = codecs.decode(decoded_text, 'utf-8')\n    except ValueError:\n      event_data.last_name = last_name\n      parser_mediator.ProduceExtractionWarning(\n          'unable to parse last name: {0:s}'.format(last_name))\n\n    event_data.birthday = self._GetRowValue(query_hash, row, 'birthday')\n    event_data.gender = self._GetRowValue(query_hash, row, 'gender')\n\n    status = self._GetRowValue(query_hash, row, 'status')\n    try:\n      decoded_text = base64_decode(status)\n      event_data.status = codecs.decode(decoded_text, 'utf-8')\n    except ValueError:\n      event_data.status = status\n      parser_mediator.ProduceExtractionWarning(\n          'unable to parse status: {0:s}'.format(status))\n\n    event_data.distance = self._GetRowValue(query_hash, row, 'distance')\n\n    is_friend = self._GetRowValue(query_hash, row, 'friend')\n    event_data.is_friend = False\n    if is_friend:\n      event_data.is_friend = True\n\n    event_data.friend_request_type = self._GetRowValue(\n        query_hash, row, 'friend_request_type')\n\n    friend_request_message = self._GetRowValue(\n        query_hash, row, 'friend_request_message')\n    try:\n      decoded_text = base64_decode(friend_request_message)\n      event_data.friend_request_message = codecs.decode(decoded_text, 'utf-8')\n    except ValueError:\n      event_data.friend_request_message = friend_request_message\n      parser_mediator.ProduceExtractionWarning(\n          'unable to parse status: {0:s}'.format(friend_request_message))\n\n    timestamp = self._GetRowValue(query_hash, row, 'last_active_time')\n    if timestamp:\n      date_time = dfdatetime_java_time.JavaTime(timestamp=timestamp)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_LAST_ACTIVE)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    timestamp = self._GetRowValue(query_hash, row, 'last_access_time')\n    if timestamp:\n      date_time = dfdatetime_java_time.JavaTime(timestamp=timestamp)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_LAST_ACCESS)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    timestamp = self._GetRowValue(query_hash, row, 'friend_request_time')\n    if timestamp:\n      date_time = dfdatetime_java_time.JavaTime(timestamp=timestamp)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_SENT)\n      parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef GetFormattedEventObject(cls, event):\n    time_string = timelib.Timestamp.CopyToIsoFormat(event.timestamp)\n\n    lines_of_text = [\n        '+-' * 40,\n        '[Timestamp]:',\n        '  {0:s}'.format(time_string)]\n\n    pathspec = getattr(event, 'pathspec', None)\n    if pathspec:\n      lines_of_text.append('[Pathspec]:')\n      attribute_string = pathspec.comparable.replace('\\n', '\\n  ')\n      attribute_string = '  {0:s}\\n'.format(attribute_string)\n      lines_of_text.append(attribute_string)\n\n    # TODO: add support for event tag after event clean up.\n\n    lines_of_text.append('[Reserved attributes]:')\n    out_additional = ['[Additional attributes]:']\n\n    for attribute_name, attribute_value in sorted(event.GetAttributes()):\n      if attribute_name not in definitions.RESERVED_VARIABLE_NAMES:\n        attribute_string = '  {{{0!s}}} {1!s}'.format(\n            attribute_name, attribute_value)\n        out_additional.append(attribute_string)\n\n      elif attribute_name not in ('pathspec', 'tag'):\n        attribute_string = '  {{{0!s}}} {1!s}'.format(\n            attribute_name, attribute_value)\n        lines_of_text.append(attribute_string)\n\n    lines_of_text.append('')\n    out_additional.append('')\n\n    lines_of_text.extend(out_additional)\n    return '\\n'.join(lines_of_text)", "response": "Retrieves a string representation of the event object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrite the event body to the output.", "response": "def WriteEventBody(self, event):\n    \"\"\"Writes the body of an event to the output.\n\n    Args:\n      event (EventObject): event.\n    \"\"\"\n    output_string = NativePythonFormatterHelper.GetFormattedEventObject(event)\n    self._output_writer.Write(output_string)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretrieves and removes the first task from the heap.", "response": "def PopTask(self):\n    \"\"\"Retrieves and removes the first task from the heap.\n\n    Returns:\n      Task: the task or None if the heap is empty.\n    \"\"\"\n    try:\n      _, task = heapq.heappop(self._heap)\n\n    except IndexError:\n      return None\n    self._task_identifiers.remove(task.identifier)\n    return task"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\npush a task onto the heap.", "response": "def PushTask(self, task):\n    \"\"\"Pushes a task onto the heap.\n\n    Args:\n      task (Task): task.\n\n    Raises:\n      ValueError: if the size of the storage file is not set in the task.\n    \"\"\"\n    storage_file_size = getattr(task, 'storage_file_size', None)\n    if not storage_file_size:\n      raise ValueError('Task storage file size not set.')\n\n    if task.file_entry_type == dfvfs_definitions.FILE_ENTRY_TYPE_DIRECTORY:\n      weight = 1\n    else:\n      weight = storage_file_size\n\n    task.merge_priority = weight\n\n    heap_values = (weight, task)\n    heapq.heappush(self._heap, heap_values)\n    self._task_identifiers.add(task.identifier)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmarks processing tasks that exceed the inactive time as abandoned.", "response": "def _AbandonInactiveProcessingTasks(self):\n    \"\"\"Marks processing tasks that exceed the inactive time as abandoned.\n\n    This method does not lock the manager and should be called by a method\n    holding the manager lock.\n    \"\"\"\n    if self._tasks_processing:\n      inactive_time = time.time() - self._TASK_INACTIVE_TIME\n      inactive_time = int(inactive_time * definitions.MICROSECONDS_PER_SECOND)\n\n      # Abandon all tasks after they're identified so as not to modify the\n      # dict while iterating over it.\n      tasks_to_abandon = []\n      for task_identifier, task in iter(self._tasks_processing.items()):\n        if task.last_processing_time < inactive_time:\n          logger.debug('Abandoned processing task: {0:s}.'.format(\n              task_identifier))\n\n          self.SampleTaskStatus(task, 'abandoned_processing')\n          tasks_to_abandon.append((task_identifier, task))\n\n      for task_identifier, task in tasks_to_abandon:\n        self._tasks_abandoned[task_identifier] = task\n        del self._tasks_processing[task_identifier]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _AbandonQueuedTasks(self):\n    # Abandon all tasks after they're identified so as not to modify the\n    # dict while iterating over it.\n    tasks_to_abandon = []\n    for task_identifier, task in iter(self._tasks_queued.items()):\n      logger.debug('Abandoned queued task: {0:s}.'.format(task_identifier))\n      tasks_to_abandon.append((task_identifier, task))\n\n    for task_identifier, task in tasks_to_abandon:\n      self._tasks_abandoned[task_identifier] = task\n      del self._tasks_queued[task_identifier]", "response": "Marks queued tasks abandoned."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if the task should be merged.", "response": "def CheckTaskToMerge(self, task):\n    \"\"\"Checks if the task should be merged.\n\n    Args:\n      task (Task): task.\n\n    Returns:\n      bool: True if the task should be merged.\n\n    Raises:\n      KeyError: if the task was not queued, processing or abandoned.\n    \"\"\"\n    with self._lock:\n      is_abandoned = task.identifier in self._tasks_abandoned\n      is_processing = task.identifier in self._tasks_processing\n      is_queued = task.identifier in self._tasks_queued\n\n      if not is_queued and not is_processing and not is_abandoned:\n        raise KeyError('Status of task {0:s} is unknown.'.format(\n            task.identifier))\n\n      return is_queued or is_processing or is_abandoned and not task.has_retry"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef CreateRetryTask(self):\n    with self._lock:\n      abandoned_task = self._GetTaskPendingRetry()\n      if not abandoned_task:\n        return None\n\n      # The abandoned task is kept in _tasks_abandoned so it can be still\n      # identified in CheckTaskToMerge and UpdateTaskAsPendingMerge.\n\n      retry_task = abandoned_task.CreateRetryTask()\n      logger.debug('Retrying task {0:s} as {1:s}.'.format(\n          abandoned_task.identifier, retry_task.identifier))\n\n      self._tasks_queued[retry_task.identifier] = retry_task\n      self._total_number_of_tasks += 1\n\n      self.SampleTaskStatus(retry_task, 'created_retry')\n\n      return retry_task", "response": "Creates a task that should be retried."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef CreateTask(self, session_identifier):\n    task = tasks.Task(session_identifier)\n    logger.debug('Created task: {0:s}.'.format(task.identifier))\n\n    with self._lock:\n      self._tasks_queued[task.identifier] = task\n      self._total_number_of_tasks += 1\n\n      self.SampleTaskStatus(task, 'created')\n\n    return task", "response": "Creates a task.\n\n    Args:\n      session_identifier (str): the identifier of the session the task is\n          part of.\n\n    Returns:\n      Task: task attribute container."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef CompleteTask(self, task):\n    with self._lock:\n      if task.identifier not in self._tasks_merging:\n        raise KeyError('Task {0:s} was not merging.'.format(task.identifier))\n\n      self.SampleTaskStatus(task, 'completed')\n\n      del self._tasks_merging[task.identifier]\n\n      logger.debug('Completed task {0:s}.'.format(task.identifier))", "response": "Completes a task.\n\n    The task is complete and can be removed from the task manager.\n\n    Args:\n      task (Task): task.\n\n    Raises:\n      KeyError: if the task was not merging."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves all failed tasks.", "response": "def GetFailedTasks(self):\n    \"\"\"Retrieves all failed tasks.\n\n    Failed tasks are tasks that were abandoned and have no retry task once\n    the foreman is done processing.\n\n    Returns:\n      list[Task]: tasks.\n    \"\"\"\n    # TODO: add check to determine foreman is done processing.\n\n    with self._lock:\n      return [task for task in self._tasks_abandoned.values()\n              if not task.has_retry]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nretrieving a task that has been processed by the given task identifier.", "response": "def GetProcessedTaskByIdentifier(self, task_identifier):\n    \"\"\"Retrieves a task that has been processed.\n\n    Args:\n      task_identifier (str): unique identifier of the task.\n\n    Returns:\n      Task: a task that has been processed.\n\n    Raises:\n      KeyError: if the task was not processing, queued or abandoned.\n    \"\"\"\n    with self._lock:\n      task = self._tasks_processing.get(task_identifier, None)\n      if not task:\n        task = self._tasks_queued.get(task_identifier, None)\n      if not task:\n        task = self._tasks_abandoned.get(task_identifier, None)\n      if not task:\n        raise KeyError('Status of task {0:s} is unknown'.format(\n            task_identifier))\n\n    return task"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef GetStatusInformation(self):\n    status = processing_status.TasksStatus()\n\n    with self._lock:\n      status.number_of_abandoned_tasks = len(self._tasks_abandoned)\n      status.number_of_queued_tasks = len(self._tasks_queued)\n      status.number_of_tasks_pending_merge = (\n          len(self._tasks_pending_merge) + len(self._tasks_merging))\n      status.number_of_tasks_processing = len(self._tasks_processing)\n      status.total_number_of_tasks = self._total_number_of_tasks\n\n    return status", "response": "Retrieves status information about the tasks."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef GetTaskPendingMerge(self, current_task):\n    next_task = self._tasks_pending_merge.PeekTask()\n    if not next_task:\n      return None\n\n    if current_task and next_task.merge_priority > current_task.merge_priority:\n      return None\n\n    with self._lock:\n      next_task = self._tasks_pending_merge.PopTask()\n\n    self._tasks_merging[next_task.identifier] = next_task\n    return next_task", "response": "Retrieves the next task that is pending merge or has a higher priority."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef HasPendingTasks(self):\n    with self._lock:\n      self._AbandonInactiveProcessingTasks()\n\n      if self._tasks_processing:\n        return True\n\n      # There are no tasks being processed, but we might be\n      # waiting for some tasks to be merged.\n      if self._HasTasksPendingMerge():\n        return True\n\n      # There are no tasks processing or pending merge, but there may\n      # still be some waiting to be retried, so we check that.\n      if self._HasTasksPendingRetry():\n        return True\n\n      # It is possible that a worker has processed a task and the foreman has\n      # not been informed about it, since there is no feedback from the worker\n      # when it pops a task from the queue.\n\n      # If we believe all the workers are idle for longer than the task\n      # inactive time (timeout) abandon all queued tasks. This ensures\n      # that processing actually stops when the foreman never gets an\n      # update from a worker.\n\n      if self._tasks_queued:\n        inactive_time = time.time() - self._TASK_INACTIVE_TIME\n        inactive_time = int(inactive_time * definitions.MICROSECONDS_PER_SECOND)\n\n        if self._latest_task_processing_time < inactive_time:\n          self._AbandonQueuedTasks()\n\n      if self._tasks_queued:\n        return True\n\n      if self._tasks_merging:\n        return True\n\n    # There are no tasks pending any work.\n    return False", "response": "Determines if there are pending tasks in the given order."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nremoves an abandoned task.", "response": "def RemoveTask(self, task):\n    \"\"\"Removes an abandoned task.\n\n    Args:\n      task (Task): task.\n\n    Raises:\n      KeyError: if the task was not abandoned or the task was abandoned and\n          was not retried.\n    \"\"\"\n    with self._lock:\n      if task.identifier not in self._tasks_abandoned:\n        raise KeyError('Task {0:s} was not abandoned.'.format(task.identifier))\n\n      if not task.has_retry:\n        raise KeyError(\n            'Will not remove a task {0:s} without retry task.'.format(\n                task.identifier))\n\n      del self._tasks_abandoned[task.identifier]\n\n      logger.debug('Removed task {0:s}.'.format(task.identifier))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntake a sample of the status of the task for profiling.", "response": "def SampleTaskStatus(self, task, status):\n    \"\"\"Takes a sample of the status of the task for profiling.\n\n    Args:\n      task (Task): a task.\n      status (str): status.\n    \"\"\"\n    if self._tasks_profiler:\n      self._tasks_profiler.Sample(task, status)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nstarts profiling. Args: configuration (ProfilingConfiguration): profiling configuration. identifier (str): identifier of the profiling session used to create the sample filename.", "response": "def StartProfiling(self, configuration, identifier):\n    \"\"\"Starts profiling.\n\n    Args:\n      configuration (ProfilingConfiguration): profiling configuration.\n      identifier (str): identifier of the profiling session used to create\n          the sample filename.\n    \"\"\"\n    if not configuration:\n      return\n\n    if configuration.HaveProfileTasks():\n      self._tasks_profiler = profilers.TasksProfiler(identifier, configuration)\n      self._tasks_profiler.Start()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef UpdateTaskAsPendingMerge(self, task):\n    with self._lock:\n      is_abandoned = task.identifier in self._tasks_abandoned\n      is_processing = task.identifier in self._tasks_processing\n      is_queued = task.identifier in self._tasks_queued\n\n      if not is_queued and not is_processing and not is_abandoned:\n        raise KeyError('Status of task {0:s} is unknown.'.format(\n            task.identifier))\n\n      if is_abandoned and task.has_retry:\n        raise KeyError('Will not merge a task {0:s} with retry task.'.format(\n            task.identifier))\n\n      if is_queued:\n        logger.debug('Task {0:s} was queued, now merging.'.format(\n            task.identifier))\n        del self._tasks_queued[task.identifier]\n\n      if is_processing:\n        logger.debug('Task {0:s} was processing, now merging.'.format(\n            task.identifier))\n        del self._tasks_processing[task.identifier]\n\n      if is_abandoned:\n        logger.debug('Task {0:s} was abandoned, now merging.'.format(\n            task.identifier))\n        del self._tasks_abandoned[task.identifier]\n\n      self._tasks_pending_merge.PushTask(task)\n\n      self.SampleTaskStatus(task, 'pending_merge')\n\n      task.UpdateProcessingTime()\n      self._UpdateLatestProcessingTime(task)", "response": "Updates the task manager to reflect the task is ready to be merged."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef UpdateTaskAsProcessingByIdentifier(self, task_identifier):\n    with self._lock:\n      task_processing = self._tasks_processing.get(task_identifier, None)\n      if task_processing:\n        task_processing.UpdateProcessingTime()\n        self._UpdateLatestProcessingTime(task_processing)\n        return\n\n      task_queued = self._tasks_queued.get(task_identifier, None)\n      if task_queued:\n        logger.debug('Task {0:s} was queued, now processing.'.format(\n            task_identifier))\n        self._tasks_processing[task_identifier] = task_queued\n        del self._tasks_queued[task_identifier]\n\n        task_queued.UpdateProcessingTime()\n        self._UpdateLatestProcessingTime(task_queued)\n        return\n\n      task_abandoned = self._tasks_abandoned.get(task_identifier, None)\n      if task_abandoned:\n        del self._tasks_abandoned[task_identifier]\n        self._tasks_processing[task_identifier] = task_abandoned\n        logger.debug('Task {0:s} was abandoned, but now processing.'.format(\n            task_identifier))\n\n        task_abandoned.UpdateProcessingTime()\n        self._UpdateLatestProcessingTime(task_abandoned)\n        return\n\n      if task_identifier in self._tasks_pending_merge:\n        # No need to update the processing time, as this task is already\n        # finished processing and is just waiting for merge.\n        return\n\n    # If we get here, we don't know what state the tasks is in, so raise.\n    raise KeyError('Status of task {0:s} is unknown.'.format(task_identifier))", "response": "Updates the task manager to reflect the task is processing."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing an SMS row.", "response": "def ParseSmsRow(self, parser_mediator, query, row, **unused_kwargs):\n    \"\"\"Parses an SMS row.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      query (str): query that created the row.\n      row (sqlite3.Row): row.\n    \"\"\"\n    query_hash = hash(query)\n\n    sms_read = self._GetRowValue(query_hash, row, 'read')\n    sms_type = self._GetRowValue(query_hash, row, 'type')\n\n    event_data = AndroidSMSEventData()\n    event_data.address = self._GetRowValue(query_hash, row, 'address')\n    event_data.body = self._GetRowValue(query_hash, row, 'body')\n    event_data.offset = self._GetRowValue(query_hash, row, 'id')\n    event_data.query = query\n    event_data.sms_read = self.SMS_READ.get(sms_read, 'UNKNOWN')\n    event_data.sms_type = self.SMS_TYPE.get(sms_type, 'UNKNOWN')\n\n    timestamp = self._GetRowValue(query_hash, row, 'date')\n    date_time = dfdatetime_java_time.JavaTime(timestamp=timestamp)\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_CREATION)\n    parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _ParseValueData(self, parser_mediator, registry_key, registry_value):\n    value_data = registry_value.data\n\n    value_data_size = len(value_data)\n    if value_data_size < 4:\n      return\n\n    header_map = self._GetDataTypeMap('programscache_header')\n\n    try:\n      header = self._ReadStructureFromByteStream(\n          value_data, 0, header_map)\n    except (ValueError, errors.ParseError) as exception:\n      parser_mediator.ProduceExtractionWarning(\n          'unable to parse header value with error: {0!s}'.format(\n              exception))\n      return\n\n    if header.format_version not in (1, 9, 12, 19):\n      parser_mediator.ProduceExtractionWarning(\n          'unsupported format version: {0:d}'.format(header.format_version))\n      return\n\n    known_folder_identifier = None\n    if header.format_version == 1:\n      value_data_offset = 8\n\n    elif header.format_version == 9:\n      value_data_offset = 6\n\n    elif header.format_version in (12, 19):\n      known_folder_identifier = uuid.UUID(bytes_le=value_data[4:20])\n      value_data_offset = 20\n\n    entry_header_map = self._GetDataTypeMap('programscache_entry_header')\n    entry_footer_map = self._GetDataTypeMap('programscache_entry_footer')\n\n    sentinel = 0\n    if header.format_version != 9:\n      try:\n        entry_footer = self._ReadStructureFromByteStream(\n            value_data[value_data_offset:], value_data_offset, entry_footer_map)\n      except (ValueError, errors.ParseError) as exception:\n        parser_mediator.ProduceExtractionWarning((\n            'unable to parse sentinel at offset: 0x{0:08x} '\n            'with error: {1!s}').format(value_data_offset, exception))\n        return\n\n      value_data_offset += entry_footer_map.GetByteSize()\n\n      sentinel = entry_footer.sentinel\n\n    link_targets = []\n    while sentinel in (0x00, 0x01):\n      if value_data_offset >= value_data_size:\n        break\n\n      try:\n        entry_header = self._ReadStructureFromByteStream(\n            value_data[value_data_offset:], value_data_offset, entry_header_map)\n      except (ValueError, errors.ParseError) as exception:\n        parser_mediator.ProduceExtractionWarning((\n            'unable to parse entry header at offset: 0x{0:08x} '\n            'with error: {1!s}').format(value_data_offset, exception))\n        break\n\n      value_data_offset += entry_header_map.GetByteSize()\n\n      display_name = '{0:s} {1:s}'.format(\n          registry_key.path, registry_value.name)\n\n      shell_items_parser = shell_items.ShellItemsParser(display_name)\n      shell_items_parser.ParseByteStream(\n          parser_mediator, value_data[value_data_offset:],\n          codepage=parser_mediator.codepage)\n\n      link_target = shell_items_parser.CopyToPath()\n      link_targets.append(link_target)\n\n      value_data_offset += entry_header.data_size\n\n      try:\n        entry_footer = self._ReadStructureFromByteStream(\n            value_data[value_data_offset:], value_data_offset, entry_footer_map)\n      except (ValueError, errors.ParseError) as exception:\n        parser_mediator.ProduceExtractionWarning((\n            'unable to parse entry footer at offset: 0x{0:08x} '\n            'with error: {1!s}').format(value_data_offset, exception))\n        return\n\n      value_data_offset += entry_footer_map.GetByteSize()\n\n      sentinel = entry_footer.sentinel\n\n    # TODO: recover remaining items.\n\n    if known_folder_identifier:\n      known_folder_identifier = '{0!s}'.format(known_folder_identifier)\n\n    event_data = windows_events.WindowsRegistryListEventData()\n    event_data.key_path = registry_key.path\n    event_data.known_folder_identifier = known_folder_identifier\n    event_data.list_name = registry_value.name\n    event_data.list_values = ' '.join([\n        '{0:d}: {1:s}'.format(index, link_target)\n        for index, link_target in enumerate(link_targets)])\n    event_data.value_name = registry_value.name\n\n    event = time_events.DateTimeValuesEvent(\n        registry_key.last_written_time, definitions.TIME_DESCRIPTION_WRITTEN)\n    parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Parses the value data."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ExtractEvents(self, parser_mediator, registry_key, **kwargs):\n    registry_value = registry_key.GetValueByName('ProgramsCache')\n    if registry_value:\n      self._ParseValueData(parser_mediator, registry_key, registry_value)\n\n    registry_value = registry_key.GetValueByName('ProgramsCacheSMP')\n    if registry_value:\n      self._ParseValueData(parser_mediator, registry_key, registry_value)\n\n    registry_value = registry_key.GetValueByName('ProgramsCacheTBP')\n    if registry_value:\n      self._ParseValueData(parser_mediator, registry_key, registry_value)\n\n    values_dict = {}\n    for registry_value in registry_key.GetValues():\n      # Ignore the default value.\n      if not registry_value.name or registry_value.name in (\n          'ProgramsCache', 'ProgramsCacheSMP', 'ProgramsCacheTBP'):\n        continue\n\n      # Ignore any value that is empty or that does not contain a string.\n      if not registry_value.data or not registry_value.DataIsString():\n        continue\n\n      values_dict[registry_value.name] = registry_value.GetDataAsObject()\n\n    event_data = windows_events.WindowsRegistryEventData()\n    event_data.key_path = registry_key.path\n    event_data.offset = registry_key.offset\n    event_data.regvalue = values_dict\n\n    event = time_events.DateTimeValuesEvent(\n        registry_key.last_written_time, definitions.TIME_DESCRIPTION_WRITTEN)\n    parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Extracts events from a Windows Registry key."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate a name for the storage file.", "response": "def _GenerateStorageFileName(self):\n    \"\"\"Generates a name for the storage file.\n\n    The result use a timestamp and the basename of the source path.\n\n    Returns:\n      str: a filename for the storage file in the form <time>-<source>.plaso\n\n    Raises:\n      BadConfigOption: raised if the source path is not set.\n    \"\"\"\n    if not self._source_path:\n      raise errors.BadConfigOption('Please define a source (--source).')\n\n    timestamp = datetime.datetime.now()\n    datetime_string = timestamp.strftime('%Y%m%dT%H%M%S')\n\n    source_path = os.path.abspath(self._source_path)\n\n    if source_path.endswith(os.path.sep):\n      source_path = os.path.dirname(source_path)\n\n    source_name = os.path.basename(source_path)\n\n    if not source_name or source_name in ('/', '\\\\'):\n      # The user passed the filesystem's root as source\n      source_name = 'ROOT'\n\n    return '{0:s}-{1:s}.plaso'.format(datetime_string, source_name)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nanalyze events from a plaso storage file and generates a report.", "response": "def AnalyzeEvents(self):\n    \"\"\"Analyzes events from a plaso storage file and generate a report.\n\n    Raises:\n      BadConfigOption: when a configuration parameter fails validation.\n      RuntimeError: if a non-recoverable situation is encountered.\n    \"\"\"\n    session = engine.BaseEngine.CreateSession(\n        command_line_arguments=self._command_line_arguments,\n        preferred_encoding=self.preferred_encoding)\n\n    storage_reader = storage_factory.StorageFactory.CreateStorageReaderForFile(\n        self._storage_file_path)\n    if not storage_reader:\n      logger.error('Format of storage file: {0:s} not supported'.format(\n          self._storage_file_path))\n      return\n\n    self._number_of_analysis_reports = (\n        storage_reader.GetNumberOfAnalysisReports())\n    storage_reader.Close()\n\n    configuration = self._CreateProcessingConfiguration(\n        self._knowledge_base)\n\n    counter = collections.Counter()\n    if self._output_format != 'null':\n      self._status_view.SetMode(self._status_view_mode)\n      self._status_view.SetStorageFileInformation(self._storage_file_path)\n\n      status_update_callback = (\n          self._status_view.GetAnalysisStatusUpdateCallback())\n\n      storage_reader = (\n          storage_factory.StorageFactory.CreateStorageReaderForFile(\n              self._storage_file_path))\n\n      # TODO: add single processing support.\n      analysis_engine = psort.PsortMultiProcessEngine(\n          use_zeromq=self._use_zeromq)\n\n      analysis_engine.ExportEvents(\n          self._knowledge_base, storage_reader, self._output_module,\n          configuration, deduplicate_events=self._deduplicate_events,\n          status_update_callback=status_update_callback,\n          time_slice=self._time_slice, use_time_slicer=self._use_time_slicer)\n\n    for item, value in iter(session.analysis_reports_counter.items()):\n      counter[item] = value\n\n    if self._quiet_mode:\n      return\n\n    self._output_writer.Write('Processing completed.\\n')\n\n    table_view = views.ViewsFactory.GetTableView(\n        self._views_format_type, title='Counter')\n    for element, count in counter.most_common():\n      if not element:\n        element = 'N/A'\n      table_view.AddRow([element, count])\n    table_view.Write(self._output_writer)\n\n    storage_reader = storage_factory.StorageFactory.CreateStorageReaderForFile(\n        self._storage_file_path)\n    self._PrintAnalysisReportsDetails(\n        storage_reader, self._number_of_analysis_reports)\n\n    self._output_writer.Write('Storage file is {0:s}\\n'.format(\n        self._storage_file_path))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ParseArguments(self):\n    loggers.ConfigureLogging()\n\n    argument_parser = argparse.ArgumentParser(\n        description=self.DESCRIPTION, epilog=self.EPILOG, add_help=False,\n        formatter_class=argparse.RawDescriptionHelpFormatter)\n\n    self.AddBasicOptions(argument_parser)\n\n    extraction_group = argument_parser.add_argument_group(\n        'extraction arguments')\n\n    argument_helper_names = ['extraction']\n    helpers_manager.ArgumentHelperManager.AddCommandLineArguments(\n        extraction_group, names=argument_helper_names)\n\n    extraction_group.add_argument(\n        '--storage_file', '--storage-file', metavar='PATH', type=str,\n        default=None, help=(\n            'The path of the storage file. If not specified, one will be made '\n            'in the form <timestamp>-<source>.plaso'))\n\n    self.AddStorageMediaImageOptions(extraction_group)\n    self.AddCredentialOptions(extraction_group)\n\n    info_group = argument_parser.add_argument_group('informational arguments')\n\n    helpers_manager.ArgumentHelperManager.AddCommandLineArguments(\n        info_group, names=['status_view'])\n\n    input_group = argument_parser.add_argument_group('input arguments')\n    input_group.add_argument(\n        '--source', dest='source', action='store',\n        type=str, help='The source to process')\n\n    helpers_manager.ArgumentHelperManager.AddCommandLineArguments(\n        input_group, names=['data_location'])\n\n    output_group = argument_parser.add_argument_group('output arguments')\n\n    helpers_manager.ArgumentHelperManager.AddCommandLineArguments(\n        output_group, names=['language'])\n\n    self.AddTimeZoneOption(output_group)\n\n    output_format_group = argument_parser.add_argument_group(\n        'output format arguments')\n\n    helpers_manager.ArgumentHelperManager.AddCommandLineArguments(\n        output_format_group, names=['output_modules'])\n\n    processing_group = argument_parser.add_argument_group(\n        'processing arguments')\n\n    self.AddPerformanceOptions(processing_group)\n    self.AddProcessingOptions(processing_group)\n\n    try:\n      options = argument_parser.parse_args()\n    except UnicodeEncodeError:\n      # If we get here we are attempting to print help in a non-Unicode\n      # terminal.\n      self._output_writer.Write('\\n')\n      self._output_writer.Write(argument_parser.format_help())\n      return False\n\n    try:\n      self.ParseOptions(options)\n    except errors.BadConfigOption as exception:\n      self._output_writer.Write('ERROR: {0!s}\\n'.format(exception))\n      self._output_writer.Write('\\n')\n      self._output_writer.Write(argument_parser.format_usage())\n      return False\n\n    loggers.ConfigureLogging(\n        debug_output=self._debug_mode, filename=self._log_file,\n        quiet_mode=self._quiet_mode)\n\n    return True", "response": "Parses the command line arguments."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ParseOptions(self, options):\n    # The extraction options are dependent on the data location.\n    helpers_manager.ArgumentHelperManager.ParseOptions(\n        options, self, names=['data_location'])\n\n    self._ReadParserPresetsFromFile()\n\n    # The output modules options are dependent on the preferred language\n    # and preferred time zone options.\n    self._ParseTimezoneOption(options)\n\n    argument_helper_names = [\n        'artifact_definitions', 'hashers', 'language', 'parsers']\n    helpers_manager.ArgumentHelperManager.ParseOptions(\n        options, self, names=argument_helper_names)\n\n    self.list_hashers = self._hasher_names_string == 'list'\n    self.list_language_identifiers = self._preferred_language == 'list'\n    self.list_parsers_and_plugins = self._parser_filter_expression == 'list'\n\n    # Check the list options first otherwise required options will raise.\n    if (self.list_hashers or self.list_language_identifiers or\n        self.list_parsers_and_plugins or self.list_timezones):\n      return\n\n    # Check output modules after the other listable options, otherwise\n    # it could raise with \"requires an output file\".\n    helpers_manager.ArgumentHelperManager.ParseOptions(\n        options, self, names=['output_modules'])\n\n    self.list_output_modules = self._output_format == 'list'\n    if self.list_output_modules:\n      return\n\n    self._ParseInformationalOptions(options)\n\n    argument_helper_names = ['extraction', 'status_view']\n    helpers_manager.ArgumentHelperManager.ParseOptions(\n        options, self, names=argument_helper_names)\n\n    self._ParseLogFileOptions(options)\n\n    self._ParseStorageMediaOptions(options)\n\n    self._ParsePerformanceOptions(options)\n    self._ParseProcessingOptions(options)\n\n    self._storage_file_path = getattr(options, 'storage_file', None)\n    if not self._storage_file_path:\n      self._storage_file_path = self._GenerateStorageFileName()\n\n    self._output_filename = getattr(options, 'write', None)\n\n    if not self._output_filename:\n      raise errors.BadConfigOption((\n          'Output format: {0:s} requires an output file '\n          '(-w OUTPUT_FILE)').format(self._output_format))\n\n    if os.path.exists(self._output_filename):\n      raise errors.BadConfigOption(\n          'Output file already exists: {0:s}.'.format(self._output_filename))\n\n    self._EnforceProcessMemoryLimit(self._process_memory_limit)\n\n    self._output_module = self._CreateOutputModule(options)", "response": "Parses the options for the current language and time zone."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nformatting the description field.", "response": "def _FormatDescription(self, event):\n    \"\"\"Formats the description.\n\n    Args:\n      event (EventObject): event.\n\n    Returns:\n      str: formatted description field.\n    \"\"\"\n    date_time_string = timelib.Timestamp.CopyToIsoFormat(\n        event.timestamp, timezone=self._output_mediator.timezone)\n    timestamp_description = event.timestamp_desc or 'UNKNOWN'\n\n    message, _ = self._output_mediator.GetFormattedMessages(event)\n    if message is None:\n      data_type = getattr(event, 'data_type', 'UNKNOWN')\n      raise errors.NoFormatterFound(\n          'Unable to find event formatter for: {0:s}.'.format(data_type))\n\n    description = '{0:s}; {1:s}; {2:s}'.format(\n        date_time_string, timestamp_description,\n        message.replace(self._DESCRIPTION_FIELD_DELIMITER, ' '))\n    return self._SanitizeField(description)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nformat the hostname field.", "response": "def _FormatHostname(self, event):\n    \"\"\"Formats the hostname.\n\n    Args:\n      event (EventObject): event.\n\n     Returns:\n       str: formatted hostname field.\n    \"\"\"\n    hostname = self._output_mediator.GetHostname(event)\n    return self._SanitizeField(hostname)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nformats the source. Args: event (EventObject): event. Returns: str: formatted source field.", "response": "def _FormatSource(self, event):\n    \"\"\"Formats the source.\n\n    Args:\n      event (EventObject): event.\n\n     Returns:\n       str: formatted source field.\n    \"\"\"\n    source_short, _ = self._output_mediator.GetFormattedSources(event)\n    if source_short is None:\n      data_type = getattr(event, 'data_type', 'UNKNOWN')\n      raise errors.NoFormatterFound(\n          'Unable to find event formatter for: {0:s}.'.format(data_type))\n\n    return self._SanitizeField(source_short)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nformatting the username field.", "response": "def _FormatUsername(self, event):\n    \"\"\"Formats the username.\n\n    Args:\n      event (EventObject): event.\n\n     Returns:\n       str: formatted username field.\n    \"\"\"\n    username = self._output_mediator.GetUsername(event)\n    return self._SanitizeField(username)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _SanitizeField(self, field):\n    if self._FIELD_DELIMITER and isinstance(field, py2to3.STRING_TYPES):\n      return field.replace(self._FIELD_DELIMITER, ' ')\n    return field", "response": "Sanitize a field for output."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nformat the notes field.", "response": "def _FormatNotes(self, event):\n    \"\"\"Formats the notes.\n\n    Args:\n      event (EventObject): event.\n\n     Returns:\n       str: formatted notes field.\n    \"\"\"\n    inode = event.inode\n    if inode is None:\n      inode = '-'\n\n    notes = getattr(event, 'notes', '')\n    if not notes:\n      display_name = getattr(event, 'display_name', '')\n      notes = 'File: {0:s} inode: {1!s}'.format(display_name, inode)\n    return self._SanitizeField(notes)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwriting the event body to the output.", "response": "def WriteEventBody(self, event):\n    \"\"\"Writes the body of an event object to the output.\n\n    Args:\n      event (EventObject): event.\n    \"\"\"\n    if not hasattr(event, 'timestamp'):\n      return\n\n    # TODO: preserve dfdatetime as an object.\n    date_time = dfdatetime_posix_time.PosixTimeInMicroseconds(\n        timestamp=event.timestamp)\n    posix_timestamp = date_time.CopyToPosixTimestamp()\n    if not posix_timestamp:\n      posix_timestamp = 0\n\n    source = self._FormatSource(event)\n    hostname = self._FormatHostname(event)\n    username = self._FormatUsername(event)\n    description = self._FormatDescription(event)\n    notes = self._FormatNotes(event)\n\n    out_write = '{0:d}|{1:s}|{2:s}|{3:s}|{4:s}|{5!s}|{6!s}\\n'.format(\n        posix_timestamp, source, hostname, username, description,\n        self._output_mediator.timezone, notes)\n\n    self._output_writer.Write(out_write)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ParseSearchRow(self, parser_mediator, query, row, **unused_kwargs):\n    query_hash = hash(query)\n\n    event_data = TwitterAndroidSearchEventData()\n    event_data.query = query\n    event_data.name = self._GetRowValue(query_hash, row, 'name')\n    event_data.search_query = self._GetRowValue(query_hash, row, 'query')\n\n    timestamp = self._GetRowValue(query_hash, row, 'time')\n    if timestamp:\n      date_time = dfdatetime_java_time.JavaTime(timestamp=timestamp)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_CREATION)\n      parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Parses a search row from the database."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ParseStatusRow(self, parser_mediator, query, row, **unused_kwargs):\n    query_hash = hash(query)\n\n    event_data = TwitterAndroidStatusEventData()\n    event_data.query = query\n    event_data.identifier = self._GetRowValue(query_hash, row, '_id')\n    event_data.author_identifier = self._GetRowValue(\n        query_hash, row, 'author_id')\n    event_data.username = self._GetRowValue(query_hash, row, 'username')\n    event_data.content = self._GetRowValue(query_hash, row, 'content')\n    event_data.favorited = self._GetRowValue(query_hash, row, 'favorited')\n    event_data.retweeted = self._GetRowValue(query_hash, row, 'retweeted')\n\n    timestamp = self._GetRowValue(query_hash, row, 'time')\n    if timestamp:\n      date_time = dfdatetime_java_time.JavaTime(timestamp=timestamp)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_CREATION)\n      parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Parses a status row from the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ParseContactRow(self, parser_mediator, query, row, **unused_kwargs):\n    query_hash = hash(query)\n\n    event_data = TwitterAndroidContactEventData()\n    event_data.query = query\n    event_data.identifier = self._GetRowValue(query_hash, row, '_id')\n    event_data.user_identifier = self._GetRowValue(query_hash, row, 'user_id')\n    event_data.username = self._GetRowValue(query_hash, row, 'username')\n    event_data.name = self._GetRowValue(query_hash, row, 'name')\n    event_data.description = self._GetRowValue(query_hash, row, 'description')\n    event_data.web_url = self._GetRowValue(query_hash, row, 'web_url')\n    event_data.location = self._GetRowValue(query_hash, row, 'location')\n    event_data.followers = self._GetRowValue(query_hash, row, 'followers')\n    event_data.friends = self._GetRowValue(query_hash, row, 'friends')\n    event_data.statuses = self._GetRowValue(query_hash, row, 'statuses')\n    event_data.image_url = self._GetRowValue(query_hash, row, 'image_url')\n\n    timestamp = self._GetRowValue(query_hash, row, 'profile_created')\n    if timestamp:\n      date_time = dfdatetime_java_time.JavaTime(timestamp=timestamp)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_CREATION)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    timestamp = self._GetRowValue(query_hash, row, 'updated')\n    if timestamp:\n      date_time = dfdatetime_java_time.JavaTime(timestamp=timestamp)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_UPDATE)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    timestamp = self._GetRowValue(query_hash, row, 'friendship_time')\n    if timestamp:\n      date_time = dfdatetime_java_time.JavaTime(timestamp=timestamp)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_MODIFICATION)\n      parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Parses a contact row from the database."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting an attribute container object into a JSON dictionary.", "response": "def _ConvertAttributeContainerToDict(cls, attribute_container):\n    \"\"\"Converts an attribute container object into a JSON dictionary.\n\n    The resulting dictionary of the JSON serialized objects consists of:\n    {\n        '__type__': 'AttributeContainer'\n        '__container_type__': ...\n        ...\n    }\n\n    Here '__type__' indicates the object base type. In this case\n    'AttributeContainer'.\n\n    '__container_type__' indicates the container type and rest of the elements\n    of the dictionary make up the attributes of the container.\n\n    Args:\n      attribute_container (AttributeContainer): attribute container.\n\n    Returns:\n      dict[str, object]: JSON serialized objects.\n\n    Raises:\n      TypeError: if not an instance of AttributeContainer.\n      ValueError: if the attribute container type is not supported.\n    \"\"\"\n    if not isinstance(\n        attribute_container, containers_interface.AttributeContainer):\n      raise TypeError('{0:s} is not an attribute container type.'.format(\n          type(attribute_container)))\n\n    container_type = getattr(attribute_container, 'CONTAINER_TYPE', None)\n    if not container_type:\n      raise ValueError('Unsupported attribute container type: {0:s}.'.format(\n          type(attribute_container)))\n\n    json_dict = {\n        '__type__': 'AttributeContainer',\n        '__container_type__': container_type,\n    }\n\n    for attribute_name, attribute_value in attribute_container.GetAttributes():\n      json_dict[attribute_name] = cls._ConvertAttributeValueToDict(\n          attribute_value)\n\n    return json_dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _ConvertAttributeValueToDict(cls, attribute_value):\n    if isinstance(attribute_value, py2to3.BYTES_TYPE):\n      encoded_value = binascii.b2a_qp(attribute_value)\n      encoded_value = codecs.decode(encoded_value, 'ascii')\n      attribute_value = {\n          '__type__': 'bytes',\n          'stream': '{0:s}'.format(encoded_value)\n      }\n\n    elif isinstance(attribute_value, (list, tuple)):\n      json_list = []\n      for list_element in attribute_value:\n        json_dict = cls._ConvertAttributeValueToDict(list_element)\n        json_list.append(json_dict)\n\n      if isinstance(attribute_value, list):\n        attribute_value = json_list\n      else:\n        attribute_value = {\n            '__type__': 'tuple',\n            'values': json_list\n        }\n\n    elif isinstance(attribute_value, collections.Counter):\n      attribute_value = cls._ConvertCollectionsCounterToDict(attribute_value)\n\n    elif isinstance(attribute_value, dfvfs_path_spec.PathSpec):\n      attribute_value = cls._ConvertPathSpecToDict(attribute_value)\n\n    elif isinstance(attribute_value, containers_interface.AttributeContainer):\n      attribute_value = cls._ConvertAttributeContainerToDict(attribute_value)\n\n    return attribute_value", "response": "Converts an attribute value into a JSON dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert a collections. Counter object into a JSON dictionary.", "response": "def _ConvertCollectionsCounterToDict(cls, collections_counter):\n    \"\"\"Converts a collections.Counter object into a JSON dictionary.\n\n    The resulting dictionary of the JSON serialized objects consists of:\n    {\n        '__type__': 'collections.Counter'\n        ...\n    }\n\n    Here '__type__' indicates the object base type. In this case\n    'collections.Counter'. The rest of the elements of the dictionary make up\n    the collections.Counter object attributes.\n\n    Args:\n      collections_counter (collections.Counter): counter.\n\n    Returns:\n      dict[str, object]: JSON serialized objects.\n\n    Raises:\n      TypeError: if not an instance of collections.Counter.\n    \"\"\"\n    if not isinstance(collections_counter, collections.Counter):\n      raise TypeError\n\n    json_dict = {'__type__': 'collections.Counter'}\n    for attribute_name, attribute_value in iter(collections_counter.items()):\n      if attribute_value is None:\n        continue\n\n      if isinstance(attribute_value, py2to3.BYTES_TYPE):\n        attribute_value = {\n            '__type__': 'bytes',\n            'stream': '{0:s}'.format(binascii.b2a_qp(attribute_value))\n        }\n\n      json_dict[attribute_name] = attribute_value\n\n    return json_dict"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _ConvertDictToObject(cls, json_dict):\n    # Use __type__ to indicate the object class type.\n    class_type = json_dict.get('__type__', None)\n    if not class_type:\n      # Dealing with a regular dict.\n      return json_dict\n\n    if class_type == 'bytes':\n      return binascii.a2b_qp(json_dict['stream'])\n\n    if class_type == 'tuple':\n      return tuple(cls._ConvertListToObject(json_dict['values']))\n\n    if class_type == 'collections.Counter':\n      return cls._ConvertDictToCollectionsCounter(json_dict)\n\n    if class_type == 'AttributeContainer':\n      # Use __container_type__ to indicate the attribute container type.\n      container_type = json_dict.get('__container_type__', None)\n\n    # Since we would like the JSON as flat as possible we handle decoding\n    # a path specification.\n    elif class_type == 'PathSpec':\n      return cls._ConvertDictToPathSpec(json_dict)\n\n    else:\n      raise ValueError('Unsupported class type: {0:s}'.format(class_type))\n\n    container_class = (\n        containers_manager.AttributeContainersManager.GetAttributeContainer(\n            container_type))\n    if not container_class:\n      raise ValueError('Unsupported container type: {0:s}'.format(\n          container_type))\n\n    container_object = container_class()\n    supported_attribute_names = container_object.GetAttributeNames()\n    for attribute_name, attribute_value in iter(json_dict.items()):\n      # Be strict about which attributes to set in non event values.\n      if (container_type not in ('event', 'event_data') and\n          attribute_name not in supported_attribute_names):\n\n        if attribute_name not in ('__container_type__', '__type__'):\n          logger.debug((\n              '[ConvertDictToObject] unsupported attribute name: '\n              '{0:s}.{1:s}').format(container_type, attribute_name))\n\n        continue\n\n      if isinstance(attribute_value, dict):\n        attribute_value = cls._ConvertDictToObject(attribute_value)\n\n      elif isinstance(attribute_value, list):\n        attribute_value = cls._ConvertListToObject(attribute_value)\n\n      setattr(container_object, attribute_name, attribute_value)\n\n    return container_object", "response": "Converts a JSON dict into an object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _ConvertDictToCollectionsCounter(cls, json_dict):\n    collections_counter = collections.Counter()\n\n    for key, value in iter(json_dict.items()):\n      if key == '__type__':\n        continue\n      collections_counter[key] = value\n\n    return collections_counter", "response": "Converts a JSON dict into a collections. Counter."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting a JSON list into an object.", "response": "def _ConvertListToObject(cls, json_list):\n    \"\"\"Converts a JSON list into an object.\n\n    Args:\n      json_list (list[object]): JSON serialized objects.\n\n    Returns:\n      list[object]: a deserialized list.\n    \"\"\"\n    list_value = []\n    for json_list_element in json_list:\n      if isinstance(json_list_element, dict):\n        list_value.append(cls._ConvertDictToObject(json_list_element))\n\n      elif isinstance(json_list_element, list):\n        list_value.append(cls._ConvertListToObject(json_list_element))\n\n      else:\n        list_value.append(json_list_element)\n\n    return list_value"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _ConvertDictToPathSpec(cls, json_dict):\n    type_indicator = json_dict.get('type_indicator', None)\n    if type_indicator:\n      del json_dict['type_indicator']\n\n    if 'parent' in json_dict:\n      json_dict['parent'] = cls._ConvertDictToPathSpec(json_dict['parent'])\n\n    # Remove the class type from the JSON dict since we cannot pass it.\n    del json_dict['__type__']\n\n    return dfvfs_path_spec_factory.Factory.NewPathSpec(\n        type_indicator, **json_dict)", "response": "Converts a dictionary into a path specification object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _ConvertPathSpecToDict(cls, path_spec_object):\n    if not isinstance(path_spec_object, dfvfs_path_spec.PathSpec):\n      raise TypeError\n\n    json_dict = {'__type__': 'PathSpec'}\n    for property_name in dfvfs_path_spec_factory.Factory.PROPERTY_NAMES:\n      property_value = getattr(path_spec_object, property_name, None)\n      if property_value is not None:\n        json_dict[property_name] = property_value\n\n    if path_spec_object.HasParent():\n      json_dict['parent'] = cls._ConvertPathSpecToDict(path_spec_object.parent)\n\n    json_dict['type_indicator'] = path_spec_object.type_indicator\n    location = getattr(path_spec_object, 'location', None)\n    if location:\n      json_dict['location'] = location\n\n    return json_dict", "response": "Converts a path specification object into a JSON dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ReadSerialized(cls, json_string):  # pylint: disable=arguments-differ\n    if json_string:\n      json_dict = json.loads(json_string)\n      return cls.ReadSerializedDict(json_dict)\n\n    return None", "response": "Reads an attribute container from serialized form."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ReadSerializedDict(cls, json_dict):\n    if json_dict:\n      json_object = cls._ConvertDictToObject(json_dict)\n      if not isinstance(json_object, containers_interface.AttributeContainer):\n        raise TypeError('{0:s} is not an attribute container type.'.format(\n            type(json_object)))\n      return json_object\n\n    return None", "response": "Reads an attribute container from serialized dictionary form."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites an attribute container to a serialized form.", "response": "def WriteSerialized(cls, attribute_container):\n    \"\"\"Writes an attribute container to serialized form.\n\n    Args:\n      attribute_container (AttributeContainer): attribute container.\n\n    Returns:\n      str: A JSON string containing the serialized form.\n    \"\"\"\n    json_dict = cls.WriteSerializedDict(attribute_container)\n    return json.dumps(json_dict)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ParseOptions(cls, options, configuration_object):\n    if not isinstance(configuration_object, tools.CLITool):\n      raise errors.BadConfigObject(\n          'Configuration object is not an instance of CLITool')\n\n    process_memory_limit = cls._ParseNumericOption(\n        options, 'process_memory_limit')\n\n    if process_memory_limit and process_memory_limit < 0:\n      raise errors.BadConfigOption(\n          'Invalid process memory limit value cannot be negative.')\n\n    setattr(configuration_object, '_process_memory_limit', process_memory_limit)", "response": "Parses and validates options."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreports an event related error.", "response": "def _ReportEventError(self, event, error_message):\n    \"\"\"Reports an event related error.\n\n    Args:\n      event (EventObject): event.\n      error_message (str): error message.\n    \"\"\"\n    event_identifier = event.GetIdentifier()\n    event_identifier_string = event_identifier.CopyToString()\n    display_name = getattr(event, 'display_name', None) or 'N/A'\n    parser_chain = getattr(event, 'parser', None) or 'N/A'\n    error_message = (\n        'Event: {0!s} data type: {1:s} display name: {2:s} '\n        'parser chain: {3:s} with error: {4:s}').format(\n            event_identifier_string, event.data_type, display_name,\n            parser_chain, error_message)\n    logger.error(error_message)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwriting the event to the output.", "response": "def WriteEvent(self, event):\n    \"\"\"Writes the event to the output.\n\n    Args:\n      event (EventObject): event.\n    \"\"\"\n    self.WriteEventStart()\n\n    try:\n      self.WriteEventBody(event)\n\n    except errors.NoFormatterFound as exception:\n      error_message = 'unable to retrieve formatter with error: {0!s}'.format(\n          exception)\n      self._ReportEventError(event, error_message)\n\n    except errors.WrongFormatter as exception:\n      error_message = 'wrong formatter with error: {0!s}'.format(exception)\n      self._ReportEventError(event, error_message)\n\n    self.WriteEventEnd()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses a contact row from the database.", "response": "def ParseContactRow(self, parser_mediator, query, row, **unused_kwargs):\n    \"\"\"Parses a contact row from the database.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      query (str): query that created the row.\n      row (sqlite3.Row): row resulting from query.\n    \"\"\"\n    query_hash = hash(query)\n\n    event_data = TwitterIOSContactEventData()\n    event_data.description = self._GetRowValue(query_hash, row, 'description')\n    event_data.followers_count = self._GetRowValue(\n        query_hash, row, 'followersCount')\n    event_data.following = self._GetRowValue(query_hash, row, 'following')\n    event_data.following_count = self._GetRowValue(\n        query_hash, row, 'followingCount')\n    event_data.location = self._GetRowValue(query_hash, row, 'location')\n    event_data.name = self._GetRowValue(query_hash, row, 'name')\n    event_data.profile_url = self._GetRowValue(\n        query_hash, row, 'profileImageUrl')\n    event_data.query = query\n    event_data.screen_name = self._GetRowValue(query_hash, row, 'screenName')\n    event_data.url = self._GetRowValue(query_hash, row, 'url')\n\n    timestamp = self._GetRowValue(query_hash, row, 'createdDate')\n    if timestamp:\n      # Convert the floating point value to an integer.\n      timestamp = int(timestamp)\n      date_time = dfdatetime_posix_time.PosixTime(timestamp=timestamp)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_CREATION)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    timestamp = self._GetRowValue(query_hash, row, 'updatedAt')\n    if timestamp:\n      # Convert the floating point value to an integer.\n      timestamp = int(timestamp)\n      date_time = dfdatetime_posix_time.PosixTime(timestamp=timestamp)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_UPDATE)\n      parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses a status row from the database.", "response": "def ParseStatusRow(self, parser_mediator, query, row, **unused_kwargs):\n    \"\"\"Parses a contact row from the database.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      query (str): query that created the row.\n      row (sqlite3.Row): row resulting from query.\n    \"\"\"\n    query_hash = hash(query)\n\n    event_data = TwitterIOSStatusEventData()\n    event_data.favorite_count = self._GetRowValue(\n        query_hash, row, 'favoriteCount')\n    event_data.favorited = self._GetRowValue(query_hash, row, 'favorited')\n    event_data.name = self._GetRowValue(query_hash, row, 'name')\n    event_data.query = query\n    event_data.retweet_count = self._GetRowValue(\n        query_hash, row, 'retweetCount')\n    event_data.text = self._GetRowValue(query_hash, row, 'text')\n    event_data.user_id = self._GetRowValue(query_hash, row, 'user_id')\n\n    timestamp = self._GetRowValue(query_hash, row, 'date')\n    if timestamp:\n      # Convert the floating point value to an integer.\n      timestamp = int(timestamp)\n      date_time = dfdatetime_posix_time.PosixTime(timestamp=timestamp)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_CREATION)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    timestamp = self._GetRowValue(query_hash, row, 'updatedAt')\n    if timestamp:\n      # Convert the floating point value to an integer.\n      timestamp = int(timestamp)\n      date_time = dfdatetime_posix_time.PosixTime(timestamp=timestamp)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_UPDATE)\n      parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ParseRecord(self, parser_mediator, key, structure):\n    if key != 'line':\n      raise errors.ParseError(\n          'Unable to parse record, unknown structure: {0:s}'.format(key))\n\n    try:\n      date_time = dfdatetime_time_elements.TimeElements(\n          time_elements_tuple=structure.date_time)\n    except ValueError:\n      parser_mediator.ProduceExtractionWarning(\n          'invalid date time value: {0!s}'.format(structure.date_time))\n      return\n\n    body_text = structure.body\n    if not body_text:\n      parser_mediator.ProduceExtractionWarning(\n          'invalid body {0:s}'.format(structure.body))\n      return\n\n    event_data = DpkgEventData()\n    event_data.body = body_text\n\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_ADDED)\n    parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Parses a structure of tokens derived from a line of a text file.\n\n    Args:\n      parser_mediator (ParserMediator): parser mediator.\n      key (str): identifier of the structure of tokens.\n      structure (pyparsing.ParseResults): structure of tokens derived from\n          a line of a text file.\n\n    Raises:\n      ParseError: when the structure type is unknown."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef VerifyStructure(self, parser_mediator, line):\n    try:\n      structure = self._DPKG_LOG_LINE.parseString(line)\n    except pyparsing.ParseException as exception:\n      logger.debug(\n          'Unable to parse Debian dpkg.log file with error: {0!s}'.format(\n              exception))\n      return False\n\n    return 'date_time' in structure and 'body' in structure", "response": "Verifies if a line from a text file is in the expected format."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreads an operating system artifact from a dictionary.", "response": "def _ReadOperatingSystemArtifactValues(self, operating_system_values):\n    \"\"\"Reads an operating system artifact from a dictionary.\n\n    Args:\n      operating_system_values (dict[str, object]): operating system values.\n\n    Returns:\n      OperatingSystemArtifact: an operating system artifact attribute container.\n\n    Raises:\n      MalformedPresetError: if the format of the operating system values are\n          not set or incorrect.\n    \"\"\"\n    if not operating_system_values:\n      raise errors.MalformedPresetError('Missing operating system values.')\n\n    family = operating_system_values.get('family', None)\n    product = operating_system_values.get('product', None)\n    version = operating_system_values.get('version', None)\n\n    if not family and not product:\n      raise errors.MalformedPresetError(\n          'Invalid operating system missing family and product.')\n\n    return artifacts.OperatingSystemArtifact(\n        family=family, product=product, version=version)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _ReadParserPresetValues(self, preset_definition_values):\n    if not preset_definition_values:\n      raise errors.MalformedPresetError('Missing preset definition values.')\n\n    name = preset_definition_values.get('name', None)\n    if not name:\n      raise errors.MalformedPresetError(\n          'Invalid preset definition missing name.')\n\n    parsers = preset_definition_values.get('parsers', None)\n    if not parsers:\n      raise errors.MalformedPresetError(\n          'Invalid preset definition missing parsers.')\n\n    parser_preset = ParserPreset(name, parsers)\n\n    for operating_system_values in preset_definition_values.get(\n        'operating_systems', []):\n      operating_system = self._ReadOperatingSystemArtifactValues(\n          operating_system_values)\n      parser_preset.operating_systems.append(operating_system)\n\n    return parser_preset", "response": "Reads a parser preset from a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _ReadPresetsFromFileObject(self, file_object):\n    yaml_generator = yaml.safe_load_all(file_object)\n\n    last_preset_definition = None\n    for yaml_definition in yaml_generator:\n      try:\n        preset_definition = self._ReadParserPresetValues(yaml_definition)\n      except errors.MalformedPresetError as exception:\n        error_location = 'At start'\n        if last_preset_definition:\n          error_location = 'After: {0:s}'.format(last_preset_definition.name)\n\n        raise errors.MalformedPresetError(\n            '{0:s} {1!s}'.format(error_location, exception))\n\n      yield preset_definition\n      last_preset_definition = preset_definition", "response": "Reads parser and parser plugins from a file - like object."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nretrieves a specific parser preset by name.", "response": "def GetPresetByName(self, name):\n    \"\"\"Retrieves a specific preset definition by name.\n\n    Args:\n      name (str): name of the preset.\n\n    Returns:\n      ParserPreset: a parser preset or None if not available.\n    \"\"\"\n    name = name.lower()\n    return self._definitions.get(name, None)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef GetPresetsByOperatingSystem(self, operating_system):\n    preset_definitions = []\n    for preset_definition in self._definitions.values():\n      for preset_operating_system in preset_definition.operating_systems:\n        if preset_operating_system.IsEquivalent(operating_system):\n          preset_definitions.append(preset_definition)\n\n    return preset_definitions", "response": "Retrieves the list of presets that correspond to the given operating system."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ReadFromFile(self, path):\n    self._definitions = {}\n\n    with open(path, 'r') as file_object:\n      for preset_definition in self._ReadPresetsFromFileObject(file_object):\n        self._definitions[preset_definition.name] = preset_definition", "response": "Reads parser and parser plugin presets from a file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndetermining the formatted message strings for an event object.", "response": "def GetMessages(self, formatter_mediator, event):\n    \"\"\"Determines the formatted message strings for an event object.\n\n    Args:\n      formatter_mediator (FormatterMediator): mediates the interactions\n          between formatters and other components, such as storage and Windows\n          EventLog resources.\n      event (EventObject): event.\n\n    Returns:\n      tuple(str, str): formatted message string and short message string.\n\n    Raises:\n      WrongFormatter: if the event object cannot be formatted by the formatter.\n    \"\"\"\n    if self.DATA_TYPE != event.data_type:\n      raise errors.WrongFormatter('Unsupported data type: {0:s}.'.format(\n          event.data_type))\n\n    event_values = event.CopyToDict()\n\n    visit_type = event_values.get('visit_type', 0)\n    transition = self._URL_TRANSITIONS.get(visit_type, None)\n    if transition:\n      transition_str = 'Transition: {0!s}'.format(transition)\n\n    extra = event_values.get('extra', None)\n    if extra:\n      if transition:\n        extra.append(transition_str)\n      event_values['extra_string'] = ' '.join(extra)\n\n    elif transition:\n      event_values['extra_string'] = transition_str\n\n    return self._ConditionalFormatMessages(event_values)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding command line arguments that the helper supports to an argument group.", "response": "def AddArguments(cls, argument_group):\n    \"\"\"Adds command line arguments the helper supports to an argument group.\n\n    This function takes an argument parser or an argument group object and adds\n    to it all the command line arguments this helper supports.\n\n    Args:\n      argument_group (argparse._ArgumentGroup|argparse.ArgumentParser):\n          argparse group.\n    \"\"\"\n    argument_group.add_argument(\n        '--virustotal-api-key', '--virustotal_api_key',\n        dest='virustotal_api_key', type=str, action='store', default=None,\n        metavar='API_KEY', help=(\n            'Specify the API key for use with VirusTotal.'))\n\n    argument_group.add_argument(\n        '--virustotal-free-rate-limit', '--virustotal_free_rate_limit',\n        dest='virustotal_free_rate_limit',\n        action='store_false', default=cls._DEFAULT_RATE_LIMIT, help=(\n            'Limit Virustotal requests to the default free API key rate of '\n            '4 requests per minute. Set this to false if you have an key '\n            'for the private API.'))\n\n    argument_group.add_argument(\n        '--virustotal-hash', '--virustotal_hash', dest='virustotal_hash',\n        type=str, action='store', choices=['md5', 'sha1', 'sha256'],\n        default=cls._DEFAULT_HASH, metavar='HASH', help=(\n            'Type of hash to query VirusTotal, the default is: {0:s}'.format(\n                cls._DEFAULT_HASH)))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ParseOptions(cls, options, analysis_plugin):\n    if not isinstance(analysis_plugin, virustotal.VirusTotalAnalysisPlugin):\n      raise errors.BadConfigObject(\n          'Analysis plugin is not an instance of VirusTotalAnalysisPlugin')\n\n    api_key = cls._ParseStringOption(options, 'virustotal_api_key')\n    if not api_key:\n      raise errors.BadConfigOption(\n          'VirusTotal API key not specified. Try again with '\n          '--virustotal-api-key.')\n\n    analysis_plugin.SetAPIKey(api_key)\n\n    enable_rate_limit = getattr(\n        options, 'virustotal_free_rate_limit', cls._DEFAULT_RATE_LIMIT)\n    if enable_rate_limit:\n      analysis_plugin.EnableFreeAPIKeyRateLimit()\n\n    lookup_hash = cls._ParseStringOption(\n        options, 'virustotal_hash', default_value=cls._DEFAULT_HASH)\n    analysis_plugin.SetLookupHash(lookup_hash)\n\n    if not analysis_plugin.TestConnection():\n      raise errors.BadConfigOption('Unable to connect to VirusTotal')", "response": "Parses and validates options."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _GetStatus(self):\n    if self._parser_mediator:\n      number_of_produced_events = (\n          self._parser_mediator.number_of_produced_events)\n      number_of_produced_sources = (\n          self._parser_mediator.number_of_produced_event_sources)\n      number_of_produced_warnings = (\n          self._parser_mediator.number_of_produced_warnings)\n    else:\n      number_of_produced_events = None\n      number_of_produced_sources = None\n      number_of_produced_warnings = None\n\n    if self._extraction_worker and self._parser_mediator:\n      last_activity_timestamp = max(\n          self._extraction_worker.last_activity_timestamp,\n          self._parser_mediator.last_activity_timestamp)\n      processing_status = self._extraction_worker.processing_status\n    else:\n      last_activity_timestamp = 0.0\n      processing_status = self._status\n\n    task_identifier = getattr(self._task, 'identifier', '')\n\n    if self._process_information:\n      used_memory = self._process_information.GetUsedMemory() or 0\n    else:\n      used_memory = 0\n\n    if self._memory_profiler:\n      self._memory_profiler.Sample('main', used_memory)\n\n    # XML RPC does not support integer values > 2 GiB so we format them\n    # as a string.\n    used_memory = '{0:d}'.format(used_memory)\n\n    status = {\n        'display_name': self._current_display_name,\n        'identifier': self._name,\n        'last_activity_timestamp': last_activity_timestamp,\n        'number_of_consumed_event_tags': None,\n        'number_of_consumed_events': self._number_of_consumed_events,\n        'number_of_consumed_sources': self._number_of_consumed_sources,\n        'number_of_consumed_warnings': None,\n        'number_of_produced_event_tags': None,\n        'number_of_produced_events': number_of_produced_events,\n        'number_of_produced_sources': number_of_produced_sources,\n        'number_of_produced_warnings': number_of_produced_warnings,\n        'processing_status': processing_status,\n        'task_identifier': task_identifier,\n        'used_memory': used_memory}\n\n    return status", "response": "Retrieves the status of the current process."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprocess a path specification.", "response": "def _ProcessPathSpec(self, extraction_worker, parser_mediator, path_spec):\n    \"\"\"Processes a path specification.\n\n    Args:\n      extraction_worker (worker.ExtractionWorker): extraction worker.\n      parser_mediator (ParserMediator): parser mediator.\n      path_spec (dfvfs.PathSpec): path specification.\n    \"\"\"\n    self._current_display_name = parser_mediator.GetDisplayNameForPathSpec(\n        path_spec)\n\n    try:\n      extraction_worker.ProcessPathSpec(parser_mediator, path_spec)\n\n    except dfvfs_errors.CacheFullError:\n      # TODO: signal engine of failure.\n      self._abort = True\n      logger.error((\n          'ABORT: detected cache full error while processing path spec: '\n          '{0:s}').format(self._current_display_name))\n\n    except Exception as exception:  # pylint: disable=broad-except\n      parser_mediator.ProduceExtractionWarning((\n          'unable to process path specification with error: '\n          '{0!s}').format(exception), path_spec=path_spec)\n\n      if self._processing_configuration.debug_output:\n        logger.warning((\n            'Unhandled exception while processing path specification: '\n            '{0:s}.').format(self._current_display_name))\n        logger.exception(exception)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _ProcessTask(self, task):\n    logger.debug('Started processing task: {0:s}.'.format(task.identifier))\n\n    if self._tasks_profiler:\n      self._tasks_profiler.Sample(task, 'processing_started')\n\n    self._task = task\n\n    storage_writer = self._storage_writer.CreateTaskStorage(task)\n\n    if self._serializers_profiler:\n      storage_writer.SetSerializersProfiler(self._serializers_profiler)\n\n    storage_writer.Open()\n\n    self._parser_mediator.SetStorageWriter(storage_writer)\n\n    storage_writer.WriteTaskStart()\n\n    try:\n      # TODO: add support for more task types.\n      self._ProcessPathSpec(\n          self._extraction_worker, self._parser_mediator, task.path_spec)\n      self._number_of_consumed_sources += 1\n\n      if self._guppy_memory_profiler:\n        self._guppy_memory_profiler.Sample()\n\n    finally:\n      storage_writer.WriteTaskCompletion(aborted=self._abort)\n\n      self._parser_mediator.SetStorageWriter(None)\n\n      storage_writer.Close()\n\n    try:\n      self._storage_writer.FinalizeTaskStorage(task)\n    except IOError:\n      pass\n\n    self._task = None\n\n    if self._tasks_profiler:\n      self._tasks_profiler.Sample(task, 'processing_completed')\n\n    logger.debug('Completed processing task: {0:s}.'.format(task.identifier))", "response": "Processes a task.\n\n    Args:\n      task (Task): task."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsignals the process to abort.", "response": "def SignalAbort(self):\n    \"\"\"Signals the process to abort.\"\"\"\n    self._abort = True\n    if self._extraction_worker:\n      self._extraction_worker.SignalAbort()\n    if self._parser_mediator:\n      self._parser_mediator.SignalAbort()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef AddArguments(cls, argument_group):\n    argument_group.add_argument(\n        '--preferred_year', '--preferred-year', dest='preferred_year',\n        type=int, action='store', default=None, metavar='YEAR', help=(\n            'When a format\\'s timestamp does not include a year, e.g. '\n            'syslog, use this as the initial year instead of attempting '\n            'auto-detection.'))\n\n    argument_group.add_argument(\n        '--process_archives', '--process-archives', dest='process_archives',\n        action='store_true', default=False, help=(\n            'Process file entries embedded within archive files, such as '\n            'archive.tar and archive.zip. This can make processing '\n            'significantly slower.'))\n\n    argument_group.add_argument(\n        '--skip_compressed_streams', '--skip-compressed-streams',\n        dest='process_compressed_streams', action='store_false', default=True,\n        help=(\n            'Skip processing file content within compressed streams, such as '\n            'syslog.gz and syslog.bz2.'))", "response": "Adds command line arguments to an argument group."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ParseOptions(cls, options, configuration_object):\n    if not isinstance(configuration_object, tools.CLITool):\n      raise errors.BadConfigObject(\n          'Configuration object is not an instance of CLITool')\n\n    preferred_year = cls._ParseNumericOption(options, 'preferred_year')\n\n    process_archives = getattr(options, 'process_archives', False)\n    process_compressed_streams = getattr(\n        options, 'process_compressed_streams', True)\n\n    setattr(configuration_object, '_preferred_year', preferred_year)\n    setattr(configuration_object, '_process_archives', process_archives)\n    setattr(\n        configuration_object, '_process_compressed_streams',\n        process_compressed_streams)", "response": "Parses and validates the options."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _GetNameFromProduct(self):\n    product = self.product or ''\n    product = product.split(' ')\n    product_lower_case = [segment.lower() for segment in product]\n    number_of_segments = len(product)\n\n    if 'windows' in product_lower_case:\n      segment_index = product_lower_case.index('windows') + 1\n      if product_lower_case[segment_index] in ('(r)', 'server'):\n        segment_index += 1\n\n      # Check if the version has a suffix.\n      suffix_segment_index = segment_index + 1\n      if (suffix_segment_index < number_of_segments and\n          product_lower_case[suffix_segment_index] == 'r2'):\n        return 'Windows {0:s} R2'.format(product[segment_index])\n\n      return 'Windows {0:s}'.format(product[segment_index])\n\n    return None", "response": "Determines the predefined operating system name from the product."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndetermines if two operating system artifacts are equivalent.", "response": "def IsEquivalent(self, other):\n    \"\"\"Determines if 2 operating system artifacts are equivalent.\n\n    This function compares the operating systems based in order of:\n    * name derived from product\n    * family and version\n    * family\n\n    Args:\n      other (OperatingSystemArtifact): operating system artifact attribute\n          container to compare with.\n\n    Returns:\n      bool: True if the operating systems are considered equivalent, False if\n          the most specific criteria do no match, or no criteria are available.\n    \"\"\"\n    if self.name and other.name:\n      return self.name == other.name\n\n    if self.name:\n      self_family, self_version_tuple = self._FAMILY_AND_VERSION_PER_NAME.get(\n          self.name, self._DEFAULT_FAMILY_AND_VERSION)\n      return (\n          self_family == other.family and\n          self_version_tuple == other.version_tuple)\n\n    if self.family and self.version:\n      if other.name:\n        other_family, other_version_tuple = (\n            self._FAMILY_AND_VERSION_PER_NAME.get(\n                other.name, self._DEFAULT_FAMILY_AND_VERSION))\n      else:\n        other_family = other.family\n        other_version_tuple = other.version_tuple\n\n      return (\n          self.family == other_family and\n          self.version_tuple == other_version_tuple)\n\n    if self.family:\n      if other.name:\n        other_family, _ = self._FAMILY_AND_VERSION_PER_NAME.get(\n            other.name, self._DEFAULT_FAMILY_AND_VERSION)\n      else:\n        other_family = other.family\n\n      return self.family == other_family\n\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _DetermineOperatingSystem(self, searcher):\n    find_specs = [\n        file_system_searcher.FindSpec(\n            location='/etc', case_sensitive=False),\n        file_system_searcher.FindSpec(\n            location='/System/Library', case_sensitive=False),\n        file_system_searcher.FindSpec(\n            location='/Windows/System32', case_sensitive=False),\n        file_system_searcher.FindSpec(\n            location='/WINNT/System32', case_sensitive=False),\n        file_system_searcher.FindSpec(\n            location='/WINNT35/System32', case_sensitive=False),\n        file_system_searcher.FindSpec(\n            location='/WTSRV/System32', case_sensitive=False)]\n\n    locations = []\n    for path_spec in searcher.Find(find_specs=find_specs):\n      relative_path = searcher.GetRelativePath(path_spec)\n      if relative_path:\n        locations.append(relative_path.lower())\n\n    # We need to check for both forward and backward slashes since the path\n    # spec will be OS dependent, as in running the tool on Windows will return\n    # Windows paths (backward slash) vs. forward slash on *NIX systems.\n    windows_locations = set([\n        '/windows/system32', '\\\\windows\\\\system32', '/winnt/system32',\n        '\\\\winnt\\\\system32', '/winnt35/system32', '\\\\winnt35\\\\system32',\n        '\\\\wtsrv\\\\system32', '/wtsrv/system32'])\n\n    operating_system = definitions.OPERATING_SYSTEM_FAMILY_UNKNOWN\n    if windows_locations.intersection(set(locations)):\n      operating_system = definitions.OPERATING_SYSTEM_FAMILY_WINDOWS_NT\n\n    elif '/system/library' in locations:\n      operating_system = definitions.OPERATING_SYSTEM_FAMILY_MACOS\n\n    elif '/etc' in locations:\n      operating_system = definitions.OPERATING_SYSTEM_FAMILY_LINUX\n\n    return operating_system", "response": "Tries to determine the underlying operating system."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstarting profiling. Args: configuration (ProfilingConfiguration): profiling configuration.", "response": "def _StartProfiling(self, configuration):\n    \"\"\"Starts profiling.\n\n    Args:\n      configuration (ProfilingConfiguration): profiling configuration.\n    \"\"\"\n    if not configuration:\n      return\n\n    if configuration.HaveProfileMemoryGuppy():\n      self._guppy_memory_profiler = profilers.GuppyMemoryProfiler(\n          self._name, configuration)\n      self._guppy_memory_profiler.Start()\n\n    if configuration.HaveProfileMemory():\n      self._memory_profiler = profilers.MemoryProfiler(\n          self._name, configuration)\n      self._memory_profiler.Start()\n\n    if configuration.HaveProfileProcessing():\n      identifier = '{0:s}-processing'.format(self._name)\n      self._processing_profiler = profilers.ProcessingProfiler(\n          identifier, configuration)\n      self._processing_profiler.Start()\n\n    if configuration.HaveProfileSerializers():\n      identifier = '{0:s}-serializers'.format(self._name)\n      self._serializers_profiler = profilers.SerializersProfiler(\n          identifier, configuration)\n      self._serializers_profiler.Start()\n\n    if configuration.HaveProfileStorage():\n      self._storage_profiler = profilers.StorageProfiler(\n          self._name, configuration)\n      self._storage_profiler.Start()\n\n    if configuration.HaveProfileTaskQueue():\n      self._task_queue_profiler = profilers.TaskQueueProfiler(\n          self._name, configuration)\n      self._task_queue_profiler.Start()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef CreateSession(\n      cls, artifact_filter_names=None, command_line_arguments=None,\n      debug_mode=False, filter_file_path=None, preferred_encoding='utf-8',\n      preferred_time_zone=None, preferred_year=None):\n    \"\"\"Creates a session attribute container.\n\n    Args:\n      artifact_filter_names (Optional[list[str]]): names of artifact definitions\n          that are used for filtering file system and Windows Registry\n          key paths.\n      command_line_arguments (Optional[str]): the command line arguments.\n      debug_mode (bool): True if debug mode was enabled.\n      filter_file_path (Optional[str]): path to a file with find specifications.\n      preferred_encoding (Optional[str]): preferred encoding.\n      preferred_time_zone (Optional[str]): preferred time zone.\n      preferred_year (Optional[int]): preferred year.\n\n    Returns:\n      Session: session attribute container.\n    \"\"\"\n    session = sessions.Session()\n\n    session.artifact_filters = artifact_filter_names\n    session.command_line_arguments = command_line_arguments\n    session.debug_mode = debug_mode\n    session.filter_file = filter_file_path\n    session.preferred_encoding = preferred_encoding\n    session.preferred_time_zone = preferred_time_zone\n    session.preferred_year = preferred_year\n\n    return session", "response": "Creates a session attribute container."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef PreprocessSources(\n      self, artifacts_registry_object, source_path_specs,\n      resolver_context=None):\n    \"\"\"Preprocesses the sources.\n\n    Args:\n      artifacts_registry_object (artifacts.ArtifactDefinitionsRegistry):\n          artifact definitions registry.\n      source_path_specs (list[dfvfs.PathSpec]): path specifications of\n          the sources to process.\n      resolver_context (Optional[dfvfs.Context]): resolver context.\n    \"\"\"\n    detected_operating_systems = []\n    for source_path_spec in source_path_specs:\n      try:\n        file_system, mount_point = self.GetSourceFileSystem(\n            source_path_spec, resolver_context=resolver_context)\n      except (RuntimeError, dfvfs_errors.BackEndError) as exception:\n        logger.error(exception)\n        continue\n\n      try:\n        searcher = file_system_searcher.FileSystemSearcher(\n            file_system, mount_point)\n\n        operating_system = self._DetermineOperatingSystem(searcher)\n        if operating_system != definitions.OPERATING_SYSTEM_FAMILY_UNKNOWN:\n          preprocess_manager.PreprocessPluginsManager.RunPlugins(\n              artifacts_registry_object, file_system, mount_point,\n              self.knowledge_base)\n\n          detected_operating_systems.append(operating_system)\n\n      finally:\n        file_system.Close()\n\n    if detected_operating_systems:\n      logger.info('Preprocessing detected operating systems: {0:s}'.format(\n          ', '.join(detected_operating_systems)))\n      self.knowledge_base.SetValue(\n          'operating_system', detected_operating_systems[0])", "response": "Preprocesses the sources.\n\n    Args:\n      artifacts_registry_object (artifacts.ArtifactDefinitionsRegistry):\n          artifact definitions registry.\n      source_path_specs (list[dfvfs.PathSpec]): path specifications of\n          the sources to process.\n      resolver_context (Optional[dfvfs.Context]): resolver context."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef BuildFilterFindSpecs(\n      self, artifact_definitions_path, custom_artifacts_path,\n      knowledge_base_object, artifact_filter_names=None, filter_file_path=None):\n    \"\"\"Builds find specifications from artifacts or filter file if available.\n\n    Args:\n      artifact_definitions_path (str): path to artifact definitions file.\n      custom_artifacts_path (str): path to custom artifact definitions file.\n      knowledge_base_object (KnowledgeBase): knowledge base.\n      artifact_filter_names (Optional[list[str]]): names of artifact\n          definitions that are used for filtering file system and Windows\n          Registry key paths.\n      filter_file_path (Optional[str]): path of filter file.\n\n    Returns:\n      list[dfvfs.FindSpec]: find specifications for the file source type.\n\n    Raises:\n      InvalidFilter: if no valid FindSpecs are built.\n    \"\"\"\n    environment_variables = knowledge_base_object.GetEnvironmentVariables()\n    find_specs = None\n    if artifact_filter_names:\n      logger.debug(\n          'building find specification based on artifacts: {0:s}'.format(\n              ', '.join(artifact_filter_names)))\n\n      artifacts_registry_object = BaseEngine.BuildArtifactsRegistry(\n          artifact_definitions_path, custom_artifacts_path)\n      self._artifacts_filter_helper = (\n          artifact_filters.ArtifactDefinitionsFilterHelper(\n              artifacts_registry_object, knowledge_base_object))\n      self._artifacts_filter_helper.BuildFindSpecs(\n          artifact_filter_names, environment_variables=environment_variables)\n\n      # If the user selected Windows Registry artifacts we have to ensure\n      # the Windows Registry files are parsed.\n      if self._artifacts_filter_helper.registry_find_specs:\n        self._artifacts_filter_helper.BuildFindSpecs(\n            self._WINDOWS_REGISTRY_FILES_ARTIFACT_NAMES,\n            environment_variables=environment_variables)\n\n      find_specs = self._artifacts_filter_helper.file_system_find_specs\n\n      if not find_specs:\n        raise errors.InvalidFilter(\n            'No valid file system find specifications were built from '\n            'artifacts.')\n\n    elif filter_file_path:\n      logger.debug(\n          'building find specification based on filter file: {0:s}'.format(\n              filter_file_path))\n\n      filter_file_object = filter_file.FilterFile(filter_file_path)\n      find_specs = filter_file_object.BuildFindSpecs(\n          environment_variables=environment_variables)\n\n      if not find_specs:\n        raise errors.InvalidFilter(\n            'No valid file system find specifications were built from filter '\n            'file.')\n\n    return find_specs", "response": "Builds find specifications from artifacts or filter file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef BuildArtifactsRegistry(\n      cls, artifact_definitions_path, custom_artifacts_path):\n    \"\"\"Build Find Specs from artifacts or filter file if available.\n\n    Args:\n       artifact_definitions_path (str): path to artifact definitions file.\n       custom_artifacts_path (str): path to custom artifact definitions file.\n\n    Returns:\n      artifacts.ArtifactDefinitionsRegistry: artifact definitions registry.\n\n    Raises:\n      RuntimeError: if no valid FindSpecs are built.\n    \"\"\"\n    if artifact_definitions_path and not os.path.isdir(\n        artifact_definitions_path):\n      raise errors.BadConfigOption(\n          'No such artifacts filter file: {0:s}.'.format(\n              artifact_definitions_path))\n\n    if custom_artifacts_path and not os.path.isfile(custom_artifacts_path):\n      raise errors.BadConfigOption(\n          'No such artifacts filter file: {0:s}.'.format(custom_artifacts_path))\n\n    registry = artifacts_registry.ArtifactDefinitionsRegistry()\n    reader = artifacts_reader.YamlArtifactsReader()\n\n    try:\n      registry.ReadFromDirectory(reader, artifact_definitions_path)\n\n    except (KeyError, artifacts_errors.FormatError) as exception:\n      raise errors.BadConfigOption((\n          'Unable to read artifact definitions from: {0:s} with error: '\n          '{1!s}').format(artifact_definitions_path, exception))\n\n    if custom_artifacts_path:\n      try:\n        registry.ReadFromFile(reader, custom_artifacts_path)\n\n      except (KeyError, artifacts_errors.FormatError) as exception:\n        raise errors.BadConfigOption((\n            'Unable to read artifact definitions from: {0:s} with error: '\n            '{1!s}').format(custom_artifacts_path, exception))\n\n    return registry", "response": "Builds the artifact definitions registry from artifacts or filter file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _GetTimestamps(self, olecf_item):\n    if not olecf_item:\n      return None, None\n\n    try:\n      creation_time = olecf_item.get_creation_time_as_integer()\n    except OverflowError as exception:\n      logger.warning(\n          'Unable to read the creation time with error: {0!s}'.format(\n              exception))\n      creation_time = 0\n\n    try:\n      modification_time = olecf_item.get_modification_time_as_integer()\n    except OverflowError as exception:\n      logger.warning(\n          'Unable to read the modification time with error: {0!s}'.format(\n              exception))\n      modification_time = 0\n\n    # If no useful events, return early.\n    if not creation_time and not modification_time:\n      return None, None\n\n    # Office template documents sometimes contain a creation time\n    # of -1 (0xffffffffffffffff).\n    if creation_time == 0xffffffffffffffff:\n      creation_time = 0\n\n    return creation_time, modification_time", "response": "Retrieves the timestamps from an OLECF item."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef GetHasherNamesFromString(cls, hasher_names_string):\n    hasher_names = []\n\n    if not hasher_names_string or hasher_names_string.strip() == 'none':\n      return hasher_names\n\n    if hasher_names_string.strip() == 'all':\n      return cls.GetHasherNames()\n\n    for hasher_name in hasher_names_string.split(','):\n      hasher_name = hasher_name.strip()\n      if not hasher_name:\n        continue\n\n      hasher_name = hasher_name.lower()\n      if hasher_name in cls._hasher_classes:\n        hasher_names.append(hasher_name)\n\n    return hasher_names", "response": "Retrieves a list of a hasher names from a comma separated string."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef GetHashersInformation(cls):\n    hashers_information = []\n    for _, hasher_class in cls.GetHasherClasses():\n      description = getattr(hasher_class, 'DESCRIPTION', '')\n      hashers_information.append((hasher_class.NAME, description))\n\n    return hashers_information", "response": "Retrieves the hashers information."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef GetHasher(cls, hasher_name):\n    hasher_name = hasher_name.lower()\n    if hasher_name not in cls._hasher_classes:\n      raise KeyError(\n          'hasher class not set for name: {0:s}.'.format(hasher_name))\n\n    hasher_class = cls._hasher_classes[hasher_name]\n    return hasher_class()", "response": "Retrieves an instance of a specific hasher."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving instances for all the specified hashers.", "response": "def GetHashers(cls, hasher_names):\n    \"\"\"Retrieves instances for all the specified hashers.\n\n    Args:\n      hasher_names (list[str]): names of the hashers to retrieve.\n\n    Returns:\n      list[BaseHasher]: hashers.\n    \"\"\"\n    hashers = []\n    for hasher_name, hasher_class in iter(cls._hasher_classes.items()):\n      if hasher_name in hasher_names:\n        hashers.append(hasher_class())\n\n    return hashers"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving the registered hashers.", "response": "def GetHasherClasses(cls, hasher_names=None):\n    \"\"\"Retrieves the registered hashers.\n\n    Args:\n      hasher_names (list[str]): names of the hashers to retrieve.\n\n    Yields:\n        tuple: containing:\n\n         str: parser name\n         type: next hasher class.\n    \"\"\"\n    for hasher_name, hasher_class in iter(cls._hasher_classes.items()):\n      if not hasher_names or hasher_name in hasher_names:\n        yield hasher_name, hasher_class"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef RegisterHasher(cls, hasher_class):\n    hasher_name = hasher_class.NAME.lower()\n    if hasher_name in cls._hasher_classes:\n      raise KeyError((\n          'hasher class already set for name: {0:s}.').format(\n              hasher_class.NAME))\n\n    cls._hasher_classes[hasher_name] = hasher_class", "response": "Registers a hasher class."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing and validates the options.", "response": "def ParseOptions(cls, options, configuration_object):\n    \"\"\"Parses and validates options.\n\n    Args:\n      options (argparse.Namespace): parser options.\n      configuration_object (CLITool): object to be configured by the argument\n          helper.\n\n    Raises:\n      BadConfigObject: when the configuration object is of the wrong type.\n      BadConfigOption: if the collection file does not exist.\n    \"\"\"\n    if not isinstance(configuration_object, tools.CLITool):\n      raise errors.BadConfigObject(\n          'Configuration object is not an instance of CLITool')\n\n    filter_file = cls._ParseStringOption(options, 'file_filter')\n\n    # Search the data location for the filter file.\n    if filter_file and not os.path.isfile(filter_file):\n      data_location = getattr(configuration_object, '_data_location', None)\n      if data_location:\n        filter_file_basename = os.path.basename(filter_file)\n        filter_file_path = os.path.join(data_location, filter_file_basename)\n        if os.path.isfile(filter_file_path):\n          filter_file = filter_file_path\n\n    if filter_file and not os.path.isfile(filter_file):\n      raise errors.BadConfigOption(\n          'No such collection filter file: {0:s}.'.format(filter_file))\n\n    setattr(configuration_object, '_filter_file', filter_file)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _GetISO8601String(self, structure):\n    fraction_of_second_length = len(structure.fraction_of_second)\n    if fraction_of_second_length not in (3, 6, 7):\n      raise ValueError(\n          'unsupported time fraction of second length: {0:d}'.format(\n              fraction_of_second_length))\n\n    try:\n      fraction_of_second = int(structure.fraction_of_second, 10)\n    except (TypeError, ValueError) as exception:\n      raise ValueError(\n          'unable to determine fraction of second with error: {0!s}'.format(\n              exception))\n\n    # TODO: improve precision support, but for now ignore the 100ns precision.\n    if fraction_of_second_length == 7:\n      fraction_of_second, _ = divmod(fraction_of_second, 10)\n\n    date_time_string = '{0:04d}-{1:02d}-{2:02d}T{3:02d}:{4:02d}:{5:02d}'.format(\n        structure.year, structure.month, structure.day, structure.hour,\n        structure.minute, structure.second)\n\n    if fraction_of_second_length > 0:\n      date_time_string = '{0:s}.{1:d}'.format(\n          date_time_string, fraction_of_second)\n\n    utc_offset_minutes = structure.get('utc_offset_minutes', None)\n    if utc_offset_minutes is not None:\n      try:\n        time_zone_offset = int(utc_offset_minutes[1:], 10)\n      except (IndexError, ValueError) as exception:\n        raise ValueError(\n            'Unable to parse time zone offset with error: {0!s}.'.format(\n                exception))\n\n      time_zone_hours, time_zone_minutes = divmod(time_zone_offset, 60)\n      date_time_string = '{0:s}{1:s}{2:02d}:{3:02d}'.format(\n          date_time_string, utc_offset_minutes[0], time_zone_hours,\n          time_zone_minutes)\n\n    return date_time_string", "response": "Retrieves an ISO 8601 date time string from the structure."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ParseRecord(self, parser_mediator, key, structure):\n    if key not in (\n        'log_entry', 'log_entry_at_end', 'log_entry_offset',\n        'log_entry_offset_at_end'):\n      raise errors.ParseError(\n          'Unable to parse record, unknown structure: {0:s}'.format(key))\n\n    try:\n      date_time_string = self._GetISO8601String(structure)\n    except ValueError as exception:\n      parser_mediator.ProduceExtractionWarning(\n          'unable to determine date time string with error: {0!s}'.format(\n              exception))\n\n    fraction_of_second_length = len(structure.fraction_of_second)\n    if fraction_of_second_length == 3:\n      date_time = dfdatetime_time_elements.TimeElementsInMilliseconds()\n    elif fraction_of_second_length in (6, 7):\n      date_time = dfdatetime_time_elements.TimeElementsInMicroseconds()\n\n    try:\n      date_time.CopyFromStringISO8601(date_time_string)\n    except ValueError as exception:\n      parser_mediator.ProduceExtractionWarning(\n          'unable to parse date time value: {0:s} with error: {1!s}'.format(\n              date_time_string, exception))\n      return\n\n    event_data = SCCMLogEventData()\n    event_data.component = structure.component\n    # TODO: pass line number to offset or remove.\n    event_data.offset = 0\n    event_data.text = structure.text\n\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_WRITTEN)\n    parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Parses a record and returns an SCCM log event object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nverifying whether content corresponds to an SCCM log file.", "response": "def VerifyStructure(self, parser_mediator, lines):\n    \"\"\"Verifies whether content corresponds to an SCCM log file.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      lines (str): one or more lines from the text file.\n\n    Returns:\n      bool: True if this is the correct parser, False otherwise.\n    \"\"\"\n    # Identify the token to which we attempt a match.\n    match = self._PARSING_COMPONENTS['msg_left_delimiter'].match\n\n    # Because logs files can lead with a partial event,\n    # we can't assume that the first character (post-BOM)\n    # in the file is the beginning of our match - so we\n    # look for match anywhere in lines.\n    return match in lines"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfetching the next token by trying to match any of the regexes in order.", "response": "def NextToken(self):\n    \"\"\"Fetch the next token by trying to match any of the regexes in order.\"\"\"\n    current_state = self.state\n    for token in self.tokens:\n      # Does the rule apply to us?\n      if not token.state_regex.match(current_state):\n        continue\n\n      # Try to match the rule\n      m = token.regex.match(self.buffer)\n      if not m:\n        continue\n\n      # The match consumes the data off the buffer (the handler can put it back\n      # if it likes)\n      # TODO: using joins might be more efficient here.\n      self.processed_buffer += self.buffer[:m.end()]\n      self.buffer = self.buffer[m.end():]\n      self.processed += m.end()\n\n      next_state = token.next_state\n      for action in token.actions:\n\n        # Is there a callback to handle this action?\n        callback = getattr(self, action, self.Default)\n\n        # Allow a callback to skip other callbacks.\n        try:\n          possible_next_state = callback(string=m.group(0), match=m)\n          if possible_next_state == self._CONTINUE_STATE:\n            continue\n          # Override the state from the Token\n          elif possible_next_state:\n            next_state = possible_next_state\n        except errors.ParseError as exception:\n          self.Error(exception)\n\n      # Update the next state\n      if next_state:\n        self.state = next_state\n\n      return token\n\n    # Check that we are making progress - if we are too full, we assume we are\n    # stuck.\n    self.Error('Expected {0:s}'.format(self.state))\n    self.processed_buffer += self.buffer[:1]\n    self.buffer = self.buffer[1:]\n    return self._ERROR_TOKEN"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\npushing the current state on the state stack.", "response": "def PushState(self, **unused_kwargs):\n    \"\"\"Push the current state on the state stack.\"\"\"\n    logging.debug('Storing state {0:s}'.format(repr(self.state)))\n    self.state_stack.append(self.state)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef PopState(self, **unused_kwargs):\n    try:\n      self.state = self.state_stack.pop()\n      logging.debug('Returned state to {0:s}'.format(self.state))\n\n      return self.state\n    except IndexError:\n      self.Error(\n          'Tried to pop the state but failed - possible recursion error')", "response": "Pop the previous state from the stack."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef PushBack(self, string='', **unused_kwargs):\n    self.buffer = string + self.buffer\n    self.processed_buffer = self.processed_buffer[:-len(string)]", "response": "Push the match back on the stream."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfeed data into the buffer.", "response": "def Feed(self, size=512):\n    \"\"\"Feed data into the buffer.\n\n    Args:\n      size: optional data size to read form the file-like object.\n    \"\"\"\n    data = self.file_object.read(size)\n    Lexer.Feed(self, data)\n    return len(data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving the next token in the buffer.", "response": "def NextToken(self):\n    \"\"\"Retrieves the next token.\n\n    Returns:\n      The next token (instance of Token) or None.\n    \"\"\"\n    # If we don't have enough data - feed ourselves: We assume\n    # that we must have at least one sector in our buffer.\n    if len(self.buffer) < 512:\n      if self.Feed() == 0 and not self.buffer:\n        return None\n\n    return Lexer.NextToken(self)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef AddArg(self, argument):\n    self.args.append(argument)\n    if len(self.args) > self.number_of_args:\n      raise errors.ParseError('Too many arguments for this expression.')\n\n    elif len(self.args) == self.number_of_args:\n      return True\n\n    return False", "response": "Adds a new argument to the sequence of arguments."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef Compile(self, filter_implementation):\n    operator = self.operator.lower()\n    if operator in ('and', '&&'):\n      method = 'AndFilter'\n    elif operator in ('or', '||'):\n      method = 'OrFilter'\n    else:\n      raise errors.ParseError(\n          'Invalid binary operator {0:s}'.format(operator))\n\n    args = [x.Compile(filter_implementation) for x in self.args]\n    return getattr(filter_implementation, method)(*args)", "response": "Compile the binary expression into a filter object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef BinaryOperator(self, string=None, **unused_kwargs):\n    self.stack.append(self.binary_expression_cls(string))", "response": "Set the binary operator."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef StringEscape(self, string, match, **unused_kwargs):\n    if match.group(1) in '\\'\"rnbt':\n      self.string += string.decode('unicode_escape')\n    else:\n      self.string += string", "response": "Escape backslashes found inside a string quote."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninserting an argument into the current expression.", "response": "def InsertArg(self, string='', **unused_kwargs):\n    \"\"\"Insert an argument to the current expression.\"\"\"\n    logging.debug('Storing Argument {0:s}'.format(string))\n\n    # This expression is complete\n    if self.current_expression.AddArg(string):\n      self.stack.append(self.current_expression)\n      self.current_expression = self.expression_cls()\n      return self.PopState()\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndetermining if a file entry matches the filter.", "response": "def Match(self, file_entry):\n    \"\"\"Determines if a file entry matches the filter.\n\n    Args:\n      file_entry (dfvfs.FileEntry): a file entry.\n\n    Returns:\n      bool: True if the file entry matches the filter.\n    \"\"\"\n    if not file_entry:\n      return False\n\n    filename = file_entry.name.lower()\n    return filename == self._filename"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nenable parser plugins. Args: plugin_includes (list[str]): names of the plugins to enable, where None or an empty list represents all plugins. Note the default plugin, if it exists, is always enabled and cannot be disabled.", "response": "def EnablePlugins(self, plugin_includes):\n    \"\"\"Enables parser plugins.\n\n    Args:\n      plugin_includes (list[str]): names of the plugins to enable, where None\n          or an empty list represents all plugins. Note the default plugin, if\n          it exists, is always enabled and cannot be disabled.\n    \"\"\"\n    self._plugins = []\n    if not self._plugin_classes:\n      return\n\n    default_plugin_name = '{0:s}_default'.format(self.NAME)\n    for plugin_name, plugin_class in iter(self._plugin_classes.items()):\n      if plugin_name == default_plugin_name:\n        self._default_plugin = plugin_class()\n        continue\n\n      if plugin_includes and plugin_name not in plugin_includes:\n        continue\n\n      plugin_object = plugin_class()\n      self._plugins.append(plugin_object)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef GetPluginObjectByName(cls, plugin_name):\n    plugin_class = cls._plugin_classes.get(plugin_name, None)\n    if plugin_class:\n      return plugin_class()\n\n    return None", "response": "Retrieves a specific plugin object by its name."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef GetPlugins(cls):\n    for plugin_name, plugin_class in iter(cls._plugin_classes.items()):\n      yield plugin_name, plugin_class", "response": "Retrieves the registered plugins."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef RegisterPlugin(cls, plugin_class):\n    plugin_name = plugin_class.NAME.lower()\n    if plugin_name in cls._plugin_classes:\n      raise KeyError((\n          'Plugin class already set for name: {0:s}.').format(\n              plugin_class.NAME))\n\n    cls._plugin_classes[plugin_name] = plugin_class", "response": "Registers a plugin class."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef Parse(self, parser_mediator):\n    file_entry = parser_mediator.GetFileEntry()\n    if not file_entry:\n      raise errors.UnableToParseFile('Invalid file entry')\n\n    parser_mediator.AppendToParserChain(self)\n    try:\n      self.ParseFileEntry(parser_mediator, file_entry)\n    finally:\n      parser_mediator.PopFromParserChain()", "response": "Parses the file entry and extracts event objects."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef Parse(self, parser_mediator, file_object):\n    if not file_object:\n      raise errors.UnableToParseFile('Invalid file object')\n\n    if self._INITIAL_FILE_OFFSET is not None:\n      file_object.seek(self._INITIAL_FILE_OFFSET, os.SEEK_SET)\n\n    parser_mediator.AppendToParserChain(self)\n    try:\n      self.ParseFileObject(parser_mediator, file_object)\n    finally:\n      parser_mediator.PopFromParserChain()", "response": "Parses a single file - like object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef GetFormatSpecification(cls):\n    format_specification = specification.FormatSpecification(cls.NAME)\n    format_specification.AddNewSignature(b'ElfFile\\x00', offset=0)\n    return format_specification", "response": "Retrieves the format specification."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _GetEventData(\n      self, parser_mediator, record_index, evtx_record, recovered=False):\n    \"\"\"Extract data from a Windows XML EventLog (EVTX) record.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      record_index (int): event record index.\n      evtx_record (pyevtx.record): event record.\n      recovered (Optional[bool]): True if the record was recovered.\n\n    Return:\n      WinEvtxRecordEventData: event data.\n    \"\"\"\n    event_data = WinEvtxRecordEventData()\n\n    try:\n      event_data.record_number = evtx_record.identifier\n    except OverflowError as exception:\n      parser_mediator.ProduceExtractionWarning((\n          'unable to read record identifier from event record: {0:d} '\n          'with error: {1!s}').format(record_index, exception))\n\n    try:\n      event_identifier = evtx_record.event_identifier\n    except OverflowError as exception:\n      parser_mediator.ProduceExtractionWarning((\n          'unable to read event identifier from event record: {0:d} '\n          'with error: {1!s}').format(record_index, exception))\n\n      event_identifier = None\n\n    try:\n      event_identifier_qualifiers = evtx_record.event_identifier_qualifiers\n    except OverflowError as exception:\n      parser_mediator.ProduceExtractionWarning((\n          'unable to read event identifier qualifiers from event record: '\n          '{0:d} with error: {1!s}').format(record_index, exception))\n\n      event_identifier_qualifiers = None\n\n    event_data.offset = evtx_record.offset\n    event_data.recovered = recovered\n\n    if event_identifier is not None:\n      event_data.event_identifier = event_identifier\n\n      if event_identifier_qualifiers is not None:\n        event_data.message_identifier = (\n            (event_identifier_qualifiers << 16) | event_identifier)\n\n    event_data.event_level = evtx_record.event_level\n    event_data.source_name = evtx_record.source_name\n\n    # Computer name is the value stored in the event record and does not\n    # necessarily corresponds with the actual hostname.\n    event_data.computer_name = evtx_record.computer_name\n    event_data.user_sid = evtx_record.user_security_identifier\n\n    event_data.strings = list(evtx_record.strings)\n\n    event_data.strings_parsed = {}\n    if event_identifier in self._EVTX_FIELD_MAP.keys():\n      rules = self._EVTX_FIELD_MAP.get(event_identifier, [])\n      for rule in rules:\n        if len(evtx_record.strings) <= rule.index:\n          parser_mediator.ProduceExtractionWarning((\n              'evtx_record.strings has unexpected length of {0:d} '\n              '(expected at least {1:d})'.format(\n                  len(evtx_record.strings), rule.index)))\n\n        event_data.strings_parsed[rule.name] = evtx_record.strings[rule.index]\n\n    event_data.xml_string = evtx_record.xml_string\n\n    return event_data", "response": "Extracts event data from a Windows XML EventLog record."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _ParseRecord(\n      self, parser_mediator, record_index, evtx_record, recovered=False):\n    \"\"\"Extract data from a Windows XML EventLog (EVTX) record.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      record_index (int): event record index.\n      evtx_record (pyevtx.record): event record.\n      recovered (Optional[bool]): True if the record was recovered.\n    \"\"\"\n    event_data = self._GetEventData(\n        parser_mediator, record_index, evtx_record, recovered=recovered)\n\n    try:\n      written_time = evtx_record.get_written_time_as_integer()\n    except OverflowError as exception:\n      parser_mediator.ProduceExtractionWarning((\n          'unable to read written time from event record: {0:d} '\n          'with error: {1!s}').format(record_index, exception))\n\n      written_time = None\n\n    if not written_time:\n      date_time = dfdatetime_semantic_time.SemanticTime('Not set')\n    else:\n      date_time = dfdatetime_filetime.Filetime(timestamp=written_time)\n\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_WRITTEN)\n    parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Extracts data from a Windows XML EventLog record."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _ParseRecords(self, parser_mediator, evtx_file):\n    # To handle errors when parsing a Windows XML EventLog (EVTX) file in the\n    # most granular way the following code iterates over every event record.\n    # The call to evt_file.get_record() and access to members of evt_record\n    # should be called within a try-except.\n\n    for record_index in range(evtx_file.number_of_records):\n      if parser_mediator.abort:\n        break\n\n      try:\n        evtx_record = evtx_file.get_record(record_index)\n        self._ParseRecord(parser_mediator, record_index, evtx_record)\n\n      except IOError as exception:\n        parser_mediator.ProduceExtractionWarning(\n            'unable to parse event record: {0:d} with error: {1!s}'.format(\n                record_index, exception))\n\n    for record_index in range(evtx_file.number_of_recovered_records):\n      if parser_mediator.abort:\n        break\n\n      try:\n        evtx_record = evtx_file.get_recovered_record(record_index)\n        self._ParseRecord(\n            parser_mediator, record_index, evtx_record, recovered=True)\n\n      except IOError as exception:\n        parser_mediator.ProduceExtractionWarning((\n            'unable to parse recovered event record: {0:d} with error: '\n            '{1!s}').format(record_index, exception))", "response": "Parses the event log records."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse a Windows XML EventLog file - like object.", "response": "def ParseFileObject(self, parser_mediator, file_object):\n    \"\"\"Parses a Windows XML EventLog (EVTX) file-like object.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      file_object (dfvfs.FileIO): a file-like object.\n    \"\"\"\n    evtx_file = pyevtx.file()\n    evtx_file.set_ascii_codepage(parser_mediator.codepage)\n\n    try:\n      evtx_file.open_file_object(file_object)\n    except IOError as exception:\n      parser_mediator.ProduceExtractionWarning(\n          'unable to open file with error: {0!s}'.format(exception))\n      return\n\n    try:\n      self._ParseRecords(parser_mediator, evtx_file)\n    finally:\n      evtx_file.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef GetMessages(self, formatter_mediator, event):\n    if self.DATA_TYPE != event.data_type:\n      raise errors.WrongFormatter('Unsupported data type: {0:s}.'.format(\n          event.data_type))\n\n    event_values = event.CopyToDict()\n\n    read_receipt = event_values.get('read_receipt', None)\n    if read_receipt is not None:\n      event_values['read_receipt'] = (\n          self._READ_RECEIPT.get(read_receipt, 'UNKNOWN'))\n\n    message_type = event_values.get('message_type', None)\n    if message_type is not None:\n      event_values['message_type'] = (\n          self._MESSAGE_TYPE.get(message_type, 'UNKNOWN'))\n\n    return self._ConditionalFormatMessages(event_values)", "response": "Determines the formatted message strings for an event object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nretrieving the format specification.", "response": "def GetFormatSpecification(cls):\n    \"\"\"Retrieves the format specification.\n\n    Returns:\n      FormatSpecification: format specification.\n    \"\"\"\n    format_specification = specification.FormatSpecification(cls.NAME)\n    format_specification.AddNewSignature(b'SCCA', offset=4)\n    format_specification.AddNewSignature(b'MAM\\x04', offset=0)\n    return format_specification"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ParseFileObject(self, parser_mediator, file_object):\n    scca_file = pyscca.file()\n\n    try:\n      scca_file.open_file_object(file_object)\n    except IOError as exception:\n      parser_mediator.ProduceExtractionWarning(\n          'unable to open file with error: {0!s}'.format(exception))\n      return\n\n    format_version = scca_file.format_version\n    executable_filename = scca_file.executable_filename\n    prefetch_hash = scca_file.prefetch_hash\n    run_count = scca_file.run_count\n    number_of_volumes = scca_file.number_of_volumes\n\n    volume_serial_numbers = []\n    volume_device_paths = []\n    path = ''\n\n    for volume_information in iter(scca_file.volumes):\n      volume_serial_number = volume_information.serial_number\n      volume_device_path = volume_information.device_path\n\n      volume_serial_numbers.append(volume_serial_number)\n      volume_device_paths.append(volume_device_path)\n\n      timestamp = volume_information.get_creation_time_as_integer()\n      if timestamp:\n        event_data = windows_events.WindowsVolumeEventData()\n        event_data.device_path = volume_device_path\n        event_data.origin = parser_mediator.GetFilename()\n        event_data.serial_number = volume_serial_number\n\n        date_time = dfdatetime_filetime.Filetime(timestamp=timestamp)\n        event = time_events.DateTimeValuesEvent(\n            date_time, definitions.TIME_DESCRIPTION_CREATION)\n        parser_mediator.ProduceEventWithEventData(event, event_data)\n\n      for filename in iter(scca_file.filenames):\n        if not filename:\n          continue\n\n        if (filename.startswith(volume_device_path) and\n            filename.endswith(executable_filename)):\n          _, _, path = filename.partition(volume_device_path)\n\n    mapped_files = []\n    for entry_index, file_metrics in enumerate(scca_file.file_metrics_entries):\n      mapped_file_string = file_metrics.filename\n      if not mapped_file_string:\n        parser_mediator.ProduceExtractionWarning(\n            'missing filename for file metrics entry: {0:d}'.format(\n                entry_index))\n        continue\n\n      file_reference = file_metrics.file_reference\n      if file_reference:\n        mapped_file_string = (\n            '{0:s} [MFT entry: {1:d}, sequence: {2:d}]').format(\n                mapped_file_string, file_reference & 0xffffffffffff,\n                file_reference >> 48)\n\n      mapped_files.append(mapped_file_string)\n\n    event_data = WinPrefetchExecutionEventData()\n    event_data.executable = executable_filename\n    event_data.mapped_files = mapped_files\n    event_data.number_of_volumes = number_of_volumes\n    event_data.path = path\n    event_data.prefetch_hash = prefetch_hash\n    event_data.run_count = run_count\n    event_data.version = format_version\n    event_data.volume_device_paths = volume_device_paths\n    event_data.volume_serial_numbers = volume_serial_numbers\n\n    timestamp = scca_file.get_last_run_time_as_integer(0)\n    if not timestamp:\n      parser_mediator.ProduceExtractionWarning('missing last run time')\n      date_time = dfdatetime_semantic_time.SemanticTime('Not set')\n    else:\n      date_time = dfdatetime_filetime.Filetime(timestamp=timestamp)\n\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_LAST_RUN)\n    parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    # Check for the 7 older last run time values available since\n    # format version 26.\n    if format_version >= 26:\n      for last_run_time_index in range(1, 8):\n        timestamp = scca_file.get_last_run_time_as_integer(last_run_time_index)\n        if not timestamp:\n          continue\n\n        date_time = dfdatetime_filetime.Filetime(timestamp=timestamp)\n        date_time_description = 'Previous {0:s}'.format(\n            definitions.TIME_DESCRIPTION_LAST_RUN)\n        event = time_events.DateTimeValuesEvent(\n            date_time, date_time_description)\n        parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    scca_file.close()", "response": "Parses a Windows Prefetch file - like object."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds command line arguments to an argument group.", "response": "def AddArguments(cls, argument_group):\n    \"\"\"Adds command line arguments to an argument group.\n\n    This function takes an argument parser or an argument group object and adds\n    to it all the command line arguments this helper supports.\n\n    Args:\n      argument_group (argparse._ArgumentGroup|argparse.ArgumentParser):\n          argparse group.\n    \"\"\"\n    argument_group.add_argument(\n        '--analysis', metavar='PLUGIN_LIST', dest='analysis_plugins',\n        default='', action='store', type=str, help=(\n            'A comma separated list of analysis plugin names to be loaded '\n            'or \"--analysis list\" to see a list of available plugins.'))\n\n    arguments = sys.argv[1:]\n    argument_index = 0\n\n    if '--analysis' in arguments:\n      argument_index = arguments.index('--analysis') + 1\n\n    if 0 < argument_index < len(arguments):\n      names = [name.strip() for name in arguments[argument_index].split(',')]\n    else:\n      names = None\n\n    if names and names != ['list']:\n      manager.ArgumentHelperManager.AddCommandLineArguments(\n          argument_group, category='analysis', names=names)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ParseOptions(cls, options, configuration_object):\n    if not isinstance(configuration_object, tools.CLITool):\n      raise errors.BadConfigObject(\n          'Configuration object is not an instance of CLITool')\n\n    analysis_plugins = cls._ParseStringOption(options, 'analysis_plugins')\n\n    if analysis_plugins and analysis_plugins.lower() != 'list':\n      plugin_names = analysis_manager.AnalysisPluginManager.GetPluginNames()\n      analysis_plugins = [name.strip() for name in analysis_plugins.split(',')]\n\n      difference = set(analysis_plugins).difference(plugin_names)\n      if difference:\n        raise errors.BadConfigOption(\n            'Non-existent analysis plugins specified: {0:s}'.format(\n                ' '.join(difference)))\n\n    setattr(configuration_object, '_analysis_plugins', analysis_plugins)", "response": "Parses and validates the options."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing and validates the options.", "response": "def ParseOptions(cls, options, configuration_object):\n    \"\"\"Parses and validates options.\n\n    Args:\n      options (argparse.Namespace): parser options.\n      configuration_object (CLITool): object to be configured by the argument\n          helper.\n\n    Raises:\n      BadConfigObject: when the configuration object is of the wrong type.\n      BadConfigOption: when a configuration parameter fails validation.\n    \"\"\"\n    if not isinstance(configuration_object, tools.CLITool):\n      raise errors.BadConfigObject(\n          'Configuration object is not an instance of CLITool')\n\n    number_of_extraction_workers = cls._ParseNumericOption(\n        options, 'workers', default_value=0)\n\n    if number_of_extraction_workers < 0:\n      raise errors.BadConfigOption(\n          'Invalid number of extraction workers value cannot be negative.')\n\n    worker_memory_limit = cls._ParseNumericOption(\n        options, 'worker_memory_limit')\n\n    if worker_memory_limit and worker_memory_limit < 0:\n      raise errors.BadConfigOption(\n          'Invalid worker memory limit value cannot be negative.')\n\n    setattr(\n        configuration_object, '_number_of_extraction_workers',\n        number_of_extraction_workers)\n    setattr(configuration_object, '_worker_memory_limit', worker_memory_limit)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _ParseUpdateKeyValue(self, parser_mediator, registry_value, key_path):\n    if not registry_value.DataIsString():\n      parser_mediator.ProduceExtractionWarning(\n          'unsupported UpdateKey value data type: {0:s}'.format(\n              registry_value.data_type_string))\n      return\n\n    date_time_string = registry_value.GetDataAsObject()\n    if not date_time_string:\n      parser_mediator.ProduceExtractionWarning('missing UpdateKey value data')\n      return\n\n    re_match = self._UPDATE_DATE_TIME_RE.match(date_time_string)\n    if not re_match:\n      parser_mediator.ProduceExtractionWarning(\n          'unsupported UpdateKey value data: {0!s}'.format(date_time_string))\n      return\n\n    month, day_of_month, year, hours, minutes, seconds, part_of_day = (\n        re_match.groups())\n\n    try:\n      year = int(year, 10)\n      month = int(month, 10)\n      day_of_month = int(day_of_month, 10)\n      hours = int(hours, 10)\n      minutes = int(minutes, 10)\n      seconds = int(seconds, 10)\n    except (TypeError, ValueError):\n      parser_mediator.ProduceExtractionWarning(\n          'invalid UpdateKey date time value: {0!s}'.format(date_time_string))\n      return\n\n    if part_of_day == 'PM':\n      hours += 12\n\n    time_elements_tuple = (year, month, day_of_month, hours, minutes, seconds)\n\n    try:\n      date_time = dfdatetime_time_elements.TimeElements(\n          time_elements_tuple=time_elements_tuple)\n      date_time.is_local_time = True\n    except ValueError:\n      parser_mediator.ProduceExtractionWarning(\n          'invalid UpdateKey date time value: {0!s}'.format(\n              time_elements_tuple))\n      return\n\n    event_data = CCleanerUpdateEventData()\n    event_data.key_path = key_path\n\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_UPDATE,\n        time_zone=parser_mediator.timezone)\n    parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Parses an UpdateKey value."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nextract events from a Windows Registry key.", "response": "def ExtractEvents(self, parser_mediator, registry_key, **kwargs):\n    \"\"\"Extracts events from a Windows Registry key.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      registry_key (dfwinreg.WinRegistryKey): Windows Registry key.\n    \"\"\"\n    values_dict = {}\n    for registry_value in registry_key.GetValues():\n      if not registry_value.name or not registry_value.data:\n        continue\n\n      if registry_value.name == 'UpdateKey':\n        self._ParseUpdateKeyValue(\n            parser_mediator, registry_value, registry_key.path)\n      else:\n        values_dict[registry_value.name] = registry_value.GetDataAsObject()\n\n    event_data = windows_events.WindowsRegistryEventData()\n    event_data.key_path = registry_key.path\n    event_data.offset = registry_key.offset\n    event_data.regvalue = values_dict\n    event_data.source_append = self._SOURCE_APPEND\n    event_data.urls = self.URLS\n\n    event = time_events.DateTimeValuesEvent(\n        registry_key.last_written_time, definitions.TIME_DESCRIPTION_WRITTEN)\n    parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nretrieve the hashing results. Returns a list of analyzer_result objects.", "response": "def GetResults(self):\n    \"\"\"Retrieves the hashing results.\n\n    Returns:\n      list[AnalyzerResult]: results.\n    \"\"\"\n    results = []\n    for hasher in self._hashers:\n      logger.debug('Processing results for hasher {0:s}'.format(hasher.NAME))\n      result = analyzer_result.AnalyzerResult()\n      result.analyzer_name = self.NAME\n      result.attribute_name = '{0:s}_hash'.format(hasher.NAME)\n      result.attribute_value = hasher.GetStringDigest()\n      results.append(result)\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nresets the internal state of the analyzer.", "response": "def Reset(self):\n    \"\"\"Resets the internal state of the analyzer.\"\"\"\n    hasher_names = hashers_manager.HashersManager.GetHasherNamesFromString(\n        self._hasher_names_string)\n    self._hashers = hashers_manager.HashersManager.GetHashers(hasher_names)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef SetHasherNames(self, hasher_names_string):\n    hasher_names = hashers_manager.HashersManager.GetHasherNamesFromString(\n        hasher_names_string)\n\n    debug_hasher_names = ', '.join(hasher_names)\n    logger.debug('Got hasher names: {0:s}'.format(debug_hasher_names))\n\n    self._hashers = hashers_manager.HashersManager.GetHashers(hasher_names)\n    self._hasher_names_string = hasher_names_string", "response": "Sets the hashers that should be enabled."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _CreateTaskStorageWriter(self, path, task):\n    return SQLiteStorageFileWriter(\n        self._session, path,\n        storage_type=definitions.STORAGE_TYPE_TASK, task=task)", "response": "Creates a task storage writer."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nextracting events from a Windows Registry key.", "response": "def ExtractEvents(self, parser_mediator, registry_key, **kwargs):\n    \"\"\"Extracts events from a Windows Registry key.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      registry_key (dfwinreg.WinRegistryKey): Windows Registry key.\n    \"\"\"\n    values_dict = {}\n\n    if registry_key.number_of_values > 0:\n      for registry_value in registry_key.GetValues():\n        value_name = registry_value.name or '(default)'\n\n        if registry_value.DataIsString():\n          value_string = '[{0:s}] {1:s}'.format(\n              registry_value.data_type_string, registry_value.GetDataAsObject())\n\n        elif registry_value.DataIsInteger():\n          value_string = '[{0:s}] {1:d}'.format(\n              registry_value.data_type_string, registry_value.GetDataAsObject())\n\n        elif registry_value.DataIsMultiString():\n          value_string = '[{0:s}] {1:s}'.format(\n              registry_value.data_type_string, ''.join(\n                  registry_value.GetDataAsObject()))\n\n        else:\n          value_string = '[{0:s}]'.format(registry_value.data_type_string)\n\n        values_dict[value_name] = value_string\n\n    # Generate at least one event object for the key.\n    event_data = windows_events.WindowsRegistryEventData()\n    event_data.key_path = registry_key.path\n    event_data.offset = registry_key.offset\n    event_data.regvalue = values_dict\n    event_data.urls = self.URLS\n\n    event = time_events.DateTimeValuesEvent(\n        registry_key.last_written_time, definitions.TIME_DESCRIPTION_WRITTEN)\n    parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    if registry_key.number_of_subkeys == 0:\n      error_string = 'Key: {0:s} missing subkeys.'.format(registry_key.path)\n      parser_mediator.ProduceExtractionWarning(error_string)\n      return\n\n    for zone_key in registry_key.GetSubkeys():\n      # TODO: these values are stored in the Description value of the\n      # zone key. This solution will break on zone values that are larger\n      # than 5.\n      path = '{0:s}\\\\{1:s}'.format(\n          registry_key.path, self._ZONE_NAMES[zone_key.name])\n\n      values_dict = {}\n\n      # TODO: this plugin currently just dumps the values and does not\n      # distinguish between what is a feature control or not.\n      for value in zone_key.GetValues():\n        # Ignore the default value.\n        if not value.name:\n          continue\n\n        if value.DataIsString():\n          value_string = value.GetDataAsObject()\n\n        elif value.DataIsInteger():\n          value_integer = value.GetDataAsObject()\n          if value.name in self._KNOWN_PERMISSIONS_VALUE_NAMES:\n            value_string = self._CONTROL_VALUES_PERMISSIONS.get(\n                value_integer, 'UNKNOWN')\n          elif value.name == '1A00':\n            value_string = self._CONTROL_VALUES_1A00.get(\n                value_integer, 'UNKNOWN')\n          elif value.name == '1C00':\n            value_string = self._CONTROL_VALUES_1C00.get(\n                value_integer, 'UNKNOWN')\n          elif value.name == '1E05':\n            value_string = self._CONTROL_VALUES_SAFETY.get(\n                value_integer, 'UNKNOWN')\n          else:\n            value_string = '{0:d}'.format(value_integer)\n\n        else:\n          value_string = '[{0:s}]'.format(value.data_type_string)\n\n        if len(value.name) == 4 and value.name != 'Icon':\n          value_description = self._FEATURE_CONTROLS.get(value.name, 'UNKNOWN')\n        else:\n          value_description = self._FEATURE_CONTROLS.get(value.name, '')\n\n        if value_description:\n          feature_control = '[{0:s}] {1:s}'.format(\n              value.name, value_description)\n        else:\n          feature_control = '[{0:s}]'.format(value.name)\n\n        values_dict[feature_control] = value_string\n\n      event_data = windows_events.WindowsRegistryEventData()\n      event_data.key_path = path\n      event_data.offset = zone_key.offset\n      event_data.regvalue = values_dict\n      event_data.urls = self.URLS\n\n      event = time_events.DateTimeValuesEvent(\n          zone_key.last_written_time, definitions.TIME_DESCRIPTION_WRITTEN)\n      parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _FormatField(self, field):\n    if self._FIELD_DELIMITER and isinstance(field, py2to3.STRING_TYPES):\n      return field.replace(self._FIELD_DELIMITER, ' ')\n    return field", "response": "Formats a field.\n\n    Args:\n      field (str): field value.\n\n     Returns:\n       str: formatted field value."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nformat the hostname field.", "response": "def _FormatHostname(self, event):\n    \"\"\"Formats the hostname.\n\n    Args:\n      event (EventObject): event.\n\n     Returns:\n       str: formatted hostname field.\n    \"\"\"\n    hostname = self._output_mediator.GetHostname(event)\n    return self._FormatField(hostname)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _FormatUsername(self, event):\n    username = self._output_mediator.GetUsername(event)\n    return self._FormatField(username)", "response": "Formats the username field."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _GetOutputValues(self, event):\n    if not hasattr(event, 'timestamp'):\n      logger.error('Unable to output event without timestamp.')\n      return None\n\n    # TODO: add function to pass event_values to GetFormattedMessages.\n    message, message_short = self._output_mediator.GetFormattedMessages(event)\n    if message is None or message_short is None:\n      data_type = getattr(event, 'data_type', 'UNKNOWN')\n      raise errors.NoFormatterFound(\n          'Unable to find event formatter for: {0:s}.'.format(data_type))\n\n    # TODO: add function to pass event_values to GetFormattedSources.\n    source_short, source = self._output_mediator.GetFormattedSources(event)\n    if source is None or source_short is None:\n      data_type = getattr(event, 'data_type', 'UNKNOWN')\n      raise errors.NoFormatterFound(\n          'Unable to find event formatter for: {0:s}.'.format(data_type))\n\n    # TODO: preserve dfdatetime as an object.\n    # TODO: add support for self._output_mediator.timezone\n    date_time = dfdatetime_posix_time.PosixTimeInMicroseconds(\n        timestamp=event.timestamp)\n\n    format_variables = self._output_mediator.GetFormatStringAttributeNames(\n        event)\n    if format_variables is None:\n      data_type = getattr(event, 'data_type', 'UNKNOWN')\n      raise errors.NoFormatterFound(\n          'Unable to find event formatter for: {0:s}.'.format(data_type))\n\n    extra_attributes = []\n    for attribute_name, attribute_value in sorted(event.GetAttributes()):\n      if (attribute_name in definitions.RESERVED_VARIABLE_NAMES or\n          attribute_name in format_variables):\n        continue\n\n      # With ! in {1!s} we force a string conversion since some of\n      # the extra attributes values can be integer, float point or\n      # boolean values.\n      extra_attributes.append(\n          '{0:s}: {1!s}'.format(attribute_name, attribute_value))\n\n    extra_attributes = '; '.join(extra_attributes)\n    extra_attributes = extra_attributes.replace('\\n', '-').replace('\\r', '')\n\n    inode = getattr(event, 'inode', None)\n    if inode is None:\n      if hasattr(event, 'pathspec') and hasattr(\n          event.pathspec, 'image_inode'):\n        inode = event.pathspec.image_inode\n    if inode is None:\n      inode = '-'\n\n    hostname = self._FormatHostname(event)\n    username = self._FormatUsername(event)\n\n    notes = []\n    note_string = getattr(event, 'notes', None)\n    if note_string:\n      notes.append(note_string)\n\n    tag = getattr(event, 'tag', None)\n    if tag:\n      notes.extend(tag.labels)\n\n    if not notes:\n      notes.append('-')\n\n    year, month, day_of_month = date_time.GetDate()\n    hours, minutes, seconds = date_time.GetTimeOfDay()\n    try:\n      date_string = '{0:02d}/{1:02d}/{2:04d}'.format(month, day_of_month, year)\n      time_string = '{0:02d}:{1:02d}:{2:02d}'.format(hours, minutes, seconds)\n    except (TypeError, ValueError):\n      self._ReportEventError(event, (\n          'unable to copy timestamp: {0!s} to a human readable date and time. '\n          'Defaulting to: \"00/00/0000\" \"--:--:--\"').format(event.timestamp))\n\n      date_string = '00/00/0000'\n      time_string = '--:--:--'\n\n    output_values = [\n        date_string,\n        time_string,\n        '{0!s}'.format(self._output_mediator.timezone),\n        '....',\n        source_short,\n        source,\n        '-',\n        username,\n        hostname,\n        message_short,\n        message,\n        '2',\n        getattr(event, 'display_name', '-'),\n        '{0!s}'.format(inode),\n        ' '.join(notes),\n        getattr(event, 'parser', '-'),\n        extra_attributes]\n\n    return output_values", "response": "Retrieves the output values from the event."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites the output values to the output.", "response": "def _WriteOutputValues(self, output_values):\n    \"\"\"Writes values to the output.\n\n    Args:\n      output_values (list[str]): output values.\n    \"\"\"\n    for index, value in enumerate(output_values):\n      if not isinstance(value, py2to3.STRING_TYPES):\n        value = ''\n      output_values[index] = value.replace(',', ' ')\n\n    output_line = ','.join(output_values)\n    output_line = '{0:s}\\n'.format(output_line)\n    self._output_writer.Write(output_line)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef WriteEventBody(self, event):\n    output_values = self._GetOutputValues(event)\n\n    output_values[3] = self._output_mediator.GetMACBRepresentation(event)\n    output_values[6] = event.timestamp_desc or '-'\n\n    self._WriteOutputValues(output_values)", "response": "Writes the event body to the output."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwrites an event MACB group to the output.", "response": "def WriteEventMACBGroup(self, event_macb_group):\n    \"\"\"Writes an event MACB group to the output.\n\n    Args:\n      event_macb_group (list[EventObject]): event MACB group.\n    \"\"\"\n    output_values = self._GetOutputValues(event_macb_group[0])\n\n    timestamp_descriptions = [\n        event.timestamp_desc for event in event_macb_group]\n    output_values[3] = (\n        self._output_mediator.GetMACBRepresentationFromDescriptions(\n            timestamp_descriptions))\n    # TODO: fix timestamp description in source.\n    output_values[6] = '; '.join(timestamp_descriptions)\n\n    self._WriteOutputValues(output_values)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef Main():\n  argument_parser = argparse.ArgumentParser(description=(\n      'Plots memory usage from profiling data.'))\n\n  argument_parser.add_argument(\n      '--output', dest='output_file', type=str, help=(\n          'path of the output file to write the graph to instead of using '\n          'interactive mode. The output format deduced from the extension '\n          'of the filename.'))\n\n  argument_parser.add_argument(\n      'profile_path', type=str, help=(\n          'path to the directory containing the profiling data.'))\n\n  options = argument_parser.parse_args()\n\n  if not os.path.isdir(options.profile_path):\n    print('No such directory: {0:s}'.format(options.profile_path))\n    return False\n\n  names = ['time', 'identifier', 'status']\n\n  measurements = {}\n\n  glob_expression = os.path.join(options.profile_path, 'tasks-*.csv.gz')\n  for csv_file_name in glob.glob(glob_expression):\n    data = numpy.genfromtxt(\n        csv_file_name, delimiter='\\t', dtype=None, encoding='utf-8',\n        names=names, skip_header=1)\n\n    label = os.path.basename(csv_file_name)\n    label = label.replace('tasks-', '').replace('.csv.gz', '')\n\n    for time, identifier, status in data:\n      if identifier not in measurements:\n        measurements[identifier] = TaskMeasurements()\n\n      task_measurement = measurements[identifier]\n\n      if status == 'completed':\n        task_measurement.completed_time = time\n        task_measurement.merging_duration = time - task_measurement.merging_time\n\n      elif status == 'created':\n        task_measurement.created_time = time\n\n      # TODO: add support for:\n      # elif status == 'merge_on_hold':\n      # elif status == 'merge_resumed':\n\n      elif status == 'merge_started':\n        task_measurement.merging_time = time\n\n      elif status == 'pending_merge':\n        task_measurement.pending_merge_time = time\n\n      elif status == 'processed':\n        task_measurement.processed_time = time\n\n      elif status == 'processing_started':\n        task_measurement.processing_time = time\n\n      elif status == 'processing_completed':\n        task_measurement.processing_duration = (\n            time - task_measurement.processing_time)\n\n      elif status == 'scheduled':\n        task_measurement.scheduled_time = time\n\n  before_pending_merge_duration = {}\n  before_queued_duration = {}\n  merging_duration = {}\n  pending_merge_duration = {}\n  processing_duration = {}\n  queued_duration = {}\n\n  for identifier, task_measurement in measurements.items():\n    before_pending_merge_duration[task_measurement.scheduled_time] = (\n        task_measurement.pending_merge_time - (\n            task_measurement.processing_time +\n            task_measurement.processing_duration))\n\n    before_queued_duration[task_measurement.scheduled_time] = (\n        task_measurement.scheduled_time - task_measurement.created_time)\n\n    merging_duration[task_measurement.merging_time] = (\n        task_measurement.merging_duration)\n\n    pending_merge_duration[task_measurement.processing_time] = (\n        task_measurement.merging_time - task_measurement.pending_merge_time)\n\n    processing_duration[task_measurement.processing_time] = (\n        task_measurement.processing_duration)\n\n    queued_duration[task_measurement.scheduled_time] = (\n        task_measurement.processing_time - task_measurement.scheduled_time)\n\n  keys = sorted(before_pending_merge_duration.keys())\n  values = [before_pending_merge_duration[key] for key in keys]\n  pyplot.plot(keys, values, label='Before pending merge')\n\n  keys = sorted(before_queued_duration.keys())\n  values = [before_queued_duration[key] for key in keys]\n  pyplot.plot(keys, values, label='Before queued')\n\n  keys = sorted(merging_duration.keys())\n  values = [merging_duration[key] for key in keys]\n  pyplot.plot(keys, values, label='Merging')\n\n  keys = sorted(pending_merge_duration.keys())\n  values = [pending_merge_duration[key] for key in keys]\n  pyplot.plot(keys, values, label='Pending merge')\n\n  keys = sorted(processing_duration.keys())\n  values = [processing_duration[key] for key in keys]\n  pyplot.plot(keys, values, label='Processing')\n\n  keys = sorted(queued_duration.keys())\n  values = [queued_duration[key] for key in keys]\n  pyplot.plot(keys, values, label='Queued')\n\n  pyplot.title('Task status duration')\n\n  pyplot.xlabel('Time')\n  pyplot.xscale('linear')\n\n  pyplot.ylabel('Duration')\n  pyplot.yscale('linear')\n\n  pyplot.legend()\n\n  if options.output_file:\n    pyplot.savefig(options.output_file)\n  else:\n    pyplot.show()\n\n  return True", "response": "The main function of the\n class."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing and validates the options.", "response": "def ParseOptions(cls, options, configuration_object):\n    \"\"\"Parses and validates options.\n\n    Args:\n      options (argparse.Namespace): parser options.\n      configuration_object (CLITool): object to be configured by the argument\n          helper.\n\n    Raises:\n      BadConfigObject: when the configuration object is of the wrong type.\n      BadConfigOption: if the required artifact definitions are not defined.\n    \"\"\"\n    if not isinstance(configuration_object, tools.CLITool):\n      raise errors.BadConfigObject(\n          'Configuration object is not an instance of CLITool')\n\n    artifact_filters = cls._ParseStringOption(options, 'artifact_filter_string')\n    artifact_filters_file = cls._ParseStringOption(\n        options, 'artifact_filters_file')\n    filter_file = cls._ParseStringOption(options, 'file_filter')\n\n    if artifact_filters and artifact_filters_file:\n      raise errors.BadConfigOption(\n          'Please only specify artifact definition names in a file '\n          'or on the command line.')\n\n    if (artifact_filters_file or artifact_filters) and filter_file:\n      raise errors.BadConfigOption(\n          'Please do not specify both artifact definitions and legacy filters.')\n\n    if artifact_filters_file and os.path.isfile(artifact_filters_file):\n      with open(artifact_filters_file) as file_object:\n        file_content = file_object.read()\n        artifact_filters = file_content.splitlines()\n    elif artifact_filters:\n      artifact_filters = [name.strip() for name in artifact_filters.split(',')]\n\n    setattr(configuration_object, '_artifact_filters', artifact_filters)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nextract events from a Windows Registry key.", "response": "def ExtractEvents(self, parser_mediator, registry_key, **kwargs):\n    \"\"\"Extracts events from a Windows Registry key.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      registry_key (dfwinreg.WinRegistryKey): Windows Registry key.\n    \"\"\"\n    for subkey in registry_key.GetSubkeys():\n      name = subkey.name\n      if not name:\n        continue\n\n      values_dict = {}\n      values_dict['Volume'] = name\n\n      label_value = subkey.GetValueByName('_LabelFromReg')\n      if label_value:\n        values_dict['Label'] = label_value.GetDataAsObject()\n\n      if name.startswith('{'):\n        values_dict['Type'] = 'Volume'\n\n      elif name.startswith('#'):\n        # The format is: ##Server_Name#Share_Name.\n        values_dict['Type'] = 'Remote Drive'\n        server_name, _, share_name = name[2:].partition('#')\n        values_dict['Remote_Server'] = server_name\n        values_dict['Share_Name'] = '\\\\{0:s}'.format(\n            share_name.replace('#', '\\\\'))\n\n      else:\n        values_dict['Type'] = 'Drive'\n\n      event_data = windows_events.WindowsRegistryEventData()\n      event_data.key_path = registry_key.path\n      event_data.offset = subkey.offset\n      event_data.regvalue = values_dict\n      event_data.urls = self.URLS\n\n      event = time_events.DateTimeValuesEvent(\n          subkey.last_written_time, definitions.TIME_DESCRIPTION_WRITTEN)\n      parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef GetMessages(self, formatter_mediator, event):\n    if self.DATA_TYPE != event.data_type:\n      raise errors.WrongFormatter('Unsupported data type: {0:s}.'.format(\n          event.data_type))\n\n    event_values = event.CopyToDict()\n\n    primary_url = event_values['primary_url']\n    secondary_url = event_values['secondary_url']\n\n    # There is apparently a bug, either in GURL.cc or\n    # content_settings_pattern.cc where URLs with file:// scheme are stored in\n    # the URL as an empty string, which is later detected as being Invalid, and\n    # Chrome produces the following example logs:\n    # content_settings_pref.cc(469)] Invalid pattern strings: https://a.com:443,\n    # content_settings_pref.cc(295)] Invalid pattern strings: ,\n    # content_settings_pref.cc(295)] Invalid pattern strings: ,*\n    # More research needed, could be related to https://crbug.com/132659\n\n    if primary_url == '':\n      subject = 'local file'\n\n    elif secondary_url in (primary_url, '*'):\n      subject = primary_url\n\n    elif secondary_url == '':\n      subject = '{0:s} embedded in local file'.format(primary_url)\n\n    else:\n      subject = '{0:s} embedded in {1:s}'.format(primary_url, secondary_url)\n\n    event_values['subject'] = subject\n\n    return self._ConditionalFormatMessages(event_values)", "response": "Determines the formatted message strings for an event object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ParseOptions(cls, options, configuration_object):\n    if not isinstance(configuration_object, tools.CLITool):\n      raise errors.BadConfigObject(\n          'Configuration object is not an instance of CLITool')\n\n    storage_file = cls._ParseStringOption(options, 'storage_file')\n\n    setattr(configuration_object, '_storage_file_path', storage_file)", "response": "Parses and validates options."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _ExtractExtensionInstallEvents(self, settings_dict, parser_mediator):\n    for extension_id, extension in sorted(settings_dict.items()):\n      install_time = extension.get('install_time', None)\n      if not install_time:\n        parser_mediator.ProduceExtractionWarning(\n            'installation time missing for extension ID {0:s}'.format(\n                extension_id))\n        continue\n\n      try:\n        install_time = int(install_time, 10)\n      except ValueError:\n        parser_mediator.ProduceExtractionWarning((\n            'unable to convert installation time for extension ID '\n            '{0:s}').format(extension_id))\n        continue\n\n      manifest = extension.get('manifest', None)\n      if not manifest:\n        parser_mediator.ProduceExtractionWarning(\n            'manifest missing for extension ID {0:s}'.format(extension_id))\n        continue\n\n      event_data = ChromeExtensionInstallationEventData()\n      event_data.extension_id = extension_id\n      event_data.extension_name = manifest.get('name', None)\n      event_data.path = extension.get('path', None)\n\n      date_time = dfdatetime_webkit_time.WebKitTime(timestamp=install_time)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_ADDED)\n      parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Extracts extension installation events from a Preferences file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _ExtractContentSettingsExceptions(self, exceptions_dict, parser_mediator):\n    for permission in exceptions_dict:\n      if permission not in self._EXCEPTIONS_KEYS:\n        continue\n\n      exception_dict = exceptions_dict.get(permission, {})\n      for urls, url_dict in exception_dict.items():\n        last_used = url_dict.get('last_used', None)\n        if not last_used:\n          continue\n\n        # If secondary_url is '*', the permission applies to primary_url.\n        # If secondary_url is a valid URL, the permission applies to\n        # elements loaded from secondary_url being embedded in primary_url.\n        primary_url, secondary_url = urls.split(',')\n\n        event_data = ChromeContentSettingsExceptionsEventData()\n        event_data.permission = permission\n        event_data.primary_url = primary_url\n        event_data.secondary_url = secondary_url\n\n        timestamp = int(last_used * 1000000)\n        date_time = dfdatetime_posix_time.PosixTimeInMicroseconds(\n            timestamp=timestamp)\n        event = time_events.DateTimeValuesEvent(\n            date_time, definitions.TIME_DESCRIPTION_LAST_VISITED)\n        parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Extracts Chrome Content Settings exceptions events from a Preferences file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse a Chrome preferences file - like object.", "response": "def ParseFileObject(self, parser_mediator, file_object):\n    \"\"\"Parses a Chrome preferences file-like object.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      file_object (dfvfs.FileIO): file-like object.\n\n    Raises:\n      UnableToParseFile: when the file cannot be parsed.\n    \"\"\"\n    # First pass check for initial character being open brace.\n    if file_object.read(1) != b'{':\n      raise errors.UnableToParseFile((\n          '[{0:s}] {1:s} is not a valid Preference file, '\n          'missing opening brace.').format(\n              self.NAME, parser_mediator.GetDisplayName()))\n\n    file_object.seek(0, os.SEEK_SET)\n    file_content = file_object.read()\n    file_content = codecs.decode(file_content, self._ENCODING)\n\n    # Second pass to verify it's valid JSON\n    try:\n      json_dict = json.loads(file_content)\n    except ValueError as exception:\n      raise errors.UnableToParseFile((\n          '[{0:s}] Unable to parse file {1:s} as JSON: {2!s}').format(\n              self.NAME, parser_mediator.GetDisplayName(), exception))\n    except IOError as exception:\n      raise errors.UnableToParseFile((\n          '[{0:s}] Unable to open file {1:s} for parsing as'\n          'JSON: {2!s}').format(\n              self.NAME, parser_mediator.GetDisplayName(), exception))\n\n    # Third pass to verify the file has the correct keys in it for Preferences\n    if not set(self.REQUIRED_KEYS).issubset(set(json_dict.keys())):\n      raise errors.UnableToParseFile('File does not contain Preference data.')\n\n    extensions_setting_dict = json_dict.get('extensions')\n    if not extensions_setting_dict:\n      raise errors.UnableToParseFile(\n          '[{0:s}] {1:s} is not a valid Preference file, '\n          'does not contain extensions value.'.format(\n              self.NAME, parser_mediator.GetDisplayName()))\n\n    extensions_dict = extensions_setting_dict.get('settings')\n    if not extensions_dict:\n      raise errors.UnableToParseFile(\n          '[{0:s}] {1:s} is not a valid Preference file, '\n          'does not contain extensions settings value.'.format(\n              self.NAME, parser_mediator.GetDisplayName()))\n\n    extensions_autoupdate_dict = extensions_setting_dict.get('autoupdate')\n    if extensions_autoupdate_dict:\n      autoupdate_lastcheck_timestamp = extensions_autoupdate_dict.get(\n          'last_check', None)\n\n      if autoupdate_lastcheck_timestamp:\n        autoupdate_lastcheck = int(autoupdate_lastcheck_timestamp, 10)\n\n        event_data = ChromeExtensionsAutoupdaterEventData()\n        event_data.message = 'Chrome extensions autoupdater last run'\n\n        date_time = dfdatetime_webkit_time.WebKitTime(\n            timestamp=autoupdate_lastcheck)\n        event = time_events.DateTimeValuesEvent(\n            date_time, definitions.TIME_DESCRIPTION_ADDED)\n        parser_mediator.ProduceEventWithEventData(event, event_data)\n\n      autoupdate_nextcheck_timestamp = extensions_autoupdate_dict.get(\n          'next_check', None)\n      if autoupdate_nextcheck_timestamp:\n        autoupdate_nextcheck = int(autoupdate_nextcheck_timestamp, 10)\n\n        event_data = ChromeExtensionsAutoupdaterEventData()\n        event_data.message = 'Chrome extensions autoupdater next run'\n\n        date_time = dfdatetime_webkit_time.WebKitTime(\n            timestamp=autoupdate_nextcheck)\n        event = time_events.DateTimeValuesEvent(\n            date_time, definitions.TIME_DESCRIPTION_ADDED)\n        parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    browser_dict = json_dict.get('browser', None)\n    if browser_dict and 'last_clear_browsing_data_time' in browser_dict:\n      last_clear_history_timestamp = browser_dict.get(\n          'last_clear_browsing_data_time', None)\n\n      if last_clear_history_timestamp:\n        last_clear_history = int(last_clear_history_timestamp, 10)\n\n        event_data = ChromeExtensionsAutoupdaterEventData()\n        event_data.message = 'Chrome history was cleared by user'\n\n        date_time = dfdatetime_webkit_time.WebKitTime(\n            timestamp=last_clear_history)\n        event = time_events.DateTimeValuesEvent(\n            date_time, definitions.TIME_DESCRIPTION_DELETED)\n        parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    self._ExtractExtensionInstallEvents(extensions_dict, parser_mediator)\n\n    profile_dict = json_dict.get('profile', None)\n    if profile_dict:\n      content_settings_dict = profile_dict.get('content_settings', None)\n      if content_settings_dict:\n        exceptions_dict = content_settings_dict.get('exceptions', None)\n        if exceptions_dict:\n          self._ExtractContentSettingsExceptions(\n              exceptions_dict, parser_mediator)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef Collect(\n      self, knowledge_base, artifact_definition, searcher, file_system):\n    \"\"\"Collects values using a file artifact definition.\n\n    Args:\n      knowledge_base (KnowledgeBase): to fill with preprocessing information.\n      artifact_definition (artifacts.ArtifactDefinition): artifact definition.\n      searcher (dfvfs.FileSystemSearcher): file system searcher to preprocess\n          the file system.\n      file_system (dfvfs.FileSystem): file system to be preprocessed.\n\n    Raises:\n      PreProcessFail: if the preprocessing fails.\n    \"\"\"\n    for source in artifact_definition.sources:\n      if source.type_indicator not in (\n          artifact_definitions.TYPE_INDICATOR_FILE,\n          artifact_definitions.TYPE_INDICATOR_PATH):\n        continue\n\n      for path in source.paths:\n        # Make sure the path separators used in the artifact definition\n        # correspond to those used by the file system.\n        path_segments = path.split(source.separator)\n\n        find_spec = file_system_searcher.FindSpec(\n            location_glob=path_segments[1:], case_sensitive=False)\n\n        for path_specification in searcher.Find(find_specs=[find_spec]):\n          self._ParsePathSpecification(\n              knowledge_base, searcher, file_system, path_specification,\n              source.separator)", "response": "Collects values using a file artifact definition."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _ParsePathSpecification(\n      self, knowledge_base, searcher, file_system, path_specification,\n      path_separator):\n    \"\"\"Parses a file system for a preprocessing attribute.\n\n    Args:\n      knowledge_base (KnowledgeBase): to fill with preprocessing information.\n      searcher (dfvfs.FileSystemSearcher): file system searcher to preprocess\n          the file system.\n      file_system (dfvfs.FileSystem): file system to be preprocessed.\n      path_specification (dfvfs.PathSpec): path specification that contains\n          the artifact value data.\n      path_separator (str): path segment separator.\n\n    Raises:\n      PreProcessFail: if the preprocessing fails.\n    \"\"\"\n    try:\n      file_entry = searcher.GetFileEntryByPathSpec(path_specification)\n    except IOError as exception:\n      relative_path = searcher.GetRelativePath(path_specification)\n      if path_separator != file_system.PATH_SEPARATOR:\n        relative_path_segments = file_system.SplitPath(relative_path)\n        relative_path = '{0:s}{1:s}'.format(\n            path_separator, path_separator.join(relative_path_segments))\n\n      raise errors.PreProcessFail((\n          'Unable to retrieve file entry: {0:s} with error: '\n          '{1!s}').format(relative_path, exception))\n\n    if file_entry:\n      self._ParseFileEntry(knowledge_base, file_entry)", "response": "Parses a path specification and returns a list of artifact value data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse a file entry for a preprocessing attribute.", "response": "def _ParseFileEntry(self, knowledge_base, file_entry):\n    \"\"\"Parses a file entry for a preprocessing attribute.\n\n    Args:\n      knowledge_base (KnowledgeBase): to fill with preprocessing information.\n      file_entry (dfvfs.FileEntry): file entry that contains the artifact\n          value data.\n\n    Raises:\n      PreProcessFail: if the preprocessing fails.\n    \"\"\"\n    file_object = file_entry.GetFileObject()\n    try:\n      self._ParseFileData(knowledge_base, file_object)\n    finally:\n      file_object.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncollects values using a Windows Registry value artifact definition.", "response": "def Collect(\n      self, knowledge_base, artifact_definition, searcher):\n    \"\"\"Collects values using a Windows Registry value artifact definition.\n\n    Args:\n      knowledge_base (KnowledgeBase): to fill with preprocessing information.\n      artifact_definition (artifacts.ArtifactDefinition): artifact definition.\n      searcher (dfwinreg.WinRegistrySearcher): Windows Registry searcher to\n          preprocess the Windows Registry.\n\n    Raises:\n      PreProcessFail: if the Windows Registry key or value cannot be read.\n    \"\"\"\n    for source in artifact_definition.sources:\n      if source.type_indicator not in (\n          artifact_definitions.TYPE_INDICATOR_WINDOWS_REGISTRY_KEY,\n          artifact_definitions.TYPE_INDICATOR_WINDOWS_REGISTRY_VALUE):\n        continue\n\n      if source.type_indicator == (\n          artifact_definitions.TYPE_INDICATOR_WINDOWS_REGISTRY_KEY):\n        key_value_pairs = [{'key': key} for key in source.keys]\n      else:\n        key_value_pairs = source.key_value_pairs\n\n      for key_value_pair in key_value_pairs:\n        key_path = key_value_pair['key']\n\n        # The artifact definitions currently incorrectly define\n        # CurrentControlSet so we correct it here for now.\n        # Also see: https://github.com/ForensicArtifacts/artifacts/issues/120\n        key_path_upper = key_path.upper()\n        if key_path_upper.startswith('%%CURRENT_CONTROL_SET%%'):\n          key_path = '{0:s}{1:s}'.format(\n              'HKEY_LOCAL_MACHINE\\\\System\\\\CurrentControlSet', key_path[23:])\n\n        find_spec = registry_searcher.FindSpec(key_path_glob=key_path)\n\n        for key_path in searcher.Find(find_specs=[find_spec]):\n          try:\n            registry_key = searcher.GetKeyByPath(key_path)\n          except IOError as exception:\n            raise errors.PreProcessFail((\n                'Unable to retrieve Windows Registry key: {0:s} with error: '\n                '{1!s}').format(key_path, exception))\n\n          if registry_key:\n            value_name = key_value_pair.get('value', None)\n            self._ParseKey(knowledge_base, registry_key, value_name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _ParseKey(self, knowledge_base, registry_key, value_name):\n    try:\n      registry_value = registry_key.GetValueByName(value_name)\n    except IOError as exception:\n      raise errors.PreProcessFail((\n          'Unable to retrieve Windows Registry key: {0:s} value: {1:s} '\n          'with error: {2!s}').format(\n              registry_key.path, value_name, exception))\n\n    if registry_value:\n      value_object = registry_value.GetDataAsObject()\n      if value_object:\n        self._ParseValueData(knowledge_base, value_object)", "response": "Parses a Windows Registry key for a preprocessing attribute."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing an F value in the Windows Registry key.", "response": "def _ParseFValue(self, registry_key):\n    \"\"\"Parses an F value.\n\n    Args:\n      registry_key (dfwinreg.WinRegistryKey): Windows Registry key.\n\n    Returns:\n      f_value: F value stored in the Windows Registry key.\n\n    Raises:\n      ParseError: if the Windows Registry key does not contain an F value or\n          F value cannot be parsed.\n    \"\"\"\n    registry_value = registry_key.GetValueByName('F')\n    if not registry_value:\n      raise errors.ParseError(\n          'missing value: \"F\" in Windows Registry key: {0:s}.'.format(\n              registry_key.name))\n\n    f_value_map = self._GetDataTypeMap('f_value')\n\n    try:\n      return self._ReadStructureFromByteStream(\n          registry_value.data, 0, f_value_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.ParseError(exception)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing a V value string.", "response": "def _ParseVValueString(\n      self, parser_mediator, data, user_information_descriptor):\n    \"\"\"Parses a V value string.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      data (bytes): Windows Registry V value data.\n      user_information_descriptor (user_information_descriptor): V value\n          user information descriptor.\n\n    Returns:\n      str: string value stored in the Windows Registry V value data.\n    \"\"\"\n    data_start_offset = (\n        user_information_descriptor.offset + self._V_VALUE_STRINGS_OFFSET)\n    data_end_offset = data_start_offset + user_information_descriptor.size\n    descriptor_data = data[data_start_offset:data_end_offset]\n\n    try:\n      username = descriptor_data.decode('utf-16-le')\n    except (UnicodeDecodeError, UnicodeEncodeError) as exception:\n      username = descriptor_data.decode('utf-16-le', errors='replace')\n      parser_mediator.ProduceExtractionWarning((\n          'unable to decode V value string with error: {0!s}. Characters '\n          'that cannot be decoded will be replaced with \"?\" or '\n          '\"\\\\ufffd\".').format(exception))\n\n    return username"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ExtractEvents(self, parser_mediator, registry_key, **kwargs):\n    names_key = registry_key.GetSubkeyByName('Names')\n    if not names_key:\n      parser_mediator.ProduceExtractionWarning('missing subkey: Names.')\n      return\n\n    last_written_time_per_username = {\n        registry_value.name: registry_value.last_written_time\n        for registry_value in names_key.GetSubkeys()}\n\n    for subkey in registry_key.GetSubkeys():\n      if subkey.name == 'Names':\n        continue\n\n      try:\n        f_value = self._ParseFValue(subkey)\n      except errors.ParseError as exception:\n        parser_mediator.ProduceExtractionWarning(\n            'unable to parse F value with error: {0!s}'.format(exception))\n        continue\n\n      registry_value = subkey.GetValueByName('V')\n      if not registry_value:\n        parser_mediator.ProduceExtractionWarning(\n            'missing Registry value: \"V\" in subkey: {0:s}.'.format(\n                subkey.name))\n        continue\n\n      v_value_map = self._GetDataTypeMap('v_value')\n\n      try:\n        v_value = self._ReadStructureFromByteStream(\n            registry_value.data, 0, v_value_map)\n      except (ValueError, errors.ParseError) as exception:\n        parser_mediator.ProduceExtractionWarning(\n            'unable to parse V value with error: {0!s}'.format(exception))\n        continue\n\n      username = self._ParseVValueString(\n          parser_mediator, registry_value.data, v_value[1])\n\n      fullname = self._ParseVValueString(\n          parser_mediator, registry_value.data, v_value[2])\n\n      comments = self._ParseVValueString(\n          parser_mediator, registry_value.data, v_value[3])\n\n      last_written_time = last_written_time_per_username.get(username, None)\n\n      # TODO: check if subkey.name == f_value.rid\n\n      if last_written_time:\n        values_dict = {\n            'account_rid': f_value.rid,\n            'login_count': f_value.number_of_logons}\n\n        if username:\n          values_dict['username'] = username\n        if fullname:\n          values_dict['full_name'] = fullname\n        if comments:\n          values_dict['comments'] = comments\n\n        event_data = windows_events.WindowsRegistryEventData()\n        event_data.key_path = registry_key.path\n        event_data.regvalue = values_dict\n        event_data.source_append = self._SOURCE_APPEND\n\n        event = time_events.DateTimeValuesEvent(\n            last_written_time, definitions.TIME_DESCRIPTION_WRITTEN)\n        parser_mediator.ProduceEventWithEventData(event, event_data)\n\n      event_data = SAMUsersWindowsRegistryEventData()\n      event_data.account_rid = f_value.rid\n      event_data.comments = comments\n      event_data.fullname = fullname\n      event_data.key_path = registry_key.path\n      event_data.login_count = f_value.number_of_logons\n      event_data.username = username\n\n      if f_value.last_login_time != 0:\n        date_time = dfdatetime_filetime.Filetime(\n            timestamp=f_value.last_login_time)\n        event = time_events.DateTimeValuesEvent(\n            date_time, definitions.TIME_DESCRIPTION_LAST_LOGIN)\n        parser_mediator.ProduceEventWithEventData(event, event_data)\n\n      if f_value.last_password_set_time != 0:\n        date_time = dfdatetime_filetime.Filetime(\n            timestamp=f_value.last_password_set_time)\n        event = time_events.DateTimeValuesEvent(\n            date_time, definitions.TIME_DESCRIPTION_LAST_PASSWORD_RESET)\n        parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Extracts events from a Windows Registry key."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse a page visit row.", "response": "def ParsePageVisitRow(self, parser_mediator, query, row, **unused_kwargs):\n    \"\"\"Parses a visited row.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      query (str): query that created the row.\n      row (sqlite3.Row): row.\n    \"\"\"\n    query_hash = hash(query)\n    was_http_non_get = self._GetRowValue(query_hash, row, 'http_non_get')\n\n    event_data = SafariHistoryPageVisitedEventData()\n    event_data.offset = self._GetRowValue(query_hash, row, 'id')\n    event_data.query = query\n    event_data.title = self._GetRowValue(query_hash, row, 'title')\n    event_data.url = self._GetRowValue(query_hash, row, 'url')\n    event_data.visit_count = self._GetRowValue(query_hash, row, 'visit_count')\n    event_data.was_http_non_get = bool(was_http_non_get)\n\n    timestamp = self._GetRowValue(query_hash, row, 'visit_time')\n    date_time = dfdatetime_cocoa_time.CocoaTime(timestamp=timestamp)\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_LAST_VISITED)\n    parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nretrieving the local path for a given inode.", "response": "def GetLocalPath(self, inode, cache, database):\n    \"\"\"Return local path for a given inode.\n\n    Args:\n      inode (int): inode number for the file.\n      cache (SQLiteCache): cache.\n      database (SQLiteDatabase): database.\n\n    Returns:\n      str: full path, including the filename of the given inode value.\n    \"\"\"\n    local_path = cache.GetResults('local_path')\n    if not local_path:\n      results = database.Query(self.LOCAL_PATH_CACHE_QUERY)\n\n      cache.CacheQueryResults(\n          results, 'local_path', 'child_inode_number',\n          ('parent_inode_number', 'filename'))\n      local_path = cache.GetResults('local_path')\n\n    parent, path = local_path.get(inode, [None, None])\n\n    # TODO: Read the local_sync_root from the sync_config.db and use that\n    # for a root value.\n    root_value = '%local_sync_root%/'\n\n    if not path:\n      return root_value\n\n    paths = []\n    while path:\n      paths.append(path)\n      parent, path = local_path.get(parent, [None, None])\n\n    if not paths:\n      return root_value\n\n    # Paths are built top level to root so we need to reverse the list to\n    # represent them in the traditional order.\n    paths.reverse()\n    return root_value + '/'.join(paths)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve the cloud path given a resource id.", "response": "def GetCloudPath(self, resource_id, cache, database):\n    \"\"\"Return cloud path given a resource id.\n\n    Args:\n      resource_id (str): resource identifier for the file.\n      cache (SQLiteCache): cache.\n      database (SQLiteDatabase): database.\n\n    Returns:\n      str: full path to the resource value.\n    \"\"\"\n    cloud_path = cache.GetResults('cloud_path')\n    if not cloud_path:\n      results = database.Query(self.CLOUD_PATH_CACHE_QUERY)\n\n      cache.CacheQueryResults(\n          results, 'cloud_path', 'resource_id', ('filename', 'parent'))\n      cloud_path = cache.GetResults('cloud_path')\n\n    if resource_id == 'folder:root':\n      return '/'\n\n    paths = []\n    parent_path, parent_id = cloud_path.get(resource_id, ['', ''])\n    while parent_path:\n      if parent_path == 'folder:root':\n        break\n      paths.append(parent_path)\n      parent_path, parent_id = cloud_path.get(parent_id, ['', ''])\n\n    if not paths:\n      return '/'\n\n    # Paths are built top level to root so we need to reverse the list to\n    # represent them in the traditional order.\n    paths.reverse()\n    return '/{0:s}/'.format('/'.join(paths))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing a cloud entry row.", "response": "def ParseCloudEntryRow(\n      self, parser_mediator, query, row, cache=None, database=None,\n      **unused_kwargs):\n    \"\"\"Parses a cloud entry row.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      query (str): query that created the row.\n      row (sqlite3.Row): row.\n      cache (SQLiteCache): cache.\n      database (SQLiteDatabase): database.\n    \"\"\"\n    query_hash = hash(query)\n\n    parent_resource_id = self._GetRowValue(\n        query_hash, row, 'parent_resource_id')\n    filename = self._GetRowValue(query_hash, row, 'filename')\n\n    cloud_path = self.GetCloudPath(parent_resource_id, cache, database)\n    cloud_filename = '{0:s}{1:s}'.format(cloud_path, filename)\n\n    event_data = GoogleDriveSnapshotCloudEntryEventData()\n    event_data.document_type = self._GetRowValue(query_hash, row, 'doc_type')\n    event_data.path = cloud_filename\n    event_data.query = query\n    event_data.shared = bool(self._GetRowValue(query_hash, row, 'shared'))\n    event_data.size = self._GetRowValue(query_hash, row, 'size')\n    event_data.url = self._GetRowValue(query_hash, row, 'url')\n\n    timestamp = self._GetRowValue(query_hash, row, 'modified')\n    date_time = dfdatetime_posix_time.PosixTime(timestamp=timestamp)\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_MODIFICATION)\n    parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    timestamp = self._GetRowValue(query_hash, row, 'created')\n    if timestamp:\n      date_time = dfdatetime_posix_time.PosixTime(timestamp=timestamp)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_CREATION)\n      parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ParseLocalEntryRow(\n      self, parser_mediator, query, row, cache=None, database=None,\n      **unused_kwargs):\n    \"\"\"Parses a local entry row.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      query (str): query that created the row.\n      row (sqlite3.Row): row.\n      cache (Optional[SQLiteCache]): cache.\n      database (Optional[SQLiteDatabase]): database.\n    \"\"\"\n    query_hash = hash(query)\n\n    inode_number = self._GetRowValue(query_hash, row, 'inode_number')\n    local_path = self.GetLocalPath(inode_number, cache, database)\n\n    event_data = GoogleDriveSnapshotLocalEntryEventData()\n    event_data.path = local_path\n    event_data.query = query\n    event_data.size = self._GetRowValue(query_hash, row, 'size')\n\n    timestamp = self._GetRowValue(query_hash, row, 'modified')\n    date_time = dfdatetime_posix_time.PosixTime(timestamp=timestamp)\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_MODIFICATION)\n    parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Parses a local entry row."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\npop an event source from the heap.", "response": "def PopEventSource(self):\n    \"\"\"Pops an event source from the heap.\n\n    Returns:\n      EventSource: an event source or None on if no event source is available.\n    \"\"\"\n    try:\n      _, _, event_source = heapq.heappop(self._heap)\n\n    except IndexError:\n      return None\n\n    return event_source"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\npush an event source onto the heap.", "response": "def PushEventSource(self, event_source):\n    \"\"\"Pushes an event source onto the heap.\n\n    Args:\n      event_source (EventSource): event source.\n    \"\"\"\n    if event_source.file_entry_type == (\n        dfvfs_definitions.FILE_ENTRY_TYPE_DIRECTORY):\n      weight = 1\n    else:\n      weight = 100\n\n    heap_values = (weight, time.time(), event_source)\n    heapq.heappush(self._heap, heap_values)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfilling the event source heap with the available written event sources.", "response": "def _FillEventSourceHeap(\n      self, storage_writer, event_source_heap, start_with_first=False):\n    \"\"\"Fills the event source heap with the available written event sources.\n\n    Args:\n      storage_writer (StorageWriter): storage writer for a session storage.\n      event_source_heap (_EventSourceHeap): event source heap.\n      start_with_first (Optional[bool]): True if the function should start\n          with the first written event source.\n    \"\"\"\n    if self._processing_profiler:\n      self._processing_profiler.StartTiming('fill_event_source_heap')\n\n    if self._processing_profiler:\n      self._processing_profiler.StartTiming('get_event_source')\n\n    if start_with_first:\n      event_source = storage_writer.GetFirstWrittenEventSource()\n    else:\n      event_source = storage_writer.GetNextWrittenEventSource()\n\n    if self._processing_profiler:\n      self._processing_profiler.StopTiming('get_event_source')\n\n    while event_source:\n      event_source_heap.PushEventSource(event_source)\n      if event_source_heap.IsFull():\n        break\n\n      if self._processing_profiler:\n        self._processing_profiler.StartTiming('get_event_source')\n\n      event_source = storage_writer.GetNextWrittenEventSource()\n\n      if self._processing_profiler:\n        self._processing_profiler.StopTiming('get_event_source')\n\n    if self._processing_profiler:\n      self._processing_profiler.StopTiming('fill_event_source_heap')"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmerges a task storage with the session storage.", "response": "def _MergeTaskStorage(self, storage_writer):\n    \"\"\"Merges a task storage with the session storage.\n\n    This function checks all task stores that are ready to merge and updates\n    the scheduled tasks. Note that to prevent this function holding up\n    the task scheduling loop only the first available task storage is merged.\n\n    Args:\n      storage_writer (StorageWriter): storage writer for a session storage used\n          to merge task storage.\n    \"\"\"\n    if self._processing_profiler:\n      self._processing_profiler.StartTiming('merge_check')\n\n    for task_identifier in storage_writer.GetProcessedTaskIdentifiers():\n      try:\n        task = self._task_manager.GetProcessedTaskByIdentifier(task_identifier)\n\n        self._task_manager.SampleTaskStatus(task, 'processed')\n\n        to_merge = self._task_manager.CheckTaskToMerge(task)\n        if not to_merge:\n          storage_writer.RemoveProcessedTaskStorage(task)\n\n          self._task_manager.RemoveTask(task)\n          self._task_manager.SampleTaskStatus(task, 'removed_processed')\n\n        else:\n          storage_writer.PrepareMergeTaskStorage(task)\n          self._task_manager.UpdateTaskAsPendingMerge(task)\n\n      except KeyError:\n        logger.error(\n            'Unable to retrieve task: {0:s} to prepare it to be merged.'.format(\n                task_identifier))\n        continue\n\n    if self._processing_profiler:\n      self._processing_profiler.StopTiming('merge_check')\n\n    task = None\n    if not self._storage_merge_reader_on_hold:\n      task = self._task_manager.GetTaskPendingMerge(self._merge_task)\n\n    # Limit the number of attribute containers from a single task-based\n    # storage file that are merged per loop to keep tasks flowing.\n    if task or self._storage_merge_reader:\n      self._status = definitions.STATUS_INDICATOR_MERGING\n\n      if self._processing_profiler:\n        self._processing_profiler.StartTiming('merge')\n\n      if task:\n        if self._storage_merge_reader:\n          self._merge_task_on_hold = self._merge_task\n          self._storage_merge_reader_on_hold = self._storage_merge_reader\n\n          self._task_manager.SampleTaskStatus(\n              self._merge_task_on_hold, 'merge_on_hold')\n\n        self._merge_task = task\n        try:\n          self._storage_merge_reader = storage_writer.StartMergeTaskStorage(\n              task)\n\n          self._task_manager.SampleTaskStatus(task, 'merge_started')\n\n        except IOError as exception:\n          logger.error((\n              'Unable to merge results of task: {0:s} '\n              'with error: {1!s}').format(task.identifier, exception))\n          self._storage_merge_reader = None\n\n      if self._storage_merge_reader:\n        fully_merged = self._storage_merge_reader.MergeAttributeContainers(\n            maximum_number_of_containers=self._MAXIMUM_NUMBER_OF_CONTAINERS)\n      else:\n        # TODO: Do something more sensible when this happens, perhaps\n        # retrying the task once that is implemented. For now, we mark the task\n        # as fully merged because we can't continue with it.\n        fully_merged = True\n\n      if self._processing_profiler:\n        self._processing_profiler.StopTiming('merge')\n\n      if fully_merged:\n        try:\n          self._task_manager.CompleteTask(self._merge_task)\n\n        except KeyError as exception:\n          logger.error(\n              'Unable to complete task: {0:s} with error: {1!s}'.format(\n                  self._merge_task.identifier, exception))\n\n        if not self._storage_merge_reader_on_hold:\n          self._merge_task = None\n          self._storage_merge_reader = None\n        else:\n          self._merge_task = self._merge_task_on_hold\n          self._storage_merge_reader = self._storage_merge_reader_on_hold\n\n          self._merge_task_on_hold = None\n          self._storage_merge_reader_on_hold = None\n\n          self._task_manager.SampleTaskStatus(\n              self._merge_task, 'merge_resumed')\n\n      self._status = definitions.STATUS_INDICATOR_RUNNING\n      self._number_of_produced_events = storage_writer.number_of_events\n      self._number_of_produced_sources = storage_writer.number_of_event_sources\n      self._number_of_produced_warnings = storage_writer.number_of_warnings"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _ProcessSources(\n      self, source_path_specs, storage_writer, filter_find_specs=None):\n    \"\"\"Processes the sources.\n\n    Args:\n      source_path_specs (list[dfvfs.PathSpec]): path specifications of\n          the sources to process.\n      storage_writer (StorageWriter): storage writer for a session storage.\n      filter_find_specs (Optional[list[dfvfs.FindSpec]]): find specifications\n          used in path specification extraction. If set, path specifications\n          that match the find specification will be processed.\n    \"\"\"\n    if self._processing_profiler:\n      self._processing_profiler.StartTiming('process_sources')\n\n    self._status = definitions.STATUS_INDICATOR_COLLECTING\n    self._number_of_consumed_event_tags = 0\n    self._number_of_consumed_events = 0\n    self._number_of_consumed_reports = 0\n    self._number_of_consumed_sources = 0\n    self._number_of_consumed_warnings = 0\n    self._number_of_produced_event_tags = 0\n    self._number_of_produced_events = 0\n    self._number_of_produced_reports = 0\n    self._number_of_produced_sources = 0\n    self._number_of_produced_warnings = 0\n\n    path_spec_generator = self._path_spec_extractor.ExtractPathSpecs(\n        source_path_specs, find_specs=filter_find_specs,\n        recurse_file_system=False, resolver_context=self._resolver_context)\n\n    for path_spec in path_spec_generator:\n      if self._abort:\n        break\n\n      # TODO: determine if event sources should be DataStream or FileEntry\n      # or both.\n      event_source = event_sources.FileEntryEventSource(path_spec=path_spec)\n      storage_writer.AddEventSource(event_source)\n\n      self._number_of_produced_sources = storage_writer.number_of_event_sources\n\n      # Update the foreman process status in case we are using a filter file.\n      self._UpdateForemanProcessStatus()\n\n      if self._status_update_callback:\n        self._status_update_callback(self._processing_status)\n\n    self._ScheduleTasks(storage_writer)\n\n    if self._abort:\n      self._status = definitions.STATUS_INDICATOR_ABORTED\n    else:\n      self._status = definitions.STATUS_INDICATOR_COMPLETED\n\n    self._number_of_produced_events = storage_writer.number_of_events\n    self._number_of_produced_sources = storage_writer.number_of_event_sources\n    self._number_of_produced_warnings = storage_writer.number_of_warnings\n\n    if self._processing_profiler:\n      self._processing_profiler.StopTiming('process_sources')\n\n    # Update the foreman process and task status in case we are using\n    # a filter file.\n    self._UpdateForemanProcessStatus()\n\n    tasks_status = self._task_manager.GetStatusInformation()\n    if self._task_queue_profiler:\n      self._task_queue_profiler.Sample(tasks_status)\n\n    self._processing_status.UpdateTasksStatus(tasks_status)\n\n    if self._status_update_callback:\n      self._status_update_callback(self._processing_status)", "response": "Processes the sources.\n\n    Args:\n      source_path_specs (list[dfvfs.PathSpec]): path specifications of\n          the sources to process.\n      storage_writer (StorageWriter): storage writer for a session storage.\n      filter_find_specs (Optional[list[dfvfs.FindSpec]]): find specifications\n          used in path specification extraction. If set, path specifications\n          that match the find specification will be processed."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nschedules a task. Args: task (Task): task. Returns: bool: True if the task was scheduled.", "response": "def _ScheduleTask(self, task):\n    \"\"\"Schedules a task.\n\n    Args:\n      task (Task): task.\n\n    Returns:\n      bool: True if the task was scheduled.\n    \"\"\"\n    if self._processing_profiler:\n      self._processing_profiler.StartTiming('schedule_task')\n\n    try:\n      self._task_queue.PushItem(task, block=False)\n      is_scheduled = True\n\n    except errors.QueueFull:\n      is_scheduled = False\n\n    if self._processing_profiler:\n      self._processing_profiler.StopTiming('schedule_task')\n\n    return is_scheduled"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nschedules tasks. Args: storage_writer (StorageWriter): storage writer for a session storage.", "response": "def _ScheduleTasks(self, storage_writer):\n    \"\"\"Schedules tasks.\n\n    Args:\n      storage_writer (StorageWriter): storage writer for a session storage.\n    \"\"\"\n    logger.debug('Task scheduler started')\n\n    self._status = definitions.STATUS_INDICATOR_RUNNING\n\n    # TODO: make tasks persistent.\n\n    # TODO: protect task scheduler loop by catch all and\n    # handle abort path.\n\n    event_source_heap = _EventSourceHeap()\n\n    self._FillEventSourceHeap(\n        storage_writer, event_source_heap, start_with_first=True)\n\n    event_source = event_source_heap.PopEventSource()\n\n    task = None\n    while event_source or self._task_manager.HasPendingTasks():\n      if self._abort:\n        break\n\n      try:\n        if not task:\n          task = self._task_manager.CreateRetryTask()\n\n        if not task and event_source:\n          task = self._task_manager.CreateTask(self._session_identifier)\n          task.file_entry_type = event_source.file_entry_type\n          task.path_spec = event_source.path_spec\n          event_source = None\n\n          self._number_of_consumed_sources += 1\n\n          if self._guppy_memory_profiler:\n            self._guppy_memory_profiler.Sample()\n\n        if task:\n          if self._ScheduleTask(task):\n            logger.debug(\n                'Scheduled task {0:s} for path specification {1:s}'.format(\n                    task.identifier, task.path_spec.comparable))\n\n            self._task_manager.SampleTaskStatus(task, 'scheduled')\n\n            task = None\n\n          else:\n            self._task_manager.SampleTaskStatus(task, 'schedule_attempted')\n\n        self._MergeTaskStorage(storage_writer)\n\n        if not event_source_heap.IsFull():\n          self._FillEventSourceHeap(storage_writer, event_source_heap)\n\n        if not task and not event_source:\n          event_source = event_source_heap.PopEventSource()\n\n      except KeyboardInterrupt:\n        self._abort = True\n\n        self._processing_status.aborted = True\n        if self._status_update_callback:\n          self._status_update_callback(self._processing_status)\n\n    for task in self._task_manager.GetFailedTasks():\n      warning = warnings.ExtractionWarning(\n          message='Worker failed to process path specification',\n          path_spec=task.path_spec)\n      self._storage_writer.AddWarning(warning)\n      self._processing_status.error_path_specs.append(task.path_spec)\n\n    self._status = definitions.STATUS_INDICATOR_IDLE\n\n    if self._abort:\n      logger.debug('Task scheduler aborted')\n    else:\n      logger.debug('Task scheduler stopped')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _StartWorkerProcess(self, process_name, storage_writer):\n    process_name = 'Worker_{0:02d}'.format(self._last_worker_number)\n    logger.debug('Starting worker process {0:s}'.format(process_name))\n\n    if self._use_zeromq:\n      queue_name = '{0:s} task queue'.format(process_name)\n      task_queue = zeromq_queue.ZeroMQRequestConnectQueue(\n          delay_open=True, linger_seconds=0, name=queue_name,\n          port=self._task_queue_port,\n          timeout_seconds=self._TASK_QUEUE_TIMEOUT_SECONDS)\n    else:\n      task_queue = self._task_queue\n\n    process = worker_process.WorkerProcess(\n        task_queue, storage_writer, self._artifacts_filter_helper,\n        self.knowledge_base, self._session_identifier,\n        self._processing_configuration,\n        enable_sigsegv_handler=self._enable_sigsegv_handler, name=process_name)\n\n    # Remove all possible log handlers to prevent a child process from logging\n    # to the main process log file and garbling the log. The log handlers are\n    # recreated after the worker process has been started.\n    for handler in logging.root.handlers:\n      logging.root.removeHandler(handler)\n      handler.close()\n\n    process.start()\n\n    loggers.ConfigureLogging(\n        debug_output=self._debug_output, filename=self._log_filename,\n        mode='a', quiet_mode=self._quiet_mode)\n\n    try:\n      self._StartMonitoringProcess(process)\n\n    except (IOError, KeyError) as exception:\n      pid = process.pid\n      logger.error((\n          'Unable to monitor replacement worker process: {0:s} '\n          '(PID: {1:d}) with error: {2!s}').format(\n              process_name, pid, exception))\n\n      self._TerminateProcess(process)\n      return None\n\n    self._RegisterProcess(process)\n\n    self._last_worker_number += 1\n\n    return process", "response": "Starts a worker process."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstopping the extraction processes.", "response": "def _StopExtractionProcesses(self, abort=False):\n    \"\"\"Stops the extraction processes.\n\n    Args:\n      abort (bool): True to indicated the stop is issued on abort.\n    \"\"\"\n    logger.debug('Stopping extraction processes.')\n    self._StopMonitoringProcesses()\n\n    # Note that multiprocessing.Queue is very sensitive regarding\n    # blocking on either a get or a put. So we try to prevent using\n    # any blocking behavior.\n\n    if abort:\n      # Signal all the processes to abort.\n      self._AbortTerminate()\n\n    logger.debug('Emptying task queue.')\n    self._task_queue.Empty()\n\n    # Wake the processes to make sure that they are not blocking\n    # waiting for the queue new items.\n    for _ in self._processes_per_pid:\n      try:\n        self._task_queue.PushItem(plaso_queue.QueueAbort(), block=False)\n      except errors.QueueFull:\n        logger.warning('Task queue full, unable to push abort message.')\n\n    # Try waiting for the processes to exit normally.\n    self._AbortJoin(timeout=self._PROCESS_JOIN_TIMEOUT)\n    self._task_queue.Close(abort=abort)\n\n    if not abort:\n      # Check if the processes are still alive and terminate them if necessary.\n      self._AbortTerminate()\n      self._AbortJoin(timeout=self._PROCESS_JOIN_TIMEOUT)\n      self._task_queue.Close(abort=True)\n\n    # Kill any lingering processes.\n    self._AbortKill()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ProcessSources(\n      self, session_identifier, source_path_specs, storage_writer,\n      processing_configuration, enable_sigsegv_handler=False,\n      filter_find_specs=None, number_of_worker_processes=0,\n      status_update_callback=None, worker_memory_limit=None):\n    \"\"\"Processes the sources and extract events.\n\n    Args:\n      session_identifier (str): identifier of the session.\n      source_path_specs (list[dfvfs.PathSpec]): path specifications of\n          the sources to process.\n      storage_writer (StorageWriter): storage writer for a session storage.\n      processing_configuration (ProcessingConfiguration): processing\n          configuration.\n      enable_sigsegv_handler (Optional[bool]): True if the SIGSEGV handler\n          should be enabled.\n      filter_find_specs (Optional[list[dfvfs.FindSpec]]): find specifications\n          used in path specification extraction.\n      number_of_worker_processes (Optional[int]): number of worker processes.\n      status_update_callback (Optional[function]): callback function for status\n          updates.\n      worker_memory_limit (Optional[int]): maximum amount of memory a worker is\n          allowed to consume, where None represents the default memory limit\n          and 0 represents no limit.\n\n    Returns:\n      ProcessingStatus: processing status.\n    \"\"\"\n    if number_of_worker_processes < 1:\n      # One worker for each \"available\" CPU (minus other processes).\n      # The number here is derived from the fact that the engine starts up:\n      # * A main process.\n      #\n      # If we want to utilize all CPUs on the system we therefore need to start\n      # up workers that amounts to the total number of CPUs - the other\n      # processes.\n      try:\n        cpu_count = multiprocessing.cpu_count() - 1\n\n        if cpu_count <= self._WORKER_PROCESSES_MINIMUM:\n          cpu_count = self._WORKER_PROCESSES_MINIMUM\n\n        elif cpu_count >= self._WORKER_PROCESSES_MAXIMUM:\n          cpu_count = self._WORKER_PROCESSES_MAXIMUM\n\n      except NotImplementedError:\n        logger.error((\n            'Unable to determine number of CPUs defaulting to {0:d} worker '\n            'processes.').format(self._WORKER_PROCESSES_MINIMUM))\n        cpu_count = self._WORKER_PROCESSES_MINIMUM\n\n      number_of_worker_processes = cpu_count\n\n    self._enable_sigsegv_handler = enable_sigsegv_handler\n    self._number_of_worker_processes = number_of_worker_processes\n\n    if worker_memory_limit is None:\n      self._worker_memory_limit = definitions.DEFAULT_WORKER_MEMORY_LIMIT\n    else:\n      self._worker_memory_limit = worker_memory_limit\n\n    # Keep track of certain values so we can spawn new extraction workers.\n    self._processing_configuration = processing_configuration\n\n    self._debug_output = processing_configuration.debug_output\n    self._filter_find_specs = filter_find_specs\n    self._log_filename = processing_configuration.log_filename\n    self._session_identifier = session_identifier\n    self._status_update_callback = status_update_callback\n    self._storage_writer = storage_writer\n\n    # Set up the task queue.\n    if not self._use_zeromq:\n      self._task_queue = multi_process_queue.MultiProcessingQueue(\n          maximum_number_of_queued_items=self._maximum_number_of_tasks)\n\n    else:\n      task_outbound_queue = zeromq_queue.ZeroMQBufferedReplyBindQueue(\n          delay_open=True, linger_seconds=0, maximum_items=1,\n          name='main_task_queue',\n          timeout_seconds=self._ZEROMQ_NO_WORKER_REQUEST_TIME_SECONDS)\n      self._task_queue = task_outbound_queue\n\n      # The ZeroMQ backed queue must be started first, so we can save its port.\n      # TODO: raises: attribute-defined-outside-init\n      # self._task_queue.name = 'Task queue'\n      self._task_queue.Open()\n      self._task_queue_port = self._task_queue.port\n\n    self._StartProfiling(self._processing_configuration.profiling)\n    self._task_manager.StartProfiling(\n        self._processing_configuration.profiling, self._name)\n\n    if self._serializers_profiler:\n      storage_writer.SetSerializersProfiler(self._serializers_profiler)\n\n    if self._storage_profiler:\n      storage_writer.SetStorageProfiler(self._storage_profiler)\n\n    # Set up the storage writer before the worker processes.\n    storage_writer.StartTaskStorage()\n\n    for worker_number in range(number_of_worker_processes):\n      # First argument to _StartWorkerProcess is not used.\n      extraction_process = self._StartWorkerProcess('', storage_writer)\n      if not extraction_process:\n        logger.error('Unable to create worker process: {0:d}'.format(\n            worker_number))\n\n    self._StartStatusUpdateThread()\n\n    try:\n      # Open the storage file after creating the worker processes otherwise\n      # the ZIP storage file will remain locked as long as the worker processes\n      # are alive.\n      storage_writer.Open()\n      storage_writer.WriteSessionStart()\n\n      try:\n        storage_writer.WritePreprocessingInformation(self.knowledge_base)\n\n        self._ProcessSources(\n            source_path_specs, storage_writer,\n            filter_find_specs=filter_find_specs)\n\n      finally:\n        storage_writer.WriteSessionCompletion(aborted=self._abort)\n\n        storage_writer.Close()\n\n    finally:\n      # Stop the status update thread after close of the storage writer\n      # so we include the storage sync to disk in the status updates.\n      self._StopStatusUpdateThread()\n\n      if self._serializers_profiler:\n        storage_writer.SetSerializersProfiler(None)\n\n      if self._storage_profiler:\n        storage_writer.SetStorageProfiler(None)\n\n      self._task_manager.StopProfiling()\n      self._StopProfiling()\n\n    try:\n      self._StopExtractionProcesses(abort=self._abort)\n\n    except KeyboardInterrupt:\n      self._AbortKill()\n\n      # The abort can leave the main process unresponsive\n      # due to incorrectly finalized IPC.\n      self._KillProcess(os.getpid())\n\n    # The task queue should be closed by _StopExtractionProcesses, this\n    # close is a failsafe, primarily due to MultiProcessingQueue's\n    # blocking behavior.\n    self._task_queue.Close(abort=True)\n\n    if self._processing_status.error_path_specs:\n      task_storage_abort = True\n    else:\n      task_storage_abort = self._abort\n\n    try:\n      storage_writer.StopTaskStorage(abort=task_storage_abort)\n    except (IOError, OSError) as exception:\n      logger.error('Unable to stop task storage with error: {0!s}'.format(\n          exception))\n\n    if self._abort:\n      logger.debug('Processing aborted.')\n      self._processing_status.aborted = True\n    else:\n      logger.debug('Processing completed.')\n\n    # Reset values.\n    self._enable_sigsegv_handler = None\n    self._number_of_worker_processes = None\n    self._worker_memory_limit = definitions.DEFAULT_WORKER_MEMORY_LIMIT\n\n    self._processing_configuration = None\n\n    self._filter_find_specs = None\n    self._session_identifier = None\n    self._status_update_callback = None\n    self._storage_writer = None\n\n    return self._processing_status", "response": "Processes the sources and extract events."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef GetMessages(self, formatter_mediator, event):\n    if self.DATA_TYPE != event.data_type:\n      raise errors.WrongFormatter('Unsupported data type: {0:s}.'.format(\n          event.data_type))\n\n    event_values = event.CopyToDict()\n\n    login_type = event_values.get('type', None)\n    if login_type is None:\n      status = 'N/A'\n    else:\n      status = self._STATUS_TYPES.get(login_type, 'UNKNOWN')\n\n    event_values['status'] = status\n\n    return self._ConditionalFormatMessages(event_values)", "response": "Determines the formatted message strings for an event object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse a message row.", "response": "def ParseMessageRow(self, parser_mediator, query, row, **unused_kwargs):\n    \"\"\"Parses a message row.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      query (str): query that created the row.\n      row (sqlite3.Row): row.\n    \"\"\"\n    query_hash = hash(query)\n\n    event_data = KikIOSMessageEventData()\n    event_data.body = self._GetRowValue(query_hash, row, 'ZBODY')\n    event_data.displayname = self._GetRowValue(query_hash, row, 'ZDISPLAYNAME')\n    event_data.message_status = self._GetRowValue(query_hash, row, 'ZSTATE')\n    event_data.message_type = self._GetRowValue(query_hash, row, 'ZTYPE')\n    event_data.offset = self._GetRowValue(query_hash, row, 'id')\n    event_data.query = query\n    event_data.username = self._GetRowValue(query_hash, row, 'ZUSERNAME')\n\n    timestamp = self._GetRowValue(query_hash, row, 'ZRECEIVEDTIMESTAMP')\n    # Convert the floating point value to an integer.\n    timestamp = int(timestamp)\n    date_time = dfdatetime_cocoa_time.CocoaTime(timestamp=timestamp)\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_CREATION)\n    parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _GetEventData(\n      self, parser_mediator, record_index, evt_record, recovered=False):\n    \"\"\"Retrieves event data from the Windows EventLog (EVT) record.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      record_index (int): event record index.\n      evt_record (pyevt.record): event record.\n      recovered (Optional[bool]): True if the record was recovered.\n\n    Returns:\n      WinEvtRecordEventData: event data.\n    \"\"\"\n    event_data = WinEvtRecordEventData()\n\n    try:\n      event_data.record_number = evt_record.identifier\n    except OverflowError as exception:\n      parser_mediator.ProduceExtractionWarning((\n          'unable to read record identifier from event record: {0:d} '\n          'with error: {1!s}').format(record_index, exception))\n\n    try:\n      event_identifier = evt_record.event_identifier\n    except OverflowError as exception:\n      parser_mediator.ProduceExtractionWarning((\n          'unable to read event identifier from event record: {0:d} '\n          'with error: {1!s}').format(record_index, exception))\n\n      event_identifier = None\n\n    event_data.offset = evt_record.offset\n    event_data.recovered = recovered\n\n    # We want the event identifier to match the behavior of that of the EVTX\n    # event records.\n    if event_identifier is not None:\n      event_data.event_identifier = event_identifier & 0xffff\n      event_data.facility = (event_identifier >> 16) & 0x0fff\n      event_data.severity = event_identifier >> 30\n      event_data.message_identifier = event_identifier\n\n    event_data.event_type = evt_record.event_type\n    event_data.event_category = evt_record.event_category\n    event_data.source_name = evt_record.source_name\n\n    # Computer name is the value stored in the event record and does not\n    # necessarily corresponds with the actual hostname.\n    event_data.computer_name = evt_record.computer_name\n    event_data.user_sid = evt_record.user_security_identifier\n\n    event_data.strings = list(evt_record.strings)\n\n    return event_data", "response": "Extracts event data from the event log record."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses a Windows EventLog record.", "response": "def _ParseRecord(\n      self, parser_mediator, record_index, evt_record, recovered=False):\n    \"\"\"Parses a Windows EventLog (EVT) record.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      record_index (int): event record index.\n      evt_record (pyevt.record): event record.\n      recovered (Optional[bool]): True if the record was recovered.\n    \"\"\"\n    event_data = self._GetEventData(\n        parser_mediator, record_index, evt_record, recovered=recovered)\n\n    try:\n      creation_time = evt_record.get_creation_time_as_integer()\n    except OverflowError as exception:\n      parser_mediator.ProduceExtractionWarning((\n          'unable to read creation time from event record: {0:d} '\n          'with error: {1!s}').format(record_index, exception))\n\n      creation_time = None\n\n    if creation_time:\n      date_time = dfdatetime_posix_time.PosixTime(timestamp=creation_time)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_CREATION)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    try:\n      written_time = evt_record.get_written_time_as_integer()\n    except OverflowError as exception:\n      parser_mediator.ProduceExtractionWarning((\n          'unable to read written time from event record: {0:d} '\n          'with error: {1!s}').format(record_index, exception))\n\n      written_time = None\n\n    if written_time:\n      date_time = dfdatetime_posix_time.PosixTime(timestamp=written_time)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_WRITTEN)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    if not creation_time and not written_time:\n      date_time = dfdatetime_semantic_time.SemanticTime('Not set')\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_NOT_A_TIME)\n      parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _ParseRecords(self, parser_mediator, evt_file):\n    # To handle errors when parsing a Windows EventLog (EVT) file in the most\n    # granular way the following code iterates over every event record. The\n    # call to evt_file.get_record() and access to members of evt_record should\n    # be called within a try-except.\n\n    for record_index in range(evt_file.number_of_records):\n      if parser_mediator.abort:\n        break\n\n      try:\n        evt_record = evt_file.get_record(record_index)\n        self._ParseRecord(parser_mediator, record_index, evt_record)\n      except IOError as exception:\n        parser_mediator.ProduceExtractionWarning(\n            'unable to parse event record: {0:d} with error: {1!s}'.format(\n                record_index, exception))\n\n    for record_index in range(evt_file.number_of_recovered_records):\n      if parser_mediator.abort:\n        break\n\n      try:\n        evt_record = evt_file.get_recovered_record(record_index)\n        self._ParseRecord(\n            parser_mediator, record_index, evt_record, recovered=True)\n      except IOError as exception:\n        parser_mediator.ProduceExtractionWarning((\n            'unable to parse recovered event record: {0:d} with error: '\n            '{1!s}').format(record_index, exception))", "response": "Parses the event log records."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ParseFileObject(self, parser_mediator, file_object):\n    evt_file = pyevt.file()\n    evt_file.set_ascii_codepage(parser_mediator.codepage)\n\n    try:\n      evt_file.open_file_object(file_object)\n    except IOError as exception:\n      parser_mediator.ProduceExtractionWarning(\n          'unable to open file with error: {0!s}'.format(exception))\n      return\n\n    try:\n      self._ParseRecords(parser_mediator, evt_file)\n    finally:\n      evt_file.close()", "response": "Parses a Windows EventLog file - like object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nverify whether the content corresponds to a Zsh extended_history file.", "response": "def VerifyStructure(self, parser_mediator, lines):\n    \"\"\"Verifies whether content corresponds to a Zsh extended_history file.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      lines (str): one or more lines from the text file.\n\n    Returns:\n      bool: True if the line was successfully parsed.\n    \"\"\"\n    if self._VERIFICATION_REGEX.match(lines):\n      return True\n\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef GetValueByPath(self, path_segments):\n    key = self.root_key\n    for path_segment in path_segments:\n      if isinstance(key, dict):\n        try:\n          key = key[path_segment]\n        except KeyError:\n          return None\n\n      elif isinstance(key, list):\n        try:\n          list_index = int(path_segment, 10)\n        except ValueError:\n          return None\n\n        key = key[list_index]\n\n      else:\n        return None\n\n      if not key:\n        return None\n\n    return key", "response": "Retrieves a value by path."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef Read(self, file_object):\n    try:\n      self.root_key = biplist.readPlist(file_object)\n\n    except (\n        biplist.NotBinaryPlistException,\n        biplist.InvalidPlistException) as exception:\n      raise IOError(exception)", "response": "Reads a plist from a file - like object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndetermine the formatted message string.", "response": "def _FormatMessage(self, format_string, event_values):\n    \"\"\"Determines the formatted message string.\n\n    Args:\n      format_string (str): message format string.\n      event_values (dict[str, object]): event values.\n\n    Returns:\n      str: formatted message string.\n    \"\"\"\n    if not isinstance(format_string, py2to3.UNICODE_TYPE):\n      logger.warning('Format string: {0:s} is non-Unicode.'.format(\n          format_string))\n\n      # Plaso code files should be in UTF-8 any thus binary strings are\n      # assumed UTF-8. If this is not the case this should be fixed.\n      format_string = format_string.decode('utf-8', errors='ignore')\n\n    try:\n      message_string = format_string.format(**event_values)\n\n    except KeyError as exception:\n      data_type = event_values.get('data_type', 'N/A')\n      display_name = event_values.get('display_name', 'N/A')\n      event_identifier = event_values.get('uuid', 'N/A')\n      parser_chain = event_values.get('parser', 'N/A')\n\n      error_message = (\n          'unable to format string: \"{0:s}\" event object is missing required '\n          'attributes: {1!s}').format(format_string, exception)\n      error_message = (\n          'Event: {0:s} data type: {1:s} display name: {2:s} '\n          'parser chain: {3:s} with error: {4:s}').format(\n              event_identifier, data_type, display_name, parser_chain,\n              error_message)\n      logger.error(error_message)\n\n      attribute_values = []\n      for attribute, value in iter(event_values.items()):\n        attribute_values.append('{0:s}: {1!s}'.format(attribute, value))\n\n      message_string = ' '.join(attribute_values)\n\n    except UnicodeDecodeError as exception:\n      data_type = event_values.get('data_type', 'N/A')\n      display_name = event_values.get('display_name', 'N/A')\n      event_identifier = event_values.get('uuid', 'N/A')\n      parser_chain = event_values.get('parser', 'N/A')\n\n      error_message = 'Unicode decode error: {0!s}'.format(exception)\n      error_message = (\n          'Event: {0:s} data type: {1:s} display name: {2:s} '\n          'parser chain: {3:s} with error: {4:s}').format(\n              event_identifier, data_type, display_name, parser_chain,\n              error_message)\n      logger.error(error_message)\n\n      message_string = ''\n\n    # Strip carriage return and linefeed form the message strings.\n    # Using replace function here because it is faster than re.sub() or\n    # string.strip().\n    return message_string.replace('\\r', '').replace('\\n', '')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndetermine the formatted message strings.", "response": "def _FormatMessages(self, format_string, short_format_string, event_values):\n    \"\"\"Determines the formatted message strings.\n\n    Args:\n      format_string (str): message format string.\n      short_format_string (str): short message format string.\n      event_values (dict[str, object]): event values.\n\n    Returns:\n      tuple(str, str): formatted message string and short message string.\n    \"\"\"\n    message_string = self._FormatMessage(format_string, event_values)\n\n    if short_format_string:\n      short_message_string = self._FormatMessage(\n          short_format_string, event_values)\n    else:\n      short_message_string = message_string\n\n    # Truncate the short message string if necessary.\n    if len(short_message_string) > 80:\n      short_message_string = '{0:s}...'.format(short_message_string[:77])\n\n    return message_string, short_message_string"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef GetFormatStringAttributeNames(self):\n    if self._format_string_attribute_names is None:\n      self._format_string_attribute_names = (\n          self._FORMAT_STRING_ATTRIBUTE_NAME_RE.findall(\n              self.FORMAT_STRING))\n\n    return set(self._format_string_attribute_names)", "response": "Retrieves the attribute names in the format string."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndetermines the formatted message strings for an event object.", "response": "def GetMessages(self, formatter_mediator, event):\n    \"\"\"Determines the formatted message strings for an event object.\n\n    Args:\n      formatter_mediator (FormatterMediator): mediates the interactions\n          between formatters and other components, such as storage and Windows\n          EventLog resources.\n      event (EventObject): event.\n\n    Returns:\n      tuple(str, str): formatted message string and short message string.\n\n    Raises:\n      WrongFormatter: if the event object cannot be formatted by the formatter.\n    \"\"\"\n    if self.DATA_TYPE != event.data_type:\n      raise errors.WrongFormatter('Unsupported data type: {0:s}.'.format(\n          event.data_type))\n\n    event_values = event.CopyToDict()\n    return self._FormatMessages(\n        self.FORMAT_STRING, self.FORMAT_STRING_SHORT, event_values)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef GetSources(self, event):\n    if self.DATA_TYPE != event.data_type:\n      raise errors.WrongFormatter('Unsupported data type: {0:s}.'.format(\n          event.data_type))\n\n    return self.SOURCE_SHORT, self.SOURCE_LONG", "response": "Determines the short and long source string for an event object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _ConditionalFormatMessages(self, event_values):\n    # Using getattr here to make sure the attribute is not set to None.\n    # if A.b = None, hasattr(A, b) is True but getattr(A, b, None) is False.\n    string_pieces = []\n    for map_index, attribute_name in enumerate(self._format_string_pieces_map):\n      if not attribute_name or attribute_name in event_values:\n        if attribute_name:\n          attribute = event_values.get(attribute_name, None)\n          # If an attribute is an int, yet has zero value we want to include\n          # that in the format string, since that is still potentially valid\n          # information. Otherwise we would like to skip it.\n          # pylint: disable=unidiomatic-typecheck\n          if (not isinstance(attribute, (bool, float)) and\n              not isinstance(attribute, py2to3.INTEGER_TYPES) and\n              not attribute):\n            continue\n\n        string_pieces.append(self.FORMAT_STRING_PIECES[map_index])\n\n    format_string = self.FORMAT_STRING_SEPARATOR.join(string_pieces)\n\n    string_pieces = []\n    for map_index, attribute_name in enumerate(\n        self._format_string_short_pieces_map):\n      if not attribute_name or event_values.get(attribute_name, None):\n        string_pieces.append(self.FORMAT_STRING_SHORT_PIECES[map_index])\n    short_format_string = self.FORMAT_STRING_SEPARATOR.join(string_pieces)\n\n    return self._FormatMessages(\n        format_string, short_format_string, event_values)", "response": "Determines the conditional formatted message strings."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef GetFormatStringAttributeNames(self):\n    if self._format_string_attribute_names is None:\n      self._format_string_attribute_names = []\n      for format_string_piece in self.FORMAT_STRING_PIECES:\n        attribute_names = self._FORMAT_STRING_ATTRIBUTE_NAME_RE.findall(\n            format_string_piece)\n\n        if attribute_names:\n          self._format_string_attribute_names.extend(attribute_names)\n\n    return set(self._format_string_attribute_names)", "response": "Retrieves the attribute names in the format string."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef GetMessages(self, formatter_mediator, event):\n    if self.DATA_TYPE != event.data_type:\n      raise errors.WrongFormatter('Unsupported data type: {0:s}.'.format(\n          event.data_type))\n\n    event_values = event.CopyToDict()\n    return self._ConditionalFormatMessages(event_values)", "response": "Determines the formatted message strings for an event object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a date time filter range to the list of time objects.", "response": "def AddDateTimeRange(\n      self, time_value, start_time_string=None, end_time_string=None):\n    \"\"\"Adds a date time filter range.\n\n    The time strings are formatted as:\n    YYYY-MM-DD hh:mm:ss.######[+-]##:##\n    Where # are numeric digits ranging from 0 to 9 and the seconds\n    fraction can be either 3 or 6 digits. The time of day, seconds fraction\n    and timezone offset are optional. The default timezone is UTC.\n\n    Args:\n      time_value (str): time value, such as, atime, ctime, crtime, dtime, bkup\n          and mtime.\n      start_time_string (str): start date and time value string.\n      end_time_string (str): end date and time value string.\n\n    Raises:\n      ValueError: If the filter is badly formed.\n    \"\"\"\n    if not isinstance(time_value, py2to3.STRING_TYPES):\n      raise ValueError('Filter type must be a string.')\n\n    if start_time_string is None and end_time_string is None:\n      raise ValueError(\n          'Filter must have either a start or an end date time value.')\n\n    time_value_lower = time_value.lower()\n    if time_value_lower not in self._SUPPORTED_TIME_VALUES:\n      raise ValueError('Unsupported time value: {0:s}.'.format(time_value))\n\n    start_date_time = None\n    if start_time_string:\n      start_date_time = time_elements.TimeElementsInMicroseconds()\n      start_date_time.CopyFromDateTimeString(start_time_string)\n\n    end_date_time = None\n    if end_time_string:\n      end_date_time = time_elements.TimeElementsInMicroseconds()\n      end_date_time.CopyFromDateTimeString(end_time_string)\n\n    # Make sure that the end timestamp occurs after the beginning.\n    # If not then we need to reverse the time range.\n    if (None not in (start_date_time, end_date_time) and\n        start_date_time > end_date_time):\n      raise ValueError(\n          'Invalid date time value start must be earlier than end.')\n\n    self._date_time_ranges.append(self._DATE_TIME_RANGE_TUPLE(\n        time_value_lower, start_date_time, end_date_time))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomparing the file entry against the filter.", "response": "def Matches(self, file_entry):\n    \"\"\"Compares the file entry against the filter.\n\n    Args:\n      file_entry (dfvfs.FileEntry): file entry to compare.\n\n    Returns:\n      bool: True if the file entry matches the filter, False if not or\n          None if the filter does not apply.\n    \"\"\"\n    if not self._date_time_ranges:\n      return None\n\n    for date_time_range in self._date_time_ranges:\n      time_attribute = self._TIME_VALUE_MAPPINGS.get(\n          date_time_range.time_value, None)\n      if not time_attribute:\n        continue\n\n      timestamp = getattr(file_entry, time_attribute, None)\n      if timestamp is None:\n        continue\n\n      if (date_time_range.start_date_time is not None and\n          timestamp < date_time_range.start_date_time):\n        return False\n\n      if (date_time_range.end_date_time is not None and\n          timestamp > date_time_range.end_date_time):\n        return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef Print(self, output_writer):\n    if self._date_time_ranges:\n      for date_time_range in self._date_time_ranges:\n        if date_time_range.start_date_time is None:\n          end_time_string = date_time_range.end_date_time.CopyToDateTimeString()\n          output_writer.Write('\\t{0:s} after {1:s}\\n'.format(\n              date_time_range.time_value, end_time_string))\n\n        elif date_time_range.end_date_time is None:\n          start_time_string = (\n              date_time_range.start_date_time.CopyToDateTimeString())\n          output_writer.Write('\\t{0:s} before {1:s}\\n'.format(\n              date_time_range.time_value, start_time_string))\n\n        else:\n          start_time_string = (\n              date_time_range.start_date_time.CopyToDateTimeString())\n          end_time_string = date_time_range.end_date_time.CopyToDateTimeString()\n          output_writer.Write('\\t{0:s} between {1:s} and {2:s}\\n'.format(\n              date_time_range.time_value, start_time_string,\n              end_time_string))", "response": "Prints a human readable version of the filter."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompare the file entry against the filter.", "response": "def Matches(self, file_entry):\n    \"\"\"Compares the file entry against the filter.\n\n    Args:\n      file_entry (dfvfs.FileEntry): file entry to compare.\n\n    Returns:\n      bool: True if the file entry matches the filter, False if not or\n          None if the filter does not apply.\n    \"\"\"\n    location = getattr(file_entry.path_spec, 'location', None)\n    if not location:\n      return None\n\n    if '.' not in location:\n      return False\n\n    _, _, extension = location.rpartition('.')\n    return extension.lower() in self._extensions"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nprinting a human readable version of the filter.", "response": "def Print(self, output_writer):\n    \"\"\"Prints a human readable version of the filter.\n\n    Args:\n      output_writer (CLIOutputWriter): output writer.\n    \"\"\"\n    if self._extensions:\n      output_writer.Write('\\textensions: {0:s}\\n'.format(\n          ', '.join(self._extensions)))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompare the file entry against the filter.", "response": "def Matches(self, file_entry):\n    \"\"\"Compares the file entry against the filter.\n\n    Args:\n      file_entry (dfvfs.FileEntry): file entry to compare.\n\n    Returns:\n      bool: True if the file entry matches the filter.\n    \"\"\"\n    if not self._names or not file_entry.IsFile():\n      return False\n\n    return file_entry.name.lower() in self._names"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprints a human readable version of the filter.", "response": "def Print(self, output_writer):\n    \"\"\"Prints a human readable version of the filter.\n\n    Args:\n      output_writer (CLIOutputWriter): output writer.\n    \"\"\"\n    if self._names:\n      output_writer.Write('\\tnames: {0:s}\\n'.format(\n          ', '.join(self._names)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninitializes the scanner form the specification store.", "response": "def _GetScanner(self, specification_store, signature_identifiers):\n    \"\"\"Initializes the scanner form the specification store.\n\n    Args:\n      specification_store (FormatSpecificationStore): a specification store.\n      signature_identifiers (list[str]): signature identifiers.\n\n    Returns:\n      pysigscan.scanner: signature scanner or None.\n    \"\"\"\n    if not specification_store:\n      return None\n\n    scanner_object = pysigscan.scanner()\n\n    for format_specification in specification_store.specifications:\n      if format_specification.identifier not in signature_identifiers:\n        continue\n\n      for signature in format_specification.signatures:\n        pattern_offset = signature.offset\n        if pattern_offset is None:\n          signature_flags = pysigscan.signature_flags.NO_OFFSET\n        elif pattern_offset < 0:\n          pattern_offset *= -1\n          signature_flags = pysigscan.signature_flags.RELATIVE_FROM_END\n        else:\n          signature_flags = pysigscan.signature_flags.RELATIVE_FROM_START\n\n        scanner_object.add_signature(\n            signature.identifier, pattern_offset, signature.pattern,\n            signature_flags)\n\n      self._signature_identifiers.append(format_specification.identifier)\n\n    return scanner_object"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef Matches(self, file_entry):\n    if not self._file_scanner or not file_entry.IsFile():\n      return None\n\n    file_object = file_entry.GetFileObject()\n    if not file_object:\n      return False\n\n    try:\n      scan_state = pysigscan.scan_state()\n      self._file_scanner.scan_file_object(scan_state, file_object)\n\n    except IOError as exception:\n      # TODO: replace location by display name.\n      location = getattr(file_entry.path_spec, 'location', '')\n      logging.error((\n          '[skipping] unable to scan file: {0:s} for signatures '\n          'with error: {1!s}').format(location, exception))\n      return False\n\n    finally:\n      file_object.close()\n\n    return scan_state.number_of_scan_results > 0", "response": "Compares the file entry against the filter."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nprinting a human readable version of the filter.", "response": "def Print(self, output_writer):\n    \"\"\"Prints a human readable version of the filter.\n\n    Args:\n      output_writer (CLIOutputWriter): output writer.\n    \"\"\"\n    if self._file_scanner:\n      output_writer.Write('\\tsignature identifiers: {0:s}\\n'.format(\n          ', '.join(self._signature_identifiers)))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompare the file entry against the filter collection.", "response": "def Matches(self, file_entry):\n    \"\"\"Compares the file entry against the filter collection.\n\n    Args:\n      file_entry (dfvfs.FileEntry): file entry to compare.\n\n    Returns:\n      bool: True if the file entry matches one of the filters. If no filters\n          are provided or applicable the result will be True.\n    \"\"\"\n    if not self._filters:\n      return True\n\n    results = []\n    for file_entry_filter in self._filters:\n      result = file_entry_filter.Matches(file_entry)\n      results.append(result)\n\n    return True in results or False not in results"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprints a human readable version of the filter.", "response": "def Print(self, output_writer):\n    \"\"\"Prints a human readable version of the filter.\n\n    Args:\n      output_writer (CLIOutputWriter): output writer.\n    \"\"\"\n    if self._filters:\n      output_writer.Write('Filters:\\n')\n      for file_entry_filter in self._filters:\n        file_entry_filter.Print(output_writer)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ExtractEvents(self, parser_mediator, registry_key, **kwargs):\n    # TODO: Test other Office versions to make sure this plugin is applicable.\n    values_dict = {}\n    for registry_value in registry_key.GetValues():\n      # Ignore any value not in the form: 'Item [0-9]+'.\n      if not registry_value.name or not self._RE_VALUE_NAME.search(\n          registry_value.name):\n        continue\n\n      # Ignore any value that is empty or that does not contain a string.\n      if not registry_value.data or not registry_value.DataIsString():\n        continue\n\n      value_string = registry_value.GetDataAsObject()\n      values = self._RE_VALUE_DATA.findall(value_string)\n\n      # Values will contain a list containing a tuple containing 2 values.\n      if len(values) != 1 or len(values[0]) != 2:\n        continue\n\n      try:\n        timestamp = int(values[0][0], 16)\n      except ValueError:\n        parser_mediator.ProduceExtractionWarning((\n            'unable to convert filetime string to an integer for '\n            'value: {0:s}.').format(registry_value.name))\n        continue\n\n      event_data = OfficeMRUWindowsRegistryEventData()\n      event_data.key_path = registry_key.path\n      event_data.offset = registry_value.offset\n      # TODO: split value string in individual values.\n      event_data.value_string = value_string\n\n      values_dict[registry_value.name] = value_string\n\n      if not timestamp:\n        date_time = dfdatetime_semantic_time.SemanticTime('Not set')\n      else:\n        date_time = dfdatetime_filetime.Filetime(timestamp=timestamp)\n\n      # TODO: determine if this should be last written time.\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_WRITTEN)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    event_data = windows_events.WindowsRegistryEventData()\n    event_data.key_path = registry_key.path\n    event_data.offset = registry_key.offset\n    event_data.regvalue = values_dict\n    event_data.source_append = self._SOURCE_APPEND\n\n    event = time_events.DateTimeValuesEvent(\n        registry_key.last_written_time, definitions.TIME_DESCRIPTION_WRITTEN)\n    parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Extracts events from a Windows Registry key."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ParseZeitgeistEventRow(\n      self, parser_mediator, query, row, **unused_kwargs):\n    \"\"\"Parses a zeitgeist event row.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      query (str): query that created the row.\n      row (sqlite3.Row): row.\n    \"\"\"\n    query_hash = hash(query)\n\n    event_data = ZeitgeistActivityEventData()\n    event_data.offset = self._GetRowValue(query_hash, row, 'id')\n    event_data.query = query\n    event_data.subject_uri = self._GetRowValue(query_hash, row, 'subj_uri')\n\n    timestamp = self._GetRowValue(query_hash, row, 'timestamp')\n    date_time = dfdatetime_java_time.JavaTime(timestamp=timestamp)\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_UNKNOWN)\n    parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Parses a zeitgeist event row."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse a row from the database.", "response": "def ParseRow(self, parser_mediator, query, row, **unused_kwargs):\n    \"\"\"Parses a row from the database.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      query (str): query that created the row.\n      row (sqlite3.Row): row.\n    \"\"\"\n    query_hash = hash(query)\n\n    event_data = AndroidWebViewCacheEventData()\n    event_data.content_length = self._GetRowValue(\n        query_hash, row, 'contentlength')\n    event_data.query = query\n    event_data.url = self._GetRowValue(query_hash, row, 'url')\n\n    timestamp = self._GetRowValue(query_hash, row, 'expires')\n    if timestamp is not None:\n      date_time = dfdatetime_java_time.JavaTime(timestamp=timestamp)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_EXPIRATION)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    timestamp = self._GetRowValue(query_hash, row, 'lastmodify')\n    if timestamp is not None:\n      date_time = dfdatetime_java_time.JavaTime(timestamp=timestamp)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_MODIFICATION)\n      parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef GetMessages(self, formatter_mediator, event):\n    if self.DATA_TYPE != event.data_type:\n      raise errors.WrongFormatter('Unsupported data type: {0:s}.'.format(\n          event.data_type))\n\n    event_values = event.CopyToDict()\n\n    document_type = event_values.get('document_type', None)\n    if document_type:\n      event_values['document_type'] = self._DOC_TYPES.get(\n          document_type, 'UNKNOWN')\n\n    shared = event_values.get('shared', False)\n    if shared:\n      event_values['shared'] = 'Shared'\n    else:\n      event_values['shared'] = 'Private'\n\n    return self._ConditionalFormatMessages(event_values)", "response": "Determines the formatted message strings for an event object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef WriteEventBody(self, event):\n    inode = getattr(event, 'inode', None)\n    if inode is None:\n      event.inode = 0\n\n    try:\n      message, _ = self._output_mediator.GetFormattedMessages(event)\n    except errors.WrongFormatter:\n      message = None\n\n    if message:\n      event.message = message\n\n    json_dict = self._JSON_SERIALIZER.WriteSerializedDict(event)\n    json_string = json.dumps(json_dict, sort_keys=True)\n    # dumps() returns an ascii-encoded byte string in Python 2.\n    if py2to3.PY_2:\n      json_string = codecs.decode(json_string, 'ascii')\n    self._output_writer.Write(json_string)\n    self._output_writer.Write('\\n')", "response": "Writes the body of an event object to the output."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _ParseRegisteredDLLs(self, parser_mediator, registry_key):\n    notify_key = registry_key.GetSubkeyByName('Notify')\n    if not notify_key:\n      return\n\n    for subkey in notify_key.GetSubkeys():\n      for trigger in self._TRIGGERS:\n        handler_value = subkey.GetValueByName(trigger)\n        if not handler_value:\n          continue\n\n        values_dict = {\n            'Application': subkey.name,\n            'Handler': handler_value.GetDataAsObject(),\n            'Trigger': trigger}\n\n        command_value = subkey.GetValueByName('DllName')\n        if command_value:\n          values_dict['Command'] = command_value.GetDataAsObject()\n\n        event_data = windows_events.WindowsRegistryEventData()\n        event_data.key_path = subkey.path\n        event_data.offset = subkey.offset\n        event_data.regvalue = values_dict\n        event_data.source_append = ': Winlogon'\n\n        event = time_events.DateTimeValuesEvent(\n            subkey.last_written_time, definitions.TIME_DESCRIPTION_WRITTEN)\n        parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Parses the registered DLLs that receive event notifications."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse the logon applications.", "response": "def _ParseLogonApplications(self, parser_mediator, registry_key):\n    \"\"\"Parses the registered logon applications.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      registry_key (dfwinreg.WinRegistryKey): Windows Registry key.\n    \"\"\"\n    for application in self._LOGON_APPLICATIONS:\n      command_value = registry_key.GetValueByName(application)\n      if not command_value:\n        continue\n\n      values_dict = {\n          'Application': application,\n          'Command': command_value.GetDataAsObject(),\n          'Trigger': 'Logon'}\n\n      event_data = windows_events.WindowsRegistryEventData()\n      event_data.key_path = registry_key.path\n      event_data.offset = registry_key.offset\n      event_data.regvalue = values_dict\n      event_data.source_append = ': Winlogon'\n\n      event = time_events.DateTimeValuesEvent(\n          registry_key.last_written_time, definitions.TIME_DESCRIPTION_WRITTEN)\n      parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ExtractEvents(self, parser_mediator, registry_key, **kwargs):\n    self._ParseLogonApplications(parser_mediator, registry_key)\n    self._ParseRegisteredDLLs(parser_mediator, registry_key)", "response": "Extracts events from a Windows Registry key."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses and validates the signature.", "response": "def _CheckSignature(self, value_data):\n    \"\"\"Parses and validates the signature.\n\n    Args:\n      value_data (bytes): value data.\n\n    Returns:\n      int: format type or None if format could not be determined.\n\n    Raises:\n      ParseError: if the value data could not be parsed.\n    \"\"\"\n    signature_map = self._GetDataTypeMap('uint32le')\n\n    try:\n      signature = self._ReadStructureFromByteStream(\n          value_data, 0, signature_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.ParseError(\n          'Unable to parse signature value with error: {0!s}'.format(\n              exception))\n\n    format_type = self._HEADER_SIGNATURES.get(signature, None)\n\n    if format_type == self._FORMAT_TYPE_2003:\n      # TODO: determine which format version is used (2003 or Vista).\n      return self._FORMAT_TYPE_2003\n\n    if format_type == self._FORMAT_TYPE_8:\n      cached_entry_signature = value_data[signature:signature + 4]\n      if cached_entry_signature in (\n          self._CACHED_ENTRY_SIGNATURE_8_0, self._CACHED_ENTRY_SIGNATURE_8_1):\n        return self._FORMAT_TYPE_8\n\n    elif format_type == self._FORMAT_TYPE_10:\n      # Windows 10 uses the same cache entry signature as Windows 8.1\n      cached_entry_signature = value_data[signature:signature + 4]\n      if cached_entry_signature == self._CACHED_ENTRY_SIGNATURE_8_1:\n        return self._FORMAT_TYPE_10\n\n    return format_type"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _GetCachedEntryDataTypeMap(\n      self, format_type, value_data, cached_entry_offset):\n    \"\"\"Determines the cached entry data type map.\n\n    Args:\n      format_type (int): format type.\n      value_data (bytes): value data.\n      cached_entry_offset (int): offset of the first cached entry data\n          relative to the start of the value data.\n\n    Returns:\n      dtfabric.DataTypeMap: data type map which contains a data type definition,\n          such as a structure, that can be mapped onto binary data or None\n          if the data type map is not defined.\n\n    Raises:\n      ParseError: if the cached entry data type map cannot be determined.\n    \"\"\"\n    if format_type not in self._SUPPORTED_FORMAT_TYPES:\n      raise errors.ParseError('Unsupported format type: {0:d}'.format(\n          format_type))\n\n    data_type_map_name = ''\n\n    if format_type == self._FORMAT_TYPE_XP:\n      data_type_map_name = 'appcompatcache_cached_entry_xp_32bit'\n\n    elif format_type in (self._FORMAT_TYPE_8, self._FORMAT_TYPE_10):\n      data_type_map_name = 'appcompatcache_cached_entry_header_8'\n\n    else:\n      cached_entry = self._ParseCommon2003CachedEntry(\n          value_data, cached_entry_offset)\n\n      # Assume the entry is 64-bit if the 32-bit path offset is 0 and\n      # the 64-bit path offset is set.\n      if (cached_entry.path_offset_32bit == 0 and\n          cached_entry.path_offset_64bit != 0):\n        number_of_bits = '64'\n      else:\n        number_of_bits = '32'\n\n      if format_type == self._FORMAT_TYPE_2003:\n        data_type_map_name = (\n            'appcompatcache_cached_entry_2003_{0:s}bit'.format(number_of_bits))\n      elif format_type == self._FORMAT_TYPE_VISTA:\n        data_type_map_name = (\n            'appcompatcache_cached_entry_vista_{0:s}bit'.format(number_of_bits))\n      elif format_type == self._FORMAT_TYPE_7:\n        data_type_map_name = (\n            'appcompatcache_cached_entry_7_{0:s}bit'.format(number_of_bits))\n\n    return self._GetDataTypeMap(data_type_map_name)", "response": "Determines the data type map for a given cached entry."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _ParseCommon2003CachedEntry(self, value_data, cached_entry_offset):\n    data_type_map = self._GetDataTypeMap(\n        'appcompatcache_cached_entry_2003_common')\n\n    try:\n      cached_entry = self._ReadStructureFromByteStream(\n          value_data[cached_entry_offset:], cached_entry_offset, data_type_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.ParseError(\n          'Unable to parse cached entry value with error: {0!s}'.format(\n              exception))\n\n    if cached_entry.path_size > cached_entry.maximum_path_size:\n      raise errors.ParseError('Path size value out of bounds.')\n\n    path_end_of_string_size = (\n        cached_entry.maximum_path_size - cached_entry.path_size)\n    if cached_entry.path_size == 0 or path_end_of_string_size != 2:\n      raise errors.ParseError('Unsupported path size values.')\n\n    return cached_entry", "response": "Parses the common 2003 cached entry structure."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _ParseCachedEntryXP(self, value_data, cached_entry_offset):\n    try:\n      cached_entry = self._ReadStructureFromByteStream(\n          value_data[cached_entry_offset:], cached_entry_offset,\n          self._cached_entry_data_type_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.ParseError(\n          'Unable to parse cached entry value with error: {0!s}'.format(\n              exception))\n\n    # TODO: have dtFabric handle string conversion.\n    string_size = 0\n    for string_index in range(0, 528, 2):\n      if (cached_entry.path[string_index] == 0 and\n          cached_entry.path[string_index + 1] == 0):\n        break\n      string_size += 2\n\n    try:\n      path = bytearray(cached_entry.path[0:string_size]).decode('utf-16-le')\n    except UnicodeDecodeError:\n      raise errors.ParseError('Unable to decode cached entry path to string')\n\n    cached_entry_object = AppCompatCacheCachedEntry()\n    cached_entry_object.cached_entry_size = (\n        self._cached_entry_data_type_map.GetByteSize())\n    cached_entry_object.file_size = cached_entry.file_size\n    cached_entry_object.last_modification_time = (\n        cached_entry.last_modification_time)\n    cached_entry_object.last_update_time = cached_entry.last_update_time\n    cached_entry_object.path = path\n\n    return cached_entry_object", "response": "Parses a Windows XP cached entry."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing a Windows 2003 cached entry.", "response": "def _ParseCachedEntry2003(self, value_data, cached_entry_offset):\n    \"\"\"Parses a Windows 2003 cached entry.\n\n    Args:\n      value_data (bytes): value data.\n      cached_entry_offset (int): offset of the first cached entry data\n          relative to the start of the value data.\n\n    Returns:\n      AppCompatCacheCachedEntry: cached entry.\n\n    Raises:\n      ParseError: if the value data could not be parsed.\n    \"\"\"\n\n    try:\n      cached_entry = self._ReadStructureFromByteStream(\n          value_data[cached_entry_offset:], cached_entry_offset,\n          self._cached_entry_data_type_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.ParseError(\n          'Unable to parse cached entry value with error: {0!s}'.format(\n              exception))\n\n    path_size = cached_entry.path_size\n    maximum_path_size = cached_entry.maximum_path_size\n    path_offset = cached_entry.path_offset\n\n    if path_offset > 0 and path_size > 0:\n      path_size += path_offset\n      maximum_path_size += path_offset\n\n      try:\n        path = value_data[path_offset:path_size].decode('utf-16-le')\n      except UnicodeDecodeError:\n        raise errors.ParseError('Unable to decode cached entry path to string')\n\n    cached_entry_object = AppCompatCacheCachedEntry()\n    cached_entry_object.cached_entry_size = (\n        self._cached_entry_data_type_map.GetByteSize())\n    cached_entry_object.file_size = getattr(cached_entry, 'file_size', None)\n    cached_entry_object.last_modification_time = (\n        cached_entry.last_modification_time)\n    cached_entry_object.path = path\n\n    return cached_entry_object"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _ParseCachedEntryVista(self, value_data, cached_entry_offset):\n    try:\n      cached_entry = self._ReadStructureFromByteStream(\n          value_data[cached_entry_offset:], cached_entry_offset,\n          self._cached_entry_data_type_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.ParseError(\n          'Unable to parse cached entry value with error: {0!s}'.format(\n              exception))\n\n    path_size = cached_entry.path_size\n    maximum_path_size = cached_entry.maximum_path_size\n    path_offset = cached_entry.path_offset\n\n    if path_offset > 0 and path_size > 0:\n      path_size += path_offset\n      maximum_path_size += path_offset\n\n      try:\n        path = value_data[path_offset:path_size].decode('utf-16-le')\n      except UnicodeDecodeError:\n        raise errors.ParseError('Unable to decode cached entry path to string')\n\n    cached_entry_object = AppCompatCacheCachedEntry()\n    cached_entry_object.cached_entry_size = (\n        self._cached_entry_data_type_map.GetByteSize())\n    cached_entry_object.insertion_flags = cached_entry.insertion_flags\n    cached_entry_object.last_modification_time = (\n        cached_entry.last_modification_time)\n    cached_entry_object.path = path\n    cached_entry_object.shim_flags = cached_entry.shim_flags\n\n    return cached_entry_object", "response": "Parses a Windows Vista cached entry."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _ParseCachedEntry8(self, value_data, cached_entry_offset):\n    try:\n      cached_entry = self._ReadStructureFromByteStream(\n          value_data[cached_entry_offset:], cached_entry_offset,\n          self._cached_entry_data_type_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.ParseError(\n          'Unable to parse cached entry value with error: {0!s}'.format(\n              exception))\n\n    if cached_entry.signature not in (\n        self._CACHED_ENTRY_SIGNATURE_8_0, self._CACHED_ENTRY_SIGNATURE_8_1):\n      raise errors.ParseError('Unsupported cache entry signature')\n\n    cached_entry_data = value_data[cached_entry_offset:]\n\n    if cached_entry.signature == self._CACHED_ENTRY_SIGNATURE_8_0:\n      data_type_map_name = 'appcompatcache_cached_entry_body_8_0'\n    elif cached_entry.signature == self._CACHED_ENTRY_SIGNATURE_8_1:\n      data_type_map_name = 'appcompatcache_cached_entry_body_8_1'\n\n    data_type_map = self._GetDataTypeMap(data_type_map_name)\n    context = dtfabric_data_maps.DataTypeMapContext()\n\n    try:\n      cached_entry_body = self._ReadStructureFromByteStream(\n          cached_entry_data[12:], cached_entry_offset + 12,\n          data_type_map, context=context)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.ParseError(\n          'Unable to parse cached entry body with error: {0!s}'.format(\n              exception))\n\n    data_offset = context.byte_size\n    data_size = cached_entry_body.data_size\n\n    cached_entry_object = AppCompatCacheCachedEntry()\n    cached_entry_object.cached_entry_size = (\n        12 + cached_entry.cached_entry_data_size)\n    cached_entry_object.insertion_flags = cached_entry_body.insertion_flags\n    cached_entry_object.last_modification_time = (\n        cached_entry_body.last_modification_time)\n    cached_entry_object.path = cached_entry_body.path\n    cached_entry_object.shim_flags = cached_entry_body.shim_flags\n\n    if data_size > 0:\n      cached_entry_object.data = cached_entry_data[\n          data_offset:data_offset + data_size]\n\n    return cached_entry_object", "response": "Parses a Windows 8. 0 or 8. 1 cached entry."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing the header. Args: format_type (int): format type. value_data (bytes): value data. Returns: AppCompatCacheHeader: header. Raises: ParseError: if the value data could not be parsed.", "response": "def _ParseHeader(self, format_type, value_data):\n    \"\"\"Parses the header.\n\n    Args:\n      format_type (int): format type.\n      value_data (bytes): value data.\n\n    Returns:\n      AppCompatCacheHeader: header.\n\n    Raises:\n      ParseError: if the value data could not be parsed.\n    \"\"\"\n    data_type_map_name = self._HEADER_DATA_TYPE_MAP_NAMES.get(format_type, None)\n    if not data_type_map_name:\n      raise errors.ParseError(\n          'Unsupported format type: {0:d}'.format(format_type))\n\n    data_type_map = self._GetDataTypeMap(data_type_map_name)\n\n    try:\n      header = self._ReadStructureFromByteStream(\n          value_data, 0, data_type_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.ParseError(\n          'Unable to parse header value with error: {0!s}'.format(\n              exception))\n\n    header_data_size = data_type_map.GetByteSize()\n    if format_type == self._FORMAT_TYPE_10:\n      header_data_size = header.signature\n\n    cache_header = AppCompatCacheHeader()\n    cache_header.header_size = header_data_size\n    cache_header.number_of_cached_entries = getattr(\n        header, 'number_of_cached_entries', 0)\n\n    return cache_header"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ExtractEvents(self, parser_mediator, registry_key, **kwargs):\n    value = registry_key.GetValueByName('AppCompatCache')\n    if not value:\n      return\n\n    value_data = value.data\n    value_data_size = len(value.data)\n\n    format_type = self._CheckSignature(value_data)\n    if not format_type:\n      parser_mediator.ProduceExtractionWarning(\n          'Unsupported signature in AppCompatCache key: {0:s}'.format(\n              registry_key.path))\n      return\n\n    header_object = self._ParseHeader(format_type, value_data)\n\n    # On Windows Vista and 2008 when the cache is empty it will\n    # only consist of the header.\n    if value_data_size <= header_object.header_size:\n      return\n\n    cached_entry_offset = header_object.header_size\n\n    self._cached_entry_data_type_map = self._GetCachedEntryDataTypeMap(\n        format_type, value_data, cached_entry_offset)\n    if not self._cached_entry_data_type_map:\n      raise errors.ParseError('Unable to determine cached entry data type.')\n\n    parse_cached_entry_function = None\n    if format_type == self._FORMAT_TYPE_XP:\n      parse_cached_entry_function = self._ParseCachedEntryXP\n    elif format_type == self._FORMAT_TYPE_2003:\n      parse_cached_entry_function = self._ParseCachedEntry2003\n    elif format_type == self._FORMAT_TYPE_VISTA:\n      parse_cached_entry_function = self._ParseCachedEntryVista\n    elif format_type == self._FORMAT_TYPE_7:\n      parse_cached_entry_function = self._ParseCachedEntry7\n    elif format_type == self._FORMAT_TYPE_8:\n      parse_cached_entry_function = self._ParseCachedEntry8\n    elif format_type == self._FORMAT_TYPE_10:\n      parse_cached_entry_function = self._ParseCachedEntry10\n\n    cached_entry_index = 0\n    while cached_entry_offset < value_data_size:\n      cached_entry_object = parse_cached_entry_function(\n          value_data, cached_entry_offset)\n\n      event_data = AppCompatCacheEventData()\n      event_data.entry_index = cached_entry_index + 1\n      event_data.key_path = registry_key.path\n      event_data.offset = cached_entry_offset\n      event_data.path = cached_entry_object.path\n\n      if cached_entry_object.last_modification_time is not None:\n        if not cached_entry_object.last_modification_time:\n          date_time = dfdatetime_semantic_time.SemanticTime('Not set')\n        else:\n          date_time = dfdatetime_filetime.Filetime(\n              timestamp=cached_entry_object.last_modification_time)\n\n        # TODO: refactor to file modification event.\n        event = time_events.DateTimeValuesEvent(\n            date_time, 'File Last Modification Time')\n        parser_mediator.ProduceEventWithEventData(event, event_data)\n\n      if cached_entry_object.last_update_time is not None:\n        if not cached_entry_object.last_update_time:\n          date_time = dfdatetime_semantic_time.SemanticTime('Not set')\n        else:\n          date_time = dfdatetime_filetime.Filetime(\n              timestamp=cached_entry_object.last_update_time)\n\n        # TODO: refactor to process run event.\n        event = time_events.DateTimeValuesEvent(\n            date_time, definitions.TIME_DESCRIPTION_LAST_RUN)\n        parser_mediator.ProduceEventWithEventData(event, event_data)\n\n      cached_entry_offset += cached_entry_object.cached_entry_size\n      cached_entry_index += 1\n\n      if (header_object.number_of_cached_entries != 0 and\n          cached_entry_index >= header_object.number_of_cached_entries):\n        break", "response": "Extracts events from a Windows Registry key."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef GetMessages(self, formatter_mediator, event):\n    regvalue = getattr(event, 'regvalue', {})\n    # Loop over all the registry value names in the service key.\n    for service_value_name in regvalue.keys():\n      # A temporary variable so we can refer to this long name more easily.\n      service_enums = human_readable_service_enums.SERVICE_ENUMS\n      # Check if we need to can make the value more human readable.\n      if service_value_name in service_enums.keys():\n        service_enum = service_enums[service_value_name]\n        # Find the human readable version of the name and fall back to the\n        # raw value if it's not found.\n        human_readable_value = service_enum.get(\n            regvalue[service_value_name],\n            regvalue[service_value_name])\n        regvalue[service_value_name] = human_readable_value\n\n    return super(WinRegistryServiceFormatter, self).GetMessages(\n        formatter_mediator, event)", "response": "Determines the formatted message strings for an event object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the deserialized content of a plist as a dictionary object.", "response": "def GetTopLevel(self, file_object):\n    \"\"\"Returns the deserialized content of a plist as a dictionary object.\n\n    Args:\n      file_object (dfvfs.FileIO): a file-like object to parse.\n\n    Returns:\n      dict[str, object]: contents of the plist.\n\n    Raises:\n      UnableToParseFile: when the file cannot be parsed.\n    \"\"\"\n    try:\n      top_level_object = biplist.readPlist(file_object)\n\n    except (biplist.InvalidPlistException,\n            biplist.NotBinaryPlistException) as exception:\n      raise errors.UnableToParseFile(\n          'Unable to parse plist with error: {0!s}'.format(exception))\n\n    return top_level_object"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ParseFileObject(self, parser_mediator, file_object):\n    filename = parser_mediator.GetFilename()\n    file_size = file_object.get_size()\n\n    if file_size <= 0:\n      raise errors.UnableToParseFile(\n          'File size: {0:d} bytes is less equal 0.'.format(file_size))\n\n    # 50MB is 10x larger than any plist seen to date.\n    if file_size > 50000000:\n      raise errors.UnableToParseFile(\n          'File size: {0:d} bytes is larger than 50 MB.'.format(file_size))\n\n    top_level_object = self.GetTopLevel(file_object)\n    if not top_level_object:\n      raise errors.UnableToParseFile(\n          'Unable to parse: {0:s} skipping.'.format(filename))\n\n    # TODO: add a parser filter.\n    matching_plugin = None\n    for plugin in self._plugins:\n      try:\n        plugin.UpdateChainAndProcess(\n            parser_mediator, plist_name=filename, top_level=top_level_object)\n        matching_plugin = plugin\n\n      except errors.WrongPlistPlugin as exception:\n        logger.debug('Wrong plugin: {0:s} for: {1:s}'.format(\n            exception.args[0], exception.args[1]))\n\n    if not matching_plugin and self._default_plugin:\n      self._default_plugin.UpdateChainAndProcess(\n          parser_mediator, plist_name=filename, top_level=top_level_object)", "response": "Parses a plist file - like object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate the number of event reports.", "response": "def UpdateNumberOfEventReports(\n      self, number_of_consumed_reports, number_of_produced_reports):\n    \"\"\"Updates the number of event reports.\n\n    Args:\n      number_of_consumed_reports (int): total number of event reports consumed\n          by the process.\n      number_of_produced_reports (int): total number of event reports produced\n          by the process.\n\n    Returns:\n      bool: True if either number of event reports has increased.\n\n    Raises:\n      ValueError: if the consumed or produced number of event reports is\n          smaller than the value of the previous update.\n    \"\"\"\n    consumed_reports_delta = 0\n    if number_of_consumed_reports is not None:\n      if number_of_consumed_reports < self.number_of_consumed_reports:\n        raise ValueError(\n            'Number of consumed reports smaller than previous update.')\n\n      consumed_reports_delta = (\n          number_of_consumed_reports - self.number_of_consumed_reports)\n\n      self.number_of_consumed_reports = number_of_consumed_reports\n      self.number_of_consumed_reports_delta = consumed_reports_delta\n\n    produced_reports_delta = 0\n    if number_of_produced_reports is not None:\n      if number_of_produced_reports < self.number_of_produced_reports:\n        raise ValueError(\n            'Number of produced reports smaller than previous update.')\n\n      produced_reports_delta = (\n          number_of_produced_reports - self.number_of_produced_reports)\n\n      self.number_of_produced_reports = number_of_produced_reports\n      self.number_of_produced_reports_delta = produced_reports_delta\n\n    return consumed_reports_delta > 0 or produced_reports_delta > 0"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating the number of events.", "response": "def UpdateNumberOfEvents(\n      self, number_of_consumed_events, number_of_produced_events):\n    \"\"\"Updates the number of events.\n\n    Args:\n      number_of_consumed_events (int): total number of events consumed by\n          the process.\n      number_of_produced_events (int): total number of events produced by\n          the process.\n\n    Returns:\n      bool: True if either number of events has increased.\n\n    Raises:\n      ValueError: if the consumed or produced number of events is smaller\n          than the value of the previous update.\n    \"\"\"\n    consumed_events_delta = 0\n    if number_of_consumed_events is not None:\n      if number_of_consumed_events < self.number_of_consumed_events:\n        raise ValueError(\n            'Number of consumed events smaller than previous update.')\n\n      consumed_events_delta = (\n          number_of_consumed_events - self.number_of_consumed_events)\n\n      self.number_of_consumed_events = number_of_consumed_events\n      self.number_of_consumed_events_delta = consumed_events_delta\n\n    produced_events_delta = 0\n    if number_of_produced_events is not None:\n      if number_of_produced_events < self.number_of_produced_events:\n        raise ValueError(\n            'Number of produced events smaller than previous update.')\n\n      produced_events_delta = (\n          number_of_produced_events - self.number_of_produced_events)\n\n      self.number_of_produced_events = number_of_produced_events\n      self.number_of_produced_events_delta = produced_events_delta\n\n    return consumed_events_delta > 0 or produced_events_delta > 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef UpdateNumberOfEventSources(\n      self, number_of_consumed_sources, number_of_produced_sources):\n    \"\"\"Updates the number of event sources.\n\n    Args:\n      number_of_consumed_sources (int): total number of event sources consumed\n          by the process.\n      number_of_produced_sources (int): total number of event sources produced\n          by the process.\n\n    Returns:\n      bool: True if either number of event sources has increased.\n\n    Raises:\n      ValueError: if the consumed or produced number of event sources is\n          smaller than the value of the previous update.\n    \"\"\"\n    consumed_sources_delta = 0\n    if number_of_consumed_sources is not None:\n      if number_of_consumed_sources < self.number_of_consumed_sources:\n        raise ValueError(\n            'Number of consumed sources smaller than previous update.')\n\n      consumed_sources_delta = (\n          number_of_consumed_sources - self.number_of_consumed_sources)\n\n      self.number_of_consumed_sources = number_of_consumed_sources\n      self.number_of_consumed_sources_delta = consumed_sources_delta\n\n    produced_sources_delta = 0\n    if number_of_produced_sources is not None:\n      if number_of_produced_sources < self.number_of_produced_sources:\n        raise ValueError(\n            'Number of produced sources smaller than previous update.')\n\n      produced_sources_delta = (\n          number_of_produced_sources - self.number_of_produced_sources)\n\n      self.number_of_produced_sources = number_of_produced_sources\n      self.number_of_produced_sources_delta = produced_sources_delta\n\n    return consumed_sources_delta > 0 or produced_sources_delta > 0", "response": "Updates the number of event sources."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef UpdateNumberOfEventTags(\n      self, number_of_consumed_event_tags, number_of_produced_event_tags):\n    \"\"\"Updates the number of event tags.\n\n    Args:\n      number_of_consumed_event_tags (int): total number of event tags consumed\n          by the process.\n      number_of_produced_event_tags (int): total number of event tags produced\n          by the process.\n\n    Returns:\n      bool: True if either number of event tags has increased.\n\n    Raises:\n      ValueError: if the consumed or produced number of event tags is smaller\n          than the value of the previous update.\n    \"\"\"\n    consumed_event_tags_delta = 0\n    if number_of_consumed_event_tags is not None:\n      if number_of_consumed_event_tags < self.number_of_consumed_event_tags:\n        raise ValueError(\n            'Number of consumed event tags smaller than previous update.')\n\n      consumed_event_tags_delta = (\n          number_of_consumed_event_tags - self.number_of_consumed_event_tags)\n\n      self.number_of_consumed_event_tags = number_of_consumed_event_tags\n      self.number_of_consumed_event_tags_delta = consumed_event_tags_delta\n\n    produced_event_tags_delta = 0\n    if number_of_produced_event_tags is not None:\n      if number_of_produced_event_tags < self.number_of_produced_event_tags:\n        raise ValueError(\n            'Number of produced event tags smaller than previous update.')\n\n      produced_event_tags_delta = (\n          number_of_produced_event_tags - self.number_of_produced_event_tags)\n\n      self.number_of_produced_event_tags = number_of_produced_event_tags\n      self.number_of_produced_event_tags_delta = produced_event_tags_delta\n\n    return consumed_event_tags_delta > 0 or produced_event_tags_delta > 0", "response": "Updates the number of event tags."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef UpdateNumberOfWarnings(\n      self, number_of_consumed_warnings, number_of_produced_warnings):\n    \"\"\"Updates the number of warnings.\n\n    Args:\n      number_of_consumed_warnings (int): total number of warnings consumed by\n          the process.\n      number_of_produced_warnings (int): total number of warnings produced by\n          the process.\n\n    Returns:\n      bool: True if either number of warnings has increased.\n\n    Raises:\n      ValueError: if the consumed or produced number of warnings is smaller\n          than the value of the previous update.\n    \"\"\"\n    consumed_warnings_delta = 0\n    if number_of_consumed_warnings is not None:\n      if number_of_consumed_warnings < self.number_of_consumed_warnings:\n        raise ValueError(\n            'Number of consumed warnings smaller than previous update.')\n\n      consumed_warnings_delta = (\n          number_of_consumed_warnings - self.number_of_consumed_warnings)\n\n      self.number_of_consumed_warnings = number_of_consumed_warnings\n      self.number_of_consumed_warnings_delta = consumed_warnings_delta\n\n    produced_warnings_delta = 0\n    if number_of_produced_warnings is not None:\n      if number_of_produced_warnings < self.number_of_produced_warnings:\n        raise ValueError(\n            'Number of produced warnings smaller than previous update.')\n\n      produced_warnings_delta = (\n          number_of_produced_warnings - self.number_of_produced_warnings)\n\n      self.number_of_produced_warnings = number_of_produced_warnings\n      self.number_of_produced_warnings_delta = produced_warnings_delta\n\n    return consumed_warnings_delta > 0 or produced_warnings_delta > 0", "response": "Updates the number of warnings."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef workers_status(self):\n    return [self._workers_status[identifier]\n            for identifier in sorted(self._workers_status.keys())]", "response": "The worker status objects sorted by identifier."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _UpdateProcessStatus(\n      self, process_status, identifier, status, pid, used_memory, display_name,\n      number_of_consumed_sources, number_of_produced_sources,\n      number_of_consumed_events, number_of_produced_events,\n      number_of_consumed_event_tags, number_of_produced_event_tags,\n      number_of_consumed_reports, number_of_produced_reports,\n      number_of_consumed_warnings, number_of_produced_warnings):\n    \"\"\"Updates a process status.\n\n    Args:\n      process_status (ProcessStatus): process status.\n      identifier (str): process identifier.\n      status (str): human readable status of the process e.g. 'Idle'.\n      pid (int): process identifier (PID).\n      used_memory (int): size of used memory in bytes.\n      display_name (str): human readable of the file entry currently being\n          processed by the process.\n      number_of_consumed_sources (int): total number of event sources consumed\n          by the process.\n      number_of_produced_sources (int): total number of event sources produced\n          by the process.\n      number_of_consumed_events (int): total number of events consumed by\n          the process.\n      number_of_produced_events (int): total number of events produced by\n          the process.\n      number_of_consumed_event_tags (int): total number of event tags consumed\n          by the process.\n      number_of_produced_event_tags (int): total number of event tags produced\n          by the process.\n      number_of_consumed_reports (int): total number of event reports consumed\n          by the process.\n      number_of_produced_reports (int): total number of event reports produced\n          by the process.\n      number_of_consumed_warnings (int): total number of warnings consumed by\n          the process.\n      number_of_produced_warnings (int): total number of warnings produced by\n          the process.\n    \"\"\"\n    new_sources = process_status.UpdateNumberOfEventSources(\n        number_of_consumed_sources, number_of_produced_sources)\n\n    new_events = process_status.UpdateNumberOfEvents(\n        number_of_consumed_events, number_of_produced_events)\n\n    new_event_tags = process_status.UpdateNumberOfEventTags(\n        number_of_consumed_event_tags, number_of_produced_event_tags)\n\n    new_warnings = process_status.UpdateNumberOfWarnings(\n        number_of_consumed_warnings, number_of_produced_warnings)\n\n    new_reports = process_status.UpdateNumberOfEventReports(\n        number_of_consumed_reports, number_of_produced_reports)\n\n    process_status.display_name = display_name\n    process_status.identifier = identifier\n    process_status.pid = pid\n    process_status.status = status\n    process_status.used_memory = used_memory\n\n    if (new_sources or new_events or new_event_tags or new_warnings or\n        new_reports):\n      process_status.last_running_time = time.time()", "response": "Updates a process status."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate the status of the foreman.", "response": "def UpdateForemanStatus(\n      self, identifier, status, pid, used_memory, display_name,\n      number_of_consumed_sources, number_of_produced_sources,\n      number_of_consumed_events, number_of_produced_events,\n      number_of_consumed_event_tags, number_of_produced_event_tags,\n      number_of_consumed_reports, number_of_produced_reports,\n      number_of_consumed_warnings, number_of_produced_warnings):\n    \"\"\"Updates the status of the foreman.\n\n    Args:\n      identifier (str): foreman identifier.\n      status (str): human readable status of the foreman e.g. 'Idle'.\n      pid (int): process identifier (PID).\n      used_memory (int): size of used memory in bytes.\n      display_name (str): human readable of the file entry currently being\n          processed by the foreman.\n      number_of_consumed_sources (int): total number of event sources consumed\n          by the foreman.\n      number_of_produced_sources (int): total number of event sources produced\n          by the foreman.\n      number_of_consumed_events (int): total number of events consumed by\n          the foreman.\n      number_of_produced_events (int): total number of events produced by\n          the foreman.\n      number_of_consumed_event_tags (int): total number of event tags consumed\n          by the foreman.\n      number_of_produced_event_tags (int): total number of event tags produced\n          by the foreman.\n      number_of_consumed_warnings (int): total number of warnings consumed by\n          the foreman.\n      number_of_produced_warnings (int): total number of warnings produced by\n          the foreman.\n      number_of_consumed_reports (int): total number of event reports consumed\n          by the process.\n      number_of_produced_reports (int): total number of event reports produced\n          by the process.\n    \"\"\"\n    if not self.foreman_status:\n      self.foreman_status = ProcessStatus()\n\n    self._UpdateProcessStatus(\n        self.foreman_status, identifier, status, pid, used_memory, display_name,\n        number_of_consumed_sources, number_of_produced_sources,\n        number_of_consumed_events, number_of_produced_events,\n        number_of_consumed_event_tags, number_of_produced_event_tags,\n        number_of_consumed_reports, number_of_produced_reports,\n        number_of_consumed_warnings, number_of_produced_warnings)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate the status of a worker.", "response": "def UpdateWorkerStatus(\n      self, identifier, status, pid, used_memory, display_name,\n      number_of_consumed_sources, number_of_produced_sources,\n      number_of_consumed_events, number_of_produced_events,\n      number_of_consumed_event_tags, number_of_produced_event_tags,\n      number_of_consumed_reports, number_of_produced_reports,\n      number_of_consumed_warnings, number_of_produced_warnings):\n    \"\"\"Updates the status of a worker.\n\n    Args:\n      identifier (str): worker identifier.\n      status (str): human readable status of the worker e.g. 'Idle'.\n      pid (int): process identifier (PID).\n      used_memory (int): size of used memory in bytes.\n      display_name (str): human readable of the file entry currently being\n          processed by the worker.\n      number_of_consumed_sources (int): total number of event sources consumed\n          by the worker.\n      number_of_produced_sources (int): total number of event sources produced\n          by the worker.\n      number_of_consumed_events (int): total number of events consumed by\n          the worker.\n      number_of_produced_events (int): total number of events produced by\n          the worker.\n      number_of_consumed_event_tags (int): total number of event tags consumed\n          by the worker.\n      number_of_produced_event_tags (int): total number of event tags produced\n          by the worker.\n      number_of_consumed_reports (int): total number of event reports consumed\n          by the process.\n      number_of_produced_reports (int): total number of event reports produced\n          by the process.\n      number_of_consumed_warnings (int): total number of warnings consumed by\n          the worker.\n      number_of_produced_warnings (int): total number of warnings produced by\n          the worker.\n    \"\"\"\n    if identifier not in self._workers_status:\n      self._workers_status[identifier] = ProcessStatus()\n\n    process_status = self._workers_status[identifier]\n    self._UpdateProcessStatus(\n        process_status, identifier, status, pid, used_memory, display_name,\n        number_of_consumed_sources, number_of_produced_sources,\n        number_of_consumed_events, number_of_produced_events,\n        number_of_consumed_event_tags, number_of_produced_event_tags,\n        number_of_consumed_reports, number_of_produced_reports,\n        number_of_consumed_warnings, number_of_produced_warnings)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndetermining the formatted message strings for an event object.", "response": "def GetMessages(self, formatter_mediator, event):\n    \"\"\"Determines the formatted message strings for an event object.\n\n    Args:\n      formatter_mediator (FormatterMediator): mediates the interactions\n          between formatters and other components, such as storage and Windows\n          EventLog resources.\n      event (EventObject): event.\n\n    Returns:\n      tuple(str, str): formatted message string and short message string.\n\n    Raises:\n      WrongFormatter: if the event object cannot be formatted by the formatter.\n    \"\"\"\n    if self.DATA_TYPE != event.data_type:\n      raise errors.WrongFormatter('Unsupported data type: {0:s}.'.format(\n          event.data_type))\n\n    event_values = event.CopyToDict()\n\n    http_headers = event_values.get('http_headers', None)\n    if http_headers:\n      event_values['http_headers'] = http_headers.replace('\\r\\n', ' - ')\n\n    if event_values.get('recovered', None):\n      event_values['recovered_string'] = '[Recovered Entry]'\n\n    cached_file_path = event_values.get('cached_filename', None)\n    if cached_file_path:\n      cache_directory_name = event_values.get('cache_directory_name', None)\n      if cache_directory_name:\n        cached_file_path = '\\\\'.join([cache_directory_name, cached_file_path])\n      event_values['cached_file_path'] = cached_file_path\n\n    return self._ConditionalFormatMessages(event_values)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlist of key paths defined by the filter.", "response": "def key_paths(self):\n    \"\"\"List of key paths defined by the filter.\"\"\"\n    if self._wow64_key_path:\n      return [self._key_path, self._wow64_key_path]\n    return [self._key_path]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndetermining if a Windows Registry key matches the filter.", "response": "def Match(self, registry_key):\n    \"\"\"Determines if a Windows Registry key matches the filter.\n\n    Args:\n      registry_key (dfwinreg.WinRegistryKey): Windows Registry key.\n\n    Returns:\n      bool: True if the keys match.\n    \"\"\"\n    key_path = registry_key.path.upper()\n    if self._key_path_prefix and self._key_path_suffix:\n      if (key_path.startswith(self._key_path_prefix) and\n          key_path.endswith(self._key_path_suffix)):\n\n        key_path_segment = key_path[\n            len(self._key_path_prefix):-len(self._key_path_suffix)]\n        if key_path_segment.startswith('ControlSet'.upper()):\n          try:\n            control_set = int(key_path_segment[10:], 10)\n          except ValueError:\n            control_set = None\n\n          # TODO: check if control_set is in bounds.\n          return control_set is not None\n\n    return key_path in (self._key_path_upper, self._wow64_key_path_upper)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndetermines if a Windows Registry key matches the filter.", "response": "def Match(self, registry_key):\n    \"\"\"Determines if a Windows Registry key matches the filter.\n\n    Args:\n      registry_key (dfwinreg.WinRegistryKey): Windows Registry key.\n\n    Returns:\n      bool: True if the keys match.\n    \"\"\"\n    value_names = frozenset([\n        registry_value.name for registry_value in registry_key.GetValues()])\n\n    return self._value_names.issubset(value_names)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprocessing a Windows Registry key or value.", "response": "def Process(self, parser_mediator, registry_key, **kwargs):\n    \"\"\"Processes a Windows Registry key or value.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      registry_key (dfwinreg.WinRegistryKey): Windows Registry key.\n\n    Raises:\n      ValueError: If the Windows Registry key is not set.\n    \"\"\"\n    if registry_key is None:\n      raise ValueError('Windows Registry key is not set.')\n\n    # This will raise if unhandled keyword arguments are passed.\n    super(WindowsRegistryPlugin, self).Process(parser_mediator, **kwargs)\n\n    self.ExtractEvents(parser_mediator, registry_key, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef UpdateChainAndProcess(self, parser_mediator, registry_key, **kwargs):\n    parser_mediator.AppendToParserChain(self)\n    try:\n      self.Process(parser_mediator, registry_key, **kwargs)\n    finally:\n      parser_mediator.PopFromParserChain()", "response": "Updates the parser chain and processes a Windows Registry key or value."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves the format specification.", "response": "def GetFormatSpecification(cls):\n    \"\"\"Retrieves the format specification.\n\n    Returns:\n      FormatSpecification: format specification.\n    \"\"\"\n    format_specification = specification.FormatSpecification(cls.NAME)\n    format_specification.AddNewSignature(b'BAAD', offset=0)\n    format_specification.AddNewSignature(b'FILE', offset=0)\n    return format_specification"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _GetDateTime(self, filetime):\n    if filetime == 0:\n      return dfdatetime_semantic_time.SemanticTime('Not set')\n\n    return dfdatetime_filetime.Filetime(timestamp=filetime)", "response": "Retrieves the date and time from a FILETIME timestamp."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _ParseMFTAttribute(self, parser_mediator, mft_entry, mft_attribute):\n    if mft_entry.is_empty() or mft_entry.base_record_file_reference != 0:\n      return\n\n    if mft_attribute.attribute_type in [\n        self._MFT_ATTRIBUTE_STANDARD_INFORMATION,\n        self._MFT_ATTRIBUTE_FILE_NAME]:\n\n      file_attribute_flags = getattr(\n          mft_attribute, 'file_attribute_flags', None)\n      name = getattr(mft_attribute, 'name', None)\n      parent_file_reference = getattr(\n          mft_attribute, 'parent_file_reference', None)\n\n      event_data = NTFSFileStatEventData()\n      event_data.attribute_type = mft_attribute.attribute_type\n      event_data.file_attribute_flags = file_attribute_flags\n      event_data.file_reference = mft_entry.file_reference\n      event_data.is_allocated = mft_entry.is_allocated()\n      event_data.name = name\n      event_data.parent_file_reference = parent_file_reference\n\n      try:\n        creation_time = mft_attribute.get_creation_time_as_integer()\n      except OverflowError as exception:\n        parser_mediator.ProduceExtractionWarning((\n            'unable to read the creation timestamp from MFT attribute: '\n            '0x{0:08x} with error: {1!s}').format(\n                mft_attribute.attribute_type, exception))\n        creation_time = None\n\n      if creation_time is not None:\n        date_time = self._GetDateTime(creation_time)\n        event = time_events.DateTimeValuesEvent(\n            date_time, definitions.TIME_DESCRIPTION_CREATION)\n        parser_mediator.ProduceEventWithEventData(event, event_data)\n\n      try:\n        modification_time = mft_attribute.get_modification_time_as_integer()\n      except OverflowError as exception:\n        parser_mediator.ProduceExtractionWarning((\n            'unable to read the modification timestamp from MFT attribute: '\n            '0x{0:08x} with error: {1!s}').format(\n                mft_attribute.attribute_type, exception))\n        modification_time = None\n\n      if modification_time is not None:\n        date_time = self._GetDateTime(modification_time)\n        event = time_events.DateTimeValuesEvent(\n            date_time, definitions.TIME_DESCRIPTION_MODIFICATION)\n        parser_mediator.ProduceEventWithEventData(event, event_data)\n\n      try:\n        access_time = mft_attribute.get_access_time_as_integer()\n      except OverflowError as exception:\n        parser_mediator.ProduceExtractionWarning((\n            'unable to read the access timestamp from MFT attribute: '\n            '0x{0:08x} with error: {1!s}').format(\n                exception, mft_attribute.attribute_type))\n        access_time = None\n\n      if access_time is not None:\n        date_time = self._GetDateTime(access_time)\n        event = time_events.DateTimeValuesEvent(\n            date_time, definitions.TIME_DESCRIPTION_LAST_ACCESS)\n        parser_mediator.ProduceEventWithEventData(event, event_data)\n\n      try:\n        entry_modification_time = (\n            mft_attribute.get_entry_modification_time_as_integer())\n      except OverflowError as exception:\n        parser_mediator.ProduceExtractionWarning((\n            'unable to read the entry modification timestamp from MFT '\n            'attribute: 0x{0:08x} with error: {1!s}').format(\n                mft_attribute.attribute_type, exception))\n        entry_modification_time = None\n\n      if entry_modification_time is not None:\n        date_time = self._GetDateTime(entry_modification_time)\n        event = time_events.DateTimeValuesEvent(\n            date_time, definitions.TIME_DESCRIPTION_ENTRY_MODIFICATION)\n        parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    elif mft_attribute.attribute_type == self._MFT_ATTRIBUTE_OBJECT_ID:\n      display_name = '$MFT: {0:d}-{1:d}'.format(\n          mft_entry.file_reference & 0xffffffffffff,\n          mft_entry.file_reference >> 48)\n\n      if mft_attribute.droid_file_identifier:\n        try:\n          self._ParseDistributedTrackingIdentifier(\n              parser_mediator, mft_attribute.droid_file_identifier,\n              display_name)\n\n        except (TypeError, ValueError) as exception:\n          parser_mediator.ProduceExtractionWarning((\n              'unable to read droid file identifier from attribute: 0x{0:08x} '\n              'with error: {1!s}').format(\n                  mft_attribute.attribute_type, exception))\n\n      if mft_attribute.birth_droid_file_identifier:\n        try:\n          self._ParseDistributedTrackingIdentifier(\n              parser_mediator, mft_attribute.droid_file_identifier,\n              display_name)\n\n        except (TypeError, ValueError) as exception:\n          parser_mediator.ProduceExtractionWarning((\n              'unable to read birth droid file identifier from attribute: '\n              '0x{0:08x} with error: {1!s}').format(\n                  mft_attribute.attribute_type, exception))", "response": "Parses an MFT attribute."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nextracting data from a NFTS MFT entry.", "response": "def _ParseMFTEntry(self, parser_mediator, mft_entry):\n    \"\"\"Extracts data from a NFTS $MFT entry.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      mft_entry (pyfsntfs.file_entry): MFT entry.\n    \"\"\"\n    for attribute_index in range(0, mft_entry.number_of_attributes):\n      try:\n        mft_attribute = mft_entry.get_attribute(attribute_index)\n        self._ParseMFTAttribute(parser_mediator, mft_entry, mft_attribute)\n\n      except IOError as exception:\n        parser_mediator.ProduceExtractionWarning((\n            'unable to parse MFT attribute: {0:d} with error: {1!s}').format(\n                attribute_index, exception))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses a NTFS MFT metadata file - like object.", "response": "def ParseFileObject(self, parser_mediator, file_object):\n    \"\"\"Parses a NTFS $MFT metadata file-like object.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      file_object (dfvfs.FileIO): file-like object.\n    \"\"\"\n    mft_metadata_file = pyfsntfs.mft_metadata_file()\n\n    try:\n      mft_metadata_file.open_file_object(file_object)\n    except IOError as exception:\n      parser_mediator.ProduceExtractionWarning(\n          'unable to open file with error: {0!s}'.format(exception))\n\n    for entry_index in range(0, mft_metadata_file.number_of_file_entries):\n      try:\n        mft_entry = mft_metadata_file.get_file_entry(entry_index)\n        self._ParseMFTEntry(parser_mediator, mft_entry)\n\n      except IOError as exception:\n        parser_mediator.ProduceExtractionWarning((\n            'unable to parse MFT entry: {0:d} with error: {1!s}').format(\n                entry_index, exception))\n\n    mft_metadata_file.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses an USN change journal record.", "response": "def _ParseUSNChangeJournal(self, parser_mediator, usn_change_journal):\n    \"\"\"Parses an USN change journal.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      usn_change_journal (pyfsntsfs.usn_change_journal): USN change journal.\n\n    Raises:\n      ParseError: if an USN change journal record cannot be parsed.\n    \"\"\"\n    if not usn_change_journal:\n      return\n\n    usn_record_map = self._GetDataTypeMap('usn_record_v2')\n\n    usn_record_data = usn_change_journal.read_usn_record()\n    while usn_record_data:\n      current_offset = usn_change_journal.get_offset()\n\n      try:\n        usn_record = self._ReadStructureFromByteStream(\n            usn_record_data, current_offset, usn_record_map)\n      except (ValueError, errors.ParseError) as exception:\n        raise errors.ParseError((\n            'Unable to parse USN record at offset: 0x{0:08x} with error: '\n            '{1!s}').format(current_offset, exception))\n\n      # Per MSDN we need to use name offset for forward compatibility.\n      name_offset = usn_record.name_offset - 60\n      utf16_stream = usn_record.name[name_offset:usn_record.name_size]\n\n      try:\n        name_string = utf16_stream.decode('utf-16-le')\n      except (UnicodeDecodeError, UnicodeEncodeError) as exception:\n        name_string = utf16_stream.decode('utf-16-le', errors='replace')\n        parser_mediator.ProduceExtractionWarning((\n            'unable to decode USN record name string with error: '\n            '{0:s}. Characters that cannot be decoded will be replaced '\n            'with \"?\" or \"\\\\ufffd\".').format(exception))\n\n      event_data = NTFSUSNChangeEventData()\n      event_data.file_attribute_flags = usn_record.file_attribute_flags\n      event_data.file_reference = usn_record.file_reference\n      event_data.filename = name_string\n      event_data.offset = current_offset\n      event_data.parent_file_reference = usn_record.parent_file_reference\n      event_data.update_reason_flags = usn_record.update_reason_flags\n      event_data.update_sequence_number = usn_record.update_sequence_number\n      event_data.update_source_flags = usn_record.update_source_flags\n\n      if not usn_record.update_date_time:\n        date_time = dfdatetime_semantic_time.SemanticTime('Not set')\n      else:\n        date_time = dfdatetime_filetime.Filetime(\n            timestamp=usn_record.update_date_time)\n\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_ENTRY_MODIFICATION)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n      usn_record_data = usn_change_journal.read_usn_record()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse a NTFS metadata file - like object.", "response": "def ParseFileObject(self, parser_mediator, file_object):\n    \"\"\"Parses a NTFS $UsnJrnl metadata file-like object.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      file_object (dfvfs.FileIO): file-like object.\n    \"\"\"\n    volume = pyfsntfs.volume()\n    try:\n      volume.open_file_object(file_object)\n    except IOError as exception:\n      parser_mediator.ProduceExtractionWarning(\n          'unable to open NTFS volume with error: {0!s}'.format(exception))\n\n    try:\n      usn_change_journal = volume.get_usn_change_journal()\n      self._ParseUSNChangeJournal(parser_mediator, usn_change_journal)\n    finally:\n      volume.close()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing a shell item.", "response": "def _ParseShellItem(self, parser_mediator, shell_item):\n    \"\"\"Parses a shell item.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      shell_item (pyfwsi.item): shell item.\n    \"\"\"\n    path_segment = self._ParseShellItemPathSegment(shell_item)\n    self._path_segments.append(path_segment)\n\n    event_data = shell_item_events.ShellItemFileEntryEventData()\n    event_data.origin = self._origin\n    event_data.shell_item_path = self.CopyToPath()\n\n    if isinstance(shell_item, pyfwsi.file_entry):\n      event_data.name = shell_item.name\n\n      for extension_block in shell_item.extension_blocks:\n        if isinstance(extension_block, pyfwsi.file_entry_extension):\n          long_name = extension_block.long_name\n          localized_name = extension_block.localized_name\n          file_reference = extension_block.file_reference\n          if file_reference:\n            file_reference = '{0:d}-{1:d}'.format(\n                file_reference & 0xffffffffffff, file_reference >> 48)\n\n          event_data.file_reference = file_reference\n          event_data.localized_name = localized_name\n          event_data.long_name = long_name\n\n          fat_date_time = extension_block.get_creation_time_as_integer()\n          if fat_date_time != 0:\n            date_time = dfdatetime_fat_date_time.FATDateTime(\n                fat_date_time=fat_date_time)\n            event = time_events.DateTimeValuesEvent(\n                date_time, definitions.TIME_DESCRIPTION_CREATION)\n            parser_mediator.ProduceEventWithEventData(event, event_data)\n\n          fat_date_time = extension_block.get_access_time_as_integer()\n          if fat_date_time != 0:\n            date_time = dfdatetime_fat_date_time.FATDateTime(\n                fat_date_time=fat_date_time)\n            event = time_events.DateTimeValuesEvent(\n                date_time, definitions.TIME_DESCRIPTION_LAST_ACCESS)\n            parser_mediator.ProduceEventWithEventData(event, event_data)\n\n      fat_date_time = shell_item.get_modification_time_as_integer()\n      if fat_date_time != 0:\n        date_time = dfdatetime_fat_date_time.FATDateTime(\n            fat_date_time=fat_date_time)\n        event = time_events.DateTimeValuesEvent(\n            date_time, definitions.TIME_DESCRIPTION_MODIFICATION)\n        parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _ParseShellItemPathSegment(self, shell_item):\n    path_segment = None\n\n    if isinstance(shell_item, pyfwsi.root_folder):\n      description = shell_folder_ids.DESCRIPTIONS.get(\n          shell_item.shell_folder_identifier, None)\n\n      if description:\n        path_segment = description\n      else:\n        path_segment = '{{{0:s}}}'.format(shell_item.shell_folder_identifier)\n\n      path_segment = '<{0:s}>'.format(path_segment)\n\n    elif isinstance(shell_item, pyfwsi.volume):\n      if shell_item.name:\n        path_segment = shell_item.name\n      elif shell_item.identifier:\n        path_segment = '{{{0:s}}}'.format(shell_item.identifier)\n\n    elif isinstance(shell_item, pyfwsi.file_entry):\n      long_name = ''\n      for extension_block in shell_item.extension_blocks:\n        if isinstance(extension_block, pyfwsi.file_entry_extension):\n          long_name = extension_block.long_name\n\n      if long_name:\n        path_segment = long_name\n      elif shell_item.name:\n        path_segment = shell_item.name\n\n    elif isinstance(shell_item, pyfwsi.network_location):\n      if shell_item.location:\n        path_segment = shell_item.location\n\n    if path_segment is None and shell_item.class_type == 0x00:\n      # TODO: check for signature 0x23febbee\n      pass\n\n    if path_segment is None:\n      path_segment = '<UNKNOWN: 0x{0:02x}>'.format(shell_item.class_type)\n\n    return path_segment", "response": "Parses a shell item path segment."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncopying the shell items to a path.", "response": "def CopyToPath(self):\n    \"\"\"Copies the shell items to a path.\n\n    Returns:\n      str: converted shell item list path or None.\n    \"\"\"\n    number_of_path_segments = len(self._path_segments)\n    if number_of_path_segments == 0:\n      return None\n\n    strings = [self._path_segments[0]]\n    number_of_path_segments -= 1\n    for path_segment in self._path_segments[1:]:\n      # Remove a trailing \\ except for the last path segment.\n      if path_segment.endswith('\\\\') and number_of_path_segments > 1:\n        path_segment = path_segment[:-1]\n\n      if ((path_segment.startswith('<') and path_segment.endswith('>')) or\n          len(strings) == 1):\n        strings.append(' {0:s}'.format(path_segment))\n      elif path_segment.startswith('\\\\'):\n        strings.append('{0:s}'.format(path_segment))\n      else:\n        strings.append('\\\\{0:s}'.format(path_segment))\n      number_of_path_segments -= 1\n\n    return ''.join(strings)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ParseByteStream(\n      self, parser_mediator, byte_stream, parent_path_segments=None,\n      codepage='cp1252'):\n    \"\"\"Parses the shell items from the byte stream.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      byte_stream (bytes): shell items data.\n      parent_path_segments (Optional[list[str]]): parent shell item path\n          segments.\n      codepage (Optional[str]): byte stream codepage.\n    \"\"\"\n    if parent_path_segments and isinstance(parent_path_segments, list):\n      self._path_segments = list(parent_path_segments)\n    else:\n      self._path_segments = []\n\n    shell_item_list = pyfwsi.item_list()\n\n    parser_mediator.AppendToParserChain(self)\n    try:\n      shell_item_list.copy_from_byte_stream(\n          byte_stream, ascii_codepage=codepage)\n\n      for shell_item in iter(shell_item_list.items):\n        self._ParseShellItem(parser_mediator, shell_item)\n    finally:\n      parser_mediator.PopFromParserChain()", "response": "Parses the shell items from a byte stream."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate the year to use for events based on last observed month.", "response": "def _UpdateYear(self, mediator, month):\n    \"\"\"Updates the year to use for events, based on last observed month.\n\n    Args:\n      mediator (ParserMediator): mediates the interactions between\n          parsers and other components, such as storage and abort signals.\n      month (int): month observed by the parser, where January is 1.\n    \"\"\"\n    if not self._year_use:\n      self._year_use = mediator.GetEstimatedYear()\n    if not self._maximum_year:\n      self._maximum_year = mediator.GetLatestYear()\n\n    if not self._last_month:\n      self._last_month = month\n      return\n\n    # Some syslog daemons allow out-of-order sequences, so allow some leeway\n    # to not cause Apr->May->Apr to cause the year to increment.\n    # See http://bugzilla.adiscon.com/show_bug.cgi?id=527\n    if self._last_month > (month + 1):\n      if self._year_use != self._maximum_year:\n        self._year_use += 1\n    self._last_month = month"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef EnablePlugins(self, plugin_includes):\n    super(SyslogParser, self).EnablePlugins(plugin_includes)\n\n    self._plugin_by_reporter = {}\n    for plugin in self._plugins:\n      self._plugin_by_reporter[plugin.REPORTER] = plugin", "response": "Enables parser plugins.\n\n    Args:\n      plugin_includes (list[str]): names of the plugins to enable, where None\n          or an empty list represents all plugins. Note that the default plugin\n          is handled separately."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse a matching entry. Args: parser_mediator (ParserMediator): mediates interactions between parsers and other components, such as storage and dfvfs. key (str): name of the parsed structure. structure (pyparsing.ParseResults): elements parsed from the file. Raises: ParseError: when the structure type is unknown.", "response": "def ParseRecord(self, parser_mediator, key, structure):\n    \"\"\"Parses a matching entry.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      key (str): name of the parsed structure.\n      structure (pyparsing.ParseResults): elements parsed from the file.\n\n    Raises:\n      ParseError: when the structure type is unknown.\n    \"\"\"\n    if key not in self._SUPPORTED_KEYS:\n      raise errors.ParseError(\n          'Unable to parse record, unknown structure: {0:s}'.format(key))\n\n    if key == 'chromeos_syslog_line':\n      date_time = dfdatetime_time_elements.TimeElementsInMicroseconds()\n\n      try:\n        date_time.CopyFromStringISO8601(structure.chromeos_date)\n      except ValueError:\n        parser_mediator.ProduceExtractionWarning(\n            'invalid date time value: {0:s}'.format(structure.chromeos_date))\n        return\n\n    else:\n      # TODO: add support for fractional seconds.\n\n      month = timelib.MONTH_DICT.get(structure.month.lower(), 0)\n      if month != 0:\n        self._UpdateYear(parser_mediator, month)\n\n      time_elements_tuple = (\n          self._year_use, month, structure.day, structure.hour,\n          structure.minute, structure.second)\n\n      try:\n        date_time = dfdatetime_time_elements.TimeElements(\n            time_elements_tuple=time_elements_tuple)\n        date_time.is_local_time = True\n      except ValueError:\n        parser_mediator.ProduceExtractionWarning(\n            'invalid date time value: {0!s}'.format(time_elements_tuple))\n        return\n\n    plugin = None\n    if key == 'syslog_comment':\n      event_data = SyslogCommentEventData()\n      event_data.body = structure.body\n      # TODO: pass line number to offset or remove.\n      event_data.offset = 0\n\n    else:\n      event_data = SyslogLineEventData()\n      event_data.body = structure.body\n      event_data.hostname = structure.hostname or None\n      # TODO: pass line number to offset or remove.\n      event_data.offset = 0\n      event_data.pid = structure.pid\n      event_data.reporter = structure.reporter\n      event_data.severity = structure.severity\n\n      plugin = self._plugin_by_reporter.get(structure.reporter, None)\n      if plugin:\n        attributes = {\n            'hostname': structure.hostname,\n            'severity': structure.severity,\n            'reporter': structure.reporter,\n            'pid': structure.pid,\n            'body': structure.body}\n\n        try:\n          # TODO: pass event_data instead of attributes.\n          plugin.Process(parser_mediator, date_time, attributes)\n\n        except errors.WrongPlugin:\n          plugin = None\n\n    if not plugin:\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_WRITTEN)\n      parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef VerifyStructure(self, parser_mediator, lines):\n    return (re.match(self._VERIFICATION_REGEX, lines) or\n            re.match(self._CHROMEOS_VERIFICATION_REGEX, lines)) is not None", "response": "Verifies that this is a syslog - formatted file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef PopEvent(self):\n    try:\n      _, _, _, event = heapq.heappop(self._heap)\n      return event\n\n    except IndexError:\n      return None", "response": "Pops an event from the heap."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\npushes an event onto the heap.", "response": "def PushEvent(self, event):\n    \"\"\"Pushes an event onto the heap.\n\n    Args:\n      event (EventObject): event.\n    \"\"\"\n    event_string = event.GetAttributeValuesString()\n    heap_values = (event.timestamp, event.timestamp_desc, event_string, event)\n    heapq.heappush(self._heap, heap_values)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef PopEvent(self):\n    try:\n      timestamp, serialized_event = heapq.heappop(self._heap)\n\n      self.data_size -= len(serialized_event)\n      return timestamp, serialized_event\n\n    except IndexError:\n      return None, None", "response": "Pops an event from the heap."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\npush a serialized event onto the heap.", "response": "def PushEvent(self, timestamp, event_data):\n    \"\"\"Pushes a serialized event onto the heap.\n\n    Args:\n      timestamp (int): event timestamp, which contains the number of\n          micro seconds since January 1, 1970, 00:00:00 UTC.\n      event_data (bytes): serialized event.\n    \"\"\"\n    heap_values = (timestamp, event_data)\n    heapq.heappush(self._heap, heap_values)\n    self.data_size += len(event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef hostname(self):\n    hostname_artifact = self._hostnames.get(self.CURRENT_SESSION, None)\n    if not hostname_artifact:\n      return ''\n\n    return hostname_artifact.name or ''", "response": "str - hostname of the current session"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a user account to the internal user accounts list.", "response": "def AddUserAccount(self, user_account, session_identifier=CURRENT_SESSION):\n    \"\"\"Adds an user account.\n\n    Args:\n      user_account (UserAccountArtifact): user account artifact.\n      session_identifier (Optional[str])): session identifier, where\n          CURRENT_SESSION represents the active session.\n\n    Raises:\n      KeyError: if the user account already exists.\n    \"\"\"\n    if session_identifier not in self._user_accounts:\n      self._user_accounts[session_identifier] = {}\n\n    user_accounts = self._user_accounts[session_identifier]\n    if user_account.identifier in user_accounts:\n      raise KeyError('User account: {0:s} already exists.'.format(\n          user_account.identifier))\n\n    user_accounts[user_account.identifier] = user_account"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef AddEnvironmentVariable(self, environment_variable):\n    name = environment_variable.name.upper()\n    if name in self._environment_variables:\n      raise KeyError('Environment variable: {0:s} already exists.'.format(\n          environment_variable.name))\n\n    self._environment_variables[name] = environment_variable", "response": "Adds an environment variable to the internal list of environment variables."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving an environment variable from the environment variable list.", "response": "def GetEnvironmentVariable(self, name):\n    \"\"\"Retrieves an environment variable.\n\n    Args:\n      name (str): name of the environment variable.\n\n    Returns:\n      EnvironmentVariableArtifact: environment variable artifact or None\n          if there was no value set for the given name.\n    \"\"\"\n    name = name.upper()\n    return self._environment_variables.get(name, None)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nretrieve the hostname related to the event.", "response": "def GetHostname(self, session_identifier=CURRENT_SESSION):\n    \"\"\"Retrieves the hostname related to the event.\n\n    If the hostname is not stored in the event it is determined based\n    on the preprocessing information that is stored inside the storage file.\n\n    Args:\n      session_identifier (Optional[str])): session identifier, where\n          CURRENT_SESSION represents the active session.\n\n    Returns:\n      str: hostname.\n    \"\"\"\n    hostname_artifact = self._hostnames.get(session_identifier, None)\n    if not hostname_artifact:\n      return ''\n\n    return hostname_artifact.name or ''"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef GetStoredHostname(self):\n    store_number = len(self._hostnames)\n    return self._hostnames.get(store_number, None)", "response": "Retrieves the stored hostname."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef GetSystemConfigurationArtifact(self, session_identifier=CURRENT_SESSION):\n    system_configuration = artifacts.SystemConfigurationArtifact()\n\n    system_configuration.code_page = self.GetValue(\n        'codepage', default_value=self._codepage)\n\n    system_configuration.hostname = self._hostnames.get(\n        session_identifier, None)\n\n    system_configuration.keyboard_layout = self.GetValue('keyboard_layout')\n    system_configuration.operating_system = self.GetValue('operating_system')\n    system_configuration.operating_system_product = self.GetValue(\n        'operating_system_product')\n    system_configuration.operating_system_version = self.GetValue(\n        'operating_system_version')\n\n    date_time = datetime.datetime(2017, 1, 1)\n    time_zone = self._time_zone.tzname(date_time)\n\n    if time_zone and isinstance(time_zone, py2to3.BYTES_TYPE):\n      time_zone = time_zone.decode('ascii')\n\n    system_configuration.time_zone = time_zone\n\n    user_accounts = self._user_accounts.get(session_identifier, {})\n    # In Python 3 dict.values() returns a type dict_values, which will cause\n    # the JSON serializer to raise a TypeError.\n    system_configuration.user_accounts = list(user_accounts.values())\n\n    return system_configuration", "response": "Retrieves the knowledge base as a system configuration artifact."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretrieves the username based on a user identifier.", "response": "def GetUsernameByIdentifier(\n      self, user_identifier, session_identifier=CURRENT_SESSION):\n    \"\"\"Retrieves the username based on an user identifier.\n\n    Args:\n      user_identifier (str): user identifier, either a UID or SID.\n      session_identifier (Optional[str])): session identifier, where\n          CURRENT_SESSION represents the active session.\n\n    Returns:\n      str: username.\n    \"\"\"\n    user_accounts = self._user_accounts.get(session_identifier, {})\n    user_account = user_accounts.get(user_identifier, None)\n    if not user_account:\n      return ''\n\n    return user_account.username or ''"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef GetUsernameForPath(self, path):\n    path = path.lower()\n\n    user_accounts = self._user_accounts.get(self.CURRENT_SESSION, {})\n    for user_account in iter(user_accounts.values()):\n      if not user_account.user_directory:\n        continue\n\n      user_directory = user_account.user_directory.lower()\n      if path.startswith(user_directory):\n        return user_account.username\n\n    return None", "response": "Retrieves a username for a specific path."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nretrieves a value by identifier.", "response": "def GetValue(self, identifier, default_value=None):\n    \"\"\"Retrieves a value by identifier.\n\n    Args:\n      identifier (str): case insensitive unique identifier for the value.\n      default_value (object): default value.\n\n    Returns:\n      object: value or default value if not available.\n\n    Raises:\n      TypeError: if the identifier is not a string type.\n    \"\"\"\n    if not isinstance(identifier, py2to3.STRING_TYPES):\n      raise TypeError('Identifier not a string type.')\n\n    identifier = identifier.lower()\n    return self._values.get(identifier, default_value)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread the knowledge base values from a system configuration artifact.", "response": "def ReadSystemConfigurationArtifact(\n      self, system_configuration, session_identifier=CURRENT_SESSION):\n    \"\"\"Reads the knowledge base values from a system configuration artifact.\n\n    Note that this overwrites existing values in the knowledge base.\n\n    Args:\n      system_configuration (SystemConfigurationArtifact): system configuration\n          artifact.\n      session_identifier (Optional[str])): session identifier, where\n          CURRENT_SESSION represents the active session.\n    \"\"\"\n    if system_configuration.code_page:\n      try:\n        self.SetCodepage(system_configuration.code_page)\n      except ValueError:\n        logger.warning(\n            'Unsupported codepage: {0:s}, defaulting to {1:s}'.format(\n                system_configuration.code_page, self._codepage))\n\n    self._hostnames[session_identifier] = system_configuration.hostname\n\n    self.SetValue('keyboard_layout', system_configuration.keyboard_layout)\n\n    self.SetValue('operating_system', system_configuration.operating_system)\n    self.SetValue(\n        'operating_system_product',\n        system_configuration.operating_system_product)\n    self.SetValue(\n        'operating_system_version',\n        system_configuration.operating_system_version)\n\n    if system_configuration.time_zone:\n      try:\n        self.SetTimeZone(system_configuration.time_zone)\n      except ValueError:\n        logger.warning(\n            'Unsupported time zone: {0:s}, defaulting to {1:s}'.format(\n                system_configuration.time_zone, self.timezone.zone))\n\n    self._user_accounts[session_identifier] = {\n        user_account.username: user_account\n        for user_account in system_configuration.user_accounts}"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the codepage. Args: codepage (str): codepage. Raises: ValueError: if the codepage is not supported.", "response": "def SetCodepage(self, codepage):\n    \"\"\"Sets the codepage.\n\n    Args:\n      codepage (str): codepage.\n\n    Raises:\n      ValueError: if the codepage is not supported.\n    \"\"\"\n    try:\n      codecs.getencoder(codepage)\n      self._codepage = codepage\n    except LookupError:\n      raise ValueError('Unsupported codepage: {0:s}'.format(codepage))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef SetEnvironmentVariable(self, environment_variable):\n    name = environment_variable.name.upper()\n    self._environment_variables[name] = environment_variable", "response": "Sets an environment variable."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the time zone of the object.", "response": "def SetTimeZone(self, time_zone):\n    \"\"\"Sets the time zone.\n\n    Args:\n      time_zone (str): time zone.\n\n    Raises:\n      ValueError: if the timezone is not supported.\n    \"\"\"\n    try:\n      self._time_zone = pytz.timezone(time_zone)\n    except (AttributeError, pytz.UnknownTimeZoneError):\n      raise ValueError('Unsupported timezone: {0!s}'.format(time_zone))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets a value by identifier.", "response": "def SetValue(self, identifier, value):\n    \"\"\"Sets a value by identifier.\n\n    Args:\n      identifier (str): case insensitive unique identifier for the value.\n      value (object): value.\n\n    Raises:\n      TypeError: if the identifier is not a string type.\n    \"\"\"\n    if not isinstance(identifier, py2to3.STRING_TYPES):\n      raise TypeError('Identifier not a string type.')\n\n    identifier = identifier.lower()\n    self._values[identifier] = value"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _CreateAnalysisPlugins(self, options):\n    if not self._analysis_plugins:\n      return {}\n\n    analysis_plugins = (\n        analysis_manager.AnalysisPluginManager.GetPluginObjects(\n            self._analysis_plugins))\n\n    for analysis_plugin in analysis_plugins.values():\n      helpers_manager.ArgumentHelperManager.ParseOptions(\n          options, analysis_plugin)\n\n    return analysis_plugins", "response": "Creates the analysis plugins."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlists the analysis modules.", "response": "def ListAnalysisPlugins(self):\n    \"\"\"Lists the analysis modules.\"\"\"\n    analysis_plugin_info = (\n        analysis_manager.AnalysisPluginManager.GetAllPluginInformation())\n\n    column_width = 10\n    for name, _, _ in analysis_plugin_info:\n      if len(name) > column_width:\n        column_width = len(name)\n\n    table_view = views.ViewsFactory.GetTableView(\n        self._views_format_type, column_names=['Name', 'Description'],\n        title='Analysis Plugins')\n    # TODO: add support for a 3 column table.\n    for name, description, type_string in analysis_plugin_info:\n      description = '{0:s} [{1:s}]'.format(description, type_string)\n      table_view.AddRow([name, description])\n    table_view.Write(self._output_writer)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlists the available hashers.", "response": "def ListHashers(self):\n    \"\"\"Lists information about the available hashers.\"\"\"\n    hashers_information = hashers_manager.HashersManager.GetHashersInformation()\n\n    table_view = views.ViewsFactory.GetTableView(\n        self._views_format_type, column_names=['Name', 'Description'],\n        title='Hashers')\n\n    for name, description in sorted(hashers_information):\n      table_view.AddRow([name, description])\n    table_view.Write(self._output_writer)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _CreateOutputModule(self, options):\n    formatter_mediator = formatters_mediator.FormatterMediator(\n        data_location=self._data_location)\n\n    try:\n      formatter_mediator.SetPreferredLanguageIdentifier(\n          self._preferred_language)\n    except (KeyError, TypeError) as exception:\n      raise RuntimeError(exception)\n\n    mediator = output_mediator.OutputMediator(\n        self._knowledge_base, formatter_mediator,\n        preferred_encoding=self.preferred_encoding)\n    mediator.SetTimezone(self._preferred_time_zone)\n\n    try:\n      output_module = output_manager.OutputManager.NewOutputModule(\n          self._output_format, mediator)\n\n    except (KeyError, ValueError) as exception:\n      raise RuntimeError(\n          'Unable to create output module with error: {0!s}'.format(\n              exception))\n\n    if output_manager.OutputManager.IsLinearOutputModule(self._output_format):\n      output_file_object = open(self._output_filename, 'wb')\n      output_writer = tools.FileObjectOutputWriter(output_file_object)\n      output_module.SetOutputWriter(output_writer)\n\n    helpers_manager.ArgumentHelperManager.ParseOptions(options, output_module)\n\n    # Check if there are parameters that have not been defined and need to\n    # in order for the output module to continue. Prompt user to supply\n    # those that may be missing.\n    missing_parameters = output_module.GetMissingArguments()\n    while missing_parameters:\n      for parameter in missing_parameters:\n        value = self._PromptUserForInput(\n            'Missing parameter {0:s} for output module'.format(parameter))\n        if value is None:\n          logger.warning(\n              'Unable to set the missing parameter for: {0:s}'.format(\n                  parameter))\n          continue\n\n        setattr(options, parameter, value)\n\n      helpers_manager.ArgumentHelperManager.ParseOptions(\n          options, output_module)\n      missing_parameters = output_module.GetMissingArguments()\n\n    return output_module", "response": "Creates the output module."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ListLanguageIdentifiers(self):\n    table_view = views.ViewsFactory.GetTableView(\n        self._views_format_type, column_names=['Identifier', 'Language'],\n        title='Language identifiers')\n    for language_id, value_list in sorted(\n        language_ids.LANGUAGE_IDENTIFIERS.items()):\n      table_view.AddRow([language_id, value_list[1]])\n    table_view.Write(self._output_writer)", "response": "Lists the language identifiers."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nretrieve the output modules information.", "response": "def _GetOutputModulesInformation(self):\n    \"\"\"Retrieves the output modules information.\n\n    Returns:\n      list[tuple[str, str]]: pairs of output module names and descriptions.\n    \"\"\"\n    output_modules_information = []\n    for name, output_class in output_manager.OutputManager.GetOutputClasses():\n      output_modules_information.append((name, output_class.DESCRIPTION))\n\n    return output_modules_information"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ListOutputModules(self):\n    table_view = views.ViewsFactory.GetTableView(\n        self._views_format_type, column_names=['Name', 'Description'],\n        title='Output Modules')\n\n    for name, output_class in output_manager.OutputManager.GetOutputClasses():\n      table_view.AddRow([name, output_class.DESCRIPTION])\n    table_view.Write(self._output_writer)\n\n    disabled_classes = list(\n        output_manager.OutputManager.GetDisabledOutputClasses())\n    if not disabled_classes:\n      return\n\n    table_view = views.ViewsFactory.GetTableView(\n        self._views_format_type, column_names=['Name', 'Description'],\n        title='Disabled Output Modules')\n    for name, output_class in disabled_classes:\n      table_view.AddRow([name, output_class.DESCRIPTION])\n    table_view.Write(self._output_writer)", "response": "Lists the output modules."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ListProfilers(self):\n    table_view = views.ViewsFactory.GetTableView(\n        self._views_format_type, column_names=['Name', 'Description'],\n        title='Profilers')\n\n    profilers_information = sorted(\n        profiling.ProfilingArgumentsHelper.PROFILERS_INFORMATION.items())\n    for name, description in profilers_information:\n      table_view.AddRow([name, description])\n    table_view.Write(self._output_writer)", "response": "Lists information about the available profilers."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncopy the identifier to a string representation.", "response": "def CopyToString(self):\n    \"\"\"Copies the identifier to a string representation.\n\n    Returns:\n      str: unique identifier or None.\n    \"\"\"\n    if self.stream_number is not None and self.entry_index is not None:\n      return '{0:d}.{1:d}'.format(self.stream_number, self.entry_index)\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncopy the identifier to a string representation.", "response": "def CopyToString(self):\n    \"\"\"Copies the identifier to a string representation.\n\n    Returns:\n      str: unique identifier or None.\n    \"\"\"\n    if self.name is not None and self.row_identifier is not None:\n      return '{0:s}.{1:d}'.format(self.name, self.row_identifier)\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nextracts events from a Windows Registry key.", "response": "def ExtractEvents(self, parser_mediator, registry_key, **kwargs):\n    \"\"\"Extracts events from a Windows Registry key.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      registry_key (dfwinreg.WinRegistryKey): Windows Registry key.\n    \"\"\"\n    event_data = windows_events.WindowsRegistryEventData()\n    event_data.key_path = registry_key.path\n    event_data.offset = registry_key.offset\n    event_data.urls = self.URLS\n\n    values_dict = {}\n    for registry_value in registry_key.GetValues():\n      value_name = registry_value.name or '(default)'\n\n      if value_name == 'BootExecute':\n        # MSDN: claims that the data type of this value is REG_BINARY\n        # although REG_MULTI_SZ is known to be used as well.\n        if registry_value.DataIsString():\n          value_string = registry_value.GetDataAsObject()\n\n        elif registry_value.DataIsMultiString():\n          value_string = ''.join(registry_value.GetDataAsObject())\n\n        elif registry_value.DataIsBinaryData():\n          value_string = registry_value.GetDataAsObject()\n\n        else:\n          value_string = ''\n          error_string = (\n              'Key: {0:s}, value: {1:s}: unsupported value data type: '\n              '{2:s}.').format(\n                  registry_key.path, value_name,\n                  registry_value.data_type_string)\n          parser_mediator.ProduceExtractionWarning(error_string)\n\n        # TODO: why does this have a separate event object? Remove this.\n        event_data.regvalue = {'BootExecute': value_string}\n\n        event = time_events.DateTimeValuesEvent(\n            registry_key.last_written_time,\n            definitions.TIME_DESCRIPTION_WRITTEN)\n        parser_mediator.ProduceEventWithEventData(event, event_data)\n\n      else:\n        values_dict[value_name] = registry_value.GetDataAsObject()\n\n    event_data.regvalue = values_dict\n\n    event = time_events.DateTimeValuesEvent(\n        registry_key.last_written_time, definitions.TIME_DESCRIPTION_WRITTEN)\n    parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef Process(self, parser_mediator, plist_name, top_level, **kwargs):\n    if not plist_name.startswith(self.PLIST_PATH):\n      raise errors.WrongPlistPlugin(self.NAME, plist_name)\n    super(AppleAccountPlugin, self).Process(\n        parser_mediator, plist_name=self.PLIST_PATH, top_level=top_level)", "response": "Check if the plist file name is a valid Apple account plist file name."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nextract relevant Apple Account entries.", "response": "def GetEntries(self, parser_mediator, match=None, **unused_kwargs):\n    \"\"\"Extracts relevant Apple Account entries.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      match (Optional[dict[str: object]]): keys extracted from PLIST_KEYS.\n    \"\"\"\n    accounts = match.get('Accounts', {})\n    for name_account, account in iter(accounts.items()):\n      first_name = account.get('FirstName', '<FirstName>')\n      last_name = account.get('LastName', '<LastName>')\n      general_description = '{0:s} ({1:s} {2:s})'.format(\n          name_account, first_name, last_name)\n\n      event_data = plist_event.PlistTimeEventData()\n      event_data.key = name_account\n      event_data.root = '/Accounts'\n\n      datetime_value = account.get('CreationDate', None)\n      if datetime_value:\n        event_data.desc = 'Configured Apple account {0:s}'.format(\n            general_description)\n\n        event = time_events.PythonDatetimeEvent(\n            datetime_value, definitions.TIME_DESCRIPTION_WRITTEN)\n        parser_mediator.ProduceEventWithEventData(event, event_data)\n\n      datetime_value = account.get('LastSuccessfulConnect', None)\n      if datetime_value:\n        event_data.desc = 'Connected Apple account {0:s}'.format(\n            general_description)\n\n        event = time_events.PythonDatetimeEvent(\n            datetime_value, definitions.TIME_DESCRIPTION_WRITTEN)\n        parser_mediator.ProduceEventWithEventData(event, event_data)\n\n      datetime_value = account.get('ValidationDate', None)\n      if datetime_value:\n        event_data.desc = 'Last validation Apple account {0:s}'.format(\n            general_description)\n\n        event = time_events.PythonDatetimeEvent(\n            datetime_value, definitions.TIME_DESCRIPTION_WRITTEN)\n        parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef AddArguments(cls, argument_group):\n    shared_4n6time_output.Shared4n6TimeOutputArgumentsHelper.AddArguments(\n        argument_group)\n    MySQL4n6TimeDatabaseArgumentsHelper.AddArguments(argument_group)", "response": "Adds command line arguments to an argument group."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ParseOptions(cls, options, output_module):\n    if not isinstance(output_module, mysql_4n6time.MySQL4n6TimeOutputModule):\n      raise errors.BadConfigObject(\n          'Output module is not an instance of MySQL4n6TimeOutputModule')\n\n    MySQL4n6TimeDatabaseArgumentsHelper.ParseOptions(options, output_module)\n    shared_4n6time_output.Shared4n6TimeOutputArgumentsHelper.ParseOptions(\n        options, output_module)", "response": "Parses and validates options."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ExtractEvents(self, parser_mediator, registry_key, **kwargs):\n    mru_values_dict = {}\n    for subkey in registry_key.GetSubkeys():\n      username_value = subkey.GetValueByName('UsernameHint')\n\n      if (username_value and username_value.data and\n          username_value.DataIsString()):\n        username = username_value.GetDataAsObject()\n      else:\n        username = 'N/A'\n\n      mru_values_dict[subkey.name] = username\n\n      event_data = windows_events.WindowsRegistryEventData()\n      event_data.key_path = subkey.path\n      event_data.offset = subkey.offset\n      event_data.regvalue = {'Username hint': username}\n      event_data.source_append = self._SOURCE_APPEND\n\n      event = time_events.DateTimeValuesEvent(\n          subkey.last_written_time, definitions.TIME_DESCRIPTION_WRITTEN)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    event_data = windows_events.WindowsRegistryEventData()\n    event_data.key_path = registry_key.path\n    event_data.offset = registry_key.offset\n    event_data.regvalue = mru_values_dict\n    event_data.source_append = self._SOURCE_APPEND\n\n    event = time_events.DateTimeValuesEvent(\n        registry_key.last_written_time, definitions.TIME_DESCRIPTION_WRITTEN)\n    parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Extracts events from a Terminal Server Client Windows Registry key."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses a bencoded file - like object.", "response": "def ParseFileObject(self, parser_mediator, file_object):\n    \"\"\"Parses a bencoded file-like object.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      file_object (dfvfs.FileIO): a file-like object.\n\n    Raises:\n      UnableToParseFile: when the file cannot be parsed.\n    \"\"\"\n    file_object.seek(0, os.SEEK_SET)\n    header = file_object.read(2)\n    if not self.BENCODE_RE.match(header):\n      raise errors.UnableToParseFile('Not a valid Bencoded file.')\n\n    file_object.seek(0, os.SEEK_SET)\n    try:\n      data_object = bencode.bdecode(file_object.read())\n\n    except (IOError, bencode.BTFailure) as exception:\n      raise errors.UnableToParseFile(\n          '[{0:s}] unable to parse file: {1:s} with error: {2!s}'.format(\n              self.NAME, parser_mediator.GetDisplayName(), exception))\n\n    if not data_object:\n      raise errors.UnableToParseFile(\n          '[{0:s}] missing decoded data for file: {1:s}'.format(\n              self.NAME, parser_mediator.GetDisplayName()))\n\n    for plugin in self._plugins:\n      try:\n        plugin.UpdateChainAndProcess(parser_mediator, data=data_object)\n      except errors.WrongBencodePlugin as exception:\n        logger.debug('[{0:s}] wrong plugin: {1!s}'.format(\n            self.NAME, exception))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _ParseHeader(self, parser_mediator, structure):\n    try:\n      date_time = dfdatetime_time_elements.TimeElementsInMilliseconds(\n          time_elements_tuple=structure.header_date_time)\n    except ValueError:\n      parser_mediator.ProduceExtractionWarning(\n          'invalid date time value: {0!s}'.format(structure.header_date_time))\n      return\n\n    event_data = SkyDriveLogEventData()\n    # TODO: refactor detail to individual event data attributes.\n    event_data.detail = '{0:s} {1:s} {2:s} {3:s} {4:s}'.format(\n        structure.log_start, structure.version_string,\n        structure.version_number, structure.local_time_string,\n        structure.details)\n\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_ADDED)\n    parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Parses the header lines and stores appropriate attributes."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses a logline and stores appropriate attributes.", "response": "def _ParseLine(self, parser_mediator, structure):\n    \"\"\"Parses a logline and store appropriate attributes.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      structure (pyparsing.ParseResults): structure of tokens derived from\n          a line of a text file.\n    \"\"\"\n    # TODO: Verify if date and time value is locale dependent.\n    month, day_of_month, year, hours, minutes, seconds, milliseconds = (\n        structure.date_time)\n\n    year += 2000\n    time_elements_tuple = (\n        year, month, day_of_month, hours, minutes, seconds, milliseconds)\n\n    try:\n      date_time = dfdatetime_time_elements.TimeElementsInMilliseconds(\n          time_elements_tuple=time_elements_tuple)\n    except ValueError:\n      parser_mediator.ProduceExtractionWarning(\n          'invalid date time value: {0!s}'.format(structure.date_time))\n      return\n\n    event_data = SkyDriveLogEventData()\n    # Replace newlines with spaces in structure.detail to preserve output.\n    # TODO: refactor detail to individual event data attributes.\n    event_data.detail = structure.detail.replace('\\n', ' ')\n    event_data.log_level = structure.log_level\n    event_data.module = structure.module\n    event_data.source_code = structure.source_code\n\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_ADDED)\n    parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing a single record structure and returns an EventObject if applicable.", "response": "def ParseRecord(self, parser_mediator, key, structure):\n    \"\"\"Parse each record structure and return an EventObject if applicable.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      key (str): identifier of the structure of tokens.\n      structure (pyparsing.ParseResults): structure of tokens derived from\n          a line of a text file.\n\n    Raises:\n      ParseError: when the structure type is unknown.\n    \"\"\"\n    if key not in ('header', 'logline'):\n      raise errors.ParseError(\n          'Unable to parse record, unknown structure: {0:s}'.format(key))\n\n    if key == 'logline':\n      self._ParseLine(parser_mediator, structure)\n\n    elif key == 'header':\n      self._ParseHeader(parser_mediator, structure)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nverifies that this file is a SkyDrive log file.", "response": "def VerifyStructure(self, parser_mediator, lines):\n    \"\"\"Verify that this file is a SkyDrive log file.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      lines (str): one or more lines from the text file.\n\n    Returns:\n      bool: True if this is the correct parser, False otherwise.\n    \"\"\"\n    try:\n      structure = self._SDF_HEADER.parseString(lines)\n    except pyparsing.ParseException:\n      logger.debug('Not a SkyDrive log file')\n      return False\n\n    try:\n      dfdatetime_time_elements.TimeElementsInMilliseconds(\n          time_elements_tuple=structure.header_date_time)\n    except ValueError:\n      logger.debug(\n          'Not a SkyDrive log file, invalid date and time: {0!s}'.format(\n              structure.header_date_time))\n      return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _ParseLogline(self, parser_mediator, structure):\n    # TODO: Verify if date and time value is locale dependent.\n    month, day_of_month, year, hours, minutes, seconds, milliseconds = (\n        structure.date_time)\n\n    time_elements_tuple = (\n        year, month, day_of_month, hours, minutes, seconds, milliseconds)\n\n    try:\n      date_time = dfdatetime_time_elements.TimeElementsInMilliseconds(\n          time_elements_tuple=time_elements_tuple)\n    except ValueError:\n      parser_mediator.ProduceExtractionWarning(\n          'invalid date time value: {0!s}'.format(structure.date_time))\n      return\n\n    event_data = SkyDriveOldLogEventData()\n    event_data.log_level = structure.log_level\n    event_data.offset = self.offset\n    event_data.source_code = structure.source_code\n    event_data.text = structure.text\n\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_ADDED)\n    parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    self._last_date_time = date_time\n    self._last_event_data = event_data", "response": "Parses a logline and stores appropriate attributes."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing an isolated header line and stores appropriate attributes.", "response": "def _ParseNoHeaderSingleLine(self, parser_mediator, structure):\n    \"\"\"Parse an isolated header line and store appropriate attributes.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      structure (pyparsing.ParseResults): structure of tokens derived from\n          a line of a text file.\n    \"\"\"\n    if not self._last_event_data:\n      logger.debug('SkyDrive, found isolated line with no previous events')\n      return\n\n    event_data = SkyDriveOldLogEventData()\n    event_data.offset = self._last_event_data.offset\n    event_data.text = structure.text\n\n    event = time_events.DateTimeValuesEvent(\n        self._last_date_time, definitions.TIME_DESCRIPTION_ADDED)\n    parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    # TODO think to a possible refactoring for the non-header lines.\n    self._last_date_time = None\n    self._last_event_data = None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse a record structure and returns an EventObject if applicable.", "response": "def ParseRecord(self, parser_mediator, key, structure):\n    \"\"\"Parse each record structure and return an EventObject if applicable.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      key (str): identifier of the structure of tokens.\n      structure (pyparsing.ParseResults): structure of tokens derived from\n          a line of a text file.\n\n    Raises:\n      ParseError: when the structure type is unknown.\n    \"\"\"\n    if key not in ('logline', 'no_header_single_line'):\n      raise errors.ParseError(\n          'Unable to parse record, unknown structure: {0:s}'.format(key))\n\n    if key == 'logline':\n      self._ParseLogline(parser_mediator, structure)\n\n    elif key == 'no_header_single_line':\n      self._ParseNoHeaderSingleLine(parser_mediator, structure)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef VerifyStructure(self, parser_mediator, line):\n    try:\n      structure = self._LINE.parseString(line)\n    except pyparsing.ParseException:\n      logger.debug('Not a SkyDrive old log file')\n      return False\n\n    day_of_month, month, year, hours, minutes, seconds, milliseconds = (\n        structure.date_time)\n\n    time_elements_tuple = (\n        year, month, day_of_month, hours, minutes, seconds, milliseconds)\n\n    try:\n      dfdatetime_time_elements.TimeElementsInMilliseconds(\n          time_elements_tuple=time_elements_tuple)\n    except ValueError:\n      logger.debug(\n          'Not a SkyDrive old log file, invalid date and time: {0!s}'.format(\n              structure.date_time))\n      return False\n\n    return True", "response": "Verify that this file is a SkyDrive old log file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing a Video row.", "response": "def ParseVideoRow(self, parser_mediator, query, row, **unused_kwargs):\n    \"\"\"Parses a Video row.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      query (str): query that created the row.\n      row (sqlite3.Row): row.\n    \"\"\"\n    query_hash = hash(query)\n\n    event_data = KodiVideoEventData()\n    event_data.filename = self._GetRowValue(query_hash, row, 'strFilename')\n    event_data.play_count = self._GetRowValue(query_hash, row, 'playCount')\n    event_data.query = query\n\n    timestamp = self._GetRowValue(query_hash, row, 'lastPlayed')\n    date_time = dfdatetime_time_elements.TimeElements()\n    date_time.CopyFromDateTimeString(timestamp)\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_LAST_VISITED)\n    parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ParseOptions(cls, options, configuration_object):\n    if not isinstance(configuration_object, tools.CLITool):\n      raise errors.BadConfigObject(\n          'Configuration object is not an instance of CLITool')\n\n    parsers = cls._ParseStringOption(options, 'parsers', default_value='')\n    parsers = parsers.replace('\\\\', '/')\n\n    # TODO: validate parser names.\n\n    setattr(configuration_object, '_parser_filter_expression', parsers)", "response": "Parses and validates the options."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncopy attributes from a session completion.", "response": "def CopyAttributesFromSessionCompletion(self, session_completion):\n    \"\"\"Copies attributes from a session completion.\n\n    Args:\n      session_completion (SessionCompletion): session completion attribute\n          container.\n\n    Raises:\n      ValueError: if the identifier of the session completion does not match\n          that of the session.\n    \"\"\"\n    if self.identifier != session_completion.identifier:\n      raise ValueError('Session identifier mismatch.')\n\n    self.aborted = session_completion.aborted\n\n    if session_completion.analysis_reports_counter:\n      self.analysis_reports_counter = (\n          session_completion.analysis_reports_counter)\n\n    self.completion_time = session_completion.timestamp\n\n    if session_completion.event_labels_counter:\n      self.event_labels_counter = session_completion.event_labels_counter\n\n    if session_completion.parsers_counter:\n      self.parsers_counter = session_completion.parsers_counter"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef CreateSessionCompletion(self):\n    self.completion_time = int(time.time() * 1000000)\n\n    session_completion = SessionCompletion()\n    session_completion.aborted = self.aborted\n    session_completion.analysis_reports_counter = self.analysis_reports_counter\n    session_completion.event_labels_counter = self.event_labels_counter\n    session_completion.identifier = self.identifier\n    session_completion.parsers_counter = self.parsers_counter\n    session_completion.timestamp = self.completion_time\n    return session_completion", "response": "Creates a session completion."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef CreateSessionStart(self):\n    session_start = SessionStart()\n    session_start.artifact_filters = self.artifact_filters\n    session_start.command_line_arguments = self.command_line_arguments\n    session_start.debug_mode = self.debug_mode\n    session_start.enabled_parser_names = self.enabled_parser_names\n    session_start.filter_file = self.filter_file\n    session_start.identifier = self.identifier\n    session_start.parser_filter_expression = self.parser_filter_expression\n    session_start.preferred_encoding = self.preferred_encoding\n    session_start.preferred_time_zone = self.preferred_time_zone\n    session_start.product_name = self.product_name\n    session_start.product_version = self.product_version\n    session_start.timestamp = self.start_time\n    return session_start", "response": "Creates a session start."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse a numeric command line argument.", "response": "def _ParseNumericOption(cls, options, argument_name, default_value=None):\n    \"\"\"Parses a numeric command line argument.\n\n    Args:\n      options (argparse.Namespace): parser options.\n      argument_name (str): name of the command line argument.\n      default_value (Optional[int]): default value of the command line argument.\n\n    Returns:\n      int: command line argument value or the default value if the command line\n          argument is not set\n\n    Raises:\n      BadConfigOption: if the command line argument value cannot be converted\n          to a Unicode string.\n    \"\"\"\n    argument_value = getattr(options, argument_name, None)\n    if argument_value is None:\n      return default_value\n\n    if not isinstance(argument_value, py2to3.INTEGER_TYPES):\n      raise errors.BadConfigOption(\n          'Unsupported option: {0:s} integer type required.'.format(\n              argument_name))\n\n    return argument_value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing a string command line argument.", "response": "def _ParseStringOption(cls, options, argument_name, default_value=None):\n    \"\"\"Parses a string command line argument.\n\n    Args:\n      options (argparse.Namespace): parser options.\n      argument_name (str): name of the command line argument.\n      default_value (Optional[str]): default value of the command line argument.\n\n    Returns:\n      str: command line argument value or the default value if the command line\n          argument is not set\n\n    Raises:\n      BadConfigOption: if the command line argument value cannot be converted\n          to a Unicode string.\n    \"\"\"\n    argument_value = getattr(options, argument_name, None)\n    if argument_value is None:\n      return default_value\n\n    if isinstance(argument_value, py2to3.BYTES_TYPE):\n      encoding = sys.stdin.encoding\n\n      # Note that sys.stdin.encoding can be None.\n      if not encoding:\n        encoding = locale.getpreferredencoding()\n      if not encoding:\n        encoding = cls._PREFERRED_ENCODING\n\n      try:\n        argument_value = argument_value.decode(encoding)\n      except UnicodeDecodeError as exception:\n        raise errors.BadConfigOption((\n            'Unable to convert option: {0:s} to Unicode with error: '\n            '{1!s}.').format(argument_name, exception))\n\n    elif not isinstance(argument_value, py2to3.UNICODE_TYPE):\n      raise errors.BadConfigOption(\n          'Unsupported option: {0:s} string type required.'.format(\n              argument_name))\n\n    return argument_value"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nretrieves the format specification.", "response": "def GetFormatSpecification(cls):\n    \"\"\"Retrieves the format specification.\n\n    Returns:\n      FormatSpecification: format specification.\n    \"\"\"\n    format_specification = specification.FormatSpecification(cls.NAME)\n    format_specification.AddNewSignature(cls._DLS_V1_SIGNATURE, offset=0)\n    format_specification.AddNewSignature(cls._DLS_V2_SIGNATURE, offset=0)\n    return format_specification"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse a DLS page header from a file - like object.", "response": "def _ParseDLSPageHeader(self, file_object, page_offset):\n    \"\"\"Parses a DLS page header from a file-like object.\n\n    Args:\n      file_object (file): file-like object to read the header from.\n      page_offset (int): offset of the start of the page header, relative\n          to the start of the file.\n\n    Returns:\n      tuple: containing:\n\n        dls_page_header: parsed record structure.\n        int: header size.\n\n    Raises:\n      ParseError: when the header cannot be parsed.\n    \"\"\"\n    page_header_map = self._GetDataTypeMap('dls_page_header')\n\n    try:\n      page_header, page_size = self._ReadStructureFromFileObject(\n          file_object, page_offset, page_header_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.ParseError(\n          'Unable to parse page header at offset: 0x{0:08x} '\n          'with error: {1!s}'.format(page_offset, exception))\n\n    if page_header.signature not in self._DLS_SIGNATURES:\n      raise errors.UnableToParseFile(\n          'Unsupported page header signature at offset: 0x{0:08x}'.format(\n              page_offset))\n\n    return page_header, page_size"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nbuilding an event data attribute container from a parsed record structure.", "response": "def _BuildEventData(self, record):\n    \"\"\"Builds an FseventsdData object from a parsed structure.\n\n    Args:\n      record (dls_record_v1|dls_record_v2): parsed record structure.\n\n    Returns:\n      FseventsdEventData: event data attribute container.\n    \"\"\"\n    event_data = FseventsdEventData()\n    event_data.path = record.path\n    event_data.flags = record.event_flags\n    event_data.event_identifier = record.event_identifier\n    # Node identifier is only set in DLS V2 records.\n    event_data.node_identifier = getattr(record, 'node_identifier', None)\n\n    return event_data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving the modification time of the file entry s parent file.", "response": "def _GetParentModificationTime(self, gzip_file_entry):\n    \"\"\"Retrieves the modification time of the file entry's parent file.\n\n    Note that this retrieves the time from the file entry of the parent of the\n    gzip file entry's path spec, which is different from trying to retrieve it\n    from the gzip file entry's parent file entry.\n\n    It would be preferable to retrieve the modification time from the metadata\n    in the gzip file itself, but it appears to not be set when the file is\n    written by fseventsd.\n\n    Args:\n      gzip_file_entry (dfvfs.FileEntry): file entry of the gzip file containing\n          the fseventsd data.\n\n    Returns:\n      dfdatetime.DateTimeValues: parent modification time, or None if not\n          available.\n    \"\"\"\n    parent_file_entry = path_spec_resolver.Resolver.OpenFileEntry(\n        gzip_file_entry.path_spec.parent)\n    if not parent_file_entry:\n      return None\n\n    return parent_file_entry.modification_time"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses a DLS file - like object.", "response": "def ParseFileObject(self, parser_mediator, file_object):\n    \"\"\"Parses an fseventsd file.\n\n    Args:\n      parser_mediator (ParserMediator): parser mediator.\n      file_object (dfvfs.FileIO): a file-like object.\n\n    Raises:\n      UnableToParseFile: when the header cannot be parsed.\n    \"\"\"\n    page_header_map = self._GetDataTypeMap('dls_page_header')\n\n    try:\n      page_header, file_offset = self._ReadStructureFromFileObject(\n          file_object, 0, page_header_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.UnableToParseFile(\n          'Unable to parse page header with error: {0!s}'.format(\n              exception))\n\n    if page_header.signature not in self._DLS_SIGNATURES:\n      raise errors.UnableToParseFile('Invalid file signature')\n\n    current_page_end = page_header.page_size\n\n    file_entry = parser_mediator.GetFileEntry()\n    date_time = self._GetParentModificationTime(file_entry)\n    # TODO: Change this to use a more representative time definition (time span)\n    # when https://github.com/log2timeline/dfdatetime/issues/65 is resolved.\n    if date_time:\n      timestamp_description = definitions.TIME_DESCRIPTION_RECORDED\n    else:\n      date_time = dfdatetime_semantic_time.SemanticTime('Not set')\n      timestamp_description = definitions.TIME_DESCRIPTION_NOT_A_TIME\n    event = time_events.DateTimeValuesEvent(date_time, timestamp_description)\n\n    file_size = file_object.get_size()\n    while file_offset < file_size:\n      if file_offset >= current_page_end:\n        try:\n          page_header, header_size = self._ParseDLSPageHeader(\n              file_object, file_offset)\n        except errors.ParseError as exception:\n          parser_mediator.ProduceExtractionWarning(\n              'Unable to parse page header with error: {0!s}'.format(\n                  exception))\n          break\n\n        current_page_end += page_header.page_size\n        file_offset += header_size\n        continue\n\n      if page_header.signature == self._DLS_V1_SIGNATURE:\n        record_map = self._GetDataTypeMap('dls_record_v1')\n      else:\n        record_map = self._GetDataTypeMap('dls_record_v2')\n\n      try:\n        record, record_length = self._ReadStructureFromFileObject(\n            file_object, file_offset, record_map)\n        file_offset += record_length\n      except (ValueError, errors.ParseError) as exception:\n        parser_mediator.ProduceExtractionWarning(\n            'Unable to parse page record with error: {0!s}'.format(\n                exception))\n        break\n\n      event_data = self._BuildEventData(record)\n      parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef DocumentVersionsRow(\n      self, parser_mediator, query, row, **unused_kwargs):\n    \"\"\"Parses a document versions row.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      query (str): query that created the row.\n      row (sqlite3.Row): row.\n    \"\"\"\n    query_hash = hash(query)\n\n    # version_path = \"PerUser/UserID/xx/client_id/version_file\"\n    # where PerUser and UserID are a real directories.\n    version_path = self._GetRowValue(query_hash, row, 'version_path')\n    path = self._GetRowValue(query_hash, row, 'path')\n\n    paths = version_path.split('/')\n    if len(paths) < 2 or not paths[1].isdigit():\n      user_sid = ''\n    else:\n      user_sid = paths[1]\n    version_path = self.ROOT_VERSION_PATH + version_path\n    path, _, _ = path.rpartition('/')\n\n    event_data = MacDocumentVersionsEventData()\n    # TODO: shouldn't this be a separate event?\n    event_data.last_time = self._GetRowValue(query_hash, row, 'last_time')\n    event_data.name = self._GetRowValue(query_hash, row, 'name')\n    event_data.path = path\n    event_data.query = query\n    # Note that the user_sid value is expected to be a string.\n    event_data.user_sid = '{0!s}'.format(user_sid)\n    event_data.version_path = version_path\n\n    timestamp = self._GetRowValue(query_hash, row, 'version_time')\n    date_time = dfdatetime_posix_time.PosixTime(timestamp=timestamp)\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_CREATION)\n    parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Parses a document versions row."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreads an utmp entry.", "response": "def _ReadEntry(self, parser_mediator, file_object, file_offset):\n    \"\"\"Reads an utmp entry.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      file_object (dfvfs.FileIO): a file-like object.\n      file_offset (int): offset of the data relative to the start of\n          the file-like object.\n\n    Returns:\n      tuple: containing:\n\n        int: timestamp, which contains the number of microseconds\n            since January 1, 1970, 00:00:00 UTC.\n        UtmpEventData: event data of the utmp entry read.\n\n    Raises:\n      ParseError: if the entry cannot be parsed.\n    \"\"\"\n    entry_map = self._GetDataTypeMap('linux_libc6_utmp_entry')\n\n    try:\n      entry, _ = self._ReadStructureFromFileObject(\n          file_object, file_offset, entry_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.ParseError((\n          'Unable to parse utmp entry at offset: 0x{0:08x} with error: '\n          '{1!s}.').format(file_offset, exception))\n\n    if entry.type not in self._SUPPORTED_TYPES:\n      raise errors.UnableToParseFile('Unsupported type: {0:d}'.format(\n          entry.type))\n\n    encoding = parser_mediator.codepage or 'utf-8'\n\n    try:\n      username = entry.username.split(b'\\x00')[0]\n      username = username.decode(encoding)\n    except UnicodeDecodeError:\n      parser_mediator.ProduceExtractionWarning(\n          'unable to decode username string')\n      username = None\n\n    try:\n      terminal = entry.terminal.split(b'\\x00')[0]\n      terminal = terminal.decode(encoding)\n    except UnicodeDecodeError:\n      parser_mediator.ProduceExtractionWarning(\n          'unable to decode terminal string')\n      terminal = None\n\n    if terminal == '~':\n      terminal = 'system boot'\n\n    try:\n      hostname = entry.hostname.split(b'\\x00')[0]\n      hostname = hostname.decode(encoding)\n    except UnicodeDecodeError:\n      parser_mediator.ProduceExtractionWarning(\n          'unable to decode hostname string')\n      hostname = None\n\n    if not hostname or hostname == ':0':\n      hostname = 'localhost'\n\n    if entry.ip_address[4:] == self._EMPTY_IP_ADDRESS[4:]:\n      ip_address = self._FormatPackedIPv4Address(entry.ip_address[:4])\n    else:\n      ip_address = self._FormatPackedIPv6Address(entry.ip_address)\n\n    # TODO: add termination status.\n    event_data = UtmpEventData()\n    event_data.hostname = hostname\n    event_data.exit_status = entry.exit_status\n    event_data.ip_address = ip_address\n    event_data.offset = file_offset\n    event_data.pid = entry.pid\n    event_data.terminal = terminal\n    event_data.terminal_identifier = entry.terminal_identifier\n    event_data.type = entry.type\n    event_data.username = username\n\n    timestamp = entry.microseconds + (\n        entry.timestamp * definitions.MICROSECONDS_PER_SECOND)\n    return timestamp, event_data"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ParseFileObject(self, parser_mediator, file_object):\n    file_offset = 0\n\n    try:\n      timestamp, event_data = self._ReadEntry(\n          parser_mediator, file_object, file_offset)\n    except errors.ParseError as exception:\n      raise errors.UnableToParseFile(\n          'Unable to parse first utmp entry with error: {0!s}'.format(\n              exception))\n\n    if not event_data.username:\n      raise errors.UnableToParseFile(\n          'Unable to parse first utmp entry with error: missing username')\n\n    if not timestamp:\n      raise errors.UnableToParseFile(\n          'Unable to parse first utmp entry with error: missing timestamp')\n\n    date_time = dfdatetime_posix_time.PosixTimeInMicroseconds(\n        timestamp=timestamp)\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_START)\n    parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    file_offset = file_object.tell()\n    file_size = file_object.get_size()\n\n    while file_offset < file_size:\n      if parser_mediator.abort:\n        break\n\n      try:\n        timestamp, event_data = self._ReadEntry(\n            parser_mediator, file_object, file_offset)\n      except errors.ParseError:\n        # Note that the utmp file can contain trailing data.\n        break\n\n      date_time = dfdatetime_posix_time.PosixTimeInMicroseconds(\n          timestamp=timestamp)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_START)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n      file_offset = file_object.tell()", "response": "Parses a utmp file - like object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ParseFileObject(self, parser_mediator, file_object):\n    data = file_object.read(self._HEADER_READ_SIZE)\n    if not data.startswith(b'<?xml'):\n      raise errors.UnableToParseFile(\n          'Not an Opera typed history file [not a XML]')\n\n    _, _, data = data.partition(b'\\n')\n    if not data.startswith(b'<typed_history'):\n      raise errors.UnableToParseFile(\n          'Not an Opera typed history file [wrong XML root key]')\n\n    # For ElementTree to work we need to work on a file object seeked\n    # to the beginning.\n    file_object.seek(0, os.SEEK_SET)\n\n    xml = ElementTree.parse(file_object)\n\n    for history_item in xml.iterfind('typed_history_item'):\n      event_data = OperaTypedHistoryEventData()\n      event_data.entry_type = history_item.get('type', None)\n      event_data.url = history_item.get('content', None)\n\n      if event_data.entry_type == 'selected':\n        event_data.entry_selection = 'Filled from autocomplete.'\n      elif event_data.entry_type == 'text':\n        event_data.entry_selection = 'Manually typed.'\n\n      last_typed_time = history_item.get('last_typed', None)\n      if last_typed_time is None:\n        parser_mediator.ProduceExtractionWarning('missing last typed time.')\n        continue\n\n      date_time = dfdatetime_time_elements.TimeElements()\n\n      try:\n        date_time.CopyFromStringISO8601(last_typed_time)\n      except ValueError as exception:\n        parser_mediator.ProduceExtractionWarning(\n            'unsupported last typed time: {0:s} with error: {1!s}.'.format(\n                last_typed_time, exception))\n        continue\n\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_LAST_VISITED)\n      parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Parses an Opera typed history file - like object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck if an URL is considered valid.", "response": "def _IsValidUrl(self, url):\n    \"\"\"Checks if an URL is considered valid.\n\n    Returns:\n      bool: True if the URL is valid.\n    \"\"\"\n    parsed_url = urlparse.urlparse(url)\n    return parsed_url.scheme in self._SUPPORTED_URL_SCHEMES"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses an Opera global history record.", "response": "def _ParseRecord(self, parser_mediator, text_file_object):\n    \"\"\"Parses an Opera global history record.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      text_file_object (dfvfs.TextFile): text file.\n\n    Returns:\n      bool: True if the record was successfully parsed.\n    \"\"\"\n    try:\n      title = text_file_object.readline()\n    except UnicodeDecodeError:\n      parser_mediator.ProduceExtractionWarning(\n          'unable to read and decode title')\n      return False\n\n    if not title:\n      return False\n\n    try:\n      url = text_file_object.readline()\n    except UnicodeDecodeError:\n      parser_mediator.ProduceExtractionWarning(\n          'unable to read and decode url')\n      return False\n\n    try:\n      timestamp = text_file_object.readline()\n    except UnicodeDecodeError:\n      parser_mediator.ProduceExtractionWarning(\n          'unable to read and decode timestamp')\n      return False\n\n    try:\n      popularity_index = text_file_object.readline()\n    except UnicodeDecodeError:\n      parser_mediator.ProduceExtractionWarning(\n          'unable to read and decode popularity index')\n      return False\n\n    event_data = OperaGlobalHistoryEventData()\n\n    event_data.url = url.strip()\n\n    title = title.strip()\n    if title != event_data.url:\n      event_data.title = title\n\n    popularity_index = popularity_index.strip()\n    try:\n      event_data.popularity_index = int(popularity_index, 10)\n    except ValueError:\n      parser_mediator.ProduceExtractionWarning(\n          'unable to convert popularity index: {0:s}'.format(popularity_index))\n\n    if event_data.popularity_index < 0:\n      event_data.description = 'First and Only Visit'\n    else:\n      event_data.description = 'Last Visit'\n\n    timestamp = timestamp.strip()\n    try:\n      timestamp = int(timestamp, 10)\n    except ValueError:\n      parser_mediator.ProduceExtractionWarning(\n          'unable to convert timestamp: {0:s}'.format(timestamp))\n      timestamp = None\n\n    if timestamp is None:\n      date_time = dfdatetime_semantic_time.SemanticTime('Invalid')\n    else:\n      date_time = dfdatetime_posix_time.PosixTime(timestamp=timestamp)\n\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_LAST_VISITED)\n    parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse and validates an Opera global history record.", "response": "def _ParseAndValidateRecord(self, parser_mediator, text_file_object):\n    \"\"\"Parses and validates an Opera global history record.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      text_file_object (dfvfs.TextFile): text file.\n\n    Returns:\n      bool: True if the record was successfully parsed.\n    \"\"\"\n    try:\n      title = text_file_object.readline(size=self._MAXIMUM_LINE_SIZE)\n      url = text_file_object.readline(size=self._MAXIMUM_LINE_SIZE)\n      timestamp = text_file_object.readline(size=self._MAXIMUM_LINE_SIZE)\n      popularity_index = text_file_object.readline(size=self._MAXIMUM_LINE_SIZE)\n    except UnicodeDecodeError:\n      return False\n\n    if len(title) == self._MAXIMUM_LINE_SIZE and title[-1] != '\\n':\n      return False\n\n    if len(url) == self._MAXIMUM_LINE_SIZE and url[-1] != '\\n':\n      return False\n\n    if len(timestamp) == self._MAXIMUM_LINE_SIZE and timestamp[-1] != '\\n':\n      return False\n\n    if (len(popularity_index) == self._MAXIMUM_LINE_SIZE and\n        popularity_index[-1] != '\\n'):\n      return False\n\n    title = title.strip()\n    url = url.strip()\n    timestamp = timestamp.strip()\n    popularity_index = popularity_index.strip()\n\n    if not title or not url or not timestamp or not popularity_index:\n      return False\n\n    event_data = OperaGlobalHistoryEventData()\n\n    if not self._IsValidUrl(url):\n      return False\n\n    event_data.url = url\n    if title != url:\n      event_data.title = title\n\n    try:\n      event_data.popularity_index = int(popularity_index, 10)\n      timestamp = int(timestamp, 10)\n    except ValueError:\n      return False\n\n    if event_data.popularity_index < 0:\n      event_data.description = 'First and Only Visit'\n    else:\n      event_data.description = 'Last Visit'\n\n    date_time = dfdatetime_posix_time.PosixTime(timestamp=timestamp)\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_LAST_VISITED)\n    parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses an Opera global history file - like object.", "response": "def ParseFileObject(self, parser_mediator, file_object):\n    \"\"\"Parses an Opera global history file-like object.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      file_object (dfvfs.FileIO): file-like object.\n\n    Raises:\n      UnableToParseFile: when the file cannot be parsed.\n    \"\"\"\n    encoding = self._ENCODING or parser_mediator.codepage\n    text_file_object = text_file.TextFile(file_object, encoding=encoding)\n    if not self._ParseAndValidateRecord(parser_mediator, text_file_object):\n      raise errors.UnableToParseFile(\n          'Unable to parse as Opera global_history.dat.')\n\n    while self._ParseRecord(parser_mediator, text_file_object):\n      pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses the original filename.", "response": "def _ParseOriginalFilename(self, file_object, format_version):\n    \"\"\"Parses the original filename.\n\n    Args:\n      file_object (FileIO): file-like object.\n      format_version (int): format version.\n\n    Returns:\n      str: filename or None on error.\n\n    Raises:\n      ParseError: if the original filename cannot be read.\n    \"\"\"\n    file_offset = file_object.tell()\n\n    if format_version == 1:\n      data_type_map = self._GetDataTypeMap(\n          'recycle_bin_metadata_utf16le_string')\n    else:\n      data_type_map = self._GetDataTypeMap(\n          'recycle_bin_metadata_utf16le_string_with_size')\n\n    try:\n      original_filename, _ = self._ReadStructureFromFileObject(\n          file_object, file_offset, data_type_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.ParseError(\n          'Unable to parse original filename with error: {0!s}'.format(\n              exception))\n\n    if format_version == 1:\n      return original_filename.rstrip('\\x00')\n\n    return original_filename.string.rstrip('\\x00')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing a Windows Recycle. Bin metadata file - like object.", "response": "def ParseFileObject(self, parser_mediator, file_object):\n    \"\"\"Parses a Windows Recycle.Bin metadata ($I) file-like object.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      file_object (dfvfs.FileIO): file-like object.\n\n    Raises:\n      UnableToParseFile: when the file cannot be parsed.\n    \"\"\"\n    # We may have to rely on filenames since this header is very generic.\n\n    # TODO: Rethink this and potentially make a better test.\n    filename = parser_mediator.GetFilename()\n    if not filename.startswith('$I'):\n      raise errors.UnableToParseFile('Filename must start with $I.')\n\n    file_header_map = self._GetDataTypeMap('recycle_bin_metadata_file_header')\n\n    try:\n      file_header, _ = self._ReadStructureFromFileObject(\n          file_object, 0, file_header_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.UnableToParseFile((\n          'Unable to parse Windows Recycle.Bin metadata file header with '\n          'error: {0!s}').format(exception))\n\n    if file_header.format_version not in self._SUPPORTED_FORMAT_VERSIONS:\n      raise errors.UnableToParseFile(\n          'Unsupported format version: {0:d}.'.format(\n              file_header.format_version))\n\n    if file_header.deletion_time == 0:\n      date_time = dfdatetime_semantic_time.SemanticTime('Not set')\n    else:\n      date_time = dfdatetime_filetime.Filetime(\n          timestamp=file_header.deletion_time)\n\n    event_data = WinRecycleBinEventData()\n    try:\n      event_data.original_filename = self._ParseOriginalFilename(\n          file_object, file_header.format_version)\n    except (ValueError, errors.ParseError) as exception:\n      parser_mediator.ProduceExtractionWarning(\n          'unable to parse original filename with error: {0!s}.'.format(\n              exception))\n\n    event_data.file_size = file_header.original_file_size\n\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_DELETED)\n    parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing an INFO - 2 record.", "response": "def _ParseInfo2Record(\n      self, parser_mediator, file_object, record_offset, record_size):\n    \"\"\"Parses an INFO-2 record.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      file_object (dfvfs.FileIO): file-like object.\n      record_offset (int): record offset.\n      record_size (int): record size.\n\n    Raises:\n      ParseError: if the record cannot be read.\n    \"\"\"\n    record_data = self._ReadData(file_object, record_offset, record_size)\n\n    record_map = self._GetDataTypeMap('recycler_info2_file_entry')\n\n    try:\n      record = self._ReadStructureFromByteStream(\n          record_data, record_offset, record_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.ParseError((\n          'Unable to map record data at offset: 0x{0:08x} with error: '\n          '{1!s}').format(record_offset, exception))\n\n    codepage = parser_mediator.codepage or 'ascii'\n\n    # The original filename can contain remnant data after the end-of-string\n    # character.\n    ascii_filename = record.original_filename.split(b'\\x00')[0]\n\n    try:\n      ascii_filename = ascii_filename.decode(codepage)\n    except UnicodeDecodeError:\n      ascii_filename = ascii_filename.decode(codepage, errors='replace')\n\n      parser_mediator.ProduceExtractionWarning(\n          'unable to decode original filename.')\n\n    unicode_filename = None\n    if record_size > 280:\n      record_offset += 280\n      utf16_string_map = self._GetDataTypeMap(\n          'recycler_info2_file_entry_utf16le_string')\n\n      try:\n        unicode_filename = self._ReadStructureFromByteStream(\n            record_data[280:], record_offset, utf16_string_map)\n      except (ValueError, errors.ParseError) as exception:\n        raise errors.ParseError((\n            'Unable to map record data at offset: 0x{0:08x} with error: '\n            '{1!s}').format(record_offset, exception))\n\n      unicode_filename = unicode_filename.rstrip('\\x00')\n\n    if record.deletion_time == 0:\n      date_time = dfdatetime_semantic_time.SemanticTime('Not set')\n    else:\n      date_time = dfdatetime_filetime.Filetime(\n          timestamp=record.deletion_time)\n\n    event_data = WinRecycleBinEventData()\n    event_data.drive_number = record.drive_number\n    event_data.original_filename = unicode_filename or ascii_filename\n    event_data.file_size = record.original_file_size\n    event_data.offset = record_offset\n    event_data.record_index = record.index\n\n    if ascii_filename != unicode_filename:\n      event_data.short_filename = ascii_filename\n\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_DELETED)\n    parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses a Windows Recycler INFO2 file - like object.", "response": "def ParseFileObject(self, parser_mediator, file_object):\n    \"\"\"Parses a Windows Recycler INFO2 file-like object.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      file_object (dfvfs.FileIO): file-like object.\n\n    Raises:\n      UnableToParseFile: when the file cannot be parsed.\n    \"\"\"\n    # Since this header value is really generic it is hard not to use filename\n    # as an indicator too.\n\n    # TODO: Rethink this and potentially make a better test.\n    filename = parser_mediator.GetFilename()\n    if not filename.startswith('INFO2'):\n      return\n\n    file_header_map = self._GetDataTypeMap('recycler_info2_file_header')\n\n    try:\n      file_header, _ = self._ReadStructureFromFileObject(\n          file_object, 0, file_header_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.UnableToParseFile((\n          'Unable to parse Windows Recycler INFO2 file header with '\n          'error: {0!s}').format(exception))\n\n    if file_header.unknown1 != 5:\n      parser_mediator.ProduceExtractionWarning('unsupported format signature.')\n      return\n\n    file_entry_size = file_header.file_entry_size\n    if file_entry_size not in (280, 800):\n      parser_mediator.ProduceExtractionWarning(\n          'unsupported file entry size: {0:d}'.format(file_entry_size))\n      return\n\n    file_offset = file_object.get_offset()\n    file_size = file_object.get_size()\n\n    while file_offset < file_size:\n      self._ParseInfo2Record(\n          parser_mediator, file_object, file_offset, file_entry_size)\n\n      file_offset += file_entry_size"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a credential configuration.", "response": "def _AddCredentialConfiguration(\n      self, path_spec, credential_type, credential_data):\n    \"\"\"Adds a credential configuration.\n\n    Args:\n      path_spec (dfvfs.PathSpec): path specification.\n      credential_type (str): credential type.\n      credential_data (bytes): credential data.\n    \"\"\"\n    credential_configuration = configurations.CredentialConfiguration(\n        credential_data=credential_data, credential_type=credential_type,\n        path_spec=path_spec)\n\n    self._credential_configurations.append(credential_configuration)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _FormatHumanReadableSize(self, size):\n    magnitude_1000 = 0\n    size_1000 = float(size)\n    while size_1000 >= 1000:\n      size_1000 /= 1000\n      magnitude_1000 += 1\n\n    magnitude_1024 = 0\n    size_1024 = float(size)\n    while size_1024 >= 1024:\n      size_1024 /= 1024\n      magnitude_1024 += 1\n\n    size_string_1000 = None\n    if 0 < magnitude_1000 <= 7:\n      size_string_1000 = '{0:.1f}{1:s}'.format(\n          size_1000, self._UNITS_1000[magnitude_1000])\n\n    size_string_1024 = None\n    if 0 < magnitude_1024 <= 7:\n      size_string_1024 = '{0:.1f}{1:s}'.format(\n          size_1024, self._UNITS_1024[magnitude_1024])\n\n    if not size_string_1000 or not size_string_1024:\n      return '{0:d} B'.format(size)\n\n    return '{0:s} / {1:s} ({2:d} B)'.format(\n        size_string_1024, size_string_1000, size)", "response": "Formats a number of bytes as a human readable string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndetermine the APFS volume identifiers.", "response": "def _GetAPFSVolumeIdentifiers(self, scan_node):\n    \"\"\"Determines the APFS volume identifiers.\n\n    Args:\n      scan_node (dfvfs.SourceScanNode): scan node.\n\n    Returns:\n      list[str]: APFS volume identifiers.\n\n    Raises:\n      SourceScannerError: if the format of or within the source is not\n          supported or the the scan node is invalid.\n      UserAbort: if the user requested to abort.\n    \"\"\"\n    if not scan_node or not scan_node.path_spec:\n      raise errors.SourceScannerError('Invalid scan node.')\n\n    volume_system = apfs_volume_system.APFSVolumeSystem()\n    volume_system.Open(scan_node.path_spec)\n\n    volume_identifiers = self._source_scanner.GetVolumeIdentifiers(\n        volume_system)\n    if not volume_identifiers:\n      return []\n\n    # TODO: refactor self._volumes to use scan options.\n    if self._volumes:\n      if self._volumes == 'all':\n        volumes = range(1, volume_system.number_of_volumes + 1)\n      else:\n        volumes = self._volumes\n\n      selected_volume_identifiers = self._NormalizedVolumeIdentifiers(\n          volume_system, volumes, prefix='apfs')\n\n      if not set(selected_volume_identifiers).difference(volume_identifiers):\n        return selected_volume_identifiers\n\n    if len(volume_identifiers) > 1:\n      try:\n        volume_identifiers = self._PromptUserForAPFSVolumeIdentifiers(\n            volume_system, volume_identifiers)\n      except KeyboardInterrupt:\n        raise errors.UserAbort('File system scan aborted.')\n\n    return self._NormalizedVolumeIdentifiers(\n        volume_system, volume_identifiers, prefix='apfs')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndetermine the TSK partition identifiers.", "response": "def _GetTSKPartitionIdentifiers(self, scan_node):\n    \"\"\"Determines the TSK partition identifiers.\n\n    This method first checks for the preferred partition number, then\n    falls back to prompt the user if no usable preferences were specified.\n\n    Args:\n      scan_node (dfvfs.SourceScanNode): scan node.\n\n    Returns:\n      list[str]: TSK partition identifiers.\n\n    Raises:\n      RuntimeError: if the volume for a specific identifier cannot be\n          retrieved.\n      SourceScannerError: if the format of or within the source\n          is not supported or the the scan node is invalid.\n    \"\"\"\n    if not scan_node or not scan_node.path_spec:\n      raise errors.SourceScannerError('Invalid scan node.')\n\n    volume_system = tsk_volume_system.TSKVolumeSystem()\n    volume_system.Open(scan_node.path_spec)\n\n    volume_identifiers = self._source_scanner.GetVolumeIdentifiers(\n        volume_system)\n    if not volume_identifiers:\n      return []\n\n    # TODO: refactor self._partitions to use scan options.\n    if self._partitions:\n      if self._partitions == 'all':\n        partitions = range(1, volume_system.number_of_volumes + 1)\n      else:\n        partitions = self._partitions\n\n      selected_volume_identifiers = self._NormalizedVolumeIdentifiers(\n          volume_system, partitions, prefix='p')\n\n      if not set(selected_volume_identifiers).difference(volume_identifiers):\n        return selected_volume_identifiers\n\n    if len(volume_identifiers) == 1:\n      return volume_identifiers\n\n    try:\n      volume_identifiers = self._PromptUserForPartitionIdentifiers(\n          volume_system, volume_identifiers)\n    except KeyboardInterrupt:\n      raise errors.UserAbort('File system scan aborted.')\n\n    return self._NormalizedVolumeIdentifiers(\n        volume_system, volume_identifiers, prefix='p')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndetermining the VSS store identifiers.", "response": "def _GetVSSStoreIdentifiers(self, scan_node):\n    \"\"\"Determines the VSS store identifiers.\n\n    Args:\n      scan_node (dfvfs.SourceScanNode): scan node.\n\n    Returns:\n      list[str]: VSS store identifiers.\n\n    Raises:\n      SourceScannerError: if the format of or within the source is not\n          supported or the scan node is invalid.\n      UserAbort: if the user requested to abort.\n    \"\"\"\n    if not scan_node or not scan_node.path_spec:\n      raise errors.SourceScannerError('Invalid scan node.')\n\n    volume_system = vshadow_volume_system.VShadowVolumeSystem()\n    volume_system.Open(scan_node.path_spec)\n\n    volume_identifiers = self._source_scanner.GetVolumeIdentifiers(\n        volume_system)\n    if not volume_identifiers:\n      return []\n\n    # TODO: refactor to use scan options.\n    if self._vss_stores:\n      if self._vss_stores == 'all':\n        vss_stores = range(1, volume_system.number_of_volumes + 1)\n      else:\n        vss_stores = self._vss_stores\n\n      selected_volume_identifiers = self._NormalizedVolumeIdentifiers(\n          volume_system, vss_stores, prefix='vss')\n\n      if not set(selected_volume_identifiers).difference(volume_identifiers):\n        return selected_volume_identifiers\n\n    try:\n      volume_identifiers = self._PromptUserForVSSStoreIdentifiers(\n          volume_system, volume_identifiers)\n\n    except KeyboardInterrupt:\n      raise errors.UserAbort('File system scan aborted.')\n\n    return self._NormalizedVolumeIdentifiers(\n        volume_system, volume_identifiers, prefix='vss')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _ParseCredentialOptions(self, options):\n    credentials = getattr(options, 'credentials', [])\n    if not isinstance(credentials, list):\n      raise errors.BadConfigOption('Unsupported credentials value.')\n\n    for credential_string in credentials:\n      credential_type, _, credential_data = credential_string.partition(':')\n      if not credential_type or not credential_data:\n        raise errors.BadConfigOption(\n            'Badly formatted credential: {0:s}.'.format(credential_string))\n\n      if credential_type not in self._SUPPORTED_CREDENTIAL_TYPES:\n        raise errors.BadConfigOption(\n            'Unsupported credential type for: {0:s}.'.format(\n                credential_string))\n\n      if credential_type in self._BINARY_DATA_CREDENTIAL_TYPES:\n        try:\n          credential_data = credential_data.decode('hex')\n        except TypeError:\n          raise errors.BadConfigOption(\n              'Unsupported credential data for: {0:s}.'.format(\n                  credential_string))\n\n      self._credentials.append((credential_type, credential_data))", "response": "Parses the credential options."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _ParseSourcePathOption(self, options):\n    self._source_path = self.ParseStringOption(options, self._SOURCE_OPTION)\n    if not self._source_path:\n      raise errors.BadConfigOption('Missing source path.')\n\n    self._source_path = os.path.abspath(self._source_path)", "response": "Parses the source path option."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _ParseStorageMediaOptions(self, options):\n    self._ParseStorageMediaImageOptions(options)\n    self._ParseVSSProcessingOptions(options)\n    self._ParseCredentialOptions(options)\n    self._ParseSourcePathOption(options)", "response": "Parses the storage media image credential and source path options."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _ParseStorageMediaImageOptions(self, options):\n    self._partitions = getattr(options, 'partitions', None)\n    if self._partitions:\n      try:\n        self._ParseVolumeIdentifiersString(self._partitions, prefix='p')\n      except ValueError:\n        raise errors.BadConfigOption('Unsupported partitions')\n\n    self._volumes = getattr(options, 'volumes', None)\n    if self._volumes:\n      try:\n        self._ParseVolumeIdentifiersString(self._volumes, prefix='apfs')\n      except ValueError:\n        raise errors.BadConfigOption('Unsupported volumes')", "response": "Parses the storage media image options."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse a user specified volume identifiers string.", "response": "def _ParseVolumeIdentifiersString(\n      self, volume_identifiers_string, prefix='v'):\n    \"\"\"Parses a user specified volume identifiers string.\n\n    Args:\n      volume_identifiers_string (str): user specified volume identifiers. A\n          range of volumes can be defined as: \"3..5\". Multiple volumes can be\n          defined as: \"1,3,5\" (a list of comma separated values). Ranges and\n          lists can also be combined as: \"1,3..5\". The first volume is 1. All\n          volumes can be defined as: \"all\".\n      prefix (Optional[str]): volume identifier prefix.\n\n    Returns:\n      list[str]: volume identifiers with prefix or the string \"all\".\n\n    Raises:\n      ValueError: if the volume identifiers string is invalid.\n    \"\"\"\n    prefix_length = 0\n    if prefix:\n      prefix_length = len(prefix)\n\n    if not volume_identifiers_string:\n      return []\n\n    if volume_identifiers_string == 'all':\n      return ['all']\n\n    volume_identifiers = set()\n    for identifiers_range in volume_identifiers_string.split(','):\n      # Determine if the range is formatted as 1..3 otherwise it indicates\n      # a single volume identifier.\n      if '..' in identifiers_range:\n        first_identifier, last_identifier = identifiers_range.split('..')\n\n        if first_identifier.startswith(prefix):\n          first_identifier = first_identifier[prefix_length:]\n\n        if last_identifier.startswith(prefix):\n          last_identifier = last_identifier[prefix_length:]\n\n        try:\n          first_identifier = int(first_identifier, 10)\n          last_identifier = int(last_identifier, 10)\n        except ValueError:\n          raise ValueError('Invalid volume identifiers range: {0:s}.'.format(\n              identifiers_range))\n\n        for volume_identifier in range(first_identifier, last_identifier + 1):\n          if volume_identifier not in volume_identifiers:\n            volume_identifier = '{0:s}{1:d}'.format(prefix, volume_identifier)\n            volume_identifiers.add(volume_identifier)\n      else:\n        identifier = identifiers_range\n        if identifier.startswith(prefix):\n          identifier = identifiers_range[prefix_length:]\n\n        try:\n          volume_identifier = int(identifier, 10)\n        except ValueError:\n          raise ValueError('Invalid volume identifier range: {0:s}.'.format(\n              identifiers_range))\n\n        volume_identifier = '{0:s}{1:d}'.format(prefix, volume_identifier)\n        volume_identifiers.add(volume_identifier)\n\n    # Note that sorted will return a list.\n    return sorted(volume_identifiers)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _ParseVSSProcessingOptions(self, options):\n    vss_only = False\n    vss_stores = None\n\n    self._process_vss = not getattr(options, 'no_vss', False)\n    if self._process_vss:\n      vss_only = getattr(options, 'vss_only', False)\n      vss_stores = getattr(options, 'vss_stores', None)\n\n    if vss_stores:\n      try:\n        self._ParseVolumeIdentifiersString(vss_stores, prefix='vss')\n      except ValueError:\n        raise errors.BadConfigOption('Unsupported VSS stores')\n\n    self._vss_only = vss_only\n    self._vss_stores = vss_stores", "response": "Parses the VSS processing options."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprints an overview of APFS volume identifiers.", "response": "def _PrintAPFSVolumeIdentifiersOverview(\n      self, volume_system, volume_identifiers):\n    \"\"\"Prints an overview of APFS volume identifiers.\n\n    Args:\n      volume_system (dfvfs.APFSVolumeSystem): volume system.\n      volume_identifiers (list[str]): allowed volume identifiers.\n\n    Raises:\n      SourceScannerError: if a volume cannot be resolved from the volume\n          identifier.\n    \"\"\"\n    header = 'The following Apple File System (APFS) volumes were found:\\n'\n    self._output_writer.Write(header)\n\n    column_names = ['Identifier', 'Name']\n    table_view = views.CLITabularTableView(column_names=column_names)\n\n    for volume_identifier in volume_identifiers:\n      volume = volume_system.GetVolumeByIdentifier(volume_identifier)\n      if not volume:\n        raise errors.SourceScannerError(\n            'Volume missing for identifier: {0:s}.'.format(\n                volume_identifier))\n\n      volume_attribute = volume.GetAttribute('name')\n      table_view.AddRow([volume.identifier, volume_attribute.value])\n\n    self._output_writer.Write('\\n')\n    table_view.Write(self._output_writer)\n    self._output_writer.Write('\\n')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _PrintTSKPartitionIdentifiersOverview(\n      self, volume_system, volume_identifiers):\n    \"\"\"Prints an overview of TSK partition identifiers.\n\n    Args:\n      volume_system (dfvfs.TSKVolumeSystem): volume system.\n      volume_identifiers (list[str]): allowed volume identifiers.\n\n    Raises:\n      SourceScannerError: if a volume cannot be resolved from the volume\n          identifier.\n    \"\"\"\n    header = 'The following partitions were found:\\n'\n    self._output_writer.Write(header)\n\n    column_names = ['Identifier', 'Offset (in bytes)', 'Size (in bytes)']\n    table_view = views.CLITabularTableView(column_names=column_names)\n\n    for volume_identifier in sorted(volume_identifiers):\n      volume = volume_system.GetVolumeByIdentifier(volume_identifier)\n      if not volume:\n        raise errors.SourceScannerError(\n            'Partition missing for identifier: {0:s}.'.format(\n                volume_identifier))\n\n      volume_extent = volume.extents[0]\n      volume_offset = '{0:d} (0x{0:08x})'.format(volume_extent.offset)\n      volume_size = self._FormatHumanReadableSize(volume_extent.size)\n\n      table_view.AddRow([volume.identifier, volume_offset, volume_size])\n\n    self._output_writer.Write('\\n')\n    table_view.Write(self._output_writer)\n    self._output_writer.Write('\\n')", "response": "Prints an overview of TSK partition identifiers."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _PrintVSSStoreIdentifiersOverview(\n      self, volume_system, volume_identifiers):\n    \"\"\"Prints an overview of VSS store identifiers.\n\n    Args:\n      volume_system (dfvfs.VShadowVolumeSystem): volume system.\n      volume_identifiers (list[str]): allowed volume identifiers.\n\n    Raises:\n      SourceScannerError: if a volume cannot be resolved from the volume\n          identifier.\n    \"\"\"\n    header = 'The following Volume Shadow Snapshots (VSS) were found:\\n'\n    self._output_writer.Write(header)\n\n    column_names = ['Identifier', 'Creation Time']\n    table_view = views.CLITabularTableView(column_names=column_names)\n\n    for volume_identifier in volume_identifiers:\n      volume = volume_system.GetVolumeByIdentifier(volume_identifier)\n      if not volume:\n        raise errors.SourceScannerError(\n            'Volume missing for identifier: {0:s}.'.format(\n                volume_identifier))\n\n      volume_attribute = volume.GetAttribute('creation_time')\n      filetime = dfdatetime_filetime.Filetime(timestamp=volume_attribute.value)\n      creation_time = filetime.CopyToDateTimeString()\n\n      if volume.HasExternalData():\n        creation_time = '{0:s}\\tWARNING: data stored outside volume'.format(\n            creation_time)\n\n      table_view.AddRow([volume.identifier, creation_time])\n\n    self._output_writer.Write('\\n')\n    table_view.Write(self._output_writer)\n    self._output_writer.Write('\\n')", "response": "Prints an overview of VSS store identifiers."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _PromptUserForAPFSVolumeIdentifiers(\n      self, volume_system, volume_identifiers):\n    \"\"\"Prompts the user to provide APFS volume identifiers.\n\n    Args:\n      volume_system (dfvfs.APFSVolumeSystem): volume system.\n      volume_identifiers (list[str]): volume identifiers including prefix.\n\n    Returns:\n      list[str]: selected volume identifiers including prefix or None.\n    \"\"\"\n    print_header = True\n    while True:\n      if print_header:\n        self._PrintAPFSVolumeIdentifiersOverview(\n            volume_system, volume_identifiers)\n\n        print_header = False\n\n      lines = self._textwrapper.wrap(self._USER_PROMPT_APFS)\n      self._output_writer.Write('\\n'.join(lines))\n      self._output_writer.Write('\\n\\nVolume identifiers: ')\n\n      try:\n        selected_volumes = self._ReadSelectedVolumes(\n            volume_system, prefix='apfs')\n        if (not selected_volumes or\n            not set(selected_volumes).difference(volume_identifiers)):\n          break\n      except ValueError:\n        pass\n\n      self._output_writer.Write('\\n')\n\n      lines = self._textwrapper.wrap(\n          'Unsupported volume identifier(s), please try again or abort with '\n          'Ctrl^C.')\n      self._output_writer.Write('\\n'.join(lines))\n      self._output_writer.Write('\\n\\n')\n\n    return selected_volumes", "response": "Prompts the user to provide APFS volume identifiers."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _PromptUserForPartitionIdentifiers(\n      self, volume_system, volume_identifiers):\n    \"\"\"Prompts the user to provide partition identifiers.\n\n    Args:\n      volume_system (dfvfs.TSKVolumeSystem): volume system.\n      volume_identifiers (list[str]): volume identifiers including prefix.\n\n    Returns:\n      list[str]: selected volume identifiers including prefix or None.\n    \"\"\"\n    print_header = True\n    while True:\n      if print_header:\n        self._PrintTSKPartitionIdentifiersOverview(\n            volume_system, volume_identifiers)\n\n        print_header = False\n\n      lines = self._textwrapper.wrap(self._USER_PROMPT_TSK)\n      self._output_writer.Write('\\n'.join(lines))\n      self._output_writer.Write('\\n\\nPartition identifiers: ')\n\n      try:\n        selected_volumes = self._ReadSelectedVolumes(volume_system, prefix='p')\n        if (selected_volumes and\n            not set(selected_volumes).difference(volume_identifiers)):\n          break\n      except ValueError:\n        pass\n\n      self._output_writer.Write('\\n')\n\n      lines = self._textwrapper.wrap(\n          'Unsupported partition identifier(s), please try again or abort with '\n          'Ctrl^C.')\n      self._output_writer.Write('\\n'.join(lines))\n      self._output_writer.Write('\\n\\n')\n\n    return selected_volumes", "response": "Prompts the user to provide partition identifiers."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _PromptUserForVSSCurrentVolume(self):\n    while True:\n      self._output_writer.Write(\n          'Volume Shadow Snapshots (VSS) were selected also process current\\n'\n          'volume? [yes, no]\\n')\n\n      process_current_volume = self._input_reader.Read()\n      process_current_volume = process_current_volume.strip()\n      process_current_volume = process_current_volume.lower()\n\n      if (not process_current_volume or\n          process_current_volume in ('no', 'yes')):\n        break\n\n      self._output_writer.Write(\n          '\\n'\n          'Unsupported option, please try again or abort with Ctrl^C.\\n'\n          '\\n')\n\n    self._output_writer.Write('\\n')\n    return not process_current_volume or process_current_volume == 'yes'", "response": "Prompts the user if the current volume with VSS should be processed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _ReadSelectedVolumes(self, volume_system, prefix='v'):\n    volume_identifiers_string = self._input_reader.Read()\n    volume_identifiers_string = volume_identifiers_string.strip()\n\n    if not volume_identifiers_string:\n      return []\n\n    selected_volumes = self._ParseVolumeIdentifiersString(\n        volume_identifiers_string, prefix=prefix)\n\n    if selected_volumes == ['all']:\n      return [\n          '{0:s}{1:d}'.format(prefix, volume_index)\n          for volume_index in range(1, volume_system.number_of_volumes + 1)]\n\n    return selected_volumes", "response": "Reads the selected volumes from the input stream."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _ScanEncryptedVolume(self, scan_context, scan_node):\n    if not scan_node or not scan_node.path_spec:\n      raise errors.SourceScannerError('Invalid or missing scan node.')\n\n    credentials = credentials_manager.CredentialsManager.GetCredentials(\n        scan_node.path_spec)\n    if not credentials:\n      raise errors.SourceScannerError('Missing credentials for scan node.')\n\n    credentials_dict = {\n        credential_type: credential_data\n        for credential_type, credential_data in self._credentials}\n\n    is_unlocked = False\n    for credential_type in credentials.CREDENTIALS:\n      credential_data = credentials_dict.get(credential_type, None)\n      if not credential_data:\n        continue\n\n      is_unlocked = self._source_scanner.Unlock(\n          scan_context, scan_node.path_spec, credential_type, credential_data)\n      if is_unlocked:\n        break\n\n    if not is_unlocked:\n      is_unlocked = self._PromptUserForEncryptedVolumeCredential(\n          scan_context, scan_node, credentials)\n\n    if is_unlocked:\n      self._source_scanner.Scan(\n          scan_context, scan_path_spec=scan_node.path_spec)", "response": "Scans an encrypted volume scan node for volume and file systems."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nscan a file system scan node for file systems.", "response": "def _ScanFileSystem(self, scan_node, base_path_specs):\n    \"\"\"Scans a file system scan node for file systems.\n\n    Args:\n      scan_node (SourceScanNode): file system scan node.\n      base_path_specs (list[PathSpec]): file system base path specifications.\n\n    Raises:\n      SourceScannerError: if the scan node is invalid.\n    \"\"\"\n    if not scan_node or not scan_node.path_spec:\n      raise errors.SourceScannerError(\n          'Invalid or missing file system scan node.')\n\n    base_path_specs.append(scan_node.path_spec)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding the credential options to the argument group.", "response": "def AddCredentialOptions(self, argument_group):\n    \"\"\"Adds the credential options to the argument group.\n\n    The credential options are use to unlock encrypted volumes.\n\n    Args:\n      argument_group (argparse._ArgumentGroup): argparse argument group.\n    \"\"\"\n    argument_group.add_argument(\n        '--credential', action='append', default=[], type=str,\n        dest='credentials', metavar='TYPE:DATA', help=(\n            'Define a credentials that can be used to unlock encrypted '\n            'volumes e.g. BitLocker. The credential is defined as type:data '\n            'e.g. \"password:BDE-test\". Supported credential types are: '\n            '{0:s}. Binary key data is expected to be passed in BASE-16 '\n            'encoding (hexadecimal). WARNING credentials passed via command '\n            'line arguments can end up in logs, so use this option with '\n            'care.').format(', '.join(self._SUPPORTED_CREDENTIAL_TYPES)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds the storage media image options to the argument group.", "response": "def AddStorageMediaImageOptions(self, argument_group):\n    \"\"\"Adds the storage media image options to the argument group.\n\n    Args:\n      argument_group (argparse._ArgumentGroup): argparse argument group.\n    \"\"\"\n    argument_group.add_argument(\n        '--partitions', '--partition', dest='partitions', action='store',\n        type=str, default=None, help=(\n            'Define partitions to be processed. A range of '\n            'partitions can be defined as: \"3..5\". Multiple partitions can '\n            'be defined as: \"1,3,5\" (a list of comma separated values). '\n            'Ranges and lists can also be combined as: \"1,3..5\". The first '\n            'partition is 1. All partitions can be specified with: \"all\".'))\n\n    argument_group.add_argument(\n        '--volumes', '--volume', dest='volumes', action='store', type=str,\n        default=None, help=(\n            'Define volumes to be processed. A range of volumes can be defined '\n            'as: \"3..5\". Multiple volumes can be defined as: \"1,3,5\" (a list '\n            'of comma separated values). Ranges and lists can also be combined '\n            'as: \"1,3..5\". The first volume is 1. All volumes can be specified '\n            'with: \"all\".'))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd the VSS processing options to the argument group.", "response": "def AddVSSProcessingOptions(self, argument_group):\n    \"\"\"Adds the VSS processing options to the argument group.\n\n    Args:\n      argument_group (argparse._ArgumentGroup): argparse argument group.\n    \"\"\"\n    argument_group.add_argument(\n        '--no_vss', '--no-vss', dest='no_vss', action='store_true',\n        default=False, help=(\n            'Do not scan for Volume Shadow Snapshots (VSS). This means that '\n            'Volume Shadow Snapshots (VSS) are not processed.'))\n\n    argument_group.add_argument(\n        '--vss_only', '--vss-only', dest='vss_only', action='store_true',\n        default=False, help=(\n            'Do not process the current volume if Volume Shadow Snapshots '\n            '(VSS) have been selected.'))\n\n    argument_group.add_argument(\n        '--vss_stores', '--vss-stores', dest='vss_stores', action='store',\n        type=str, default=None, help=(\n            'Define Volume Shadow Snapshots (VSS) (or stores that need to be '\n            'processed. A range of stores can be defined as: \"3..5\". '\n            'Multiple stores can be defined as: \"1,3,5\" (a list of comma '\n            'separated values). Ranges and lists can also be combined as: '\n            '\"1,3..5\". The first store is 1. All stores can be defined as: '\n            '\"all\".'))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ScanSource(self, source_path):\n    # Symbolic links are resolved here and not earlier to preserve the user\n    # specified source path in storage and reporting.\n    if os.path.islink(source_path):\n      source_path = os.path.realpath(source_path)\n\n    if (not source_path.startswith('\\\\\\\\.\\\\') and\n        not os.path.exists(source_path)):\n      raise errors.SourceScannerError(\n          'No such device, file or directory: {0:s}.'.format(source_path))\n\n    scan_context = source_scanner.SourceScannerContext()\n    scan_context.OpenSourcePath(source_path)\n\n    try:\n      self._source_scanner.Scan(scan_context)\n    except (ValueError, dfvfs_errors.BackEndError) as exception:\n      raise errors.SourceScannerError(\n          'Unable to scan source with error: {0!s}.'.format(exception))\n\n    if scan_context.source_type not in (\n        scan_context.SOURCE_TYPE_STORAGE_MEDIA_DEVICE,\n        scan_context.SOURCE_TYPE_STORAGE_MEDIA_IMAGE):\n      scan_node = scan_context.GetRootScanNode()\n      self._source_path_specs.append(scan_node.path_spec)\n      return scan_context\n\n    # Get the first node where where we need to decide what to process.\n    scan_node = scan_context.GetRootScanNode()\n    while len(scan_node.sub_nodes) == 1:\n      scan_node = scan_node.sub_nodes[0]\n\n    base_path_specs = []\n    if scan_node.type_indicator != (\n        dfvfs_definitions.TYPE_INDICATOR_TSK_PARTITION):\n      self._ScanVolume(scan_context, scan_node, base_path_specs)\n\n    else:\n      # Determine which partition needs to be processed.\n      partition_identifiers = self._GetTSKPartitionIdentifiers(scan_node)\n      if not partition_identifiers:\n        raise errors.SourceScannerError('No partitions found.')\n\n      for partition_identifier in partition_identifiers:\n        location = '/{0:s}'.format(partition_identifier)\n        sub_scan_node = scan_node.GetSubNodeByLocation(location)\n        self._ScanVolume(scan_context, sub_scan_node, base_path_specs)\n\n    if not base_path_specs:\n      raise errors.SourceScannerError(\n          'No supported file system found in source.')\n\n    self._source_path_specs = base_path_specs\n\n    return scan_context", "response": "Scans the source path for volume and file systems."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a Windows path resolver.", "response": "def _CreateWindowsPathResolver(\n      self, file_system, mount_point, environment_variables):\n    \"\"\"Create a Windows path resolver and sets the environment variables.\n\n    Args:\n      file_system (dfvfs.FileSystem): file system.\n      mount_point (dfvfs.PathSpec): mount point path specification.\n      environment_variables (list[EnvironmentVariableArtifact]): environment\n          variables.\n\n    Returns:\n      dfvfs.WindowsPathResolver: Windows path resolver.\n    \"\"\"\n    if environment_variables is None:\n      environment_variables = []\n\n    path_resolver = windows_path_resolver.WindowsPathResolver(\n        file_system, mount_point)\n\n    for environment_variable in environment_variables:\n      name = environment_variable.name.lower()\n      if name not in ('systemroot', 'userprofile'):\n        continue\n\n      path_resolver.SetEnvironmentVariable(\n          environment_variable.name, environment_variable.value)\n\n    return path_resolver"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _OpenPathSpec(self, path_specification, ascii_codepage='cp1252'):\n    if not path_specification:\n      return None\n\n    file_entry = self._file_system.GetFileEntryByPathSpec(path_specification)\n    if file_entry is None:\n      return None\n\n    file_object = file_entry.GetFileObject()\n    if file_object is None:\n      return None\n\n    registry_file = dfwinreg_regf.REGFWinRegistryFile(\n        ascii_codepage=ascii_codepage)\n\n    try:\n      registry_file.Open(file_object)\n    except IOError as exception:\n      logger.warning(\n          'Unable to open Windows Registry file with error: {0!s}'.format(\n              exception))\n      file_object.close()\n      return None\n\n    return registry_file", "response": "Opens the Windows Registry file specified by the path specification."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef Open(self, path, ascii_codepage='cp1252'):\n    path_specification = self._path_resolver.ResolvePath(path)\n    if path_specification is None:\n      return None\n\n    return self._OpenPathSpec(path_specification)", "response": "Opens the Windows Registry file specified by the path."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncollects values from the file system.", "response": "def CollectFromFileSystem(\n      cls, artifacts_registry, knowledge_base, searcher, file_system):\n    \"\"\"Collects values from Windows Registry values.\n\n    Args:\n      artifacts_registry (artifacts.ArtifactDefinitionsRegistry): artifacts\n          definitions registry.\n      knowledge_base (KnowledgeBase): to fill with preprocessing information.\n      searcher (dfvfs.FileSystemSearcher): file system searcher to preprocess\n          the file system.\n      file_system (dfvfs.FileSystem): file system to be preprocessed.\n    \"\"\"\n    for preprocess_plugin in cls._file_system_plugins.values():\n      artifact_definition = artifacts_registry.GetDefinitionByName(\n          preprocess_plugin.ARTIFACT_DEFINITION_NAME)\n      if not artifact_definition:\n        logger.warning('Missing artifact definition: {0:s}'.format(\n            preprocess_plugin.ARTIFACT_DEFINITION_NAME))\n        continue\n\n      logger.debug('Running file system preprocessor plugin: {0:s}'.format(\n          preprocess_plugin.ARTIFACT_DEFINITION_NAME))\n      try:\n        preprocess_plugin.Collect(\n            knowledge_base, artifact_definition, searcher, file_system)\n      except (IOError, errors.PreProcessFail) as exception:\n        logger.warning((\n            'Unable to collect value from artifact definition: {0:s} '\n            'with error: {1!s}').format(\n                preprocess_plugin.ARTIFACT_DEFINITION_NAME, exception))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncollecting values from a given knowledge base.", "response": "def CollectFromKnowledgeBase(cls, knowledge_base):\n    \"\"\"Collects values from knowledge base values.\n\n    Args:\n      knowledge_base (KnowledgeBase): to fill with preprocessing information.\n    \"\"\"\n    for preprocess_plugin in cls._knowledge_base_plugins.values():\n      logger.debug('Running knowledge base preprocessor plugin: {0:s}'.format(\n          preprocess_plugin.__class__.__name__))\n      try:\n        preprocess_plugin.Collect(knowledge_base)\n      except errors.PreProcessFail as exception:\n        logger.warning(\n            'Unable to collect knowledge base value with error: {0!s}'.format(\n                exception))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncollecting values from the Windows Registry.", "response": "def CollectFromWindowsRegistry(\n      cls, artifacts_registry, knowledge_base, searcher):\n    \"\"\"Collects values from Windows Registry values.\n\n    Args:\n      artifacts_registry (artifacts.ArtifactDefinitionsRegistry): artifacts\n          definitions registry.\n      knowledge_base (KnowledgeBase): to fill with preprocessing information.\n      searcher (dfwinreg.WinRegistrySearcher): Windows Registry searcher to\n          preprocess the Windows Registry.\n    \"\"\"\n    for preprocess_plugin in cls._windows_registry_plugins.values():\n      artifact_definition = artifacts_registry.GetDefinitionByName(\n          preprocess_plugin.ARTIFACT_DEFINITION_NAME)\n      if not artifact_definition:\n        logger.warning('Missing artifact definition: {0:s}'.format(\n            preprocess_plugin.ARTIFACT_DEFINITION_NAME))\n        continue\n\n      logger.debug('Running Windows Registry preprocessor plugin: {0:s}'.format(\n          preprocess_plugin.ARTIFACT_DEFINITION_NAME))\n      try:\n        preprocess_plugin.Collect(knowledge_base, artifact_definition, searcher)\n      except (IOError, errors.PreProcessFail) as exception:\n        logger.warning((\n            'Unable to collect value from artifact definition: {0:s} '\n            'with error: {1!s}').format(\n                preprocess_plugin.ARTIFACT_DEFINITION_NAME, exception))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves the names of the registered artifact definitions.", "response": "def GetNames(cls):\n    \"\"\"Retrieves the names of the registered artifact definitions.\n\n    Returns:\n      list[str]: registered artifact definitions names.\n    \"\"\"\n    names = []\n    for plugin_class in cls._plugins.values():\n      name = getattr(plugin_class, 'ARTIFACT_DEFINITION_NAME', None)\n      if name:\n        names.append(name)\n\n    return names"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef RegisterPlugin(cls, plugin_class):\n    name = getattr(\n        plugin_class, 'ARTIFACT_DEFINITION_NAME', plugin_class.__name__)\n    name = name.lower()\n    if name in cls._plugins:\n      raise KeyError(\n          'Artifact plugin class already set for name: {0:s}.'.format(name))\n\n    preprocess_plugin = plugin_class()\n\n    cls._plugins[name] = preprocess_plugin\n\n    if isinstance(\n        preprocess_plugin, interface.FileSystemArtifactPreprocessorPlugin):\n      cls._file_system_plugins[name] = preprocess_plugin\n\n    elif isinstance(\n        preprocess_plugin, interface.KnowledgeBasePreprocessorPlugin):\n      cls._knowledge_base_plugins[name] = preprocess_plugin\n\n    elif isinstance(\n        preprocess_plugin,\n        interface.WindowsRegistryKeyArtifactPreprocessorPlugin):\n      cls._windows_registry_plugins[name] = preprocess_plugin", "response": "Registers an artifact preprocessor plugin class."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef RunPlugins(\n      cls, artifacts_registry, file_system, mount_point, knowledge_base):\n    \"\"\"Runs the preprocessing plugins.\n\n    Args:\n      artifacts_registry (artifacts.ArtifactDefinitionsRegistry): artifacts\n          definitions registry.\n      file_system (dfvfs.FileSystem): file system to be preprocessed.\n      mount_point (dfvfs.PathSpec): mount point path specification that refers\n          to the base location of the file system.\n      knowledge_base (KnowledgeBase): to fill with preprocessing information.\n    \"\"\"\n    searcher = file_system_searcher.FileSystemSearcher(file_system, mount_point)\n\n    cls.CollectFromFileSystem(\n        artifacts_registry, knowledge_base, searcher, file_system)\n\n    # Run the Registry plugins separately so we do not have to open\n    # Registry files for every preprocess plugin.\n\n    environment_variables = None\n    if knowledge_base:\n      environment_variables = knowledge_base.GetEnvironmentVariables()\n\n    registry_file_reader = FileSystemWinRegistryFileReader(\n        file_system, mount_point, environment_variables=environment_variables)\n    win_registry = dfwinreg_registry.WinRegistry(\n        registry_file_reader=registry_file_reader)\n\n    searcher = registry_searcher.WinRegistrySearcher(win_registry)\n\n    cls.CollectFromWindowsRegistry(\n        artifacts_registry, knowledge_base, searcher)\n\n    cls.CollectFromKnowledgeBase(knowledge_base)\n\n    if not knowledge_base.HasUserAccounts():\n      logger.warning('Unable to find any user accounts on the system.')", "response": "Runs the preprocessing plugins."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ParseFileObject(self, parser_mediator, file_object):\n    display_name = parser_mediator.GetDisplayName()\n\n    if not zipfile.is_zipfile(file_object):\n      raise errors.UnableToParseFile(\n          '[{0:s}] unable to parse file: {1:s} with error: {2:s}'.format(\n              self.NAME, display_name, 'Not a Zip file.'))\n\n    try:\n      zip_file = zipfile.ZipFile(file_object, 'r', allowZip64=True)\n      self._ProcessZipFileWithPlugins(parser_mediator, zip_file)\n      zip_file.close()\n\n    # Some non-ZIP files return true for is_zipfile but will fail with a\n    # negative seek (IOError) or another error.\n    except (zipfile.BadZipfile, struct.error) as exception:\n      raise errors.UnableToParseFile(\n          '[{0:s}] unable to parse file: {1:s} with error: {2!s}'.format(\n              self.NAME, display_name, exception))", "response": "Parses a compound ZIP file - like object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _ProcessZipFileWithPlugins(self, parser_mediator, zip_file):\n    archive_members = zip_file.namelist()\n    for plugin in self._plugins:\n      try:\n        plugin.UpdateChainAndProcess(\n            parser_mediator, zip_file=zip_file, archive_members=archive_members)\n      except errors.WrongCompoundZIPPlugin as exception:\n        logger.debug('[{0:s}] wrong plugin: {1!s}'.format(\n            self.NAME, exception))", "response": "Processes a zip file using all compound zip plugins."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the year from a POSIX timestamp.", "response": "def GetYearFromPosixTime(posix_time, timezone=pytz.UTC):\n  \"\"\"Gets the year from a POSIX timestamp\n\n  The POSIX time is the number of seconds since 1970-01-01 00:00:00 UTC.\n\n  Args:\n    posix_time: An integer containing the number of seconds since\n                1970-01-01 00:00:00 UTC.\n    timezone: Optional timezone of the POSIX timestamp.\n\n  Returns:\n    The year of the POSIX timestamp.\n\n  Raises:\n    ValueError: If the posix timestamp is out of the range of supported values.\n  \"\"\"\n  datetime_object = datetime.datetime.fromtimestamp(posix_time, tz=timezone)\n  return datetime_object.year"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncopies a timestamp from a string containing a date and time value.", "response": "def CopyFromString(cls, time_string):\n    \"\"\"Copies a timestamp from a string containing a date and time value.\n\n    Args:\n      time_string: A string containing a date and time value formatted as:\n                   YYYY-MM-DD hh:mm:ss.######[+-]##:##\n                   Where # are numeric digits ranging from 0 to 9 and the\n                   seconds fraction can be either 3 or 6 digits. The time\n                   of day, seconds fraction and timezone offset are optional.\n                   The default timezone is UTC.\n\n    Returns:\n      The timestamp which is an integer containing the number of micro seconds\n      since January 1, 1970, 00:00:00 UTC.\n\n    Raises:\n      ValueError: if the time string is invalid or not supported.\n    \"\"\"\n    if not time_string:\n      raise ValueError('Invalid time string.')\n\n    time_string_length = len(time_string)\n\n    # The time string should at least contain 'YYYY-MM-DD'.\n    if (time_string_length < 10 or time_string[4] != '-' or\n        time_string[7] != '-'):\n      raise ValueError('Invalid time string.')\n\n    # If a time of day is specified the time string it should at least\n    # contain 'YYYY-MM-DD hh:mm:ss'.\n    if (time_string_length > 10 and (\n        time_string_length < 19 or time_string[10] != ' ' or\n        time_string[13] != ':' or time_string[16] != ':')):\n      raise ValueError('Invalid time string.')\n\n    try:\n      year = int(time_string[0:4], 10)\n    except ValueError:\n      raise ValueError('Unable to parse year.')\n\n    try:\n      month = int(time_string[5:7], 10)\n    except ValueError:\n      raise ValueError('Unable to parse month.')\n\n    if month not in range(1, 13):\n      raise ValueError('Month value out of bounds.')\n\n    try:\n      day_of_month = int(time_string[8:10], 10)\n    except ValueError:\n      raise ValueError('Unable to parse day of month.')\n\n    if day_of_month not in range(1, 32):\n      raise ValueError('Day of month value out of bounds.')\n\n    hours = 0\n    minutes = 0\n    seconds = 0\n\n    if time_string_length > 10:\n      try:\n        hours = int(time_string[11:13], 10)\n      except ValueError:\n        raise ValueError('Unable to parse hours.')\n\n      if hours not in range(0, 24):\n        raise ValueError('Hours value out of bounds.')\n\n      try:\n        minutes = int(time_string[14:16], 10)\n      except ValueError:\n        raise ValueError('Unable to parse minutes.')\n\n      if minutes not in range(0, 60):\n        raise ValueError('Minutes value out of bounds.')\n\n      try:\n        seconds = int(time_string[17:19], 10)\n      except ValueError:\n        raise ValueError('Unable to parse day of seconds.')\n\n      if seconds not in range(0, 60):\n        raise ValueError('Seconds value out of bounds.')\n\n    micro_seconds = 0\n    timezone_offset = 0\n\n    if time_string_length > 19:\n      if time_string[19] != '.':\n        timezone_index = 19\n      else:\n        for timezone_index in range(19, time_string_length):\n          if time_string[timezone_index] in ['+', '-']:\n            break\n\n          # The calculation that follow rely on the timezone index to point\n          # beyond the string in case no timezone offset was defined.\n          if timezone_index == time_string_length - 1:\n            timezone_index += 1\n\n      if timezone_index > 19:\n        fraction_of_seconds_length = timezone_index - 20\n        if fraction_of_seconds_length not in [3, 6]:\n          raise ValueError('Invalid time string.')\n\n        try:\n          micro_seconds = int(time_string[20:timezone_index], 10)\n        except ValueError:\n          raise ValueError('Unable to parse fraction of seconds.')\n\n        if fraction_of_seconds_length == 3:\n          micro_seconds *= 1000\n\n      if timezone_index < time_string_length:\n        if (time_string_length - timezone_index != 6 or\n            time_string[timezone_index + 3] != ':'):\n          raise ValueError('Invalid time string.')\n\n        try:\n          timezone_offset = int(time_string[\n              timezone_index + 1:timezone_index + 3])\n        except ValueError:\n          raise ValueError('Unable to parse timezone hours offset.')\n\n        if timezone_offset not in range(0, 24):\n          raise ValueError('Timezone hours offset value out of bounds.')\n\n        # Note that when the sign of the timezone offset is negative\n        # the difference needs to be added. We do so by flipping the sign.\n        if time_string[timezone_index] == '-':\n          timezone_offset *= 60\n        else:\n          timezone_offset *= -60\n\n        try:\n          timezone_offset += int(time_string[\n              timezone_index + 4:timezone_index + 6])\n        except ValueError:\n          raise ValueError('Unable to parse timezone minutes offset.')\n\n        timezone_offset *= 60\n\n    timestamp = int(calendar.timegm((\n        year, month, day_of_month, hours, minutes, seconds)))\n\n    return ((timestamp + timezone_offset) * 1000000) + micro_seconds"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef CopyToDatetime(cls, timestamp, timezone, raise_error=False):\n    datetime_object = datetime.datetime(1970, 1, 1, 0, 0, 0, 0, tzinfo=pytz.UTC)\n\n    if not timestamp:\n      if raise_error:\n        raise ValueError('Missing timestamp value')\n      return datetime_object\n\n    try:\n      datetime_object += datetime.timedelta(microseconds=timestamp)\n      return datetime_object.astimezone(timezone)\n    except OverflowError as exception:\n      if raise_error:\n        raise\n\n      logging.error((\n          'Unable to copy {0:d} to a datetime object with error: '\n          '{1!s}').format(timestamp, exception))\n\n    return datetime_object", "response": "Copies the timestamp to a datetime object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef CopyToIsoFormat(cls, timestamp, timezone=pytz.UTC, raise_error=False):\n    datetime_object = cls.CopyToDatetime(\n        timestamp, timezone, raise_error=raise_error)\n    return datetime_object.isoformat()", "response": "Copies the timestamp to an ISO 8601 formatted string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef FromTimeString(\n      cls, time_string, dayfirst=False, gmt_as_timezone=True,\n      timezone=pytz.UTC):\n    \"\"\"Converts a string containing a date and time value into a timestamp.\n\n    Args:\n      time_string: String that contains a date and time value.\n      dayfirst: An optional boolean argument. If set to true then the\n                parser will change the precedence in which it parses timestamps\n                from MM-DD-YYYY to DD-MM-YYYY (and YYYY-MM-DD will be\n                YYYY-DD-MM, etc).\n      gmt_as_timezone: Sometimes the dateutil parser will interpret GMT and UTC\n                       the same way, that is not make a distinction. By default\n                       this is set to true, that is GMT can be interpreted\n                       differently than UTC. If that is not the expected result\n                       this attribute can be set to false.\n      timezone: Optional timezone object (instance of pytz.timezone) that\n                the data and time value in the string represents. This value\n                is used when the timezone cannot be determined from the string.\n\n    Returns:\n      The timestamp which is an integer containing the number of micro seconds\n      since January 1, 1970, 00:00:00 UTC or 0 on error.\n\n    Raises:\n      TimestampError: if the time string could not be parsed.\n    \"\"\"\n    if not gmt_as_timezone and time_string.endswith(' GMT'):\n      time_string = '{0:s}UTC'.format(time_string[:-3])\n\n    try:\n      # TODO: deprecate the use of dateutil parser.\n      datetime_object = dateutil.parser.parse(time_string, dayfirst=dayfirst)\n\n    except (TypeError, ValueError) as exception:\n      raise errors.TimestampError((\n          'Unable to convert time string: {0:s} in to a datetime object '\n          'with error: {1!s}').format(time_string, exception))\n\n    if datetime_object.tzinfo:\n      datetime_object = datetime_object.astimezone(pytz.UTC)\n    else:\n      datetime_object = timezone.localize(datetime_object)\n\n    posix_time = int(calendar.timegm(datetime_object.utctimetuple()))\n    timestamp = posix_time * definitions.MICROSECONDS_PER_SECOND\n    return timestamp + datetime_object.microsecond", "response": "Converts a string containing a date and time value into a timestamp."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert the timestamp in localtime of the given timezone to UTC.", "response": "def LocaltimeToUTC(cls, timestamp, timezone, is_dst=False):\n    \"\"\"Converts the timestamp in localtime of the timezone to UTC.\n\n    Args:\n      timestamp: The timestamp which is an integer containing the number\n                 of micro seconds since January 1, 1970, 00:00:00 UTC.\n      timezone: The timezone (pytz.timezone) object.\n      is_dst: A boolean to indicate the timestamp is corrected for daylight\n              savings time (DST) only used for the DST transition period.\n\n    Returns:\n      The timestamp which is an integer containing the number of micro seconds\n      since January 1, 1970, 00:00:00 UTC or 0 on error.\n    \"\"\"\n    if timezone and timezone != pytz.UTC:\n      datetime_object = (\n          datetime.datetime(1970, 1, 1, 0, 0, 0, 0, tzinfo=None) +\n          datetime.timedelta(microseconds=timestamp))\n\n      # Check if timezone is UTC since utcoffset() does not support is_dst\n      # for UTC and will raise.\n      datetime_delta = timezone.utcoffset(datetime_object, is_dst=is_dst)\n      seconds_delta = int(datetime_delta.total_seconds())\n      timestamp -= seconds_delta * definitions.MICROSECONDS_PER_SECOND\n\n    return timestamp"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntake a timestamp value and rounds it to a second precision.", "response": "def RoundToSeconds(cls, timestamp):\n    \"\"\"Takes a timestamp value and rounds it to a second precision.\"\"\"\n    leftovers = timestamp % definitions.MICROSECONDS_PER_SECOND\n    scrubbed = timestamp - leftovers\n    rounded = round(float(leftovers) / definitions.MICROSECONDS_PER_SECOND)\n\n    return int(scrubbed + rounded * definitions.MICROSECONDS_PER_SECOND)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _StripThenGetNicknameAndText(self, text):\n    stripped = self.STRIPPER.transformString(text)\n    structure = self.MSG_ENTRY.parseString(stripped)\n    text = structure.text.replace('\\t', ' ')\n    return structure.nickname, text", "response": "Strips decorators from text and gets nickname and text if available."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses a log record.", "response": "def ParseRecord(self, parser_mediator, key, structure):\n    \"\"\"Parses a log record structure.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      key (str): name of the parsed structure.\n      structure (pyparsing.ParseResults): structure parsed from the log file.\n    \"\"\"\n    if key != 'logline':\n      logger.warning(\n          'Unable to parse record, unknown structure: {0:s}'.format(key))\n      return\n\n    try:\n      timestamp = int(structure.timestamp)\n    except ValueError:\n      logger.debug('Invalid timestamp string {0:s}, skipping record'.format(\n          structure.timestamp))\n      return\n\n    try:\n      nickname, text = self._StripThenGetNicknameAndText(structure.text)\n    except pyparsing.ParseException:\n      logger.debug('Error parsing entry at offset {0:d}'.format(self._offset))\n      return\n\n    event_data = XChatScrollbackEventData()\n    event_data.nickname = nickname\n    event_data.offset = self._offset\n    event_data.text = text\n\n    date_time = dfdatetime_posix_time.PosixTime(timestamp=timestamp)\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_ADDED)\n    parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef VerifyStructure(self, parser_mediator, line):\n    structure = self.LOG_LINE\n\n    try:\n      parsed_structure = structure.parseString(line)\n    except pyparsing.ParseException:\n      logger.debug('Not a XChat scrollback log file')\n      return False\n\n    try:\n      int(parsed_structure.timestamp, 10)\n    except ValueError:\n      logger.debug('Not a XChat scrollback log file, invalid timestamp string')\n      return False\n\n    return True", "response": "Verify that this file is a XChat scrollback log file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ExtractEvents(self, parser_mediator, registry_key, **kwargs):\n    if registry_key is None:\n      return\n\n    values_dict = {}\n    for value_name in self._VALUE_NAMES:\n      registry_value = registry_key.GetValueByName(value_name)\n      if not registry_value:\n        continue\n\n      value_data = registry_value.GetDataAsObject()\n      if value_data is None:\n        continue\n\n      values_dict[value_name] = value_data\n\n    event_data = windows_events.WindowsRegistryEventData()\n    event_data.key_path = registry_key.path\n    event_data.offset = registry_key.offset\n    event_data.regvalue = values_dict\n\n    event = time_events.DateTimeValuesEvent(\n        registry_key.last_written_time, definitions.TIME_DESCRIPTION_WRITTEN)\n    parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Extracts events from a Windows Registry key."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _ParseContainerTable(self, parser_mediator, table, container_name):\n    if table is None:\n      raise ValueError('Missing table value.')\n\n    for record_index, esedb_record in enumerate(table.records):\n      if parser_mediator.abort:\n        break\n\n      # TODO: add support for:\n      # wpnidm, iecompat, iecompatua, DNTException, DOMStore\n      if container_name == 'Content':\n        value_mappings = self._CONTAINER_TABLE_VALUE_MAPPINGS\n      else:\n        value_mappings = None\n\n      try:\n        record_values = self._GetRecordValues(\n            parser_mediator, table.name, esedb_record,\n            value_mappings=value_mappings)\n\n      except UnicodeDecodeError:\n        parser_mediator.ProduceExtractionWarning((\n            'Unable to retrieve record values from record: {0:d} '\n            'in table: {1:s}').format(record_index, table.name))\n        continue\n\n      if (container_name in self._SUPPORTED_CONTAINER_NAMES or\n          container_name.startswith('MSHist')):\n        access_count = record_values.get('AccessCount', None)\n        cached_filename = record_values.get('Filename', None)\n        cached_file_size = record_values.get('FileSize', None)\n        cache_identifier = record_values.get('CacheId', None)\n        container_identifier = record_values.get('ContainerId', None)\n        entry_identifier = record_values.get('EntryId', None)\n        file_extension = record_values.get('FileExtension', None)\n        redirect_url = record_values.get('RedirectUrl', None)\n        sync_count = record_values.get('SyncCount', None)\n\n        url = record_values.get('Url', '')\n        # Ignore an URL that start with a binary value.\n        if ord(url[0]) < 0x20 or ord(url[0]) == 0x7f:\n          url = None\n\n        request_headers = record_values.get('RequestHeaders', None)\n        # Ignore non-Unicode request headers values.\n        if not isinstance(request_headers, py2to3.UNICODE_TYPE):\n          request_headers = None\n\n        response_headers = record_values.get('ResponseHeaders', None)\n        # Ignore non-Unicode response headers values.\n        if not isinstance(response_headers, py2to3.UNICODE_TYPE):\n          response_headers = None\n\n        event_data = MsieWebCacheContainerEventData()\n        event_data.access_count = access_count\n        event_data.cached_filename = cached_filename\n        event_data.cached_file_size = cached_file_size\n        event_data.cache_identifier = cache_identifier\n        event_data.container_identifier = container_identifier\n        event_data.entry_identifier = entry_identifier\n        event_data.file_extension = file_extension\n        event_data.redirect_url = redirect_url\n        event_data.request_headers = request_headers\n        event_data.response_headers = response_headers\n        event_data.sync_count = sync_count\n        event_data.url = url\n\n        timestamp = record_values.get('SyncTime', None)\n        if timestamp:\n          date_time = dfdatetime_filetime.Filetime(timestamp=timestamp)\n          event = time_events.DateTimeValuesEvent(\n              date_time, 'Synchronization time')\n          parser_mediator.ProduceEventWithEventData(event, event_data)\n\n        timestamp = record_values.get('CreationTime', None)\n        if timestamp:\n          date_time = dfdatetime_filetime.Filetime(timestamp=timestamp)\n          event = time_events.DateTimeValuesEvent(\n              date_time, definitions.TIME_DESCRIPTION_CREATION)\n          parser_mediator.ProduceEventWithEventData(event, event_data)\n\n        timestamp = record_values.get('ExpiryTime', None)\n        if timestamp:\n          date_time = dfdatetime_filetime.Filetime(timestamp=timestamp)\n          event = time_events.DateTimeValuesEvent(\n              date_time, definitions.TIME_DESCRIPTION_EXPIRATION)\n          parser_mediator.ProduceEventWithEventData(event, event_data)\n\n        timestamp = record_values.get('ModifiedTime', None)\n        if timestamp:\n          date_time = dfdatetime_filetime.Filetime(timestamp=timestamp)\n          event = time_events.DateTimeValuesEvent(\n              date_time, definitions.TIME_DESCRIPTION_MODIFICATION)\n          parser_mediator.ProduceEventWithEventData(event, event_data)\n\n        timestamp = record_values.get('AccessedTime', None)\n        if timestamp:\n          date_time = dfdatetime_filetime.Filetime(timestamp=timestamp)\n          event = time_events.DateTimeValuesEvent(\n              date_time, definitions.TIME_DESCRIPTION_LAST_ACCESS)\n          parser_mediator.ProduceEventWithEventData(event, event_data)\n\n        timestamp = record_values.get('PostCheckTime', None)\n        if timestamp:\n          date_time = dfdatetime_filetime.Filetime(timestamp=timestamp)\n          event = time_events.DateTimeValuesEvent(\n              date_time, 'Post check time')\n          parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Parses a Container_# table."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse the Containers table.", "response": "def ParseContainersTable(\n      self, parser_mediator, database=None, table=None, **unused_kwargs):\n    \"\"\"Parses the Containers table.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      database (Optional[pyesedb.file]): ESE database.\n      table (Optional[pyesedb.table]): table.\n\n    Raises:\n      ValueError: if the database or table value is missing.\n    \"\"\"\n    if database is None:\n      raise ValueError('Missing database value.')\n\n    if table is None:\n      raise ValueError('Missing table value.')\n\n    for esedb_record in table.records:\n      if parser_mediator.abort:\n        break\n\n      record_values = self._GetRecordValues(\n          parser_mediator, table.name, esedb_record)\n\n      event_data = MsieWebCacheContainersEventData()\n      event_data.container_identifier = record_values.get('ContainerId', None)\n      event_data.directory = record_values.get('Directory', None)\n      event_data.name = record_values.get('Name', None)\n      event_data.set_identifier = record_values.get('SetId', None)\n\n      timestamp = record_values.get('LastScavengeTime', None)\n      if timestamp:\n        date_time = dfdatetime_filetime.Filetime(timestamp=timestamp)\n        event = time_events.DateTimeValuesEvent(\n            date_time, 'Last Scavenge Time')\n        parser_mediator.ProduceEventWithEventData(event, event_data)\n\n      timestamp = record_values.get('LastAccessTime', None)\n      if timestamp:\n        date_time = dfdatetime_filetime.Filetime(timestamp=timestamp)\n        event = time_events.DateTimeValuesEvent(\n            date_time, definitions.TIME_DESCRIPTION_LAST_ACCESS)\n        parser_mediator.ProduceEventWithEventData(event, event_data)\n\n      container_identifier = record_values.get('ContainerId', None)\n      container_name = record_values.get('Name', None)\n\n      if not container_identifier or not container_name:\n        continue\n\n      table_name = 'Container_{0:d}'.format(container_identifier)\n      esedb_table = database.get_table_by_name(table_name)\n      if not esedb_table:\n        parser_mediator.ProduceExtractionWarning(\n            'Missing table: {0:s}'.format(table_name))\n        continue\n\n      self._ParseContainerTable(parser_mediator, esedb_table, container_name)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the LeakFiles table.", "response": "def ParseLeakFilesTable(\n      self, parser_mediator, database=None, table=None, **unused_kwargs):\n    \"\"\"Parses the LeakFiles table.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      database (Optional[pyesedb.file]): ESE database.\n      table (Optional[pyesedb.table]): table.\n\n    Raises:\n      ValueError: if the database or table value is missing.\n    \"\"\"\n    if database is None:\n      raise ValueError('Missing database value.')\n\n    if table is None:\n      raise ValueError('Missing table value.')\n\n    for esedb_record in table.records:\n      if parser_mediator.abort:\n        break\n\n      record_values = self._GetRecordValues(\n          parser_mediator, table.name, esedb_record)\n\n      event_data = MsieWebCacheLeakFilesEventData()\n      event_data.cached_filename = record_values.get('Filename', None)\n      event_data.leak_identifier = record_values.get('LeakId', None)\n\n      timestamp = record_values.get('CreationTime', None)\n      if timestamp:\n        date_time = dfdatetime_filetime.Filetime(timestamp=timestamp)\n        event = time_events.DateTimeValuesEvent(\n            date_time, definitions.TIME_DESCRIPTION_CREATION)\n        parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ParsePartitionsTable(\n      self, parser_mediator, database=None, table=None, **unused_kwargs):\n    \"\"\"Parses the Partitions table.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      database (Optional[pyesedb.file]): ESE database.\n      table (Optional[pyesedb.table]): table.\n\n    Raises:\n      ValueError: if the database or table value is missing.\n    \"\"\"\n    if database is None:\n      raise ValueError('Missing database value.')\n\n    if table is None:\n      raise ValueError('Missing table value.')\n\n    for esedb_record in table.records:\n      if parser_mediator.abort:\n        break\n\n      record_values = self._GetRecordValues(\n          parser_mediator, table.name, esedb_record)\n\n      event_data = MsieWebCachePartitionsEventData()\n      event_data.directory = record_values.get('Directory', None)\n      event_data.partition_identifier = record_values.get('PartitionId', None)\n      event_data.partition_type = record_values.get('PartitionType', None)\n      event_data.table_identifier = record_values.get('TableId', None)\n\n      timestamp = record_values.get('LastScavengeTime', None)\n      if timestamp:\n        date_time = dfdatetime_filetime.Filetime(timestamp=timestamp)\n        event = time_events.DateTimeValuesEvent(\n            date_time, 'Last Scavenge Time')\n        parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Parses the Partitions table."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef GetMessages(self, unused_formatter_mediator, event):\n    if self.DATA_TYPE != event.data_type:\n      raise errors.WrongFormatter(\n          'Unsupported data type: {0:s}.'.format(event.data_type))\n\n    event_values = event.CopyToDict()\n    for formattable_value_name, formatter in self.VALUE_FORMATTERS.items():\n      if formattable_value_name in event_values:\n        value = event_values[formattable_value_name]\n        event_values[formattable_value_name] = formatter(value)\n\n    return self._ConditionalFormatMessages(event_values)", "response": "Determines the formatted message strings for an event object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _ParseRecord(self, parser_mediator, file_object, record_offset):\n    record_strings_data_offset = file_object.tell()\n    record_strings_data_size = record_offset - record_strings_data_offset\n\n    record_strings_data = self._ReadData(\n        file_object, record_strings_data_offset, record_strings_data_size)\n\n    record_map = self._GetDataTypeMap('asl_record')\n\n    try:\n      record, record_data_size = self._ReadStructureFromFileObject(\n          file_object, record_offset, record_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.UnableToParseFile((\n          'Unable to parse record at offset: 0x{0:08x} with error: '\n          '{1!s}').format(record_offset, exception))\n\n    hostname = self._ParseRecordString(\n        record_strings_data, record_strings_data_offset,\n        record.hostname_string_offset)\n\n    sender = self._ParseRecordString(\n        record_strings_data, record_strings_data_offset,\n        record.sender_string_offset)\n\n    facility = self._ParseRecordString(\n        record_strings_data, record_strings_data_offset,\n        record.facility_string_offset)\n\n    message = self._ParseRecordString(\n        record_strings_data, record_strings_data_offset,\n        record.message_string_offset)\n\n    file_offset = record_offset + record_data_size\n    additional_data_size = record.data_size + 6 - record_data_size\n\n    if additional_data_size % 8 != 0:\n      raise errors.ParseError(\n          'Invalid record additional data size: {0:d}.'.format(\n              additional_data_size))\n\n    additional_data = self._ReadData(\n        file_object, file_offset, additional_data_size)\n\n    extra_fields = {}\n    for additional_data_offset in range(0, additional_data_size - 8, 16):\n      record_extra_field = self._ParseRecordExtraField(\n          additional_data[additional_data_offset:], file_offset)\n\n      file_offset += 16\n\n      name = self._ParseRecordString(\n          record_strings_data, record_strings_data_offset,\n          record_extra_field.name_string_offset)\n\n      value = self._ParseRecordString(\n          record_strings_data, record_strings_data_offset,\n          record_extra_field.value_string_offset)\n\n      if name is not None:\n        extra_fields[name] = value\n\n    # TODO: implement determine previous record offset\n\n    event_data = ASLEventData()\n    event_data.computer_name = hostname\n    event_data.extra_information = ', '.join([\n        '{0:s}: {1:s}'.format(name, value)\n        for name, value in sorted(extra_fields.items())])\n    event_data.facility = facility\n    event_data.group_id = record.group_identifier\n    event_data.level = record.alert_level\n    event_data.message_id = record.message_identifier\n    event_data.message = message\n    event_data.pid = record.process_identifier\n    event_data.read_gid = record.real_group_identifier\n    event_data.read_uid = record.real_user_identifier\n    event_data.record_position = record_offset\n    event_data.sender = sender\n    # Note that the user_sid value is expected to be a string.\n    event_data.user_sid = '{0:d}'.format(record.user_identifier)\n\n    microseconds, _ = divmod(record.written_time_nanoseconds, 1000)\n    timestamp = (record.written_time * 1000000) + microseconds\n\n    # TODO: replace by PosixTimeInNanoseconds.\n    date_time = dfdatetime_posix_time.PosixTimeInMicroseconds(\n        timestamp=timestamp)\n    # TODO: replace by written time.\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_CREATION)\n    parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    return record.next_record_offset", "response": "Parses a record and produces events."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _ParseRecordExtraField(self, byte_stream, file_offset):\n    extra_field_map = self._GetDataTypeMap('asl_record_extra_field')\n\n    try:\n      record_extra_field = self._ReadStructureFromByteStream(\n          byte_stream, file_offset, extra_field_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.ParseError((\n          'Unable to parse record extra field at offset: 0x{0:08x} with error: '\n          '{1!s}').format(file_offset, exception))\n\n    return record_extra_field", "response": "Parses a record extra field."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses a record string.", "response": "def _ParseRecordString(\n      self, record_strings_data, record_strings_data_offset, string_offset):\n    \"\"\"Parses a record string.\n\n    Args:\n      record_strings_data (bytes): record strings data.\n      record_strings_data_offset (int): offset of the record strings data\n          relative to the start of the file.\n      string_offset (int): offset of the string relative to the start of\n          the file.\n\n    Returns:\n      str: record string or None if string offset is 0.\n\n    Raises:\n      ParseError: if the record string cannot be parsed.\n    \"\"\"\n    if string_offset == 0:\n      return None\n\n    if string_offset & self._STRING_OFFSET_MSB:\n      if (string_offset >> 60) != 8:\n        raise errors.ParseError('Invalid inline record string flag.')\n\n      string_size = (string_offset >> 56) & 0x0f\n      if string_size >= 8:\n        raise errors.ParseError('Invalid inline record string size.')\n\n      string_data = bytes(bytearray([\n          string_offset >> (8 * byte_index) & 0xff\n          for byte_index in range(6, -1, -1)]))\n\n      try:\n        return string_data[:string_size].decode('utf-8')\n      except UnicodeDecodeError as exception:\n        raise errors.ParseError(\n            'Unable to decode inline record string with error: {0!s}.'.format(\n                exception))\n\n    data_offset = string_offset - record_strings_data_offset\n    record_string_map = self._GetDataTypeMap('asl_record_string')\n\n    try:\n      record_string = self._ReadStructureFromByteStream(\n          record_strings_data[data_offset:], string_offset, record_string_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.ParseError((\n          'Unable to parse record string at offset: 0x{0:08x} with error: '\n          '{1!s}').format(string_offset, exception))\n\n    return record_string.string.rstrip('\\x00')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ParseFileObject(self, parser_mediator, file_object):\n    file_header_map = self._GetDataTypeMap('asl_file_header')\n\n    try:\n      file_header, _ = self._ReadStructureFromFileObject(\n          file_object, 0, file_header_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.UnableToParseFile(\n          'Unable to parse file header with error: {0!s}'.format(\n              exception))\n\n    if file_header.signature != self._FILE_SIGNATURE:\n      raise errors.UnableToParseFile('Invalid file signature.')\n\n    # TODO: generate event for creation time.\n\n    file_size = file_object.get_size()\n\n    if file_header.first_log_entry_offset > 0:\n      last_log_entry_offset = 0\n      file_offset = file_header.first_log_entry_offset\n\n      while file_offset < file_size:\n        last_log_entry_offset = file_offset\n\n        try:\n          file_offset = self._ParseRecord(\n              parser_mediator, file_object, file_offset)\n        except errors.ParseError as exception:\n          parser_mediator.ProduceExtractionWarning(\n              'unable to parse record with error: {0!s}'.format(exception))\n          return\n\n        if file_offset == 0:\n          break\n\n      if last_log_entry_offset != file_header.last_log_entry_offset:\n        parser_mediator.ProduceExtractionWarning(\n            'last log entry offset does not match value in file header.')", "response": "Parses an ASL file - like object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds the path segments to the internal path segments table.", "response": "def _AddPathSegments(self, path, ignore_list):\n    \"\"\"Adds the path segments to the table.\n\n    Args:\n      path: a string containing the path.\n      ignore_list: a list of path segment indexes to ignore, where 0 is the\n                   index of the first path segment relative from the root.\n    \"\"\"\n    path_segments = path.split(self._path_segment_separator)\n    for path_segment_index, path_segment in enumerate(path_segments):\n      if path_segment_index not in self.path_segments_per_index:\n        self.path_segments_per_index[path_segment_index] = {}\n\n      if path_segment_index not in ignore_list:\n        path_segments = self.path_segments_per_index[path_segment_index]\n\n        if path_segment not in path_segments:\n          path_segments[path_segment] = []\n\n        paths_per_segment_list = path_segments[path_segment]\n        paths_per_segment_list.append(path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ToDebugString(self):\n    text_parts = ['Path segment index\\tPath segments(s)']\n    for index, path_segments in self.path_segments_per_index.items():\n      text_parts.append('{0:d}\\t\\t\\t[{1:s}]'.format(\n          index, ', '.join(path_segments)))\n\n    text_parts.append('')\n\n    return '\\n'.join(text_parts)", "response": "Converts the path filter table into a debug string."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef AddIndex(self, path_segment_index):\n    if path_segment_index in self._weight_per_index:\n      raise ValueError('Path segment index already set.')\n\n    self._weight_per_index[path_segment_index] = 0", "response": "Adds a path segment index and sets its weight to 0."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a weight for a specific path segment index.", "response": "def AddWeight(self, path_segment_index, weight):\n    \"\"\"Adds a weight for a specific path segment index.\n\n    Args:\n      path_segment_index: an integer containing the path segment index.\n      weight: an integer containing the weight.\n\n    Raises:\n      ValueError: if the path segment weights do not contain\n                  the path segment index.\n    \"\"\"\n    if path_segment_index not in self._weight_per_index:\n      raise ValueError('Path segment index not set.')\n\n    self._weight_per_index[path_segment_index] += weight\n\n    if weight not in self._indexes_per_weight:\n      self._indexes_per_weight[weight] = []\n\n    self._indexes_per_weight[weight].append(path_segment_index)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef SetWeight(self, path_segment_index, weight):\n    if path_segment_index not in self._weight_per_index:\n      raise ValueError('Path segment index not set.')\n\n    self._weight_per_index[path_segment_index] = weight\n\n    if weight not in self._indexes_per_weight:\n      self._indexes_per_weight[weight] = []\n\n    self._indexes_per_weight[weight].append(path_segment_index)", "response": "Sets a weight for a specific path segment index."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ToDebugString(self):\n    text_parts = ['Path segment index\\tWeight']\n    for path_segment_index, weight in self._weight_per_index.items():\n      text_parts.append('{0:d}\\t\\t\\t{1:d}'.format(\n          path_segment_index, weight))\n    text_parts.append('')\n\n    text_parts.append('Weight\\t\\t\\tPath segment index(es)')\n    for weight, path_segment_indexes in self._indexes_per_weight.items():\n      text_parts.append('{0:d}\\t\\t\\t{1!s}'.format(\n          weight, path_segment_indexes))\n    text_parts.append('')\n\n    return '\\n'.join(text_parts)", "response": "Converts the path segment weights into a debug string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _BuildScanTreeNode(self, path_filter_table, ignore_list):\n    # Make a copy of the lists because the function is going to alter them\n    # and the changes must remain in scope of the function.\n    paths_list = list(path_filter_table.paths)\n    ignore_list = list(ignore_list)\n\n    similarity_weights = _PathSegmentWeights()\n    occurrence_weights = _PathSegmentWeights()\n    value_weights = _PathSegmentWeights()\n\n    for path_segment_index in path_filter_table.path_segments_per_index.keys():\n      # Skip a path segment index for which no path segments are defined\n      # in the path filter table.\n      if not path_filter_table.path_segments_per_index[path_segment_index]:\n        continue\n\n      similarity_weights.AddIndex(path_segment_index)\n      occurrence_weights.AddIndex(path_segment_index)\n      value_weights.AddIndex(path_segment_index)\n\n      path_segments = path_filter_table.GetPathSegments(path_segment_index)\n\n      number_of_path_segments = len(path_segments.keys())\n      if number_of_path_segments > 1:\n        occurrence_weights.SetWeight(\n            path_segment_index, number_of_path_segments)\n\n      for paths_per_segment_list in path_segments.values():\n        path_segment_weight = len(paths_per_segment_list)\n        if path_segment_weight > 1:\n          similarity_weights.AddWeight(path_segment_index, path_segment_weight)\n\n    path_segment_index = self._GetMostSignificantPathSegmentIndex(\n        paths_list, similarity_weights, occurrence_weights, value_weights)\n\n    ignore_list.append(path_segment_index)\n\n    if path_segment_index < 0:\n      raise ValueError('Invalid path segment index value out of bounds.')\n\n    scan_tree_node = PathFilterScanTreeNode(path_segment_index)\n\n    path_segments = path_filter_table.GetPathSegments(path_segment_index)\n\n    for path_segment, paths_per_segment_list in path_segments.items():\n      if not paths_per_segment_list:\n        raise ValueError('Invalid number of paths value out of bounds.')\n\n      if len(paths_per_segment_list) == 1:\n        for path in paths_per_segment_list:\n          scan_tree_node.AddPathSegment(path_segment, path)\n\n      else:\n        sub_path_filter_table = _PathFilterTable(\n            paths_per_segment_list, ignore_list,\n            path_segment_separator=self._path_segment_separator)\n\n        scan_sub_node = self._BuildScanTreeNode(\n            sub_path_filter_table, ignore_list)\n\n        scan_tree_node.AddPathSegment(path_segment, scan_sub_node)\n\n      for path in paths_per_segment_list:\n        paths_list.remove(path)\n\n    number_of_paths = len(paths_list)\n    if number_of_paths == 1:\n      scan_tree_node.SetDefaultValue(paths_list[0])\n\n    elif number_of_paths > 1:\n      path_filter_table = _PathFilterTable(\n          paths_list, ignore_list,\n          path_segment_separator=self._path_segment_separator)\n\n      scan_sub_node = self._BuildScanTreeNode(path_filter_table, ignore_list)\n\n      scan_tree_node.SetDefaultValue(scan_sub_node)\n\n    return scan_tree_node", "response": "Builds a scan tree node."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving the index of the most significant path segment in the tree tree.", "response": "def _GetMostSignificantPathSegmentIndex(\n      self, paths, similarity_weights, occurrence_weights, value_weights):\n    \"\"\"Retrieves the index of the most significant path segment.\n\n    Args:\n      paths: a list of strings containing the paths.\n      similarity_weights: the similarity weights object (instance of\n                          _PathSegmentWeights).\n      occurrence_weights: the occurrence weights object (instance of\n                          _PathSegmentWeights).\n      value_weights: the value weights object (instance of _PathSegmentWeights).\n\n    Returns:\n      An integer containing the path segment index.\n\n    Raises:\n      ValueError: when paths is an empty list.\n    \"\"\"\n    if not paths:\n      raise ValueError('Missing paths.')\n\n    number_of_paths = len(paths)\n\n    path_segment_index = None\n    if number_of_paths == 1:\n      path_segment_index = self._GetPathSegmentIndexForValueWeights(\n          value_weights)\n\n    elif number_of_paths == 2:\n      path_segment_index = self._GetPathSegmentIndexForOccurrenceWeights(\n          occurrence_weights, value_weights)\n\n    elif number_of_paths > 2:\n      path_segment_index = self._GetPathSegmentIndexForSimilarityWeights(\n          similarity_weights, occurrence_weights, value_weights)\n\n    return path_segment_index"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve the index of the path segment based on occurrence weights.", "response": "def _GetPathSegmentIndexForOccurrenceWeights(\n      self, occurrence_weights, value_weights):\n    \"\"\"Retrieves the index of the path segment based on occurrence weights.\n\n    Args:\n      occurrence_weights: the occurrence weights object (instance of\n                          _PathSegmentWeights).\n      value_weights: the value weights object (instance of _PathSegmentWeights).\n\n    Returns:\n      An integer containing the path segment index.\n    \"\"\"\n    largest_weight = occurrence_weights.GetLargestWeight()\n\n    if largest_weight > 0:\n      occurrence_weight_indexes = occurrence_weights.GetIndexesForWeight(\n          largest_weight)\n      number_of_occurrence_indexes = len(occurrence_weight_indexes)\n    else:\n      number_of_occurrence_indexes = 0\n\n    path_segment_index = None\n    if number_of_occurrence_indexes == 0:\n      path_segment_index = self._GetPathSegmentIndexForValueWeights(\n          value_weights)\n\n    elif number_of_occurrence_indexes == 1:\n      path_segment_index = occurrence_weight_indexes[0]\n\n    else:\n      largest_weight = 0\n\n      for occurrence_index in occurrence_weight_indexes:\n        value_weight = value_weights.GetWeightForIndex(occurrence_index)\n\n        if not path_segment_index or largest_weight < value_weight:\n          largest_weight = value_weight\n          path_segment_index = occurrence_index\n\n    return path_segment_index"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _GetPathSegmentIndexForSimilarityWeights(\n      self, similarity_weights, occurrence_weights, value_weights):\n    \"\"\"Retrieves the index of the path segment based on similarity weights.\n\n    Args:\n      similarity_weights: the similarity weights object (instance of\n                          _PathSegmentWeights).\n      occurrence_weights: the occurrence weights object (instance of\n                          _PathSegmentWeights).\n      value_weights: the value weights object (instance of _PathSegmentWeights).\n\n    Returns:\n      An integer containing the path segment index.\n    \"\"\"\n    largest_weight = similarity_weights.GetLargestWeight()\n\n    if largest_weight > 0:\n      similarity_weight_indexes = similarity_weights.GetIndexesForWeight(\n          largest_weight)\n      number_of_similarity_indexes = len(similarity_weight_indexes)\n    else:\n      number_of_similarity_indexes = 0\n\n    path_segment_index = None\n    if number_of_similarity_indexes == 0:\n      path_segment_index = self._GetPathSegmentIndexForOccurrenceWeights(\n          occurrence_weights, value_weights)\n\n    elif number_of_similarity_indexes == 1:\n      path_segment_index = similarity_weight_indexes[0]\n\n    else:\n      largest_weight = 0\n      largest_value_weight = 0\n\n      for similarity_index in similarity_weight_indexes:\n        occurrence_weight = occurrence_weights.GetWeightForIndex(\n            similarity_index)\n\n        if largest_weight > 0 and largest_weight == occurrence_weight:\n          value_weight = value_weights.GetWeightForIndex(similarity_index)\n\n          if largest_value_weight < value_weight:\n            largest_weight = 0\n\n        if not path_segment_index or largest_weight < occurrence_weight:\n          largest_weight = occurrence_weight\n          path_segment_index = similarity_index\n\n          largest_value_weight = value_weights.GetWeightForIndex(\n              similarity_index)\n\n    return path_segment_index", "response": "Retrieves the index of the path segment based on similarity weights."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nretrieve the index of the path segment based on value weights.", "response": "def _GetPathSegmentIndexForValueWeights(self, value_weights):\n    \"\"\"Retrieves the index of the path segment based on value weights.\n\n    Args:\n      value_weights: the value weights object (instance of _PathSegmentWeights).\n\n    Returns:\n      An integer containing the path segment index.\n\n    Raises:\n      RuntimeError: is no path segment index can be found.\n    \"\"\"\n    largest_weight = value_weights.GetLargestWeight()\n\n    if largest_weight > 0:\n      value_weight_indexes = value_weights.GetIndexesForWeight(largest_weight)\n    else:\n      value_weight_indexes = []\n\n    if value_weight_indexes:\n      path_segment_index = value_weight_indexes[0]\n    else:\n      path_segment_index = value_weights.GetFirstAvailableIndex()\n\n    if path_segment_index is None:\n      raise RuntimeError('No path segment index found.')\n\n    return path_segment_index"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if a path matches the scan tree - based path filter.", "response": "def CheckPath(self, path, path_segment_separator=None):\n    \"\"\"Checks if a path matches the scan tree-based path filter.\n\n    Args:\n      path: a string containing the path.\n      path_segment_separator: optional string containing the path segment\n                              separator. None defaults to the path segment\n                              separator that was set when the path filter\n                              scan tree was initialized.\n\n    Returns:\n      A boolean indicating if the path matches the filter.\n    \"\"\"\n    if not self._case_sensitive:\n      path = path.lower()\n\n    if path_segment_separator is None:\n      path_segment_separator = self._path_segment_separator\n\n    path_segments = path.split(path_segment_separator)\n    number_of_path_segments = len(path_segments)\n\n    scan_object = self._root_node\n    while scan_object:\n      if isinstance(scan_object, py2to3.STRING_TYPES):\n        break\n\n      if scan_object.path_segment_index >= number_of_path_segments:\n        scan_object = scan_object.default_value\n        continue\n\n      path_segment = path_segments[scan_object.path_segment_index]\n      scan_object = scan_object.GetScanObject(path_segment)\n\n    if not isinstance(scan_object, py2to3.STRING_TYPES):\n      return False\n\n    filter_path_segments = scan_object.split(self._path_segment_separator)\n    return filter_path_segments == path_segments"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd a path segment to the tree.", "response": "def AddPathSegment(self, path_segment, scan_object):\n    \"\"\"Adds a path segment.\n\n    Args:\n      path_segment: a string containing the path segment.\n      scan_object: a scan object, either a scan tree sub node (instance of\n                   PathFilterScanTreeNode) or a string containing a path.\n\n    Raises:\n      ValueError: if the node already contains a scan object for\n                  the path segment.\n    \"\"\"\n    if path_segment in self._path_segments:\n      raise ValueError('Path segment already set.')\n\n    if isinstance(scan_object, PathFilterScanTreeNode):\n      scan_object.parent = self\n\n    self._path_segments[path_segment] = scan_object"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef SetDefaultValue(self, scan_object):\n    if (not isinstance(scan_object, PathFilterScanTreeNode) and\n        not isinstance(scan_object, py2to3.STRING_TYPES)):\n      raise TypeError('Unsupported scan object type.')\n\n    if self.default_value:\n      raise ValueError('Default value already set.')\n\n    self.default_value = scan_object", "response": "Sets the default value for this scan object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts the path filter scan tree node into a debug string.", "response": "def ToDebugString(self, indentation_level=1):\n    \"\"\"Converts the path filter scan tree node into a debug string.\n\n    Args:\n      indentation_level: an integer containing the text indentation level.\n\n    Returns:\n      A string containing a debug representation of the path filter scan\n      tree node.\n    \"\"\"\n    indentation = '  ' * indentation_level\n\n    text_parts = ['{0:s}path segment index: {1:d}\\n'.format(\n        indentation, self.path_segment_index)]\n\n    for path_segment, scan_object in self._path_segments.items():\n      text_parts.append('{0:s}path segment: {1:s}\\n'.format(\n          indentation, path_segment))\n\n      if isinstance(scan_object, PathFilterScanTreeNode):\n        text_parts.append('{0:s}scan tree node:\\n'.format(indentation))\n        text_parts.append(scan_object.ToDebugString(indentation_level + 1))\n\n      elif isinstance(scan_object, py2to3.STRING_TYPES):\n        text_parts.append('{0:s}path: {1:s}\\n'.format(\n            indentation, scan_object))\n\n    text_parts.append('{0:s}default value:\\n'.format(indentation))\n\n    if isinstance(self.default_value, PathFilterScanTreeNode):\n      text_parts.append('{0:s}scan tree node:\\n'.format(indentation))\n      text_parts.append(self.default_value.ToDebugString(indentation_level + 1))\n\n    elif isinstance(self.default_value, py2to3.STRING_TYPES):\n      text_parts.append('{0:s}pattern: {1:s}\\n'.format(\n          indentation, self.default_value))\n\n    text_parts.append('\\n')\n\n    return ''.join(text_parts)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse a MRUListEx entry value.", "response": "def _ParseMRUListExEntryValue(\n      self, parser_mediator, registry_key, entry_index, entry_number,\n      values_dict, value_strings, parent_path_segments, codepage='cp1252'):\n    \"\"\"Parses the MRUListEx entry value.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      registry_key (dfwinreg.WinRegistryKey): Windows Registry key that contains\n           the MRUListEx value.\n      entry_index (int): MRUListEx entry index.\n      entry_number (int): entry number.\n      values_dict (dict[str, object]): values of the key.\n      value_strings (dict[str, str]): value names and strings.\n      parent_path_segments (list[str]): parent shell item path segments.\n      codepage (Optional[str]): extended ASCII string codepage.\n\n    Returns:\n      str: path segment of the shell item.\n    \"\"\"\n    value = registry_key.GetValueByName('{0:d}'.format(entry_number))\n    path_segment = 'N/A'\n    value_string = ''\n    if value is None:\n      parser_mediator.ProduceExtractionWarning(\n          'Missing MRUListEx entry value: {0:d} in key: {1:s}.'.format(\n              entry_number, registry_key.path))\n\n    elif not value.DataIsBinaryData():\n      parser_mediator.ProduceExtractionWarning(\n          'Non-binary MRUListEx entry value: {0:d} in key: {1:s}.'.format(\n              entry_number, registry_key.path))\n\n    elif value.data:\n      shell_items_parser = shell_items.ShellItemsParser(registry_key.path)\n      shell_items_parser.ParseByteStream(\n          parser_mediator, value.data,\n          parent_path_segments=parent_path_segments, codepage=codepage)\n\n      path_segment = shell_items_parser.GetUpperPathSegment()\n      value_string = shell_items_parser.CopyToPath()\n\n      value_strings[entry_number] = value_string\n\n      value_string = 'Shell item path: {0:s}'.format(value_string)\n\n    value_text = 'Index: {0:d} [MRU Value {1:d}]'.format(\n        entry_index + 1, entry_number)\n\n    values_dict[value_text] = value_string\n\n    return path_segment"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _ParseMRUListExValue(self, registry_key):\n    mrulistex_value = registry_key.GetValueByName('MRUListEx')\n\n    # The key exists but does not contain a value named \"MRUList\".\n    if not mrulistex_value:\n      return None\n\n    mrulistex_entries_map = self._GetDataTypeMap('mrulistex_entries')\n\n    context = dtfabric_data_maps.DataTypeMapContext(values={\n        'data_size': len(mrulistex_value.data)})\n\n    return self._ReadStructureFromByteStream(\n        mrulistex_value.data, 0, mrulistex_entries_map, context=context)", "response": "Parses the MRUListEx value in a given Windows Registry key."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing a subsystem key.", "response": "def _ParseSubKey(\n      self, parser_mediator, registry_key, parent_path_segments,\n      codepage='cp1252'):\n    \"\"\"Extract event objects from a MRUListEx Registry key.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      registry_key (dfwinreg.WinRegistryKey): Windows Registry key.\n      parent_path_segments (list[str]): parent shell item path segments.\n      codepage (Optional[str]): extended ASCII string codepage.\n    \"\"\"\n    try:\n      mrulistex = self._ParseMRUListExValue(registry_key)\n    except (ValueError, errors.ParseError) as exception:\n      parser_mediator.ProduceExtractionWarning(\n          'unable to parse MRUListEx value with error: {0!s}'.format(exception))\n      return\n\n    if not mrulistex:\n      return\n\n    entry_numbers = {}\n    values_dict = {}\n    value_strings = {}\n\n    found_terminator = False\n    for index, entry_number in enumerate(mrulistex):\n      # The MRU list is terminated with -1 (0xffffffff).\n      if entry_number == -1:\n        continue\n\n      if found_terminator:\n        parser_mediator.ProduceExtractionWarning((\n            'found additional MRUListEx entries after terminator in key: '\n            '{0:s}.').format(registry_key.path))\n\n        # Only create one parser error per terminator.\n        found_terminator = False\n\n      path_segment = self._ParseMRUListExEntryValue(\n          parser_mediator, registry_key, index, entry_number, values_dict,\n          value_strings, parent_path_segments, codepage=codepage)\n\n      entry_numbers[entry_number] = path_segment\n\n    event_data = windows_events.WindowsRegistryEventData()\n    event_data.key_path = registry_key.path\n    event_data.offset = registry_key.offset\n    event_data.regvalue = values_dict\n    event_data.source_append = self._SOURCE_APPEND\n    event_data.urls = self.URLS\n\n    event = time_events.DateTimeValuesEvent(\n        registry_key.last_written_time, definitions.TIME_DESCRIPTION_WRITTEN)\n    parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    for entry_number, path_segment in iter(entry_numbers.items()):\n      sub_key_name = '{0:d}'.format(entry_number)\n      sub_key = registry_key.GetSubkeyByName(sub_key_name)\n      if not sub_key:\n        parser_mediator.ProduceExtractionWarning(\n            'Missing BagMRU sub key: {0:d} in key: {1:s}.'.format(\n                entry_number, registry_key.path))\n        continue\n\n      parent_path_segments.append(path_segment)\n      self._ParseSubKey(\n          parser_mediator, sub_key, parent_path_segments, codepage=codepage)\n      parent_path_segments.pop()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ExtractEvents(\n      self, parser_mediator, registry_key, codepage='cp1252', **kwargs):\n    \"\"\"Extracts events from a Windows Registry key.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      registry_key (dfwinreg.WinRegistryKey): Windows Registry key.\n      codepage (Optional[str]): extended ASCII string codepage.\n    \"\"\"\n    self._ParseSubKey(parser_mediator, registry_key, [], codepage=codepage)", "response": "Extracts events from a Windows Registry key."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef GetEntries(self, parser_mediator, match=None, **unused_kwargs):\n    version = match.get('LastAttemptSystemVersion', 'N/A')\n    pending = match.get('LastUpdatesAvailable', None)\n\n    event_data = plist_event.PlistTimeEventData()\n    event_data.desc = 'Last MacOS {0:s} full update.'.format(version)\n    event_data.key = ''\n    event_data.root = '/'\n\n    datetime_value = match.get('LastFullSuccessfulDate', None)\n    if datetime_value:\n      event = time_events.PythonDatetimeEvent(\n          datetime_value, definitions.TIME_DESCRIPTION_WRITTEN)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    datetime_value = match.get('LastSuccessfulDate', None)\n    if datetime_value and pending:\n      software = []\n      for update in match.get('RecommendedUpdates', []):\n        identifier = update.get('Identifier', '<IDENTIFIER>')\n        product_key = update.get('Product Key', '<PRODUCT_KEY>')\n\n        software.append('{0:s}({1:s})'.format(identifier, product_key))\n\n      if not software:\n        return\n\n      software = ','.join(software)\n      event_data.desc = (\n          'Last Mac OS {0!s} partially update, pending {1!s}: '\n          '{2:s}.').format(version, pending, software)\n\n      event = time_events.PythonDatetimeEvent(\n          datetime_value, definitions.TIME_DESCRIPTION_WRITTEN)\n      parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Extracts relevant update entries from the MacOS update list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing the value data for a preprocessing attribute.", "response": "def _ParseValueData(self, knowledge_base, value_data):\n    \"\"\"Parses Windows Registry value data for a preprocessing attribute.\n\n    Args:\n      knowledge_base (KnowledgeBase): to fill with preprocessing information.\n      value_data (object): Windows Registry value data.\n\n    Raises:\n      errors.PreProcessFail: if the preprocessing fails.\n    \"\"\"\n    if not isinstance(value_data, py2to3.UNICODE_TYPE):\n      raise errors.PreProcessFail(\n          'Unsupported Windows Registry value type: {0:s} for '\n          'artifact: {1:s}.'.format(\n              type(value_data), self.ARTIFACT_DEFINITION_NAME))\n\n    environment_variable = artifacts.EnvironmentVariableArtifact(\n        case_sensitive=False, name=self._NAME, value=value_data)\n\n    try:\n      logger.debug('setting environment variable: {0:s} to: \"{1:s}\"'.format(\n          self._NAME, value_data))\n      knowledge_base.AddEnvironmentVariable(environment_variable)\n    except KeyError:\n      # TODO: add and store preprocessing errors.\n      pass"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse the path specification and adds the environment variable to the artifact value data.", "response": "def _ParsePathSpecification(\n      self, knowledge_base, searcher, file_system, path_specification,\n      path_separator):\n    \"\"\"Parses artifact file system data for a preprocessing attribute.\n\n    Args:\n      knowledge_base (KnowledgeBase): to fill with preprocessing information.\n      searcher (dfvfs.FileSystemSearcher): file system searcher to preprocess\n          the file system.\n      file_system (dfvfs.FileSystem): file system to be preprocessed.\n      path_specification (dfvfs.PathSpec): path specification that contains\n          the artifact value data.\n      path_separator (str): path segment separator.\n\n    Raises:\n      errors.PreProcessFail: if the preprocessing fails.\n    \"\"\"\n    relative_path = searcher.GetRelativePath(path_specification)\n    if not relative_path:\n      raise errors.PreProcessFail(\n          'Unable to read: {0:s} with error: missing relative path'.format(\n              self.ARTIFACT_DEFINITION_NAME))\n\n    if path_separator != file_system.PATH_SEPARATOR:\n      relative_path_segments = file_system.SplitPath(relative_path)\n      relative_path = '{0:s}{1:s}'.format(\n          path_separator, path_separator.join(relative_path_segments))\n\n    environment_variable = artifacts.EnvironmentVariableArtifact(\n        case_sensitive=False, name=self._NAME, value=relative_path)\n\n    try:\n      logger.debug('setting environment variable: {0:s} to: \"{1:s}\"'.format(\n          self._NAME, relative_path))\n      knowledge_base.AddEnvironmentVariable(environment_variable)\n    except KeyError:\n      # TODO: add and store preprocessing errors.\n      pass"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef Collect(self, knowledge_base):\n    environment_variable = knowledge_base.GetEnvironmentVariable('programdata')\n    allusersappdata = getattr(environment_variable, 'value', None)\n\n    if not allusersappdata:\n      environment_variable = knowledge_base.GetEnvironmentVariable(\n          'allusersprofile')\n      allusersdata = getattr(environment_variable, 'value', None)\n\n      if allusersdata:\n        allusersappdata = '\\\\'.join([allusersdata, 'Application Data'])\n\n    if allusersappdata:\n      environment_variable = artifacts.EnvironmentVariableArtifact(\n          case_sensitive=False, name='allusersappdata', value=allusersappdata)\n\n      try:\n        logger.debug('setting environment variable: {0:s} to: \"{1:s}\"'.format(\n            'allusersappdata', allusersappdata))\n        knowledge_base.AddEnvironmentVariable(environment_variable)\n      except KeyError:\n        # TODO: add and store preprocessing errors.\n        pass", "response": "Collects values from the knowledge base."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses the value data for a preprocessing attribute.", "response": "def _ParseValueData(self, knowledge_base, value_data):\n    \"\"\"Parses Windows Registry value data for a preprocessing attribute.\n\n    Args:\n      knowledge_base (KnowledgeBase): to fill with preprocessing information.\n      value_data (object): Windows Registry value data.\n\n    Raises:\n      errors.PreProcessFail: if the preprocessing fails.\n    \"\"\"\n    if not isinstance(value_data, py2to3.UNICODE_TYPE):\n      raise errors.PreProcessFail(\n          'Unsupported Windows Registry value type: {0:s} for '\n          'artifact: {1:s}.'.format(\n              type(value_data), self.ARTIFACT_DEFINITION_NAME))\n\n    # Map the Windows code page name to a Python equivalent name.\n    codepage = 'cp{0:s}'.format(value_data)\n\n    if not knowledge_base.codepage:\n      try:\n        knowledge_base.SetCodepage(codepage)\n      except ValueError:\n        # TODO: add and store preprocessing errors.\n        pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _ParseValueData(self, knowledge_base, value_data):\n    if not isinstance(value_data, py2to3.UNICODE_TYPE):\n      raise errors.PreProcessFail(\n          'Unsupported Windows Registry value type: {0:s} for '\n          'artifact: {1:s}.'.format(\n              type(value_data), self.ARTIFACT_DEFINITION_NAME))\n\n    if not knowledge_base.GetHostname():\n      hostname_artifact = artifacts.HostnameArtifact(name=value_data)\n      knowledge_base.SetHostname(hostname_artifact)", "response": "Parses the value data for a preprocessing attribute."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncollects values from the given knowledge base.", "response": "def Collect(self, knowledge_base):\n    \"\"\"Collects values from the knowledge base.\n\n    Args:\n      knowledge_base (KnowledgeBase): to fill with preprocessing information.\n\n    Raises:\n      PreProcessFail: if the preprocessing fails.\n    \"\"\"\n    environment_variable = knowledge_base.GetEnvironmentVariable(\n        'programdata')\n    allusersprofile = getattr(environment_variable, 'value', None)\n\n    if not allusersprofile:\n      environment_variable = knowledge_base.GetEnvironmentVariable(\n          'allusersprofile')\n      allusersprofile = getattr(environment_variable, 'value', None)\n\n      if allusersprofile:\n        environment_variable = artifacts.EnvironmentVariableArtifact(\n            case_sensitive=False, name='programdata', value=allusersprofile)\n\n        try:\n          logger.debug('setting environment variable: {0:s} to: \"{1:s}\"'.format(\n              'programdata', allusersprofile))\n          knowledge_base.AddEnvironmentVariable(environment_variable)\n        except KeyError:\n          # TODO: add and store preprocessing errors.\n          pass"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing the value data for a preprocessing attribute.", "response": "def _ParseValueData(self, knowledge_base, value_data):\n    \"\"\"Parses Windows Registry value data for a preprocessing attribute.\n\n    Args:\n      knowledge_base (KnowledgeBase): to fill with preprocessing information.\n      value_data (object): Windows Registry value data.\n\n    Raises:\n      errors.PreProcessFail: if the preprocessing fails.\n    \"\"\"\n    if not isinstance(value_data, py2to3.UNICODE_TYPE):\n      raise errors.PreProcessFail(\n          'Unsupported Windows Registry value type: {0:s} for '\n          'artifact: {1:s}.'.format(\n              type(value_data), self.ARTIFACT_DEFINITION_NAME))\n\n    if not knowledge_base.GetValue('operating_system_product'):\n      knowledge_base.SetValue('operating_system_product', value_data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _ParseValueData(self, knowledge_base, value_data):\n    if not isinstance(value_data, py2to3.UNICODE_TYPE):\n      raise errors.PreProcessFail(\n          'Unsupported Windows Registry value type: {0:s} for '\n          'artifact: {1:s}.'.format(\n              type(value_data), self.ARTIFACT_DEFINITION_NAME))\n\n    # Map the Windows time zone name to a Python equivalent name.\n    lookup_key = value_data.replace(' ', '')\n\n    time_zone = time_zones.TIME_ZONES.get(lookup_key, value_data)\n    # TODO: check if time zone is set in knowledge base.\n    if time_zone:\n      try:\n        # Catch and warn about unsupported preprocessor plugin.\n        knowledge_base.SetTimeZone(time_zone)\n      except ValueError:\n        # TODO: add and store preprocessing errors.\n        time_zone = value_data\n        logger.warning('Unable to map: \"{0:s}\" to time zone'.format(\n            value_data))", "response": "Parses the value data for a preprocessing attribute."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving the username from a Windows profile path.", "response": "def _GetUsernameFromProfilePath(self, path):\n    \"\"\"Retrieves the username from a Windows profile path.\n\n    Trailing path path segment are ignored.\n\n    Args:\n      path (str): a Windows path with '\\\\' as path segment separator.\n\n    Returns:\n      str: basename which is the last path segment.\n    \"\"\"\n    # Strip trailing key separators.\n    while path and path[-1] == '\\\\':\n      path = path[:-1]\n\n    if path:\n      _, _, path = path.rpartition('\\\\')\n    return path"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _ParseKey(self, knowledge_base, registry_key, value_name):\n    user_account = artifacts.UserAccountArtifact(\n        identifier=registry_key.name, path_separator='\\\\')\n\n    registry_value = registry_key.GetValueByName('ProfileImagePath')\n    if registry_value:\n      profile_path = registry_value.GetDataAsObject()\n      username = self._GetUsernameFromProfilePath(profile_path)\n\n      user_account.user_directory = profile_path or None\n      user_account.username = username or None\n\n    try:\n      knowledge_base.AddUserAccount(user_account)\n    except KeyError:\n      # TODO: add and store preprocessing errors.\n      pass", "response": "Parses a Windows Registry key for a preprocessing attribute."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef AddArguments(cls, argument_group):\n    argument_group.add_argument(\n        '--status_view', '--status-view', dest='status_view_mode',\n        choices=['linear', 'none', 'window'], action='store',\n        metavar='TYPE', default=status_view.StatusView.MODE_WINDOW, help=(\n            'The processing status view mode: \"linear\", \"none\" or \"window\".'))", "response": "Adds command line arguments to an argument group."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ParseOptions(cls, options, configuration_object):\n    if not isinstance(configuration_object, tools.CLITool):\n      raise errors.BadConfigObject(\n          'Configuration object is not an instance of CLITool')\n\n    status_view_mode = cls._ParseStringOption(\n        options, 'status_view_mode',\n        default_value=status_view.StatusView.MODE_WINDOW)\n\n    setattr(configuration_object, '_status_view_mode', status_view_mode)", "response": "Parses and validates options."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef AddArguments(cls, argument_group):\n    argument_group.add_argument(\n        '--append', dest='append', action='store_true', default=False,\n        required=cls._DEFAULT_APPEND, help=(\n            'Defines whether the intention is to append to an already '\n            'existing database or overwrite it. Defaults to overwrite.'))\n    argument_group.add_argument(\n        '--evidence', dest='evidence', type=str,\n        default=cls._DEFAULT_EVIDENCE, action='store', required=False,\n        help='Set the evidence field to a specific value, defaults to empty.')\n    argument_group.add_argument(\n        '--fields', dest='fields', type=str, action='store',\n        default=cls._DEFAULT_FIELDS, help=(\n            'Defines which fields should be indexed in the database.'))\n    argument_group.add_argument(\n        '--additional_fields', dest='additional_fields', type=str,\n        action='store', default='', help=(\n            'Defines extra fields to be included in the output, in addition to'\n            ' the default fields, which are {0:s}.'.format(\n                cls._DEFAULT_FIELDS)))", "response": "Adds command line arguments to an argument group."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ParseOptions(cls, options, output_module):\n    if not isinstance(output_module, shared_4n6time.Shared4n6TimeOutputModule):\n      raise errors.BadConfigObject(\n          'Output module is not an instance of Shared4n6TimeOutputModule')\n\n    append = getattr(options, 'append', cls._DEFAULT_APPEND)\n    evidence = cls._ParseStringOption(\n        options, 'evidence', default_value=cls._DEFAULT_EVIDENCE)\n    fields = cls._ParseStringOption(\n        options, 'fields', default_value=cls._DEFAULT_FIELDS)\n    additional_fields = cls._ParseStringOption(\n        options, 'additional_fields')\n\n    if additional_fields:\n      fields = '{0:s},{1:s}'.format(fields, additional_fields)\n\n    output_module.SetAppendMode(append)\n    output_module.SetEvidence(evidence)\n    output_module.SetFields([\n        field_name.strip() for field_name in fields.split(',')])", "response": "Parses and validates the options."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads a single line of text from the file - like object.", "response": "def readline(self, size=None):\n    \"\"\"Reads a single line of text.\n\n    The functions reads one entire line from the file-like object. A trailing\n    end-of-line indicator (newline by default) is kept in the byte string (but\n    may be absent when a file ends with an incomplete line). An empty byte\n    string is returned only when end-of-file is encountered immediately.\n\n    Args:\n      size (Optional[int]): maximum byte size to read. If present and\n          non-negative, it is a maximum byte count (including the trailing\n          end-of-line) and an incomplete line may be returned.\n\n    Returns:\n      bytes: line of text.\n\n    Raises:\n      ValueError: if the specified size is less than zero or greater\n          than the maximum size allowed.\n    \"\"\"\n    if size is not None and size < 0:\n      raise ValueError('Invalid size value smaller than zero.')\n\n    if size is not None and size > self.MAXIMUM_READ_BUFFER_SIZE:\n      raise ValueError(\n          'Invalid size value exceeds maximum value {0:d}.'.format(\n              self.MAXIMUM_READ_BUFFER_SIZE))\n\n    if not self._lines:\n      if self._lines_buffer_offset >= self._file_object_size:\n        return b''\n\n      read_size = size\n      if not read_size:\n        read_size = self.MAXIMUM_READ_BUFFER_SIZE\n\n      if self._lines_buffer_offset + read_size > self._file_object_size:\n        size = self._file_object_size - self._lines_buffer_offset\n\n      self._file_object.seek(self._lines_buffer_offset, os.SEEK_SET)\n      read_buffer = self._file_object.read(read_size)\n\n      self._lines_buffer_offset += len(read_buffer)\n\n      self._lines = read_buffer.split(self.end_of_line)\n      if self._lines_buffer:\n        self._lines[0] = b''.join([self._lines_buffer, self._lines[0]])\n        self._lines_buffer = b''\n\n      if read_buffer[self._end_of_line_length:] != self.end_of_line:\n        self._lines_buffer = self._lines.pop()\n\n      for index, line in enumerate(self._lines):\n        self._lines[index] = b''.join([line, self.end_of_line])\n\n      if (self._lines_buffer and\n          self._lines_buffer_offset >= self._file_object_size):\n        self._lines.append(self._lines_buffer)\n        self._lines_buffer = b''\n\n    if not self._lines:\n      line = self._lines_buffer\n      self._lines_buffer = b''\n\n    elif not size or size >= len(self._lines[0]):\n      line = self._lines.pop(0)\n\n    else:\n      line = self._lines[0]\n      self._lines[0] = line[size:]\n      line = line[:size]\n\n    self._current_offset += len(line)\n\n    return line"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ParseOptions(cls, options, analysis_plugin):\n    if not isinstance(analysis_plugin, sessionize.SessionizeAnalysisPlugin):\n      raise errors.BadConfigObject(\n          'Analysis plugin is not an instance of SessionizeAnalysisPlugin')\n\n    maximum_pause = cls._ParseNumericOption(\n        options, 'sessionize_maximumpause', default_value=10)\n\n    if maximum_pause <= 0:\n      raise errors.BadConfigOption(\n          'Maximum pause value {0:d} is not supported. '\n          'Value must be greater than 0.'.format(maximum_pause))\n    analysis_plugin.SetMaximumPause(maximum_pause)", "response": "Parses and validates the options."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalling the function via RPC.", "response": "def CallFunction(self):\n    \"\"\"Calls the function via RPC.\"\"\"\n    if self._xmlrpc_proxy is None:\n      return None\n\n    rpc_call = getattr(self._xmlrpc_proxy, self._RPC_FUNCTION_NAME, None)\n    if rpc_call is None:\n      return None\n\n    try:\n      return rpc_call()  # pylint: disable=not-callable\n    except (\n        expat.ExpatError, SocketServer.socket.error,\n        xmlrpclib.Fault) as exception:\n      logger.warning('Unable to make RPC call with error: {0!s}'.format(\n          exception))\n      return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nopen a RPC communication channel to the server.", "response": "def Open(self, hostname, port):\n    \"\"\"Opens a RPC communication channel to the server.\n\n    Args:\n      hostname (str): hostname or IP address to connect to for requests.\n      port (int): port to connect to for requests.\n\n    Returns:\n      bool: True if the communication channel was established.\n    \"\"\"\n    server_url = 'http://{0:s}:{1:d}'.format(hostname, port)\n\n    try:\n      self._xmlrpc_proxy = xmlrpclib.ServerProxy(\n          server_url, allow_none=True)\n    except SocketServer.socket.error as exception:\n      logger.warning((\n          'Unable to connect to RPC server on {0:s}:{1:d} with error: '\n          '{2!s}').format(hostname, port, exception))\n      return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nopens the RPC communication channel for clients.", "response": "def _Open(self, hostname, port):\n    \"\"\"Opens the RPC communication channel for clients.\n\n    Args:\n      hostname (str): hostname or IP address to connect to for requests.\n      port (int): port to connect to for requests.\n\n    Returns:\n      bool: True if the communication channel was successfully opened.\n    \"\"\"\n    try:\n      self._xmlrpc_server = SimpleXMLRPCServer.SimpleXMLRPCServer(\n          (hostname, port), logRequests=False, allow_none=True)\n    except SocketServer.socket.error as exception:\n      logger.warning((\n          'Unable to bind a RPC server on {0:s}:{1:d} with error: '\n          '{2!s}').format(hostname, port, exception))\n      return False\n\n    self._xmlrpc_server.register_function(\n        self._callback, self._RPC_FUNCTION_NAME)\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstarting the process status RPC server.", "response": "def Start(self, hostname, port):\n    \"\"\"Starts the process status RPC server.\n\n    Args:\n      hostname (str): hostname or IP address to connect to for requests.\n      port (int): port to connect to for requests.\n\n    Returns:\n      bool: True if the RPC server was successfully started.\n    \"\"\"\n    if not self._Open(hostname, port):\n      return False\n\n    self._rpc_thread = threading.Thread(\n        name=self._THREAD_NAME, target=self._xmlrpc_server.serve_forever)\n    self._rpc_thread.start()\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nstops the process status RPC server.", "response": "def Stop(self):\n    \"\"\"Stops the process status RPC server.\"\"\"\n    self._Close()\n\n    if self._rpc_thread.isAlive():\n      self._rpc_thread.join()\n    self._rpc_thread = None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving a list of the registered analysis plugins.", "response": "def GetAllPluginInformation(cls, show_all=True):\n    \"\"\"Retrieves a list of the registered analysis plugins.\n\n    Args:\n      show_all (Optional[bool]): True if all analysis plugin names should\n          be listed.\n\n    Returns:\n      list[tuple[str, str, str]]: the name, docstring and type string of each\n          analysis plugin in alphabetical order.\n    \"\"\"\n    results = []\n    for plugin_class in iter(cls._plugin_classes.values()):\n      plugin_object = plugin_class()\n      if not show_all and not plugin_class.ENABLE_IN_EXTRACTION:\n        continue\n\n      # TODO: Use a specific description variable, not the docstring.\n      doc_string, _, _ = plugin_class.__doc__.partition('\\n')\n      type_string = cls._PLUGIN_TYPE_STRINGS.get(plugin_object.plugin_type)\n      information_tuple = (plugin_object.plugin_name, doc_string, type_string)\n      results.append(information_tuple)\n\n    return sorted(results)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve the plugin objects for the given plugin names.", "response": "def GetPluginObjects(cls, plugin_names):\n    \"\"\"Retrieves the plugin objects.\n\n    Args:\n      plugin_names (list[str]): names of plugins that should be retrieved.\n\n    Returns:\n      dict[str, AnalysisPlugin]: analysis plugins per name.\n    \"\"\"\n    plugin_objects = {}\n    for plugin_name, plugin_class in iter(cls._plugin_classes.items()):\n      if plugin_name not in plugin_names:\n        continue\n\n      plugin_objects[plugin_name] = plugin_class()\n\n    return plugin_objects"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef AddArguments(cls, argument_group):\n    argument_group.add_argument(\n        '--name', '--timeline_name', '--timeline-name',\n        dest='timeline_name', type=str, action='store',\n        default=cls._DEFAULT_NAME, required=False, help=(\n            'The name of the timeline in Timesketch. Default: '\n            'hostname if present in the storage file. If no hostname '\n            'is found then manual input is used.'))\n\n    argument_group.add_argument(\n        '--index', dest='index', type=str, action='store',\n        default=cls._DEFAULT_UUID, required=False, help=(\n            'The name of the Elasticsearch index. Default: Generate a random '\n            'UUID'))\n\n    argument_group.add_argument(\n        '--flush_interval', '--flush-interval', dest='flush_interval',\n        type=int, action='store', default=cls._DEFAULT_FLUSH_INTERVAL,\n        required=False, help=(\n            'The number of events to queue up before sent in bulk '\n            'to Elasticsearch.'))\n\n    argument_group.add_argument(\n        '--doc_type', dest='document_type', type=str,\n        action='store', default=cls._DEFAULT_DOCUMENT_TYPE, help=(\n            'Name of the document type that will be used in ElasticSearch.'))\n\n    argument_group.add_argument(\n        '--username', dest='username', type=str,\n        action='store', default=cls._DEFAULT_USERNAME, help=(\n            'Username of a Timesketch user that will own the timeline.'))", "response": "Adds command line arguments to an argument group."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse and validates the options.", "response": "def ParseOptions(cls, options, output_module):\n    \"\"\"Parses and validates options.\n\n    Args:\n      options (argparse.Namespace): parser options.\n      output_module (TimesketchOutputModule): output module to configure.\n\n    Raises:\n      BadConfigObject: when the output module object is of the wrong type.\n      BadConfigOption: when a configuration parameter fails validation.\n    \"\"\"\n    if not isinstance(output_module, timesketch_out.TimesketchOutputModule):\n      raise errors.BadConfigObject(\n          'Output module is not an instance of TimesketchOutputModule')\n\n    document_type = cls._ParseStringOption(\n        options, 'document_type', default_value=cls._DEFAULT_DOCUMENT_TYPE)\n    output_module.SetDocumentType(document_type)\n\n    flush_interval = cls._ParseNumericOption(\n        options, 'flush_interval', default_value=cls._DEFAULT_FLUSH_INTERVAL)\n    output_module.SetFlushInterval(flush_interval)\n\n    index = cls._ParseStringOption(\n        options, 'index', default_value=cls._DEFAULT_UUID)\n    output_module.SetIndexName(index)\n\n    name = cls._ParseStringOption(\n        options, 'timeline_name', default_value=cls._DEFAULT_NAME)\n    output_module.SetTimelineName(name)\n\n    username = cls._ParseStringOption(\n        options, 'username', default_value=cls._DEFAULT_USERNAME)\n    output_module.SetTimelineOwner(username)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ParseNotificationcenterRow(\n      self, parser_mediator, query, row, **unused_kwargs):\n    \"\"\"Parses a message row.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      query (str): query that created the row.\n      row (sqlite3.Row): row.\n    \"\"\"\n    query_hash = hash(query)\n\n    event_data = MacNotificationCenterEventData()\n    event_data.bundle_name = self._GetRowValue(query_hash, row, 'bundle_name')\n    event_data.presented = self._GetRowValue(query_hash, row, 'presented')\n\n    blob = self._GetRowValue(query_hash, row, 'dataBlob')\n\n    try:\n      full_biplist = biplist.readPlistFromString(blob)\n      # req is the 'req' dictionary from the plist containing extra information\n      # about the notification entry.\n      req = full_biplist['req']\n\n    except (biplist.InvalidPlistException, KeyError) as exception:\n      parser_mediator.ProduceExtractionWarning(\n          'unable to read plist from database with error: {0!s}'.format(\n              exception))\n      return\n\n    event_data.title = req.get('titl', None)\n    event_data.subtitle = req.get('subt', None)\n    event_data.body = req.get('body', None)\n\n    timestamp = self._GetRowValue(query_hash, row, 'timestamp')\n    date_time = dfdatetime_cocoa_time.CocoaTime(timestamp=timestamp)\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_CREATION)\n    parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Parses a NotificationCenter row."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef Process(self, parser_mediator, zip_file, archive_members):\n    if not self.REQUIRED_PATHS:\n      raise ValueError('REQUIRED_PATHS not specified')\n\n    if not set(archive_members).issuperset(self.REQUIRED_PATHS):\n      raise errors.WrongCompoundZIPPlugin(self.NAME)\n\n    logger.debug('Compound ZIP Plugin used: {0:s}'.format(self.NAME))\n\n    self.InspectZipFile(parser_mediator, zip_file)", "response": "Processes a zip file and returns a tuple of the archive members."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating the counters of the entire storage.", "response": "def _CalculateStorageCounters(self, storage_reader):\n    \"\"\"Calculates the counters of the entire storage.\n\n    Args:\n      storage_reader (StorageReader): storage reader.\n\n    Returns:\n      dict[str,collections.Counter]: storage counters.\n    \"\"\"\n    analysis_reports_counter = collections.Counter()\n    analysis_reports_counter_error = False\n    event_labels_counter = collections.Counter()\n    event_labels_counter_error = False\n    parsers_counter = collections.Counter()\n    parsers_counter_error = False\n\n    for session in storage_reader.GetSessions():\n      # Check for a dict for backwards compatibility.\n      if isinstance(session.analysis_reports_counter, dict):\n        analysis_reports_counter += collections.Counter(\n            session.analysis_reports_counter)\n      elif isinstance(session.analysis_reports_counter, collections.Counter):\n        analysis_reports_counter += session.analysis_reports_counter\n      else:\n        analysis_reports_counter_error = True\n\n      # Check for a dict for backwards compatibility.\n      if isinstance(session.event_labels_counter, dict):\n        event_labels_counter += collections.Counter(\n            session.event_labels_counter)\n      elif isinstance(session.event_labels_counter, collections.Counter):\n        event_labels_counter += session.event_labels_counter\n      else:\n        event_labels_counter_error = True\n\n      # Check for a dict for backwards compatibility.\n      if isinstance(session.parsers_counter, dict):\n        parsers_counter += collections.Counter(session.parsers_counter)\n      elif isinstance(session.parsers_counter, collections.Counter):\n        parsers_counter += session.parsers_counter\n      else:\n        parsers_counter_error = True\n\n    storage_counters = {}\n\n    warnings_by_path_spec = collections.Counter()\n    warnings_by_parser_chain = collections.Counter()\n\n    for warning in list(storage_reader.GetWarnings()):\n      warnings_by_path_spec[warning.path_spec.comparable] += 1\n      warnings_by_parser_chain[warning.parser_chain] += 1\n\n    storage_counters['warnings_by_path_spec'] = warnings_by_path_spec\n    storage_counters['warnings_by_parser_chain'] = warnings_by_parser_chain\n\n    if not analysis_reports_counter_error:\n      storage_counters['analysis_reports'] = analysis_reports_counter\n\n    if not event_labels_counter_error:\n      storage_counters['event_labels'] = event_labels_counter\n\n    if not parsers_counter_error:\n      storage_counters['parsers'] = parsers_counter\n\n    return storage_counters"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _CompareStores(self, storage_reader, compare_storage_reader):\n    storage_counters = self._CalculateStorageCounters(storage_reader)\n    compare_storage_counters = self._CalculateStorageCounters(\n        compare_storage_reader)\n\n    # TODO: improve comparison, currently only total numbers are compared.\n\n    return storage_counters == compare_storage_counters", "response": "Compares the contents of two stores."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprints the details of the analysis reports.", "response": "def _PrintAnalysisReportsDetails(self, storage_reader):\n    \"\"\"Prints the details of the analysis reports.\n\n    Args:\n      storage_reader (StorageReader): storage reader.\n    \"\"\"\n    if not storage_reader.HasAnalysisReports():\n      self._output_writer.Write('No analysis reports stored.\\n\\n')\n      return\n\n    for index, analysis_report in enumerate(\n        storage_reader.GetAnalysisReports()):\n      title = 'Analysis report: {0:d}'.format(index)\n      table_view = views.ViewsFactory.GetTableView(\n          self._views_format_type, title=title)\n\n      table_view.AddRow(['String', analysis_report.GetString()])\n\n      table_view.Write(self._output_writer)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _PrintWarningCounters(self, storage_counters):\n    warnings_by_pathspec = storage_counters.get('warnings_by_path_spec', {})\n    warnings_by_parser_chain = storage_counters.get(\n        'warnings_by_parser_chain', {})\n    if not warnings_by_parser_chain:\n      self._output_writer.Write('No warnings stored.\\n\\n')\n      return\n\n    table_view = views.ViewsFactory.GetTableView(\n        self._views_format_type, title='Warnings generated per parser',\n        column_names=['Parser (plugin) name', 'Number of warnings'])\n    for parser_chain, count in warnings_by_parser_chain.items():\n      parser_chain = parser_chain or '<No parser>'\n      table_view.AddRow([parser_chain, '{0:d}'.format(count)])\n    table_view.Write(self._output_writer)\n\n    table_view = views.ViewsFactory.GetTableView(\n        self._views_format_type, title='Pathspecs with most warnings',\n        column_names=['Number of warnings', 'Pathspec'])\n\n    top_pathspecs = warnings_by_pathspec.most_common(10)\n    for pathspec, count in top_pathspecs:\n      for path_index, line in enumerate(pathspec.split('\\n')):\n        if not line:\n          continue\n\n        if path_index == 0:\n          table_view.AddRow(['{0:d}'.format(count), line])\n        else:\n          table_view.AddRow(['', line])\n\n    table_view.Write(self._output_writer)", "response": "Prints a summary of the warnings."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprint the details of the warnings.", "response": "def _PrintWarningsDetails(self, storage):\n    \"\"\"Prints the details of the warnings.\n\n    Args:\n      storage (BaseStore): storage.\n    \"\"\"\n    if not storage.HasWarnings():\n      self._output_writer.Write('No warnings stored.\\n\\n')\n      return\n\n    for index, warning in enumerate(storage.GetWarnings()):\n      title = 'Warning: {0:d}'.format(index)\n      table_view = views.ViewsFactory.GetTableView(\n          self._views_format_type, title=title)\n\n      table_view.AddRow(['Message', warning.message])\n      table_view.AddRow(['Parser chain', warning.parser_chain])\n\n      path_specification = warning.path_spec.comparable\n      for path_index, line in enumerate(path_specification.split('\\n')):\n        if not line:\n          continue\n\n        if path_index == 0:\n          table_view.AddRow(['Path specification', line])\n        else:\n          table_view.AddRow(['', line])\n\n      table_view.Write(self._output_writer)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _PrintEventLabelsCounter(\n      self, event_labels_counter, session_identifier=None):\n    \"\"\"Prints the event labels counter.\n\n    Args:\n      event_labels_counter (collections.Counter): number of event tags per\n          label.\n      session_identifier (Optional[str]): session identifier.\n    \"\"\"\n    if not event_labels_counter:\n      return\n\n    title = 'Event tags generated per label'\n    if session_identifier:\n      title = '{0:s}: {1:s}'.format(title, session_identifier)\n\n    table_view = views.ViewsFactory.GetTableView(\n        self._views_format_type,\n        column_names=['Label', 'Number of event tags'], title=title)\n\n    for key, value in sorted(event_labels_counter.items()):\n      if key == 'total':\n        continue\n      table_view.AddRow([key, value])\n\n    try:\n      total = event_labels_counter['total']\n    except KeyError:\n      total = 'N/A'\n\n    table_view.AddRow(['Total', total])\n\n    table_view.Write(self._output_writer)", "response": "Prints the event labels counter."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprints the parsers counter.", "response": "def _PrintParsersCounter(self, parsers_counter, session_identifier=None):\n    \"\"\"Prints the parsers counter\n\n    Args:\n      parsers_counter (collections.Counter): number of events per parser or\n          parser plugin.\n      session_identifier (Optional[str]): session identifier.\n    \"\"\"\n    if not parsers_counter:\n      return\n\n    title = 'Events generated per parser'\n    if session_identifier:\n      title = '{0:s}: {1:s}'.format(title, session_identifier)\n\n    table_view = views.ViewsFactory.GetTableView(\n        self._views_format_type,\n        column_names=['Parser (plugin) name', 'Number of events'],\n        title=title)\n\n    for key, value in sorted(parsers_counter.items()):\n      if key == 'total':\n        continue\n      table_view.AddRow([key, value])\n\n    table_view.AddRow(['Total', parsers_counter['total']])\n\n    table_view.Write(self._output_writer)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprints the details of the preprocessing information.", "response": "def _PrintPreprocessingInformation(self, storage_reader, session_number=None):\n    \"\"\"Prints the details of the preprocessing information.\n\n    Args:\n      storage_reader (StorageReader): storage reader.\n      session_number (Optional[int]): session number.\n    \"\"\"\n    knowledge_base_object = knowledge_base.KnowledgeBase()\n\n    storage_reader.ReadPreprocessingInformation(knowledge_base_object)\n\n    # TODO: replace session_number by session_identifier.\n    system_configuration = knowledge_base_object.GetSystemConfigurationArtifact(\n        session_identifier=session_number)\n    if not system_configuration:\n      return\n\n    title = 'System configuration'\n    table_view = views.ViewsFactory.GetTableView(\n        self._views_format_type, title=title)\n\n    hostname = 'N/A'\n    if system_configuration.hostname:\n      hostname = system_configuration.hostname.name\n\n    operating_system = system_configuration.operating_system or 'N/A'\n    operating_system_product = (\n        system_configuration.operating_system_product or 'N/A')\n    operating_system_version = (\n        system_configuration.operating_system_version or 'N/A')\n    code_page = system_configuration.code_page or 'N/A'\n    keyboard_layout = system_configuration.keyboard_layout or 'N/A'\n    time_zone = system_configuration.time_zone or 'N/A'\n\n    table_view.AddRow(['Hostname', hostname])\n    table_view.AddRow(['Operating system', operating_system])\n    table_view.AddRow(['Operating system product', operating_system_product])\n    table_view.AddRow(['Operating system version', operating_system_version])\n    table_view.AddRow(['Code page', code_page])\n    table_view.AddRow(['Keyboard layout', keyboard_layout])\n    table_view.AddRow(['Time zone', time_zone])\n\n    table_view.Write(self._output_writer)\n\n    title = 'User accounts'\n    table_view = views.ViewsFactory.GetTableView(\n        self._views_format_type,\n        column_names=['Username', 'User directory'], title=title)\n\n    for user_account in system_configuration.user_accounts:\n      table_view.AddRow([\n          user_account.username, user_account.user_directory])\n\n    table_view.Write(self._output_writer)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprint the details of the sessions.", "response": "def _PrintSessionsDetails(self, storage_reader):\n    \"\"\"Prints the details of the sessions.\n\n    Args:\n      storage_reader (BaseStore): storage.\n    \"\"\"\n    for session_number, session in enumerate(storage_reader.GetSessions()):\n      session_identifier = uuid.UUID(hex=session.identifier)\n      session_identifier = '{0!s}'.format(session_identifier)\n\n      start_time = 'N/A'\n      if session.start_time is not None:\n        start_time = timelib.Timestamp.CopyToIsoFormat(session.start_time)\n\n      completion_time = 'N/A'\n      if session.completion_time is not None:\n        completion_time = timelib.Timestamp.CopyToIsoFormat(\n            session.completion_time)\n\n      enabled_parser_names = 'N/A'\n      if session.enabled_parser_names:\n        enabled_parser_names = ', '.join(sorted(session.enabled_parser_names))\n\n      command_line_arguments = session.command_line_arguments or 'N/A'\n      parser_filter_expression = session.parser_filter_expression or 'N/A'\n      preferred_encoding = session.preferred_encoding or 'N/A'\n      # Workaround for some older Plaso releases writing preferred encoding as\n      # bytes.\n      if isinstance(preferred_encoding, py2to3.BYTES_TYPE):\n        preferred_encoding = preferred_encoding.decode('utf-8')\n      if session.artifact_filters:\n        artifact_filters_string = ', '.join(session.artifact_filters)\n      else:\n        artifact_filters_string = 'N/A'\n      filter_file = session.filter_file or 'N/A'\n\n      title = 'Session: {0:s}'.format(session_identifier)\n      table_view = views.ViewsFactory.GetTableView(\n          self._views_format_type, title=title)\n\n      table_view.AddRow(['Start time', start_time])\n      table_view.AddRow(['Completion time', completion_time])\n      table_view.AddRow(['Product name', session.product_name])\n      table_view.AddRow(['Product version', session.product_version])\n      table_view.AddRow(['Command line arguments', command_line_arguments])\n      table_view.AddRow(['Parser filter expression', parser_filter_expression])\n      table_view.AddRow(['Enabled parser and plugins', enabled_parser_names])\n      table_view.AddRow(['Preferred encoding', preferred_encoding])\n      table_view.AddRow(['Debug mode', session.debug_mode])\n      table_view.AddRow(['Artifact filters', artifact_filters_string])\n      table_view.AddRow(['Filter file', filter_file])\n\n      table_view.Write(self._output_writer)\n\n      if self._verbose:\n        self._PrintPreprocessingInformation(storage_reader, session_number + 1)\n\n        self._PrintParsersCounter(\n            session.parsers_counter, session_identifier=session_identifier)\n\n        self._PrintAnalysisReportCounter(\n            session.analysis_reports_counter,\n            session_identifier=session_identifier)\n\n        self._PrintEventLabelsCounter(\n            session.event_labels_counter,\n            session_identifier=session_identifier)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprinting a sessions overview.", "response": "def _PrintSessionsOverview(self, storage_reader):\n    \"\"\"Prints a sessions overview.\n\n    Args:\n      storage_reader (StorageReader): storage reader.\n    \"\"\"\n    table_view = views.ViewsFactory.GetTableView(\n        self._views_format_type, title='Sessions')\n\n    for session in storage_reader.GetSessions():\n      start_time = timelib.Timestamp.CopyToIsoFormat(\n          session.start_time)\n      session_identifier = uuid.UUID(hex=session.identifier)\n      session_identifier = '{0!s}'.format(session_identifier)\n      table_view.AddRow([session_identifier, start_time])\n\n    table_view.Write(self._output_writer)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nprinting information about the store as human - readable text.", "response": "def _PrintStorageInformationAsText(self, storage_reader):\n    \"\"\"Prints information about the store as human-readable text.\n\n    Args:\n      storage_reader (StorageReader): storage reader.\n    \"\"\"\n    table_view = views.ViewsFactory.GetTableView(\n        self._views_format_type, title='Plaso Storage Information')\n    table_view.AddRow(['Filename', os.path.basename(self._storage_file_path)])\n    table_view.AddRow(['Format version', storage_reader.format_version])\n    table_view.AddRow(\n        ['Serialization format', storage_reader.serialization_format])\n    table_view.Write(self._output_writer)\n\n    if storage_reader.storage_type == definitions.STORAGE_TYPE_SESSION:\n      self._PrintSessionsOverview(storage_reader)\n      self._PrintSessionsDetails(storage_reader)\n\n      storage_counters = self._CalculateStorageCounters(storage_reader)\n\n      if 'parsers' not in storage_counters:\n        self._output_writer.Write(\n            'Unable to determine number of events generated per parser.\\n')\n      else:\n        self._PrintParsersCounter(storage_counters['parsers'])\n\n      if 'analysis_reports' not in storage_counters:\n        self._output_writer.Write(\n            'Unable to determine number of reports generated per plugin.\\n')\n      else:\n        self._PrintAnalysisReportCounter(storage_counters['analysis_reports'])\n\n      if 'event_labels' not in storage_counters:\n        self._output_writer.Write(\n            'Unable to determine number of event tags generated per label.\\n')\n      else:\n        self._PrintEventLabelsCounter(storage_counters['event_labels'])\n\n      self._PrintWarningCounters(storage_counters)\n\n      if self._verbose:\n        self._PrintWarningsDetails(storage_reader)\n\n      self._PrintAnalysisReportsDetails(storage_reader)\n\n    elif storage_reader.storage_type == definitions.STORAGE_TYPE_TASK:\n      self._PrintTasksInformation(storage_reader)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprinting a summary of sessions as machine - readable JSON.", "response": "def _PrintStorageInformationAsJSON(self, storage_reader):\n    \"\"\"Writes a summary of sessions as machine-readable JSON.\n\n    Args:\n      storage_reader (StorageReader): storage reader.\n    \"\"\"\n    serializer = json_serializer.JSONAttributeContainerSerializer\n    storage_counters = self._CalculateStorageCounters(storage_reader)\n    storage_counters_json = json.dumps(storage_counters)\n    self._output_writer.Write('{')\n    self._output_writer.Write('\"storage_counters\": {0:s}'.format(\n        storage_counters_json))\n    self._output_writer.Write(',\\n')\n    self._output_writer.Write(' \"sessions\": {')\n    for index, session in enumerate(storage_reader.GetSessions()):\n      json_string = serializer.WriteSerialized(session)\n      if index != 0:\n        self._output_writer.Write(',\\n')\n      self._output_writer.Write('\"session_{0:s}\": {1:s} '.format(\n          session.identifier, json_string))\n    self._output_writer.Write('}}')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _PrintTasksInformation(self, storage_reader):\n    table_view = views.ViewsFactory.GetTableView(\n        self._views_format_type, title='Tasks')\n\n    for task_start, _ in storage_reader.GetSessions():\n      start_time = timelib.Timestamp.CopyToIsoFormat(\n          task_start.timestamp)\n      task_identifier = uuid.UUID(hex=task_start.identifier)\n      task_identifier = '{0!s}'.format(task_identifier)\n      table_view.AddRow([task_identifier, start_time])\n\n    table_view.Write(self._output_writer)", "response": "Prints information about the tasks."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncompares the contents of two stores.", "response": "def CompareStores(self):\n    \"\"\"Compares the contents of two stores.\n\n    Returns:\n      bool: True if the content of the stores is identical.\n    \"\"\"\n    storage_reader = storage_factory.StorageFactory.CreateStorageReaderForFile(\n        self._storage_file_path)\n    if not storage_reader:\n      logger.error(\n          'Format of storage file: {0:s} not supported'.format(\n              self._storage_file_path))\n      return False\n\n    compare_storage_reader = (\n        storage_factory.StorageFactory.CreateStorageReaderForFile(\n            self._compare_storage_file_path))\n    if not compare_storage_reader:\n      logger.error(\n          'Format of storage file: {0:s} not supported'.format(\n              self._compare_storage_file_path))\n      return False\n\n    try:\n      result = self._CompareStores(storage_reader, compare_storage_reader)\n\n    finally:\n      compare_storage_reader.Close()\n      storage_reader.Close()\n\n    if result:\n      self._output_writer.Write('Storage files are identical.\\n')\n    else:\n      self._output_writer.Write('Storage files are different.\\n')\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ParseArguments(self):\n    loggers.ConfigureLogging()\n\n    argument_parser = argparse.ArgumentParser(\n        description=self.DESCRIPTION, add_help=False,\n        formatter_class=argparse.RawDescriptionHelpFormatter)\n\n    self.AddBasicOptions(argument_parser)\n\n    argument_helper_names = ['storage_file']\n    if self._CanEnforceProcessMemoryLimit():\n      argument_helper_names.append('process_resources')\n    helpers_manager.ArgumentHelperManager.AddCommandLineArguments(\n        argument_parser, names=argument_helper_names)\n\n    argument_parser.add_argument(\n        '--compare', dest='compare_storage_file', type=str,\n        action='store', default='', metavar='STORAGE_FILE', help=(\n            'The path of the storage file to compare against.'))\n\n    argument_parser.add_argument(\n        '--output_format', '--output-format', dest='output_format', type=str,\n        choices=['text', 'json'], action='store', default='text',\n        metavar='FORMAT', help=(\n            'Format of the output, the default is: text. Supported options: '\n            'json, text.'))\n\n    argument_parser.add_argument(\n        '-v', '--verbose', dest='verbose', action='store_true',\n        default=False, help='Print verbose output.')\n\n    argument_parser.add_argument(\n        '-w', '--write', metavar='OUTPUTFILE', dest='write',\n        help='Output filename.')\n\n    try:\n      options = argument_parser.parse_args()\n    except UnicodeEncodeError:\n      # If we get here we are attempting to print help in a non-Unicode\n      # terminal.\n      self._output_writer.Write('\\n')\n      self._output_writer.Write(argument_parser.format_help())\n      return False\n\n    try:\n      self.ParseOptions(options)\n    except errors.BadConfigOption as exception:\n      self._output_writer.Write('ERROR: {0!s}\\n'.format(exception))\n      self._output_writer.Write('\\n')\n      self._output_writer.Write(argument_parser.format_usage())\n      return False\n\n    loggers.ConfigureLogging(\n        debug_output=self._debug_mode, filename=self._log_file,\n        quiet_mode=self._quiet_mode)\n\n    return True", "response": "Parses the command line arguments."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse the command line options.", "response": "def ParseOptions(self, options):\n    \"\"\"Parses the options.\n\n    Args:\n      options (argparse.Namespace): command line arguments.\n\n    Raises:\n      BadConfigOption: if the options are invalid.\n    \"\"\"\n    self._ParseInformationalOptions(options)\n\n    self._verbose = getattr(options, 'verbose', False)\n\n    self._output_filename = getattr(options, 'write', None)\n\n    argument_helper_names = ['process_resources', 'storage_file']\n    helpers_manager.ArgumentHelperManager.ParseOptions(\n        options, self, names=argument_helper_names)\n\n    # TODO: move check into _CheckStorageFile.\n    if not self._storage_file_path:\n      raise errors.BadConfigOption('Missing storage file option.')\n\n    if not os.path.isfile(self._storage_file_path):\n      raise errors.BadConfigOption(\n          'No such storage file: {0:s}.'.format(self._storage_file_path))\n\n    compare_storage_file_path = self.ParseStringOption(\n        options, 'compare_storage_file')\n    if compare_storage_file_path:\n      if not os.path.isfile(compare_storage_file_path):\n        raise errors.BadConfigOption(\n            'No such storage file: {0:s}.'.format(compare_storage_file_path))\n\n      self._compare_storage_file_path = compare_storage_file_path\n      self.compare_storage_information = True\n\n    self._output_format = self.ParseStringOption(options, 'output_format')\n\n    if self._output_filename:\n      if os.path.exists(self._output_filename):\n        raise errors.BadConfigOption(\n            'Output file already exists: {0:s}.'.format(self._output_filename))\n      output_file_object = open(self._output_filename, 'wb')\n      self._output_writer = tools.FileObjectOutputWriter(output_file_object)\n\n    self._EnforceProcessMemoryLimit(self._process_memory_limit)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nprinting the storage information.", "response": "def PrintStorageInformation(self):\n    \"\"\"Prints the storage information.\"\"\"\n    storage_reader = storage_factory.StorageFactory.CreateStorageReaderForFile(\n        self._storage_file_path)\n    if not storage_reader:\n      logger.error(\n          'Format of storage file: {0:s} not supported'.format(\n              self._storage_file_path))\n      return\n\n    try:\n      if self._output_format == 'json':\n        self._PrintStorageInformationAsJSON(storage_reader)\n      elif self._output_format == 'text':\n        self._PrintStorageInformationAsText(storage_reader)\n    finally:\n      storage_reader.Close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _GetFileSystemTypeFromFileEntry(self, file_entry):\n    if file_entry.type_indicator != dfvfs_definitions.TYPE_INDICATOR_TSK:\n      return file_entry.type_indicator\n\n    # TODO: Implement fs_type in dfVFS and remove this implementation\n    # once that is in place.\n    file_system = file_entry.GetFileSystem()\n    fs_info = file_system.GetFsInfo()\n    if fs_info.info:\n      type_string = '{0!s}'.format(fs_info.info.ftype)\n      if type_string.startswith('TSK_FS_TYPE_'):\n        type_string = type_string[12:]\n      if type_string.endswith('_DETECT'):\n        type_string = type_string[:-7]\n\n    return type_string", "response": "Retrieves the file system type indicator of a file entry."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ParseFileEntry(self, parser_mediator, file_entry):\n    stat_object = file_entry.GetStat()\n    if not stat_object:\n      return\n\n    file_system_type = self._GetFileSystemTypeFromFileEntry(file_entry)\n\n    event_data = FileStatEventData()\n    event_data.file_entry_type = stat_object.type\n    event_data.file_size = getattr(stat_object, 'size', None)\n    event_data.file_system_type = file_system_type\n    event_data.is_allocated = file_entry.IsAllocated()\n\n    if file_entry.access_time:\n      event = time_events.DateTimeValuesEvent(\n          file_entry.access_time, definitions.TIME_DESCRIPTION_LAST_ACCESS)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    if file_entry.creation_time:\n      event = time_events.DateTimeValuesEvent(\n          file_entry.creation_time, definitions.TIME_DESCRIPTION_CREATION)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    if file_entry.change_time:\n      event = time_events.DateTimeValuesEvent(\n          file_entry.change_time, definitions.TIME_DESCRIPTION_CHANGE)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    if file_entry.modification_time:\n      event = time_events.DateTimeValuesEvent(\n          file_entry.modification_time,\n          definitions.TIME_DESCRIPTION_MODIFICATION)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    for time_attribute, usage in self._TIMESTAMP_DESCRIPTIONS.items():\n      posix_time = getattr(stat_object, time_attribute, None)\n      if posix_time is None:\n        continue\n\n      nano_time_attribute = '{0:s}_nano'.format(time_attribute)\n      nano_time_attribute = getattr(stat_object, nano_time_attribute, None)\n\n      timestamp = posix_time * 1000000\n      if nano_time_attribute is not None:\n        # Note that the _nano values are in intervals of 100th nano seconds.\n        micro_time_attribute, _ = divmod(nano_time_attribute, 10)\n        timestamp += micro_time_attribute\n\n      # TSK will return 0 if the timestamp is not set.\n      if (file_entry.type_indicator == dfvfs_definitions.TYPE_INDICATOR_TSK and\n          not timestamp):\n        continue\n\n      date_time = dfdatetime_posix_time.PosixTimeInMicroseconds(\n          timestamp=timestamp)\n      event = time_events.DateTimeValuesEvent(date_time, usage)\n      parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Parses a file entry and returns a new object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _FormatArgToken(self, token_data):\n    return {\n        'string': token_data.argument_value.rstrip('\\x00'),\n        'num_arg': token_data.argument_index,\n        'is': token_data.argument_name}", "response": "Formats an argument token as a dictionary of values."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nformats an attribute token as a dictionary of values.", "response": "def _FormatAttrToken(self, token_data):\n    \"\"\"Formats an attribute token as a dictionary of values.\n\n    Args:\n      token_data (bsm_token_data_attr32|bsm_token_data_attr64): AUT_ATTR32 or\n          AUT_ATTR64 token data.\n\n    Returns:\n      dict[str, str]: token values.\n    \"\"\"\n    return {\n        'mode': token_data.file_mode,\n        'uid': token_data.user_identifier,\n        'gid': token_data.group_identifier,\n        'system_id': token_data.file_system_identifier,\n        'node_id': token_data.file_identifier,\n        'device': token_data.device}"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nformat a data token as a dictionary of values.", "response": "def _FormatDataToken(self, token_data):\n    \"\"\"Formats a data token as a dictionary of values.\n\n    Args:\n      token_data (bsm_token_data_data): AUT_DATA token data.\n\n    Returns:\n      dict[str, str]: token values.\n    \"\"\"\n    format_string = bsmtoken.BSM_TOKEN_DATA_PRINT.get(\n        token_data.data_format, 'UNKNOWN')\n\n    if token_data.data_format == 4:\n      data = bytes(bytearray(token_data.data)).split(b'\\x00')[0]\n      data = data.decode('utf-8')\n    else:\n      data = ''.join(['{0:02x}'.format(byte) for byte in token_data.data])\n    return {\n        'format': format_string,\n        'data': data}"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nformats an extended IPv4 address token as a dictionary of values.", "response": "def _FormatInAddrExToken(self, token_data):\n    \"\"\"Formats an extended IPv4 address token as a dictionary of values.\n\n    Args:\n      token_data (bsm_token_data_in_addr_ex): AUT_IN_ADDR_EX token data.\n\n    Returns:\n      dict[str, str]: token values.\n    \"\"\"\n    protocol = bsmtoken.BSM_PROTOCOLS.get(token_data.net_type, 'UNKNOWN')\n    if token_data.net_type == 4:\n      ip_address = self._FormatPackedIPv6Address(token_data.ip_address[:4])\n    elif token_data.net_type == 16:\n      ip_address = self._FormatPackedIPv6Address(token_data.ip_address)\n    return {\n        'protocols': protocol,\n        'net_type': token_data.net_type,\n        'address': ip_address}"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nformat an IPC permissions token as a dictionary of values.", "response": "def _FormatIPCPermToken(self, token_data):\n    \"\"\"Formats an IPC permissions token as a dictionary of values.\n\n    Args:\n      token_data (bsm_token_data_ipc_perm): AUT_IPC_PERM token data.\n\n    Returns:\n      dict[str, str]: token values.\n    \"\"\"\n    return {\n        'user_id': token_data.user_identifier,\n        'group_id': token_data.group_identifier,\n        'creator_user_id': token_data.creator_user_identifier,\n        'creator_group_id': token_data.creator_group_identifier,\n        'access': token_data.access_mode}"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _FormatIPToken(self, token_data):\n    data = ''.join(['{0:02x}'.format(byte) for byte in token_data.data])\n    return {'IPv4_Header': data}", "response": "Formats an IPv4 packet header token as a dictionary of values."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _FormatOpaqueToken(self, token_data):\n    data = ''.join(['{0:02x}'.format(byte) for byte in token_data.data])\n    return {'data': data}", "response": "Formats an opaque token as a dictionary of values."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _FormatOtherFileToken(self, token_data):\n    # TODO: if this timestamp is useful, it must be extracted as a separate\n    # event object.\n    timestamp = token_data.microseconds + (\n        token_data.timestamp * definitions.MICROSECONDS_PER_SECOND)\n    date_time = dfdatetime_posix_time.PosixTimeInMicroseconds(\n        timestamp=timestamp)\n    date_time_string = date_time.CopyToDateTimeString()\n\n    return {\n        'string': token_data.name.rstrip('\\x00'),\n        'timestamp': date_time_string}", "response": "Formats an other file token as a dictionary of values."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _FormatReturnOrExitToken(self, token_data):\n    error_string = bsmtoken.BSM_ERRORS.get(token_data.status, 'UNKNOWN')\n    return {\n        'error': error_string,\n        'token_status': token_data.status,\n        'call_status': token_data.return_value}", "response": "Formats a return or exit token as a dictionary of values."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nformatting an extended socket token as a dictionary of values.", "response": "def _FormatSocketExToken(self, token_data):\n    \"\"\"Formats an extended socket token as a dictionary of values.\n\n    Args:\n      token_data (bsm_token_data_socket_ex): AUT_SOCKET_EX token data.\n\n    Returns:\n      dict[str, str]: token values.\n    \"\"\"\n    if token_data.socket_domain == 10:\n      local_ip_address = self._FormatPackedIPv6Address(\n          token_data.local_ip_address)\n      remote_ip_address = self._FormatPackedIPv6Address(\n          token_data.remote_ip_address)\n    else:\n      local_ip_address = self._FormatPackedIPv4Address(\n          token_data.local_ip_address)\n      remote_ip_address = self._FormatPackedIPv4Address(\n          token_data.remote_ip_address)\n\n    return {\n        'from': local_ip_address,\n        'from_port': token_data.local_port,\n        'to': remote_ip_address,\n        'to_port': token_data.remote_port}"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _FormatSocketInet32Token(self, token_data):\n    protocol = bsmtoken.BSM_PROTOCOLS.get(token_data.socket_family, 'UNKNOWN')\n    ip_address = self._FormatPackedIPv4Address(token_data.ip_addresss)\n    return {\n        'protocols': protocol,\n        'family': token_data.socket_family,\n        'port': token_data.port_number,\n        'address': ip_address}", "response": "Formats an Internet socket token as a dictionary of values."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nformat an Internet socket token as a dictionary of values.", "response": "def _FormatSocketInet128Token(self, token_data):\n    \"\"\"Formats an Internet socket token as a dictionary of values.\n\n    Args:\n      token_data (bsm_token_data_sockinet64): AUT_SOCKINET128 token data.\n\n    Returns:\n      dict[str, str]: token values.\n    \"\"\"\n    protocol = bsmtoken.BSM_PROTOCOLS.get(token_data.socket_family, 'UNKNOWN')\n    ip_address = self._FormatPackedIPv6Address(token_data.ip_addresss)\n    return {\n        'protocols': protocol,\n        'family': token_data.socket_family,\n        'port': token_data.port_number,\n        'address': ip_address}"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nformats an Unix socket token as a dictionary of values.", "response": "def _FormatSocketUnixToken(self, token_data):\n    \"\"\"Formats an Unix socket token as a dictionary of values.\n\n    Args:\n      token_data (bsm_token_data_sockunix): AUT_SOCKUNIX token data.\n\n    Returns:\n      dict[str, str]: token values.\n    \"\"\"\n    protocol = bsmtoken.BSM_PROTOCOLS.get(token_data.socket_family, 'UNKNOWN')\n    return {\n        'protocols': protocol,\n        'family': token_data.socket_family,\n        'path': token_data.socket_path}"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nformatting a subject or process token as a dictionary of values.", "response": "def _FormatSubjectOrProcessToken(self, token_data):\n    \"\"\"Formats a subject or process token as a dictionary of values.\n\n    Args:\n      token_data (bsm_token_data_subject32|bsm_token_data_subject64):\n          AUT_SUBJECT32, AUT_PROCESS32, AUT_SUBJECT64 or AUT_PROCESS64 token\n          data.\n\n    Returns:\n      dict[str, str]: token values.\n    \"\"\"\n    ip_address = self._FormatPackedIPv4Address(token_data.ip_address)\n    return {\n        'aid': token_data.audit_user_identifier,\n        'euid': token_data.effective_user_identifier,\n        'egid': token_data.effective_group_identifier,\n        'uid': token_data.real_user_identifier,\n        'gid': token_data.real_group_identifier,\n        'pid': token_data.process_identifier,\n        'session_id': token_data.session_identifier,\n        'terminal_port': token_data.terminal_port,\n        'terminal_ip': ip_address}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nformat a subject or process token as a dictionary of values.", "response": "def _FormatSubjectExOrProcessExToken(self, token_data):\n    \"\"\"Formats a subject or process token as a dictionary of values.\n\n    Args:\n      token_data (bsm_token_data_subject32_ex|bsm_token_data_subject64_ex):\n          AUT_SUBJECT32_EX, AUT_PROCESS32_EX, AUT_SUBJECT64_EX or\n          AUT_PROCESS64_EX token data.\n\n    Returns:\n      dict[str, str]: token values.\n    \"\"\"\n    if token_data.net_type == 4:\n      ip_address = self._FormatPackedIPv4Address(token_data.ip_address)\n    elif token_data.net_type == 16:\n      ip_address = self._FormatPackedIPv6Address(token_data.ip_address)\n    else:\n      ip_address = 'unknown'\n\n    return {\n        'aid': token_data.audit_user_identifier,\n        'euid': token_data.effective_user_identifier,\n        'egid': token_data.effective_group_identifier,\n        'uid': token_data.real_user_identifier,\n        'gid': token_data.real_group_identifier,\n        'pid': token_data.process_identifier,\n        'session_id': token_data.session_identifier,\n        'terminal_port': token_data.terminal_port,\n        'terminal_ip': ip_address}"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nformat the token data as a dictionary of values.", "response": "def _FormatTokenData(self, token_type, token_data):\n    \"\"\"Formats the token data as a dictionary of values.\n\n    Args:\n      token_type (int): token type.\n      token_data (object): token data.\n\n    Returns:\n      dict[str, str]: formatted token values or an empty dictionary if no\n          formatted token values could be determined.\n    \"\"\"\n    token_data_format_function = self._TOKEN_DATA_FORMAT_FUNCTIONS.get(\n        token_type)\n    if token_data_format_function:\n      token_data_format_function = getattr(\n          self, token_data_format_function, None)\n\n    if not token_data_format_function:\n      return {}\n\n    return token_data_format_function(token_data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse an event record.", "response": "def _ParseRecord(self, parser_mediator, file_object):\n    \"\"\"Parses an event record.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      file_object (dfvfs.FileIO): file-like object.\n\n    Raises:\n      ParseError: if the event record cannot be read.\n    \"\"\"\n    header_record_offset = file_object.tell()\n\n    # Check the header token type before reading the token data to prevent\n    # variable size tokens to consume a large amount of memory.\n    token_type = self._ParseTokenType(file_object, header_record_offset)\n    if token_type not in self._HEADER_TOKEN_TYPES:\n      raise errors.ParseError(\n          'Unsupported header token type: 0x{0:02x}'.format(token_type))\n\n    token_type, token_data = self._ParseToken(file_object, header_record_offset)\n\n    if token_data.format_version != 11:\n      raise errors.ParseError('Unsupported format version type: {0:d}'.format(\n          token_data.format_version))\n\n    timestamp = token_data.microseconds + (\n        token_data.timestamp * definitions.MICROSECONDS_PER_SECOND)\n\n    event_type = token_data.event_type\n    header_record_size = token_data.record_size\n    record_end_offset = header_record_offset + header_record_size\n\n    event_tokens = []\n    return_token_values = None\n\n    file_offset = file_object.tell()\n    while file_offset < record_end_offset:\n      token_type, token_data = self._ParseToken(file_object, file_offset)\n      if not token_data:\n        raise errors.ParseError('Unsupported token type: 0x{0:02x}'.format(\n            token_type))\n\n      file_offset = file_object.tell()\n\n      if token_type == self._TOKEN_TYPE_AUT_TRAILER:\n        break\n\n      token_type_string = self._TOKEN_TYPES.get(token_type, 'UNKNOWN')\n      token_values = self._FormatTokenData(token_type, token_data)\n      event_tokens.append({token_type_string: token_values})\n\n      if token_type in (\n          self._TOKEN_TYPE_AUT_RETURN32, self._TOKEN_TYPE_AUT_RETURN64):\n        return_token_values = token_values\n\n    if token_data.signature != self._TRAILER_TOKEN_SIGNATURE:\n      raise errors.ParseError('Unsupported signature in trailer token.')\n\n    if token_data.record_size != header_record_size:\n      raise errors.ParseError(\n          'Mismatch of event record size between header and trailer token.')\n\n    event_data = BSMEventData()\n    event_data.event_type = event_type\n    event_data.extra_tokens = event_tokens\n    event_data.offset = header_record_offset\n    event_data.record_length = header_record_size\n    event_data.return_value = return_token_values\n\n    date_time = dfdatetime_posix_time.PosixTimeInMicroseconds(\n        timestamp=timestamp)\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_CREATION)\n    parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse a token. Args: file_object (dfvfs.FileIO): file-like object. file_offset (int): offset of the token relative to the start of the file-like object. Returns: tuple: containing: int: token type object: token data or None if the token type is not supported.", "response": "def _ParseToken(self, file_object, file_offset):\n    \"\"\"Parses a token.\n\n    Args:\n      file_object (dfvfs.FileIO): file-like object.\n      file_offset (int): offset of the token relative to the start of\n          the file-like object.\n\n    Returns:\n      tuple: containing:\n        int: token type\n        object: token data or None if the token type is not supported.\n    \"\"\"\n    token_type = self._ParseTokenType(file_object, file_offset)\n    token_data = None\n\n    token_data_map_name = self._DATA_TYPE_MAP_PER_TOKEN_TYPE.get(\n        token_type, None)\n    if token_data_map_name:\n      token_data_map = self._GetDataTypeMap(token_data_map_name)\n\n      token_data, _ = self._ReadStructureFromFileObject(\n          file_object, file_offset + 1, token_data_map)\n\n    return token_type, token_data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse a token type from a file - like object.", "response": "def _ParseTokenType(self, file_object, file_offset):\n    \"\"\"Parses a token type.\n\n    Args:\n      file_object (dfvfs.FileIO): file-like object.\n      file_offset (int): offset of the token relative to the start of\n          the file-like object.\n\n    Returns:\n      int: token type\n    \"\"\"\n    token_type_map = self._GetDataTypeMap('uint8')\n\n    token_type, _ = self._ReadStructureFromFileObject(\n        file_object, file_offset, token_type_map)\n\n    return token_type"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse a BSM file - like object.", "response": "def ParseFileObject(self, parser_mediator, file_object):\n    \"\"\"Parses a BSM file-like object.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      file_object (dfvfs.FileIO): a file-like object.\n\n    Raises:\n      UnableToParseFile: when the file cannot be parsed.\n    \"\"\"\n    file_offset = file_object.get_offset()\n    file_size = file_object.get_size()\n    while file_offset < file_size:\n      try:\n        self._ParseRecord(parser_mediator, file_object)\n      except errors.ParseError as exception:\n        if file_offset == 0:\n          raise errors.UnableToParseFile(\n              'Unable to parse first event record with error: {0!s}'.format(\n                  exception))\n\n        # TODO: skip to next event record.\n\n      file_offset = file_object.get_offset()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _GetTimeElementsTuple(self, timestamp):\n    year, month, day_of_month, hours, minutes, seconds = (\n        int(hexdigit[0] + hexdigit[1], 16) for hexdigit in zip(\n            timestamp[::2], timestamp[1::2]))\n\n    return (year + 1970, month + 1, day_of_month, hours, minutes, seconds)", "response": "Retrieves a time elements tuple from the given timestamp."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse a single row of the log file and produces events.", "response": "def ParseRow(self, parser_mediator, row_offset, row):\n    \"\"\"Parses a line of the log file and produces events.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      row_offset (int): line number of the row.\n      row (dict[str, str]): fields of a single row, as specified in COLUMNS.\n    \"\"\"\n    time_elements_tuple = self._GetTimeElementsTuple(row['time'])\n\n    try:\n      date_time = dfdatetime_time_elements.TimeElements(\n          time_elements_tuple=time_elements_tuple)\n      date_time.is_local_time = True\n    except ValueError:\n      parser_mediator.ProduceExtractionWarning(\n          'invalid date time value: {0!s}'.format(time_elements_tuple))\n      return\n\n    # TODO: remove unused attributes.\n    event_data = SymantecEventData()\n    event_data.access = row.get('access', None)\n    event_data.action0 = row.get('action0', None)\n    event_data.action1 = row.get('action1', None)\n    event_data.action1_status = row.get('action1_status', None)\n    event_data.action2 = row.get('action2', None)\n    event_data.action2_status = row.get('action2_status', None)\n    event_data.address = row.get('address', None)\n    event_data.backup_id = row.get('backup_id', None)\n    event_data.cat = row.get('cat', None)\n    event_data.cleaninfo = row.get('cleaninfo', None)\n    event_data.clientgroup = row.get('clientgroup', None)\n    event_data.compressed = row.get('compressed', None)\n    event_data.computer = row.get('computer', None)\n    event_data.definfo = row.get('definfo', None)\n    event_data.defseqnumber = row.get('defseqnumber', None)\n    event_data.deleteinfo = row.get('deleteinfo', None)\n    event_data.depth = row.get('depth', None)\n    event_data.description = row.get('description', None)\n    event_data.domain_guid = row.get('domain_guid', None)\n    event_data.domainname = row.get('domainname', None)\n    event_data.err_code = row.get('err_code', None)\n    event_data.event_data = row.get('event_data', None)\n    event_data.event = row.get('event', None)\n    event_data.extra = row.get('extra', None)\n    event_data.file = row.get('file', None)\n    event_data.flags = row.get('flags', None)\n    event_data.groupid = row.get('groupid', None)\n    event_data.guid = row.get('guid', None)\n    event_data.license_expiration_dt = row.get('license_expiration_dt', None)\n    event_data.license_feature_name = row.get('license_feature_name', None)\n    event_data.license_feature_ver = row.get('license_feature_ver', None)\n    event_data.license_fulfillment_id = row.get('license_fulfillment_id', None)\n    event_data.license_lifecycle = row.get('license_lifecycle', None)\n    event_data.license_seats_delta = row.get('license_seats_delta', None)\n    event_data.license_seats = row.get('license_seats', None)\n    event_data.license_seats_total = row.get('license_seats_total', None)\n    event_data.license_serial_num = row.get('license_serial_num', None)\n    event_data.license_start_dt = row.get('license_start_dt', None)\n    event_data.logger = row.get('logger', None)\n    event_data.login_domain = row.get('login_domain', None)\n    event_data.log_session_guid = row.get('log_session_guid', None)\n    event_data.macaddr = row.get('macaddr', None)\n    event_data.new_ext = row.get('new_ext', None)\n    event_data.ntdomain = row.get('ntdomain', None)\n    event_data.offset = row_offset\n    event_data.parent = row.get('parent', None)\n    event_data.quarfwd_status = row.get('quarfwd_status', None)\n    event_data.remote_machine_ip = row.get('remote_machine_ip', None)\n    event_data.remote_machine = row.get('remote_machine', None)\n    event_data.scanid = row.get('scanid', None)\n    event_data.snd_status = row.get('snd_status', None)\n    event_data.status = row.get('status', None)\n    event_data.still_infected = row.get('still_infected', None)\n    event_data.time = row.get('time', None)\n    event_data.user = row.get('user', None)\n    event_data.vbin_id = row.get('vbin_id', None)\n    event_data.vbin_session_id = row.get('vbin_session_id', None)\n    event_data.version = row.get('version:', None)\n    event_data.virus_id = row.get('virus_id', None)\n    event_data.virus = row.get('virus', None)\n    event_data.virustype = row.get('virustype', None)\n\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_WRITTEN)\n    parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nverifying if a line of the file is in the expected format.", "response": "def VerifyRow(self, parser_mediator, row):\n    \"\"\"Verifies if a line of the file is in the expected format.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      row (dict[str, str]): fields of a single row, as specified in COLUMNS.\n\n    Returns:\n      bool: True if this is the correct parser, False otherwise.\n    \"\"\"\n    try:\n      time_elements_tuple = self._GetTimeElementsTuple(row['time'])\n    except (TypeError, ValueError):\n      return False\n\n    try:\n      dfdatetime_time_elements.TimeElements(\n          time_elements_tuple=time_elements_tuple)\n    except ValueError:\n      return False\n\n    try:\n      my_event = int(row['event'], 10)\n    except (TypeError, ValueError):\n      return False\n\n    if my_event < 1 or my_event > 77:\n      return False\n\n    try:\n      category = int(row['cat'], 10)\n    except (TypeError, ValueError):\n      return False\n\n    if category < 1 or category > 4:\n      return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef GetEntries(self, parser_mediator, match=None, **unused_kwargs):\n    stores = match.get('Stores', {})\n    for volume_name, volume in iter(stores.items()):\n      datetime_value = volume.get('CreationDate', None)\n      if not datetime_value:\n        continue\n\n      partial_path = volume['PartialPath']\n\n      event_data = plist_event.PlistTimeEventData()\n      event_data.desc = 'Spotlight Volume {0:s} ({1:s}) activated.'.format(\n          volume_name, partial_path)\n      event_data.key = ''\n      event_data.root = '/Stores'\n\n      event = time_events.PythonDatetimeEvent(\n          datetime_value, definitions.TIME_DESCRIPTION_WRITTEN)\n      parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Extracts relevant Volume Configuration Spotlight entries."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ExtractEvents(self, parser_mediator, registry_key, **kwargs):\n    version_value = registry_key.GetValueByName('Version')\n    count_subkey = registry_key.GetSubkeyByName('Count')\n\n    if not version_value:\n      parser_mediator.ProduceExtractionWarning('missing version value')\n      return\n\n    if not version_value.DataIsInteger():\n      parser_mediator.ProduceExtractionWarning(\n          'unsupported version value data type')\n      return\n\n    format_version = version_value.GetDataAsObject()\n    if format_version not in (3, 5):\n      parser_mediator.ProduceExtractionWarning(\n          'unsupported format version: {0:d}'.format(format_version))\n      return\n\n    if not count_subkey:\n      parser_mediator.ProduceExtractionWarning('missing count subkey')\n      return\n\n    userassist_entry_index = 0\n\n    for registry_value in count_subkey.GetValues():\n      try:\n        # Note that Python 2 codecs.decode() does not support keyword arguments\n        # such as encodings='rot-13'.\n        value_name = codecs.decode(registry_value.name, 'rot-13')\n      except UnicodeEncodeError as exception:\n        logger.debug((\n            'Unable to decode UserAssist string: {0:s} with error: {1!s}.\\n'\n            'Attempting piecewise decoding.').format(\n                registry_value.name, exception))\n\n        characters = []\n        for char in registry_value.name:\n          if ord(char) < 128:\n            try:\n              characters.append(char.decode('rot-13'))\n            except UnicodeEncodeError:\n              characters.append(char)\n          else:\n            characters.append(char)\n\n        value_name = ''.join(characters)\n\n      if format_version == 5:\n        path_segments = value_name.split('\\\\')\n\n        for segment_index, path_segment in enumerate(path_segments):\n          # Remove the { } from the path segment to get the GUID.\n          guid = path_segments[segment_index][1:-1]\n          path_segments[segment_index] = known_folder_ids.PATHS.get(\n              guid, path_segment)\n\n        value_name = '\\\\'.join(path_segments)\n        # Check if we might need to substitute values.\n        if '%' in value_name:\n          # TODO: fix missing self._knowledge_base\n          # pylint: disable=no-member\n          environment_variables = self._knowledge_base.GetEnvironmentVariables()\n          value_name = path_helper.PathHelper.ExpandWindowsPath(\n              value_name, environment_variables)\n\n      if value_name == 'UEME_CTLSESSION':\n        continue\n\n      if format_version == 3:\n        entry_map = self._GetDataTypeMap('user_assist_entry_v3')\n      elif format_version == 5:\n        entry_map = self._GetDataTypeMap('user_assist_entry_v5')\n      else:\n        parser_mediator.ProduceExtractionWarning(\n            'unsupported format version: {0:d}'.format(format_version))\n        continue\n\n      if not registry_value.DataIsBinaryData():\n        parser_mediator.ProduceExtractionWarning(\n            'unsupported value data type: {0:s}'.format(\n                registry_value.data_type_string))\n        continue\n\n      entry_data_size = entry_map.GetByteSize()\n      value_data_size = len(registry_value.data)\n      if entry_data_size != value_data_size:\n        parser_mediator.ProduceExtractionWarning(\n            'unsupported value data size: {0:d}'.format(value_data_size))\n        continue\n\n      try:\n        user_assist_entry = self._ReadStructureFromByteStream(\n            registry_value.data, 0, entry_map)\n      except (ValueError, errors.ParseError) as exception:\n        parser_mediator.ProduceExtractionWarning(\n            'unable to parse UserAssist entry value with error: {0!s}'.format(\n                exception))\n        continue\n\n      event_data = UserAssistWindowsRegistryEventData()\n      event_data.key_path = count_subkey.path\n      event_data.number_of_executions = user_assist_entry.number_of_executions\n      event_data.value_name = value_name\n\n      if format_version == 3:\n        if event_data.number_of_executions > 5:\n          event_data.number_of_executions -= 5\n\n      elif format_version == 5:\n        userassist_entry_index += 1\n\n        event_data.application_focus_count = (\n            user_assist_entry.application_focus_count)\n        event_data.application_focus_duration = (\n            user_assist_entry.application_focus_duration)\n        event_data.entry_index = userassist_entry_index\n\n      timestamp = user_assist_entry.last_execution_time\n      if not timestamp:\n        date_time = dfdatetime_semantic_time.SemanticTime('Not set')\n      else:\n        date_time = dfdatetime_filetime.Filetime(timestamp=timestamp)\n\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_LAST_RUN)\n      parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Extracts events from a Windows Registry key."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nextracting data from the resume folder files.", "response": "def GetEntries(self, parser_mediator, data=None, **unused_kwargs):\n    \"\"\"Extract data from Transmission's resume folder files.\n\n    This is the main parsing engine for the parser. It determines if\n    the selected file is the proper file to parse and extracts current\n    running torrents.\n\n    Transmission stores an individual Bencoded file for each active download\n    in a folder named resume under the user's application data folder.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      data (Optional[dict[str, object]]): bencode data values.\n    \"\"\"\n    seeding_time = data.get('seeding-time-seconds', None)\n\n    event_data = TransmissionEventData()\n    event_data.destination = data.get('destination', None)\n    # Convert seconds to minutes.\n    event_data.seedtime, _ = divmod(seeding_time, 60)\n\n    # Create timeline events based on extracted values.\n    timestamp = data.get('added-date', None)\n    if timestamp:\n      date_time = dfdatetime_posix_time.PosixTime(timestamp=timestamp)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_ADDED)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    timestamp = data.get('done-date', None)\n    if timestamp:\n      date_time = dfdatetime_posix_time.PosixTime(timestamp=timestamp)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_FILE_DOWNLOADED)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    timestamp = data.get('activity-date', None)\n    if timestamp:\n      date_time = dfdatetime_posix_time.PosixTime(timestamp=timestamp)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_LAST_ACCESS)\n      parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a processing configuration.", "response": "def _CreateProcessingConfiguration(self, knowledge_base):\n    \"\"\"Creates a processing configuration.\n\n    Args:\n      knowledge_base (KnowledgeBase): contains information from the source\n          data needed for parsing.\n\n    Returns:\n      ProcessingConfiguration: processing configuration.\n\n    Raises:\n      BadConfigOption: if more than 1 parser and parser plugins preset\n          was found for the detected operating system.\n    \"\"\"\n    # TODO: pass preferred_encoding.\n    configuration = configurations.ProcessingConfiguration()\n    configuration.artifact_filters = self._artifact_filters\n    configuration.credentials = self._credential_configurations\n    configuration.debug_output = self._debug_mode\n    configuration.event_extraction.text_prepend = self._text_prepend\n    configuration.extraction.hasher_file_size_limit = (\n        self._hasher_file_size_limit)\n    configuration.extraction.hasher_names_string = self._hasher_names_string\n    configuration.extraction.process_archives = self._process_archives\n    configuration.extraction.process_compressed_streams = (\n        self._process_compressed_streams)\n    configuration.extraction.yara_rules_string = self._yara_rules_string\n    configuration.filter_file = self._filter_file\n    configuration.input_source.mount_path = self._mount_path\n    configuration.log_filename = self._log_file\n    configuration.parser_filter_expression = self._parser_filter_expression\n    configuration.preferred_year = self._preferred_year\n    configuration.profiling.directory = self._profiling_directory\n    configuration.profiling.sample_rate = self._profiling_sample_rate\n    configuration.profiling.profilers = self._profilers\n    configuration.temporary_directory = self._temporary_directory\n\n    if not configuration.parser_filter_expression:\n      operating_system = knowledge_base.GetValue('operating_system')\n      operating_system_product = knowledge_base.GetValue(\n          'operating_system_product')\n      operating_system_version = knowledge_base.GetValue(\n          'operating_system_version')\n      preset_definitions = (\n          parsers_manager.ParsersManager.GetPresetsForOperatingSystem(\n              operating_system, operating_system_product,\n              operating_system_version))\n\n      if preset_definitions:\n        preset_names = [\n            preset_definition.name for preset_definition in preset_definitions]\n        filter_expression = ','.join(preset_names)\n\n        logger.info('Parser filter expression set to: {0:s}'.format(\n            filter_expression))\n        configuration.parser_filter_expression = filter_expression\n\n    return configuration"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing the performance options.", "response": "def _ParsePerformanceOptions(self, options):\n    \"\"\"Parses the performance options.\n\n    Args:\n      options (argparse.Namespace): command line arguments.\n\n    Raises:\n      BadConfigOption: if the options are invalid.\n    \"\"\"\n    self._buffer_size = getattr(options, 'buffer_size', 0)\n    if self._buffer_size:\n      # TODO: turn this into a generic function that supports more size\n      # suffixes both MB and MiB and also that does not allow m as a valid\n      # indicator for MiB since m represents milli not Mega.\n      try:\n        if self._buffer_size[-1].lower() == 'm':\n          self._buffer_size = int(self._buffer_size[:-1], 10)\n          self._buffer_size *= self._BYTES_IN_A_MIB\n        else:\n          self._buffer_size = int(self._buffer_size, 10)\n      except ValueError:\n        raise errors.BadConfigOption(\n            'Invalid buffer size: {0!s}.'.format(self._buffer_size))\n\n    self._queue_size = self.ParseNumericOption(options, 'queue_size')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse the processing options.", "response": "def _ParseProcessingOptions(self, options):\n    \"\"\"Parses the processing options.\n\n    Args:\n      options (argparse.Namespace): command line arguments.\n\n    Raises:\n      BadConfigOption: if the options are invalid.\n    \"\"\"\n    self._single_process_mode = getattr(options, 'single_process', False)\n\n    argument_helper_names = [\n        'process_resources', 'temporary_directory', 'workers', 'zeromq']\n    helpers_manager.ArgumentHelperManager.ParseOptions(\n        options, self, names=argument_helper_names)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _ReadParserPresetsFromFile(self):\n    self._presets_file = os.path.join(\n        self._data_location, self._PRESETS_FILE_NAME)\n    if not os.path.isfile(self._presets_file):\n      raise errors.BadConfigOption(\n          'No such parser presets file: {0:s}.'.format(self._presets_file))\n\n    try:\n      parsers_manager.ParsersManager.ReadPresetsFromFile(self._presets_file)\n    except errors.MalformedPresetError as exception:\n      raise errors.BadConfigOption(\n          'Unable to read presets from file with error: {0!s}'.format(\n              exception))", "response": "Reads the parser presets from the presets. yaml file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _SetExtractionParsersAndPlugins(self, configuration, session):\n    names_generator = parsers_manager.ParsersManager.GetParserAndPluginNames(\n        parser_filter_expression=configuration.parser_filter_expression)\n\n    session.enabled_parser_names = list(names_generator)\n    session.parser_filter_expression = configuration.parser_filter_expression", "response": "Sets the parsers and plugins before extraction."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _SetExtractionPreferredTimeZone(self, knowledge_base):\n    # Note session.preferred_time_zone will default to UTC but\n    # self._preferred_time_zone is None when not set.\n    if self._preferred_time_zone:\n      try:\n        knowledge_base.SetTimeZone(self._preferred_time_zone)\n      except ValueError:\n        # pylint: disable=protected-access\n        logger.warning(\n            'Unsupported time zone: {0:s}, defaulting to {1:s}'.format(\n                self._preferred_time_zone, knowledge_base._time_zone.zone))", "response": "Sets the preferred time zone before extraction."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef AddPerformanceOptions(self, argument_group):\n    argument_group.add_argument(\n        '--buffer_size', '--buffer-size', '--bs', dest='buffer_size',\n        action='store', default=0, help=(\n            'The buffer size for the output (defaults to 196MiB).'))\n\n    argument_group.add_argument(\n        '--queue_size', '--queue-size', dest='queue_size', action='store',\n        default=0, help=(\n            'The maximum number of queued items per worker '\n            '(defaults to {0:d})').format(self._DEFAULT_QUEUE_SIZE))", "response": "Adds the performance options to the argument group."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef AddProcessingOptions(self, argument_group):\n    argument_group.add_argument(\n        '--single_process', '--single-process', dest='single_process',\n        action='store_true', default=False, help=(\n            'Indicate that the tool should run in a single process.'))\n\n    argument_helper_names = ['temporary_directory', 'workers', 'zeromq']\n    if self._CanEnforceProcessMemoryLimit():\n      argument_helper_names.append('process_resources')\n    helpers_manager.ArgumentHelperManager.AddCommandLineArguments(\n        argument_group, names=argument_helper_names)", "response": "Adds the processing options to the argument group."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlist information about the available parsers and plugins.", "response": "def ListParsersAndPlugins(self):\n    \"\"\"Lists information about the available parsers and plugins.\"\"\"\n    parsers_information = parsers_manager.ParsersManager.GetParsersInformation()\n\n    table_view = views.ViewsFactory.GetTableView(\n        self._views_format_type, column_names=['Name', 'Description'],\n        title='Parsers')\n\n    for name, description in sorted(parsers_information):\n      table_view.AddRow([name, description])\n    table_view.Write(self._output_writer)\n\n    parser_names = parsers_manager.ParsersManager.GetNamesOfParsersWithPlugins()\n    for parser_name in parser_names:\n      plugins_information = (\n          parsers_manager.ParsersManager.GetParserPluginsInformation(\n              parser_filter_expression=parser_name))\n\n      table_title = 'Parser plugins: {0:s}'.format(parser_name)\n      table_view = views.ViewsFactory.GetTableView(\n          self._views_format_type, column_names=['Name', 'Description'],\n          title=table_title)\n      for name, description in sorted(plugins_information):\n        table_view.AddRow([name, description])\n      table_view.Write(self._output_writer)\n\n    title = 'Parser presets'\n    if self._presets_file:\n      source_path = os.path.dirname(os.path.dirname(os.path.dirname(\n          os.path.abspath(__file__))))\n\n      presets_file = self._presets_file\n      if presets_file.startswith(source_path):\n        presets_file = presets_file[len(source_path) + 1:]\n\n      title = '{0:s} ({1:s})'.format(title, presets_file)\n\n    presets_information = parsers_manager.ParsersManager.GetPresetsInformation()\n\n    table_view = views.ViewsFactory.GetTableView(\n        self._views_format_type, column_names=['Name', 'Parsers and plugins'],\n        title=title)\n    for name, description in sorted(presets_information):\n      table_view.AddRow([name, description])\n    table_view.Write(self._output_writer)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef GetEntries(\n      self, parser_mediator, cookie_data=None, url=None, **kwargs):\n    \"\"\"Extracts event objects from the cookie.\n\n    Args:\n      parser_mediator (ParserMediator): parser mediator.\n      cookie_data (str): cookie data.\n      url (str): URL or path where the cookie got set.\n    \"\"\"\n    fields = cookie_data.split('.')\n    number_of_fields = len(fields)\n\n    if number_of_fields not in (1, 6):\n      parser_mediator.ProduceExtractionWarning(\n          'unsupported number of fields: {0:d} in cookie: {1:s}'.format(\n              number_of_fields, self.COOKIE_NAME))\n      return\n\n    if number_of_fields == 1:\n      domain_hash = None\n      visitor_identifier = None\n      first_visit_posix_time = None\n      previous_visit_posix_time = None\n\n      try:\n        # TODO: fix that we're losing precision here use dfdatetime.\n        last_visit_posix_time = int(fields[0], 10) / 10000000\n      except ValueError:\n        last_visit_posix_time = None\n\n      number_of_sessions = None\n\n    elif number_of_fields == 6:\n      domain_hash = fields[0]\n      visitor_identifier = fields[1]\n\n      # TODO: Double check this time is stored in UTC and not local time.\n      try:\n        first_visit_posix_time = int(fields[2], 10)\n      except ValueError:\n        first_visit_posix_time = None\n\n      try:\n        previous_visit_posix_time = int(fields[3], 10)\n      except ValueError:\n        previous_visit_posix_time = None\n\n      try:\n        last_visit_posix_time = int(fields[4], 10)\n      except ValueError:\n        last_visit_posix_time = None\n\n      try:\n        number_of_sessions = int(fields[5], 10)\n      except ValueError:\n        number_of_sessions = None\n\n    event_data = GoogleAnalyticsEventData('utma')\n    event_data.cookie_name = self.COOKIE_NAME\n    event_data.domain_hash = domain_hash\n    event_data.sessions = number_of_sessions\n    event_data.url = url\n    event_data.visitor_id = visitor_identifier\n\n    if first_visit_posix_time is not None:\n      date_time = dfdatetime_posix_time.PosixTime(\n          timestamp=first_visit_posix_time)\n      event = time_events.DateTimeValuesEvent(\n          date_time, 'Analytics Creation Time')\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    if previous_visit_posix_time is not None:\n      date_time = dfdatetime_posix_time.PosixTime(\n          timestamp=previous_visit_posix_time)\n      event = time_events.DateTimeValuesEvent(\n          date_time, 'Analytics Previous Time')\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    date_time = None\n    if last_visit_posix_time is not None:\n      date_time = dfdatetime_posix_time.PosixTime(\n          timestamp=last_visit_posix_time)\n      timestamp_description = definitions.TIME_DESCRIPTION_LAST_VISITED\n    elif first_visit_posix_time is None and previous_visit_posix_time is None:\n      # If both creation_time and written_time are None produce an event\n      # object without a timestamp.\n      date_time = dfdatetime_semantic_time.SemanticTime('Not set')\n      timestamp_description = definitions.TIME_DESCRIPTION_NOT_A_TIME\n\n    if date_time is not None:\n      event = time_events.DateTimeValuesEvent(date_time, timestamp_description)\n      parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Extracts the event objects from the cookie data."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nextracts the entries from the cookie.", "response": "def GetEntries(\n      self, parser_mediator, cookie_data=None, url=None, **kwargs):\n    \"\"\"Extracts event objects from the cookie.\n\n    Args:\n      parser_mediator (ParserMediator): parser mediator.\n      cookie_data (bytes): cookie data.\n      url (str): URL or path where the cookie got set.\n    \"\"\"\n    fields = cookie_data.split('.')\n    number_of_fields = len(fields)\n\n    if number_of_fields not in (1, 4):\n      parser_mediator.ProduceExtractionWarning(\n          'unsupported number of fields: {0:d} in cookie: {1:s}'.format(\n              number_of_fields, self.COOKIE_NAME))\n      return\n\n    if number_of_fields == 1:\n      domain_hash = None\n\n      try:\n        # TODO: fix that we're losing precision here use dfdatetime.\n        last_visit_posix_time = int(fields[0], 10) / 10000000\n      except ValueError:\n        last_visit_posix_time = None\n\n      number_of_pages_viewed = None\n\n    elif number_of_fields == 4:\n      domain_hash = fields[0]\n\n      try:\n        number_of_pages_viewed = int(fields[1], 10)\n      except ValueError:\n        number_of_pages_viewed = None\n\n      try:\n        if fields[2] in ('8', '9'):\n          # TODO: fix that we're losing precision here use dfdatetime.\n          last_visit_posix_time = int(fields[3], 10) / 1000\n        else:\n          last_visit_posix_time = int(fields[3], 10)\n      except ValueError:\n        last_visit_posix_time = None\n\n    if last_visit_posix_time is not None:\n      date_time = dfdatetime_posix_time.PosixTime(\n          timestamp=last_visit_posix_time)\n      timestamp_description = definitions.TIME_DESCRIPTION_LAST_VISITED\n    else:\n      date_time = dfdatetime_semantic_time.SemanticTime('Not set')\n      timestamp_description = definitions.TIME_DESCRIPTION_NOT_A_TIME\n\n    event_data = GoogleAnalyticsEventData('utmb')\n    event_data.cookie_name = self.COOKIE_NAME\n    event_data.domain_hash = domain_hash\n    event_data.pages_viewed = number_of_pages_viewed\n    event_data.url = url\n\n    event = time_events.DateTimeValuesEvent(date_time, timestamp_description)\n    parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nextract the entries from the cookie.", "response": "def GetEntries(\n      self, parser_mediator, cookie_data=None, url=None, **kwargs):\n    \"\"\"Extracts event objects from the cookie.\n\n    Args:\n      parser_mediator (ParserMediator): parser mediator.\n      cookie_data (bytes): cookie data.\n      url (str): URL or path where the cookie got set.\n    \"\"\"\n    fields = cookie_data.split('.')\n    number_of_fields = len(fields)\n\n    if number_of_fields != 1:\n      parser_mediator.ProduceExtractionWarning(\n          'unsupported number of fields: {0:d} in cookie: {1:s}'.format(\n              number_of_fields, self.COOKIE_NAME))\n      return\n\n    try:\n      # TODO: fix that we're losing precision here use dfdatetime.\n      last_visit_posix_time = int(fields[0], 10) / 10000000\n    except ValueError:\n      last_visit_posix_time = None\n\n    if last_visit_posix_time is not None:\n      date_time = dfdatetime_posix_time.PosixTime(\n          timestamp=last_visit_posix_time)\n      timestamp_description = definitions.TIME_DESCRIPTION_LAST_VISITED\n    else:\n      date_time = dfdatetime_semantic_time.SemanticTime('Not set')\n      timestamp_description = definitions.TIME_DESCRIPTION_NOT_A_TIME\n\n    event_data = GoogleAnalyticsEventData('utmt')\n    event_data.cookie_name = self.COOKIE_NAME\n    event_data.url = url\n\n    event = time_events.DateTimeValuesEvent(date_time, timestamp_description)\n    parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nextracts the entries from the cookie.", "response": "def GetEntries(\n      self, parser_mediator, cookie_data=None, url=None, **kwargs):\n    \"\"\"Extracts event objects from the cookie.\n\n    Args:\n      parser_mediator (ParserMediator): parser mediator.\n      cookie_data (str): cookie data.\n      url (str): URL or path where the cookie got set.\n    \"\"\"\n    fields = cookie_data.split('.')\n    number_of_fields = len(fields)\n\n    if number_of_fields > 5:\n      variables = '.'.join(fields[4:])\n      fields = fields[0:4]\n      fields.append(variables)\n      number_of_fields = len(fields)\n\n    if number_of_fields not in (1, 5):\n      parser_mediator.ProduceExtractionWarning(\n          'unsupported number of fields: {0:d} in cookie: {1:s}'.format(\n              number_of_fields, self.COOKIE_NAME))\n      return\n\n    if number_of_fields == 1:\n      domain_hash = None\n\n      try:\n        # TODO: fix that we're losing precision here use dfdatetime.\n        last_visit_posix_time = int(fields[0], 10) / 10000000\n      except ValueError:\n        last_visit_posix_time = None\n\n      number_of_sessions = None\n      number_of_sources = None\n      extra_attributes = {}\n\n    elif number_of_fields == 5:\n      domain_hash = fields[0]\n\n      try:\n        last_visit_posix_time = int(fields[1], 10)\n      except ValueError:\n        last_visit_posix_time = None\n\n      try:\n        number_of_sessions = int(fields[2], 10)\n      except ValueError:\n        number_of_sessions = None\n\n      try:\n        number_of_sources = int(fields[3], 10)\n      except ValueError:\n        number_of_sources = None\n\n      extra_variables = fields[4].split('|')\n\n      extra_attributes = {}\n      for variable in extra_variables:\n        key, _, value = variable.partition('=')\n\n        # Urllib2 in Python 2 requires a 'str' argument, not 'unicode'. We thus\n        # need to convert the value argument to 'str\" and back again, but only\n        # in Python 2.\n        if isinstance(value, py2to3.UNICODE_TYPE) and py2to3.PY_2:\n          try:\n            value = codecs.decode(value, 'ascii')\n          except UnicodeEncodeError:\n            value = codecs.decode(value, 'ascii', errors='replace')\n            parser_mediator.ProduceExtractionWarning(\n                'Cookie contains non 7-bit ASCII characters, which have been '\n                'replaced with a \"?\".')\n\n        value = urlparse.unquote(value)\n\n        if py2to3.PY_2:\n          try:\n            value = codecs.encode(value, 'utf-8')\n          except UnicodeDecodeError:\n            value = codecs.encode(value, 'utf-8', errors='replace')\n            parser_mediator.ProduceExtractionWarning(\n                'Cookie value did not contain a Unicode string. Non UTF-8 '\n                'characters have been replaced.')\n\n        extra_attributes[key] = value\n\n    if last_visit_posix_time is not None:\n      date_time = dfdatetime_posix_time.PosixTime(\n          timestamp=last_visit_posix_time)\n      timestamp_description = definitions.TIME_DESCRIPTION_LAST_VISITED\n    else:\n      date_time = dfdatetime_semantic_time.SemanticTime('Not set')\n      timestamp_description = definitions.TIME_DESCRIPTION_NOT_A_TIME\n\n    event_data = GoogleAnalyticsEventData('utmz')\n    event_data.cookie_name = self.COOKIE_NAME\n    event_data.domain_hash = domain_hash\n    event_data.sessions = number_of_sessions\n    event_data.sources = number_of_sources\n    event_data.url = url\n\n    for key, value in iter(extra_attributes.items()):\n      setattr(event_data, key, value)\n\n    event = time_events.DateTimeValuesEvent(date_time, timestamp_description)\n    parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse a Java WebStart Cache IDX file - like object.", "response": "def ParseFileObject(self, parser_mediator, file_object):\n    \"\"\"Parses a Java WebStart Cache IDX file-like object.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      file_object (dvfvs.FileIO): a file-like object to parse.\n\n    Raises:\n      UnableToParseFile: when the file cannot be parsed.\n    \"\"\"\n    file_header_map = self._GetDataTypeMap('java_idx_file_header')\n\n    try:\n      file_header, file_offset = self._ReadStructureFromFileObject(\n          file_object, 0, file_header_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.UnableToParseFile(\n          'Unable to parse file header with error: {0!s}'.format(\n              exception))\n\n    if not file_header.format_version in self._SUPPORTED_FORMAT_VERSIONS:\n      raise errors.UnableToParseFile('Unsupported format version.')\n\n    if file_header.format_version == 602:\n      section1_map = self._GetDataTypeMap('java_idx_602_section1')\n    elif file_header.format_version in (603, 604):\n      section1_map = self._GetDataTypeMap('java_idx_603_section1')\n    elif file_header.format_version == 605:\n      section1_map = self._GetDataTypeMap('java_idx_605_section1')\n\n    try:\n      section1, data_size = self._ReadStructureFromFileObject(\n          file_object, file_offset, section1_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.UnableToParseFile((\n          'Unable to parse section 1 (format version: {0:d}) with error: '\n          '{1!s}').format(file_header.format_version, exception))\n\n    file_offset += data_size\n\n    if file_header.format_version == 602:\n      section2_map = self._GetDataTypeMap('java_idx_602_section2')\n    elif file_header.format_version in (603, 604, 605):\n      file_offset = 128\n      section2_map = self._GetDataTypeMap('java_idx_603_section2')\n\n    try:\n      section2, data_size = self._ReadStructureFromFileObject(\n          file_object, file_offset, section2_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.UnableToParseFile((\n          'Unable to parse section 2 (format version: {0:d}) with error: '\n          '{1!s}').format(file_header.format_version, exception))\n\n    file_offset += data_size\n\n    if not section2.url:\n      raise errors.UnableToParseFile('URL not found in file.')\n\n    date_http_header = None\n    for _ in range(section2.number_of_http_headers):\n      http_header_map = self._GetDataTypeMap('java_idx_http_header')\n      try:\n        http_header, data_size = self._ReadStructureFromFileObject(\n            file_object, file_offset, http_header_map)\n      except (ValueError, errors.ParseError) as exception:\n        parser_mediator.ProduceExtractionWarning(\n            'Unable to parse HTTP header value at offset: 0x{0:08x}'.format(\n                file_offset))\n        break\n\n      file_offset += data_size\n\n      if http_header.name == 'date':\n        date_http_header = http_header\n        break\n\n    event_data = JavaIDXEventData()\n    event_data.idx_version = file_header.format_version\n    event_data.ip_address = getattr(section2, 'ip_address', None)\n    event_data.url = section2.url\n\n    date_time = dfdatetime_java_time.JavaTime(\n        timestamp=section1.modification_time)\n    # TODO: Move the timestamp description into definitions.\n    event = time_events.DateTimeValuesEvent(date_time, 'File Hosted Date')\n    parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    if section1.expiration_time:\n      date_time = dfdatetime_java_time.JavaTime(\n          timestamp=section1.expiration_time)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_EXPIRATION)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    if date_http_header:\n      # A HTTP header date and string \"should\" be in UTC or have an associated\n      # time zone information in the string itself. If that is not the case\n      # then there is no reliable method for plaso to determine the proper\n      # time zone, so the assumption is that it is UTC.\n      try:\n        download_date = timelib.Timestamp.FromTimeString(\n            date_http_header.value, gmt_as_timezone=False)\n      except errors.TimestampError:\n        parser_mediator.ProduceExtractionWarning(\n            'Unable to parse date HTTP header value: {0:s}'.format(\n                date_http_header.value))\n\n      if download_date:\n        event = time_events.TimestampEvent(\n            download_date, definitions.TIME_DESCRIPTION_FILE_DOWNLOADED)\n        parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing a log header.", "response": "def _ParseHeader(self, parser_mediator, structure):\n    \"\"\"Parses a log header.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      structure (pyparsing.ParseResults): structure of tokens derived from\n          a line of a text file.\n    \"\"\"\n    _, month, day, hours, minutes, seconds, year = structure.date_time\n\n    month = timelib.MONTH_DICT.get(month.lower(), 0)\n\n    time_elements_tuple = (year, month, day, hours, minutes, seconds)\n\n    try:\n      date_time = dfdatetime_time_elements.TimeElements(\n          time_elements_tuple=time_elements_tuple)\n      date_time.is_local_time = True\n    except ValueError:\n      parser_mediator.ProduceExtractionWarning(\n          'invalid date time value: {0!s}'.format(structure.date_time))\n      return\n\n    self._last_month = month\n\n    event_data = XChatLogEventData()\n\n    if structure.log_action[0] == 'BEGIN':\n      self._xchat_year = year\n      event_data.text = 'XChat start logging'\n\n    elif structure.log_action[0] == 'END':\n      self._xchat_year = None\n      event_data.text = 'XChat end logging'\n\n    else:\n      logger.debug('Unknown log action: {0:s}.'.format(\n          ' '.join(structure.log_action)))\n      return\n\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_ADDED,\n        time_zone=parser_mediator.timezone)\n    parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _ParseLogLine(self, parser_mediator, structure):\n    if not self._xchat_year:\n      return\n\n    time_elements_tuple = self._GetTimeElementsTuple(structure)\n\n    try:\n      date_time = dfdatetime_time_elements.TimeElements(\n          time_elements_tuple=time_elements_tuple)\n      date_time.is_local_time = True\n    except ValueError:\n      parser_mediator.ProduceExtractionWarning(\n          'invalid date time value: {0!s}'.format(structure.date_time))\n      return\n\n    self._last_month = time_elements_tuple[1]\n\n    event_data = XChatLogEventData()\n    event_data.nickname = structure.nickname\n    # The text string contains multiple unnecessary whitespaces that need to\n    # be removed, thus the split and re-join.\n    event_data.text = ' '.join(structure.text.split())\n\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_ADDED,\n        time_zone=parser_mediator.timezone)\n    parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Parses a log line."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ParseRecord(self, parser_mediator, key, structure):\n    if key not in ('header', 'header_signature', 'logline'):\n      raise errors.ParseError(\n          'Unable to parse record, unknown structure: {0:s}'.format(key))\n\n    if key == 'logline':\n      self._ParseLogLine(parser_mediator, structure)\n\n    elif key == 'header':\n      self._ParseHeader(parser_mediator, structure)\n\n    elif key == 'header_signature':\n      # If this key is matched (after others keys failed) we got a different\n      # localized header and we should stop parsing until a new good header\n      # is found. Stop parsing is done setting xchat_year to 0.\n      # Note that the code assumes that LINE_STRUCTURES will be used in the\n      # exact order as defined!\n      logger.warning('Unknown locale header.')\n      self._xchat_year = 0", "response": "Parses a log record structure and produces events."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef VerifyStructure(self, parser_mediator, line):\n    try:\n      structure = self._HEADER.parseString(line)\n    except pyparsing.ParseException:\n      logger.debug('Not a XChat log file')\n      return False\n\n    _, month, day, hours, minutes, seconds, year = structure.date_time\n\n    month = timelib.MONTH_DICT.get(month.lower(), 0)\n\n    time_elements_tuple = (year, month, day, hours, minutes, seconds)\n\n    try:\n      dfdatetime_time_elements.TimeElements(\n          time_elements_tuple=time_elements_tuple)\n    except ValueError:\n      logger.debug('Not a XChat log file, invalid date and time: {0!s}'.format(\n          structure.date_time))\n      return False\n\n    return True", "response": "Verify that this file is a XChat log file."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds command line arguments to an argument group.", "response": "def AddArguments(cls, argument_group):\n    \"\"\"Adds command line arguments the helper supports to an argument group.\n\n    This function takes an argument parser or an argument group object and adds\n    to it all the command line arguments this helper supports.\n\n    Args:\n      argument_group (argparse._ArgumentGroup|argparse.ArgumentParser):\n          argparse group.\n    \"\"\"\n    argument_group.add_argument(\n        '--user', dest='username', type=str, action='store',\n        default=cls._DEFAULT_USERNAME, metavar='USERNAME', required=False,\n        help='The username used to connect to the database.')\n    argument_group.add_argument(\n        '--password', dest='password', type=str, action='store',\n        default=cls._DEFAULT_PASSWORD, metavar='PASSWORD', help=(\n            'The password for the database user.'))\n    argument_group.add_argument(\n        '--db_name', '--db-name', dest='db_name', action='store',\n        type=str, default=cls._DEFAULT_NAME, required=False, help=(\n            'The name of the database to connect to.'))\n\n    server_config.ServerArgumentsHelper.AddArguments(argument_group)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ParseOptions(cls, options, output_module):\n    if not hasattr(output_module, 'SetCredentials'):\n      raise errors.BadConfigObject('Unable to set username information.')\n\n    if not hasattr(output_module, 'SetDatabaseName'):\n      raise errors.BadConfigObject('Unable to set database information.')\n\n    username = cls._ParseStringOption(\n        options, 'username', default_value=cls._DEFAULT_USERNAME)\n    password = cls._ParseStringOption(\n        options, 'password', default_value=cls._DEFAULT_PASSWORD)\n    name = cls._ParseStringOption(\n        options, 'db_name', default_value=cls._DEFAULT_NAME)\n\n    output_module.SetCredentials(username=username, password=password)\n    output_module.SetDatabaseName(name)\n    server_config.ServerArgumentsHelper.ParseOptions(options, output_module)", "response": "Parses and validates the options."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ParseOptions(cls, options, configuration_object):\n    if not isinstance(configuration_object, tools.CLITool):\n      raise errors.BadConfigObject(\n          'Configuration object is not an instance of CLITool')\n\n    filter_collection = getattr(\n        configuration_object, '_filter_collection', None)\n    if not filter_collection:\n      raise errors.BadConfigObject(\n          'Filter collection missing from configuration object')\n\n    date_filters = getattr(options, 'date_filters', None)\n    if not date_filters:\n      return\n\n    file_entry_filter = file_entry_filters.DateTimeFileEntryFilter()\n\n    for date_filter in date_filters:\n      date_filter_pieces = date_filter.split(',')\n      if len(date_filter_pieces) != 3:\n        raise errors.BadConfigOption(\n            'Badly formed date filter: {0:s}'.format(date_filter))\n\n      time_value, start_time_string, end_time_string = date_filter_pieces\n      time_value = time_value.strip()\n      start_time_string = start_time_string.strip()\n      end_time_string = end_time_string.strip()\n\n      try:\n        file_entry_filter.AddDateTimeRange(\n            time_value, start_time_string=start_time_string,\n            end_time_string=end_time_string)\n      except ValueError:\n        raise errors.BadConfigOption(\n            'Badly formed date filter: {0:s}'.format(date_filter))\n\n    filter_collection.AddFilter(file_entry_filter)", "response": "Parses and validates the options."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _ParseLNKFile(\n      self, parser_mediator, file_entry, file_offset, remaining_file_size):\n    \"\"\"Parses a LNK file stored within the .customDestinations-ms file.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      file_entry (dfvfs.FileEntry): a file entry.\n      file_offset (int): offset to the LNK file, relative to the start of the\n          .customDestinations-ms file.\n      remaining_file_size (int): size of the data remaining in the\n          .customDestinations-ms file.\n\n    Returns:\n      int: size of the LNK file data or 0 if the LNK file could not be read.\n    \"\"\"\n    path_spec = path_spec_factory.Factory.NewPathSpec(\n        definitions.TYPE_INDICATOR_DATA_RANGE, range_offset=file_offset,\n        range_size=remaining_file_size, parent=file_entry.path_spec)\n\n    display_name = '{0:s} # 0x{1:08x}'.format(file_entry.name, file_offset)\n\n    try:\n      lnk_file_object = resolver.Resolver.OpenFileObject(path_spec)\n    except (dfvfs_errors.BackEndError, RuntimeError) as exception:\n      message = (\n          'unable to open LNK file: {0:s} with error: {1!s}').format(\n              display_name, exception)\n      parser_mediator.ProduceExtractionWarning(message)\n      return 0\n\n    parser_mediator.AppendToParserChain(self._WINLNK_PARSER)\n    try:\n      lnk_file_object.seek(0, os.SEEK_SET)\n      self._WINLNK_PARSER.ParseFileLNKFile(\n          parser_mediator, lnk_file_object, display_name)\n    finally:\n      parser_mediator.PopFromParserChain()\n\n    # We cannot trust the file size in the LNK data so we get the last offset\n    # that was read instead.\n    lnk_file_size = lnk_file_object.get_offset()\n\n    lnk_file_object.close()\n\n    return lnk_file_size", "response": "Parses a LNK file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ParseFileObject(self, parser_mediator, file_object):\n    file_entry = parser_mediator.GetFileEntry()\n    display_name = parser_mediator.GetDisplayName()\n\n    file_header_map = self._GetDataTypeMap('custom_file_header')\n\n    try:\n      file_header, file_offset = self._ReadStructureFromFileObject(\n          file_object, 0, file_header_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.UnableToParseFile((\n          'Invalid Custom Destination: {0:s} - unable to parse file header '\n          'with error: {1!s}').format(display_name, exception))\n\n    if file_header.unknown1 != 2:\n      raise errors.UnableToParseFile((\n          'Unsupported Custom Destination file: {0:s} - invalid unknown1: '\n          '{1:d}.').format(display_name, file_header.unknown1))\n\n    if file_header.header_values_type > 2:\n      raise errors.UnableToParseFile((\n          'Unsupported Custom Destination file: {0:s} - invalid header value '\n          'type: {1:d}.').format(display_name, file_header.header_values_type))\n\n    if file_header.header_values_type == 0:\n      data_map_name = 'custom_file_header_value_type_0'\n    else:\n      data_map_name = 'custom_file_header_value_type_1_or_2'\n\n    file_header_value_map = self._GetDataTypeMap(data_map_name)\n\n    try:\n      _, value_data_size = self._ReadStructureFromFileObject(\n          file_object, file_offset, file_header_value_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.UnableToParseFile((\n          'Invalid Custom Destination: {0:s} - unable to parse file header '\n          'value with error: {1!s}').format(display_name, exception))\n\n    file_offset += value_data_size\n    file_size = file_object.get_size()\n    remaining_file_size = file_size - file_offset\n\n    entry_header_map = self._GetDataTypeMap('custom_entry_header')\n    file_footer_map = self._GetDataTypeMap('custom_file_footer')\n\n    # The Custom Destination file does not have a unique signature in\n    # the file header that is why we use the first LNK class identifier (GUID)\n    # as a signature.\n    first_guid_checked = False\n    while remaining_file_size > 4:\n      try:\n        entry_header, entry_data_size = self._ReadStructureFromFileObject(\n            file_object, file_offset, entry_header_map)\n\n      except (ValueError, errors.ParseError) as exception:\n        if not first_guid_checked:\n          raise errors.UnableToParseFile((\n              'Invalid Custom Destination file: {0:s} - unable to parse '\n              'entry header with error: {1!s}').format(\n                  display_name, exception))\n\n        parser_mediator.ProduceExtractionWarning(\n            'unable to parse entry header with error: {0!s}'.format(\n                exception))\n        break\n\n      if entry_header.guid != self._LNK_GUID:\n        if not first_guid_checked:\n          raise errors.UnableToParseFile((\n              'Unsupported Custom Destination file: {0:s} - invalid entry '\n              'header signature offset: 0x{1:08x}.').format(\n                  display_name, file_offset))\n\n        try:\n          # Check if we found the footer instead of an entry header.\n          file_footer, _ = self._ReadStructureFromFileObject(\n              file_object, file_offset, file_footer_map)\n\n          if file_footer.signature != self._FILE_FOOTER_SIGNATURE:\n            parser_mediator.ProduceExtractionWarning(\n                'invalid entry header signature at offset: 0x{0:08x}'.format(\n                    file_offset))\n\n        except (ValueError, errors.ParseError) as exception:\n          parser_mediator.ProduceExtractionWarning((\n              'unable to parse footer at offset: 0x{0:08x} with error: '\n              '{1!s}').format(file_offset, exception))\n          break\n\n        # TODO: add support for Jump List LNK file recovery.\n        break\n\n      first_guid_checked = True\n      file_offset += entry_data_size\n      remaining_file_size -= entry_data_size\n\n      lnk_file_size = self._ParseLNKFile(\n          parser_mediator, file_entry, file_offset, remaining_file_size)\n\n      file_offset += lnk_file_size\n      remaining_file_size -= lnk_file_size\n\n    try:\n      file_footer, _ = self._ReadStructureFromFileObject(\n          file_object, file_offset, file_footer_map)\n\n      if file_footer.signature != self._FILE_FOOTER_SIGNATURE:\n        parser_mediator.ProduceExtractionWarning(\n            'invalid footer signature at offset: 0x{0:08x}'.format(file_offset))\n\n    except (ValueError, errors.ParseError) as exception:\n      parser_mediator.ProduceExtractionWarning((\n          'unable to parse footer at offset: 0x{0:08x} with error: '\n          '{1!s}').format(file_offset, exception))", "response": "Parses a. customDestinations - ms file - like object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ParseMessagesRow(self, parser_mediator, query, row, **unused_kwargs):\n    query_hash = hash(query)\n\n    event_data = HangoutsMessageData()\n    event_data.sender = self._GetRowValue(query_hash, row, 'full_name')\n    event_data.body = self._GetRowValue(query_hash, row, 'text')\n    event_data.offset = self._GetRowValue(query_hash, row, '_id')\n    event_data.query = query\n    event_data.message_status = self._GetRowValue(query_hash, row, 'status')\n    event_data.message_type = self._GetRowValue(query_hash, row, 'type')\n\n    timestamp = self._GetRowValue(query_hash, row, 'timestamp')\n    date_time = dfdatetime_posix_time.PosixTimeInMicroseconds(\n        timestamp=timestamp)\n\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_CREATION)\n    parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Parses a Messages row."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse and validates options.", "response": "def ParseOptions(cls, options, configuration_object):\n    \"\"\"Parses and validates options.\n\n    Args:\n      options (argparse.Namespace): parser options.\n      configuration_object (CLITool): object to be configured by the argument\n          helper.\n\n    Raises:\n      BadConfigObject: when the configuration object is of the wrong type.\n    \"\"\"\n    if not isinstance(configuration_object, tools.CLITool):\n      raise errors.BadConfigObject(\n          'Configuration object is not an instance of CLITool')\n\n    temporary_directory = getattr(options, 'temporary_directory', None)\n    if temporary_directory and not os.path.isdir(temporary_directory):\n      raise errors.BadConfigOption(\n          'No such temporary directory: {0:s}'.format(temporary_directory))\n\n    setattr(configuration_object, '_temporary_directory', temporary_directory)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _Connect(self):\n    elastic_host = {'host': self._host, 'port': self._port}\n\n    if self._url_prefix:\n      elastic_host['url_prefix'] = self._url_prefix\n\n    elastic_http_auth = None\n    if self._username is not None:\n      elastic_http_auth = (self._username, self._password)\n\n    self._client = elasticsearch.Elasticsearch(\n        [elastic_host],\n        http_auth=elastic_http_auth,\n        use_ssl=self._use_ssl,\n        ca_certs=self._ca_certs\n    )\n\n    logger.debug(\n        ('Connected to Elasticsearch server: {0:s} port: {1:d}'\n         'URL prefix {2!s}.').format(self._host, self._port, self._url_prefix))", "response": "Connects to an Elasticsearch server."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate an Elasticsearch index if it does not exist.", "response": "def _CreateIndexIfNotExists(self, index_name, mappings):\n    \"\"\"Creates an Elasticsearch index if it does not exist.\n\n    Args:\n      index_name (str): mame of the index.\n      mappings (dict[str, object]): mappings of the index.\n\n    Raises:\n      RuntimeError: if the Elasticsearch index cannot be created.\n    \"\"\"\n    try:\n      if not self._client.indices.exists(index_name):\n        self._client.indices.create(\n            body={'mappings': mappings}, index=index_name)\n\n    except elasticsearch.exceptions.ConnectionError as exception:\n      raise RuntimeError(\n          'Unable to create Elasticsearch index with error: {0!s}'.format(\n              exception))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninserts the buffered event documents into Elasticsearch.", "response": "def _FlushEvents(self):\n    \"\"\"Inserts the buffered event documents into Elasticsearch.\"\"\"\n    try:\n      # pylint: disable=unexpected-keyword-arg\n      # pylint does not recognizes request_timeout as a valid kwarg. According\n      # to http://elasticsearch-py.readthedocs.io/en/master/api.html#timeout\n      # it should be supported.\n      self._client.bulk(\n          body=self._event_documents, doc_type=self._document_type,\n          index=self._index_name, request_timeout=self._DEFAULT_REQUEST_TIMEOUT)\n\n    except ValueError as exception:\n      # Ignore problematic events\n      logger.warning('Unable to bulk insert with error: {0!s}'.format(\n          exception))\n\n    logger.debug('Inserted {0:d} events into Elasticsearch'.format(\n        self._number_of_buffered_events))\n\n    self._event_documents = []\n    self._number_of_buffered_events = 0"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsanitizes the event for use in Elasticsearch.", "response": "def _GetSanitizedEventValues(self, event):\n    \"\"\"Sanitizes the event for use in Elasticsearch.\n\n    The event values need to be sanitized to prevent certain values from\n    causing problems when indexing with Elasticsearch. For example the path\n    specification is a nested dictionary which will cause problems for\n    Elasticsearch automatic indexing.\n\n    Args:\n      event (EventObject): event.\n\n    Returns:\n      dict[str, object]: sanitized event values.\n\n    Raises:\n      NoFormatterFound: if no event formatter can be found to match the data\n          type in the event.\n    \"\"\"\n    event_values = {}\n    for attribute_name, attribute_value in event.GetAttributes():\n      # Ignore the regvalue attribute as it cause issues when indexing.\n      if attribute_name == 'regvalue':\n        continue\n\n      if attribute_name == 'pathspec':\n        try:\n          attribute_value = JsonPathSpecSerializer.WriteSerialized(\n              attribute_value)\n        except TypeError:\n          continue\n      event_values[attribute_name] = attribute_value\n\n    # Add a string representation of the timestamp.\n    try:\n      attribute_value = timelib.Timestamp.RoundToSeconds(event.timestamp)\n    except TypeError as exception:\n      logger.warning((\n          'Unable to round timestamp {0!s}. error: {1!s}. '\n          'Defaulting to 0').format(event.timestamp, exception))\n      attribute_value = 0\n\n    attribute_value = timelib.Timestamp.CopyToIsoFormat(\n        attribute_value, timezone=self._output_mediator.timezone)\n    event_values['datetime'] = attribute_value\n\n    message, _ = self._output_mediator.GetFormattedMessages(event)\n    if message is None:\n      data_type = getattr(event, 'data_type', 'UNKNOWN')\n      raise errors.NoFormatterFound(\n          'Unable to find event formatter for: {0:s}.'.format(data_type))\n\n    event_values['message'] = message\n\n    # Tags needs to be a list for Elasticsearch to index correctly.\n    try:\n      labels = list(event_values['tag'].labels)\n    except (KeyError, AttributeError):\n      labels = []\n    event_values['tag'] = labels\n\n    source_short, source = self._output_mediator.GetFormattedSources(event)\n    if source is None or source_short is None:\n      data_type = getattr(event, 'data_type', 'UNKNOWN')\n      raise errors.NoFormatterFound(\n          'Unable to find event formatter for: {0:s}.'.format(data_type))\n\n    event_values['source_short'] = source_short\n    event_values['source_long'] = source\n\n    return event_values"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _InsertEvent(self, event, force_flush=False):\n    if event:\n      event_document = {'index': {\n          '_index': self._index_name, '_type': self._document_type}}\n      event_values = self._GetSanitizedEventValues(event)\n\n      self._event_documents.append(event_document)\n      self._event_documents.append(event_values)\n      self._number_of_buffered_events += 1\n\n    if force_flush or self._number_of_buffered_events > self._flush_interval:\n      self._FlushEvents()", "response": "Inserts an event into Elasticsearch."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef SetDocumentType(self, document_type):\n    self._document_type = document_type\n    logger.debug('Elasticsearch document type: {0:s}'.format(document_type))", "response": "Sets the document type."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef SetFlushInterval(self, flush_interval):\n    self._flush_interval = flush_interval\n    logger.debug('Elasticsearch flush interval: {0:d}'.format(flush_interval))", "response": "Sets the flush interval for the bulk\n          insert."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the index name.", "response": "def SetIndexName(self, index_name):\n    \"\"\"Set the index name.\n\n    Args:\n      index_name (str): name of the index.\n    \"\"\"\n    self._index_name = index_name\n    logger.debug('Elasticsearch index name: {0:s}'.format(index_name))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef SetServerInformation(self, server, port):\n    self._host = server\n    self._port = port\n    logger.debug('Elasticsearch server: {0!s} port: {1:d}'.format(\n        server, port))", "response": "Sets the server information."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the username for the cluster.", "response": "def SetUsername(self, username):\n    \"\"\"Sets the username.\n\n    Args:\n      username (str): username to authenticate with.\n    \"\"\"\n    self._username = username\n    logger.debug('Elasticsearch username: {0!s}'.format(username))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef SetUseSSL(self, use_ssl):\n    self._use_ssl = use_ssl\n    logger.debug('Elasticsearch use_ssl: {0!s}'.format(use_ssl))", "response": "Sets the use of ssl."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the path to the CA certificates file.", "response": "def SetCACertificatesPath(self, ca_certificates_path):\n    \"\"\"Sets the path to the CA certificates.\n\n    Args:\n      ca_certificates_path (str): path to file containing a list of root\n        certificates to trust.\n\n    Raises:\n      BadConfigOption: if the CA certificates file does not exist.\n    \"\"\"\n    if not ca_certificates_path:\n      return\n\n    if not os.path.exists(ca_certificates_path):\n      raise errors.BadConfigOption(\n          'No such certificate file: {0:s}.'.format(ca_certificates_path))\n\n    self._ca_certs = ca_certificates_path\n    logger.debug('Elasticsearch ca_certs: {0!s}'.format(ca_certificates_path))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse a launch services quarantine row.", "response": "def ParseLSQuarantineRow(\n      self, parser_mediator, query, row, **unused_kwargs):\n    \"\"\"Parses a launch services quarantine event row.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      query (str): query that created the row.\n      row (sqlite3.Row): row.\n    \"\"\"\n    query_hash = hash(query)\n\n    event_data = LsQuarantineEventData()\n    event_data.agent = self._GetRowValue(query_hash, row, 'Agent')\n    event_data.data = self._GetRowValue(query_hash, row, 'Data')\n    event_data.query = query\n    event_data.url = self._GetRowValue(query_hash, row, 'URL')\n\n    timestamp = self._GetRowValue(query_hash, row, 'Time')\n    date_time = dfdatetime_cocoa_time.CocoaTime(timestamp=timestamp)\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_FILE_DOWNLOADED)\n    parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nretrieve a time elements tuple from the structure.", "response": "def _GetTimeElementsTuple(self, structure):\n    \"\"\"Retrieves a time elements tuple from the structure.\n\n    Args:\n      structure (pyparsing.ParseResults): structure of tokens derived from\n          a line of a text file.\n\n    Returns:\n      tuple: containing:\n        year (int): year.\n        month (int): month, where 1 represents January.\n        day_of_month (int): day of month, where 1 is the first day of the month.\n        hours (int): hours.\n        minutes (int): minutes.\n        seconds (int): seconds.\n    \"\"\"\n    month, day, hours, minutes, seconds = structure.date_time\n\n    # Note that dfdatetime_time_elements.TimeElements will raise ValueError\n    # for an invalid month.\n    month = timelib.MONTH_DICT.get(month.lower(), 0)\n\n    if month != 0 and month < self._last_month:\n      # Gap detected between years.\n      self._year_use += 1\n\n    return (self._year_use, month, day, hours, minutes, seconds)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse a single log line and produce an event object.", "response": "def _ParseLogLine(self, parser_mediator, structure, key):\n    \"\"\"Parse a single log line and produce an event object.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      structure (pyparsing.ParseResults): structure of tokens derived from\n          a line of a text file.\n      key (str): name of the parsed structure.\n    \"\"\"\n    time_elements_tuple = self._GetTimeElementsTuple(structure)\n\n    try:\n      date_time = dfdatetime_time_elements.TimeElements(\n          time_elements_tuple=time_elements_tuple)\n    except ValueError:\n      parser_mediator.ProduceExtractionWarning(\n          'invalid date time value: {0!s}'.format(structure.date_time))\n      return\n\n    self._last_month = time_elements_tuple[1]\n\n    if key == 'logline':\n      self._previous_structure = structure\n      message = structure.message\n    else:\n      message = 'Repeated {0:d} times: {1:s}'.format(\n          structure.times, self._previous_structure.message)\n      structure = self._previous_structure\n\n    # It uses CarsNotIn structure which leaves whitespaces\n    # at the beginning of the sender and the caller.\n\n    event_data = MacOSSecuritydLogEventData()\n    event_data.caller = structure.caller.strip() or 'unknown'\n    event_data.facility = structure.facility\n    event_data.level = structure.level\n    event_data.message = message\n    event_data.security_api = structure.security_api or 'unknown'\n    event_data.sender_pid = structure.sender_pid\n    event_data.sender = structure.sender.strip()\n\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_ADDED)\n    parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef VerifyStructure(self, parser_mediator, line):\n    self._last_month = 0\n    self._year_use = parser_mediator.GetEstimatedYear()\n\n    try:\n      structure = self.SECURITYD_LINE.parseString(line)\n    except pyparsing.ParseException:\n      logger.debug('Not a MacOS securityd log file')\n      return False\n\n    time_elements_tuple = self._GetTimeElementsTuple(structure)\n\n    try:\n      dfdatetime_time_elements.TimeElements(\n          time_elements_tuple=time_elements_tuple)\n    except ValueError:\n      logger.debug(\n          'Not a MacOS securityd log file, invalid date and time: {0!s}'.format(\n              structure.date_time))\n      return False\n\n    self._last_month = time_elements_tuple[1]\n\n    return True", "response": "Verify that this file is a securityd log file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _GetWinevtRcDatabaseReader(self):\n    if not self._winevt_database_reader and self._data_location:\n      database_path = os.path.join(\n          self._data_location, self._WINEVT_RC_DATABASE)\n      if not os.path.isfile(database_path):\n        return None\n\n      self._winevt_database_reader = (\n          winevt_rc.WinevtResourcesSqlite3DatabaseReader())\n      if not self._winevt_database_reader.Open(database_path):\n        self._winevt_database_reader = None\n\n    return self._winevt_database_reader", "response": "Opens the Windows Event Log resource database reader."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nretrieve the message string for a specific Windows Event Log source.", "response": "def GetWindowsEventMessage(self, log_source, message_identifier):\n    \"\"\"Retrieves the message string for a specific Windows Event Log source.\n\n    Args:\n      log_source (str): Event Log source, such as \"Application Error\".\n      message_identifier (int): message identifier.\n\n    Returns:\n      str: message string or None if not available.\n    \"\"\"\n    database_reader = self._GetWinevtRcDatabaseReader()\n    if not database_reader:\n      return None\n\n    if self._lcid != self.DEFAULT_LCID:\n      message_string = database_reader.GetMessage(\n          log_source, self.lcid, message_identifier)\n      if message_string:\n        return message_string\n\n    return database_reader.GetMessage(\n        log_source, self.DEFAULT_LCID, message_identifier)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef SetPreferredLanguageIdentifier(self, language_identifier):\n    if not isinstance(language_identifier, py2to3.STRING_TYPES):\n      raise ValueError('Language identifier is not a string.')\n\n    values = language_ids.LANGUAGE_IDENTIFIERS.get(\n        language_identifier.lower(), None)\n    if not values:\n      raise KeyError('Language identifier: {0:s} is not defined.'.format(\n          language_identifier))\n    self._language_identifier = language_identifier\n    self._lcid = values[0]", "response": "Sets the preferred language identifier."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef AddCommandLineArguments(\n      cls, argument_group, category=None, names=None):\n    \"\"\"Adds command line arguments to a configuration object.\n\n    Args:\n      argument_group (argparse._ArgumentGroup|argparse.ArgumentParser):\n          argparse group.\n      category (Optional[str]): category of helpers to apply to\n          the group, such as storage, output, where None will apply the\n          arguments to all helpers. The category can be used to add arguments\n          to a specific group of registered helpers.\n      names (Optional[list[str]]): names of argument helpers to apply,\n          where None will apply the arguments to all helpers.\n    \"\"\"\n    # Process the helper classes in alphabetical order this is needed to\n    # keep the argument order consistent.\n    for helper_name, helper_class in sorted(cls._helper_classes.items()):\n      if ((category and helper_class.CATEGORY != category) or\n          (names and helper_name not in names)):\n        continue\n\n      helper_class.AddArguments(argument_group)", "response": "Adds command line arguments to the configuration object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses and validates the given options using the appropriate helper classes.", "response": "def ParseOptions(cls, options, config_object, category=None, names=None):\n    \"\"\"Parses and validates arguments using the appropriate helpers.\n\n    Args:\n      options (argparse.Namespace): parser options.\n      config_object (object): object to be configured by an argument helper.\n      category (Optional[str]): category of helpers to apply to\n          the group, such as storage, output, where None will apply the\n          arguments to all helpers. The category can be used to add arguments\n          to a specific group of registered helpers.\n      names (Optional[list[str]]): names of argument helpers to apply,\n          where None will apply the arguments to all helpers.\n    \"\"\"\n    for helper_name, helper_class in cls._helper_classes.items():\n      if ((category and helper_class.CATEGORY != category) or\n          (names and helper_name not in names)):\n        continue\n\n      try:\n        helper_class.ParseOptions(options, config_object)\n      except errors.BadConfigObject:\n        pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _QueryHash(self, digest):\n    if not self._url:\n      self._url = '{0:s}://{1:s}:{2:d}/file/find'.format(\n          self._protocol, self._host, self._port)\n\n    request_data = {self.lookup_hash: digest}\n\n    try:\n      json_response = self.MakeRequestAndDecodeJSON(\n          self._url, 'POST', data=request_data)\n\n    except errors.ConnectionError as exception:\n      json_response = None\n      logger.error('Unable to query Viper with error: {0!s}.'.format(\n          exception))\n\n    return json_response", "response": "Queries Viper Server for a specfic hash."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlooking up hashes in Viper using the HTTP API.", "response": "def Analyze(self, hashes):\n    \"\"\"Looks up hashes in Viper using the Viper HTTP API.\n\n    Args:\n      hashes (list[str]): hashes to look up.\n\n    Returns:\n      list[HashAnalysis]: hash analysis.\n\n    Raises:\n      RuntimeError: If no host has been set for Viper.\n    \"\"\"\n    hash_analyses = []\n    for digest in hashes:\n      json_response = self._QueryHash(digest)\n      hash_analysis = interface.HashAnalysis(digest, json_response)\n      hash_analyses.append(hash_analysis)\n\n    return hash_analyses"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef SetProtocol(self, protocol):\n    if protocol not in self.SUPPORTED_PROTOCOLS:\n      raise ValueError('Unsupported protocol: {0!s}'.format(protocol))\n\n    self._protocol = protocol", "response": "Sets the protocol that will be used to query Viper."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef GenerateLabels(self, hash_information):\n    if not hash_information:\n      return ['viper_not_present']\n\n    projects = []\n    tags = []\n    for project, entries in iter(hash_information.items()):\n      if not entries:\n        continue\n\n      projects.append(project)\n\n      for entry in entries:\n        if entry['tags']:\n          tags.extend(entry['tags'])\n\n    if not projects:\n      return ['viper_not_present']\n    strings = ['viper_present']\n\n    for project_name in projects:\n      label = events.EventTag.CopyTextToLabel(\n          project_name, prefix='viper_project_')\n      strings.append(label)\n\n    for tag_name in tags:\n      label = events.EventTag.CopyTextToLabel(tag_name, prefix='viper_tag_')\n      strings.append(label)\n\n    return strings", "response": "Generates a list of strings that will be used in the event tag."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the protocol that will be used to query Viper.", "response": "def SetProtocol(self, protocol):\n    \"\"\"Sets the protocol that will be used to query Viper.\n\n    Args:\n      protocol (str): protocol to use to query Viper. Either 'http' or 'https'.\n\n    Raises:\n      ValueError: If an invalid protocol is selected.\n    \"\"\"\n    protocol = protocol.lower().strip()\n    if protocol not in ['http', 'https']:\n      raise ValueError('Invalid protocol specified for Viper lookup')\n    self._analyzer.SetProtocol(protocol)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _ProcessPathSpec(self, extraction_worker, parser_mediator, path_spec):\n    self._current_display_name = parser_mediator.GetDisplayNameForPathSpec(\n        path_spec)\n\n    try:\n      extraction_worker.ProcessPathSpec(parser_mediator, path_spec)\n\n    except KeyboardInterrupt:\n      self._abort = True\n\n      self._processing_status.aborted = True\n      if self._status_update_callback:\n        self._status_update_callback(self._processing_status)\n\n    # We cannot recover from a CacheFullError and abort processing when\n    # it is raised.\n    except dfvfs_errors.CacheFullError:\n      # TODO: signal engine of failure.\n      self._abort = True\n      logger.error((\n          'ABORT: detected cache full error while processing '\n          'path spec: {0:s}').format(self._current_display_name))\n\n    # All exceptions need to be caught here to prevent the worker\n    # from being killed by an uncaught exception.\n    except Exception as exception:  # pylint: disable=broad-except\n      parser_mediator.ProduceExtractionWarning((\n          'unable to process path specification with error: '\n          '{0!s}').format(exception), path_spec=path_spec)\n\n      if getattr(self._processing_configuration, 'debug_output', False):\n        logger.warning(\n            'Unhandled exception while processing path spec: {0:s}.'.format(\n                self._current_display_name))\n        logger.exception(exception)\n\n        pdb.post_mortem()", "response": "Processes a path specification."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprocesses the sources. Args: source_path_specs (list[dfvfs.PathSpec]): path specifications of the sources to process. extraction_worker (worker.ExtractionWorker): extraction worker. parser_mediator (ParserMediator): parser mediator. storage_writer (StorageWriter): storage writer for a session storage. filter_find_specs (Optional[list[dfvfs.FindSpec]]): find specifications used in path specification extraction.", "response": "def _ProcessSources(\n      self, source_path_specs, extraction_worker, parser_mediator,\n      storage_writer, filter_find_specs=None):\n    \"\"\"Processes the sources.\n\n    Args:\n      source_path_specs (list[dfvfs.PathSpec]): path specifications of\n          the sources to process.\n      extraction_worker (worker.ExtractionWorker): extraction worker.\n      parser_mediator (ParserMediator): parser mediator.\n      storage_writer (StorageWriter): storage writer for a session storage.\n      filter_find_specs (Optional[list[dfvfs.FindSpec]]): find specifications\n          used in path specification extraction.\n    \"\"\"\n    if self._processing_profiler:\n      self._processing_profiler.StartTiming('process_sources')\n\n    number_of_consumed_sources = 0\n\n    self._UpdateStatus(\n        definitions.STATUS_INDICATOR_COLLECTING, '',\n        number_of_consumed_sources, storage_writer)\n\n    display_name = ''\n    path_spec_generator = self._path_spec_extractor.ExtractPathSpecs(\n        source_path_specs, find_specs=filter_find_specs,\n        recurse_file_system=False,\n        resolver_context=parser_mediator.resolver_context)\n\n    for path_spec in path_spec_generator:\n      if self._abort:\n        break\n\n      display_name = parser_mediator.GetDisplayNameForPathSpec(path_spec)\n\n      # TODO: determine if event sources should be DataStream or FileEntry\n      # or both.\n      event_source = event_sources.FileEntryEventSource(path_spec=path_spec)\n      storage_writer.AddEventSource(event_source)\n\n      self._UpdateStatus(\n          definitions.STATUS_INDICATOR_COLLECTING, display_name,\n          number_of_consumed_sources, storage_writer)\n\n    # Force the status update here to make sure the status is up to date.\n    self._UpdateStatus(\n        definitions.STATUS_INDICATOR_RUNNING, display_name,\n        number_of_consumed_sources, storage_writer, force=True)\n\n    if self._processing_profiler:\n      self._processing_profiler.StartTiming('get_event_source')\n\n    event_source = storage_writer.GetFirstWrittenEventSource()\n\n    if self._processing_profiler:\n      self._processing_profiler.StopTiming('get_event_source')\n\n    while event_source:\n      if self._abort:\n        break\n\n      self._ProcessPathSpec(\n          extraction_worker, parser_mediator, event_source.path_spec)\n      number_of_consumed_sources += 1\n\n      if self._guppy_memory_profiler:\n        self._guppy_memory_profiler.Sample()\n\n      self._UpdateStatus(\n          extraction_worker.processing_status, self._current_display_name,\n          number_of_consumed_sources, storage_writer)\n\n      if self._processing_profiler:\n        self._processing_profiler.StartTiming('get_event_source')\n\n      event_source = storage_writer.GetNextWrittenEventSource()\n\n      if self._processing_profiler:\n        self._processing_profiler.StopTiming('get_event_source')\n\n    if self._abort:\n      status = definitions.STATUS_INDICATOR_ABORTED\n    else:\n      status = definitions.STATUS_INDICATOR_COMPLETED\n\n    # Force the status update here to make sure the status is up to date\n    # on exit.\n    self._UpdateStatus(\n        status, '', number_of_consumed_sources, storage_writer, force=True)\n\n    if self._processing_profiler:\n      self._processing_profiler.StopTiming('process_sources')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _UpdateStatus(\n      self, status, display_name, number_of_consumed_sources, storage_writer,\n      force=False):\n    \"\"\"Updates the processing status.\n\n    Args:\n      status (str): human readable status of the processing e.g. 'Idle'.\n      display_name (str): human readable of the file entry currently being\n          processed.\n      number_of_consumed_sources (int): number of consumed sources.\n      storage_writer (StorageWriter): storage writer for a session storage.\n      force (Optional[bool]): True if the update should be forced ignoring\n          the last status update time.\n    \"\"\"\n    current_timestamp = time.time()\n    if not force and current_timestamp < (\n        self._last_status_update_timestamp + self._STATUS_UPDATE_INTERVAL):\n      return\n\n    if status == definitions.STATUS_INDICATOR_IDLE:\n      status = definitions.STATUS_INDICATOR_RUNNING\n\n    used_memory = self._process_information.GetUsedMemory() or 0\n\n    self._processing_status.UpdateForemanStatus(\n        self._name, status, self._pid, used_memory, display_name,\n        number_of_consumed_sources, storage_writer.number_of_event_sources, 0,\n        storage_writer.number_of_events, 0, 0, 0, 0, 0,\n        storage_writer.number_of_warnings)\n\n    if self._status_update_callback:\n      self._status_update_callback(self._processing_status)\n\n    self._last_status_update_timestamp = current_timestamp", "response": "Updates the status of the current process."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ProcessSources(\n      self, source_path_specs, storage_writer, resolver_context,\n      processing_configuration, filter_find_specs=None,\n      status_update_callback=None):\n    \"\"\"Processes the sources.\n\n    Args:\n      source_path_specs (list[dfvfs.PathSpec]): path specifications of\n          the sources to process.\n      storage_writer (StorageWriter): storage writer for a session storage.\n      resolver_context (dfvfs.Context): resolver context.\n      processing_configuration (ProcessingConfiguration): processing\n          configuration.\n      filter_find_specs (Optional[list[dfvfs.FindSpec]]): find specifications\n          used in path specification extraction.\n      status_update_callback (Optional[function]): callback function for status\n          updates.\n\n    Returns:\n      ProcessingStatus: processing status.\n    \"\"\"\n    parser_mediator = parsers_mediator.ParserMediator(\n        storage_writer, self.knowledge_base,\n        artifacts_filter_helper=self._artifacts_filter_helper,\n        preferred_year=processing_configuration.preferred_year,\n        resolver_context=resolver_context,\n        temporary_directory=processing_configuration.temporary_directory)\n\n    parser_mediator.SetEventExtractionConfiguration(\n        processing_configuration.event_extraction)\n\n    parser_mediator.SetInputSourceConfiguration(\n        processing_configuration.input_source)\n\n    extraction_worker = worker.EventExtractionWorker(\n        parser_filter_expression=(\n            processing_configuration.parser_filter_expression))\n\n    extraction_worker.SetExtractionConfiguration(\n        processing_configuration.extraction)\n\n    self._processing_configuration = processing_configuration\n    self._status_update_callback = status_update_callback\n\n    logger.debug('Processing started.')\n\n    parser_mediator.StartProfiling(\n        self._processing_configuration.profiling, self._name,\n        self._process_information)\n    self._StartProfiling(self._processing_configuration.profiling)\n\n    if self._processing_profiler:\n      extraction_worker.SetProcessingProfiler(self._processing_profiler)\n\n    if self._serializers_profiler:\n      storage_writer.SetSerializersProfiler(self._serializers_profiler)\n\n    if self._storage_profiler:\n      storage_writer.SetStorageProfiler(self._storage_profiler)\n\n    storage_writer.Open()\n    storage_writer.WriteSessionStart()\n\n    try:\n      storage_writer.WritePreprocessingInformation(self.knowledge_base)\n\n      self._ProcessSources(\n          source_path_specs, extraction_worker, parser_mediator,\n          storage_writer, filter_find_specs=filter_find_specs)\n\n    finally:\n      storage_writer.WriteSessionCompletion(aborted=self._abort)\n\n      storage_writer.Close()\n\n      if self._processing_profiler:\n        extraction_worker.SetProcessingProfiler(None)\n\n      if self._serializers_profiler:\n        storage_writer.SetSerializersProfiler(None)\n\n      if self._storage_profiler:\n        storage_writer.SetStorageProfiler(None)\n\n      self._StopProfiling()\n      parser_mediator.StopProfiling()\n\n    if self._abort:\n      logger.debug('Processing aborted.')\n      self._processing_status.aborted = True\n    else:\n      logger.debug('Processing completed.')\n\n    self._processing_configuration = None\n    self._status_update_callback = None\n\n    return self._processing_status", "response": "Processes the sources.\n\n    Args:\n      source_path_specs (list[dfvfs.PathSpec]): path specifications of\n          the sources to process.\n      storage_writer (StorageWriter): storage writer for a session storage.\n      resolver_context (dfvfs.Context): resolver context.\n      processing_configuration (ProcessingConfiguration): processing\n          configuration.\n      filter_find_specs (Optional[list[dfvfs.FindSpec]]): find specifications\n          used in path specification extraction.\n      status_update_callback (Optional[function]): callback function for status\n          updates.\n\n    Returns:\n      ProcessingStatus: processing status."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef GetMessages(self, formatter_mediator, event):\n    if self.DATA_TYPE != event.data_type:\n      raise errors.WrongFormatter(\n          'Invalid event object - unsupported data type: {0:s}'.format(\n              event.data_type))\n\n    event_values = event.CopyToDict()\n\n    number_of_volumes = event_values.get('number_of_volumes', 0)\n    volume_serial_numbers = event_values.get('volume_serial_numbers', None)\n    volume_device_paths = event_values.get('volume_device_paths', None)\n    volumes_strings = []\n    for volume_index in range(0, number_of_volumes):\n      if not volume_serial_numbers:\n        volume_serial_number = 'UNKNOWN'\n      else:\n        volume_serial_number = volume_serial_numbers[volume_index]\n\n      if not volume_device_paths:\n        volume_device_path = 'UNKNOWN'\n      else:\n        volume_device_path = volume_device_paths[volume_index]\n\n      volumes_strings.append((\n          'volume: {0:d} [serial number: 0x{1:08X}, device path: '\n          '{2:s}]').format(\n              volume_index + 1, volume_serial_number, volume_device_path))\n\n    if volumes_strings:\n      event_values['volumes_string'] = ', '.join(volumes_strings)\n\n    return self._ConditionalFormatMessages(event_values)", "response": "Determines the formatted message strings for an event object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread a date time attribute value.", "response": "def _ReadAttributeValueDateTime(\n      self, attribute_values_data, record_offset, attribute_values_data_offset,\n      attribute_value_offset):\n    \"\"\"Reads a date time attribute value.\n\n    Args:\n      attribute_values_data (bytes): attribute values data.\n      record_offset (int): offset of the record relative to the start of\n          the file.\n      attribute_values_data_offset (int): offset of the attribute values data\n          relative to the start of the record.\n      attribute_value_offset (int): offset of the attribute relative to\n          the start of the record.\n\n    Returns:\n      str: date and time values.\n\n    Raises:\n      ParseError: if the attribute value cannot be read.\n    \"\"\"\n    if attribute_value_offset == 0:\n      return None\n\n    data_type_map = self._GetDataTypeMap('keychain_date_time')\n\n    file_offset = (\n        record_offset + attribute_values_data_offset + attribute_value_offset)\n\n    attribute_value_offset -= attribute_values_data_offset + 1\n    attribute_value_data = attribute_values_data[attribute_value_offset:]\n\n    try:\n      date_time_attribute_value = self._ReadStructureFromByteStream(\n          attribute_value_data, file_offset, data_type_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.ParseError((\n          'Unable to map date time attribute value data at offset: 0x{0:08x} '\n          'with error: {1!s}').format(file_offset, exception))\n\n    return date_time_attribute_value.date_time.rstrip('\\x00')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _ReadAttributeValueInteger(\n      self, attribute_values_data, record_offset, attribute_values_data_offset,\n      attribute_value_offset):\n    \"\"\"Reads an integer attribute value.\n\n    Args:\n      attribute_values_data (bytes): attribute values data.\n      record_offset (int): offset of the record relative to the start of\n          the file.\n      attribute_values_data_offset (int): offset of the attribute values data\n          relative to the start of the record.\n      attribute_value_offset (int): offset of the attribute relative to\n          the start of the record.\n\n    Returns:\n      int: integer value or None if attribute value offset is not set.\n\n    Raises:\n      ParseError: if the attribute value cannot be read.\n    \"\"\"\n    if attribute_value_offset == 0:\n      return None\n\n    data_type_map = self._GetDataTypeMap('uint32be')\n\n    file_offset = (\n        record_offset + attribute_values_data_offset + attribute_value_offset)\n\n    attribute_value_offset -= attribute_values_data_offset + 1\n    attribute_value_data = attribute_values_data[attribute_value_offset:]\n\n    try:\n      return self._ReadStructureFromByteStream(\n          attribute_value_data, file_offset, data_type_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.ParseError((\n          'Unable to map integer attribute value data at offset: 0x{0:08x} '\n          'with error: {1!s}').format(file_offset, exception))", "response": "Reads an integer attribute value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _ReadAttributeValueString(\n      self, attribute_values_data, record_offset, attribute_values_data_offset,\n      attribute_value_offset):\n    \"\"\"Reads a string attribute value.\n\n    Args:\n      attribute_values_data (bytes): attribute values data.\n      record_offset (int): offset of the record relative to the start of\n          the file.\n      attribute_values_data_offset (int): offset of the attribute values data\n          relative to the start of the record.\n      attribute_value_offset (int): offset of the attribute relative to\n          the start of the record.\n\n    Returns:\n      str: string value or None if attribute value offset is not set.\n\n    Raises:\n      ParseError: if the attribute value cannot be read.\n    \"\"\"\n    if attribute_value_offset == 0:\n      return None\n\n    data_type_map = self._GetDataTypeMap('keychain_string')\n\n    file_offset = (\n        record_offset + attribute_values_data_offset + attribute_value_offset)\n\n    attribute_value_offset -= attribute_values_data_offset + 1\n    attribute_value_data = attribute_values_data[attribute_value_offset:]\n\n    try:\n      string_attribute_value = self._ReadStructureFromByteStream(\n          attribute_value_data, file_offset, data_type_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.ParseError((\n          'Unable to map string attribute value data at offset: 0x{0:08x} '\n          'with error: {1!s}').format(file_offset, exception))\n\n    return string_attribute_value.string", "response": "Reads a string attribute value from the attribute values data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading the file header.", "response": "def _ReadFileHeader(self, file_object):\n    \"\"\"Reads the file header.\n\n    Args:\n      file_object (file): file-like object.\n\n    Returns:\n      keychain_file_header: file header.\n\n    Raises:\n      ParseError: if the file header cannot be read.\n    \"\"\"\n    data_type_map = self._GetDataTypeMap('keychain_file_header')\n\n    file_header, _ = self._ReadStructureFromFileObject(\n        file_object, 0, data_type_map)\n\n    if file_header.signature != self._FILE_SIGNATURE:\n      raise errors.ParseError('Unsupported file signature.')\n\n    if (file_header.major_format_version != self._MAJOR_VERSION or\n        file_header.minor_format_version != self._MINOR_VERSION):\n      raise errors.ParseError('Unsupported format version: {0:s}.{1:s}'.format(\n          file_header.major_format_version, file_header.minor_format_version))\n\n    return file_header"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _ReadRecord(self, tables, file_object, record_offset, record_type):\n    table = tables.get(record_type, None)\n    if not table:\n      raise errors.ParseError(\n          'Missing table for relation identifier: 0x{0:08}'.format(record_type))\n\n    record_header = self._ReadRecordHeader(file_object, record_offset)\n\n    record = collections.OrderedDict()\n\n    if table.columns:\n      attribute_value_offsets = self._ReadRecordAttributeValueOffset(\n          file_object, record_offset + 24, len(table.columns))\n\n    file_offset = file_object.tell()\n    record_data_offset = file_offset - record_offset\n    record_data_size = record_header.data_size - (file_offset - record_offset)\n    record_data = file_object.read(record_data_size)\n\n    if record_header.key_data_size > 0:\n      record['_key_'] = record_data[:record_header.key_data_size]\n\n    if table.columns:\n      for index, column in enumerate(table.columns):\n        attribute_data_read_function = self._ATTRIBUTE_DATA_READ_FUNCTIONS.get(\n            column.attribute_data_type, None)\n        if attribute_data_read_function:\n          attribute_data_read_function = getattr(\n              self, attribute_data_read_function, None)\n\n        if not attribute_data_read_function:\n          attribute_value = None\n        else:\n          attribute_value = attribute_data_read_function(\n              record_data, record_offset, record_data_offset,\n              attribute_value_offsets[index])\n\n        record[column.attribute_name] = attribute_value\n\n    table.records.append(record)", "response": "Reads a record from the file - like object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading the record attribute value offsets.", "response": "def _ReadRecordAttributeValueOffset(\n      self, file_object, file_offset, number_of_attribute_values):\n    \"\"\"Reads the record attribute value offsets.\n\n    Args:\n      file_object (file): file-like object.\n      file_offset (int): offset of the record attribute values offsets relative\n          to the start of the file.\n      number_of_attribute_values (int): number of attribute values.\n\n    Returns:\n      keychain_record_attribute_value_offsets: record attribute value offsets.\n\n    Raises:\n      ParseError: if the record attribute value offsets cannot be read.\n    \"\"\"\n    offsets_data_size = number_of_attribute_values * 4\n\n    offsets_data = file_object.read(offsets_data_size)\n\n    context = dtfabric_data_maps.DataTypeMapContext(values={\n        'number_of_attribute_values': number_of_attribute_values})\n\n    data_type_map = self._GetDataTypeMap(\n        'keychain_record_attribute_value_offsets')\n\n    try:\n      attribute_value_offsets = self._ReadStructureFromByteStream(\n          offsets_data, file_offset, data_type_map, context=context)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.ParseError((\n          'Unable to map record attribute value offsets data at offset: '\n          '0x{0:08x} with error: {1!s}').format(file_offset, exception))\n\n    return attribute_value_offsets"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _ReadRecordHeader(self, file_object, record_header_offset):\n    data_type_map = self._GetDataTypeMap('keychain_record_header')\n\n    record_header, _ = self._ReadStructureFromFileObject(\n        file_object, record_header_offset, data_type_map)\n\n    return record_header", "response": "Reads the record header."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _ReadRecordSchemaAttributes(self, tables, file_object, record_offset):\n    record_header = self._ReadRecordHeader(file_object, record_offset)\n\n    attribute_value_offsets = self._ReadRecordAttributeValueOffset(\n        file_object, record_offset + 24, 6)\n\n    file_offset = file_object.tell()\n    attribute_values_data_offset = file_offset - record_offset\n    attribute_values_data_size = record_header.data_size - (\n        file_offset - record_offset)\n    attribute_values_data = file_object.read(attribute_values_data_size)\n\n    relation_identifier = self._ReadAttributeValueInteger(\n        attribute_values_data, record_offset, attribute_values_data_offset,\n        attribute_value_offsets[0])\n\n    attribute_identifier = self._ReadAttributeValueInteger(\n        attribute_values_data, record_offset, attribute_values_data_offset,\n        attribute_value_offsets[1])\n\n    attribute_name_data_type = self._ReadAttributeValueInteger(\n        attribute_values_data, record_offset, attribute_values_data_offset,\n        attribute_value_offsets[2])\n\n    attribute_name = self._ReadAttributeValueString(\n        attribute_values_data, record_offset, attribute_values_data_offset,\n        attribute_value_offsets[3])\n\n    # TODO: handle attribute_value_offsets[4]\n\n    attribute_data_type = self._ReadAttributeValueInteger(\n        attribute_values_data, record_offset, attribute_values_data_offset,\n        attribute_value_offsets[5])\n\n    table = tables.get(relation_identifier, None)\n    if not table:\n      raise errors.ParseError(\n          'Missing table for relation identifier: 0x{0:08}'.format(\n              relation_identifier))\n\n    if attribute_name is None and attribute_value_offsets[1] != 0:\n      attribute_value_offset = attribute_value_offsets[1]\n      attribute_value_offset -= attribute_values_data_offset + 1\n      attribute_name = attribute_values_data[\n          attribute_value_offset:attribute_value_offset + 4]\n      attribute_name = attribute_name.decode('ascii')\n\n    column = KeychainDatabaseColumn()\n    column.attribute_data_type = attribute_data_type\n    column.attribute_identifier = attribute_identifier\n    column.attribute_name = attribute_name\n\n    table.columns.append(column)\n\n    table = tables.get(self._RECORD_TYPE_CSSM_DL_DB_SCHEMA_ATTRIBUTES, None)\n    if not table:\n      raise errors.ParseError('Missing CSSM_DL_DB_SCHEMA_ATTRIBUTES table.')\n\n    record = collections.OrderedDict({\n        'RelationID': relation_identifier,\n        'AttributeID': attribute_identifier,\n        'AttributeNameFormat': attribute_name_data_type,\n        'AttributeName': attribute_name,\n        'AttributeFormat': attribute_data_type})\n\n    table.records.append(record)", "response": "Reads a schema attributes from a CSSM_DL_DB_SCHEMA_ATTRIBUTES record."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread a schema indexes record.", "response": "def _ReadRecordSchemaIndexes(self, tables, file_object, record_offset):\n    \"\"\"Reads a schema indexes (CSSM_DL_DB_SCHEMA_INDEXES) record.\n\n    Args:\n      tables (dict[int, KeychainDatabaseTable]): tables per identifier.\n      file_object (file): file-like object.\n      record_offset (int): offset of the record relative to the start of\n          the file.\n\n    Raises:\n      ParseError: if the record cannot be read.\n    \"\"\"\n    _ = self._ReadRecordHeader(file_object, record_offset)\n\n    attribute_value_offsets = self._ReadRecordAttributeValueOffset(\n        file_object, record_offset + 24, 5)\n\n    if attribute_value_offsets != (0x2d, 0x31, 0x35, 0x39, 0x3d):\n      raise errors.ParseError('Unsupported record attribute value offsets')\n\n    file_offset = file_object.tell()\n    data_type_map = self._GetDataTypeMap('keychain_record_schema_indexes')\n\n    record_values, _ = self._ReadStructureFromFileObject(\n        file_object, file_offset, data_type_map)\n\n    if record_values.relation_identifier not in tables:\n      raise errors.ParseError(\n          'CSSM_DL_DB_SCHEMA_INDEXES defines relation identifier not defined '\n          'in CSSM_DL_DB_SCHEMA_INFO.')\n\n    table = tables.get(self._RECORD_TYPE_CSSM_DL_DB_SCHEMA_INDEXES, None)\n    if not table:\n      raise errors.ParseError('Missing CSSM_DL_DB_SCHEMA_INDEXES table.')\n\n    record = collections.OrderedDict({\n        'RelationID': record_values.relation_identifier,\n        'IndexID': record_values.index_identifier,\n        'AttributeID': record_values.attribute_identifier,\n        'IndexType': record_values.index_type,\n        'IndexedDataLocation': record_values.index_data_location})\n\n    table.records.append(record)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads a schema information record.", "response": "def _ReadRecordSchemaInformation(self, tables, file_object, record_offset):\n    \"\"\"Reads a schema information (CSSM_DL_DB_SCHEMA_INFO) record.\n\n    Args:\n      tables (dict[int, KeychainDatabaseTable]): tables per identifier.\n      file_object (file): file-like object.\n      record_offset (int): offset of the record relative to the start of\n          the file.\n\n    Raises:\n      ParseError: if the record cannot be read.\n    \"\"\"\n    _ = self._ReadRecordHeader(file_object, record_offset)\n\n    attribute_value_offsets = self._ReadRecordAttributeValueOffset(\n        file_object, record_offset + 24, 2)\n\n    if attribute_value_offsets != (0x21, 0x25):\n      raise errors.ParseError('Unsupported record attribute value offsets')\n\n    file_offset = file_object.tell()\n    data_type_map = self._GetDataTypeMap('keychain_record_schema_information')\n\n    record_values, _ = self._ReadStructureFromFileObject(\n        file_object, file_offset, data_type_map)\n\n    relation_name = record_values.relation_name.decode('ascii')\n\n    table = KeychainDatabaseTable()\n    table.relation_identifier = record_values.relation_identifier\n    table.relation_name = relation_name\n\n    tables[table.relation_identifier] = table\n\n    table = tables.get(self._RECORD_TYPE_CSSM_DL_DB_SCHEMA_INFO, None)\n    if not table:\n      raise errors.ParseError('Missing CSSM_DL_DB_SCHEMA_INFO table.')\n\n    record = collections.OrderedDict({\n        'RelationID': record_values.relation_identifier,\n        'RelationName': relation_name})\n\n    table.records.append(record)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads the table. Args: tables (dict[int, KeychainDatabaseTable]): tables per identifier. file_object (file): file-like object. table_offset (int): offset of the table relative to the start of the file. Raises: ParseError: if the table cannot be read.", "response": "def _ReadTable(self, tables, file_object, table_offset):\n    \"\"\"Reads the table.\n\n    Args:\n      tables (dict[int, KeychainDatabaseTable]): tables per identifier.\n      file_object (file): file-like object.\n      table_offset (int): offset of the table relative to the start of\n          the file.\n\n    Raises:\n      ParseError: if the table cannot be read.\n    \"\"\"\n    table_header = self._ReadTableHeader(file_object, table_offset)\n\n    for record_offset in table_header.record_offsets:\n      if record_offset == 0:\n        continue\n\n      record_offset += table_offset\n\n      if table_header.record_type == self._RECORD_TYPE_CSSM_DL_DB_SCHEMA_INFO:\n        self._ReadRecordSchemaInformation(tables, file_object, record_offset)\n      elif table_header.record_type == (\n          self._RECORD_TYPE_CSSM_DL_DB_SCHEMA_INDEXES):\n        self._ReadRecordSchemaIndexes(tables, file_object, record_offset)\n      elif table_header.record_type == (\n          self._RECORD_TYPE_CSSM_DL_DB_SCHEMA_ATTRIBUTES):\n        self._ReadRecordSchemaAttributes(tables, file_object, record_offset)\n      else:\n        self._ReadRecord(\n            tables, file_object, record_offset, table_header.record_type)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _ReadTableHeader(self, file_object, table_header_offset):\n    data_type_map = self._GetDataTypeMap('keychain_table_header')\n\n    table_header, _ = self._ReadStructureFromFileObject(\n        file_object, table_header_offset, data_type_map)\n\n    return table_header", "response": "Reads the table header."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread the tables array.", "response": "def _ReadTablesArray(self, file_object, tables_array_offset):\n    \"\"\"Reads the tables array.\n\n    Args:\n      file_object (file): file-like object.\n      tables_array_offset (int): offset of the tables array relative to\n          the start of the file.\n\n    Returns:\n      dict[int, KeychainDatabaseTable]: tables per identifier.\n\n    Raises:\n      ParseError: if the tables array cannot be read.\n    \"\"\"\n    # TODO: implement https://github.com/libyal/dtfabric/issues/12 and update\n    # keychain_tables_array definition.\n\n    data_type_map = self._GetDataTypeMap('keychain_tables_array')\n\n    tables_array, _ = self._ReadStructureFromFileObject(\n        file_object, tables_array_offset, data_type_map)\n\n    tables = collections.OrderedDict()\n    for table_offset in tables_array.table_offsets:\n      self._ReadTable(tables, file_object, tables_array_offset + table_offset)\n\n    return tables"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing a date time value.", "response": "def _ParseDateTimeValue(self, parser_mediator, date_time_value):\n    \"\"\"Parses a date time value.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      date_time_value (str): date time value\n          (CSSM_DB_ATTRIBUTE_FORMAT_TIME_DATE) in the format: \"YYYYMMDDhhmmssZ\".\n\n    Returns:\n      dfdatetime.TimeElements: date and time extracted from the value or None\n          if the value does not represent a valid string.\n    \"\"\"\n    if date_time_value[14] != 'Z':\n      parser_mediator.ProduceExtractionWarning(\n          'invalid date and time value: {0!s}'.format(date_time_value))\n      return None\n\n    try:\n      year = int(date_time_value[0:4], 10)\n      month = int(date_time_value[4:6], 10)\n      day_of_month = int(date_time_value[6:8], 10)\n      hours = int(date_time_value[8:10], 10)\n      minutes = int(date_time_value[10:12], 10)\n      seconds = int(date_time_value[12:14], 10)\n    except (TypeError, ValueError):\n      parser_mediator.ProduceExtractionWarning(\n          'invalid date and time value: {0!s}'.format(date_time_value))\n      return None\n\n    time_elements_tuple = (year, month, day_of_month, hours, minutes, seconds)\n\n    try:\n      return dfdatetime_time_elements.TimeElements(\n          time_elements_tuple=time_elements_tuple)\n    except ValueError:\n      parser_mediator.ProduceExtractionWarning(\n          'invalid date and time value: {0!s}'.format(date_time_value))\n      return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _ParseBinaryDataAsString(self, parser_mediator, binary_data_value):\n    if not binary_data_value:\n      return None\n\n    try:\n      return binary_data_value.decode('utf-8')\n    except UnicodeDecodeError:\n      parser_mediator.ProduceExtractionWarning(\n          'invalid binary data string value: {0:s}'.format(\n              repr(binary_data_value)))\n      return None", "response": "Parses a binary data value as a string."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _ParseApplicationPasswordRecord(self, parser_mediator, record):\n    key = record.get('_key_', None)\n    if not key or not key.startswith(b'ssgp'):\n      raise errors.ParseError((\n          'Unsupported application password record key value does not start '\n          'with: \"ssgp\".'))\n\n    event_data = KeychainApplicationRecordEventData()\n    event_data.account_name = self._ParseBinaryDataAsString(\n        parser_mediator, record['acct'])\n    event_data.comments = self._ParseBinaryDataAsString(\n        parser_mediator, record['crtr'])\n    event_data.entry_name = self._ParseBinaryDataAsString(\n        parser_mediator, record['PrintName'])\n    ssgp_hash = codecs.encode(key[4:], 'hex')\n    event_data.ssgp_hash = codecs.decode(ssgp_hash, 'utf-8')\n    event_data.text_description = self._ParseBinaryDataAsString(\n        parser_mediator, record['desc'])\n\n    date_time = self._ParseDateTimeValue(parser_mediator, record['cdat'])\n    if date_time:\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_CREATION)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    date_time = self._ParseDateTimeValue(parser_mediator, record['mdat'])\n    if date_time:\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_MODIFICATION)\n      parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Extracts the information from an application password record."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _ParseInternetPasswordRecord(self, parser_mediator, record):\n    key = record.get('_key_', None)\n    if not key or not key.startswith(b'ssgp'):\n      raise errors.ParseError((\n          'Unsupported Internet password record key value does not start '\n          'with: \"ssgp\".'))\n\n    protocol_string = codecs.decode('{0:08x}'.format(record['ptcl']), 'hex')\n    protocol_string = codecs.decode(protocol_string, 'utf-8')\n\n    event_data = KeychainInternetRecordEventData()\n    event_data.account_name = self._ParseBinaryDataAsString(\n        parser_mediator, record['acct'])\n    event_data.comments = self._ParseBinaryDataAsString(\n        parser_mediator, record['crtr'])\n    event_data.entry_name = self._ParseBinaryDataAsString(\n        parser_mediator, record['PrintName'])\n    event_data.protocol = self._PROTOCOL_TRANSLATION_DICT.get(\n        protocol_string, protocol_string)\n    ssgp_hash = codecs.encode(key[4:], 'hex')\n    event_data.ssgp_hash = codecs.decode(ssgp_hash, 'utf-8')\n    event_data.text_description = self._ParseBinaryDataAsString(\n        parser_mediator, record['desc'])\n    event_data.type_protocol = self._ParseBinaryDataAsString(\n        parser_mediator, record['atyp'])\n    event_data.where = self._ParseBinaryDataAsString(\n        parser_mediator, record['srvr'])\n\n    date_time = self._ParseDateTimeValue(parser_mediator, record['cdat'])\n    if date_time:\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_CREATION)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    date_time = self._ParseDateTimeValue(parser_mediator, record['mdat'])\n    if date_time:\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_MODIFICATION)\n      parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Extracts the information from an Internet password record."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ParseFileObject(self, parser_mediator, file_object):\n    try:\n      file_header = self._ReadFileHeader(file_object)\n    except (ValueError, errors.ParseError):\n      raise errors.UnableToParseFile('Unable to parse file header.')\n\n    tables = self._ReadTablesArray(file_object, file_header.tables_array_offset)\n\n    table = tables.get(self._RECORD_TYPE_APPLICATION_PASSWORD, None)\n    if table:\n      for record in table.records:\n        self._ParseApplicationPasswordRecord(parser_mediator, record)\n\n    table = tables.get(self._RECORD_TYPE_INTERNET_PASSWORD, None)\n    if table:\n      for record in table.records:\n        self._ParseInternetPasswordRecord(parser_mediator, record)", "response": "Parses a MacOS keychain file - like object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nformat the date and time of the event.", "response": "def _FormatDateTime(self, event):\n    \"\"\"Formats the date and time.\n\n    Args:\n      event (EventObject): event.\n\n    Returns:\n      str: date and time string or \"N/A\" if no event timestamp is available.\n    \"\"\"\n    if not event.timestamp:\n      return 'N/A'\n\n    # TODO: preserve dfdatetime as an object.\n    # TODO: add support for self._output_mediator.timezone\n    date_time = dfdatetime_posix_time.PosixTimeInMicroseconds(\n        timestamp=event.timestamp)\n\n    year, month, day_of_month = date_time.GetDate()\n    hours, minutes, seconds = date_time.GetTimeOfDay()\n\n    try:\n      return '{0:04d}-{1:02d}-{2:02d} {3:02d}:{4:02d}:{5:02d}'.format(\n          year, month, day_of_month, hours, minutes, seconds)\n    except (TypeError, ValueError):\n      self._ReportEventError(event, (\n          'unable to copy timestamp: {0!s} to a human readable date and '\n          'time. Defaulting to: \"0000-00-00 00:00:00\"').format(event.timestamp))\n\n      return '0000-00-00 00:00:00'"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsanitizes the event for use in 4n6time.", "response": "def _GetSanitizedEventValues(self, event):\n    \"\"\"Sanitizes the event for use in 4n6time.\n\n    Args:\n      event (EventObject): event.\n\n    Returns:\n      dict[str, object]: dictionary containing the sanitized event values.\n\n    Raises:\n      NoFormatterFound: If no event formatter can be found to match the data\n          type in the event object.\n    \"\"\"\n    data_type = getattr(event, 'data_type', 'UNKNOWN')\n\n    event_formatter = self._output_mediator.GetEventFormatter(event)\n    if not event_formatter:\n      raise errors.NoFormatterFound(\n          'Unable to find event formatter for: {0:s}.'.format(data_type))\n\n    message, _ = self._output_mediator.GetFormattedMessages(event)\n    if message is None:\n      raise errors.NoFormatterFound(\n          'Unable to find event formatter for: {0:s}.'.format(data_type))\n\n    source_short, source = self._output_mediator.GetFormattedSources(event)\n    if source is None or source_short is None:\n      raise errors.NoFormatterFound(\n          'Unable to find event formatter for: {0:s}.'.format(data_type))\n\n    datetime_string = self._FormatDateTime(event)\n\n    format_variables = self._output_mediator.GetFormatStringAttributeNames(\n        event)\n    if format_variables is None:\n      raise errors.NoFormatterFound(\n          'Unable to find event formatter for: {0:s}.'.format(data_type))\n\n    extra_attributes = []\n    for attribute_name, attribute_value in sorted(event.GetAttributes()):\n      if (attribute_name in definitions.RESERVED_VARIABLE_NAMES or\n          attribute_name in format_variables):\n        continue\n      extra_attributes.append(\n          '{0:s}: {1!s} '.format(attribute_name, attribute_value))\n\n    extra_attributes = ' '.join(extra_attributes)\n\n    inode = event.inode\n    if inode is None and hasattr(event, 'pathspec'):\n      inode = getattr(event.pathspec, 'inode', '-')\n    if inode is None:\n      inode = '-'\n\n    tags = None\n    if getattr(event, 'tag', None):\n      tags = getattr(event.tag, 'tags', None)\n\n    taglist = ''\n    if isinstance(tags, (list, tuple)):\n      taglist = ','.join(tags)\n\n    offset = event.offset\n    if offset is None:\n      offset = 0\n\n    row = {\n        'timezone': '{0!s}'.format(self._output_mediator.timezone),\n        'MACB': self._output_mediator.GetMACBRepresentation(event),\n        'source': source_short,\n        'sourcetype': source,\n        'type': event.timestamp_desc or '-',\n        'user': getattr(event, 'username', '-'),\n        'host': getattr(event, 'hostname', '-'),\n        'description': message,\n        'filename': getattr(event, 'filename', '-'),\n        'inode': inode,\n        'notes': getattr(event, 'notes', '-'),\n        'format': getattr(event, 'parser', '-'),\n        'extra': extra_attributes,\n        'datetime': datetime_string,\n        'reportnotes': '',\n        'inreport': '',\n        'tag': taglist,\n        'offset': offset,\n        'vss_store_number': self._GetVSSNumber(event),\n        'URL': getattr(event, 'url', '-'),\n        'record_number': getattr(event, 'record_number', 0),\n        'event_identifier': getattr(event, 'event_identifier', '-'),\n        'event_type': getattr(event, 'event_type', '-'),\n        'source_name': getattr(event, 'source_name', '-'),\n        'user_sid': getattr(event, 'user_sid', '-'),\n        'computer_name': getattr(event, 'computer_name', '-'),\n        'evidence': self._evidence}\n\n    return row"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _GetDistinctValues(self, field_name):\n    self._cursor.execute(\n        'SELECT {0:s}, COUNT({0:s}) FROM log2timeline GROUP BY {0:s}'.format(\n            field_name))\n\n    result = {}\n    row = self._cursor.fetchone()\n    while row:\n      if row[0]:\n        result[row[0]] = row[1]\n      row = self._cursor.fetchone()\n    return result", "response": "Query database for unique field types."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nquery database for unique tag types.", "response": "def _ListTags(self):\n    \"\"\"Query database for unique tag types.\"\"\"\n    all_tags = []\n    self._cursor.execute('SELECT DISTINCT tag FROM log2timeline')\n\n    # This cleans up the messy SQL return.\n    tag_row = self._cursor.fetchone()\n    while tag_row:\n      tag_string = tag_row[0]\n      if tag_string:\n        tags = tag_string.split(',')\n        for tag in tags:\n          if tag not in all_tags:\n            all_tags.append(tag)\n      tag_row = self._cursor.fetchone()\n    # TODO: make this method an iterator.\n    return all_tags"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef Close(self):\n    # Build up indices for the fields specified in the args.\n    # It will commit the inserts automatically before creating index.\n    if not self._append:\n      for field_name in self._fields:\n        query = 'CREATE INDEX {0:s}_idx ON log2timeline ({0:s})'.format(\n            field_name)\n        self._cursor.execute(query)\n        if self._set_status:\n          self._set_status('Created index: {0:s}'.format(field_name))\n\n    # Get meta info and save into their tables.\n    if self._set_status:\n      self._set_status('Creating metadata...')\n\n    for field in self._META_FIELDS:\n      values = self._GetDistinctValues(field)\n      self._cursor.execute('DELETE FROM l2t_{0:s}s'.format(field))\n      for name, frequency in iter(values.items()):\n        self._cursor.execute((\n            'INSERT INTO l2t_{0:s}s ({0:s}s, frequency) '\n            'VALUES(\"{1:s}\", {2:d}) ').format(field, name, frequency))\n    self._cursor.execute('DELETE FROM l2t_tags')\n    for tag in self._ListTags():\n      self._cursor.execute('INSERT INTO l2t_tags (tag) VALUES (?)', [tag])\n\n    if self._set_status:\n      self._set_status('Database created.')\n\n    self._connection.commit()\n    self._cursor.close()\n    self._connection.close()\n\n    self._cursor = None\n    self._connection = None", "response": "Disconnects from the database."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nopen the database and creates the required tables.", "response": "def Open(self):\n    \"\"\"Connects to the database and creates the required tables.\n\n    Raises:\n      IOError: if the specified output file already exists.\n      OSError: if the specified output file already exists.\n      ValueError: if the filename is not set.\n    \"\"\"\n    if not self._filename:\n      raise ValueError('Missing filename.')\n\n    if not self._append and os.path.isfile(self._filename):\n      raise IOError((\n          'Unable to use an already existing file for output '\n          '[{0:s}]').format(self._filename))\n\n    self._connection = sqlite3.connect(self._filename)\n    self._cursor = self._connection.cursor()\n\n    # Create table in database.\n    if not self._append:\n      self._cursor.execute(self._CREATE_TABLE_QUERY)\n\n      for field in self._META_FIELDS:\n        query = 'CREATE TABLE l2t_{0:s}s ({0:s}s TEXT, frequency INT)'.format(\n            field)\n        self._cursor.execute(query)\n        if self._set_status:\n          self._set_status('Created table: l2t_{0:s}'.format(field))\n\n      self._cursor.execute('CREATE TABLE l2t_tags (tag TEXT)')\n      if self._set_status:\n        self._set_status('Created table: l2t_tags')\n\n      query = 'CREATE TABLE l2t_saved_query (name TEXT, query TEXT)'\n      self._cursor.execute(query)\n      if self._set_status:\n        self._set_status('Created table: l2t_saved_query')\n\n      query = (\n          'CREATE TABLE l2t_disk (disk_type INT, mount_path TEXT, '\n          'dd_path TEXT, dd_offset TEXT, storage_file TEXT, export_path TEXT)')\n      self._cursor.execute(query)\n\n      query = (\n          'INSERT INTO l2t_disk (disk_type, mount_path, dd_path, dd_offset, '\n          'storage_file, export_path) VALUES (0, \"\", \"\", \"\", \"\", \"\")')\n      self._cursor.execute(query)\n      if self._set_status:\n        self._set_status('Created table: l2t_disk')\n\n    self._count = 0"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef WriteEventBody(self, event):\n    # sqlite seems to support milli seconds precision but that seems\n    # not to be used by 4n6time\n    row = self._GetSanitizedEventValues(event)\n\n    self._cursor.execute(self._INSERT_QUERY, row)\n    self._count += 1\n    # Commit the current transaction every 10000 inserts.\n    if self._count % 10000 == 0:\n      self._connection.commit()\n      if self._set_status:\n        self._set_status('Inserting event: {0:d}'.format(self._count))", "response": "Writes the body of an event to the output."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nbuild a list of dfvfs. FindSpec objects from a filter file.", "response": "def BuildFindSpecs(self, environment_variables=None):\n    \"\"\"Build find specification from a filter file.\n\n    Args:\n      environment_variables (Optional[list[EnvironmentVariableArtifact]]):\n          environment variables.\n\n    Returns:\n      list[dfvfs.FindSpec]: find specification.\n    \"\"\"\n    path_attributes = {}\n    if environment_variables:\n      for environment_variable in environment_variables:\n        attribute_name = environment_variable.name.lower()\n        attribute_value = environment_variable.value\n        if not isinstance(attribute_value, py2to3.STRING_TYPES):\n          continue\n\n        # Remove the drive letter.\n        if len(attribute_value) > 2 and attribute_value[1] == ':':\n          _, _, attribute_value = attribute_value.rpartition(':')\n\n        if attribute_value.startswith('\\\\'):\n          attribute_value = attribute_value.replace('\\\\', '/')\n\n        path_attributes[attribute_name] = attribute_value\n\n    find_specs = []\n    with open(self._path, 'r') as file_object:\n      for line in file_object:\n        line = line.strip()\n        if line.startswith('#'):\n          continue\n\n        if path_attributes:\n          try:\n            line = line.format(**path_attributes)\n          except KeyError as exception:\n            logger.error((\n                'Unable to expand path filter: {0:s} with error: '\n                '{1!s}').format(line, exception))\n            continue\n\n        if not line.startswith('/'):\n          logger.warning((\n              'The path filter must be defined as an absolute path: '\n              '{0:s}').format(line))\n          continue\n\n        # Convert the path filters into a list of path segments and strip\n        # the root path segment.\n        path_segments = line.split('/')\n        path_segments.pop(0)\n\n        if not path_segments[-1]:\n          logger.warning(\n              'Empty last path segment in path filter: {0:s}'.format(line))\n          continue\n\n        find_spec = file_system_searcher.FindSpec(\n            location_regex=path_segments, case_sensitive=False)\n        find_specs.append(find_spec)\n\n    return find_specs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _GetParserFilters(cls, parser_filter_expression):\n    if not parser_filter_expression:\n      return {}, {}\n\n    includes = {}\n    excludes = {}\n\n    preset_names = cls._presets.GetNames()\n\n    for parser_filter in parser_filter_expression.split(','):\n      parser_filter = parser_filter.strip()\n      if not parser_filter:\n        continue\n\n      if parser_filter.startswith('!'):\n        parser_filter = parser_filter[1:]\n        active_dict = excludes\n      else:\n        active_dict = includes\n\n      parser_filter = parser_filter.lower()\n      if parser_filter in preset_names:\n        for parser_in_category in cls._GetParsersFromPresetCategory(\n            parser_filter):\n          parser, _, plugin = parser_in_category.partition('/')\n          active_dict.setdefault(parser, [])\n          if plugin:\n            active_dict[parser].append(plugin)\n\n      else:\n        parser, _, plugin = parser_filter.partition('/')\n        active_dict.setdefault(parser, [])\n        if plugin:\n          active_dict[parser].append(plugin)\n\n    cls._ReduceParserFilters(includes, excludes)\n    return includes, excludes", "response": "Retrieves the parsers and plugins to include and exclude from the selection."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _GetParsersFromPresetCategory(cls, category):\n    preset_definition = cls._presets.GetPresetByName(category)\n    if preset_definition is None:\n      return []\n\n    preset_names = cls._presets.GetNames()\n    parser_names = set()\n\n    for element_name in preset_definition.parsers:\n      if element_name in preset_names:\n        category_parser_names = cls._GetParsersFromPresetCategory(element_name)\n        parser_names.update(category_parser_names)\n      else:\n        parser_names.add(element_name)\n\n    return sorted(parser_names)", "response": "Retrieves the parser names of specific preset category."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _ReduceParserFilters(cls, includes, excludes):\n    if not includes or not excludes:\n      return\n\n    for parser_name in set(includes).intersection(excludes):\n      # Check parser and plugin list for exact equivalence.\n      if includes[parser_name] == excludes[parser_name]:\n        logger.warning(\n            'Parser {0:s} was in both the inclusion and exclusion lists. '\n            'Ignoring included parser.'.format(parser_name))\n        includes.pop(parser_name)\n        continue\n\n      # Remove plugins that defined are in both inclusion and exclusion lists.\n      plugin_includes = includes[parser_name]\n      plugin_excludes = excludes[parser_name]\n      intersection = set(plugin_includes).intersection(plugin_excludes)\n      if not intersection:\n        continue\n\n      logger.warning(\n          'Parser {0:s} plugins: {1:s} in both the inclusion and exclusion '\n          'lists. Ignoring included plugins.'.format(\n              parser_name, ', '.join(intersection)))\n      plugins_list = list(set(plugin_includes).difference(intersection))\n      includes[parser_name] = plugins_list\n\n    # Remove excluded parsers that do not run.\n    parsers_to_pop = []\n    for parser_name in excludes:\n      if parser_name in includes:\n        continue\n\n      logger.warning(\n          'The excluded parser: {0:s} is not associated with the included '\n          'parsers: {1:s}. Ignoring excluded parser.'.format(\n              parser_name, ', '.join(includes.keys())))\n      parsers_to_pop.append(parser_name)\n\n    for parser_name in parsers_to_pop:\n      excludes.pop(parser_name)", "response": "Reduces the parsers and plugins to include and exclude lists."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef CreateSignatureScanner(cls, specification_store):\n    scanner_object = pysigscan.scanner()\n\n    for format_specification in specification_store.specifications:\n      for signature in format_specification.signatures:\n        pattern_offset = signature.offset\n\n        if pattern_offset is None:\n          signature_flags = pysigscan.signature_flags.NO_OFFSET\n        elif pattern_offset < 0:\n          pattern_offset *= -1\n          signature_flags = pysigscan.signature_flags.RELATIVE_FROM_END\n        else:\n          signature_flags = pysigscan.signature_flags.RELATIVE_FROM_START\n\n        scanner_object.add_signature(\n            signature.identifier, pattern_offset, signature.pattern,\n            signature_flags)\n\n    return scanner_object", "response": "Creates a signature scanner for format specifications with signatures."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef GetFormatsWithSignatures(cls, parser_filter_expression=None):\n    specification_store = specification.FormatSpecificationStore()\n    remainder_list = []\n\n    for parser_name, parser_class in cls.GetParsers(\n        parser_filter_expression=parser_filter_expression):\n      format_specification = parser_class.GetFormatSpecification()\n\n      if format_specification and format_specification.signatures:\n        specification_store.AddSpecification(format_specification)\n        # The plist parser is a special case, where it both defines a signature\n        # and also needs to be applied 'brute-force' to non-matching files,\n        # as the signature matches binary plists, but not XML or JSON plists.\n        if parser_name == 'plist':\n          remainder_list.append(parser_name)\n      else:\n        remainder_list.append(parser_name)\n\n    return specification_store, remainder_list", "response": "Retrieves the format specifications that have signatures."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef GetNamesOfParsersWithPlugins(cls):\n    parser_names = []\n\n    for parser_name, parser_class in cls.GetParsers():\n      if parser_class.SupportsPlugins():\n        parser_names.append(parser_name)\n\n    return sorted(parser_names)", "response": "Retrieves the names of all parsers with plugins."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef GetParserAndPluginNames(cls, parser_filter_expression=None):\n    parser_and_plugin_names = []\n    for parser_name, parser_class in cls.GetParsers(\n        parser_filter_expression=parser_filter_expression):\n      parser_and_plugin_names.append(parser_name)\n\n      if parser_class.SupportsPlugins():\n        for plugin_name, _ in parser_class.GetPlugins():\n          parser_and_plugin_names.append(\n              '{0:s}/{1:s}'.format(parser_name, plugin_name))\n\n    return parser_and_plugin_names", "response": "Retrieves the parser and plugin names."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef GetParserPluginsInformation(cls, parser_filter_expression=None):\n    parser_plugins_information = []\n    for _, parser_class in cls.GetParsers(\n        parser_filter_expression=parser_filter_expression):\n      if parser_class.SupportsPlugins():\n        for plugin_name, plugin_class in parser_class.GetPlugins():\n          description = getattr(plugin_class, 'DESCRIPTION', '')\n          parser_plugins_information.append((plugin_name, description))\n\n    return parser_plugins_information", "response": "Retrieves the parser plugins information."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nretrieves a specific parser object by its name.", "response": "def GetParserObjectByName(cls, parser_name):\n    \"\"\"Retrieves a specific parser object by its name.\n\n    Args:\n      parser_name (str): name of the parser.\n\n    Returns:\n      BaseParser: parser object or None.\n    \"\"\"\n    parser_class = cls._parser_classes.get(parser_name, None)\n    if parser_class:\n      return parser_class()\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nretrieve the parser objects for the given parser filter expression.", "response": "def GetParserObjects(cls, parser_filter_expression=None):\n    \"\"\"Retrieves the parser objects.\n\n    Args:\n      parser_filter_expression (Optional[str]): parser filter expression,\n          where None represents all parsers and plugins.\n\n    Returns:\n      dict[str, BaseParser]: parsers per name.\n    \"\"\"\n    includes, excludes = cls._GetParserFilters(parser_filter_expression)\n\n    parser_objects = {}\n    for parser_name, parser_class in iter(cls._parser_classes.items()):\n      # If there are no includes all parsers are included by default.\n      if not includes and parser_name in excludes:\n        continue\n\n      if includes and parser_name not in includes:\n        continue\n\n      parser_object = parser_class()\n      if parser_class.SupportsPlugins():\n        plugin_includes = None\n        if parser_name in includes:\n          plugin_includes = includes[parser_name]\n\n        parser_object.EnablePlugins(plugin_includes)\n\n      parser_objects[parser_name] = parser_object\n\n    return parser_objects"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef GetParsers(cls, parser_filter_expression=None):\n    includes, excludes = cls._GetParserFilters(parser_filter_expression)\n\n    for parser_name, parser_class in iter(cls._parser_classes.items()):\n      # If there are no includes all parsers are included by default.\n      if not includes and parser_name in excludes:\n        continue\n\n      if includes and parser_name not in includes:\n        continue\n\n      yield parser_name, parser_class", "response": "Retrieves the registered parsers and plugins and plugins from a parser filter string."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef GetParsersInformation(cls):\n    parsers_information = []\n    for _, parser_class in cls.GetParsers():\n      description = getattr(parser_class, 'DESCRIPTION', '')\n      parsers_information.append((parser_class.NAME, description))\n\n    return parsers_information", "response": "Retrieves the parsers information."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef GetPresetsInformation(cls):\n    parser_presets_information = []\n    for preset_definition in ParsersManager.GetPresets():\n      preset_information_tuple = (\n          preset_definition.name, ', '.join(preset_definition.parsers))\n      # TODO: refactor to pass PresetDefinition.\n      parser_presets_information.append(preset_information_tuple)\n\n    return parser_presets_information", "response": "Retrieves the presets information."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef GetPresetsForOperatingSystem(\n      cls, operating_system, operating_system_product,\n      operating_system_version):\n    \"\"\"Determines the presets for a specific operating system.\n\n    Args:\n      operating_system (str): operating system for example \"Windows\". This\n          should be one of the values in definitions.OPERATING_SYSTEM_FAMILIES.\n      operating_system_product (str): operating system product for\n          example \"Windows XP\" as determined by preprocessing.\n      operating_system_version (str): operating system version for\n          example \"5.1\" as determined by preprocessing.\n\n    Returns:\n      list[PresetDefinition]: preset definitions, where an empty list\n          represents all parsers and parser plugins (no preset).\n    \"\"\"\n    operating_system = artifacts.OperatingSystemArtifact(\n        family=operating_system, product=operating_system_product,\n        version=operating_system_version)\n\n    return cls._presets.GetPresetsByOperatingSystem(operating_system)", "response": "Returns a list of presets for a specific operating system."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nregistering a parser class.", "response": "def RegisterParser(cls, parser_class):\n    \"\"\"Registers a parser class.\n\n    The parser classes are identified based on their lower case name.\n\n    Args:\n      parser_class (type): parser class (subclass of BaseParser).\n\n    Raises:\n      KeyError: if parser class is already set for the corresponding name.\n    \"\"\"\n    parser_name = parser_class.NAME.lower()\n    if parser_name in cls._parser_classes:\n      raise KeyError('Parser class already set for name: {0:s}.'.format(\n          parser_class.NAME))\n\n    cls._parser_classes[parser_name] = parser_class"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndetermines the formatted message strings for an event object.", "response": "def GetMessages(self, formatter_mediator, event):\n    \"\"\"Determines the formatted message strings for an event object.\n\n    Args:\n      formatter_mediator (FormatterMediator): mediates the interactions\n          between formatters and other components, such as storage and Windows\n          EventLog resources.\n      event (EventObject): event.\n\n    Returns:\n      tuple(str, str): formatted message string and short message string.\n\n    Raises:\n      WrongFormatter: if the event object cannot be formatted by the formatter.\n    \"\"\"\n    if self.DATA_TYPE != event.data_type:\n      raise errors.WrongFormatter('Unsupported data type: {0:s}.'.format(\n          event.data_type))\n\n    event_values = event.CopyToDict()\n\n    security = event_values.get('security', None)\n    if security:\n      security_flags = []\n      for flag, description in iter(self._SECURITY_VALUES.items()):\n        if security & flag:\n          security_flags.append(description)\n\n      security_string = '0x{0:08x}: {1:s}'.format(\n          security, ','.join(security_flags))\n\n      event_values['security'] = security_string\n\n    for key, value in iter(event_values.items()):\n      if isinstance(value, py2to3.BYTES_TYPE):\n        event_values[key] = repr(value)\n\n    return self._ConditionalFormatMessages(event_values)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates a SHA - 256 hash of the contents of the file entry.", "response": "def _CalculateDigestHash(self, file_entry, data_stream_name):\n    \"\"\"Calculates a SHA-256 digest of the contents of the file entry.\n\n    Args:\n      file_entry (dfvfs.FileEntry): file entry whose content will be hashed.\n      data_stream_name (str): name of the data stream whose content is to be\n          hashed.\n\n    Returns:\n      str: hexadecimal representation of the SHA-256 hash or None if the digest\n          cannot be determined.\n    \"\"\"\n    file_object = file_entry.GetFileObject(data_stream_name=data_stream_name)\n    if not file_object:\n      return None\n\n    try:\n      file_object.seek(0, os.SEEK_SET)\n\n      hasher_object = hashers_manager.HashersManager.GetHasher('sha256')\n\n      data = file_object.read(self._READ_BUFFER_SIZE)\n      while data:\n        hasher_object.Update(data)\n        data = file_object.read(self._READ_BUFFER_SIZE)\n\n    finally:\n      file_object.close()\n\n    return hasher_object.GetStringDigest()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a sanitized path of both destination directory and filename.", "response": "def _CreateSanitizedDestination(\n      self, source_file_entry, source_path_spec, source_data_stream_name,\n      destination_path):\n    \"\"\"Creates a sanitized path of both destination directory and filename.\n\n    This function replaces non-printable and other characters defined in\n     _DIRTY_CHARACTERS with an underscore \"_\".\n\n    Args:\n      source_file_entry (dfvfs.FileEntry): file entry of the source file.\n      source_path_spec (dfvfs.PathSpec): path specification of the source file.\n      source_data_stream_name (str): name of the data stream of the source file\n          entry.\n      destination_path (str): path of the destination directory.\n\n    Returns:\n      tuple[str, str]: sanitized paths of both destination directory and\n          filename.\n    \"\"\"\n    file_system = source_file_entry.GetFileSystem()\n    path = getattr(source_path_spec, 'location', None)\n    path_segments = file_system.SplitPath(path)\n\n    # Sanitize each path segment.\n    for index, path_segment in enumerate(path_segments):\n      path_segments[index] = ''.join([\n          character if character not in self._DIRTY_CHARACTERS else '_'\n          for character in path_segment])\n\n    target_filename = path_segments.pop()\n\n    parent_path_spec = getattr(source_file_entry.path_spec, 'parent', None)\n\n    while parent_path_spec:\n      if parent_path_spec.type_indicator == (\n          dfvfs_definitions.TYPE_INDICATOR_TSK_PARTITION):\n        path_segments.insert(0, parent_path_spec.location[1:])\n        break\n\n      elif parent_path_spec.type_indicator == (\n          dfvfs_definitions.TYPE_INDICATOR_VSHADOW):\n        path_segments.insert(0, parent_path_spec.location[1:])\n\n      parent_path_spec = getattr(parent_path_spec, 'parent', None)\n\n    target_directory = os.path.join(destination_path, *path_segments)\n\n    if source_data_stream_name:\n      target_filename = '{0:s}_{1:s}'.format(\n          target_filename, source_data_stream_name)\n\n    return target_directory, target_filename"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _Extract(\n      self, source_path_specs, destination_path, output_writer,\n      skip_duplicates=True):\n    \"\"\"Extracts files.\n\n    Args:\n      source_path_specs (list[dfvfs.PathSpec]): path specifications to extract.\n      destination_path (str): path where the extracted files should be stored.\n      output_writer (CLIOutputWriter): output writer.\n      skip_duplicates (Optional[bool]): True if files with duplicate content\n          should be skipped.\n    \"\"\"\n    output_writer.Write('Extracting file entries.\\n')\n    path_spec_generator = self._path_spec_extractor.ExtractPathSpecs(\n        source_path_specs, resolver_context=self._resolver_context)\n\n    for path_spec in path_spec_generator:\n      self._ExtractFileEntry(\n          path_spec, destination_path, output_writer,\n          skip_duplicates=skip_duplicates)", "response": "Extracts the contents of the source_path_specs into the destination_path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _ExtractDataStream(\n      self, file_entry, data_stream_name, destination_path, output_writer,\n      skip_duplicates=True):\n    \"\"\"Extracts a data stream.\n\n    Args:\n      file_entry (dfvfs.FileEntry): file entry containing the data stream.\n      data_stream_name (str): name of the data stream.\n      destination_path (str): path where the extracted files should be stored.\n      output_writer (CLIOutputWriter): output writer.\n      skip_duplicates (Optional[bool]): True if files with duplicate content\n          should be skipped.\n    \"\"\"\n    if not data_stream_name and not file_entry.IsFile():\n      return\n\n    display_name = path_helper.PathHelper.GetDisplayNameForPathSpec(\n        file_entry.path_spec)\n\n    if skip_duplicates:\n      try:\n        digest = self._CalculateDigestHash(file_entry, data_stream_name)\n      except (IOError, dfvfs_errors.BackEndError) as exception:\n        output_writer.Write((\n            '[skipping] unable to read content of file entry: {0:s} '\n            'with error: {1!s}\\n').format(display_name, exception))\n        return\n\n      if not digest:\n        output_writer.Write(\n            '[skipping] unable to read content of file entry: {0:s}\\n'.format(\n                display_name))\n        return\n\n      duplicate_display_name = self._digests.get(digest, None)\n      if duplicate_display_name:\n        output_writer.Write((\n            '[skipping] file entry: {0:s} is a duplicate of: {1:s} with '\n            'digest: {2:s}\\n').format(\n                display_name, duplicate_display_name, digest))\n        return\n\n      self._digests[digest] = display_name\n\n    target_directory, target_filename = self._CreateSanitizedDestination(\n        file_entry, file_entry.path_spec, data_stream_name, destination_path)\n\n    if not os.path.isdir(target_directory):\n      os.makedirs(target_directory)\n\n    target_path = os.path.join(target_directory, target_filename)\n\n    if os.path.exists(target_path):\n      output_writer.Write((\n          '[skipping] unable to export contents of file entry: {0:s} '\n          'because exported file: {1:s} already exists.\\n').format(\n              display_name, target_path))\n      return\n\n    try:\n      self._WriteFileEntry(file_entry, data_stream_name, target_path)\n    except (IOError, dfvfs_errors.BackEndError) as exception:\n      output_writer.Write((\n          '[skipping] unable to export contents of file entry: {0:s} '\n          'with error: {1!s}\\n').format(display_name, exception))\n\n      try:\n        os.remove(target_path)\n      except (IOError, OSError):\n        pass", "response": "Extracts a data stream from a file entry."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _ExtractFileEntry(\n      self, path_spec, destination_path, output_writer, skip_duplicates=True):\n    \"\"\"Extracts a file entry.\n\n    Args:\n      path_spec (dfvfs.PathSpec): path specification of the source file.\n      destination_path (str): path where the extracted files should be stored.\n      output_writer (CLIOutputWriter): output writer.\n      skip_duplicates (Optional[bool]): True if files with duplicate content\n          should be skipped.\n    \"\"\"\n    file_entry = path_spec_resolver.Resolver.OpenFileEntry(path_spec)\n\n    if not file_entry:\n      logger.warning('Unable to open file entry for path spec: {0:s}'.format(\n          path_spec.comparable))\n      return\n\n    if not self._filter_collection.Matches(file_entry):\n      return\n\n    file_entry_processed = False\n    for data_stream in file_entry.data_streams:\n      if self._abort:\n        break\n\n      self._ExtractDataStream(\n          file_entry, data_stream.name, destination_path, output_writer,\n          skip_duplicates=skip_duplicates)\n\n      file_entry_processed = True\n\n    if not file_entry_processed:\n      self._ExtractDataStream(\n          file_entry, '', destination_path, output_writer,\n          skip_duplicates=skip_duplicates)", "response": "Extracts a file entry from the source file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _ExtractWithFilter(\n      self, source_path_specs, destination_path, output_writer,\n      artifact_filters, filter_file, artifact_definitions_path,\n      custom_artifacts_path, skip_duplicates=True):\n    \"\"\"Extracts files using a filter expression.\n\n    This method runs the file extraction process on the image and\n    potentially on every VSS if that is wanted.\n\n    Args:\n      source_path_specs (list[dfvfs.PathSpec]): path specifications to extract.\n      destination_path (str): path where the extracted files should be stored.\n      output_writer (CLIOutputWriter): output writer.\n      artifact_definitions_path (str): path to artifact definitions file.\n      custom_artifacts_path (str): path to custom artifact definitions file.\n      artifact_filters (list[str]): names of artifact definitions that are\n          used for filtering file system and Windows Registry key paths.\n      filter_file (str): path of the file that contains the filter file path\n          filters.\n      skip_duplicates (Optional[bool]): True if files with duplicate content\n          should be skipped.\n    \"\"\"\n    extraction_engine = engine.BaseEngine()\n\n    # If the source is a directory or a storage media image\n    # run pre-processing.\n    if self._source_type in self._SOURCE_TYPES_TO_PREPROCESS:\n      self._PreprocessSources(extraction_engine)\n\n    for source_path_spec in source_path_specs:\n      file_system, mount_point = self._GetSourceFileSystem(\n          source_path_spec, resolver_context=self._resolver_context)\n\n      display_name = path_helper.PathHelper.GetDisplayNameForPathSpec(\n          source_path_spec)\n      output_writer.Write(\n          'Extracting file entries from: {0:s}\\n'.format(display_name))\n\n      filter_find_specs = extraction_engine.BuildFilterFindSpecs(\n          artifact_definitions_path, custom_artifacts_path,\n          extraction_engine.knowledge_base, artifact_filters, filter_file)\n\n      searcher = file_system_searcher.FileSystemSearcher(\n          file_system, mount_point)\n      for path_spec in searcher.Find(find_specs=filter_find_specs):\n        self._ExtractFileEntry(\n            path_spec, destination_path, output_writer,\n            skip_duplicates=skip_duplicates)\n\n      file_system.Close()", "response": "Extracts files using a filter expression."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretrieves the file system of the source.", "response": "def _GetSourceFileSystem(self, source_path_spec, resolver_context=None):\n    \"\"\"Retrieves the file system of the source.\n\n    Args:\n      source_path_spec (dfvfs.PathSpec): source path specification of the file\n          system.\n      resolver_context (dfvfs.Context): resolver context.\n\n    Returns:\n      tuple: containing:\n\n        dfvfs.FileSystem: file system.\n        dfvfs.PathSpec: mount point path specification that refers\n            to the base location of the file system.\n\n    Raises:\n      RuntimeError: if source path specification is not set.\n    \"\"\"\n    if not source_path_spec:\n      raise RuntimeError('Missing source.')\n\n    file_system = path_spec_resolver.Resolver.OpenFileSystem(\n        source_path_spec, resolver_context=resolver_context)\n\n    type_indicator = source_path_spec.type_indicator\n    if path_spec_factory.Factory.IsSystemLevelTypeIndicator(type_indicator):\n      mount_point = source_path_spec\n    else:\n      mount_point = source_path_spec.parent\n\n    return file_system, mount_point"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _ParseExtensionsString(self, extensions_string):\n    if not extensions_string:\n      return\n\n    extensions_string = extensions_string.lower()\n    extensions = [\n        extension.strip() for extension in extensions_string.split(',')]\n    file_entry_filter = file_entry_filters.ExtensionsFileEntryFilter(extensions)\n    self._filter_collection.AddFilter(file_entry_filter)", "response": "Parses the extensions string and adds the filter to the filter collection."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _ParseNamesString(self, names_string):\n    if not names_string:\n      return\n\n    names_string = names_string.lower()\n    names = [name.strip() for name in names_string.split(',')]\n    file_entry_filter = file_entry_filters.NamesFileEntryFilter(names)\n    self._filter_collection.AddFilter(file_entry_filter)", "response": "Parses the names string into a list of file entries."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses the filter options.", "response": "def _ParseFilterOptions(self, options):\n    \"\"\"Parses the filter options.\n\n    Args:\n      options (argparse.Namespace): command line arguments.\n\n    Raises:\n      BadConfigOption: if the options are invalid.\n    \"\"\"\n    names = ['artifact_filters', 'date_filters', 'filter_file']\n    helpers_manager.ArgumentHelperManager.ParseOptions(\n        options, self, names=names)\n\n    extensions_string = self.ParseStringOption(options, 'extensions_string')\n    self._ParseExtensionsString(extensions_string)\n\n    names_string = getattr(options, 'names_string', None)\n    self._ParseNamesString(names_string)\n\n    signature_identifiers = getattr(options, 'signature_identifiers', None)\n    try:\n      self._ParseSignatureIdentifiers(\n          self._data_location, signature_identifiers)\n    except (IOError, ValueError) as exception:\n      raise errors.BadConfigOption(exception)\n\n    if self._artifact_filters or self._filter_file:\n      self.has_filters = True\n    else:\n      self.has_filters = self._filter_collection.HasFilters()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses the signature identifiers from the format specification file.", "response": "def _ParseSignatureIdentifiers(self, data_location, signature_identifiers):\n    \"\"\"Parses the signature identifiers.\n\n    Args:\n      data_location (str): location of the format specification file, for\n          example, \"signatures.conf\".\n      signature_identifiers (str): comma separated signature identifiers.\n\n    Raises:\n      IOError: if the format specification file could not be read from\n          the specified data location.\n      OSError: if the format specification file could not be read from\n          the specified data location.\n      ValueError: if no data location was specified.\n    \"\"\"\n    if not signature_identifiers:\n      return\n\n    if not data_location:\n      raise ValueError('Missing data location.')\n\n    path = os.path.join(data_location, 'signatures.conf')\n    if not os.path.exists(path):\n      raise IOError(\n          'No such format specification file: {0:s}'.format(path))\n\n    try:\n      specification_store = self._ReadSpecificationFile(path)\n    except IOError as exception:\n      raise IOError((\n          'Unable to read format specification file: {0:s} with error: '\n          '{1!s}').format(path, exception))\n\n    signature_identifiers = signature_identifiers.lower()\n    signature_identifiers = [\n        identifier.strip() for identifier in signature_identifiers.split(',')]\n    file_entry_filter = file_entry_filters.SignaturesFileEntryFilter(\n        specification_store, signature_identifiers)\n    self._filter_collection.AddFilter(file_entry_filter)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread the format specification file.", "response": "def _ReadSpecificationFile(self, path):\n    \"\"\"Reads the format specification file.\n\n    Args:\n      path (str): path of the format specification file.\n\n    Returns:\n      FormatSpecificationStore: format specification store.\n    \"\"\"\n    specification_store = specification.FormatSpecificationStore()\n\n    with io.open(\n        path, 'rt', encoding=self._SPECIFICATION_FILE_ENCODING) as file_object:\n      for line in file_object.readlines():\n        line = line.strip()\n        if not line or line.startswith('#'):\n          continue\n\n        try:\n          identifier, offset, pattern = line.split()\n        except ValueError:\n          logger.error('[skipping] invalid line: {0:s}'.format(line))\n          continue\n\n        try:\n          offset = int(offset, 10)\n        except ValueError:\n          logger.error('[skipping] invalid offset in line: {0:s}'.format(line))\n          continue\n\n        try:\n          # TODO: find another way to do this that doesn't use an undocumented\n          # API.\n          pattern = codecs.escape_decode(pattern)[0]\n        # ValueError is raised e.g. when the patterns contains \"\\xg1\".\n        except ValueError:\n          logger.error(\n              '[skipping] invalid pattern in line: {0:s}'.format(line))\n          continue\n\n        format_specification = specification.FormatSpecification(identifier)\n        format_specification.AddNewSignature(pattern, offset=offset)\n        specification_store.AddSpecification(format_specification)\n\n    return specification_store"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrite the contents of the source file entry to the destination file.", "response": "def _WriteFileEntry(self, file_entry, data_stream_name, destination_file):\n    \"\"\"Writes the contents of the source file entry to a destination file.\n\n    Note that this function will overwrite an existing file.\n\n    Args:\n      file_entry (dfvfs.FileEntry): file entry whose content is to be written.\n      data_stream_name (str): name of the data stream whose content is to be\n          written.\n      destination_file (str): path of the destination file.\n    \"\"\"\n    source_file_object = file_entry.GetFileObject(\n        data_stream_name=data_stream_name)\n    if not source_file_object:\n      return\n\n    try:\n      with open(destination_file, 'wb') as destination_file_object:\n        source_file_object.seek(0, os.SEEK_SET)\n\n        data = source_file_object.read(self._COPY_BUFFER_SIZE)\n        while data:\n          destination_file_object.write(data)\n          data = source_file_object.read(self._COPY_BUFFER_SIZE)\n\n    finally:\n      source_file_object.close()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding the filter options to the argument group.", "response": "def AddFilterOptions(self, argument_group):\n    \"\"\"Adds the filter options to the argument group.\n\n    Args:\n      argument_group (argparse._ArgumentGroup): argparse argument group.\n    \"\"\"\n    names = ['artifact_filters', 'date_filters', 'filter_file']\n    helpers_manager.ArgumentHelperManager.AddCommandLineArguments(\n        argument_group, names=names)\n\n    argument_group.add_argument(\n        '-x', '--extensions', dest='extensions_string', action='store',\n        type=str, metavar='EXTENSIONS', help=(\n            'Filter on file name extensions. This option accepts multiple '\n            'multiple comma separated values e.g. \"csv,docx,pst\".'))\n\n    argument_group.add_argument(\n        '--names', dest='names_string', action='store',\n        type=str, metavar='NAMES', help=(\n            'Filter on file names.  This option accepts a comma separated '\n            'string denoting all file names, e.g. -x '\n            '\"NTUSER.DAT,UsrClass.dat\".'))\n\n    argument_group.add_argument(\n        '--signatures', dest='signature_identifiers', action='store',\n        type=str, metavar='IDENTIFIERS', help=(\n            'Filter on file format signature identifiers. This option '\n            'accepts multiple comma separated values e.g. \"esedb,lnk\". '\n            'Use \"list\" to show an overview of the supported file format '\n            'signatures.'))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ListSignatureIdentifiers(self):\n    if not self._data_location:\n      raise errors.BadConfigOption('Missing data location.')\n\n    path = os.path.join(self._data_location, 'signatures.conf')\n    if not os.path.exists(path):\n      raise errors.BadConfigOption(\n          'No such format specification file: {0:s}'.format(path))\n\n    try:\n      specification_store = self._ReadSpecificationFile(path)\n    except IOError as exception:\n      raise errors.BadConfigOption((\n          'Unable to read format specification file: {0:s} with error: '\n          '{1!s}').format(path, exception))\n\n    identifiers = []\n    for format_specification in specification_store.specifications:\n      identifiers.append(format_specification.identifier)\n\n    self._output_writer.Write('Available signature identifiers:\\n')\n    self._output_writer.Write(\n        '\\n'.join(textwrap.wrap(', '.join(sorted(identifiers)), 79)))\n    self._output_writer.Write('\\n\\n')", "response": "Lists the signature identifiers."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ParseOptions(self, options):\n    # The data location is required to list signatures.\n    helpers_manager.ArgumentHelperManager.ParseOptions(\n        options, self, names=['data_location'])\n\n    # Check the list options first otherwise required options will raise.\n    signature_identifiers = self.ParseStringOption(\n        options, 'signature_identifiers')\n    if signature_identifiers == 'list':\n      self.list_signature_identifiers = True\n\n    if self.list_signature_identifiers:\n      return\n\n    self._ParseInformationalOptions(options)\n    self._ParseLogFileOptions(options)\n\n    self._ParseStorageMediaOptions(options)\n\n    self._destination_path = self.ParseStringOption(\n        options, 'path', default_value='export')\n\n    if not self._data_location:\n      logger.warning('Unable to automatically determine data location.')\n\n    argument_helper_names = ['artifact_definitions', 'process_resources']\n    helpers_manager.ArgumentHelperManager.ParseOptions(\n        options, self, names=argument_helper_names)\n\n    self._ParseFilterOptions(options)\n\n    if (getattr(options, 'no_vss', False) or\n        getattr(options, 'include_duplicates', False)):\n      self._skip_duplicates = False\n\n    self._EnforceProcessMemoryLimit(self._process_memory_limit)", "response": "Parses the options and initializes the front - end."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ProcessSources(self):\n    scan_context = self.ScanSource(self._source_path)\n    self._source_type = scan_context.source_type\n\n    self._output_writer.Write('Export started.\\n')\n\n    if not os.path.isdir(self._destination_path):\n      os.makedirs(self._destination_path)\n\n    if self._artifact_filters or self._filter_file:\n      self._ExtractWithFilter(\n          self._source_path_specs, self._destination_path, self._output_writer,\n          self._artifact_filters, self._filter_file,\n          self._artifact_definitions_path, self._custom_artifacts_path,\n          skip_duplicates=self._skip_duplicates)\n    else:\n      self._Extract(\n          self._source_path_specs, self._destination_path, self._output_writer,\n          skip_duplicates=self._skip_duplicates)\n\n    self._output_writer.Write('Export completed.\\n')\n    self._output_writer.Write('\\n')", "response": "Processes the sources.\n\n    Raises:\n      SourceScannerError: if the source scanner could not find a supported\n          file system.\n      UserAbort: if the user initiated an abort."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef GetMessages(self, formatter_mediator, event):\n    if self.DATA_TYPE != event.data_type:\n      raise errors.WrongFormatter('Unsupported data type: {0:s}.'.format(\n          event.data_type))\n\n    event_values = event.CopyToDict()\n\n    restore_point_event_type = event_values.get(\n        'restore_point_event_type', None)\n    if restore_point_event_type is not None:\n      event_values['restore_point_event_type'] = (\n          self._RESTORE_POINT_EVENT_TYPES.get(\n              restore_point_event_type, 'UNKNOWN'))\n\n    restore_point_type = event_values.get('restore_point_type', None)\n    if restore_point_type is not None:\n      event_values['restore_point_type'] = (\n          self._RESTORE_POINT_EVENT_TYPES.get(restore_point_type, 'UNKNOWN'))\n\n    return self._ConditionalFormatMessages(event_values)", "response": "Determines the formatted message strings for an event object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _ConvertValueBinaryDataToFloatingPointValue(self, value):\n    if not value:\n      return None\n\n    value_length = len(value)\n    if value_length not in (4, 8):\n      raise errors.ParseError('Unsupported value data size: {0:d}'.format(\n          value_length))\n\n    if value_length == 4:\n      floating_point_map = self._GetDataTypeMap('float32le')\n    elif value_length == 8:\n      floating_point_map = self._GetDataTypeMap('float64le')\n\n    try:\n      return self._ReadStructureFromByteStream(value, 0, floating_point_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.ParseError(\n          'Unable to parse floating-point value with error: {0!s}'.format(\n              exception))", "response": "Converts a binary data value into a floating - point value."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretrieves the identifier mappings from the SruDbIdMapTable table.", "response": "def _GetIdentifierMappings(self, parser_mediator, cache, database):\n    \"\"\"Retrieves the identifier mappings from SruDbIdMapTable table.\n\n    In the SRUM database individual tables contain numeric identifiers for\n    the application (\"AppId\") and user identifier (\"UserId\"). A more descriptive\n    string of these values can be found in the SruDbIdMapTable. For example the\n    numeric value of 42 mapping to DiagTrack. This method will cache the\n    mappings of a specific SRUM database.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      cache (ESEDBCache): cache, which contains information about\n          the identifiers stored in the SruDbIdMapTable table.\n      database (pyesedb.file): ESE database.\n\n    Returns:\n      dict[int, str]: mapping of numeric identifiers to their string\n          representation.\n    \"\"\"\n    identifier_mappings = cache.GetResults('SruDbIdMapTable', default_value={})\n    if not identifier_mappings:\n      esedb_table = database.get_table_by_name('SruDbIdMapTable')\n      if not esedb_table:\n        parser_mediator.ProduceExtractionWarning(\n            'unable to retrieve table: SruDbIdMapTable')\n      else:\n        identifier_mappings = self._ParseIdentifierMappingsTable(\n            parser_mediator, esedb_table)\n\n      cache.StoreDictInCache('SruDbIdMapTable', identifier_mappings)\n\n    return identifier_mappings"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses a GUID table.", "response": "def _ParseGUIDTable(\n      self, parser_mediator, cache, database, esedb_table, values_map,\n      event_data_class):\n    \"\"\"Parses a table with a GUID as name.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      cache (ESEDBCache): cache, which contains information about\n          the identifiers stored in the SruDbIdMapTable table.\n      database (pyesedb.file): ESE database.\n      esedb_table (pyesedb.table): table.\n      values_map (dict[str, str]): mapping of table columns to event data\n          attribute names.\n      event_data_class (type): event data class.\n\n    Raises:\n      ValueError: if the cache, database or table value is missing.\n    \"\"\"\n    if cache is None:\n      raise ValueError('Missing cache value.')\n\n    if database is None:\n      raise ValueError('Missing database value.')\n\n    if esedb_table is None:\n      raise ValueError('Missing table value.')\n\n    identifier_mappings = self._GetIdentifierMappings(\n        parser_mediator, cache, database)\n\n    for esedb_record in esedb_table.records:\n      if parser_mediator.abort:\n        break\n\n      record_values = self._GetRecordValues(\n          parser_mediator, esedb_table.name, esedb_record,\n          value_mappings=self._GUID_TABLE_VALUE_MAPPINGS)\n\n      event_data = event_data_class()\n\n      for attribute_name, column_name in values_map.items():\n        record_value = record_values.get(column_name, None)\n        if attribute_name in ('application', 'user_identifier'):\n          # Human readable versions of AppId and UserId values are stored\n          # in the SruDbIdMapTable table; also referred to as identifier\n          # mapping. Here we look up the numeric identifier stored in the GUID\n          # table in SruDbIdMapTable.\n          record_value = identifier_mappings.get(record_value, record_value)\n\n        setattr(event_data, attribute_name, record_value)\n\n      timestamp = record_values.get('TimeStamp')\n      if timestamp:\n        date_time = dfdatetime_ole_automation_date.OLEAutomationDate(\n            timestamp=timestamp)\n        timestamp_description = definitions.TIME_DESCRIPTION_SAMPLE\n      else:\n        date_time = dfdatetime_semantic_time.SemanticTime('Not set')\n        timestamp_description = definitions.TIME_DESCRIPTION_NOT_A_TIME\n\n      event = time_events.DateTimeValuesEvent(date_time, timestamp_description)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n      timestamp = record_values.get('ConnectStartTime')\n      if timestamp:\n        date_time = dfdatetime_filetime.Filetime(timestamp=timestamp)\n        event = time_events.DateTimeValuesEvent(\n            date_time, definitions.TIME_DESCRIPTION_FIRST_CONNECTED)\n        parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nextracting an identifier mapping from a SruDbIdMapTable record.", "response": "def _ParseIdentifierMappingRecord(\n      self, parser_mediator, table_name, esedb_record):\n    \"\"\"Extracts an identifier mapping from a SruDbIdMapTable record.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      table_name (str): name of the table the record is stored in.\n      esedb_record (pyesedb.record): record.\n\n    Returns:\n      tuple[int, str]: numeric identifier and its string representation or\n          None, None if no identifier mapping can be retrieved from the record.\n    \"\"\"\n    record_values = self._GetRecordValues(\n        parser_mediator, table_name, esedb_record)\n\n    identifier = record_values.get('IdIndex', None)\n    if identifier is None:\n      parser_mediator.ProduceExtractionWarning(\n          'IdIndex value missing from table: SruDbIdMapTable')\n      return None, None\n\n    identifier_type = record_values.get('IdType', None)\n    if identifier_type not in self._SUPPORTED_IDENTIFIER_TYPES:\n      parser_mediator.ProduceExtractionWarning(\n          'unsupported IdType value: {0!s} in table: SruDbIdMapTable'.format(\n              identifier_type))\n      return None, None\n\n    mapped_value = record_values.get('IdBlob', None)\n    if mapped_value is None:\n      parser_mediator.ProduceExtractionWarning(\n          'IdBlob value missing from table: SruDbIdMapTable')\n      return None, None\n\n    if identifier_type == 3:\n      try:\n        fwnt_identifier = pyfwnt.security_identifier()\n        fwnt_identifier.copy_from_byte_stream(mapped_value)\n        mapped_value = fwnt_identifier.get_string()\n      except IOError:\n        parser_mediator.ProduceExtractionWarning(\n            'unable to decode IdBlob value as Windows NT security identifier')\n        return None, None\n\n    else:\n      try:\n        mapped_value = mapped_value.decode('utf-16le').rstrip('\\0')\n      except UnicodeDecodeError:\n        parser_mediator.ProduceExtractionWarning(\n            'unable to decode IdBlob value as UTF-16 little-endian string')\n        return None, None\n\n    return identifier, mapped_value"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _ParseIdentifierMappingsTable(self, parser_mediator, esedb_table):\n    identifier_mappings = {}\n\n    for esedb_record in esedb_table.records:\n      if parser_mediator.abort:\n        break\n\n      identifier, mapped_value = self._ParseIdentifierMappingRecord(\n          parser_mediator, esedb_table.name, esedb_record)\n      if identifier is None or mapped_value is None:\n        continue\n\n      if identifier in identifier_mappings:\n        parser_mediator.ProduceExtractionWarning(\n            'identifier: {0:d} already exists in mappings.'.format(identifier))\n        continue\n\n      identifier_mappings[identifier] = mapped_value\n\n    return identifier_mappings", "response": "Extracts identifier mappings from the SruDbIdMapTable table."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the application resource usage table.", "response": "def ParseApplicationResourceUsage(\n      self, parser_mediator, cache=None, database=None, table=None,\n      **unused_kwargs):\n    \"\"\"Parses the application resource usage table.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      cache (Optional[ESEDBCache]): cache, which contains information about\n          the identifiers stored in the SruDbIdMapTable table.\n      database (Optional[pyesedb.file]): ESE database.\n      table (Optional[pyesedb.table]): table.\n    \"\"\"\n    self._ParseGUIDTable(\n        parser_mediator, cache, database, table,\n        self._APPLICATION_RESOURCE_USAGE_VALUES_MAP,\n        SRUMApplicationResourceUsageEventData)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ParseNetworkDataUsage(\n      self, parser_mediator, cache=None, database=None, table=None,\n      **unused_kwargs):\n    \"\"\"Parses the network data usage monitor table.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      cache (Optional[ESEDBCache]): cache, which contains information about\n          the identifiers stored in the SruDbIdMapTable table.\n      database (Optional[pyesedb.file]): ESE database.\n      table (Optional[pyesedb.table]): table.\n    \"\"\"\n    self._ParseGUIDTable(\n        parser_mediator, cache, database, table,\n        self._NETWORK_DATA_USAGE_VALUES_MAP, SRUMNetworkDataUsageEventData)", "response": "Parses the network data usage monitor table."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses the network connectivity usage monitor table.", "response": "def ParseNetworkConnectivityUsage(\n      self, parser_mediator, cache=None, database=None, table=None,\n      **unused_kwargs):\n    \"\"\"Parses the network connectivity usage monitor table.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      cache (Optional[ESEDBCache]): cache, which contains information about\n          the identifiers stored in the SruDbIdMapTable table.\n      database (Optional[pyesedb.file]): ESE database.\n      table (Optional[pyesedb.table]): table.\n    \"\"\"\n    # TODO: consider making ConnectStartTime + ConnectedTime an event.\n    self._ParseGUIDTable(\n        parser_mediator, cache, database, table,\n        self._NETWORK_CONNECTIVITY_USAGE_VALUES_MAP,\n        SRUMNetworkConnectivityUsageEventData)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ExtractEvents(self, parser_mediator, registry_key, **kwargs):\n    for subkey in registry_key.GetSubkeys():\n      values_dict = {}\n      values_dict['subkey_name'] = subkey.name\n\n      vendor_identification = None\n      product_identification = None\n      try:\n        subkey_name_parts = subkey.name.split('&')\n        if len(subkey_name_parts) >= 2:\n          vendor_identification = subkey_name_parts[0]\n          product_identification = subkey_name_parts[1]\n      except ValueError as exception:\n        logger.warning(\n            'Unable to split string: {0:s} with error: {1!s}'.format(\n                subkey.name, exception))\n\n      if vendor_identification and product_identification:\n        values_dict['vendor'] = vendor_identification\n        values_dict['product'] = product_identification\n\n      for devicekey in subkey.GetSubkeys():\n        values_dict['serial'] = devicekey.name\n\n        event_data = windows_events.WindowsRegistryEventData()\n        event_data.key_path = registry_key.path\n        event_data.offset = registry_key.offset\n        event_data.regvalue = values_dict\n        event_data.source_append = self._SOURCE_APPEND\n\n        # Last USB connection per USB device recorded in the Registry.\n        event = time_events.DateTimeValuesEvent(\n            devicekey.last_written_time,\n            definitions.TIME_DESCRIPTION_LAST_CONNECTED)\n        parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Extracts events from a Windows Registry key."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _FormatPackedIPv6Address(self, packed_ip_address):\n    # Note that socket.inet_ntop() is not supported on Windows.\n    octet_pairs = zip(packed_ip_address[0::2], packed_ip_address[1::2])\n    octet_pairs = [octet1 << 8 | octet2 for octet1, octet2 in octet_pairs]\n    # TODO: omit \":0000\" from the string.\n    return ':'.join([\n        '{0:04x}'.format(octet_pair) for octet_pair in octet_pairs])", "response": "Formats a packed IPv6 address as a human readable string."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading a structure from a file - like object.", "response": "def _ReadStructureFromFileObject(\n      self, file_object, file_offset, data_type_map):\n    \"\"\"Reads a structure from a file-like object.\n\n    If the data type map has a fixed size this method will read the predefined\n    number of bytes from the file-like object. If the data type map has a\n    variable size, depending on values in the byte stream, this method will\n    continue to read from the file-like object until the data type map can be\n    successfully mapped onto the byte stream or until an error occurs.\n\n    Args:\n      file_object (dfvfs.FileIO): a file-like object to parse.\n      file_offset (int): offset of the structure data relative to the start\n          of the file-like object.\n      data_type_map (dtfabric.DataTypeMap): data type map of the structure.\n\n    Returns:\n      tuple[object, int]: structure values object and data size of\n          the structure.\n\n    Raises:\n      ParseError: if the structure cannot be read.\n      ValueError: if file-like object or data type map is missing.\n    \"\"\"\n    context = None\n    data = b''\n    last_data_size = 0\n\n    data_size = data_type_map.GetByteSize()\n    if not data_size:\n      data_size = data_type_map.GetSizeHint()\n\n    while data_size != last_data_size:\n      read_offset = file_offset + last_data_size\n      read_size = data_size - last_data_size\n      data_segment = self._ReadData(file_object, read_offset, read_size)\n\n      data = b''.join([data, data_segment])\n\n      try:\n        context = dtfabric_data_maps.DataTypeMapContext()\n        structure_values_object = data_type_map.MapByteStream(\n            data, context=context)\n        return structure_values_object, data_size\n\n      except dtfabric_errors.ByteStreamTooSmallError:\n        pass\n\n      except dtfabric_errors.MappingError as exception:\n        raise errors.ParseError((\n            'Unable to map {0:s} data at offset: 0x{1:08x} with error: '\n            '{2!s}').format(data_type_map.name, file_offset, exception))\n\n      last_data_size = data_size\n      data_size = data_type_map.GetSizeHint(context=context)\n\n    raise errors.ParseError(\n        'Unable to read {0:s} at offset: 0x{1:08x}'.format(\n            data_type_map.name, file_offset))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef SetRawFields(self, raw_fields):\n    self._raw_fields = raw_fields\n\n    if raw_fields:\n      logger.debug('Elasticsearch adding raw (non-analyzed) fields.')\n    else:\n      logger.debug('Elasticsearch not adding raw (non-analyzed) fields.')", "response": "Sets the raw fields of the object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconnect to the Elasticsearch server and creates the index.", "response": "def WriteHeader(self):\n    \"\"\"Connects to the Elasticsearch server and creates the index.\"\"\"\n    mappings = {}\n\n    if self._raw_fields:\n      if self._document_type not in mappings:\n        mappings[self._document_type] = {}\n\n      mappings[self._document_type]['dynamic_templates'] = [{\n          'strings': {\n              'match_mapping_type': 'string',\n              'mapping': {\n                  'fields': {\n                      'raw': {\n                          'type': 'keyword',\n                          'index': 'false',\n                          'ignore_above': self._ELASTIC_ANALYZER_STRING_LIMIT\n                      }\n                  }\n              }\n          }\n      }]\n\n    self._Connect()\n\n    self._CreateIndexIfNotExists(self._index_name, mappings)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsearch the plist key hierarchy for keys with matching names.", "response": "def _FindKeys(self, key, names, matches):\n    \"\"\"Searches the plist key hierarchy for keys with matching names.\n\n    If a match is found a tuple of the key name and value is added to\n    the matches list.\n\n    Args:\n      key (dict[str, object]): plist key.\n      names (list[str]): names of the keys to match.\n      matches (list[str]): keys with matching names.\n    \"\"\"\n    for name, subkey in iter(key.items()):\n      if name in names:\n        matches.append((name, subkey))\n\n      if isinstance(subkey, dict):\n        self._FindKeys(subkey, names, matches)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing the artifact value data.", "response": "def _ParseFileData(self, knowledge_base, file_object):\n    \"\"\"Parses file content (data) for a preprocessing attribute.\n\n    Args:\n      knowledge_base (KnowledgeBase): to fill with preprocessing information.\n      file_object (dfvfs.FileIO): file-like object that contains the artifact\n          value data.\n\n    Raises:\n      errors.PreProcessFail: if the preprocessing fails.\n    \"\"\"\n    plist_file = plist.PlistFile()\n\n    try:\n      plist_file.Read(file_object)\n\n    except IOError as exception:\n      raise errors.PreProcessFail(\n          'Unable to read: {0:s} with error: {1!s}'.format(\n              self.ARTIFACT_DEFINITION_NAME, exception))\n\n    if not plist_file.root_key:\n      raise errors.PreProcessFail((\n          'Unable to read: {0:s} with error: missing root key').format(\n              self.ARTIFACT_DEFINITION_NAME))\n\n    matches = []\n\n    self._FindKeys(plist_file.root_key, self._PLIST_KEYS, matches)\n    if not matches:\n      raise errors.PreProcessFail(\n          'Unable to read: {0:s} with error: no such keys: {1:s}.'.format(\n              self.ARTIFACT_DEFINITION_NAME, ', '.join(self._PLIST_KEYS)))\n\n    name = None\n    value = None\n    for name, value in matches:\n      if value:\n        break\n\n    if value is None:\n      raise errors.PreProcessFail((\n          'Unable to read: {0:s} with error: no values found for keys: '\n          '{1:s}.').format(\n              self.ARTIFACT_DEFINITION_NAME, ', '.join(self._PLIST_KEYS)))\n\n    self._ParsePlistKeyValue(knowledge_base, name, value)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse a plist key value.", "response": "def _ParsePlistKeyValue(self, knowledge_base, name, value):\n    \"\"\"Parses a plist key value.\n\n    Args:\n      knowledge_base (KnowledgeBase): to fill with preprocessing information.\n      name (str): name of the plist key.\n      value (str): value of the plist key.\n    \"\"\"\n    if not knowledge_base.GetHostname():\n      if name in self._PLIST_KEYS:\n        hostname_artifact = artifacts.HostnameArtifact(name=value)\n        knowledge_base.SetHostname(hostname_artifact)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses a plist key value.", "response": "def _ParsePlistKeyValue(self, knowledge_base, name, value):\n    \"\"\"Parses a plist key value.\n\n    Args:\n      knowledge_base (KnowledgeBase): to fill with preprocessing information.\n      name (str): name of the plist key.\n      value (str): value of the plist key.\n    \"\"\"\n    if not knowledge_base.GetValue('keyboard_layout'):\n      if name in self._PLIST_KEYS:\n        if isinstance(value, (list, tuple)):\n          value = value[0]\n\n        _, _, keyboard_layout = value.rpartition('.')\n\n        knowledge_base.SetValue('keyboard_layout', keyboard_layout)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _ParsePlistKeyValue(self, knowledge_base, name, value):\n    if not knowledge_base.GetValue('operating_system_version'):\n      if name in self._PLIST_KEYS:\n        knowledge_base.SetValue('operating_system_version', value)", "response": "Parses a plist key value."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse the artifact file system data for a preprocessing attribute.", "response": "def _ParseFileEntry(self, knowledge_base, file_entry):\n    \"\"\"Parses artifact file system data for a preprocessing attribute.\n\n    Args:\n      knowledge_base (KnowledgeBase): to fill with preprocessing information.\n      file_entry (dfvfs.FileEntry): file entry that contains the artifact\n          value data.\n\n    Raises:\n      errors.PreProcessFail: if the preprocessing fails.\n    \"\"\"\n    if not file_entry or not file_entry.link:\n      raise errors.PreProcessFail(\n          'Unable to read: {0:s} with error: not a symbolic link'.format(\n              self.ARTIFACT_DEFINITION_NAME))\n\n    _, _, time_zone = file_entry.link.partition('zoneinfo/')\n    # TODO: check if time zone is set in knowledge base.\n    if time_zone:\n      try:\n        knowledge_base.SetTimeZone(time_zone)\n      except ValueError:\n        # TODO: add and store preprocessing errors.\n        pass"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nretrieve plist keys defaulting to empty values.", "response": "def _GetKeysDefaultEmpty(self, top_level, keys, depth=1):\n    \"\"\"Retrieves plist keys, defaulting to empty values.\n\n    Args:\n      top_level (plistlib._InternalDict): top level plist object.\n      keys (set[str]): names of keys that should be returned.\n      depth (int): depth within the plist, where 1 is top level.\n\n    Returns:\n      dict[str, str]: values of the requested keys.\n    \"\"\"\n    keys = set(keys)\n    match = {}\n\n    if depth == 1:\n      for key in keys:\n        value = top_level.get(key, None)\n        if value is not None:\n          match[key] = value\n    else:\n      for _, parsed_key, parsed_value in plist_interface.RecurseKey(\n          top_level, depth=depth):\n        if parsed_key in keys:\n          match[parsed_key] = parsed_value\n          if set(match.keys()) == keys:\n            return match\n    return match"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nretrieve the root key of a plist file.", "response": "def _GetPlistRootKey(self, file_entry):\n    \"\"\"Retrieves the root key of a plist file.\n\n    Args:\n      file_entry (dfvfs.FileEntry): file entry of the plist.\n\n    Returns:\n      dict[str, object]: plist root key.\n\n    Raises:\n      errors.PreProcessFail: if the preprocessing fails.\n    \"\"\"\n    file_object = file_entry.GetFileObject()\n\n    try:\n      plist_file = plist.PlistFile()\n      plist_file.Read(file_object)\n\n    except IOError as exception:\n      location = getattr(file_entry.path_spec, 'location', '')\n      raise errors.PreProcessFail(\n          'Unable to read plist file: {0:s} with error: {1!s}'.format(\n              location, exception))\n\n    finally:\n      file_object.close()\n\n    return plist_file.root_key"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses the artifact file system data for a preprocessing attribute.", "response": "def _ParseFileEntry(self, knowledge_base, file_entry):\n    \"\"\"Parses artifact file system data for a preprocessing attribute.\n\n    Args:\n      knowledge_base (KnowledgeBase): to fill with preprocessing information.\n      file_entry (dfvfs.FileEntry): file entry that contains the artifact\n          value data.\n\n    Raises:\n      errors.PreProcessFail: if the preprocessing fails.\n    \"\"\"\n    root_key = self._GetPlistRootKey(file_entry)\n    if not root_key:\n      location = getattr(file_entry.path_spec, 'location', '')\n      raise errors.PreProcessFail((\n          'Unable to read: {0:s} plist: {1:s} with error: missing root '\n          'key.').format(self.ARTIFACT_DEFINITION_NAME, location))\n\n    try:\n      match = self._GetKeysDefaultEmpty(root_key, self._KEYS)\n    except KeyError as exception:\n      location = getattr(file_entry.path_spec, 'location', '')\n      raise errors.PreProcessFail(\n          'Unable to read: {0:s} plist: {1:s} with error: {2!s}'.format(\n              self.ARTIFACT_DEFINITION_NAME, location, exception))\n\n    name = match.get('name', [None])[0]\n    uid = match.get('uid', [None])[0]\n\n    if not name or not uid:\n      # TODO: add and store preprocessing errors.\n      return\n\n    user_account = artifacts.UserAccountArtifact(\n        identifier=uid, username=name)\n    user_account.group_identifier = match.get('gid', [None])[0]\n    user_account.full_name = match.get('realname', [None])[0]\n    user_account.shell = match.get('shell', [None])[0]\n    user_account.user_directory = match.get('home', [None])[0]\n\n    try:\n      knowledge_base.AddUserAccount(user_account)\n    except KeyError:\n      # TODO: add and store preprocessing errors.\n      pass"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing a cookie row.", "response": "def ParseCookieRow(self, parser_mediator, query, row, **unused_kwargs):\n    \"\"\"Parses a cookie row.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      query (str): query that created the row.\n      row (sqlite3.Row): row.\n    \"\"\"\n    query_hash = hash(query)\n\n    cookie_data = self._GetRowValue(query_hash, row, 'value')\n    cookie_name = self._GetRowValue(query_hash, row, 'name')\n\n    hostname = self._GetRowValue(query_hash, row, 'host')\n    if hostname.startswith('.'):\n      hostname = hostname[1:]\n\n    is_secure = bool(self._GetRowValue(query_hash, row, 'isSecure'))\n    if is_secure:\n      url_scheme = 'https'\n    else:\n      url_scheme = 'http'\n\n    path = self._GetRowValue(query_hash, row, 'path')\n    url = '{0:s}://{1:s}{2:s}'.format(url_scheme, hostname, path)\n\n    event_data = FirefoxCookieEventData()\n    event_data.cookie_name = cookie_name\n    event_data.data = cookie_data\n    event_data.host = hostname\n    event_data.httponly = bool(self._GetRowValue(query_hash, row, 'isHttpOnly'))\n    event_data.offset = self._GetRowValue(query_hash, row, 'id')\n    event_data.path = path\n    event_data.query = query\n    event_data.secure = is_secure\n    event_data.url = url\n\n    timestamp = self._GetRowValue(query_hash, row, 'creationTime')\n    if timestamp:\n      date_time = dfdatetime_posix_time.PosixTimeInMicroseconds(\n          timestamp=timestamp)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_CREATION)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    timestamp = self._GetRowValue(query_hash, row, 'lastAccessed')\n    if timestamp:\n      date_time = dfdatetime_posix_time.PosixTimeInMicroseconds(\n          timestamp=timestamp)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_LAST_ACCESS)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    timestamp = self._GetRowValue(query_hash, row, 'expiry')\n    if timestamp:\n      # Expiry time (nsCookieService::GetExpiry in\n      # netwerk/cookie/nsCookieService.cpp).\n      # It's calculated as the difference between the server time and the time\n      # the server wants the cookie to expire and adding that difference to the\n      # client time. This localizes the client time regardless of whether or not\n      # the TZ environment variable was set on the client.\n\n      date_time = dfdatetime_posix_time.PosixTime(\n          timestamp=timestamp)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_EXPIRATION)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    # Go through all cookie plugins to see if there are is any specific parsing\n    # needed.\n    for cookie_plugin in self._cookie_plugins:\n      try:\n        cookie_plugin.UpdateChainAndProcess(\n            parser_mediator, cookie_name=cookie_name, cookie_data=cookie_data,\n            url=url)\n      except errors.WrongPlugin:\n        pass"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nflattening nested dictionaries and lists by yielding it s values.", "response": "def RecurseKey(recur_item, depth=15, key_path=''):\n  \"\"\"Flattens nested dictionaries and lists by yielding it's values.\n\n  The hierarchy of a plist file is a series of nested dictionaries and lists.\n  This is a helper function helps plugins navigate the structure without\n  having to reimplement their own recursive methods.\n\n  This method implements an overridable depth limit to prevent processing\n  extremely deeply nested plists. If the limit is reached a debug message is\n  logged indicating which key processing stopped on.\n\n  Example Input Plist:\n    recur_item = { DeviceRoot: { DeviceMAC1: [Value1, Value2, Value3],\n                                 DeviceMAC2: [Value1, Value2, Value3]}}\n\n  Example Output:\n    ('', DeviceRoot, {DeviceMACs...})\n    (DeviceRoot, DeviceMAC1, [Value1, Value2, Value3])\n    (DeviceRoot, DeviceMAC2, [Value1, Value2, Value3])\n\n  Args:\n    recur_item: An object to be checked for additional nested items.\n    depth: Optional integer indication the current recursion depth.\n           This value is used to ensure we stop at the maximum recursion depth.\n    key_path: Optional path of the current working key.\n\n  Yields:\n    A tuple of the key path, key, and value from a plist.\n  \"\"\"\n  if depth < 1:\n    logger.debug('Recursion limit hit for key: {0:s}'.format(key_path))\n    return\n\n  if isinstance(recur_item, (list, tuple)):\n    for recur in recur_item:\n      for key in RecurseKey(recur, depth=depth, key_path=key_path):\n        yield key\n    return\n\n  if not hasattr(recur_item, 'items'):\n    return\n\n  for subkey, value in iter(recur_item.items()):\n    yield key_path, subkey, value\n\n    if isinstance(value, dict):\n      value = [value]\n\n    if isinstance(value, list):\n      for item in value:\n        if not isinstance(item, dict):\n          continue\n\n        subkey_path = '{0:s}/{1:s}'.format(key_path, subkey)\n        for tuple_value in RecurseKey(\n            item, depth=depth - 1, key_path=subkey_path):\n          yield tuple_value"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _GetKeys(self, top_level, keys, depth=1):\n    match = {}\n    if not isinstance(top_level, dict):\n      # Return an empty dict here if top_level is a list object, which happens\n      # if the plist file is flat.\n      return match\n    keys = set(keys)\n\n    if depth == 1:\n      for key in keys:\n        match[key] = top_level.get(key, None)\n    else:\n      for _, parsed_key, parsed_value in RecurseKey(top_level, depth=depth):\n        if parsed_key in keys:\n          match[parsed_key] = parsed_value\n          if set(match.keys()) == keys:\n            return match\n    return match", "response": "Internal function to return the keys nested in a plist dict."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprocessing the plist file.", "response": "def Process(self, parser_mediator, plist_name, top_level, **kwargs):\n    \"\"\"Determine if this is the correct plugin; if so proceed with processing.\n\n    Process() checks if the current plist being processed is a match for a\n    plugin by comparing the PATH and KEY requirements defined by a plugin.  If\n    both match processing continues; else raise WrongPlistPlugin.\n\n    This function also extracts the required keys as defined in self.PLIST_KEYS\n    from the plist and stores the result in self.match[key] and calls\n    self.GetEntries() which holds the processing logic implemented by the\n    plugin.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      plist_name (str): name of the plist.\n      top_level (dict[str, object]): plist top-level key.\n\n    Raises:\n      WrongPlistPlugin: If this plugin is not able to process the given file.\n      ValueError: If top_level or plist_name are not set.\n    \"\"\"\n    if plist_name is None or top_level is None:\n      raise ValueError('Top level or plist name are not set.')\n\n    if plist_name.lower() != self.PLIST_PATH.lower():\n      raise errors.WrongPlistPlugin(self.NAME, plist_name)\n\n    if isinstance(top_level, dict):\n      if not set(top_level.keys()).issuperset(self.PLIST_KEYS):\n        raise errors.WrongPlistPlugin(self.NAME, plist_name)\n\n    else:\n      # Make sure we are getting back an object that has an iterator.\n      if not hasattr(top_level, '__iter__'):\n        raise errors.WrongPlistPlugin(self.NAME, plist_name)\n\n      # This is a list and we need to just look at the first level\n      # of keys there.\n      keys = []\n      for top_level_entry in top_level:\n        if isinstance(top_level_entry, dict):\n          keys.extend(top_level_entry.keys())\n\n      # Compare this is a set, which removes possible duplicate entries\n      # in the list.\n      if not set(keys).issuperset(self.PLIST_KEYS):\n        raise errors.WrongPlistPlugin(self.NAME, plist_name)\n\n    # This will raise if unhandled keyword arguments are passed.\n    super(PlistPlugin, self).Process(parser_mediator)\n\n    logger.debug('Plist Plugin Used: {0:s} for: {1:s}'.format(\n        self.NAME, plist_name))\n    match = self._GetKeys(top_level, self.PLIST_KEYS)\n    self.GetEntries(parser_mediator, top_level=top_level, match=match)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nextract events from a Windows Registry key.", "response": "def ExtractEvents(self, parser_mediator, registry_key, **kwargs):\n    \"\"\"Extracts events from a Windows Registry key.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      registry_key (dfwinreg.WinRegistryKey): Windows Registry key.\n    \"\"\"\n    values_dict = {}\n\n    if registry_key.number_of_values == 0:\n      values_dict['Value'] = 'No values stored in key.'\n\n    else:\n      for registry_value in registry_key.GetValues():\n        value_name = registry_value.name or '(default)'\n\n        if registry_value.data is None:\n          value_string = '[{0:s}] Empty'.format(\n              registry_value.data_type_string)\n\n        elif registry_value.DataIsString():\n          value_string = registry_value.GetDataAsObject()\n          value_string = '[{0:s}] {1:s}'.format(\n              registry_value.data_type_string, value_string)\n\n        elif registry_value.DataIsInteger():\n          value_integer = registry_value.GetDataAsObject()\n          value_string = '[{0:s}] {1:d}'.format(\n              registry_value.data_type_string, value_integer)\n\n        elif registry_value.DataIsMultiString():\n          multi_string = registry_value.GetDataAsObject()\n          if not isinstance(multi_string, (list, tuple)):\n            value_string = '[{0:s}]'.format(registry_value.data_type_string)\n            # TODO: Add a flag or some sort of an anomaly alert.\n          else:\n            value_string = '[{0:s}] {1:s}'.format(\n                registry_value.data_type_string, ''.join(multi_string))\n\n        else:\n          value_string = '[{0:s}]'.format(registry_value.data_type_string)\n\n        values_dict[value_name] = value_string\n\n    event_data = windows_events.WindowsRegistryEventData()\n    event_data.key_path = registry_key.path\n    event_data.offset = registry_key.offset\n    event_data.regvalue = values_dict\n\n    event = time_events.DateTimeValuesEvent(\n        registry_key.last_written_time, definitions.TIME_DESCRIPTION_WRITTEN)\n    parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nextracting relevant BT entries.", "response": "def GetEntries(self, parser_mediator, match=None, **unused_kwargs):\n    \"\"\"Extracts relevant BT entries.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      match (Optional[dict[str: object]]): keys extracted from PLIST_KEYS.\n    \"\"\"\n    device_cache = match.get('DeviceCache', {})\n    for device, value in iter(device_cache.items()):\n      name = value.get('Name', '')\n      if name:\n        name = ''.join(('Name:', name))\n\n      event_data = plist_event.PlistTimeEventData()\n      event_data.root = '/DeviceCache'\n\n      datetime_value = value.get('LastInquiryUpdate', None)\n      if datetime_value:\n        event_data.desc = ' '.join(\n            filter(None, ('Bluetooth Discovery', name)))\n        event_data.key = '{0:s}/LastInquiryUpdate'.format(device)\n\n        event = time_events.PythonDatetimeEvent(\n            datetime_value, definitions.TIME_DESCRIPTION_WRITTEN)\n        parser_mediator.ProduceEventWithEventData(event, event_data)\n\n        if device in match.get('PairedDevices', []):\n          event_data.desc = 'Paired:True {0:s}'.format(name)\n          event_data.key = device\n\n          event = time_events.PythonDatetimeEvent(\n              datetime_value, definitions.TIME_DESCRIPTION_WRITTEN)\n          parser_mediator.ProduceEventWithEventData(event, event_data)\n\n      datetime_value = value.get('LastNameUpdate', None)\n      if datetime_value:\n        event_data.desc = ' '.join(filter(None, ('Device Name Set', name)))\n        event_data.key = '{0:s}/LastNameUpdate'.format(device)\n\n        event = time_events.PythonDatetimeEvent(\n            datetime_value, definitions.TIME_DESCRIPTION_WRITTEN)\n        parser_mediator.ProduceEventWithEventData(event, event_data)\n\n      datetime_value = value.get('LastServicesUpdate', None)\n      if datetime_value:\n        event_data.desc = ' '.join(filter(None, ('Services Updated', name)))\n        event_data.key = '{0:s}/LastServicesUpdate'.format(device)\n\n        event = time_events.PythonDatetimeEvent(\n            datetime_value, definitions.TIME_DESCRIPTION_WRITTEN)\n        parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse the file content for a hostname preprocessing attribute.", "response": "def _ParseFileData(self, knowledge_base, file_object):\n    \"\"\"Parses file content (data) for a hostname preprocessing attribute.\n\n    Args:\n      knowledge_base (KnowledgeBase): to fill with preprocessing information.\n      file_object (dfvfs.FileIO): file-like object that contains the artifact\n          value data.\n\n    Raises:\n      errors.PreProcessFail: if the preprocessing fails.\n    \"\"\"\n    text_file_object = dfvfs_text_file.TextFile(file_object, encoding='utf-8')\n\n    if not knowledge_base.GetHostname():\n      hostname = text_file_object.readline()\n      hostname = hostname.strip()\n      if hostname:\n        hostname_artifact = artifacts.HostnameArtifact(name=hostname)\n        knowledge_base.SetHostname(hostname_artifact)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses the artifact value data.", "response": "def _ParseFileData(self, knowledge_base, file_object):\n    \"\"\"Parses file content (data) for system product preprocessing attribute.\n\n    Args:\n      knowledge_base (KnowledgeBase): to fill with preprocessing information.\n      file_object (dfvfs.FileIO): file-like object that contains the artifact\n          value data.\n\n    Raises:\n      errors.PreProcessFail: if the preprocessing fails.\n    \"\"\"\n    text_file_object = dfvfs_text_file.TextFile(file_object, encoding='utf-8')\n\n    system_product = text_file_object.readline()\n    system_product = system_product.strip()\n\n    if not knowledge_base.GetValue('operating_system_product'):\n      if system_product:\n        knowledge_base.SetValue('operating_system_product', system_product)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses the file content for the artifact value.", "response": "def _ParseFileData(self, knowledge_base, file_object):\n    \"\"\"Parses file content (data) for system product preprocessing attribute.\n\n    Args:\n      knowledge_base (KnowledgeBase): to fill with preprocessing information.\n      file_object (dfvfs.FileIO): file-like object that contains the artifact\n          value data.\n\n    Raises:\n      errors.PreProcessFail: if the preprocessing fails.\n    \"\"\"\n    text_file_object = dfvfs_text_file.TextFile(file_object, encoding='utf-8')\n\n    system_product = text_file_object.readline()\n\n    # Only parse known default /etc/issue file contents.\n    if system_product.startswith('Debian GNU/Linux '):\n      system_product, _, _ = system_product.partition('\\\\')\n      system_product = system_product.rstrip()\n\n    else:\n      system_product = None\n\n    if not knowledge_base.GetValue('operating_system_product'):\n      if system_product:\n        knowledge_base.SetValue('operating_system_product', system_product)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses the artifact value data.", "response": "def _ParseFileData(self, knowledge_base, file_object):\n    \"\"\"Parses file content (data) for system product preprocessing attribute.\n\n    Args:\n      knowledge_base (KnowledgeBase): to fill with preprocessing information.\n      file_object (dfvfs.FileIO): file-like object that contains the artifact\n          value data.\n\n    Raises:\n      errors.PreProcessFail: if the preprocessing fails.\n    \"\"\"\n    text_file_object = dfvfs_text_file.TextFile(file_object, encoding='utf-8')\n\n    product_values = {}\n    for line in text_file_object.readlines():\n      line = line.strip()\n      if line.startswith('#'):\n        continue\n      key, value = line.split('=')\n      key = key.strip().upper()\n      value = value.strip().strip('\"')\n      product_values[key] = value\n\n    if not knowledge_base.GetValue('operating_system_product'):\n      system_product = product_values.get('DISTRIB_DESCRIPTION', None)\n      if system_product:\n        knowledge_base.SetValue('operating_system_product', system_product)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse the artifact file system data for a preprocessing attribute.", "response": "def _ParseFileEntry(self, knowledge_base, file_entry):\n    \"\"\"Parses artifact file system data for a preprocessing attribute.\n\n    Args:\n      knowledge_base (KnowledgeBase): to fill with preprocessing information.\n      file_entry (dfvfs.FileEntry): file entry that contains the artifact\n          value data.\n\n    Raises:\n      errors.PreProcessFail: if the preprocessing fails.\n    \"\"\"\n    if file_entry.link:\n      # Determine the timezone based on the file path.\n      _, _, time_zone = file_entry.link.partition('zoneinfo/')\n\n    else:\n      # Determine the timezone based on the timezone information file.\n      file_object = file_entry.GetFileObject()\n\n      time_zone = None\n      try:\n        time_zone_file = tz.tzfile(file_object)\n        date_time = datetime.datetime(2017, 1, 1)\n        time_zone = time_zone_file.tzname(date_time)\n\n      except ValueError:\n        # TODO: add and store preprocessing errors.\n        logger.error('Unable to read time zone information file.')\n\n      finally:\n        file_object.close()\n\n    # TODO: check if time zone is set in knowledge base.\n    if time_zone:\n      try:\n        knowledge_base.SetTimeZone(time_zone)\n      except ValueError:\n        # TODO: add and store preprocessing errors.\n        logger.error('Unable to set time zone in knowledge base.')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _ParseFileData(self, knowledge_base, file_object):\n    line_reader = line_reader_file.BinaryLineReader(file_object)\n\n    try:\n      reader = line_reader_file.BinaryDSVReader(line_reader, b':')\n    except csv.Error as exception:\n      raise errors.PreProcessFail(\n          'Unable to read: {0:s} with error: {1!s}'.format(\n              self.ARTIFACT_DEFINITION_NAME, exception))\n\n    for row in reader:\n      if len(row) < 7 or not row[0] or not row[2]:\n        # TODO: add and store preprocessing errors.\n        continue\n\n      try:\n        username = row[0].decode('utf-8')\n      except UnicodeDecodeError:\n        # TODO: add and store preprocessing errors.\n        logger.error('Unable to decode username.')\n        continue\n\n      try:\n        identifier = row[2].decode('utf-8')\n      except UnicodeDecodeError:\n        # TODO: add and store preprocessing errors.\n        logger.error('Unable to decode identifier.')\n        continue\n\n      group_identifier = None\n      if row[3]:\n        try:\n          group_identifier = row[3].decode('utf-8')\n        except UnicodeDecodeError:\n          # TODO: add and store preprocessing errors.\n          logger.error('Unable to decode group identifier.')\n\n      full_name = None\n      if row[4]:\n        try:\n          full_name = row[4].decode('utf-8')\n        except UnicodeDecodeError:\n          # TODO: add and store preprocessing errors.\n          logger.error('Unable to decode full name.')\n\n      user_directory = None\n      if row[5]:\n        try:\n          user_directory = row[5].decode('utf-8')\n        except UnicodeDecodeError:\n          # TODO: add and store preprocessing errors.\n          logger.error('Unable to decode user directory.')\n\n      shell = None\n      if row[6]:\n        try:\n          shell = row[6].decode('utf-8')\n        except UnicodeDecodeError:\n          # TODO: add and store preprocessing errors.\n          logger.error('Unable to decode shell.')\n\n      user_account = artifacts.UserAccountArtifact(\n          identifier=identifier, username=username)\n      user_account.group_identifier = group_identifier\n      user_account.full_name = full_name\n      user_account.user_directory = user_directory\n      user_account.shell = shell\n\n      try:\n        knowledge_base.AddUserAccount(user_account)\n      except KeyError:\n        # TODO: add and store preprocessing errors.\n        pass", "response": "Parses the file content for user account preprocessing attributes."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef AddArguments(cls, argument_group):\n    argument_group.add_argument(\n        '--index_name', dest='index_name', type=str, action='store',\n        default=cls._DEFAULT_INDEX_NAME, help=(\n            'Name of the index in ElasticSearch.'))\n    argument_group.add_argument(\n        '--doc_type', dest='document_type', type=str,\n        action='store', default=cls._DEFAULT_DOCUMENT_TYPE, help=(\n            'Name of the document type that will be used in ElasticSearch.'))\n    argument_group.add_argument(\n        '--flush_interval', dest='flush_interval', type=int,\n        action='store', default=cls._DEFAULT_FLUSH_INTERVAL, help=(\n            'Events to queue up before bulk insert to ElasticSearch.'))\n    argument_group.add_argument(\n        '--raw_fields', dest='raw_fields', action='store_true',\n        default=cls._DEFAULT_RAW_FIELDS, help=(\n            'Export string fields that will not be analyzed by Lucene.'))\n    argument_group.add_argument(\n        '--elastic_user', dest='elastic_user', action='store',\n        default=cls._DEFAULT_ELASTIC_USER, help=(\n            'Username to use for Elasticsearch authentication.'))\n    argument_group.add_argument(\n        '--use_ssl', dest='use_ssl', action='store_true',\n        help='Enforces use of ssl.')\n    argument_group.add_argument(\n        '--ca_certificates_file_path', dest='ca_certificates_file_path',\n        action='store', type=str, default=cls._DEFAULT_CA_CERTS, help=(\n            'Path to a file containing a list of root certificates to trust.'))\n    argument_group.add_argument(\n        '--elastic_url_prefix', dest='elastic_url_prefix', type=str,\n        action='store', default=cls._DEFAULT_URL_PREFIX, help=(\n            'URL prefix for elastic search.'))\n\n    ElasticSearchServerArgumentsHelper.AddArguments(argument_group)", "response": "Adds command line arguments to an argument group."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses and validates the options.", "response": "def ParseOptions(cls, options, output_module):\n    \"\"\"Parses and validates options.\n\n    Args:\n      options (argparse.Namespace): parser options.\n      output_module (OutputModule): output module to configure.\n\n    Raises:\n      BadConfigObject: when the output module object is of the wrong type.\n      BadConfigOption: when a configuration parameter fails validation.\n    \"\"\"\n    elastic_output_modules = (\n        elastic.ElasticsearchOutputModule, elastic.ElasticsearchOutputModule)\n    if not isinstance(output_module, elastic_output_modules):\n      raise errors.BadConfigObject(\n          'Output module is not an instance of ElasticsearchOutputModule')\n\n    index_name = cls._ParseStringOption(\n        options, 'index_name', default_value=cls._DEFAULT_INDEX_NAME)\n    document_type = cls._ParseStringOption(\n        options, 'document_type', default_value=cls._DEFAULT_DOCUMENT_TYPE)\n    flush_interval = cls._ParseNumericOption(\n        options, 'flush_interval', default_value=cls._DEFAULT_FLUSH_INTERVAL)\n    raw_fields = getattr(\n        options, 'raw_fields', cls._DEFAULT_RAW_FIELDS)\n    elastic_user = cls._ParseStringOption(\n        options, 'elastic_user', default_value=cls._DEFAULT_ELASTIC_USER)\n\n    use_ssl = getattr(options, 'use_ssl', False)\n\n    ca_certificates_path = cls._ParseStringOption(\n        options, 'ca_certificates_file_path',\n        default_value=cls._DEFAULT_CA_CERTS)\n    elastic_url_prefix = cls._ParseStringOption(\n        options, 'elastic_url_prefix', default_value=cls._DEFAULT_URL_PREFIX)\n\n    if elastic_user is not None:\n      elastic_password = getpass.getpass(\n          'Enter your Elasticsearch password: ')\n    else:\n      elastic_password = None\n\n    ElasticSearchServerArgumentsHelper.ParseOptions(options, output_module)\n    output_module.SetIndexName(index_name)\n    output_module.SetDocumentType(document_type)\n    output_module.SetFlushInterval(flush_interval)\n    output_module.SetRawFields(raw_fields)\n    output_module.SetUsername(elastic_user)\n    output_module.SetPassword(elastic_password)\n    output_module.SetUseSSL(use_ssl)\n    output_module.SetCACertificatesPath(ca_certificates_path)\n    output_module.SetURLPrefix(elastic_url_prefix)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _DictToListOfStrings(self, data_dict):\n    ret_list = []\n    for key, value in iter(data_dict.items()):\n      if key in ('body', 'datetime', 'type', 'room', 'rooms', 'id'):\n        continue\n      ret_list.append('{0:s} = {1!s}'.format(key, value))\n\n    return ret_list", "response": "Converts a dictionary into a list of strings."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _ExtractJQuery(self, jquery_raw):\n    data_part = ''\n    if not jquery_raw:\n      return {}\n\n    if '[' in jquery_raw:\n      _, _, first_part = jquery_raw.partition('[')\n      data_part, _, _ = first_part.partition(']')\n    elif jquery_raw.startswith('//'):\n      _, _, first_part = jquery_raw.partition('{')\n      data_part = '{{{0:s}'.format(first_part)\n    elif '({' in jquery_raw:\n      _, _, first_part = jquery_raw.partition('(')\n      data_part, _, _ = first_part.rpartition(')')\n\n    if not data_part:\n      return {}\n\n    try:\n      data_dict = json.loads(data_part)\n    except ValueError:\n      return {}\n\n    return data_dict", "response": "Extracts values from a JQuery string."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse chat comment data.", "response": "def _ParseChatData(self, data):\n    \"\"\"Parses chat comment data.\n\n    Args:\n      data (dict[str, object]): chat comment data as returned by SQLite.\n\n    Returns:\n      dict[str, object]: parsed chat comment data.\n    \"\"\"\n    data_store = {}\n\n    if 'body' in data:\n      body = data.get('body', '').replace('\\n', ' ')\n      if body.startswith('//') and '{' in body:\n        body_dict = self._ExtractJQuery(body)\n        title, _, _ = body.partition('{')\n        body = '{0:s} <{1!s}>'.format(\n            title[2:], self._DictToListOfStrings(body_dict))\n    else:\n      body = 'No text.'\n\n    data_store['text'] = body\n\n    room = data.get('rooms', None)\n    if not room:\n      room = data.get('room', None)\n    if room:\n      data_store['room'] = room\n\n    data_store['id'] = data.get('id', None)\n    user = data.get('user', None)\n    if user:\n      try:\n        user_sid = int(user)\n        data_store['sid'] = user_sid\n      except (ValueError, TypeError):\n        data_store['user'] = user\n\n    return data_store"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse a single row from the receiver and cache response table.", "response": "def ParseReceiverData(\n      self, parser_mediator, query, row, **unused_kwargs):\n    \"\"\"Parses a single row from the receiver and cache response table.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      query (str): query that created the row.\n      row (sqlite3.Row): row.\n    \"\"\"\n    query_hash = hash(query)\n\n    data = {}\n    key_url = self._GetRowValue(query_hash, row, 'request_key')\n\n    data_dict = {}\n    description = 'MacKeeper Entry'\n    # Check the URL, since that contains vital information about the type of\n    # event we are dealing with.\n    if key_url.endswith('plist'):\n      description = 'Configuration Definition'\n      data['text'] = 'Plist content added to cache.'\n\n    elif key_url.startswith('http://event.zeobit.com'):\n      description = 'MacKeeper Event'\n      try:\n        _, _, part = key_url.partition('?')\n        data['text'] = part.replace('&', ' ')\n      except UnicodeDecodeError:\n        data['text'] = 'N/A'\n\n    elif key_url.startswith('http://account.zeobit.com'):\n      description = 'Account Activity'\n      _, _, activity = key_url.partition('#')\n      if activity:\n        data['text'] = 'Action started: {0:s}'.format(activity)\n      else:\n        data['text'] = 'Unknown activity.'\n\n    elif key_url.startswith('http://support.') and 'chat' in key_url:\n      description = 'Chat '\n      try:\n        jquery = self._GetRowValue(query_hash, row, 'data')\n        jquery = codecs.decode(jquery, 'utf-8')\n      except UnicodeDecodeError:\n        jquery = ''\n\n      data_dict = self._ExtractJQuery(jquery)\n      data = self._ParseChatData(data_dict)\n\n      data['entry_type'] = data_dict.get('type', '')\n      if data['entry_type'] == 'comment':\n        description += 'Comment'\n      elif data['entry_type'] == 'outgoing':\n        description += 'Outgoing Message'\n      elif data['entry_type'] == 'incoming':\n        description += 'Incoming Message'\n      else:\n        # Empty or not known entry type, generic status message.\n        description += 'Entry'\n        data['text'] = ';'.join(self._DictToListOfStrings(data_dict))\n        if not data['text']:\n          data['text'] = 'No additional data.'\n\n    event_data = MacKeeperCacheEventData()\n    event_data.description = description\n    event_data.event_type = data.get('event_type', None)\n    event_data.offset = self._GetRowValue(query_hash, row, 'id')\n    event_data.query = query\n    event_data.record_id = data.get('id', None)\n    event_data.room = data.get('room', None)\n    event_data.text = data.get('text', None)\n    event_data.url = key_url\n    event_data.user_name = data.get('user', None)\n    event_data.user_sid = data.get('sid', None)\n\n    time_value = self._GetRowValue(query_hash, row, 'time_string')\n    if isinstance(time_value, py2to3.INTEGER_TYPES):\n      date_time = dfdatetime_java_time.JavaTime(timestamp=time_value)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_ADDED)\n\n    else:\n      try:\n        timestamp = timelib.Timestamp.FromTimeString(time_value)\n      except errors.TimestampError:\n        parser_mediator.ProduceExtractionWarning(\n            'Unable to parse time string: {0:s}'.format(time_value))\n        return\n\n      event = time_events.TimestampEvent(\n          timestamp, definitions.TIME_DESCRIPTION_ADDED)\n\n    parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nretrieve the corresponding formatter object for a specific data type.", "response": "def GetFormatterObject(cls, data_type):\n    \"\"\"Retrieves the formatter object for a specific data type.\n\n    Args:\n      data_type (str): data type.\n\n    Returns:\n      EventFormatter: corresponding formatter or the default formatter if\n          not available.\n    \"\"\"\n    data_type = data_type.lower()\n    if data_type not in cls._formatter_objects:\n      formatter_object = None\n\n      if data_type in cls._formatter_classes:\n        formatter_class = cls._formatter_classes[data_type]\n        # TODO: remove the need to instantiate the Formatter classes\n        # and use class methods only.\n        formatter_object = formatter_class()\n\n      if not formatter_object:\n        logger.warning(\n            'Using default formatter for data type: {0:s}'.format(data_type))\n        formatter_object = default.DefaultFormatter()\n\n      cls._formatter_objects[data_type] = formatter_object\n\n    return cls._formatter_objects[data_type]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef GetMessageStrings(cls, formatter_mediator, event):\n    formatter_object = cls.GetFormatterObject(event.data_type)\n    return formatter_object.GetMessages(formatter_mediator, event)", "response": "Retrieves the formatted message strings for a specific event object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef GetSourceStrings(cls, event):\n    # TODO: change this to return the long variant first so it is consistent\n    # with GetMessageStrings.\n    formatter_object = cls.GetFormatterObject(event.data_type)\n    return formatter_object.GetSources(event)", "response": "Retrieves the formatted source strings for a specific event object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nretrieving all section names.", "response": "def _GetSectionNames(self, pefile_object):\n    \"\"\"Retrieves all PE section names.\n\n    Args:\n      pefile_object (pefile.PE): pefile object.\n\n    Returns:\n      list[str]: names of the sections.\n    \"\"\"\n    section_names = []\n    for section in pefile_object.sections:\n      section_name = getattr(section, 'Name', b'')\n      # Ensure the name is decoded correctly.\n      try:\n        section_name = '{0:s}'.format(section_name.decode('unicode_escape'))\n      except UnicodeDecodeError:\n        section_name = '{0:s}'.format(repr(section_name))\n      section_names.append(section_name)\n\n    return section_names"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving timestamps from the import directory if available.", "response": "def _GetImportTimestamps(self, pefile_object):\n    \"\"\"Retrieves timestamps from the import directory, if available.\n\n    Args:\n      pefile_object (pefile.PE): pefile object.\n\n    Returns:\n      list[int]: import timestamps.\n    \"\"\"\n    import_timestamps = []\n    if not hasattr(pefile_object, 'DIRECTORY_ENTRY_IMPORT'):\n      return import_timestamps\n    for importdata in pefile_object.DIRECTORY_ENTRY_IMPORT:\n      dll_name = getattr(importdata, 'dll', '')\n      try:\n        dll_name = dll_name.decode('ascii')\n      except UnicodeDecodeError:\n        dll_name = dll_name.decode('ascii', errors='replace')\n      if not dll_name:\n        dll_name = '<NO DLL NAME>'\n\n      timestamp = getattr(importdata.struct, 'TimeDateStamp', 0)\n      if timestamp:\n        import_timestamps.append([dll_name, timestamp])\n    return import_timestamps"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving timestamps from resource directory entries if available.", "response": "def _GetResourceTimestamps(self, pefile_object):\n    \"\"\"Retrieves timestamps from resource directory entries, if available.\n\n    Args:\n      pefile_object (pefile.PE): pefile object.\n\n    Returns:\n      list[int]: resource timestamps.\n    \"\"\"\n    timestamps = []\n    if not hasattr(pefile_object, 'DIRECTORY_ENTRY_RESOURCE'):\n      return timestamps\n    for entrydata in pefile_object.DIRECTORY_ENTRY_RESOURCE.entries:\n      directory = entrydata.directory\n      timestamp = getattr(directory, 'TimeDateStamp', 0)\n      if timestamp:\n        timestamps.append(timestamp)\n    return timestamps"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _GetLoadConfigTimestamp(self, pefile_object):\n    if not hasattr(pefile_object, 'DIRECTORY_ENTRY_LOAD_CONFIG'):\n      return None\n    timestamp = getattr(\n        pefile_object.DIRECTORY_ENTRY_LOAD_CONFIG.struct, 'TimeDateStamp', 0)\n    return timestamp", "response": "Retrieves the timestamp from the Load Configuration directory."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve timestamps from delay import entries if available.", "response": "def _GetDelayImportTimestamps(self, pefile_object):\n    \"\"\"Retrieves timestamps from delay import entries, if available.\n\n    Args:\n      pefile_object (pefile.PE): pefile object.\n\n    Returns:\n      tuple[str, int]: name of the DLL being imported and the second is\n          the timestamp of the entry.\n    \"\"\"\n    delay_import_timestamps = []\n    if not hasattr(pefile_object, 'DIRECTORY_ENTRY_DELAY_IMPORT'):\n      return delay_import_timestamps\n    for importdata in pefile_object.DIRECTORY_ENTRY_DELAY_IMPORT:\n      dll_name = importdata.dll\n      try:\n        dll_name = dll_name.decode('ascii')\n      except UnicodeDecodeError:\n        dll_name = dll_name.decode('ascii', errors='replace')\n\n      timestamp = getattr(importdata.struct, 'dwTimeStamp', 0)\n      delay_import_timestamps.append([dll_name, timestamp])\n    return delay_import_timestamps"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ParseFileObject(self, parser_mediator, file_object):\n    pe_data = file_object.read()\n    try:\n      pefile_object = pefile.PE(data=pe_data, fast_load=True)\n      pefile_object.parse_data_directories(\n          directories=[\n              pefile.DIRECTORY_ENTRY['IMAGE_DIRECTORY_ENTRY_IMPORT'],\n              pefile.DIRECTORY_ENTRY['IMAGE_DIRECTORY_ENTRY_EXPORT'],\n              pefile.DIRECTORY_ENTRY['IMAGE_DIRECTORY_ENTRY_RESOURCE'],\n              pefile.DIRECTORY_ENTRY['IMAGE_DIRECTORY_ENTRY_DELAY_IMPORT'],])\n    except:\n      raise errors.UnableToParseFile()\n\n    event_data = PEEventData()\n    event_data.imphash = pefile_object.get_imphash()\n    event_data.pe_type = self._GetPEType(pefile_object)\n    event_data.section_names = self._GetSectionNames(pefile_object)\n\n    # TODO: remove after refactoring the pe event formatter.\n    event_data.data_type = 'pe:compilation:compilation_time'\n\n    timestamp = getattr(pefile_object.FILE_HEADER, 'TimeDateStamp', None)\n    # TODO: handle timestamp is None.\n    date_time = dfdatetime_posix_time.PosixTime(timestamp=timestamp)\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_CREATION)\n    parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    for dll_name, timestamp in self._GetImportTimestamps(pefile_object):\n      if timestamp:\n        event_data.dll_name = dll_name\n        event_data.data_type = 'pe:import:import_time'\n\n        date_time = dfdatetime_posix_time.PosixTime(timestamp=timestamp)\n        event = time_events.DateTimeValuesEvent(\n            date_time, definitions.TIME_DESCRIPTION_MODIFICATION)\n        parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    for dll_name, timestamp in self._GetDelayImportTimestamps(pefile_object):\n      if timestamp:\n        event_data.dll_name = dll_name\n        event_data.data_type = 'pe:delay_import:import_time'\n\n        date_time = dfdatetime_posix_time.PosixTime(timestamp=timestamp)\n        event = time_events.DateTimeValuesEvent(\n            date_time, definitions.TIME_DESCRIPTION_MODIFICATION)\n        parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    event_data.dll_name = None\n\n    for timestamp in self._GetResourceTimestamps(pefile_object):\n      if timestamp:\n        event_data.data_type = 'pe:resource:creation_time'\n\n        date_time = dfdatetime_posix_time.PosixTime(timestamp=timestamp)\n        event = time_events.DateTimeValuesEvent(\n            date_time, definitions.TIME_DESCRIPTION_MODIFICATION)\n        parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    timestamp = self._GetLoadConfigTimestamp(pefile_object)\n    if timestamp:\n      event_data.data_type = 'pe:load_config:modification_time'\n\n      date_time = dfdatetime_posix_time.PosixTime(timestamp=timestamp)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_MODIFICATION)\n      parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Parses a Portable Executable file - like object."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates a reader for the values in the log file.", "response": "def _CreateDictReader(self, line_reader):\n    \"\"\"Iterates over the log lines and provide a reader for the values.\n\n    Args:\n      line_reader (iter): yields each line in the log file.\n\n    Yields:\n      dict[str, str]: column values keyed by column header.\n    \"\"\"\n    for line in line_reader:\n      if isinstance(line, py2to3.BYTES_TYPE):\n        try:\n          line = codecs.decode(line, self._encoding)\n        except UnicodeDecodeError as exception:\n          raise errors.UnableToParseFile(\n              'Unable decode line with error: {0!s}'.format(exception))\n\n      stripped_line = line.strip()\n      values = stripped_line.split(self.DELIMITER)\n      number_of_values = len(values)\n      number_of_columns = len(self.COLUMNS)\n\n      if number_of_values < self.MIN_COLUMNS:\n        raise errors.UnableToParseFile(\n            'Expected at least {0:d} values, found {1:d}'.format(\n                self.MIN_COLUMNS, number_of_values))\n\n      if number_of_values > number_of_columns:\n        raise errors.UnableToParseFile(\n            'Expected at most {0:d} values, found {1:d}'.format(\n                number_of_columns, number_of_values))\n\n      yield dict(zip(self.COLUMNS, values))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse a timestamp from a Trend Micro log row.", "response": "def _ParseTimestamp(self, parser_mediator, row):\n    \"\"\"Provides a timestamp for the given row.\n\n    If the Trend Micro log comes from a version that provides a POSIX timestamp,\n    use that directly; it provides the advantages of UTC and of second\n    precision. Otherwise fall back onto the local-timezone date and time.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      row (dict[str, str]): fields of a single row, as specified in COLUMNS.\n\n    Returns:\n      dfdatetime.interface.DateTimeValue: date and time value.\n    \"\"\"\n    timestamp = row.get('timestamp', None)\n    if timestamp is not None:\n      try:\n        timestamp = int(timestamp, 10)\n      except (ValueError, TypeError):\n        parser_mediator.ProduceExtractionWarning(\n            'Unable to parse timestamp value: {0!s}'.format(timestamp))\n\n      return dfdatetime_posix_time.PosixTime(timestamp=timestamp)\n\n    # The timestamp is not available; parse the local date and time instead.\n    try:\n      return self._ConvertToTimestamp(row['date'], row['time'])\n    except ValueError as exception:\n      parser_mediator.ProduceExtractionWarning((\n          'Unable to parse time string: \"{0:s} {1:s}\" with error: '\n          '{2!s}').format(repr(row['date']), repr(row['time']), exception))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _ConvertToTimestamp(self, date, time):\n    # Check that the strings have the correct length.\n    if len(date) != 8:\n      raise ValueError(\n          'Unsupported length of date string: {0!s}'.format(repr(date)))\n\n    if len(time) < 3 or len(time) > 4:\n      raise ValueError(\n          'Unsupported length of time string: {0!s}'.format(repr(time)))\n\n    # Extract the date.\n    try:\n      year = int(date[:4], 10)\n      month = int(date[4:6], 10)\n      day = int(date[6:8], 10)\n    except (TypeError, ValueError):\n      raise ValueError('Unable to parse date string: {0!s}'.format(repr(date)))\n\n    # Extract the time. Note that a single-digit hour value has no leading zero.\n    try:\n      hour = int(time[:-2], 10)\n      minutes = int(time[-2:], 10)\n    except (TypeError, ValueError):\n      raise ValueError('Unable to parse time string: {0!s}'.format(repr(date)))\n\n    time_elements_tuple = (year, month, day, hour, minutes, 0)\n    date_time = dfdatetime_time_elements.TimeElements(\n        time_elements_tuple=time_elements_tuple)\n    date_time.is_local_time = True\n    # TODO: add functionality to dfdatetime to control precision.\n    date_time._precision = dfdatetime_definitions.PRECISION_1_MINUTE  # pylint: disable=protected-access\n\n    return date_time", "response": "Converts a date and time string into a timestamp."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ParseRow(self, parser_mediator, row_offset, row):\n    timestamp = self._ParseTimestamp(parser_mediator, row)\n    if timestamp is None:\n      return\n\n    try:\n      action = int(row['action'], 10)\n    except (ValueError, TypeError):\n      action = None\n\n    try:\n      scan_type = int(row['scan_type'], 10)\n    except (ValueError, TypeError):\n      scan_type = None\n\n    event_data = TrendMicroAVEventData()\n    event_data.action = action\n    event_data.filename = row['filename']\n    event_data.offset = row_offset\n    event_data.path = row['path']\n    event_data.scan_type = scan_type\n    event_data.threat = row['threat']\n\n    event = time_events.DateTimeValuesEvent(\n        timestamp, definitions.TIME_DESCRIPTION_WRITTEN)\n    parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Parses a single row of the log file and produces events."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nverifies that a line of the file is in the expected format.", "response": "def VerifyRow(self, parser_mediator, row):\n    \"\"\"Verifies if a line of the file is in the expected format.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      row (dict[str, str]): fields of a single row, as specified in COLUMNS.\n\n    Returns:\n      bool: True if this is the correct parser, False otherwise.\n    \"\"\"\n    if len(row) < self.MIN_COLUMNS:\n      return False\n\n    # Check the date format!\n    # If it doesn't parse, then this isn't a Trend Micro AV log.\n    try:\n      timestamp = self._ConvertToTimestamp(row['date'], row['time'])\n    except (ValueError, TypeError):\n      return False\n\n    if timestamp is None:\n      return False\n\n    # Check that the action value is plausible.\n    try:\n      action = int(row['action'], 10)\n    except (ValueError, TypeError):\n      return False\n\n    if action not in formatter.SCAN_RESULTS:\n      return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing a single row of the log file and produces events.", "response": "def ParseRow(self, parser_mediator, row_offset, row):\n    \"\"\"Parses a line of the log file and produces events.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      row_offset (int): line number of the row.\n      row (dict[str, str]): fields of a single row, as specified in COLUMNS.\n    \"\"\"\n    timestamp = self._ParseTimestamp(parser_mediator, row)\n    if timestamp is None:\n      return\n\n    event_data = TrendMicroUrlEventData()\n    event_data.offset = row_offset\n\n    # Convert and store integer values.\n    for field in (\n        'credibility_rating', 'credibility_score', 'policy_identifier',\n        'threshold', 'block_mode'):\n      try:\n        value = int(row[field], 10)\n      except (ValueError, TypeError):\n        value = None\n      setattr(event_data, field, value)\n\n    # Store string values.\n    for field in ('url', 'group_name', 'group_code', 'application_name', 'ip'):\n      setattr(event_data, field, row[field])\n\n    event = time_events.DateTimeValuesEvent(\n        timestamp, definitions.TIME_DESCRIPTION_WRITTEN)\n    parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nverify that a line of the file is in the expected format.", "response": "def VerifyRow(self, parser_mediator, row):\n    \"\"\"Verifies if a line of the file is in the expected format.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      row (dict[str, str]): fields of a single row, as specified in COLUMNS.\n\n    Returns:\n      bool: True if this is the correct parser, False otherwise.\n    \"\"\"\n    if len(row) < self.MIN_COLUMNS:\n      return False\n\n    # Check the date format!\n    # If it doesn't parse, then this isn't a Trend Micro AV log.\n    try:\n      timestamp = self._ConvertToTimestamp(row['date'], row['time'])\n    except ValueError:\n      return False\n\n    if timestamp is None:\n      return False\n\n    try:\n      block_mode = int(row['block_mode'], 10)\n    except (ValueError, TypeError):\n      return False\n\n    if block_mode not in formatter.BLOCK_MODES:\n      return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef AddArguments(cls, argument_group):\n    argument_group.add_argument(\n        '--server', dest='server', type=str, action='store',\n        default=cls._DEFAULT_SERVER, metavar='HOSTNAME',\n        help='The hostname or server IP address of the server.')\n    argument_group.add_argument(\n        '--port', dest='port', type=int, action='store',\n        default=cls._DEFAULT_PORT, metavar='PORT',\n        help='The port number of the server.')", "response": "Adds command line arguments to an argument group."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses and validates the options.", "response": "def ParseOptions(cls, options, output_module):\n    \"\"\"Parses and validates options.\n\n    Args:\n      options (argparse.Namespace): parser options.\n      output_module (OutputModule): output module to configure.\n\n    Raises:\n      BadConfigObject: when the output module object does not have the\n          SetServerInformation method.\n    \"\"\"\n    if not hasattr(output_module, 'SetServerInformation'):\n      raise errors.BadConfigObject('Unable to set server information.')\n\n    server = cls._ParseStringOption(\n        options, 'server', default_value=cls._DEFAULT_SERVER)\n    port = cls._ParseNumericOption(\n        options, 'port', default_value=cls._DEFAULT_PORT)\n\n    output_module.SetServerInformation(server, port)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _QueryHashes(self, digests):\n    url_parameters = {'apikey': self._api_key, 'resource': ', '.join(digests)}\n\n    try:\n      json_response = self.MakeRequestAndDecodeJSON(\n          self._VIRUSTOTAL_API_REPORT_URL, 'GET', params=url_parameters)\n    except errors.ConnectionError as exception:\n      json_response = None\n      logger.error('Unable to query VirusTotal with error: {0!s}.'.format(\n          exception))\n\n    return json_response", "response": "Queries VirusTotal for a list of hashes."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef Analyze(self, hashes):\n    if not self._api_key:\n      raise RuntimeError('No API key specified for VirusTotal lookup.')\n\n    hash_analyses = []\n\n    json_response = self._QueryHashes(hashes) or []\n\n    # VirusTotal returns a dictionary when a single hash is queried\n    # and a list when multiple hashes are queried.\n    if isinstance(json_response, dict):\n      json_response = [json_response]\n\n    for result in json_response:\n      resource = result['resource']\n      hash_analysis = interface.HashAnalysis(resource, result)\n      hash_analyses.append(hash_analysis)\n\n    return hash_analyses", "response": "Looks up hashes in VirusTotal using the HTTP API."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef EnableFreeAPIKeyRateLimit(self):\n    self._analyzer.hashes_per_batch = 4\n    self._analyzer.wait_after_analysis = 60\n    self._analysis_queue_timeout = self._analyzer.wait_after_analysis + 1", "response": "Configures Rate limiting for queries to VirusTotal."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a list of strings that will be used in the event tag.", "response": "def GenerateLabels(self, hash_information):\n    \"\"\"Generates a list of strings that will be used in the event tag.\n\n    Args:\n      hash_information (dict[str, object]): the JSON decoded contents of the\n          result of a VirusTotal lookup, as produced by the VirusTotalAnalyzer.\n\n    Returns:\n      list[str]: strings describing the results from VirusTotal.\n    \"\"\"\n    response_code = hash_information['response_code']\n    if response_code == self._VIRUSTOTAL_NOT_PRESENT_RESPONSE_CODE:\n      return ['virustotal_not_present']\n\n    if response_code == self._VIRUSTOTAL_PRESENT_RESPONSE_CODE:\n      positives = hash_information['positives']\n      if positives > 0:\n        return ['virustotal_detections_{0:d}'.format(positives)]\n\n      return ['virsutotal_no_detections']\n\n    if response_code == self._VIRUSTOTAL_ANALYSIS_PENDING_RESPONSE_CODE:\n      return ['virustotal_analysis_pending']\n\n    logger.error(\n        'VirusTotal returned unknown response code {0!s}'.format(\n            response_code))\n    return ['virustotal_unknown_response_code_{0:d}'.format(response_code)]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndetermine the formatted message strings for an event object.", "response": "def GetMessages(self, formatter_mediator, event):\n    \"\"\"Determines the formatted message strings for an event object.\n\n    Args:\n      formatter_mediator (FormatterMediator): mediates the interactions\n          between formatters and other components, such as storage and Windows\n          EventLog resources.\n      event (EventObject): event.\n\n    Returns:\n      tuple(str, str): formatted message string and short message string.\n\n    Raises:\n      WrongFormatter: if the event object cannot be formatted by the formatter.\n    \"\"\"\n    if self.DATA_TYPE != event.data_type:\n      raise errors.WrongFormatter('Unsupported data type: {0:s}.'.format(\n          event.data_type))\n\n    event_values = event.CopyToDict()\n\n    priority_level = event_values.get('level', None)\n    if isinstance(priority_level, py2to3.INTEGER_TYPES):\n      event_values['level'] = '{0:s} ({1:d})'.format(\n          self._PRIORITY_LEVELS.get(priority_level, 'UNKNOWN'), priority_level)\n\n    # If no rights are assigned the value is 0xffffffff (-1).\n    read_uid = event_values.get('read_uid', None)\n    if read_uid == -1:\n      event_values['read_uid'] = 'ALL'\n\n    # If no rights are assigned the value is 0xffffffff (-1).\n    read_gid = event_values.get('read_gid', None)\n    if read_gid == -1:\n      event_values['read_gid'] = 'ALL'\n\n    # TODO: get the real name for the user of the group having the uid or gid.\n    return self._ConditionalFormatMessages(event_values)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing a generic windows timeline row.", "response": "def ParseGenericRow(\n      self, parser_mediator, query, row, **unused_kwargs):\n    \"\"\"Parses a generic windows timeline row.\n\n      Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      query (str): query that created the row.\n      row (sqlite3.Row): row.\n    \"\"\"\n    query_hash = hash(query)\n\n    event_data = WindowsTimelineGenericEventData()\n\n    # Payload is JSON serialized as binary data in a BLOB field, with the text\n    # encoded as UTF-8.\n    payload_json_bytes = bytes(self._GetRowValue(query_hash, row, 'Payload'))\n    payload_json_string = payload_json_bytes.decode('utf-8')\n    # AppId is JSON stored as unicode text.\n    appid_entries_string = self._GetRowValue(query_hash, row, 'AppId')\n\n    payload = json.loads(payload_json_string)\n    appid_entries = json.loads(appid_entries_string)\n\n    # Attempt to populate the package_identifier field by checking each of\n    # these fields in the AppId JSON.\n    package_id_locations = [\n        'packageId', 'x_exe_path', 'windows_win32', 'windows_universal',\n        'alternateId']\n    for location in package_id_locations:\n      for entry in appid_entries:\n        if entry['platform'] == location and entry['application'] != '':\n          event_data.package_identifier = entry['application']\n          break\n      if event_data.package_identifier is None:\n        # package_identifier has been populated and we're done.\n        break\n\n    if 'description' in payload:\n      event_data.description = payload['description']\n    else:\n      event_data.description = ''\n\n    if 'appDisplayName' in payload and payload['appDisplayName'] != '':\n      event_data.application_display_name = payload['appDisplayName']\n    elif 'displayText' in payload and payload['displayText'] != '':\n      # Fall back to displayText if appDisplayName isn't available\n      event_data.application_display_name = payload['displayText']\n\n    timestamp = self._GetRowValue(query_hash, row, 'StartTime')\n    date_time = dfdatetime_posix_time.PosixTime(timestamp=timestamp)\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_START)\n    parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ParseUserEngagedRow(\n      self, parser_mediator, query, row, **unused_kwargs):\n    \"\"\"Parses a timeline row that describes a user interacting with an app.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      query (str): query that created the row.\n      row (sqlite3.Row): row.\n    \"\"\"\n    query_hash = hash(query)\n\n    event_data = WindowsTimelineUserEngagedEventData()\n    event_data.package_identifier = self._GetRowValue(\n        query_hash, row, 'PackageName')\n\n    # Payload is JSON serialized as binary data in a BLOB field, with the text\n    # encoded as UTF-8.\n    payload_json_bytes = bytes(self._GetRowValue(query_hash, row, 'Payload'))\n    payload_json_string = payload_json_bytes.decode('utf-8')\n    payload = json.loads(payload_json_string)\n\n    if 'reportingApp' in payload:\n      event_data.reporting_app = payload['reportingApp']\n    if 'activeDurationSeconds' in payload:\n      event_data.active_duration_seconds = int(payload['activeDurationSeconds'])\n\n    timestamp = self._GetRowValue(query_hash, row, 'StartTime')\n    date_time = dfdatetime_posix_time.PosixTime(timestamp=timestamp)\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_START)\n    parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Parses a user - engaged timeline row."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ParseCallsRow(self, parser_mediator, query, row, **unused_kwargs):\n    query_hash = hash(query)\n\n    call_type = self._GetRowValue(query_hash, row, 'type')\n    call_type = self.CALL_TYPE.get(call_type, 'UNKNOWN')\n    duration = self._GetRowValue(query_hash, row, 'duration')\n    timestamp = self._GetRowValue(query_hash, row, 'date')\n\n    event_data = AndroidCallEventData()\n    event_data.call_type = call_type\n    event_data.duration = self._GetRowValue(query_hash, row, 'duration')\n    event_data.name = self._GetRowValue(query_hash, row, 'name')\n    event_data.number = self._GetRowValue(query_hash, row, 'number')\n    event_data.offset = self._GetRowValue(query_hash, row, 'id')\n    event_data.query = query\n\n    date_time = dfdatetime_java_time.JavaTime(timestamp=timestamp)\n    event = time_events.DateTimeValuesEvent(date_time, 'Call Started')\n    parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    if duration:\n      if isinstance(duration, py2to3.STRING_TYPES):\n        try:\n          duration = int(duration, 10)\n        except ValueError:\n          duration = 0\n\n      # The duration is in seconds and the date value in milliseconds.\n      timestamp += duration * 1000\n\n      date_time = dfdatetime_java_time.JavaTime(timestamp=timestamp)\n      event = time_events.DateTimeValuesEvent(date_time, 'Call Ended')\n      parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Parses a Call record row."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef Process(self, parser_mediator, plist_name, top_level, **kwargs):\n    super(MacUserPlugin, self).Process(\n        parser_mediator, plist_name=self.PLIST_PATH, top_level=top_level)", "response": "Checks if the plist file name is a valid MacOS system account plist file name."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nextract relevant user timestamp entries.", "response": "def GetEntries(self, parser_mediator, match=None, **unused_kwargs):\n    \"\"\"Extracts relevant user timestamp entries.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      match (Optional[dict[str: object]]): keys extracted from PLIST_KEYS.\n    \"\"\"\n    if 'name' not in match or 'uid' not in match:\n      return\n\n    account = match['name'][0]\n    uid = match['uid'][0]\n\n    for policy in match.get('passwordpolicyoptions', []):\n      try:\n        xml_policy = ElementTree.fromstring(policy)\n      except (ElementTree.ParseError, LookupError) as exception:\n        logger.error((\n            'Unable to parse XML structure for an user policy, account: '\n            '{0:s} and uid: {1!s}, with error: {2!s}').format(\n                account, uid, exception))\n        continue\n\n      for dict_elements in xml_policy.iterfind('dict'):\n        key_values = [value.text for value in iter(dict_elements)]\n        # Taking a list and converting it to a dict, using every other item\n        # as the key and the other one as the value.\n        policy_dict = dict(zip(key_values[0::2], key_values[1::2]))\n\n      time_string = policy_dict.get('passwordLastSetTime', None)\n      if time_string and time_string != '2001-01-01T00:00:00Z':\n        try:\n          date_time = dfdatetime_time_elements.TimeElements()\n          date_time.CopyFromStringISO8601(time_string)\n        except ValueError:\n          date_time = None\n          parser_mediator.ProduceExtractionWarning(\n              'unable to parse password last set time string: {0:s}'.format(\n                  time_string))\n\n        shadow_hash_data = match.get('ShadowHashData', None)\n        if date_time and isinstance(shadow_hash_data, (list, tuple)):\n          # Extract the hash password information.\n          # It is store in the attribute ShadowHasData which is\n          # a binary plist data; However biplist only extracts one\n          # level of binary plist, then it returns this information\n          # as a string.\n\n          # TODO: change this into a DataRange instead. For this we\n          # need the file offset and size of the ShadowHashData value data.\n          shadow_hash_data = shadow_hash_data[0]\n\n          resolver_context = context.Context()\n          fake_file = fake_file_io.FakeFile(\n              resolver_context, shadow_hash_data)\n          shadow_hash_data_path_spec = fake_path_spec.FakePathSpec(\n              location='ShadowHashData')\n          fake_file.open(path_spec=shadow_hash_data_path_spec)\n\n          try:\n            plist_file = biplist.readPlist(fake_file)\n          except biplist.InvalidPlistException:\n            plist_file = {}\n          salted_hash = plist_file.get('SALTED-SHA512-PBKDF2', None)\n          if salted_hash:\n            salt_hex_bytes = codecs.encode(salted_hash['salt'], 'hex')\n            salt_string = codecs.decode(salt_hex_bytes, 'ascii')\n            entropy_hex_bytes = codecs.encode(salted_hash['entropy'], 'hex')\n            entropy_string = codecs.decode(entropy_hex_bytes, 'ascii')\n            password_hash = '$ml${0:d}${1:s}${2:s}'.format(\n                salted_hash['iterations'], salt_string, entropy_string)\n          else:\n            password_hash = 'N/A'\n\n          event_data = plist_event.PlistTimeEventData()\n          event_data.desc = (\n              'Last time {0:s} ({1!s}) changed the password: {2!s}').format(\n                  account, uid, password_hash)\n          event_data.key = 'passwordLastSetTime'\n          event_data.root = self._ROOT\n\n          event = time_events.DateTimeValuesEvent(\n              date_time, definitions.TIME_DESCRIPTION_WRITTEN)\n          parser_mediator.ProduceEventWithEventData(event, event_data)\n\n      time_string = policy_dict.get('lastLoginTimestamp', None)\n      if time_string and time_string != '2001-01-01T00:00:00Z':\n        try:\n          date_time = dfdatetime_time_elements.TimeElements()\n          date_time.CopyFromStringISO8601(time_string)\n        except ValueError:\n          date_time = None\n          parser_mediator.ProduceExtractionWarning(\n              'unable to parse last login time string: {0:s}'.format(\n                  time_string))\n\n        if date_time:\n          event_data = plist_event.PlistTimeEventData()\n          event_data.desc = 'Last login from {0:s} ({1!s})'.format(\n              account, uid)\n          event_data.key = 'lastLoginTimestamp'\n          event_data.root = self._ROOT\n\n          event = time_events.DateTimeValuesEvent(\n              date_time, definitions.TIME_DESCRIPTION_WRITTEN)\n          parser_mediator.ProduceEventWithEventData(event, event_data)\n\n      time_string = policy_dict.get('failedLoginTimestamp', None)\n      if time_string and time_string != '2001-01-01T00:00:00Z':\n        try:\n          date_time = dfdatetime_time_elements.TimeElements()\n          date_time.CopyFromStringISO8601(time_string)\n        except ValueError:\n          date_time = None\n          parser_mediator.ProduceExtractionWarning(\n              'unable to parse failed login time string: {0:s}'.format(\n                  time_string))\n\n        if date_time:\n          event_data = plist_event.PlistTimeEventData()\n          event_data.desc = (\n              'Last failed login from {0:s} ({1!s}) ({2!s} times)').format(\n                  account, uid, policy_dict.get('failedLoginCount', 0))\n          event_data.key = 'failedLoginTimestamp'\n          event_data.root = self._ROOT\n\n          event = time_events.DateTimeValuesEvent(\n              date_time, definitions.TIME_DESCRIPTION_WRITTEN)\n          parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef GetMessages(self, formatter_mediator, event):\n    if self.DATA_TYPE != event.data_type:\n      raise errors.WrongFormatter('Unsupported data type: {0:s}.'.format(\n          event.data_type))\n\n    event_values = event.CopyToDict()\n\n    regvalue = event_values.get('regvalue', {})\n    string_parts = []\n    for key, value in sorted(regvalue.items()):\n      string_parts.append('{0:s}: {1!s}'.format(key, value))\n    event_values['text'] = ' '.join(string_parts)\n\n    return self._ConditionalFormatMessages(event_values)", "response": "Determines the formatted message strings for an event object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef Match(self, registry_key):\n    key_path = registry_key.path.upper()\n    # Prevent this filter matching non-string MRUList values.\n    for ignore_key_path_suffix in self._IGNORE_KEY_PATH_SUFFIXES:\n      if key_path.endswith(ignore_key_path_suffix):\n        return False\n\n    return super(MRUListStringRegistryKeyFilter, self).Match(registry_key)", "response": "Determines if a Windows Registry key matches the filter."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing the MRUList value in a given Windows Registry key.", "response": "def _ParseMRUListValue(self, registry_key):\n    \"\"\"Parses the MRUList value in a given Registry key.\n\n    Args:\n      registry_key (dfwinreg.WinRegistryKey): Windows Registry key that contains\n           the MRUList value.\n\n    Returns:\n      mrulist_entries: MRUList entries or None if not available.\n    \"\"\"\n    mrulist_value = registry_key.GetValueByName('MRUList')\n\n    # The key exists but does not contain a value named \"MRUList\".\n    if not mrulist_value:\n      return None\n\n    mrulist_entries_map = self._GetDataTypeMap('mrulist_entries')\n\n    context = dtfabric_data_maps.DataTypeMapContext(values={\n        'data_size': len(mrulist_value.data)})\n\n    return self._ReadStructureFromByteStream(\n        mrulist_value.data, 0, mrulist_entries_map, context=context)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _ParseMRUListKey(self, parser_mediator, registry_key, codepage='cp1252'):\n    try:\n      mrulist = self._ParseMRUListValue(registry_key)\n    except (ValueError, errors.ParseError) as exception:\n      parser_mediator.ProduceExtractionWarning(\n          'unable to parse MRUList value with error: {0!s}'.format(exception))\n      return\n\n    if not mrulist:\n      return\n\n    values_dict = {}\n    found_terminator = False\n    for entry_index, entry_letter in enumerate(mrulist):\n      # The MRU list is terminated with '\\0' (0x0000).\n      if entry_letter == 0:\n        break\n\n      if found_terminator:\n        parser_mediator.ProduceExtractionWarning((\n            'found additional MRUList entries after terminator in key: '\n            '{0:s}.').format(registry_key.path))\n\n        # Only create one parser error per terminator.\n        found_terminator = False\n\n      entry_letter = chr(entry_letter)\n\n      value_string = self._ParseMRUListEntryValue(\n          parser_mediator, registry_key, entry_index, entry_letter,\n          codepage=codepage)\n\n      value_text = 'Index: {0:d} [MRU Value {1:s}]'.format(\n          entry_index + 1, entry_letter)\n\n      values_dict[value_text] = value_string\n\n    event_data = windows_events.WindowsRegistryEventData()\n    event_data.key_path = registry_key.path\n    event_data.offset = registry_key.offset\n    event_data.regvalue = values_dict\n    event_data.source_append = self._SOURCE_APPEND\n\n    event = time_events.DateTimeValuesEvent(\n        registry_key.last_written_time, definitions.TIME_DESCRIPTION_WRITTEN)\n    parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Parses a MRUList key and returns an event object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse the MRUList entry value.", "response": "def _ParseMRUListEntryValue(\n      self, parser_mediator, registry_key, entry_index, entry_letter, **kwargs):\n    \"\"\"Parses the MRUList entry value.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      registry_key (dfwinreg.WinRegistryKey): Windows Registry key that contains\n           the MRUList value.\n      entry_index (int): MRUList entry index.\n      entry_letter (str): character value representing the entry.\n\n    Returns:\n      str: MRUList entry value.\n    \"\"\"\n    value_string = ''\n\n    value = registry_key.GetValueByName('{0:s}'.format(entry_letter))\n    if value is None:\n      parser_mediator.ProduceExtractionWarning(\n          'missing MRUList value: {0:s} in key: {1:s}.'.format(\n              entry_letter, registry_key.path))\n\n    elif value.DataIsString():\n      value_string = value.GetDataAsObject()\n\n    elif value.DataIsBinaryData():\n      logger.debug((\n          '[{0:s}] Non-string MRUList entry value: {1:s} parsed as string '\n          'in key: {2:s}.').format(self.NAME, entry_letter, registry_key.path))\n\n      utf16le_string_map = self._GetDataTypeMap('utf16le_string')\n\n      try:\n        value_string = self._ReadStructureFromByteStream(\n            value.data, 0, utf16le_string_map)\n      except (ValueError, errors.ParseError) as exception:\n        parser_mediator.ProduceExtractionWarning((\n            'unable to parse MRUList entry value: {0:s} with error: '\n            '{1!s}').format(entry_letter, exception))\n\n      value_string = value_string.rstrip('\\x00')\n\n    return value_string"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ExtractEvents(\n      self, parser_mediator, registry_key, codepage='cp1252', **kwargs):\n    \"\"\"Extracts events from a Windows Registry key.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      registry_key (dfwinreg.WinRegistryKey): Windows Registry key.\n      codepage (Optional[str]): extended ASCII string codepage.\n    \"\"\"\n    self._ParseMRUListKey(parser_mediator, registry_key, codepage=codepage)", "response": "Extracts events from a Windows Registry key."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses the MRUList entry value.", "response": "def _ParseMRUListEntryValue(\n      self, parser_mediator, registry_key, entry_index, entry_letter,\n      codepage='cp1252', **kwargs):\n    \"\"\"Parses the MRUList entry value.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      registry_key (dfwinreg.WinRegistryKey): Windows Registry key that contains\n           the MRUList value.\n      entry_index (int): MRUList entry index.\n      entry_letter (str): character value representing the entry.\n      codepage (Optional[str]): extended ASCII string codepage.\n\n    Returns:\n      str: MRUList entry value.\n    \"\"\"\n    value_string = ''\n\n    value = registry_key.GetValueByName('{0:s}'.format(entry_letter))\n    if value is None:\n      parser_mediator.ProduceExtractionWarning(\n          'missing MRUList value: {0:s} in key: {1:s}.'.format(\n              entry_letter, registry_key.path))\n\n    elif not value.DataIsBinaryData():\n      parser_mediator.ProduceExtractionWarning(\n          'Non-binary MRUList entry value: {1:s} in key: {2:s}.'.format(\n              entry_letter, registry_key.path))\n\n    elif value.data:\n      shell_items_parser = shell_items.ShellItemsParser(registry_key.path)\n      shell_items_parser.ParseByteStream(\n          parser_mediator, value.data, codepage=codepage)\n\n      value_string = 'Shell item path: {0:s}'.format(\n          shell_items_parser.CopyToPath())\n\n    return value_string"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert all strings in a DSV row dict to Unicode.", "response": "def _ConvertRowToUnicode(self, parser_mediator, row):\n    \"\"\"Converts all strings in a DSV row dict to Unicode.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      row (dict[str, bytes]): a row from a DSV file, where the dictionary\n          key contains the column name and the value a binary string.\n\n    Returns:\n      dict[str, str]: a row from the DSV file, where the dictionary key\n          contains the column name and the value a Unicode string.\n    \"\"\"\n    for key, value in iter(row.items()):\n      if isinstance(value, py2to3.UNICODE_TYPE):\n        continue\n\n      try:\n        row[key] = value.decode(self._encoding)\n      except UnicodeDecodeError:\n        replaced_value = value.decode(self._encoding, errors='replace')\n        parser_mediator.ProduceExtractionWarning(\n            'error decoding DSV value: {0:s} as {1:s}, characters have been '\n            'replaced in {2:s}'.format(key, self._encoding, replaced_value))\n        row[key] = replaced_value\n\n    return row"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _CreateDictReader(self, line_reader):\n    delimiter = self.DELIMITER\n    quotechar = self.QUOTE_CHAR\n    magic_test_string = self._MAGIC_TEST_STRING\n    # Python 3 csv module requires arguments to constructor to be of type str.\n    if py2to3.PY_3:\n      delimiter = delimiter.decode(self._encoding)\n      quotechar = quotechar.decode(self._encoding)\n      magic_test_string = magic_test_string.decode(self._encoding)\n\n    return csv.DictReader(\n        line_reader, delimiter=delimiter, fieldnames=self.COLUMNS,\n        quotechar=quotechar, restkey=magic_test_string,\n        restval=magic_test_string)", "response": "Returns a reader that processes each row and yields dictionaries."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a line reader for the given file - like object.", "response": "def _CreateLineReader(self, file_object):\n    \"\"\"Creates an object that reads lines from a text file.\n\n    The line reader is advanced to the beginning of the DSV content, skipping\n    any header lines.\n\n    Args:\n      file_object (dfvfs.FileIO): file-like object.\n\n    Returns:\n      TextFile|BinaryLineReader: an object that implements an iterator\n          over lines in a text file.\n\n    Raises:\n      UnicodeDecodeError: if the file cannot be read with the specified\n          encoding.\n    \"\"\"\n    # The Python 2 csv module reads bytes and the Python 3 csv module Unicode\n    # reads strings.\n    if py2to3.PY_3:\n      line_reader = text_file.TextFile(\n          file_object, encoding=self._encoding, end_of_line=self._end_of_line)\n\n      # pylint: disable=protected-access\n      maximum_read_buffer_size = line_reader._MAXIMUM_READ_BUFFER_SIZE\n\n    else:\n      line_reader = line_reader_file.BinaryLineReader(\n          file_object, end_of_line=self._end_of_line)\n\n      maximum_read_buffer_size = line_reader.MAXIMUM_READ_BUFFER_SIZE\n\n    # Line length is one less than the maximum read buffer size so that we\n    # tell if there's a line that doesn't end at the end before the end of\n    # the file.\n    if self._maximum_line_length > maximum_read_buffer_size:\n      self._maximum_line_length = maximum_read_buffer_size - 1\n\n    # If we specifically define a number of lines we should skip, do that here.\n    for _ in range(0, self.NUMBER_OF_HEADER_LINES):\n      line_reader.readline(self._maximum_line_length)\n    return line_reader"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _HasExpectedLineLength(self, file_object):\n    original_file_position = file_object.tell()\n    line_reader = self._CreateLineReader(file_object)\n    for _ in range(0, 20):\n      # Attempt to read a line that is longer than any line that should be in\n      # the file.\n      sample_line = line_reader.readline(self._maximum_line_length + 1)\n      if len(sample_line) > self._maximum_line_length:\n        file_object.seek(original_file_position)\n        return False\n    file_object.seek(original_file_position)\n    return True", "response": "Determines if a file has lines of the expected length."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses a DSV text file - like object.", "response": "def ParseFileObject(self, parser_mediator, file_object):\n    \"\"\"Parses a DSV text file-like object.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      file_object (dfvfs.FileIO): file-like object.\n\n    Raises:\n      UnableToParseFile: when the file cannot be parsed.\n    \"\"\"\n    # TODO: Replace this with detection of the file encoding via byte-order\n    # marks. Also see: https://github.com/log2timeline/plaso/issues/1971\n    if not self._encoding:\n      self._encoding = parser_mediator.codepage\n\n    try:\n      if not self._HasExpectedLineLength(file_object):\n        display_name = parser_mediator.GetDisplayName()\n        raise errors.UnableToParseFile((\n            '[{0:s}] Unable to parse DSV file: {1:s} with error: '\n            'unexpected line length.').format(self.NAME, display_name))\n    except UnicodeDecodeError as exception:\n      display_name = parser_mediator.GetDisplayName()\n      raise errors.UnableToParseFile(\n          '[{0:s}] Unable to parse DSV file: {1:s} with error: {2!s}.'.format(\n              self.NAME, display_name, exception))\n\n    try:\n      line_reader = self._CreateLineReader(file_object)\n      reader = self._CreateDictReader(line_reader)\n      row_offset = line_reader.tell()\n      row = next(reader)\n    except (StopIteration, csv.Error, UnicodeDecodeError) as exception:\n      display_name = parser_mediator.GetDisplayName()\n      raise errors.UnableToParseFile(\n          '[{0:s}] Unable to parse DSV file: {1:s} with error: {2!s}.'.format(\n              self.NAME, display_name, exception))\n\n    number_of_columns = len(self.COLUMNS)\n    number_of_records = len(row)\n\n    if number_of_records != number_of_columns:\n      display_name = parser_mediator.GetDisplayName()\n      raise errors.UnableToParseFile((\n          '[{0:s}] Unable to parse DSV file: {1:s}. Wrong number of '\n          'records (expected: {2:d}, got: {3:d})').format(\n              self.NAME, display_name, number_of_columns,\n              number_of_records))\n\n    for key, value in row.items():\n      if self._MAGIC_TEST_STRING in (key, value):\n        display_name = parser_mediator.GetDisplayName()\n        raise errors.UnableToParseFile((\n            '[{0:s}] Unable to parse DSV file: {1:s}. Signature '\n            'mismatch.').format(self.NAME, display_name))\n\n    row = self._ConvertRowToUnicode(parser_mediator, row)\n\n    if not self.VerifyRow(parser_mediator, row):\n      display_name = parser_mediator.GetDisplayName()\n      raise errors.UnableToParseFile((\n          '[{0:s}] Unable to parse DSV file: {1:s}. Verification '\n          'failed.').format(self.NAME, display_name))\n\n    self.ParseRow(parser_mediator, row_offset, row)\n    row_offset = line_reader.tell()\n\n    for row in reader:\n      if parser_mediator.abort:\n        break\n      row = self._ConvertRowToUnicode(parser_mediator, row)\n      self.ParseRow(parser_mediator, row_offset, row)\n      row_offset = line_reader.tell()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ParseFileObject(self, parser_mediator, file_object):\n    regf_file = pyregf.file() # pylint: disable=no-member\n    try:\n      regf_file.open_file_object(file_object)\n    except IOError:\n      # The error is currently ignored -> see TODO above related to the\n      # fixing of handling multiple parsers for the same file format.\n      return\n\n    root_key = regf_file.get_root_key()\n    if root_key is None:\n      regf_file.close()\n      return\n\n    root_file_key = root_key.get_sub_key_by_path(self._AMCACHE_ROOT_FILE_KEY)\n    if root_file_key is None:\n      regf_file.close()\n      return\n\n    for volume_key in root_file_key.sub_keys:\n      for am_entry in volume_key.sub_keys:\n        self._ProcessAMCacheFileKey(am_entry, parser_mediator)\n\n    root_program_key = root_key.get_sub_key_by_path(\n        self._AMCACHE_ROOT_PROGRAM_KEY)\n    if root_program_key is None:\n      regf_file.close()\n      return\n\n    for am_entry in root_program_key.sub_keys:\n      self._ProcessAMCacheProgramKey(am_entry, parser_mediator)\n\n    regf_file.close()", "response": "Parses an Amcache. hve file - like object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _ProcessAMCacheProgramKey(self, am_entry, parser_mediator):\n    amcache_datetime = am_entry.get_value_by_name(\n        self._AMCACHE_P_INSTALLDATE).get_data_as_integer()\n    event_data = AmcacheProgramEventData()\n\n    name = am_entry.get_value_by_name(self._AMCACHE_P_NAME)\n    if name:\n      event_data.name = name.get_data_as_string()\n\n    version = am_entry.get_value_by_name(self._AMCACHE_P_VERSION)\n    if version:\n      event_data.version = version.get_data_as_string()\n\n    publisher = am_entry.get_value_by_name(self._AMCACHE_P_PUBLISHER)\n    if publisher:\n      event_data.publisher = publisher.get_data_as_string()\n\n    languagecode = am_entry.get_value_by_name(self._AMCACHE_P_LANGUAGECODE)\n    if languagecode:\n      event_data.languagecode = languagecode.get_data_as_string()\n\n    entrytype = am_entry.get_value_by_name(self._AMCACHE_P_ENTRYTYPE)\n    if entrytype:\n      event_data.entrytype = entrytype.get_data_as_string()\n\n    uninstallkey = am_entry.get_value_by_name(self._AMCACHE_P_UNINSTALLKEY)\n    if uninstallkey:\n      uninstallkey = uninstallkey.get_data()\n      uninstallkey = uninstallkey.decode('utf-16-LE')\n      event_data.uninstallkey = uninstallkey\n\n    filepaths = am_entry.get_value_by_name(self._AMCACHE_P_FILEPATHS)\n    if filepaths:\n      filepaths = filepaths.get_data()\n      filepaths = filepaths.decode('utf-16-LE')\n      event_data.filepaths = filepaths\n\n    productcode = am_entry.get_value_by_name(self._AMCACHE_P_PRODUCTCODE)\n    if productcode:\n      event_data.productcode = productcode.get_data_as_string()\n\n    packagecode = am_entry.get_value_by_name(self._AMCACHE_P_PACKAGECODE)\n    if packagecode:\n      event_data.packagecode = packagecode.get_data_as_string()\n\n    msiproductcode = am_entry.get_value_by_name(self._AMCACHE_P_MSIPRODUCTCODE)\n    if msiproductcode:\n      msiproductcode = msiproductcode.get_data()\n      msiproductcode = msiproductcode.decode('utf-16-LE')\n      event_data.msiproductcode = msiproductcode\n\n    msipackagecode = am_entry.get_value_by_name(self._AMCACHE_P_MSIPACKAGECODE)\n    if msipackagecode:\n      msipackagecode = msipackagecode.get_data()\n      msipackagecode = msipackagecode.decode('utf-16-LE')\n      event_data.msipackagecode = msipackagecode\n\n    files = am_entry.get_value_by_name(self._AMCACHE_P_FILES)\n    if files:\n      files = files.get_data()\n      files = files.decode('utf-16-LE')\n      event_data.files = files\n\n    event = time_events.DateTimeValuesEvent(\n        posix_time.PosixTime(amcache_datetime),\n        definitions.TIME_DESCRIPTION_INSTALLATION)\n    parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Processes an Amcache Programs key."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _ProcessAMCacheFileKey(self, am_entry, parser_mediator):\n    amcache_datetime = am_entry.get_value_by_name(\n        self._AMCACHE_DATETIME).get_data_as_integer()\n    event_data = AmcacheEventData()\n\n    event_data.full_path = am_entry.get_value_by_name(\n        self._AMCACHE_FULL_PATH).get_data_as_string()\n    # Strip off the 4 leading zero's from the sha1 hash.\n    event_data.sha1 = am_entry.get_value_by_name(\n        self._AMCACHE_SHA1).get_data_as_string()[4:]\n\n    productname = am_entry.get_value_by_name(self._AMCACHE_PRODUCTNAME)\n    if productname:\n      event_data.productname = productname.get_data_as_string()\n\n    companyname = am_entry.get_value_by_name(self._AMCACHE_COMPANYNAME)\n    if companyname:\n      event_data.companyname = companyname.get_data_as_string()\n\n    fileversion = am_entry.get_value_by_name(self._AMCACHE_FILEVERSION)\n    if fileversion:\n      event_data.fileversion = fileversion.get_data_as_string()\n\n    languagecode = am_entry.get_value_by_name(self._AMCACHE_LANGUAGECODE)\n    if languagecode:\n      event_data.languagecode = languagecode.get_data_as_integer()\n\n    filesize = am_entry.get_value_by_name(self._AMCACHE_FILESIZE)\n    if filesize:\n      event_data.filesize = filesize.get_data_as_integer()\n\n    filedescription = am_entry.get_value_by_name(self._AMCACHE_FILEDESCRIPTION)\n    if filedescription:\n      event_data.filedescription = filedescription.get_data_as_string()\n\n    linkerts = am_entry.get_value_by_name(self._AMCACHE_LINKERTS)\n    if linkerts:\n      event_data.linkerts = linkerts.get_data_as_integer()\n\n    lastmodifiedts = am_entry.get_value_by_name(self._AMCACHE_LASTMODIFIEDTS)\n    if lastmodifiedts:\n      event_data.lastmodifiedts = lastmodifiedts.get_data_as_integer()\n\n    createdts = am_entry.get_value_by_name(self._AMCACHE_CREATEDTS)\n    if createdts:\n      event_data.createdts = createdts.get_data_as_integer()\n\n    programid = am_entry.get_value_by_name(self._AMCACHE_PROGRAMID)\n    if programid:\n      event_data.programid = programid.get_data_as_string()\n\n    event = time_events.DateTimeValuesEvent(\n        filetime.Filetime(amcache_datetime),\n        definitions.TIME_DESCRIPTION_MODIFICATION)\n    parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    if event_data.createdts:\n      event = time_events.DateTimeValuesEvent(\n          filetime.Filetime(event_data.createdts),\n          definitions.TIME_DESCRIPTION_CREATION)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    if event_data.lastmodifiedts:\n      event = time_events.DateTimeValuesEvent(\n          filetime.Filetime(event_data.lastmodifiedts),\n          definitions.TIME_DESCRIPTION_MODIFICATION)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    if event_data.linkerts:\n      event = time_events.DateTimeValuesEvent(\n          posix_time.PosixTime(event_data.linkerts),\n          definitions.TIME_DESCRIPTION_CHANGE)\n      parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Parses an Amcache Root / File key for events."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nestablish a connection to an nsrlsvr instance.", "response": "def _GetSocket(self):\n    \"\"\"Establishes a connection to an nsrlsvr instance.\n\n    Returns:\n      socket._socketobject: socket connected to an nsrlsvr instance or None if\n          a connection cannot be established.\n    \"\"\"\n    try:\n      return socket.create_connection(\n          (self._host, self._port), self._SOCKET_TIMEOUT)\n\n    except socket.error as exception:\n      logger.error(\n          'Unable to connect to nsrlsvr with error: {0!s}.'.format(exception))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _QueryHash(self, nsrl_socket, digest):\n    try:\n      query = 'QUERY {0:s}\\n'.format(digest).encode('ascii')\n    except UnicodeDecodeError:\n      logger.error('Unable to encode digest: {0!s} to ASCII.'.format(digest))\n      return False\n\n    response = None\n\n    try:\n      nsrl_socket.sendall(query)\n      response = nsrl_socket.recv(self._RECEIVE_BUFFER_SIZE)\n\n    except socket.error as exception:\n      logger.error('Unable to query nsrlsvr with error: {0!s}.'.format(\n          exception))\n\n    if not response:\n      return False\n\n    # Strip end-of-line characters since they can differ per platform on which\n    # nsrlsvr is running.\n    response = response.strip()\n    # nsrlsvr returns \"OK 1\" if the has was found or \"OK 0\" if not.\n    return response == b'OK 1'", "response": "Queries nsrlsvr for a specific hash."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef Analyze(self, hashes):\n    logger.debug(\n        'Opening connection to {0:s}:{1:d}'.format(self._host, self._port))\n\n    nsrl_socket = self._GetSocket()\n    if not nsrl_socket:\n      self.SignalAbort()\n      return []\n\n    hash_analyses = []\n    for digest in hashes:\n      response = self._QueryHash(nsrl_socket, digest)\n      if response is None:\n        continue\n\n      hash_analysis = interface.HashAnalysis(digest, response)\n      hash_analyses.append(hash_analysis)\n\n    nsrl_socket.close()\n\n    logger.debug(\n        'Closed connection to {0:s}:{1:d}'.format(self._host, self._port))\n\n    return hash_analyses", "response": "Looks up hashes in nsrlsvr."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef GetEntries(self, parser_mediator, match=None, **unused_kwargs):\n    backup_alias_map = self._GetDataTypeMap('timemachine_backup_alias')\n\n    destinations = match.get('Destinations', [])\n    for destination in destinations:\n      backup_alias_data = destination.get('BackupAlias', b'')\n      try:\n        backup_alias = self._ReadStructureFromByteStream(\n            backup_alias_data, 0, backup_alias_map)\n        alias = backup_alias.string\n\n      except (ValueError, errors.ParseError) as exception:\n        parser_mediator.ProduceExtractionWarning(\n            'unable to parse backup alias value with error: {0!s}'.format(\n                exception))\n        alias = 'Unknown alias'\n\n      destination_identifier = (\n          destination.get('DestinationID', None) or 'Unknown device')\n\n      event_data = plist_event.PlistTimeEventData()\n      event_data.desc = 'TimeMachine Backup in {0:s} ({1:s})'.format(\n          alias, destination_identifier)\n      event_data.key = 'item/SnapshotDates'\n      event_data.root = '/Destinations'\n\n      snapshot_dates = destination.get('SnapshotDates', [])\n      for datetime_value in snapshot_dates:\n        event = time_events.PythonDatetimeEvent(\n            datetime_value, definitions.TIME_DESCRIPTION_WRITTEN)\n        parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Extracts relevant TimeMachine entries."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nretrieve the Id value from a Task Cache Tree key.", "response": "def _GetIdValue(self, registry_key):\n    \"\"\"Retrieves the Id value from Task Cache Tree key.\n\n    Args:\n      registry_key (dfwinreg.WinRegistryKey): Windows Registry key.\n\n    Yields:\n      tuple: containing:\n\n        dfwinreg.WinRegistryKey: Windows Registry key.\n        dfwinreg.WinRegistryValue: Windows Registry value.\n    \"\"\"\n    id_value = registry_key.GetValueByName('Id')\n    if id_value:\n      yield registry_key, id_value\n\n    for sub_key in registry_key.GetSubkeys():\n      for value_key, id_value in self._GetIdValue(sub_key):\n        yield value_key, id_value"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ExtractEvents(self, parser_mediator, registry_key, **kwargs):\n    dynamic_info_size_error_reported = False\n\n    tasks_key = registry_key.GetSubkeyByName('Tasks')\n    tree_key = registry_key.GetSubkeyByName('Tree')\n\n    if not tasks_key or not tree_key:\n      parser_mediator.ProduceExtractionWarning(\n          'Task Cache is missing a Tasks or Tree sub key.')\n      return\n\n    task_guids = {}\n    for sub_key in tree_key.GetSubkeys():\n      for value_key, id_value in self._GetIdValue(sub_key):\n        # TODO: improve this check to a regex.\n        # The GUID is in the form {%GUID%} and stored an UTF-16 little-endian\n        # string and should be 78 bytes in size.\n        id_value_data_size = len(id_value.data)\n        if id_value_data_size != 78:\n          parser_mediator.ProduceExtractionWarning(\n              'unsupported Id value data size: {0:d}.'.format(\n                  id_value_data_size))\n          continue\n\n        guid_string = id_value.GetDataAsObject()\n        task_guids[guid_string] = value_key.name\n\n    dynamic_info_map = self._GetDataTypeMap('dynamic_info_record')\n    dynamic_info2_map = self._GetDataTypeMap('dynamic_info2_record')\n\n    dynamic_info_size = dynamic_info_map.GetByteSize()\n    dynamic_info2_size = dynamic_info2_map.GetByteSize()\n\n    for sub_key in tasks_key.GetSubkeys():\n      dynamic_info_value = sub_key.GetValueByName('DynamicInfo')\n      if not dynamic_info_value:\n        continue\n\n      dynamic_info_record_map = None\n      dynamic_info_value_data_size = len(dynamic_info_value.data)\n      if dynamic_info_value_data_size == dynamic_info_size:\n        dynamic_info_record_map = dynamic_info_map\n      elif dynamic_info_value_data_size == dynamic_info2_size:\n        dynamic_info_record_map = dynamic_info2_map\n      else:\n        if not dynamic_info_size_error_reported:\n          parser_mediator.ProduceExtractionWarning(\n              'unsupported DynamicInfo value data size: {0:d}.'.format(\n                  dynamic_info_value_data_size))\n          dynamic_info_size_error_reported = True\n        continue\n\n      try:\n        dynamic_info_record = self._ReadStructureFromByteStream(\n            dynamic_info_value.data, 0, dynamic_info_record_map)\n      except (ValueError, errors.ParseError) as exception:\n        parser_mediator.ProduceExtractionWarning(\n            'unable to parse DynamicInfo record with error: {0!s}.'.format(\n                exception))\n\n      name = task_guids.get(sub_key.name, sub_key.name)\n\n      values_dict = {}\n      values_dict['Task: {0:s}'.format(name)] = '[ID: {0:s}]'.format(\n          sub_key.name)\n\n      event_data = windows_events.WindowsRegistryEventData()\n      event_data.key_path = registry_key.path\n      event_data.offset = registry_key.offset\n      event_data.regvalue = values_dict\n\n      event = time_events.DateTimeValuesEvent(\n          registry_key.last_written_time, definitions.TIME_DESCRIPTION_WRITTEN)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n      event_data = TaskCacheEventData()\n      event_data.task_name = name\n      event_data.task_identifier = sub_key.name\n\n      last_registered_time = dynamic_info_record.last_registered_time\n      if last_registered_time:\n        # Note this is likely either the last registered time or\n        # the update time.\n        date_time = dfdatetime_filetime.Filetime(timestamp=last_registered_time)\n        event = time_events.DateTimeValuesEvent(\n            date_time, 'Last registered time')\n        parser_mediator.ProduceEventWithEventData(event, event_data)\n\n      launch_time = dynamic_info_record.launch_time\n      if launch_time:\n        # Note this is likely the launch time.\n        date_time = dfdatetime_filetime.Filetime(timestamp=launch_time)\n        event = time_events.DateTimeValuesEvent(\n            date_time, 'Launch time')\n        parser_mediator.ProduceEventWithEventData(event, event_data)\n\n      unknown_time = getattr(dynamic_info_record, 'unknown_time', None)\n      if unknown_time:\n        date_time = dfdatetime_filetime.Filetime(timestamp=unknown_time)\n        event = time_events.DateTimeValuesEvent(\n            date_time, definitions.TIME_DESCRIPTION_UNKNOWN)\n        parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Extracts events from a Task Cache key."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nverifying a PLS Recall record.", "response": "def _VerifyRecord(self, pls_record):\n    \"\"\"Verifies a PLS Recall record.\n\n    Args:\n      pls_record (pls_recall_record): a PLS Recall record to verify.\n\n    Returns:\n      bool: True if this is a valid PLS Recall record, False otherwise.\n    \"\"\"\n    # Verify that the timestamp is no more than six years into the future.\n    # Six years is an arbitrary time length just to evaluate the timestamp\n    # against some value. There is no guarantee that this will catch everything.\n    # TODO: Add a check for similarly valid value back in time. Maybe if it the\n    # timestamp is before 1980 we are pretty sure it is invalid?\n    # TODO: This is a very flaky assumption. Find a better one.\n    future_timestamp = (\n        timelib.Timestamp.GetNow() + self._SIX_YEARS_IN_MICRO_SECONDS)\n\n    if pls_record.last_written_time > future_timestamp:\n      return False\n\n    # Take the first word from the query field and attempt to match that against\n    # known query keywords.\n    first_word, _, _ = pls_record.query.partition(' ')\n\n    if first_word.lower() not in self._PLS_KEYWORD:\n      return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses a PLSRecall. dat file - like object.", "response": "def ParseFileObject(self, parser_mediator, file_object):\n    \"\"\"Parses a PLSRecall.dat file-like object.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      file_object (dfvfs.FileIO): a file-like object.\n\n    Raises:\n      UnableToParseFile: when the file cannot be parsed.\n    \"\"\"\n    file_offset = 0\n    file_size = file_object.get_size()\n    record_map = self._GetDataTypeMap('pls_recall_record')\n\n    while file_offset < file_size:\n      try:\n        pls_record, record_data_size = self._ReadStructureFromFileObject(\n            file_object, file_offset, record_map)\n      except (ValueError, errors.ParseError) as exception:\n        if file_offset == 0:\n          raise errors.UnableToParseFile('Unable to parse first record.')\n\n        parser_mediator.ProduceExtractionWarning((\n            'unable to parse record at offset: 0x{0:08x} with error: '\n            '{1!s}').format(file_offset, exception))\n        break\n\n      if file_offset == 0 and not self._VerifyRecord(pls_record):\n        raise errors.UnableToParseFile('Verification of first record failed.')\n\n      event_data = PlsRecallEventData()\n      event_data.database_name = pls_record.database_name.rstrip('\\x00')\n      event_data.sequence_number = pls_record.sequence_number\n      event_data.offset = file_offset\n      event_data.query = pls_record.query.rstrip('\\x00')\n      event_data.username = pls_record.username.rstrip('\\x00')\n\n      date_time = dfdatetime_delphi_date_time.DelphiDateTime(\n          timestamp=pls_record.last_written_time)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_WRITTEN)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n      file_offset += record_data_size"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef GetMessages(self, formatter_mediator, event):\n    if self.DATA_TYPE != event.data_type:\n      raise errors.WrongFormatter('Unsupported data type: {0:s}.'.format(\n          event.data_type))\n\n    event_values = event.CopyToDict()\n\n    regvalue = event_values.get('regvalue', {})\n    string_parts = []\n    for key, value in sorted(regvalue.items()):\n      string_parts.append('{0:s}: {1!s}'.format(key, value))\n    event_values['text'] = ' '.join(string_parts)\n\n    urls = event_values.get('urls', [])\n    if urls:\n      event_values['urls'] = ' - '.join(urls)\n\n    if 'key_path' in event_values:\n      format_string = self.FORMAT_STRING\n    else:\n      format_string = self.FORMAT_STRING_ALTERNATIVE\n\n    return self._FormatMessages(\n        format_string, self.FORMAT_STRING_SHORT, event_values)", "response": "Determines the formatted message strings for an event object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef GetSources(self, event):\n    if self.DATA_TYPE != event.data_type:\n      raise errors.WrongFormatter('Unsupported data type: {0:s}.'.format(\n          event.data_type))\n\n    source_long = getattr(event, 'source_long', 'UNKNOWN')\n    source_append = getattr(event, 'source_append', None)\n    if source_append:\n      source_long = '{0:s} {1:s}'.format(source_long, source_append)\n\n    return self.SOURCE_SHORT, source_long", "response": "Determines the short and long source for an event object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ExamineEvent(self, mediator, event):\n    if event.data_type not in self._DATATYPES:\n      return\n\n    url = getattr(event, 'url', None)\n    if url is None:\n      return\n    parsed_url = urlparse.urlparse(url)\n    domain = getattr(parsed_url, 'netloc', None)\n    if domain in self._domains:\n      # We've already found an event containing this domain.\n      return\n    self._domains.append(domain)", "response": "Analyzes an event and extracts domains from it."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef CompileReport(self, mediator):\n    lines_of_text = ['Listing domains visited by all users']\n    for domain in sorted(self._domains):\n      lines_of_text.append(domain)\n\n    lines_of_text.append('')\n    report_text = '\\n'.join(lines_of_text)\n    return reports.AnalysisReport(plugin_name=self.NAME, text=report_text)", "response": "Compiles an analysis report."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndetermining if a Windows Registry key matches the filter.", "response": "def Match(self, registry_key):\n    \"\"\"Determines if a Windows Registry key matches the filter.\n\n    Args:\n      registry_key (dfwinreg.WinRegistryKey): Windows Registry key.\n\n    Returns:\n      bool: True if the Windows Registry key matches the filter.\n    \"\"\"\n    key_path_upper = registry_key.path.upper()\n    # Prevent this filter matching non-string MRUListEx values.\n    for ignore_key_path_suffix in self._IGNORE_KEY_PATH_SUFFIXES:\n      if key_path_upper.endswith(ignore_key_path_suffix):\n        return False\n\n    for ignore_key_path_segment in self._IGNORE_KEY_PATH_SEGMENTS:\n      if ignore_key_path_segment in key_path_upper:\n        return False\n\n    return super(MRUListExStringRegistryKeyFilter, self).Match(registry_key)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses a MRUListEx key and returns a new object.", "response": "def _ParseMRUListExKey(\n      self, parser_mediator, registry_key, codepage='cp1252'):\n    \"\"\"Extract event objects from a MRUListEx Registry key.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      registry_key (dfwinreg.WinRegistryKey): Windows Registry key.\n      codepage (Optional[str]): extended ASCII string codepage.\n    \"\"\"\n    try:\n      mrulistex = self._ParseMRUListExValue(registry_key)\n    except (ValueError, errors.ParseError) as exception:\n      parser_mediator.ProduceExtractionWarning(\n          'unable to parse MRUListEx value with error: {0!s}'.format(exception))\n      return\n\n    if not mrulistex:\n      return\n\n    values_dict = {}\n    found_terminator = False\n    for entry_index, entry_number in enumerate(mrulistex):\n      # The MRU list is terminated with -1 (0xffffffff).\n      if entry_number == -1:\n        break\n\n      if found_terminator:\n        parser_mediator.ProduceExtractionWarning((\n            'found additional MRUListEx entries after terminator in key: '\n            '{0:s}.').format(registry_key.path))\n\n        # Only create one parser error per terminator.\n        found_terminator = False\n\n      value_string = self._ParseMRUListExEntryValue(\n          parser_mediator, registry_key, entry_index, entry_number,\n          codepage=codepage)\n\n      value_text = 'Index: {0:d} [MRU Value {1:d}]'.format(\n          entry_index + 1, entry_number)\n\n      values_dict[value_text] = value_string\n\n    event_data = windows_events.WindowsRegistryEventData()\n    event_data.key_path = registry_key.path\n    event_data.offset = registry_key.offset\n    event_data.regvalue = values_dict\n    event_data.source_append = self._SOURCE_APPEND\n\n    event = time_events.DateTimeValuesEvent(\n        registry_key.last_written_time, definitions.TIME_DESCRIPTION_WRITTEN)\n    parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _ParseMRUListExEntryValue(\n      self, parser_mediator, registry_key, entry_index, entry_number, **kwargs):\n    \"\"\"Parses the MRUListEx entry value.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      registry_key (dfwinreg.WinRegistryKey): Windows Registry key that contains\n           the MRUListEx value.\n      entry_index (int): MRUListEx entry index.\n      entry_number (int): entry number.\n\n    Returns:\n      str: MRUList entry value.\n    \"\"\"\n    value_string = ''\n\n    value = registry_key.GetValueByName('{0:d}'.format(entry_number))\n    if value is None:\n      parser_mediator.ProduceExtractionWarning(\n          'missing MRUListEx value: {0:d} in key: {1:s}.'.format(\n              entry_number, registry_key.path))\n\n    elif value.DataIsString():\n      value_string = value.GetDataAsObject()\n\n    elif value.DataIsBinaryData():\n      utf16le_string_map = self._GetDataTypeMap('utf16le_string')\n\n      try:\n        value_string = self._ReadStructureFromByteStream(\n            value.data, 0, utf16le_string_map)\n      except (ValueError, errors.ParseError) as exception:\n        parser_mediator.ProduceExtractionWarning((\n            'unable to parse MRUListEx entry value: {0:d} with error: '\n            '{1!s}').format(entry_number, exception))\n\n      value_string = value_string.rstrip('\\x00')\n\n    return value_string", "response": "Parses the MRUListEx entry value."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nextract events from a Windows Registry key.", "response": "def ExtractEvents(\n      self, parser_mediator, registry_key, codepage='cp1252', **kwargs):\n    \"\"\"Extracts events from a Windows Registry key.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      registry_key (dfwinreg.WinRegistryKey): Windows Registry key.\n      codepage (Optional[str]): extended ASCII string codepage.\n    \"\"\"\n    self._ParseMRUListExKey(parser_mediator, registry_key, codepage=codepage)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse the MRUListEx entry value.", "response": "def _ParseMRUListExEntryValue(\n      self, parser_mediator, registry_key, entry_index, entry_number,\n      codepage='cp1252', **kwargs):\n    \"\"\"Parses the MRUListEx entry value.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      registry_key (dfwinreg.WinRegistryKey): Windows Registry key that contains\n           the MRUListEx value.\n      entry_index (int): MRUListEx entry index.\n      entry_number (int): entry number.\n      codepage (Optional[str]): extended ASCII string codepage.\n\n    Returns:\n      str: MRUList entry value.\n    \"\"\"\n    value_string = ''\n\n    value = registry_key.GetValueByName('{0:d}'.format(entry_number))\n    if value is None:\n      parser_mediator.ProduceExtractionWarning(\n          'missing MRUListEx value: {0:d} in key: {1:s}.'.format(\n              entry_number, registry_key.path))\n\n    elif not value.DataIsBinaryData():\n      logger.debug((\n          '[{0:s}] Non-binary MRUListEx entry value: {1:d} in key: '\n          '{2:s}.').format(self.NAME, entry_number, registry_key.path))\n\n    elif value.data:\n      shell_items_parser = shell_items.ShellItemsParser(registry_key.path)\n      shell_items_parser.ParseByteStream(\n          parser_mediator, value.data, codepage=codepage)\n\n      value_string = 'Shell item path: {0:s}'.format(\n          shell_items_parser.CopyToPath())\n\n    return value_string"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _ParseMRUListExEntryValue(\n      self, parser_mediator, registry_key, entry_index, entry_number,\n      codepage='cp1252', **kwargs):\n    \"\"\"Parses the MRUListEx entry value.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      registry_key (dfwinreg.WinRegistryKey): Windows Registry key that contains\n           the MRUListEx value.\n      entry_index (int): MRUListEx entry index.\n      entry_number (int): entry number.\n      codepage (Optional[str]): extended ASCII string codepage.\n\n    Returns:\n      str: MRUList entry value.\n    \"\"\"\n    value_string = ''\n\n    value = registry_key.GetValueByName('{0:d}'.format(entry_number))\n    if value is None:\n      parser_mediator.ProduceExtractionWarning(\n          'missing MRUListEx value: {0:d} in key: {1:s}.'.format(\n              entry_number, registry_key.path))\n\n    elif not value.DataIsBinaryData():\n      logger.debug((\n          '[{0:s}] Non-binary MRUListEx entry value: {1:d} in key: '\n          '{2:s}.').format(self.NAME, entry_number, registry_key.path))\n\n    elif value.data:\n      utf16le_string_map = self._GetDataTypeMap('utf16le_string')\n\n      context = dtfabric_data_maps.DataTypeMapContext()\n\n      try:\n        path = self._ReadStructureFromByteStream(\n            value.data, 0, utf16le_string_map, context=context)\n      except (ValueError, errors.ParseError) as exception:\n        parser_mediator.ProduceExtractionWarning((\n            'unable to parse MRUListEx entry value: {0:d} with error: '\n            '{1!s}').format(entry_number, exception))\n        return value_string\n\n      path = path.rstrip('\\x00')\n\n      shell_item_data = value.data[context.byte_size:]\n\n      if not shell_item_data:\n        parser_mediator.ProduceExtractionWarning((\n            'missing shell item in MRUListEx value: {0:d} in key: '\n            '{1:s}.').format(entry_number, registry_key.path))\n        value_string = 'Path: {0:s}'.format(path)\n\n      else:\n        shell_items_parser = shell_items.ShellItemsParser(registry_key.path)\n        shell_items_parser.ParseByteStream(\n            parser_mediator, shell_item_data, codepage=codepage)\n\n        value_string = 'Path: {0:s}, Shell item: [{1:s}]'.format(\n            path, shell_items_parser.CopyToPath())\n\n    return value_string", "response": "Parses the MRUListEx entry value."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ExtractEvents(\n      self, parser_mediator, registry_key, codepage='cp1252', **kwargs):\n    \"\"\"Extracts events from a Windows Registry key.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      registry_key (dfwinreg.WinRegistryKey): Windows Registry key.\n      codepage (Optional[str]): extended ASCII string codepage.\n    \"\"\"\n    self._ParseMRUListExKey(parser_mediator, registry_key, codepage=codepage)\n\n    if registry_key.name == 'RecentDocs':\n      # For the RecentDocs MRUListEx we also need to parse its subkeys\n      # since the Registry key path does not support wildcards yet.\n      for subkey in registry_key.GetSubkeys():\n        self._ParseMRUListExKey(parser_mediator, subkey, codepage=codepage)", "response": "Extracts events from a Windows Registry key."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _ParseComment(self, structure):\n    if structure[1] == 'Date:':\n      self._year, self._month, self._day_of_month, _, _, _ = structure.date_time\n    elif structure[1] == 'Fields:':\n      self._ParseFieldsMetadata(structure)", "response": "Parses a comment.\n\n    Args:\n      structure (pyparsing.ParseResults): structure parsed from the log file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _ParseFieldsMetadata(self, structure):\n    fields = structure.fields.split(' ')\n\n    log_line_structure = pyparsing.Empty()\n    if fields[0] == 'date' and fields[1] == 'time':\n      log_line_structure += self.DATE_TIME.setResultsName('date_time')\n      fields = fields[2:]\n\n    for member in fields:\n      log_line_structure += self._LOG_LINE_STRUCTURES.get(member, self.URI)\n\n    updated_structures = []\n    for line_structure in self._line_structures:\n      if line_structure[0] != 'logline':\n        updated_structures.append(line_structure)\n    updated_structures.append(('logline', log_line_structure))\n    # TODO: self._line_structures is a work-around and this needs\n    # a structural fix.\n    self._line_structures = updated_structures", "response": "Parses the fields metadata and updates the log line definition to match."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse a single log line and produce an event object.", "response": "def _ParseLogLine(self, parser_mediator, structure):\n    \"\"\"Parse a single log line and produce an event object.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      structure (pyparsing.ParseResults): structure parsed from the log file.\n    \"\"\"\n    if structure.date_time:\n      time_elements_tuple = structure.date_time\n\n    elif structure.date and structure.time:\n      year, month, day_of_month = structure.date\n      hours, minutes, seconds = structure.time\n      time_elements_tuple = (year, month, day_of_month, hours, minutes, seconds)\n\n    elif structure.time:\n      hours, minutes, seconds = structure.time\n      time_elements_tuple = (\n          self._year, self._month, self._day_of_month, hours, minutes, seconds)\n\n    else:\n      parser_mediator.ProduceExtractionWarning('missing date and time values')\n      return\n\n    try:\n      date_time = dfdatetime_time_elements.TimeElements(\n          time_elements_tuple=time_elements_tuple)\n    except ValueError:\n      parser_mediator.ProduceExtractionWarning(\n          'invalid date time value: {0!s}'.format(time_elements_tuple))\n      return\n\n    event_data = IISEventData()\n\n    for key, value in iter(structure.items()):\n      if key in ('date', 'date_time', 'time') or value == '-':\n        continue\n\n      if isinstance(value, pyparsing.ParseResults):\n        value = ''.join(value)\n\n      setattr(event_data, key, value)\n\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_WRITTEN)\n    parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nverifying that this file is an IIS log file.", "response": "def VerifyStructure(self, parser_mediator, line):\n    \"\"\"Verify that this file is an IIS log file.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between\n          parsers and other components, such as storage and dfvfs.\n      line (str): line from a text file.\n\n    Returns:\n      bool: True if the line was successfully parsed.\n    \"\"\"\n    # TODO: self._line_structures is a work-around and this needs\n    # a structural fix.\n    self._line_structures = self.LINE_STRUCTURES\n\n    self._day_of_month = None\n    self._month = None\n    self._year = None\n\n    # TODO: Examine other versions of the file format and if this parser should\n    # support them. For now just checking if it contains the IIS header.\n    if self._SIGNATURE in line:\n      return True\n\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nextracts a container or graph identifier from a JSON file s path.", "response": "def _GetIdentifierFromPath(self, parser_mediator):\n    \"\"\"Extracts a container or a graph ID from a JSON file's path.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n\n    Returns:\n      str: container or graph identifier.\n    \"\"\"\n    file_entry = parser_mediator.GetFileEntry()\n    path = file_entry.path_spec.location\n    file_system = file_entry.GetFileSystem()\n    path_segments = file_system.SplitPath(path)\n    return path_segments[-2]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _ParseLayerConfigJSON(self, parser_mediator, file_object):\n    file_content = file_object.read()\n    file_content = codecs.decode(file_content, self._ENCODING)\n\n    json_dict = json.loads(file_content)\n\n    if 'docker_version' not in json_dict:\n      raise errors.UnableToParseFile(\n          'not a valid Docker layer configuration file, missing '\n          '\\'docker_version\\' key.')\n\n    if 'created' in json_dict:\n      layer_creation_command_array = [\n          x.strip() for x in json_dict['container_config']['Cmd']]\n      layer_creation_command = ' '.join(layer_creation_command_array).replace(\n          '\\t', '')\n\n      event_data = DockerJSONLayerEventData()\n      event_data.command = layer_creation_command\n      event_data.layer_id = self._GetIdentifierFromPath(parser_mediator)\n\n      timestamp = timelib.Timestamp.FromTimeString(json_dict['created'])\n      event = time_events.TimestampEvent(\n          timestamp, definitions.TIME_DESCRIPTION_ADDED)\n      parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Parses a Docker filesystem layer configuration file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing a Docker container configuration file.", "response": "def _ParseContainerConfigJSON(self, parser_mediator, file_object):\n    \"\"\"Extracts events from a Docker container configuration file.\n\n    The path of each container config file is:\n    DOCKER_DIR/containers/<container_id>/config.json\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      file_object (dfvfs.FileIO): a file-like object.\n\n    Raises:\n      UnableToParseFile: when the file is not a valid container config file.\n    \"\"\"\n    file_content = file_object.read()\n    file_content = codecs.decode(file_content, self._ENCODING)\n\n    json_dict = json.loads(file_content)\n\n    if 'Driver' not in json_dict:\n      raise errors.UnableToParseFile(\n          'not a valid Docker container configuration file, ' 'missing '\n          '\\'Driver\\' key.')\n\n    container_id_from_path = self._GetIdentifierFromPath(parser_mediator)\n    container_id_from_json = json_dict.get('ID', None)\n    if not container_id_from_json:\n      raise errors.UnableToParseFile(\n          'not a valid Docker layer configuration file, the \\'ID\\' key is '\n          'missing from the JSON dict (should be {0:s})'.format(\n              container_id_from_path))\n\n    if container_id_from_json != container_id_from_path:\n      raise errors.UnableToParseFile(\n          'not a valid Docker container configuration file. The \\'ID\\' key of '\n          'the JSON dict ({0:s}) is different from the layer ID taken from the'\n          ' path to the file ({1:s}) JSON file.)'.format(\n              container_id_from_json, container_id_from_path))\n\n    if 'Config' in json_dict and 'Hostname' in json_dict['Config']:\n      container_name = json_dict['Config']['Hostname']\n    else:\n      container_name = 'Unknown container name'\n\n    event_data = DockerJSONContainerEventData()\n    event_data.container_id = container_id_from_path\n    event_data.container_name = container_name\n\n    if 'State' in json_dict:\n      if 'StartedAt' in json_dict['State']:\n        event_data.action = 'Container Started'\n\n        timestamp = timelib.Timestamp.FromTimeString(\n            json_dict['State']['StartedAt'])\n        event = time_events.TimestampEvent(\n            timestamp, definitions.TIME_DESCRIPTION_START)\n        parser_mediator.ProduceEventWithEventData(event, event_data)\n\n      if 'FinishedAt' in json_dict['State']:\n        if json_dict['State']['FinishedAt'] != '0001-01-01T00:00:00Z':\n          event_data.action = 'Container Finished'\n\n          # If the timestamp is 0001-01-01T00:00:00Z, the container\n          # is still running, so we don't generate a Finished event\n          timestamp = timelib.Timestamp.FromTimeString(\n              json_dict['State']['FinishedAt'])\n          event = time_events.TimestampEvent(\n              timestamp, definitions.TIME_DESCRIPTION_END)\n          parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    created_time = json_dict.get('Created', None)\n    if created_time:\n      event_data.action = 'Container Created'\n\n      timestamp = timelib.Timestamp.FromTimeString(created_time)\n      event = time_events.TimestampEvent(\n          timestamp, definitions.TIME_DESCRIPTION_ADDED)\n      parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses a Docker container log file - like object.", "response": "def _ParseContainerLogJSON(self, parser_mediator, file_object):\n    \"\"\"Extract events from a Docker container log files.\n\n    The format is one JSON formatted log message per line.\n\n    The path of each container log file (which logs the container stdout and\n    stderr) is:\n    DOCKER_DIR/containers/<container_id>/<container_id>-json.log\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      file_object (dfvfs.FileIO): a file-like object.\n    \"\"\"\n    container_id = self._GetIdentifierFromPath(parser_mediator)\n\n    text_file_object = text_file.TextFile(file_object)\n    for log_line in text_file_object:\n      json_log_line = json.loads(log_line)\n\n      time = json_log_line.get('time', None)\n      if not time:\n        continue\n\n      event_data = DockerJSONContainerLogEventData()\n      event_data.container_id = container_id\n      event_data.log_line = json_log_line.get('log', None)\n      event_data.log_source = json_log_line.get('stream', None)\n      # TODO: pass line number to offset or remove.\n      event_data.offset = 0\n\n      timestamp = timelib.Timestamp.FromTimeString(time)\n\n      event = time_events.TimestampEvent(\n          timestamp, definitions.TIME_DESCRIPTION_WRITTEN)\n      parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses a Docker configuration and log file - like object.", "response": "def ParseFileObject(self, parser_mediator, file_object):\n    \"\"\"Parses various Docker configuration and log files in JSON format.\n\n    This methods checks whether the file_object points to a docker JSON config\n    or log file, and calls the corresponding _Parse* function to generate\n    Events.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      file_object (dfvfs.FileIO): a file-like object.\n\n    Raises:\n      UnableToParseFile: when the file cannot be parsed.\n      ValueError: if the JSON file cannot be decoded.\n    \"\"\"\n    # Trivial JSON format check: first character must be an open brace.\n    if file_object.read(1) != b'{':\n      raise errors.UnableToParseFile(\n          'is not a valid JSON file, missing opening brace.')\n\n    file_object.seek(0, os.SEEK_SET)\n\n    file_entry = parser_mediator.GetFileEntry()\n\n    file_system = file_entry.GetFileSystem()\n\n    json_file_path = parser_mediator.GetDisplayName()\n    split_path = file_system.SplitPath(json_file_path)\n    try:\n      if 'containers' in split_path:\n        if 'config.json' in split_path:\n          self._ParseContainerConfigJSON(parser_mediator, file_object)\n        if json_file_path.endswith('-json.log'):\n          self._ParseContainerLogJSON(parser_mediator, file_object)\n      elif 'graph' in split_path:\n        if 'json' in split_path:\n          self._ParseLayerConfigJSON(parser_mediator, file_object)\n    except ValueError as exception:\n      if exception == 'No JSON object could be decoded':\n        raise errors.UnableToParseFile(exception)\n      else:\n        raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ParseMessageRow(self, parser_mediator, query, row, **unused_kwargs):\n    query_hash = hash(query)\n\n    event_data = IMessageEventData()\n    event_data.attachment_location = self._GetRowValue(\n        query_hash, row, 'attachment_location')\n    event_data.imessage_id = self._GetRowValue(query_hash, row, 'imessage_id')\n    event_data.message_type = self._GetRowValue(query_hash, row, 'message_type')\n    event_data.offset = self._GetRowValue(query_hash, row, 'ROWID')\n    event_data.query = query\n    event_data.read_receipt = self._GetRowValue(query_hash, row, 'read_receipt')\n    event_data.service = self._GetRowValue(query_hash, row, 'service')\n    event_data.text = self._GetRowValue(query_hash, row, 'text')\n\n    timestamp = self._GetRowValue(query_hash, row, 'date')\n    date_time = dfdatetime_cocoa_time.CocoaTime(timestamp=timestamp)\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_CREATION)\n    parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Parses a message row."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _ConvertValueBinaryDataToUBInt64(self, value):\n    if not value:\n      return None\n\n    integer_map = self._GetDataTypeMap('uint64be')\n\n    try:\n      return self._ReadStructureFromByteStream(value, 0, integer_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.ParseError(\n          'Unable to parse integer value with error: {0!s}'.format(\n              exception))", "response": "Converts a binary data value into an unsigned 64 - bit integer."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _GetRecordValue(self, record, value_entry):\n    column_type = record.get_column_type(value_entry)\n    long_value = None\n\n    if record.is_long_value(value_entry):\n      long_value = record.get_value_data_as_long_value(value_entry)\n\n    if record.is_multi_value(value_entry):\n      # TODO: implement\n      raise ValueError('Multi value support not implemented yet.')\n\n    if column_type == pyesedb.column_types.NULL:\n      return None\n\n    if column_type == pyesedb.column_types.BOOLEAN:\n      # TODO: implement\n      raise ValueError('Boolean value support not implemented yet.')\n\n    if column_type in self.INTEGER_COLUMN_TYPES:\n      if long_value:\n        raise ValueError('Long integer value not supported.')\n      return record.get_value_data_as_integer(value_entry)\n\n    if column_type in self.FLOATING_POINT_COLUMN_TYPES:\n      if long_value:\n        raise ValueError('Long floating point value not supported.')\n      return record.get_value_data_as_floating_point(value_entry)\n\n    if column_type in self.STRING_COLUMN_TYPES:\n      if long_value:\n        return long_value.get_data_as_string()\n      return record.get_value_data_as_string(value_entry)\n\n    if column_type == pyesedb.column_types.GUID:\n      # TODO: implement\n      raise ValueError('GUID value support not implemented yet.')\n\n    if long_value:\n      return long_value.get_data()\n    return record.get_value_data(value_entry)", "response": "Retrieves a specific value from the record."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves the values from the record.", "response": "def _GetRecordValues(\n      self, parser_mediator, table_name, record, value_mappings=None):\n    \"\"\"Retrieves the values from the record.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      table_name (str): name of the table.\n      record (pyesedb.record): ESE record.\n      value_mappings (Optional[dict[str, str]): value mappings, which map\n          the column name to a callback method.\n\n    Returns:\n      dict[str,object]: values per column name.\n    \"\"\"\n    record_values = {}\n\n    for value_entry in range(0, record.number_of_values):\n      if parser_mediator.abort:\n        break\n\n      column_name = record.get_column_name(value_entry)\n      if column_name in record_values:\n        logger.warning(\n            '[{0:s}] duplicate column: {1:s} in table: {2:s}'.format(\n                self.NAME, column_name, table_name))\n        continue\n\n      value_callback = None\n      if value_mappings and column_name in value_mappings:\n        value_callback_method = value_mappings.get(column_name)\n        if value_callback_method:\n          value_callback = getattr(self, value_callback_method, None)\n          if value_callback is None:\n            logger.warning((\n                '[{0:s}] missing value callback method: {1:s} for column: '\n                '{2:s} in table: {3:s}').format(\n                    self.NAME, value_callback_method, column_name, table_name))\n\n      if value_callback:\n        try:\n          value_data = record.get_value_data(value_entry)\n          value = value_callback(value_data)\n\n        except Exception as exception:  # pylint: disable=broad-except\n          logger.error(exception)\n          value = None\n          parser_mediator.ProduceExtractionWarning((\n              'unable to parse value: {0:s} with callback: {1:s} with error: '\n              '{2!s}').format(column_name, value_callback_method, exception))\n\n      else:\n        try:\n          value = self._GetRecordValue(record, value_entry)\n        except ValueError as exception:\n          value = None\n          parser_mediator.ProduceExtractionWarning(\n              'unable to parse value: {0:s} with error: {1!s}'.format(\n                  column_name, exception))\n\n      record_values[column_name] = value\n\n    return record_values"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nextracts the entries from the database.", "response": "def GetEntries(self, parser_mediator, cache=None, database=None, **kwargs):\n    \"\"\"Extracts event objects from the database.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      cache (Optional[ESEDBCache]): cache.\n      database (Optional[pyesedb.file]): ESE database.\n\n    Raises:\n      ValueError: If the database attribute is not valid.\n    \"\"\"\n    if database is None:\n      raise ValueError('Invalid database.')\n\n    for table_name, callback_method in iter(self._tables.items()):\n      if parser_mediator.abort:\n        break\n\n      if not callback_method:\n        # Table names without a callback method are allowed to improve\n        # the detection of a database based on its table names.\n        continue\n\n      callback = getattr(self, callback_method, None)\n      if callback is None:\n        logger.warning(\n            '[{0:s}] missing callback method: {1:s} for table: {2:s}'.format(\n                self.NAME, callback_method, table_name))\n        continue\n\n      esedb_table = database.get_table_by_name(table_name)\n      if not esedb_table:\n        logger.warning('[{0:s}] missing table: {1:s}'.format(\n            self.NAME, table_name))\n        continue\n\n      # The database is passed in case the database contains table names\n      # that are assigned dynamically and cannot be defined by\n      # the table name-callback mechanism.\n      callback(\n          parser_mediator, cache=cache, database=database, table=esedb_table,\n          **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef Process(self, parser_mediator, cache=None, database=None, **kwargs):\n    if database is None:\n      raise ValueError('Invalid database.')\n\n    # This will raise if unhandled keyword arguments are passed.\n    super(ESEDBPlugin, self).Process(parser_mediator)\n\n    self.GetEntries(\n        parser_mediator, cache=cache, database=database, **kwargs)", "response": "Determines if this is the appropriate plugin for the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef GetEntries(self, parser_mediator, top_level=None, **unused_kwargs):\n    for root, key, datetime_value in interface.RecurseKey(top_level):\n      if not isinstance(datetime_value, datetime.datetime):\n        continue\n\n      event_data = plist_event.PlistTimeEventData()\n      event_data.key = key\n      event_data.root = root\n\n      event = time_events.PythonDatetimeEvent(\n          datetime_value, definitions.TIME_DESCRIPTION_WRITTEN)\n      parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Extracts entries from a Plist."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef Process(self, parser_mediator, plist_name, top_level, **kwargs):\n    logger.debug('Plist {0:s} plugin used for: {1:s}'.format(\n        self.NAME, plist_name))\n    self.GetEntries(parser_mediator, top_level=top_level, **kwargs)", "response": "Process the plist with the default plugin."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ParseRecord(self, parser_mediator, key, structure):\n    if key != 'log_entry':\n      raise errors.ParseError(\n          'Unable to parse record, unknown structure: {0:s}'.format(key))\n\n    event_data = BashHistoryEventData()\n    event_data.command = structure.command\n\n    date_time = dfdatetime_posix_time.PosixTime(timestamp=structure.timestamp)\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_MODIFICATION)\n    parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Parses a record and produces a Bash history event."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef VerifyStructure(self, parser_mediator, lines):\n    match_generator = self._VERIFICATION_GRAMMAR.scanString(lines, maxMatches=1)\n    return bool(list(match_generator))", "response": "Verifies that this is a bash history file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _GetNetworkInfo(self, signatures_key):\n    network_info = {}\n    for category in signatures_key.GetSubkeys():\n      for signature in category.GetSubkeys():\n        profile_guid_value = signature.GetValueByName('ProfileGuid')\n        if profile_guid_value:\n          profile_guid = profile_guid_value.GetDataAsObject()\n        else:\n          continue\n\n        default_gateway_mac_value = signature.GetValueByName(\n            'DefaultGatewayMac')\n        if default_gateway_mac_value:\n          default_gateway_mac = ':'.join([\n              '{0:02x}'.format(octet)\n              for octet in bytearray(default_gateway_mac_value.data)])\n        else:\n          default_gateway_mac = None\n\n        dns_suffix_value = signature.GetValueByName('DnsSuffix')\n        if dns_suffix_value:\n          dns_suffix = dns_suffix_value.GetDataAsObject()\n        else:\n          dns_suffix = None\n\n        network_info[profile_guid] = (default_gateway_mac, dns_suffix)\n\n    return network_info", "response": "Retrieves the network info within the signatures subkey."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing a SYSTEMTIME date and time value from a byte stream.", "response": "def _ParseSystemTime(self, byte_stream):\n    \"\"\"Parses a SYSTEMTIME date and time value from a byte stream.\n\n    Args:\n      byte_stream (bytes): byte stream.\n\n    Returns:\n      dfdatetime.Systemtime: SYSTEMTIME date and time value or None if no\n          value is set.\n\n    Raises:\n      ParseError: if the SYSTEMTIME could not be parsed.\n    \"\"\"\n    systemtime_map = self._GetDataTypeMap('systemtime')\n\n    try:\n      systemtime = self._ReadStructureFromByteStream(\n          byte_stream, 0, systemtime_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.ParseError(\n          'Unable to parse SYSTEMTIME value with error: {0!s}'.format(\n              exception))\n\n    system_time_tuple = (\n        systemtime.year, systemtime.month, systemtime.weekday,\n        systemtime.day_of_month, systemtime.hours, systemtime.minutes,\n        systemtime.seconds, systemtime.milliseconds)\n\n    if system_time_tuple == self._EMPTY_SYSTEM_TIME_TUPLE:\n      return None\n\n    try:\n      return dfdatetime_systemtime.Systemtime(\n          system_time_tuple=system_time_tuple)\n    except ValueError:\n      raise errors.ParseError(\n          'Invalid SYSTEMTIME value: {0!s}'.format(system_time_tuple))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nextracts events from a Windows Registry key.", "response": "def ExtractEvents(self, parser_mediator, registry_key, **kwargs):\n    \"\"\"Extracts events from a Windows Registry key.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      registry_key (dfwinreg.WinRegistryKey): Windows Registry key.\n    \"\"\"\n    network_info = {}\n    signatures = registry_key.GetSubkeyByName('Signatures')\n    if signatures:\n      network_info = self._GetNetworkInfo(signatures)\n\n    profiles = registry_key.GetSubkeyByName('Profiles')\n    if not profiles:\n      return\n\n    for subkey in profiles.GetSubkeys():\n      default_gateway_mac, dns_suffix = network_info.get(\n          subkey.name, (None, None))\n\n      event_data = WindowsRegistryNetworkEventData()\n      event_data.default_gateway_mac = default_gateway_mac\n      event_data.dns_suffix = dns_suffix\n\n      ssid_value = subkey.GetValueByName('ProfileName')\n      if ssid_value:\n        event_data.ssid = ssid_value.GetDataAsObject()\n\n      description_value = subkey.GetValueByName('Description')\n      if description_value:\n        event_data.description = description_value.GetDataAsObject()\n\n      connection_type_value = subkey.GetValueByName('NameType')\n      if connection_type_value:\n        connection_type = connection_type_value.GetDataAsObject()\n        # TODO: move to formatter.\n        connection_type = self._CONNECTION_TYPE.get(\n            connection_type, 'unknown')\n        event_data.connection_type = connection_type\n\n      date_created_value = subkey.GetValueByName('DateCreated')\n      if date_created_value:\n        try:\n          date_time = self._ParseSystemTime(date_created_value.data)\n        except errors.ParseError as exception:\n          date_time = None\n          parser_mediator.ProduceExtractionWarning(\n              'unable to parse date created with error: {0!s}'.format(\n                  exception))\n\n        if date_time:\n          event = time_events.DateTimeValuesEvent(\n              date_time, definitions.TIME_DESCRIPTION_CREATION)\n          parser_mediator.ProduceEventWithEventData(event, event_data)\n\n      date_last_connected_value = subkey.GetValueByName('DateLastConnected')\n      if date_last_connected_value:\n        try:\n          date_time = self._ParseSystemTime(date_last_connected_value.data)\n        except errors.ParseError as exception:\n          date_time = None\n          parser_mediator.ProduceExtractionWarning(\n              'unable to parse date last connected with error: {0!s}'.format(\n                  exception))\n\n        if date_time:\n          event = time_events.DateTimeValuesEvent(\n              date_time, definitions.TIME_DESCRIPTION_LAST_CONNECTED)\n          parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndetermine the formatted message strings for an event object.", "response": "def GetMessages(self, formatter_mediator, event):\n    \"\"\"Determines the formatted message strings for an event object.\n\n    Args:\n      formatter_mediator (FormatterMediator): mediates the interactions\n          between formatters and other components, such as storage and Windows\n          EventLog resources.\n      event (EventObject): event.\n\n    Returns:\n      tuple(str, str): formatted message string and short message string.\n\n    Raises:\n      WrongFormatter: if the event object cannot be formatted by the formatter.\n    \"\"\"\n    if self.DATA_TYPE != event.data_type:\n      raise errors.WrongFormatter('Unsupported data type: {0:s}.'.format(\n          event.data_type))\n\n    event_values = event.CopyToDict()\n\n    message_type = event_values.get('message_type', None)\n    if message_type is not None:\n      event_values['message_type'] = (\n          self._MESSAGE_TYPE.get(message_type, 'UNKNOWN'))\n\n    message_status = event_values.get('message_status', None)\n    if message_status is not None:\n      event_values['message_status'] = (\n          self._MESSAGE_STATUS.get(message_status, 'UNKNOWN'))\n\n    return self._ConditionalFormatMessages(event_values)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef CreateRetryTask(self):\n    retry_task = Task(session_identifier=self.session_identifier)\n    retry_task.file_entry_type = self.file_entry_type\n    retry_task.merge_priority = self.merge_priority\n    retry_task.path_spec = self.path_spec\n    retry_task.storage_file_size = self.storage_file_size\n\n    self.has_retry = True\n\n    return retry_task", "response": "Creates a new task to retry a previously abandoned task."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a task completion.", "response": "def CreateTaskCompletion(self):\n    \"\"\"Creates a task completion.\n\n    Returns:\n      TaskCompletion: task completion attribute container.\n    \"\"\"\n    self.completion_time = int(\n        time.time() * definitions.MICROSECONDS_PER_SECOND)\n\n    task_completion = TaskCompletion()\n    task_completion.aborted = self.aborted\n    task_completion.identifier = self.identifier\n    task_completion.session_identifier = self.session_identifier\n    task_completion.timestamp = self.completion_time\n    return task_completion"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef CreateTaskStart(self):\n    task_start = TaskStart()\n    task_start.identifier = self.identifier\n    task_start.session_identifier = self.session_identifier\n    task_start.timestamp = self.start_time\n    return task_start", "response": "Creates a task start."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the Service DLL for a service in the Registry if it exists.", "response": "def GetServiceDll(self, key):\n    \"\"\"Get the Service DLL for a service, if it exists.\n\n    Checks for a ServiceDLL for in the Parameters subkey of a service key in\n    the Registry.\n\n    Args:\n      key (dfwinreg.WinRegistryKey): a Windows Registry key.\n\n    Returns:\n      str: path of the service DLL or None.\n    \"\"\"\n    parameters_key = key.GetSubkeyByName('Parameters')\n    if not parameters_key:\n      return None\n\n    service_dll = parameters_key.GetValueByName('ServiceDll')\n    if not service_dll:\n      return None\n\n    return service_dll.GetDataAsObject()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nextracting events from a Windows Registry key.", "response": "def ExtractEvents(self, parser_mediator, registry_key, **kwargs):\n    \"\"\"Extracts events from a Windows Registry key.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      registry_key (dfwinreg.WinRegistryKey): Windows Registry key.\n    \"\"\"\n    values_dict = {}\n\n    service_type_value = registry_key.GetValueByName('Type')\n    service_start_value = registry_key.GetValueByName('Start')\n\n    # Grab the ServiceDLL value if it exists.\n    if service_type_value and service_start_value:\n      service_dll = self.GetServiceDll(registry_key)\n      if service_dll:\n        values_dict['ServiceDll'] = service_dll\n\n      # Gather all the other string and integer values and insert as they are.\n      for value in registry_key.GetValues():\n        if not value.name:\n          continue\n        if value.name not in values_dict:\n          if value.DataIsString() or value.DataIsInteger():\n            values_dict[value.name] = value.GetDataAsObject()\n          elif value.DataIsMultiString():\n            values_dict[value.name] = ', '.join(value.GetDataAsObject())\n\n      # Create a specific service event, so that we can recognize and expand\n      # certain values when we're outputting the event.\n      event_data = windows_events.WindowsRegistryServiceEventData()\n      event_data.key_path = registry_key.path\n      event_data.offset = registry_key.offset\n      event_data.regvalue = values_dict\n      event_data.urls = self.URLS\n\n      event = time_events.DateTimeValuesEvent(\n          registry_key.last_written_time, definitions.TIME_DESCRIPTION_WRITTEN)\n      parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef AddNewSignature(self, pattern, offset=None):\n    self.signatures.append(Signature(pattern, offset=offset))", "response": "Adds a new signature to the internal list of signatures."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef AddNewSpecification(self, identifier):\n    if identifier in self._format_specifications:\n      raise KeyError(\n          'Format specification {0:s} is already defined in store.'.format(\n              identifier))\n\n    self._format_specifications[identifier] = FormatSpecification(identifier)\n\n    return self._format_specifications[identifier]", "response": "Adds a new format specification."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef AddSpecification(self, specification):\n    if specification.identifier in self._format_specifications:\n      raise KeyError(\n          'Format specification {0:s} is already defined in store.'.format(\n              specification.identifier))\n\n    self._format_specifications[specification.identifier] = specification\n\n    for signature in specification.signatures:\n      signature_index = len(self._signature_map)\n\n      signature_identifier = '{0:s}:{1:d}'.format(\n          specification.identifier, signature_index)\n\n      if signature_identifier in self._signature_map:\n        raise KeyError('Signature {0:s} is already defined in map.'.format(\n            signature_identifier))\n\n      signature.SetIdentifier(signature_identifier)\n      self._signature_map[signature_identifier] = specification", "response": "Adds a format specification to the store."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving the format specification.", "response": "def GetFormatSpecification(cls):\n    \"\"\"Retrieves the format specification.\n\n    Returns:\n      FormatSpecification: format specification.\n    \"\"\"\n    format_specification = specification.FormatSpecification(cls.NAME)\n\n    # OLECF\n    format_specification.AddNewSignature(\n        b'\\xd0\\xcf\\x11\\xe0\\xa1\\xb1\\x1a\\xe1', offset=0)\n\n    # OLECF beta\n    format_specification.AddNewSignature(\n        b'\\x0e\\x11\\xfc\\x0d\\xd0\\xcf\\x11\\x0e', offset=0)\n\n    return format_specification"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses an OLE Compound File - like object.", "response": "def ParseFileObject(self, parser_mediator, file_object):\n    \"\"\"Parses an OLE Compound File (OLECF) file-like object.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      file_object (dfvfs.FileIO): file-like object.\n    \"\"\"\n    olecf_file = pyolecf.file()\n    olecf_file.set_ascii_codepage(parser_mediator.codepage)\n\n    try:\n      olecf_file.open_file_object(file_object)\n    except IOError as exception:\n      parser_mediator.ProduceExtractionWarning(\n          'unable to open file with error: {0!s}'.format(exception))\n      return\n\n    root_item = olecf_file.root_item\n    if not root_item:\n      return\n\n    # Get a list of all items in the root item from the OLECF file.\n    item_names = [item.name for item in root_item.sub_items]\n\n    # Compare the list of available plugin objects.\n    # We will try to use every plugin against the file (except\n    # the default plugin) and run it. Only if none of the plugins\n    # works will we use the default plugin.\n\n    item_names = frozenset(item_names)\n\n    try:\n      for plugin in self._plugins:\n        if parser_mediator.abort:\n          break\n\n        if not plugin.REQUIRED_ITEMS.issubset(item_names):\n          continue\n\n        try:\n          plugin.UpdateChainAndProcess(parser_mediator, root_item=root_item)\n\n        except Exception as exception:  # pylint: disable=broad-except\n          parser_mediator.ProduceExtractionWarning((\n              'plugin: {0:s} unable to parse OLECF file with error: '\n              '{1!s}').format(plugin.NAME, exception))\n\n      if self._default_plugin and not parser_mediator.abort:\n        try:\n          self._default_plugin.UpdateChainAndProcess(\n              parser_mediator, root_item=root_item)\n\n        except Exception as exception:  # pylint: disable=broad-except\n          parser_mediator.ProduceExtractionWarning((\n              'plugin: {0:s} unable to parse OLECF file with error: '\n              '{1!s}').format(self._default_plugin.NAME, exception))\n\n    finally:\n      olecf_file.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndetermines the formatted message strings for an event object.", "response": "def GetMessages(self, formatter_mediator, event):\n    \"\"\"Determines the formatted message strings for an event object.\n\n    Args:\n      formatter_mediator (FormatterMediator): mediates the interactions\n          between formatters and other components, such as storage and Windows\n          EventLog resources.\n      event (EventObject): event.\n\n    Returns:\n      tuple(str, str): formatted message string and short message string.\n\n    Raises:\n      WrongFormatter: if the event object cannot be formatted by the formatter.\n    \"\"\"\n    if self.DATA_TYPE != event.data_type:\n      raise errors.WrongFormatter('Unsupported data type: {0:s}.'.format(\n          event.data_type))\n\n    event_values = event.CopyToDict()\n\n    event = event_values.get('event', None)\n    if event:\n      event_values['event_map'] = self.EVENT_NAMES.get(event, 'Unknown')\n\n    category = event_values.get('cat', None)\n    if category:\n      event_values['category_map'] = self.CATEGORY_NAMES.get(\n          category, 'Unknown')\n\n    action = event_values.get('action0', None)\n    if action:\n      event_values['action0_map'] = self.ACTION_0_NAMES.get(action, 'Unknown')\n\n    action = event_values.get('action1', None)\n    if action:\n      event_values['action1_map'] = self.ACTION_1_2_NAMES.get(\n          action, 'Unknown')\n\n    action = event_values.get('action2', None)\n    if action:\n      event_values['action2_map'] = self.ACTION_1_2_NAMES.get(\n          action, 'Unknown')\n\n    return self._ConditionalFormatMessages(event_values)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef GetEntries(self, parser_mediator, match=None, **unused_kwargs):\n    devices = match.get('Devices', {})\n    for device_identifier, device_information in iter(devices.items()):\n      datetime_value = device_information.get('Connected', None)\n      if not datetime_value:\n        continue\n\n      event_data = IPodPlistEventData()\n      event_data.device_id = device_identifier\n\n      # TODO: refactor.\n      for key, value in iter(device_information.items()):\n        if key == 'Connected':\n          continue\n        attribute_name = key.lower().replace(' ', '_')\n        setattr(event_data, attribute_name, value)\n\n      event = time_events.PythonDatetimeEvent(\n          datetime_value, definitions.TIME_DESCRIPTION_LAST_CONNECTED)\n      parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Extracts device information from the iPod plist."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nretrieving the event formatter for a specific event type.", "response": "def GetEventFormatter(self, event):\n    \"\"\"Retrieves the event formatter for a specific event type.\n\n    Args:\n      event (EventObject): event.\n\n    Returns:\n      EventFormatter: event formatter or None.\n    \"\"\"\n    data_type = getattr(event, 'data_type', None)\n    if not data_type:\n      return None\n\n    return formatters_manager.FormattersManager.GetFormatterObject(\n        event.data_type)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef GetFormattedMessages(self, event):\n    event_formatter = self.GetEventFormatter(event)\n    if not event_formatter:\n      return None, None\n\n    return event_formatter.GetMessages(self._formatter_mediator, event)", "response": "Retrieves the formatted messages related to the event."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef GetFormattedSources(self, event):\n    event_formatter = self.GetEventFormatter(event)\n    if not event_formatter:\n      return None, None\n\n    return event_formatter.GetSources(event)", "response": "Retrieves the formatted sources related to the event."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef GetFormatStringAttributeNames(self, event):\n    event_formatter = self.GetEventFormatter(event)\n    if not event_formatter:\n      return None\n\n    return event_formatter.GetFormatStringAttributeNames()", "response": "Retrieves the attribute names in the format string."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve the hostname related to the event.", "response": "def GetHostname(self, event, default_hostname='-'):\n    \"\"\"Retrieves the hostname related to the event.\n\n    Args:\n      event (EventObject): event.\n      default_hostname (Optional[str]): default hostname.\n\n    Returns:\n      str: hostname.\n    \"\"\"\n    hostname = getattr(event, 'hostname', None)\n    if hostname:\n      return hostname\n\n    session_identifier = event.GetSessionIdentifier()\n    if session_identifier is None:\n      return default_hostname\n\n    hostname = self._knowledge_base.GetHostname(\n        session_identifier=session_identifier)\n    return hostname or default_hostname"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nretrieving the MACB representation of the event object.", "response": "def GetMACBRepresentation(self, event):\n    \"\"\"Retrieves the MACB representation.\n\n    Args:\n      event (EventObject): event.\n\n    Returns:\n      str: MACB representation.\n    \"\"\"\n    data_type = getattr(event, 'data_type', None)\n    if not data_type:\n      return '....'\n\n    # The filestat parser is somewhat limited.\n    if data_type == 'fs:stat':\n      descriptions = event.timestamp_desc.split(';')\n\n      return_characters = ['.', '.', '.', '.']\n      for description in descriptions:\n        if description in (\n            'mtime', definitions.TIME_DESCRIPTION_MODIFICATION):\n          return_characters[0] = 'M'\n        elif description in (\n            'atime', definitions.TIME_DESCRIPTION_LAST_ACCESS):\n          return_characters[1] = 'A'\n        elif description in (\n            'ctime', definitions.TIME_DESCRIPTION_CHANGE):\n          return_characters[2] = 'C'\n        elif description in (\n            'crtime', definitions.TIME_DESCRIPTION_CREATION):\n          return_characters[3] = 'B'\n\n      return ''.join(return_characters)\n\n    # Access time.\n    if event.timestamp_desc in [\n        definitions.TIME_DESCRIPTION_LAST_ACCESS,\n        definitions.TIME_DESCRIPTION_ACCOUNT_CREATED,\n        definitions.TIME_DESCRIPTION_LAST_VISITED,\n        definitions.TIME_DESCRIPTION_START,\n        definitions.TIME_DESCRIPTION_LAST_SHUTDOWN,\n        definitions.TIME_DESCRIPTION_LAST_LOGIN,\n        definitions.TIME_DESCRIPTION_LAST_PASSWORD_RESET,\n        definitions.TIME_DESCRIPTION_LAST_CONNECTED,\n        definitions.TIME_DESCRIPTION_LAST_RUN,\n        definitions.TIME_DESCRIPTION_LAST_PRINTED]:\n      return '.A..'\n\n    # Content modification.\n    if event.timestamp_desc in [\n        definitions.TIME_DESCRIPTION_MODIFICATION,\n        definitions.TIME_DESCRIPTION_WRITTEN,\n        definitions.TIME_DESCRIPTION_DELETED]:\n      return 'M...'\n\n    # Content creation time.\n    if event.timestamp_desc in [\n        definitions.TIME_DESCRIPTION_CREATION,\n        definitions.TIME_DESCRIPTION_ADDED,\n        definitions.TIME_DESCRIPTION_FILE_DOWNLOADED,\n        definitions.TIME_DESCRIPTION_FIRST_CONNECTED]:\n      return '...B'\n\n    # Metadata modification.\n    if event.timestamp_desc in [\n        definitions.TIME_DESCRIPTION_CHANGE,\n        definitions.TIME_DESCRIPTION_ENTRY_MODIFICATION]:\n      return '..C.'\n\n    return '....'"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndetermines the MACB representation from the timestamp descriptions.", "response": "def GetMACBRepresentationFromDescriptions(self, timestamp_descriptions):\n    \"\"\"Determines the MACB representation from the timestamp descriptions.\n\n    MACB representation is a shorthand for representing one or more of\n    modification, access, change, birth timestamp descriptions as the letters\n    \"MACB\" or a \".\" if the corresponding timestamp is not set.\n\n    Note that this is an output format shorthand and does not guarantee that\n    the timestamps represent the same occurrence.\n\n    Args:\n      timestamp_descriptions (list[str]): timestamp descriptions, which are\n          defined in definitions.TIME_DESCRIPTIONS.\n\n    Returns:\n      str: MACB representation.\n    \"\"\"\n    macb_representation = []\n\n    if ('mtime' in timestamp_descriptions or\n        definitions.TIME_DESCRIPTION_MODIFICATION in timestamp_descriptions):\n      macb_representation.append('M')\n    else:\n      macb_representation.append('.')\n\n    if ('atime' in timestamp_descriptions or\n        definitions.TIME_DESCRIPTION_LAST_ACCESS in timestamp_descriptions):\n      macb_representation.append('A')\n    else:\n      macb_representation.append('.')\n\n    if ('ctime' in timestamp_descriptions or\n        definitions.TIME_DESCRIPTION_CHANGE in timestamp_descriptions):\n      macb_representation.append('C')\n    else:\n      macb_representation.append('.')\n\n    if ('crtime' in timestamp_descriptions or\n        definitions.TIME_DESCRIPTION_CREATION in timestamp_descriptions):\n      macb_representation.append('B')\n    else:\n      macb_representation.append('.')\n\n    return ''.join(macb_representation)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef GetUsername(self, event, default_username='-'):\n    username = getattr(event, 'username', None)\n    if username and username != '-':\n      return username\n\n    session_identifier = event.GetSessionIdentifier()\n    if session_identifier is None:\n      return default_username\n\n    user_sid = getattr(event, 'user_sid', None)\n    username = self._knowledge_base.GetUsernameByIdentifier(\n        user_sid, session_identifier=session_identifier)\n    return username or default_username", "response": "Retrieves the username related to the event."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef SetTimezone(self, timezone):\n    if not timezone:\n      return\n\n    try:\n      self._timezone = pytz.timezone(timezone)\n    except pytz.UnknownTimeZoneError:\n      raise ValueError('Unsupported timezone: {0:s}'.format(timezone))", "response": "Sets the timezone.\n\n    Args:\n      timezone (str): timezone.\n\n    Raises:\n      ValueError: if the timezone is not supported."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef DeregisterAttributeContainer(cls, attribute_container_class):\n    container_type = attribute_container_class.CONTAINER_TYPE.lower()\n    if container_type not in cls._attribute_container_classes:\n      raise KeyError(\n          'Attribute container class not set for container type: '\n          '{0:s}.'.format(attribute_container_class.CONTAINER_TYPE))\n\n    del cls._attribute_container_classes[container_type]", "response": "Deregisters an attribute container class."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef Process(self, parser_mediator, date_time, syslog_tokens, **kwargs):\n    body = syslog_tokens.get('body', None)\n    if not body:\n      raise AttributeError('Missing required attribute: body')\n\n    for key, grammar in iter(self.MESSAGE_GRAMMARS):\n      try:\n        tokens = grammar.parseString(body)\n        syslog_tokens.update(tokens.asDict())\n        self.ParseMessage(parser_mediator, key, date_time, syslog_tokens)\n        return\n\n      except pyparsing.ParseException:\n        pass\n\n    raise errors.WrongPlugin('Unable to create event from: {0:s}'.format(body))", "response": "Processes the syslog_tokens and returns the event object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ParseOptions(cls, options, output_module):\n    if not isinstance(output_module, sqlite_4n6time.SQLite4n6TimeOutputModule):\n      raise errors.BadConfigObject(\n          'Output module is not an instance of SQLite4n6TimeOutputModule')\n\n    shared_4n6time_output.Shared4n6TimeOutputArgumentsHelper.ParseOptions(\n        options, output_module)\n\n    filename = getattr(options, 'write', None)\n    if not filename:\n      raise errors.BadConfigOption(\n          'Output filename was not provided use \"-w filename\" to specify.')\n\n    output_module.SetFilename(filename)", "response": "Parses and validates the options."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _GetFlagValues(self, flags):\n    event_types = []\n    for event_flag, description in self._FLAG_VALUES.items():\n      if event_flag & flags:\n        event_types.append(description)\n    return ', '.join(event_types)", "response": "Returns a comma separated string containing the descriptions of the events indicated by a set of fsevents flags."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses an OLECF item.", "response": "def _ParseItem(self, parser_mediator, olecf_item):\n    \"\"\"Parses an OLECF item.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      olecf_item (pyolecf.item): OLECF item.\n\n    Returns:\n      bool: True if an event was produced.\n    \"\"\"\n    result = False\n\n    event_data = OLECFItemEventData()\n    event_data.name = olecf_item.name\n    event_data.offset = 0\n    event_data.size = olecf_item.size\n\n    creation_time, modification_time = self._GetTimestamps(olecf_item)\n    if creation_time:\n      date_time = dfdatetime_filetime.Filetime(timestamp=creation_time)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_CREATION)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n      result = True\n\n    if modification_time:\n      date_time = dfdatetime_filetime.Filetime(timestamp=modification_time)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_MODIFICATION)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n      result = True\n\n    for sub_item in olecf_item.sub_items:\n      if self._ParseItem(parser_mediator, sub_item):\n        result = True\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef Process(self, parser_mediator, root_item=None, **kwargs):\n    # This will raise if unhandled keyword arguments are passed.\n    super(DefaultOLECFPlugin, self).Process(parser_mediator, **kwargs)\n\n    if not root_item:\n      raise ValueError('Root item not set.')\n\n    if not self._ParseItem(parser_mediator, root_item):\n      event_data = OLECFItemEventData()\n      event_data.name = root_item.name\n      event_data.offset = 0\n      event_data.size = root_item.size\n\n      # If no event was produced, produce at least one for the root item.\n      date_time = dfdatetime_semantic_time.SemanticTime('Not set')\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_CREATION)\n      parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Parses an OLECF file and returns an object containing the root item."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef GetEventTaggingRules(self):\n    tagging_rules = {}\n\n    label_name = None\n    with io.open(self._path, 'r', encoding='utf-8') as tagging_file:\n      for line in tagging_file.readlines():\n        line = line.rstrip()\n\n        stripped_line = line.lstrip()\n        if not stripped_line or stripped_line[0] == '#':\n          continue\n\n        if not line[0].isspace():\n          label_name = line\n          tagging_rules[label_name] = []\n          continue\n\n        if not label_name:\n          continue\n\n        filter_object = event_filter.EventObjectFilter()\n\n        try:\n          filter_object.CompileFilter(stripped_line)\n        except errors.ParseError as exception:\n          raise errors.TaggingFileError((\n              'Unable to compile filter for label: {0:s} with error: '\n              '{1!s}').format(label_name, exception))\n\n        if filter_object not in tagging_rules[label_name]:\n          tagging_rules[label_name].append(filter_object)\n\n    return tagging_rules", "response": "Retrieves the event tagging rules from the tagging file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nretrieve a specific string value from the data dict.", "response": "def _GetStringValue(self, data_dict, name, default_value=None):\n    \"\"\"Retrieves a specific string value from the data dict.\n\n    Args:\n      data_dict (dict[str, list[str]): values per name.\n      name (str): name of the value to retrieve.\n      default_value (Optional[object]): value to return if the name has no value\n          set in data_dict.\n\n    Returns:\n      str: value represented as a string.\n    \"\"\"\n    values = data_dict.get(name, None)\n    if not values:\n      return default_value\n\n    for index, value in enumerate(values):\n      if ',' in value:\n        values[index] = '\"{0:s}\"'.format(value)\n\n    return ', '.join(values)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _ParseAttribute(self, file_object):\n    file_offset = file_object.tell()\n    attribute_map = self._GetDataTypeMap('cups_ipp_attribute')\n\n    try:\n      attribute, _ = self._ReadStructureFromFileObject(\n          file_object, file_offset, attribute_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.ParseError(\n          'Unable to parse attribute with error: {0!s}'.format(exception))\n\n    value = None\n    if attribute.tag_value in self._INTEGER_TAG_VALUES:\n      # TODO: correct file offset to point to the start of value_data.\n      value = self._ParseIntegerValue(attribute.value_data, file_offset)\n\n    elif attribute.tag_value == self._TAG_VALUE_BOOLEAN:\n      value = self._ParseBooleanValue(attribute.value_data)\n\n    elif attribute.tag_value == self._TAG_VALUE_DATE_TIME:\n      # TODO: correct file offset to point to the start of value_data.\n      value = self._ParseDateTimeValue(attribute.value_data, file_offset)\n\n    elif attribute.tag_value in self._STRING_WITHOUT_LANGUAGE_VALUES:\n      value = attribute.value_data.decode(self._last_charset_attribute)\n\n    elif attribute.tag_value in self._ASCII_STRING_VALUES:\n      value = attribute.value_data.decode('ascii')\n\n      if attribute.tag_value == self._TAG_VALUE_CHARSET:\n        self._last_charset_attribute = value\n\n    else:\n      value = attribute.value_data\n\n    return attribute.name, value", "response": "Parses a CUPS IPP attribute from a file - like object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _ParseAttributesGroup(self, file_object):\n    tag_value_map = self._GetDataTypeMap('int8')\n    tag_value = 0\n\n    while tag_value != self._DELIMITER_TAG_END_OF_ATTRIBUTES:\n      file_offset = file_object.tell()\n\n      tag_value, _ = self._ReadStructureFromFileObject(\n          file_object, file_offset, tag_value_map)\n\n      if tag_value >= 0x10:\n        file_object.seek(file_offset, os.SEEK_SET)\n\n        yield self._ParseAttribute(file_object)\n\n      elif (tag_value != self._DELIMITER_TAG_END_OF_ATTRIBUTES and\n            tag_value not in self._DELIMITER_TAGS):\n        raise errors.ParseError((\n            'Unsupported attributes groups start tag value: '\n            '0x{0:02x}.').format(tag_value))", "response": "Parses a CUPS IPP attributes group from a file - like object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse a boolean value.", "response": "def _ParseBooleanValue(self, byte_stream):\n    \"\"\"Parses a boolean value.\n\n    Args:\n      byte_stream (bytes): byte stream.\n\n    Returns:\n      bool: boolean value.\n\n    Raises:\n      ParseError: when the boolean value cannot be parsed.\n    \"\"\"\n    if byte_stream == b'\\x00':\n      return False\n\n    if byte_stream == b'\\x01':\n      return True\n\n    raise errors.ParseError('Unsupported boolean value.')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse a CUPS IPP RFC2579 date - time value from a byte stream.", "response": "def _ParseDateTimeValue(self, byte_stream, file_offset):\n    \"\"\"Parses a CUPS IPP RFC2579 date-time value from a byte stream.\n\n    Args:\n      byte_stream (bytes): byte stream.\n      file_offset (int): offset of the attribute data relative to the start of\n          the file-like object.\n\n    Returns:\n      dfdatetime.RFC2579DateTime: RFC2579 date-time stored in the value.\n\n    Raises:\n      ParseError: when the RFC2579 date-time value cannot be parsed.\n    \"\"\"\n    datetime_value_map = self._GetDataTypeMap('cups_ipp_datetime_value')\n\n    try:\n      value = self._ReadStructureFromByteStream(\n          byte_stream, file_offset, datetime_value_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.ParseError(\n          'Unable to parse datetime value with error: {0!s}'.format(exception))\n\n    direction_from_utc = chr(value.direction_from_utc)\n    rfc2579_date_time_tuple = (\n        value.year, value.month, value.day_of_month,\n        value.hours, value.minutes, value.seconds, value.deciseconds,\n        direction_from_utc, value.hours_from_utc, value.minutes_from_utc)\n    return dfdatetime_rfc2579_date_time.RFC2579DateTime(\n        rfc2579_date_time_tuple=rfc2579_date_time_tuple)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse an integer value from a byte stream.", "response": "def _ParseIntegerValue(self, byte_stream, file_offset):\n    \"\"\"Parses an integer value.\n\n    Args:\n      byte_stream (bytes): byte stream.\n      file_offset (int): offset of the attribute data relative to the start of\n          the file-like object.\n\n    Returns:\n      int: integer value.\n\n    Raises:\n      ParseError: when the integer value cannot be parsed.\n    \"\"\"\n    data_type_map = self._GetDataTypeMap('int32be')\n\n    try:\n      return self._ReadStructureFromByteStream(\n          byte_stream, file_offset, data_type_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.ParseError(\n          'Unable to parse integer value with error: {0!s}'.format(exception))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _ParseHeader(self, parser_mediator, file_object):\n    header_map = self._GetDataTypeMap('cups_ipp_header')\n\n    try:\n      header, _ = self._ReadStructureFromFileObject(file_object, 0, header_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.UnableToParseFile(\n          '[{0:s}] Unable to parse header with error: {1!s}'.format(\n              self.NAME, exception))\n\n    format_version = '{0:d}.{1:d}'.format(\n        header.major_version, header.minor_version)\n    if format_version not in self._SUPPORTED_FORMAT_VERSIONS:\n      raise errors.UnableToParseFile(\n          '[{0:s}] Unsupported format version {1:s}.'.format(\n              self.NAME, format_version))\n\n    if header.operation_identifier != 5:\n      # TODO: generate ExtractionWarning instead of printing debug output.\n      display_name = parser_mediator.GetDisplayName()\n      logger.debug((\n          '[{0:s}] Non-standard operation identifier: 0x{1:08x} in file header '\n          'of: {2:s}.').format(\n              self.NAME, header.operation_identifier, display_name))", "response": "Parses a CUPS IPP header from a file - like object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses a CUPS IPP file - like object.", "response": "def ParseFileObject(self, parser_mediator, file_object):\n    \"\"\"Parses a CUPS IPP file-like object.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      file_object (dfvfs.FileIO): file-like object.\n\n    Raises:\n      UnableToParseFile: when the file cannot be parsed.\n    \"\"\"\n    self._last_charset_attribute = 'ascii'\n\n    self._ParseHeader(parser_mediator, file_object)\n\n    data_dict = {}\n    time_dict = {}\n\n    try:\n      for name, value in self._ParseAttributesGroup(file_object):\n        name = self._ATTRIBUTE_NAME_TRANSLATION.get(name, name)\n\n        if name in self._DATE_TIME_VALUE_NAMES:\n          time_dict.setdefault(name, []).append(value)\n        else:\n          data_dict.setdefault(name, []).append(value)\n\n    except (ValueError, errors.ParseError) as exception:\n      parser_mediator.ProduceExtractionWarning(\n          'unable to parse attributes with error: {0!s}'.format(exception))\n      return\n\n    event_data = CupsIppEventData()\n    event_data.application = self._GetStringValue(data_dict, 'application')\n    event_data.computer_name = self._GetStringValue(data_dict, 'computer_name')\n    event_data.copies = data_dict.get('copies', [0])[0]\n    event_data.data_dict = data_dict\n    event_data.doc_type = self._GetStringValue(data_dict, 'doc_type')\n    event_data.job_id = self._GetStringValue(data_dict, 'job_id')\n    event_data.job_name = self._GetStringValue(data_dict, 'job_name')\n    event_data.user = self._GetStringValue(data_dict, 'user')\n    event_data.owner = self._GetStringValue(data_dict, 'owner')\n    event_data.printer_id = self._GetStringValue(data_dict, 'printer_id')\n    event_data.uri = self._GetStringValue(data_dict, 'uri')\n\n    for name, usage in iter(self._DATE_TIME_VALUES.items()):\n      for date_time in time_dict.get(name, []):\n        event = time_events.DateTimeValuesEvent(date_time, usage)\n        parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    for name, usage in iter(self._POSIX_TIME_VALUES.items()):\n      for time_value in time_dict.get(name, []):\n        date_time = dfdatetime_posix_time.PosixTime(timestamp=time_value)\n        event = time_events.DateTimeValuesEvent(date_time, usage)\n        parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _CreateEventTag(self, event, comment, labels):\n    event_identifier = event.GetIdentifier()\n\n    event_tag = events.EventTag(comment=comment)\n    event_tag.SetEventIdentifier(event_identifier)\n    event_tag.AddLabels(labels)\n\n    event_identifier_string = event_identifier.CopyToString()\n    logger.debug('Created event tag: {0:s} for event: {1:s}'.format(\n        comment, event_identifier_string))\n\n    return event_tag", "response": "Creates an event tag."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _HandleHashAnalysis(self, hash_analysis):\n    tags = []\n    labels = self.GenerateLabels(hash_analysis.hash_information)\n    path_specifications = self._hash_pathspecs.pop(hash_analysis.subject_hash)\n    for path_specification in path_specifications:\n      event_identifiers = self._event_identifiers_by_pathspec.pop(\n          path_specification, [])\n\n      if not labels:\n        continue\n\n      for event_identifier in event_identifiers:\n        event_tag = events.EventTag(comment=self._comment)\n        event_tag.SetEventIdentifier(event_identifier)\n        event_tag.AddLabels(labels)\n\n        tags.append(event_tag)\n\n    return path_specifications, labels, tags", "response": "Handles the analysis of a given hash."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _EnsureRequesterStarted(self):\n    if not self._analyzer_started:\n      self._analyzer.start()\n      self._analyzer_started = True", "response": "Checks if the analyzer is running and starts it if not."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ExamineEvent(self, mediator, event):\n    self._EnsureRequesterStarted()\n\n    path_spec = event.pathspec\n    event_identifiers = self._event_identifiers_by_pathspec[path_spec]\n\n    event_identifier = event.GetIdentifier()\n    event_identifiers.append(event_identifier)\n\n    if event.data_type not in self.DATA_TYPES or not self._analyzer.lookup_hash:\n      return\n\n    lookup_hash = '{0:s}_hash'.format(self._analyzer.lookup_hash)\n    lookup_hash = getattr(event, lookup_hash, None)\n    if not lookup_hash:\n      display_name = mediator.GetDisplayNameForPathSpec(path_spec)\n      logger.warning((\n          'Lookup hash attribute: {0:s}_hash missing from event that '\n          'originated from: {1:s}.').format(\n              self._analyzer.lookup_hash, display_name))\n      return\n\n    path_specs = self._hash_pathspecs[lookup_hash]\n    path_specs.append(path_spec)\n    # There may be multiple path specification that have the same hash. We only\n    # want to look them up once.\n    if len(path_specs) == 1:\n      self.hash_queue.put(lookup_hash)", "response": "Evaluates whether an event contains the right data for a hash lookup."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndetermine if the plugin should continue trying to compile the report.", "response": "def _ContinueReportCompilation(self):\n    \"\"\"Determines if the plugin should continue trying to compile the report.\n\n    Returns:\n      bool: True if the plugin should continue, False otherwise.\n    \"\"\"\n    analyzer_alive = self._analyzer.is_alive()\n    hash_queue_has_tasks = self.hash_queue.unfinished_tasks > 0\n    analysis_queue = not self.hash_analysis_queue.empty()\n\n    # pylint: disable=consider-using-ternary\n    return (analyzer_alive and hash_queue_has_tasks) or analysis_queue"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _LogProgressUpdateIfReasonable(self):\n    next_log_time = (\n        self._time_of_last_status_log +\n        self.SECONDS_BETWEEN_STATUS_LOG_MESSAGES)\n    current_time = time.time()\n    if current_time < next_log_time:\n      return\n    completion_time = time.ctime(current_time + self.EstimateTimeRemaining())\n    log_message = (\n        '{0:s} hash analysis plugin running. {1:d} hashes in queue, '\n        'estimated completion time {2:s}.'.format(\n            self.NAME, self.hash_queue.qsize(), completion_time))\n    logger.info(log_message)\n    self._time_of_last_status_log = current_time", "response": "Prints a progress update if enough time has passed."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef CompileReport(self, mediator):\n    # TODO: refactor to update the counter on demand instead of\n    # during reporting.\n    path_specs_per_labels_counter = collections.Counter()\n    tags = []\n    while self._ContinueReportCompilation():\n      try:\n        self._LogProgressUpdateIfReasonable()\n        hash_analysis = self.hash_analysis_queue.get(\n            timeout=self._analysis_queue_timeout)\n      except Queue.Empty:\n        # The result queue is empty, but there could still be items that need\n        # to be processed by the analyzer.\n        continue\n      pathspecs, labels, new_tags = self._HandleHashAnalysis(\n          hash_analysis)\n\n      tags.extend(new_tags)\n      for label in labels:\n        path_specs_per_labels_counter[label] += len(pathspecs)\n\n    self._analyzer.SignalAbort()\n\n    lines_of_text = ['{0:s} hash tagging results'.format(self.NAME)]\n    for label, count in sorted(path_specs_per_labels_counter.items()):\n      line_of_text = (\n          '{0:d} path specifications tagged with label: {1:s}'.format(\n              count, label))\n      lines_of_text.append(line_of_text)\n    lines_of_text.append('')\n    report_text = '\\n'.join(lines_of_text)\n\n    for event_tag in tags:\n      mediator.ProduceEventTag(event_tag)\n\n    return reports.AnalysisReport(\n        plugin_name=self.NAME, text=report_text)", "response": "Compiles an analysis report."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nestimates how long until all hashes have been analyzed.", "response": "def EstimateTimeRemaining(self):\n    \"\"\"Estimates how long until all hashes have been analyzed.\n\n    Returns:\n      int: estimated number of seconds until all hashes have been analyzed.\n    \"\"\"\n    number_of_hashes = self.hash_queue.qsize()\n    hashes_per_batch = self._analyzer.hashes_per_batch\n    wait_time_per_batch = self._analyzer.wait_after_analysis\n    analyses_performed = self._analyzer.analyses_performed\n\n    if analyses_performed == 0:\n      average_analysis_time = self._analyzer.seconds_spent_analyzing\n    else:\n      average_analysis_time, _ = divmod(\n          self._analyzer.seconds_spent_analyzing, analyses_performed)\n\n    batches_remaining, _ = divmod(number_of_hashes, hashes_per_batch)\n    estimated_seconds_per_batch = average_analysis_time + wait_time_per_batch\n    return batches_remaining * estimated_seconds_per_batch"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nretrieves a list of items from a queue.", "response": "def _GetHashes(self, target_queue, max_hashes):\n    \"\"\"Retrieves a list of items from a queue.\n\n    Args:\n      target_queue (Queue.queue): queue to retrieve hashes from.\n      max_hashes (int): maximum number of items to retrieve from the\n          target_queue.\n\n    Returns:\n      list[object]: list of at most max_hashes elements from the target_queue.\n          The list may have no elements if the target_queue is empty.\n    \"\"\"\n    hashes = []\n    for _ in range(0, max_hashes):\n      try:\n        item = target_queue.get_nowait()\n      except Queue.Empty:\n        continue\n      hashes.append(item)\n    return hashes"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the hash to query.", "response": "def SetLookupHash(self, lookup_hash):\n    \"\"\"Sets the hash to query.\n\n    Args:\n      lookup_hash (str): name of the hash attribute to look up.\n\n    Raises:\n      ValueError: if the lookup hash is not supported.\n    \"\"\"\n    if lookup_hash not in self.SUPPORTED_HASHES:\n      raise ValueError('Unsupported lookup hash: {0!s}'.format(lookup_hash))\n\n    self.lookup_hash = lookup_hash"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _CheckPythonVersionAndDisableWarnings(self):\n    if self._checked_for_old_python_version:\n      return\n    if sys.version_info[0:3] < (2, 7, 9):\n      logger.warning(\n          'You are running a version of Python prior to 2.7.9. Your version '\n          'of Python has multiple weaknesses in its SSL implementation that '\n          'can allow an attacker to read or modify SSL encrypted data. '\n          'Please update. Further SSL warnings will be suppressed. See '\n          'https://www.python.org/dev/peps/pep-0466/ for more information.')\n\n      # Some distributions de-vendor urllib3 from requests, so we have to\n      # check if this has occurred and disable warnings in the correct\n      # package.\n      urllib3_module = urllib3\n      if not urllib3_module:\n        if hasattr(requests, 'packages'):\n          urllib3_module = getattr(requests.packages, 'urllib3')\n\n      if urllib3_module and hasattr(urllib3_module, 'disable_warnings'):\n        urllib3_module.disable_warnings()\n\n    self._checked_for_old_python_version = True", "response": "Checks python version and disables SSL warnings."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef MakeRequestAndDecodeJSON(self, url, method, **kwargs):\n    method_upper = method.upper()\n    if method_upper not in ('GET', 'POST'):\n      raise ValueError('Method {0:s} is not supported')\n\n    if url.lower().startswith('https'):\n      self._CheckPythonVersionAndDisableWarnings()\n\n    try:\n      if method_upper == 'GET':\n        response = requests.get(url, **kwargs)\n\n      elif method_upper == 'POST':\n        response = requests.post(url, **kwargs)\n\n      response.raise_for_status()\n\n    except requests.ConnectionError as exception:\n      error_string = 'Unable to connect to {0:s} with error: {1!s}'.format(\n          url, exception)\n      raise errors.ConnectionError(error_string)\n\n    except requests.HTTPError as exception:\n      error_string = '{0:s} returned a HTTP error: {1!s}'.format(\n          url, exception)\n      raise errors.ConnectionError(error_string)\n\n    return response.json()", "response": "Makes a HTTP request and decodes the results as JSON."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses an application usage row.", "response": "def ParseApplicationUsageRow(\n      self, parser_mediator, query, row, **unused_kwargs):\n    \"\"\"Parses an application usage row.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      query (str): query that created the row.\n      row (sqlite3.Row): row.\n    \"\"\"\n    query_hash = hash(query)\n\n    # TODO: replace usage by definition(s) in eventdata. Not sure which values\n    # it will hold here.\n    application_name = self._GetRowValue(query_hash, row, 'event')\n    usage = 'Application {0:s}'.format(application_name)\n\n    event_data = MacOSApplicationUsageEventData()\n    event_data.application = self._GetRowValue(query_hash, row, 'app_path')\n    event_data.app_version = self._GetRowValue(query_hash, row, 'app_version')\n    event_data.bundle_id = self._GetRowValue(query_hash, row, 'bundle_id')\n    event_data.count = self._GetRowValue(query_hash, row, 'number_times')\n    event_data.query = query\n\n    timestamp = self._GetRowValue(query_hash, row, 'last_time')\n    date_time = dfdatetime_posix_time.PosixTime(timestamp=timestamp)\n    event = time_events.DateTimeValuesEvent(date_time, usage)\n    parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the start timestamp of the slice.", "response": "def start_timestamp(self):\n    \"\"\"int: slice start timestamp or None.\"\"\"\n    if self.event_timestamp:\n      return self.event_timestamp - (\n          self.duration * self._MICRO_SECONDS_PER_MINUTE)\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nanalyzing the contents of a specific data stream of a file entry.", "response": "def _AnalyzeDataStream(self, mediator, file_entry, data_stream_name):\n    \"\"\"Analyzes the contents of a specific data stream of a file entry.\n\n    The results of the analyzers are set in the parser mediator as attributes\n    that are added to produced event objects. Note that some file systems\n    allow directories to have data streams, e.g. NTFS.\n\n    Args:\n      mediator (ParserMediator): mediates the interactions between\n          parsers and other components, such as storage and abort signals.\n      file_entry (dfvfs.FileEntry): file entry whose data stream is to be\n          analyzed.\n      data_stream_name (str): name of the data stream.\n\n    Raises:\n      RuntimeError: if the file-like object cannot be retrieved from\n          the file entry.\n    \"\"\"\n    display_name = mediator.GetDisplayName()\n    logger.debug('[AnalyzeDataStream] analyzing file: {0:s}'.format(\n        display_name))\n\n    if self._processing_profiler:\n      self._processing_profiler.StartTiming('analyzing')\n\n    try:\n      file_object = file_entry.GetFileObject(data_stream_name=data_stream_name)\n      if not file_object:\n        raise RuntimeError((\n            'Unable to retrieve file-like object for file entry: '\n            '{0:s}.').format(display_name))\n\n      try:\n        self._AnalyzeFileObject(mediator, file_object)\n      finally:\n        file_object.close()\n\n    finally:\n      if self._processing_profiler:\n        self._processing_profiler.StopTiming('analyzing')\n\n    logger.debug(\n        '[AnalyzeDataStream] completed analyzing file: {0:s}'.format(\n            display_name))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _AnalyzeFileObject(self, mediator, file_object):\n    maximum_read_size = max([\n        analyzer_object.SIZE_LIMIT for analyzer_object in self._analyzers])\n\n    hashers_only = True\n    for analyzer_object in self._analyzers:\n      if not isinstance(analyzer_object, hashing_analyzer.HashingAnalyzer):\n        hashers_only = False\n        break\n\n    file_size = file_object.get_size()\n\n    if (hashers_only and self._hasher_file_size_limit and\n        file_size > self._hasher_file_size_limit):\n      return\n\n    file_object.seek(0, os.SEEK_SET)\n\n    data = file_object.read(maximum_read_size)\n    while data:\n      if self._abort:\n        break\n\n      for analyzer_object in self._analyzers:\n        if self._abort:\n          break\n\n        if (not analyzer_object.INCREMENTAL_ANALYZER and\n            file_size > analyzer_object.SIZE_LIMIT):\n          continue\n\n        if (isinstance(analyzer_object, hashing_analyzer.HashingAnalyzer) and\n            self._hasher_file_size_limit and\n            file_size > self._hasher_file_size_limit):\n          continue\n\n        self.processing_status = analyzer_object.PROCESSING_STATUS_HINT\n\n        analyzer_object.Analyze(data)\n\n        self.last_activity_timestamp = time.time()\n\n      data = file_object.read(maximum_read_size)\n\n    display_name = mediator.GetDisplayName()\n    for analyzer_object in self._analyzers:\n      if self._abort:\n        break\n\n      for result in analyzer_object.GetResults():\n        logger.debug((\n            '[AnalyzeFileObject] attribute {0:s}:{1:s} calculated for '\n            'file: {2:s}.').format(\n                result.attribute_name, result.attribute_value, display_name))\n\n        mediator.AddEventAttribute(\n            result.attribute_name, result.attribute_value)\n\n      analyzer_object.Reset()\n\n    self.processing_status = definitions.STATUS_INDICATOR_RUNNING", "response": "Processes a file - like object with analyzers."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndetermining if analysis and extraction of a data stream can be skipped.", "response": "def _CanSkipDataStream(self, file_entry, data_stream):\n    \"\"\"Determines if analysis and extraction of a data stream can be skipped.\n\n    This is used to prevent Plaso trying to run analyzers or extract content\n    from a pipe or socket it encounters while processing a mounted filesystem.\n\n    Args:\n      file_entry (dfvfs.FileEntry): file entry to consider for skipping.\n      data_stream (dfvfs.DataStream): data stream to consider for skipping.\n\n    Returns:\n      bool: True if the data stream can be skipped.\n    \"\"\"\n    if file_entry.IsFile():\n      return False\n\n    if data_stream.IsDefault():\n      return True\n\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndetermining if content extraction of a file entry can be skipped.", "response": "def _CanSkipContentExtraction(self, file_entry):\n    \"\"\"Determines if content extraction of a file entry can be skipped.\n\n    Args:\n      file_entry (dfvfs.FileEntry): file entry of which to determine content\n          extraction can be skipped.\n\n    Returns:\n      bool: True if content extraction can be skipped.\n    \"\"\"\n    # TODO: make this filtering solution more generic. Also see:\n    # https://github.com/log2timeline/plaso/issues/467\n    location = getattr(file_entry.path_spec, 'location', None)\n    if not location:\n      return False\n\n    data_stream_name = getattr(file_entry.path_spec, 'data_stream', None)\n    if data_stream_name:\n      return False\n\n    file_system = file_entry.GetFileSystem()\n\n    path_segments = file_system.SplitPath(location)\n    if not path_segments:\n      return False\n\n    if self._CHROME_CACHE_DATA_FILE_RE.match(path_segments[-1]):\n      location_segments = path_segments[:-1]\n      location_segments.append('index')\n      location = file_system.JoinPath(location_segments)\n      index_path_spec = path_spec_factory.Factory.NewPathSpec(\n          file_entry.type_indicator, location=location,\n          parent=file_entry.path_spec.parent)\n\n      if file_system.FileEntryExistsByPathSpec(index_path_spec):\n        # TODO: improve this check if \"index\" is a Chrome Cache index file.\n        return True\n\n    elif self._FIREFOX_CACHE_DATA_FILE_RE.match(path_segments[-1]):\n      location_segments = path_segments[:-4]\n      location_segments.append('_CACHE_MAP_')\n      location = file_system.JoinPath(location_segments)\n      cache_map_path_spec = path_spec_factory.Factory.NewPathSpec(\n          file_entry.type_indicator, location=location,\n          parent=file_entry.path_spec.parent)\n\n      if file_system.FileEntryExistsByPathSpec(cache_map_path_spec):\n        # TODO: improve this check if \"_CACHE_MAP_\" is a Firefox Cache\n        # version 1 cache map file.\n        return True\n\n    elif self._FIREFOX_CACHE2_DATA_FILE_RE.match(path_segments[-1]):\n      location_segments = path_segments[:-2]\n      location_segments.append('index')\n      location = file_system.JoinPath(location_segments)\n      index_path_spec = path_spec_factory.Factory.NewPathSpec(\n          file_entry.type_indicator, location=location,\n          parent=file_entry.path_spec.parent)\n\n      if file_system.FileEntryExistsByPathSpec(index_path_spec):\n        # TODO: improve this check if \"index\" is a Firefox Cache version 2\n        # index file.\n        return True\n\n    elif len(path_segments) == 1 and path_segments[0].lower() in (\n        'hiberfil.sys', 'pagefile.sys', 'swapfile.sys'):\n      return True\n\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nextracting the content from a data stream.", "response": "def _ExtractContentFromDataStream(\n      self, mediator, file_entry, data_stream_name):\n    \"\"\"Extracts content from a data stream.\n\n    Args:\n      mediator (ParserMediator): mediates the interactions between\n          parsers and other components, such as storage and abort signals.\n      file_entry (dfvfs.FileEntry): file entry to extract its content.\n      data_stream_name (str): name of the data stream whose content is to be\n          extracted.\n    \"\"\"\n    self.processing_status = definitions.STATUS_INDICATOR_EXTRACTING\n\n    if self._processing_profiler:\n      self._processing_profiler.StartTiming('extracting')\n\n    self._event_extractor.ParseDataStream(\n        mediator, file_entry, data_stream_name)\n\n    if self._processing_profiler:\n      self._processing_profiler.StopTiming('extracting')\n\n    self.processing_status = definitions.STATUS_INDICATOR_RUNNING\n\n    self.last_activity_timestamp = time.time()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _ExtractMetadataFromFileEntry(self, mediator, file_entry, data_stream):\n    # Do not extract metadata from the root file entry when it is virtual.\n    if file_entry.IsRoot() and file_entry.type_indicator not in (\n        self._TYPES_WITH_ROOT_METADATA):\n      return\n\n    # We always want to extract the file entry metadata but we only want\n    # to parse it once per file entry, so we only use it if we are\n    # processing the default data stream of regular files.\n    if data_stream and not data_stream.IsDefault():\n      return\n\n    display_name = mediator.GetDisplayName()\n    logger.debug(\n        '[ExtractMetadataFromFileEntry] processing file entry: {0:s}'.format(\n            display_name))\n\n    self.processing_status = definitions.STATUS_INDICATOR_EXTRACTING\n\n    if self._processing_profiler:\n      self._processing_profiler.StartTiming('extracting')\n\n    self._event_extractor.ParseFileEntryMetadata(mediator, file_entry)\n\n    if self._processing_profiler:\n      self._processing_profiler.StopTiming('extracting')\n\n    self.processing_status = definitions.STATUS_INDICATOR_RUNNING", "response": "Extracts metadata from a file entry."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _GetArchiveTypes(self, mediator, path_spec):\n    try:\n      type_indicators = analyzer.Analyzer.GetArchiveTypeIndicators(\n          path_spec, resolver_context=mediator.resolver_context)\n    except IOError as exception:\n      type_indicators = []\n\n      warning_message = (\n          'analyzer failed to determine archive type indicators '\n          'with error: {0!s}').format(exception)\n      mediator.ProduceExtractionWarning(warning_message, path_spec=path_spec)\n\n    return type_indicators", "response": "Determines if a data stream contains an archive such as TAR or ZIP."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _GetCompressedStreamTypes(self, mediator, path_spec):\n    try:\n      type_indicators = analyzer.Analyzer.GetCompressedStreamTypeIndicators(\n          path_spec, resolver_context=mediator.resolver_context)\n    except IOError as exception:\n      type_indicators = []\n\n      warning_message = (\n          'analyzer failed to determine compressed stream type indicators '\n          'with error: {0!s}').format(exception)\n      mediator.ProduceExtractionWarning(warning_message, path_spec=path_spec)\n\n    return type_indicators", "response": "Determines if a data stream contains a compressed stream such as gzip."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndetermine if the file entry is a metadata file.", "response": "def _IsMetadataFile(self, file_entry):\n    \"\"\"Determines if the file entry is a metadata file.\n\n    Args:\n      file_entry (dfvfs.FileEntry): a file entry object.\n\n    Returns:\n      bool: True if the file entry is a metadata file.\n    \"\"\"\n    if (file_entry.type_indicator == dfvfs_definitions.TYPE_INDICATOR_TSK and\n        file_entry.path_spec.location in self._METADATA_FILE_LOCATIONS_TSK):\n      return True\n\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _ProcessArchiveTypes(self, mediator, path_spec, type_indicators):\n    number_of_type_indicators = len(type_indicators)\n    if number_of_type_indicators == 0:\n      return\n\n    self.processing_status = definitions.STATUS_INDICATOR_COLLECTING\n\n    if number_of_type_indicators > 1:\n      display_name = mediator.GetDisplayName()\n      logger.debug((\n          'Found multiple format type indicators: {0:s} for '\n          'archive file: {1:s}').format(type_indicators, display_name))\n\n    for type_indicator in type_indicators:\n      if type_indicator == dfvfs_definitions.TYPE_INDICATOR_TAR:\n        archive_path_spec = path_spec_factory.Factory.NewPathSpec(\n            dfvfs_definitions.TYPE_INDICATOR_TAR, location='/',\n            parent=path_spec)\n\n      elif type_indicator == dfvfs_definitions.TYPE_INDICATOR_ZIP:\n        archive_path_spec = path_spec_factory.Factory.NewPathSpec(\n            dfvfs_definitions.TYPE_INDICATOR_ZIP, location='/',\n            parent=path_spec)\n\n      else:\n        archive_path_spec = None\n\n        warning_message = (\n            'unsupported archive format type indicator: {0:s}').format(\n                type_indicator)\n        mediator.ProduceExtractionWarning(\n            warning_message, path_spec=path_spec)\n\n      if archive_path_spec:\n        try:\n          path_spec_generator = self._path_spec_extractor.ExtractPathSpecs(\n              [archive_path_spec], resolver_context=mediator.resolver_context)\n\n          for generated_path_spec in path_spec_generator:\n            if self._abort:\n              break\n\n            event_source = event_sources.FileEntryEventSource(\n                path_spec=generated_path_spec)\n            event_source.file_entry_type = (\n                dfvfs_definitions.FILE_ENTRY_TYPE_FILE)\n            mediator.ProduceEventSource(event_source)\n\n            self.last_activity_timestamp = time.time()\n\n        except (IOError, errors.MaximumRecursionDepth) as exception:\n          warning_message = (\n              'unable to process archive file with error: {0!s}').format(\n                  exception)\n          mediator.ProduceExtractionWarning(\n              warning_message, path_spec=generated_path_spec)", "response": "Processes a data stream containing archive types such as TAR or ZIP."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _ProcessCompressedStreamTypes(self, mediator, path_spec, type_indicators):\n    number_of_type_indicators = len(type_indicators)\n    if number_of_type_indicators == 0:\n      return\n\n    self.processing_status = definitions.STATUS_INDICATOR_COLLECTING\n\n    if number_of_type_indicators > 1:\n      display_name = mediator.GetDisplayName()\n      logger.debug((\n          'Found multiple format type indicators: {0:s} for '\n          'compressed stream file: {1:s}').format(\n              type_indicators, display_name))\n\n    for type_indicator in type_indicators:\n      if type_indicator == dfvfs_definitions.TYPE_INDICATOR_BZIP2:\n        compressed_stream_path_spec = path_spec_factory.Factory.NewPathSpec(\n            dfvfs_definitions.TYPE_INDICATOR_COMPRESSED_STREAM,\n            compression_method=dfvfs_definitions.COMPRESSION_METHOD_BZIP2,\n            parent=path_spec)\n\n      elif type_indicator == dfvfs_definitions.TYPE_INDICATOR_GZIP:\n        compressed_stream_path_spec = path_spec_factory.Factory.NewPathSpec(\n            dfvfs_definitions.TYPE_INDICATOR_GZIP, parent=path_spec)\n\n      else:\n        compressed_stream_path_spec = None\n\n        warning_message = (\n            'unsupported compressed stream format type indicators: '\n            '{0:s}').format(type_indicator)\n        mediator.ProduceExtractionWarning(\n            warning_message, path_spec=path_spec)\n\n      if compressed_stream_path_spec:\n        event_source = event_sources.FileEntryEventSource(\n            path_spec=compressed_stream_path_spec)\n        event_source.file_entry_type = dfvfs_definitions.FILE_ENTRY_TYPE_FILE\n        mediator.ProduceEventSource(event_source)\n\n        self.last_activity_timestamp = time.time()", "response": "Processes a data stream containing compressed stream types such as bz2 or gzip."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprocess a directory file entry.", "response": "def _ProcessDirectory(self, mediator, file_entry):\n    \"\"\"Processes a directory file entry.\n\n    Args:\n      mediator (ParserMediator): mediates the interactions between\n          parsers and other components, such as storage and abort signals.\n      file_entry (dfvfs.FileEntry): file entry of the directory.\n    \"\"\"\n    self.processing_status = definitions.STATUS_INDICATOR_COLLECTING\n\n    if self._processing_profiler:\n      self._processing_profiler.StartTiming('collecting')\n\n    for sub_file_entry in file_entry.sub_file_entries:\n      if self._abort:\n        break\n\n      try:\n        if not sub_file_entry.IsAllocated():\n          continue\n\n      except dfvfs_errors.BackEndError as exception:\n        warning_message = (\n            'unable to process directory entry: {0:s} with error: '\n            '{1!s}').format(sub_file_entry.name, exception)\n        mediator.ProduceExtractionWarning(\n            warning_message, path_spec=file_entry.path_spec)\n        continue\n\n      # For TSK-based file entries only, ignore the virtual /$OrphanFiles\n      # directory.\n      if sub_file_entry.type_indicator == dfvfs_definitions.TYPE_INDICATOR_TSK:\n        if file_entry.IsRoot() and sub_file_entry.name == '$OrphanFiles':\n          continue\n\n      event_source = event_sources.FileEntryEventSource(\n          path_spec=sub_file_entry.path_spec)\n\n      # TODO: move this into a dfVFS file entry property.\n      stat_object = sub_file_entry.GetStat()\n      if stat_object:\n        event_source.file_entry_type = stat_object.type\n\n      mediator.ProduceEventSource(event_source)\n\n      self.last_activity_timestamp = time.time()\n\n    if self._processing_profiler:\n      self._processing_profiler.StopTiming('collecting')\n\n    self.processing_status = definitions.STATUS_INDICATOR_RUNNING"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprocesses a file entry.", "response": "def _ProcessFileEntry(self, mediator, file_entry):\n    \"\"\"Processes a file entry.\n\n    Args:\n      mediator (ParserMediator): mediates the interactions between\n          parsers and other components, such as storage and abort signals.\n      file_entry (dfvfs.FileEntry): file entry.\n    \"\"\"\n    display_name = mediator.GetDisplayName()\n    logger.debug(\n        '[ProcessFileEntry] processing file entry: {0:s}'.format(display_name))\n\n    reference_count = mediator.resolver_context.GetFileObjectReferenceCount(\n        file_entry.path_spec)\n\n    try:\n      if self._IsMetadataFile(file_entry):\n        self._ProcessMetadataFile(mediator, file_entry)\n\n      else:\n        file_entry_processed = False\n        for data_stream in file_entry.data_streams:\n          if self._abort:\n            break\n\n          if self._CanSkipDataStream(file_entry, data_stream):\n            logger.debug((\n                '[ProcessFileEntry] Skipping datastream {0:s} for {1:s}: '\n                '{2:s}').format(\n                    data_stream.name, file_entry.type_indicator, display_name))\n            continue\n\n          self._ProcessFileEntryDataStream(mediator, file_entry, data_stream)\n\n          file_entry_processed = True\n\n        if not file_entry_processed:\n          # For when the file entry does not contain a data stream.\n          self._ProcessFileEntryDataStream(mediator, file_entry, None)\n\n    finally:\n      new_reference_count = (\n          mediator.resolver_context.GetFileObjectReferenceCount(\n              file_entry.path_spec))\n      if reference_count != new_reference_count:\n        # Clean up after parsers that do not call close explicitly.\n        if mediator.resolver_context.ForceRemoveFileObject(\n            file_entry.path_spec):\n          logger.warning(\n              'File-object not explicitly closed for file: {0:s}'.format(\n                  display_name))\n\n    logger.debug(\n        '[ProcessFileEntry] done processing file entry: {0:s}'.format(\n            display_name))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprocesses a specific data stream of a file entry.", "response": "def _ProcessFileEntryDataStream(self, mediator, file_entry, data_stream):\n    \"\"\"Processes a specific data stream of a file entry.\n\n    Args:\n      mediator (ParserMediator): mediates the interactions between\n          parsers and other components, such as storage and abort signals.\n      file_entry (dfvfs.FileEntry): file entry containing the data stream.\n      data_stream (dfvfs.DataStream): data stream or None if the file entry\n          has no data stream.\n    \"\"\"\n    display_name = mediator.GetDisplayName()\n    data_stream_name = getattr(data_stream, 'name', '') or ''\n    logger.debug((\n        '[ProcessFileEntryDataStream] processing data stream: \"{0:s}\" of '\n        'file entry: {1:s}').format(data_stream_name, display_name))\n\n    mediator.ClearEventAttributes()\n\n    if data_stream and self._analyzers:\n      # Since AnalyzeDataStream generates event attributes it needs to be\n      # called before producing events.\n      self._AnalyzeDataStream(mediator, file_entry, data_stream.name)\n\n    self._ExtractMetadataFromFileEntry(mediator, file_entry, data_stream)\n\n    # Not every file entry has a data stream. In such cases we want to\n    # extract the metadata only.\n    if not data_stream:\n      return\n\n    # Determine if the content of the file entry should not be extracted.\n    skip_content_extraction = self._CanSkipContentExtraction(file_entry)\n    if skip_content_extraction:\n      display_name = mediator.GetDisplayName()\n      logger.debug(\n          'Skipping content extraction of: {0:s}'.format(display_name))\n      self.processing_status = definitions.STATUS_INDICATOR_IDLE\n      return\n\n    path_spec = copy.deepcopy(file_entry.path_spec)\n    if data_stream and not data_stream.IsDefault():\n      path_spec.data_stream = data_stream.name\n\n    archive_types = []\n    compressed_stream_types = []\n\n    if self._process_compressed_streams:\n      compressed_stream_types = self._GetCompressedStreamTypes(\n          mediator, path_spec)\n\n    if not compressed_stream_types:\n      archive_types = self._GetArchiveTypes(mediator, path_spec)\n\n    if archive_types:\n      if self._process_archives:\n        self._ProcessArchiveTypes(mediator, path_spec, archive_types)\n\n      if dfvfs_definitions.TYPE_INDICATOR_ZIP in archive_types:\n        # ZIP files are the base of certain file formats like docx.\n        self._ExtractContentFromDataStream(\n            mediator, file_entry, data_stream.name)\n\n    elif compressed_stream_types:\n      self._ProcessCompressedStreamTypes(\n          mediator, path_spec, compressed_stream_types)\n\n    else:\n      self._ExtractContentFromDataStream(\n          mediator, file_entry, data_stream.name)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _ProcessMetadataFile(self, mediator, file_entry):\n    self.processing_status = definitions.STATUS_INDICATOR_EXTRACTING\n\n    self._event_extractor.ParseFileEntryMetadata(mediator, file_entry)\n    for data_stream in file_entry.data_streams:\n      if self._abort:\n        break\n      self.last_activity_timestamp = time.time()\n\n      self._event_extractor.ParseMetadataFile(\n          mediator, file_entry, data_stream.name)", "response": "Processes a metadata file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _SetHashers(self, hasher_names_string):\n    if not hasher_names_string or hasher_names_string == 'none':\n      return\n\n    analyzer_object = analyzers_manager.AnalyzersManager.GetAnalyzerInstance(\n        'hashing')\n    analyzer_object.SetHasherNames(hasher_names_string)\n    self._analyzers.append(analyzer_object)", "response": "Sets the hasher names."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the Yara rules.", "response": "def _SetYaraRules(self, yara_rules_string):\n    \"\"\"Sets the Yara rules.\n\n    Args:\n      yara_rules_string (str): unparsed Yara rule definitions.\n    \"\"\"\n    if not yara_rules_string:\n      return\n\n    analyzer_object = analyzers_manager.AnalyzersManager.GetAnalyzerInstance(\n        'yara')\n    analyzer_object.SetRules(yara_rules_string)\n    self._analyzers.append(analyzer_object)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ProcessPathSpec(self, mediator, path_spec):\n    self.last_activity_timestamp = time.time()\n    self.processing_status = definitions.STATUS_INDICATOR_RUNNING\n\n    file_entry = path_spec_resolver.Resolver.OpenFileEntry(\n        path_spec, resolver_context=mediator.resolver_context)\n\n    if file_entry is None:\n      display_name = mediator.GetDisplayNameForPathSpec(path_spec)\n      logger.warning(\n          'Unable to open file entry with path spec: {0:s}'.format(\n              display_name))\n      self.processing_status = definitions.STATUS_INDICATOR_IDLE\n      return\n\n    mediator.SetFileEntry(file_entry)\n\n    try:\n      if file_entry.IsDirectory():\n        self._ProcessDirectory(mediator, file_entry)\n      self._ProcessFileEntry(mediator, file_entry)\n\n    finally:\n      mediator.ResetFileEntry()\n\n      self.last_activity_timestamp = time.time()\n      self.processing_status = definitions.STATUS_INDICATOR_IDLE", "response": "Processes a path specification."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef SetExtractionConfiguration(self, configuration):\n    self._hasher_file_size_limit = configuration.hasher_file_size_limit\n    self._SetHashers(configuration.hasher_names_string)\n    self._process_archives = configuration.process_archives\n    self._process_compressed_streams = configuration.process_compressed_streams\n    self._SetYaraRules(configuration.yara_rules_string)", "response": "Sets the extraction configuration settings."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses a single log line and produce an event object.", "response": "def _ParseLogLine(self, parser_mediator, structure, key):\n    \"\"\"Parse a single log line and produce an event object.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      key (str): identifier of the structure of tokens.\n      structure (pyparsing.ParseResults): structure of tokens derived from\n          a line of a text file.\n    \"\"\"\n    time_elements_tuple = self._GetTimeElementsTuple(structure)\n\n    try:\n      date_time = dfdatetime_time_elements.TimeElements(\n          time_elements_tuple=time_elements_tuple)\n    except ValueError:\n      parser_mediator.ProduceExtractionWarning(\n          'invalid date time value: {0!s}'.format(structure.date_time))\n      return\n\n    self._last_month = time_elements_tuple[1]\n\n    # If the actual entry is a repeated entry, we take the basic information\n    # from the previous entry, but use the timestamp from the actual entry.\n    if key == 'logline':\n      self._previous_structure = structure\n    else:\n      structure = self._previous_structure\n\n    event_data = MacAppFirewallLogEventData()\n    event_data.action = structure.action\n    event_data.agent = structure.agent\n    event_data.computer_name = structure.computer_name\n    # Due to the use of CharsNotIn pyparsing structure contains whitespaces\n    # that need to be removed.\n    event_data.process_name = structure.process_name.strip()\n    event_data.status = structure.status\n\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_ADDED)\n    parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nverifying that this file is a Mac AppFirewall log file.", "response": "def VerifyStructure(self, parser_mediator, line):\n    \"\"\"Verify that this file is a Mac AppFirewall log file.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      line (str): line from a text file.\n\n    Returns:\n      bool: True if the line is in the expected format, False if not.\n    \"\"\"\n    self._last_month = 0\n    self._year_use = parser_mediator.GetEstimatedYear()\n\n    try:\n      structure = self.FIREWALL_LINE.parseString(line)\n    except pyparsing.ParseException as exception:\n      logger.debug((\n          'Unable to parse file as a Mac AppFirewall log file with error: '\n          '{0!s}').format(exception))\n      return False\n\n    if structure.action != 'creating /var/log/appfirewall.log':\n      logger.debug(\n          'Not a Mac AppFirewall log file, invalid action: {0!s}'.format(\n              structure.action))\n      return False\n\n    if structure.status != 'Error':\n      logger.debug(\n          'Not a Mac AppFirewall log file, invalid status: {0!s}'.format(\n              structure.status))\n      return False\n\n    time_elements_tuple = self._GetTimeElementsTuple(structure)\n\n    try:\n      dfdatetime_time_elements.TimeElements(\n          time_elements_tuple=time_elements_tuple)\n    except ValueError:\n      logger.debug((\n          'Not a Mac AppFirewall log file, invalid date and time: '\n          '{0!s}').format(structure.date_time))\n      return False\n\n    self._last_month = time_elements_tuple[1]\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndetermine if a parser can process a file entry.", "response": "def _CheckParserCanProcessFileEntry(self, parser, file_entry):\n    \"\"\"Determines if a parser can process a file entry.\n\n    Args:\n      file_entry (dfvfs.FileEntry): file entry.\n      parser (BaseParser): parser.\n\n    Returns:\n      bool: True if the file entry can be processed by the parser object.\n    \"\"\"\n    for filter_object in parser.FILTERS:\n      if filter_object.Match(file_entry):\n        return True\n\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndetermine if a file - like object matches one of the known signatures.", "response": "def _GetSignatureMatchParserNames(self, file_object):\n    \"\"\"Determines if a file-like object matches one of the known signatures.\n\n    Args:\n      file_object (file): file-like object whose contents will be checked\n          for known signatures.\n\n    Returns:\n      list[str]: parser names for which the contents of the file-like object\n          matches their known signatures.\n    \"\"\"\n    parser_names = []\n    scan_state = pysigscan.scan_state()\n    self._file_scanner.scan_file_object(scan_state, file_object)\n\n    for scan_result in iter(scan_state.scan_results):\n      format_specification = (\n          self._formats_with_signatures.GetSpecificationBySignature(\n              scan_result.identifier))\n\n      if format_specification.identifier not in parser_names:\n        parser_names.append(format_specification.identifier)\n\n    return parser_names"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninitialize the parser objects.", "response": "def _InitializeParserObjects(self, parser_filter_expression=None):\n    \"\"\"Initializes the parser objects.\n\n    Args:\n      parser_filter_expression (Optional[str]): the parser filter expression,\n          None represents all parsers and plugins.\n\n          The parser filter expression is a comma separated value string that\n          denotes a list of parser names to include and/or exclude. Each entry\n          can have the value of:\n\n          * An exact match of a list of parsers, or a preset (see\n            data/presets.yaml for the list of predefined presets).\n          * A name of a single parser (case insensitive), e.g. msiecf.\n          * A glob name for a single parser, e.g. '*msie*' (case insensitive).\n    \"\"\"\n    self._formats_with_signatures, non_sigscan_parser_names = (\n        parsers_manager.ParsersManager.GetFormatsWithSignatures(\n            parser_filter_expression=parser_filter_expression))\n\n    self._non_sigscan_parser_names = []\n    for parser_name in non_sigscan_parser_names:\n      if parser_name not in ('filestat', 'usnjrnl'):\n        self._non_sigscan_parser_names.append(parser_name)\n\n    self._file_scanner = parsers_manager.ParsersManager.CreateSignatureScanner(\n        self._formats_with_signatures)\n\n    self._parsers = parsers_manager.ParsersManager.GetParserObjects(\n        parser_filter_expression=parser_filter_expression)\n\n    active_parser_names = ', '.join(sorted(self._parsers.keys()))\n    logger.debug('Active parsers: {0:s}'.format(active_parser_names))\n\n    self._filestat_parser = self._parsers.get('filestat', None)\n    if 'filestat' in self._parsers:\n      del self._parsers['filestat']\n\n    self._mft_parser = self._parsers.get('mft', None)\n\n    self._usnjrnl_parser = self._parsers.get('usnjrnl', None)\n    if 'usnjrnl' in self._parsers:\n      del self._parsers['usnjrnl']"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _ParseDataStreamWithParser(\n      self, parser_mediator, parser, file_entry, data_stream_name):\n    \"\"\"Parses a data stream of a file entry with a specific parser.\n\n    Args:\n      parser_mediator (ParserMediator): parser mediator.\n      parser (BaseParser): parser.\n      file_entry (dfvfs.FileEntry): file entry.\n      data_stream_name (str): data stream name.\n\n    Raises:\n      RuntimeError: if the file-like object is missing.\n    \"\"\"\n    file_object = file_entry.GetFileObject(data_stream_name=data_stream_name)\n    if not file_object:\n      raise RuntimeError(\n          'Unable to retrieve file-like object from file entry.')\n\n    try:\n      self._ParseFileEntryWithParser(\n          parser_mediator, parser, file_entry, file_object=file_object)\n\n    finally:\n      file_object.close()", "response": "Parses a data stream of a file entry with a specific parser."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _ParseFileEntryWithParser(\n      self, parser_mediator, parser, file_entry, file_object=None):\n    \"\"\"Parses a file entry with a specific parser.\n\n    Args:\n      parser_mediator (ParserMediator): parser mediator.\n      parser (BaseParser): parser.\n      file_entry (dfvfs.FileEntry): file entry.\n      file_object (Optional[file]): file-like object to parse.\n          If not set the parser will use the parser mediator to open\n          the file entry's default data stream as a file-like object.\n\n    Returns:\n      int: parse result which is _PARSE_RESULT_FAILURE if the file entry\n          could not be parsed, _PARSE_RESULT_SUCCESS if the file entry\n          successfully was parsed or _PARSE_RESULT_UNSUPPORTED when\n          UnableToParseFile was raised.\n\n    Raises:\n      TypeError: if parser object is not a supported parser type.\n    \"\"\"\n    if not isinstance(parser, (\n        parsers_interface.FileEntryParser, parsers_interface.FileObjectParser)):\n      raise TypeError('Unsupported parser object type.')\n\n    parser_mediator.ClearParserChain()\n\n    reference_count = (\n        parser_mediator.resolver_context.GetFileObjectReferenceCount(\n            file_entry.path_spec))\n\n    parser_mediator.SampleStartTiming(parser.NAME)\n\n    try:\n      if isinstance(parser, parsers_interface.FileEntryParser):\n        parser.Parse(parser_mediator)\n      elif isinstance(parser, parsers_interface.FileObjectParser):\n        parser.Parse(parser_mediator, file_object)\n      result = self._PARSE_RESULT_SUCCESS\n\n    # We catch IOError so we can determine the parser that generated the error.\n    except (IOError, dfvfs_errors.BackEndError) as exception:\n      display_name = parser_mediator.GetDisplayName(file_entry)\n      logger.warning(\n          '{0:s} unable to parse file: {1:s} with error: {2!s}'.format(\n              parser.NAME, display_name, exception))\n      result = self._PARSE_RESULT_FAILURE\n\n    except errors.UnableToParseFile as exception:\n      display_name = parser_mediator.GetDisplayName(file_entry)\n      logger.debug(\n          '{0:s} unable to parse file: {1:s} with error: {2!s}'.format(\n              parser.NAME, display_name, exception))\n      result = self._PARSE_RESULT_UNSUPPORTED\n\n    finally:\n      parser_mediator.SampleStopTiming(parser.NAME)\n      parser_mediator.SampleMemoryUsage(parser.NAME)\n\n      new_reference_count = (\n          parser_mediator.resolver_context.GetFileObjectReferenceCount(\n              file_entry.path_spec))\n      if reference_count != new_reference_count:\n        display_name = parser_mediator.GetDisplayName(file_entry)\n        logger.warning((\n            '[{0:s}] did not explicitly close file-object for file: '\n            '{1:s}.').format(parser.NAME, display_name))\n\n    return result", "response": "Parses a file entry with a specific parser."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing a file entry with a specific parsers.", "response": "def _ParseFileEntryWithParsers(\n      self, parser_mediator, parser_names, file_entry, file_object=None):\n    \"\"\"Parses a file entry with a specific parsers.\n\n    Args:\n      parser_mediator (ParserMediator): parser mediator.\n      parser_names (list[str]): names of parsers.\n      file_entry (dfvfs.FileEntry): file entry.\n      file_object (Optional[file]): file-like object to parse.\n          If not set the parser will use the parser mediator to open\n          the file entry's default data stream as a file-like object.\n\n    Returns:\n      int: parse result which is _PARSE_RESULT_FAILURE if the file entry\n          could not be parsed, _PARSE_RESULT_SUCCESS if the file entry\n          successfully was parsed or _PARSE_RESULT_UNSUPPORTED when\n          UnableToParseFile was raised or no names of parser were provided.\n\n    Raises:\n      RuntimeError: if the parser object is missing.\n    \"\"\"\n    parse_results = self._PARSE_RESULT_UNSUPPORTED\n    for parser_name in parser_names:\n      parser = self._parsers.get(parser_name, None)\n      if not parser:\n        raise RuntimeError(\n            'Parser object missing for parser: {0:s}'.format(parser_name))\n\n      if parser.FILTERS:\n        if not self._CheckParserCanProcessFileEntry(parser, file_entry):\n          parse_results = self._PARSE_RESULT_SUCCESS\n          continue\n\n      display_name = parser_mediator.GetDisplayName(file_entry)\n      logger.debug((\n          '[ParseFileEntryWithParsers] parsing file: {0:s} with parser: '\n          '{1:s}').format(display_name, parser_name))\n\n      parse_result = self._ParseFileEntryWithParser(\n          parser_mediator, parser, file_entry, file_object=file_object)\n\n      if parse_result == self._PARSE_RESULT_FAILURE:\n        return self._PARSE_RESULT_FAILURE\n\n      if parse_result == self._PARSE_RESULT_SUCCESS:\n        parse_results = self._PARSE_RESULT_SUCCESS\n\n    return parse_results"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse a data stream of a file entry.", "response": "def ParseDataStream(self, parser_mediator, file_entry, data_stream_name):\n    \"\"\"Parses a data stream of a file entry with the enabled parsers.\n\n    Args:\n      parser_mediator (ParserMediator): parser mediator.\n      file_entry (dfvfs.FileEntry): file entry.\n      data_stream_name (str): data stream name.\n\n    Raises:\n      RuntimeError: if the file-like object or the parser object is missing.\n    \"\"\"\n    file_object = file_entry.GetFileObject(data_stream_name=data_stream_name)\n    if not file_object:\n      raise RuntimeError(\n          'Unable to retrieve file-like object from file entry.')\n\n    try:\n      parser_names = self._GetSignatureMatchParserNames(file_object)\n\n      parse_with_non_sigscan_parsers = True\n      if parser_names:\n        parse_result = self._ParseFileEntryWithParsers(\n            parser_mediator, parser_names, file_entry, file_object=file_object)\n        if parse_result in (\n            self._PARSE_RESULT_FAILURE, self._PARSE_RESULT_SUCCESS):\n          parse_with_non_sigscan_parsers = False\n\n      if parse_with_non_sigscan_parsers:\n        self._ParseFileEntryWithParsers(\n            parser_mediator, self._non_sigscan_parser_names, file_entry,\n            file_object=file_object)\n\n    finally:\n      file_object.close()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse the file entry metadata e. g. file system data.", "response": "def ParseFileEntryMetadata(self, parser_mediator, file_entry):\n    \"\"\"Parses the file entry metadata e.g. file system data.\n\n    Args:\n      parser_mediator (ParserMediator): parser mediator.\n      file_entry (dfvfs.FileEntry): file entry.\n    \"\"\"\n    if self._filestat_parser:\n      self._ParseFileEntryWithParser(\n          parser_mediator, self._filestat_parser, file_entry)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ParseMetadataFile(\n      self, parser_mediator, file_entry, data_stream_name):\n    \"\"\"Parses a metadata file.\n\n    Args:\n      parser_mediator (ParserMediator): parser mediator.\n      file_entry (dfvfs.FileEntry): file entry.\n      data_stream_name (str): data stream name.\n    \"\"\"\n    parent_path_spec = getattr(file_entry.path_spec, 'parent', None)\n    filename_upper = file_entry.name.upper()\n    if (self._mft_parser and parent_path_spec and\n        filename_upper in ('$MFT', '$MFTMIRR') and not data_stream_name):\n      self._ParseDataStreamWithParser(\n          parser_mediator, self._mft_parser, file_entry, '')\n\n    elif (self._usnjrnl_parser and parent_path_spec and\n          filename_upper == '$USNJRNL' and data_stream_name == '$J'):\n      # To be able to ignore the sparse data ranges the UsnJrnl parser\n      # needs to read directly from the volume.\n      volume_file_object = path_spec_resolver.Resolver.OpenFileObject(\n          parent_path_spec, resolver_context=parser_mediator.resolver_context)\n\n      try:\n        self._ParseFileEntryWithParser(\n            parser_mediator, self._usnjrnl_parser, file_entry,\n            file_object=volume_file_object)\n      finally:\n        volume_file_object.close()", "response": "Parses a metadata file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate an MD5 hash value of the date and time values of a NTFS file entry.", "response": "def _CalculateNTFSTimeHash(self, file_entry):\n    \"\"\"Calculates an MD5 from the date and time value of a NTFS file entry.\n\n    Args:\n      file_entry (dfvfs.FileEntry): file entry.\n\n    Returns:\n      str: hexadecimal representation of the MD5 hash value of the date and\n          time values of the file entry.\n    \"\"\"\n    date_time_values = []\n\n    access_time = getattr(file_entry, 'access_time', None)\n    if access_time:\n      date_time_string = access_time.CopyToDateTimeString()\n      date_time_values.append('atime:{0:s}'.format(date_time_string))\n\n    creation_time = getattr(file_entry, 'creation_time', None)\n    if creation_time:\n      date_time_string = creation_time.CopyToDateTimeString()\n      date_time_values.append('crtime:{0:s}'.format(date_time_string))\n\n    modification_time = getattr(file_entry, 'modification_time', None)\n    if modification_time:\n      date_time_string = modification_time.CopyToDateTimeString()\n      date_time_values.append('mtime:{0:s}'.format(date_time_string))\n\n    # file_entry.change_time is an alias of file_entry.entry_modification_time.\n    change_time = getattr(file_entry, 'change_time', None)\n    if change_time:\n      date_time_string = change_time.CopyToDateTimeString()\n      date_time_values.append('ctime:{0:s}'.format(date_time_string))\n\n    date_time_values = ''.join(date_time_values)\n    date_time_values = date_time_values.encode('ascii')\n\n    hash_value = hashlib.md5()\n    hash_value.update(date_time_values)\n    return hash_value.hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _ExtractPathSpecs(\n      self, path_spec, find_specs=None, recurse_file_system=True,\n      resolver_context=None):\n    \"\"\"Extracts path specification from a specific source.\n\n    Args:\n      path_spec (dfvfs.PathSpec): path specification.\n      find_specs (Optional[list[dfvfs.FindSpec]]): find specifications\n          used in path specification extraction.\n      recurse_file_system (Optional[bool]): True if extraction should\n          recurse into a file system.\n      resolver_context (Optional[dfvfs.Context]): resolver context.\n\n    Yields:\n      dfvfs.PathSpec: path specification of a file entry found in the source.\n    \"\"\"\n    try:\n      file_entry = path_spec_resolver.Resolver.OpenFileEntry(\n          path_spec, resolver_context=resolver_context)\n    except (\n        dfvfs_errors.AccessError, dfvfs_errors.BackEndError,\n        dfvfs_errors.PathSpecError) as exception:\n      logger.error(\n          'Unable to open file entry with error: {0!s}'.format(exception))\n      return\n\n    if not file_entry:\n      logger.warning('Unable to open: {0:s}'.format(path_spec.comparable))\n      return\n\n    if (not file_entry.IsDirectory() and not file_entry.IsFile() and\n        not file_entry.IsDevice()):\n      logger.warning((\n          'Source path specification not a device, file or directory.\\n'\n          '{0:s}').format(path_spec.comparable))\n      return\n\n    if file_entry.IsFile():\n      yield path_spec\n\n    else:\n      for extracted_path_spec in self._ExtractPathSpecsFromFileSystem(\n          path_spec, find_specs=find_specs,\n          recurse_file_system=recurse_file_system,\n          resolver_context=resolver_context):\n        yield extracted_path_spec", "response": "Extracts a path specification from a specific source."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _ExtractPathSpecsFromDirectory(self, file_entry, depth=0):\n    if depth >= self._MAXIMUM_DEPTH:\n      raise errors.MaximumRecursionDepth('Maximum recursion depth reached.')\n\n    # Need to do a breadth-first search otherwise we'll hit the Python\n    # maximum recursion depth.\n    sub_directories = []\n\n    for sub_file_entry in file_entry.sub_file_entries:\n      try:\n        if not sub_file_entry.IsAllocated() or sub_file_entry.IsLink():\n          continue\n      except dfvfs_errors.BackEndError as exception:\n        logger.warning(\n            'Unable to process file: {0:s} with error: {1!s}'.format(\n                sub_file_entry.path_spec.comparable.replace(\n                    '\\n', ';'), exception))\n        continue\n\n      # For TSK-based file entries only, ignore the virtual /$OrphanFiles\n      # directory.\n      if sub_file_entry.type_indicator == dfvfs_definitions.TYPE_INDICATOR_TSK:\n        if file_entry.IsRoot() and sub_file_entry.name == '$OrphanFiles':\n          continue\n\n      if sub_file_entry.IsDirectory():\n        sub_directories.append(sub_file_entry)\n\n      elif sub_file_entry.IsFile():\n        # If we are dealing with a VSS we want to calculate a hash\n        # value based on available timestamps and compare that to previously\n        # calculated hash values, and only include the file into the queue if\n        # the hash does not match.\n        if self._duplicate_file_check:\n          hash_value = self._CalculateNTFSTimeHash(sub_file_entry)\n\n          inode = getattr(sub_file_entry.path_spec, 'inode', 0)\n          if inode in self._hashlist:\n            if hash_value in self._hashlist[inode]:\n              continue\n\n          self._hashlist.setdefault(inode, []).append(hash_value)\n\n      for path_spec in self._ExtractPathSpecsFromFile(sub_file_entry):\n        yield path_spec\n\n    for sub_file_entry in sub_directories:\n      try:\n        for path_spec in self._ExtractPathSpecsFromDirectory(\n            sub_file_entry, depth=(depth + 1)):\n          yield path_spec\n\n      except (\n          IOError, dfvfs_errors.AccessError, dfvfs_errors.BackEndError,\n          dfvfs_errors.PathSpecError) as exception:\n        logger.warning('{0!s}'.format(exception))", "response": "Extracts the path specification from a directory."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _ExtractPathSpecsFromFile(self, file_entry):\n    produced_main_path_spec = False\n    for data_stream in file_entry.data_streams:\n      # Make a copy so we don't make the changes on a path specification\n      # directly. Otherwise already produced path specifications can be\n      # altered in the process.\n      path_spec = copy.deepcopy(file_entry.path_spec)\n      if data_stream.name:\n        setattr(path_spec, 'data_stream', data_stream.name)\n      yield path_spec\n\n      if not data_stream.name:\n        produced_main_path_spec = True\n\n    if not produced_main_path_spec:\n      yield file_entry.path_spec", "response": "Extracts path specifications from a file entry."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _ExtractPathSpecsFromFileSystem(\n      self, path_spec, find_specs=None, recurse_file_system=True,\n      resolver_context=None):\n    \"\"\"Extracts path specification from a file system within a specific source.\n\n    Args:\n      path_spec (dfvfs.PathSpec): path specification of the root of\n          the file system.\n      find_specs (Optional[list[dfvfs.FindSpec]]): find specifications.\n      recurse_file_system (Optional[bool]): True if extraction should\n          recurse into a file system.\n      resolver_context (Optional[dfvfs.Context]): resolver context.\n\n    Yields:\n      dfvfs.PathSpec: path specification of a file entry found in\n          the file system.\n    \"\"\"\n    try:\n      file_system = path_spec_resolver.Resolver.OpenFileSystem(\n          path_spec, resolver_context=resolver_context)\n    except (\n        dfvfs_errors.AccessError, dfvfs_errors.BackEndError,\n        dfvfs_errors.PathSpecError) as exception:\n      logger.error(\n          'Unable to open file system with error: {0!s}'.format(exception))\n      return\n\n    try:\n      if find_specs:\n        searcher = file_system_searcher.FileSystemSearcher(\n            file_system, path_spec)\n        for extracted_path_spec in searcher.Find(find_specs=find_specs):\n          yield extracted_path_spec\n\n      elif recurse_file_system:\n        file_entry = file_system.GetFileEntryByPathSpec(path_spec)\n        if file_entry:\n          for extracted_path_spec in self._ExtractPathSpecsFromDirectory(\n              file_entry):\n            yield extracted_path_spec\n\n      else:\n        yield path_spec\n\n    except (\n        dfvfs_errors.AccessError, dfvfs_errors.BackEndError,\n        dfvfs_errors.PathSpecError) as exception:\n      logger.warning('{0!s}'.format(exception))\n\n    finally:\n      file_system.Close()", "response": "Extracts a path specification from a file system within a specific source."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nextracting a path specification from a specific source.", "response": "def ExtractPathSpecs(\n      self, path_specs, find_specs=None, recurse_file_system=True,\n      resolver_context=None):\n    \"\"\"Extracts path specification from a specific source.\n\n    Args:\n      path_specs (Optional[list[dfvfs.PathSpec]]): path specifications.\n      find_specs (Optional[list[dfvfs.FindSpec]]): find specifications.\n      recurse_file_system (Optional[bool]): True if extraction should\n          recurse into a file system.\n      resolver_context (Optional[dfvfs.Context]): resolver context.\n\n    Yields:\n      dfvfs.PathSpec: path specification of a file entry found in the source.\n    \"\"\"\n    for path_spec in path_specs:\n      for extracted_path_spec in self._ExtractPathSpecs(\n          path_spec, find_specs=find_specs,\n          recurse_file_system=recurse_file_system,\n          resolver_context=resolver_context):\n        yield extracted_path_spec"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving the property value as a Python object.", "response": "def _GetValueAsObject(self, property_value):\n    \"\"\"Retrieves the property value as a Python object.\n\n    Args:\n      property_value (pyolecf.property_value): OLECF property value.\n\n    Returns:\n      object: property value as a Python object.\n    \"\"\"\n    if property_value.type == pyolecf.value_types.BOOLEAN:\n      return property_value.data_as_boolean\n\n    if property_value.type in self._INTEGER_TYPES:\n      return property_value.data_as_integer\n\n    if property_value.type in self._STRING_TYPES:\n      return property_value.data_as_string\n\n    try:\n      data = property_value.data\n    except IOError:\n      data = None\n\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading properties from a property set.", "response": "def _ReadPropertySet(self, property_set):\n    \"\"\"Reads properties from a property set.\n\n    Args:\n      property_set (pyolecf.property_set): OLECF property set.\n    \"\"\"\n    # Combine the values of multiple property sections\n    # but do not override properties that are already set.\n    for property_section in property_set.sections:\n      if property_section.class_identifier != self._CLASS_IDENTIFIER:\n        continue\n\n      for property_value in property_section.properties:\n        property_name = self._PROPERTY_NAMES.get(\n            property_value.identifier, None)\n        if not property_name:\n          property_name = '0x{0:04}'.format(property_value.identifier)\n\n        value = self._GetValueAsObject(property_value)\n        if self._PROPERTY_VALUE_MAPPINGS:\n          value_callback_name = self._PROPERTY_VALUE_MAPPINGS.get(\n              property_name, None)\n          if value_callback_name:\n            value_callback_method = getattr(self, value_callback_name, None)\n            if value_callback_method:\n              value = value_callback_method(value)\n\n        if property_name in self._DATE_TIME_PROPERTIES:\n          properties_dict = self.date_time_properties\n          value = dfdatetime_filetime.Filetime(timestamp=value)\n        else:\n          properties_dict = self._properties\n\n        if property_name not in properties_dict:\n          properties_dict[property_name] = value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving the properties as event data.", "response": "def GetEventData(self, data_type):\n    \"\"\"Retrieves the properties as event data.\n\n    Args:\n      data_type (str): event data type.\n\n    Returns:\n      EventData: event data.\n    \"\"\"\n    event_data = events.EventData(data_type=data_type)\n    for property_name, property_value in iter(self._properties.items()):\n      if isinstance(property_value, py2to3.BYTES_TYPE):\n        property_value = repr(property_value)\n      setattr(event_data, property_name, property_value)\n\n    return event_data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse a document summary information OLECF item.", "response": "def Process(self, parser_mediator, root_item=None, **kwargs):\n    \"\"\"Parses a document summary information OLECF item.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      root_item (Optional[pyolecf.item]): root item of the OLECF file.\n\n    Raises:\n      ValueError: If the root item is not set.\n    \"\"\"\n    # This will raise if unhandled keyword arguments are passed.\n    super(DocumentSummaryInformationOLECFPlugin, self).Process(\n        parser_mediator, **kwargs)\n\n    if not root_item:\n      raise ValueError('Root item not set.')\n\n    root_creation_time, root_modification_time = self._GetTimestamps(root_item)\n\n    for item_name in self.REQUIRED_ITEMS:\n      item = root_item.get_sub_item_by_name(item_name)\n      if not item:\n        continue\n\n      summary_information = OLECFDocumentSummaryInformation(item)\n      event_data = summary_information.GetEventData(\n          data_type='olecf:document_summary_info')\n      event_data.name = 'Document Summary Information'\n\n      if root_creation_time:\n        date_time = dfdatetime_filetime.Filetime(\n            timestamp=root_creation_time)\n        event = OLECFDocumentSummaryInformationEvent(\n            date_time, definitions.TIME_DESCRIPTION_CREATION)\n        parser_mediator.ProduceEventWithEventData(event, event_data)\n\n      if root_modification_time:\n        date_time = dfdatetime_filetime.Filetime(\n            timestamp=root_modification_time)\n        event = OLECFDocumentSummaryInformationEvent(\n            date_time, definitions.TIME_DESCRIPTION_MODIFICATION)\n        parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef Process(self, parser_mediator, root_item=None, **kwargs):\n    # This will raise if unhandled keyword arguments are passed.\n    super(SummaryInformationOLECFPlugin, self).Process(\n        parser_mediator, **kwargs)\n\n    if not root_item:\n      raise ValueError('Root item not set.')\n\n    root_creation_time, root_modification_time = self._GetTimestamps(root_item)\n\n    for item_name in self.REQUIRED_ITEMS:\n      item = root_item.get_sub_item_by_name(item_name)\n      if not item:\n        continue\n\n      summary_information = OLECFSummaryInformation(item)\n      event_data = summary_information.GetEventData(\n          data_type='olecf:summary_info')\n      event_data.name = 'Summary Information'\n\n      for property_name, date_time in iter(\n          summary_information.date_time_properties.items()):\n        date_time_description = self._DATE_TIME_DESCRIPTIONS.get(\n            property_name, definitions.TIME_DESCRIPTION_UNKNOWN)\n        event = OLECFSummaryInformationEvent(date_time, date_time_description)\n        parser_mediator.ProduceEventWithEventData(event, event_data)\n\n      if root_creation_time:\n        date_time = dfdatetime_filetime.Filetime(\n            timestamp=root_creation_time)\n        event = OLECFSummaryInformationEvent(\n            date_time, definitions.TIME_DESCRIPTION_CREATION)\n        parser_mediator.ProduceEventWithEventData(event, event_data)\n\n      if root_modification_time:\n        date_time = dfdatetime_filetime.Filetime(\n            timestamp=root_modification_time)\n        event = OLECFSummaryInformationEvent(\n            date_time, definitions.TIME_DESCRIPTION_MODIFICATION)\n        parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Parses a summary information OLECF item."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef Main():\n  argument_parser = argparse.ArgumentParser(description=(\n      'Plots memory usage from profiling data.'))\n\n  argument_parser.add_argument(\n      '--output', dest='output_file', type=str, help=(\n          'path of the output file to write the graph to instead of using '\n          'interactive mode. The output format deduced from the extension '\n          'of the filename.'))\n\n  argument_parser.add_argument(\n      'profile_path', type=str, help=(\n          'path to the directory containing the profiling data.'))\n\n  options = argument_parser.parse_args()\n\n  if not os.path.isdir(options.profile_path):\n    print('No such directory: {0:s}'.format(options.profile_path))\n    return False\n\n  names = ['time', 'queued', 'processing', 'to_merge', 'abandoned', 'total']\n\n  glob_expression = os.path.join(options.profile_path, 'task_queue-*.csv.gz')\n  for csv_file_name in glob.glob(glob_expression):\n    data = numpy.genfromtxt(\n        csv_file_name, delimiter='\\t', dtype=None, encoding='utf-8',\n        names=names, skip_header=1)\n\n    pyplot.plot(data['time'], data['queued'], label='queued')\n    pyplot.plot(data['time'], data['processing'], label='processing')\n    pyplot.plot(data['time'], data['to_merge'], label='to merge')\n    pyplot.plot(data['time'], data['abandoned'], label='abandoned')\n\n  pyplot.title('Number of tasks over time')\n\n  pyplot.xlabel('Time')\n  pyplot.xscale('linear')\n\n  pyplot.ylabel('Number of tasks')\n  pyplot.yscale('linear')\n\n  pyplot.legend()\n\n  if options.output_file:\n    pyplot.savefig(options.output_file)\n  else:\n    pyplot.show()\n\n  return True", "response": "The main function of the\n formula."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if a Windows Registry key path is compatible by dfWinReg.", "response": "def CheckKeyCompatibility(cls, key_path):\n    \"\"\"Checks if a Windows Registry key path is supported by dfWinReg.\n\n    Args:\n      key_path (str): path of the Windows Registry key.\n\n    Returns:\n      bool: True if key is compatible or False if not.\n    \"\"\"\n    key_path_upper = key_path.upper()\n    for key_path_prefix in cls._COMPATIBLE_REGISTRY_KEY_PATH_PREFIXES:\n      if key_path_upper.startswith(key_path_prefix):\n        return True\n\n    logger.warning('Key path: \"{0:s}\" is currently not supported'.format(\n        key_path))\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef BuildFindSpecs(self, artifact_filter_names, environment_variables=None):\n    find_specs = []\n    for name in artifact_filter_names:\n      definition = self._artifacts_registry.GetDefinitionByName(name)\n      if not definition:\n        logger.debug('undefined artifact definition: {0:s}'.format(name))\n        continue\n\n      logger.debug('building find spec from artifact definition: {0:s}'.format(\n          name))\n      artifact_find_specs = self._BuildFindSpecsFromArtifact(\n          definition, environment_variables)\n      find_specs.extend(artifact_find_specs)\n\n    for find_spec in find_specs:\n      if isinstance(find_spec, file_system_searcher.FindSpec):\n        self.file_system_find_specs.append(find_spec)\n\n      elif isinstance(find_spec, registry_searcher.FindSpec):\n        self.registry_find_specs.append(find_spec)\n\n      else:\n        logger.warning('Unsupported find specification type: {0:s}'.format(\n            type(find_spec)))", "response": "Builds find specifications from artifact definitions."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _BuildFindSpecsFromArtifact(self, definition, environment_variables):\n    find_specs = []\n    for source in definition.sources:\n      if source.type_indicator == artifact_types.TYPE_INDICATOR_FILE:\n        for path_entry in set(source.paths):\n          specifications = self._BuildFindSpecsFromFileSourcePath(\n              path_entry, source.separator, environment_variables,\n              self._knowledge_base.user_accounts)\n          find_specs.extend(specifications)\n          self.file_system_artifact_names.add(definition.name)\n\n      elif (source.type_indicator ==\n            artifact_types.TYPE_INDICATOR_WINDOWS_REGISTRY_KEY):\n        for key_path in set(source.keys):\n          if ArtifactDefinitionsFilterHelper.CheckKeyCompatibility(key_path):\n            specifications = self._BuildFindSpecsFromRegistrySourceKey(key_path)\n            find_specs.extend(specifications)\n            self.registry_artifact_names.add(definition.name)\n\n      elif (source.type_indicator ==\n            artifact_types.TYPE_INDICATOR_WINDOWS_REGISTRY_VALUE):\n        # TODO: Handle Registry Values Once Supported in dfwinreg.\n        # https://github.com/log2timeline/dfwinreg/issues/98\n\n        # Use set-comprehension to create a set of the source key paths.\n        key_paths = {\n            key_value['key'] for key_value in source.key_value_pairs}\n        key_paths_string = ', '.join(key_paths)\n\n        logger.warning((\n            'Windows Registry values are not supported, extracting keys: '\n            '\"{0!s}\"').format(key_paths_string))\n\n        for key_path in key_paths:\n          if ArtifactDefinitionsFilterHelper.CheckKeyCompatibility(key_path):\n            specifications = self._BuildFindSpecsFromRegistrySourceKey(key_path)\n            find_specs.extend(specifications)\n            self.registry_artifact_names.add(definition.name)\n\n      elif (source.type_indicator ==\n            artifact_types.TYPE_INDICATOR_ARTIFACT_GROUP):\n        for name in source.names:\n          specifications = self._BuildFindSpecsFromGroupName(\n              name, environment_variables)\n          find_specs.extend(specifications)\n\n      else:\n        logger.warning(\n            'Unsupported artifact definition source type: \"{0:s}\"'.format(\n                source.type_indicator))\n\n    return find_specs", "response": "Builds find specifications from an artifact definition."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _BuildFindSpecsFromGroupName(self, group_name, environment_variables):\n    definition = self._artifacts_registry.GetDefinitionByName(group_name)\n    if not definition:\n      return None\n\n    return self._BuildFindSpecsFromArtifact(definition, environment_variables)", "response": "Builds find specifications from a group name."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _BuildFindSpecsFromFileSourcePath(\n      self, source_path, path_separator, environment_variables, user_accounts):\n    \"\"\"Builds find specifications from a file source type.\n\n    Args:\n      source_path (str): file system path defined by the source.\n      path_separator (str): file system path segment separator.\n      environment_variables (list[str]): environment variable attributes used to\n          dynamically populate environment variables in key.\n      user_accounts (list[str]): identified user accounts stored in the\n          knowledge base.\n\n    Returns:\n      list[dfvfs.FindSpec]: find specifications for the file source type.\n    \"\"\"\n    find_specs = []\n    for path_glob in path_helper.PathHelper.ExpandRecursiveGlobs(\n        source_path, path_separator):\n      logger.debug('building find spec from path glob: {0:s}'.format(\n          path_glob))\n\n      for path in path_helper.PathHelper.ExpandUsersVariablePath(\n          path_glob, path_separator, user_accounts):\n        logger.debug('building find spec from path: {0:s}'.format(path))\n\n        if '%' in path:\n          path = path_helper.PathHelper.ExpandWindowsPath(\n              path, environment_variables)\n          logger.debug('building find spec from expanded path: {0:s}'.format(\n              path))\n\n        if not path.startswith(path_separator):\n          logger.warning((\n              'The path filter must be defined as an absolute path: '\n              '\"{0:s}\"').format(path))\n          continue\n\n        # Convert the path filters into a list of path segments and\n        # strip the root path segment.\n        path_segments = path.split(path_separator)\n\n        # Remove initial root entry\n        path_segments.pop(0)\n\n        if not path_segments[-1]:\n          logger.warning(\n              'Empty last path segment in path filter: \"{0:s}\"'.format(path))\n          path_segments.pop(-1)\n\n        try:\n          find_spec = file_system_searcher.FindSpec(\n              location_glob=path_segments, case_sensitive=False)\n        except ValueError as exception:\n          logger.error((\n              'Unable to build find specification for path: \"{0:s}\" with '\n              'error: {1!s}').format(path, exception))\n          continue\n\n        find_specs.append(find_spec)\n\n    return find_specs", "response": "Builds a list of find specifications from a file source path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _BuildFindSpecsFromRegistrySourceKey(self, key_path):\n    find_specs = []\n    for key_path_glob in path_helper.PathHelper.ExpandRecursiveGlobs(\n        key_path, '\\\\'):\n      logger.debug('building find spec from key path glob: {0:s}'.format(\n          key_path_glob))\n\n      key_path_glob_upper = key_path_glob.upper()\n      if key_path_glob_upper.startswith('HKEY_USERS\\\\%%USERS.SID%%'):\n        key_path_glob = 'HKEY_CURRENT_USER{0:s}'.format(key_path_glob[26:])\n\n      find_spec = registry_searcher.FindSpec(key_path_glob=key_path_glob)\n      find_specs.append(find_spec)\n\n    return find_specs", "response": "Builds find specifications from a Windows Registry source key."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nretrieving the page for the extension from the Chrome store website.", "response": "def _GetChromeWebStorePage(self, extension_identifier):\n    \"\"\"Retrieves the page for the extension from the Chrome store website.\n\n    Args:\n      extension_identifier (str): Chrome extension identifier.\n\n    Returns:\n      str: page content or None.\n    \"\"\"\n    web_store_url = self._WEB_STORE_URL.format(xid=extension_identifier)\n    try:\n      response = requests.get(web_store_url)\n\n    except (requests.ConnectionError, requests.HTTPError) as exception:\n      logger.warning((\n          '[{0:s}] unable to retrieve URL: {1:s} with error: {2!s}').format(\n              self.NAME, web_store_url, exception))\n      return None\n\n    return response.text"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngive a path give back the path segment separator as a best guess.", "response": "def _GetPathSegmentSeparator(self, path):\n    \"\"\"Given a path give back the path separator as a best guess.\n\n    Args:\n      path (str): path.\n\n    Returns:\n      str: path segment separator.\n    \"\"\"\n    if path.startswith('\\\\') or path[1:].startswith(':\\\\'):\n      return '\\\\'\n\n    if path.startswith('/'):\n      return '/'\n\n    if '/' and '\\\\' in path:\n      # Let's count slashes and guess which one is the right one.\n      forward_count = len(path.split('/'))\n      backward_count = len(path.split('\\\\'))\n\n      if forward_count > backward_count:\n        return '/'\n\n      return '\\\\'\n\n    # Now we are sure there is only one type of separators yet\n    # the path does not start with one.\n    if '/' in path:\n      return '/'\n\n    return '\\\\'"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _GetTitleFromChromeWebStore(self, extension_identifier):\n    # Check if we have already looked this extension up.\n    if extension_identifier in self._extensions:\n      return self._extensions.get(extension_identifier)\n\n    page_content = self._GetChromeWebStorePage(extension_identifier)\n    if not page_content:\n      logger.warning(\n          '[{0:s}] no data returned for extension identifier: {1:s}'.format(\n              self.NAME, extension_identifier))\n      return None\n\n    first_line, _, _ = page_content.partition('\\n')\n    match = self._TITLE_RE.search(first_line)\n    name = None\n    if match:\n      title = match.group(1)\n      if title.startswith('Chrome Web Store - '):\n        name = title[19:]\n      elif title.endswith('- Chrome Web Store'):\n        name = title[:-19]\n\n    if not name:\n      self._extensions[extension_identifier] = 'UNKNOWN'\n      return None\n\n    self._extensions[extension_identifier] = name\n    return name", "response": "Retrieves the name of the extension from the Chrome store website."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompile an analysis report.", "response": "def CompileReport(self, mediator):\n    \"\"\"Compiles an analysis report.\n\n    Args:\n      mediator (AnalysisMediator): mediates interactions between analysis\n          plugins and other components, such as storage and dfvfs.\n\n    Returns:\n      AnalysisReport: analysis report.\n    \"\"\"\n    lines_of_text = []\n    for user, extensions in sorted(self._results.items()):\n      lines_of_text.append(' == USER: {0:s} =='.format(user))\n      for extension, extension_identifier in sorted(extensions):\n        lines_of_text.append('  {0:s} [{1:s}]'.format(\n            extension, extension_identifier))\n      lines_of_text.append('')\n\n    lines_of_text.append('')\n    report_text = '\\n'.join(lines_of_text)\n    analysis_report = reports.AnalysisReport(\n        plugin_name=self.NAME, text=report_text)\n    analysis_report.report_dict = self._results\n    return analysis_report"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ExamineEvent(self, mediator, event):\n    # Only interested in filesystem events.\n    if event.data_type != 'fs:stat':\n      return\n\n    filename = getattr(event, 'filename', None)\n    if not filename:\n      return\n\n    # Determine if we have a Chrome extension ID.\n    if 'chrome' not in filename.lower():\n      return\n\n    if not self._sep:\n      self._sep = self._GetPathSegmentSeparator(filename)\n\n    if '{0:s}Extensions{0:s}'.format(self._sep) not in filename:\n      return\n\n    # Now we have extension IDs, let's check if we've got the\n    # folder, nothing else.\n    paths = filename.split(self._sep)\n    if paths[-2] != 'Extensions':\n      return\n\n    extension_identifier = paths[-1]\n    if extension_identifier == 'Temp':\n      return\n\n    # Get the user and ID.\n    user = mediator.GetUsernameForPath(filename)\n\n    # We still want this information in here, so that we can\n    # manually deduce the username.\n    if not user:\n      if len(filename) > 25:\n        user = 'Not found ({0:s}...)'.format(filename[0:25])\n      else:\n        user = 'Not found ({0:s})'.format(filename)\n\n    extension_string = self._GetTitleFromChromeWebStore(extension_identifier)\n    if not extension_string:\n      extension_string = extension_identifier\n\n    self._results.setdefault(user, [])\n    if (extension_string, extension_identifier) not in self._results[user]:\n      self._results[user].append((extension_string, extension_identifier))", "response": "Analyzes an event and returns a list of the events that can be processed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _GetMessage(self, event_object):\n    # TODO: move this somewhere where the mediator can be instantiated once.\n    formatter_mediator = formatters_mediator.FormatterMediator()\n\n    result = ''\n    try:\n      result, _ = formatters_manager.FormattersManager.GetMessageStrings(\n          formatter_mediator, event_object)\n    except KeyError as exception:\n      logging.warning(\n          'Unable to correctly assemble event with error: {0!s}'.format(\n              exception))\n\n    return result", "response": "Returns a properly formatted message string."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _GetSources(self, event_object):\n    try:\n      source_short, source_long = (\n          formatters_manager.FormattersManager.GetSourceStrings(event_object))\n    except KeyError as exception:\n      logging.warning(\n          'Unable to correctly assemble event with error: {0!s}'.format(\n              exception))\n\n    return source_short, source_long", "response": "Returns properly formatted source strings."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef Compile(self, filter_implementation):\n    self.attribute = self.swap_source.get(self.attribute, self.attribute)\n    arguments = [self.attribute]\n    op_str = self.operator.lower()\n    operator = filter_implementation.OPS.get(op_str, None)\n\n    if not operator:\n      raise errors.ParseError('Unknown operator {0:s} provided.'.format(\n          self.operator))\n\n    # Plaso specific implementation - if we are comparing a timestamp\n    # to a value, we use our specific implementation that compares\n    # timestamps in a \"human readable\" format.\n    if self.attribute == 'timestamp':\n      args = []\n      for argument in self.args:\n        args.append(DateCompareObject(argument))\n      self.args = args\n\n    for argument in self.args:\n      if isinstance(argument, DateCompareObject):\n        if 'Less' in str(operator):\n          TimeRangeCache.SetUpperTimestamp(argument.data)\n        else:\n          TimeRangeCache.SetLowerTimestamp(argument.data)\n    arguments.extend(self.args)\n    expander = filter_implementation.FILTERS['ValueExpander']\n    ops = operator(arguments=arguments, value_expander=expander)\n    if not self.bool_value:\n      if hasattr(ops, 'FlipBool'):\n        ops.FlipBool()\n\n    return ops", "response": "Compiles the filter implementation."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef SetLowerTimestamp(cls, timestamp):\n    if not hasattr(cls, '_lower'):\n      cls._lower = timestamp\n      return\n\n    if timestamp < cls._lower:\n      cls._lower = timestamp", "response": "Sets the lower bound timestamp."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef SetUpperTimestamp(cls, timestamp):\n    if not hasattr(cls, '_upper'):\n      cls._upper = timestamp\n      return\n\n    if timestamp > cls._upper:\n      cls._upper = timestamp", "response": "Sets the upper bound timestamp."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the first and last timestamp of filter range.", "response": "def GetTimeRange(cls):\n    \"\"\"Return the first and last timestamp of filter range.\"\"\"\n    first = getattr(cls, '_lower', 0)\n    last = getattr(cls, '_upper', cls.MAX_INT64)\n\n    if first < last:\n      return first, last\n\n    return last, first"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _ParseLogLine(self, parser_mediator, structure):\n    try:\n      date_time = dfdatetime_time_elements.TimeElements(\n          time_elements_tuple=structure.date_time)\n      # TODO: check if date and time values are local time or in UTC.\n      date_time.is_local_time = True\n    except ValueError:\n      parser_mediator.ProduceExtractionWarning(\n          'invalid date time value: {0!s}'.format(structure.date_time))\n      return\n\n    event_data = SophosAVLogEventData()\n    event_data.text = structure.text\n\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_ADDED,\n        time_zone=parser_mediator.timezone)\n    parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Parses a log line."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nverifying that this file is a Sophos Anti - Virus log file.", "response": "def VerifyStructure(self, parser_mediator, line):\n    \"\"\"Verify that this file is a Sophos Anti-Virus log file.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfVFS.\n      line (str): line from a text file.\n\n    Returns:\n      bool: True if the line is in the expected format, False if not.\n    \"\"\"\n    try:\n      structure = self._LOG_LINE.parseString(line)\n    except pyparsing.ParseException:\n      logger.debug('Not a Sophos Anti-Virus log file')\n      return False\n\n    # Expect spaces at position 9 and 16.\n    if ' ' not in (line[8], line[15]):\n      logger.debug('Not a Sophos Anti-Virus log file')\n      return False\n\n    try:\n      dfdatetime_time_elements.TimeElements(\n          time_elements_tuple=structure.date_time)\n    except ValueError:\n      logger.debug((\n          'Not a Sophos Anti-Virus log file, invalid date and time: '\n          '{0!s}').format(structure.date_time))\n      return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _GetPropertyValue(self, parser_mediator, properties, property_name):\n    property_value = properties.get(property_name, None)\n    if isinstance(property_value, py2to3.BYTES_TYPE):\n      try:\n        # TODO: get encoding form XML metadata.\n        property_value = property_value.decode('utf-8')\n      except UnicodeDecodeError:\n        parser_mediator.ProduceExtractionWarning(\n            'unable to decode property: {0:s}'.format(property_name))\n\n    return property_value", "response": "Retrieves a property value."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nformatting a camel case property name as snake case.", "response": "def _FormatPropertyName(self, property_name):\n    \"\"\"Formats a camel case property name as snake case.\n\n    Args:\n      property_name (str): property name in camel case.\n\n    Returns:\n      str: property name in snake case.\n    \"\"\"\n    # TODO: Add Unicode support.\n    fix_key = re.sub(r'(.)([A-Z][a-z]+)', r'\\1_\\2', property_name)\n    return re.sub(r'([a-z0-9])([A-Z])', r'\\1_\\2', fix_key).lower()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses a properties XML file.", "response": "def _ParsePropertiesXMLFile(self, xml_data):\n    \"\"\"Parses a properties XML file.\n\n    Args:\n      xml_data (bytes): data of a _rels/.rels XML file.\n\n    Returns:\n      dict[str, object]: properties.\n\n    Raises:\n      zipfile.BadZipfile: if the properties XML file cannot be read.\n    \"\"\"\n    xml_root = ElementTree.fromstring(xml_data)\n\n    properties = {}\n    for xml_element in xml_root.iter():\n      if not xml_element.text:\n        continue\n\n      # The property name is formatted as: {URL}name\n      # For example: {http://purl.org/dc/terms/}modified\n      _, _, name = xml_element.tag.partition('}')\n\n      # Do not including the 'lpstr' attribute because it is very verbose.\n      if name == 'lpstr':\n        continue\n\n      property_name = self._PROPERTY_NAMES.get(name, None)\n      if not property_name:\n        property_name = self._FormatPropertyName(name)\n\n      properties[property_name] = xml_element.text\n\n    return properties"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _ParseRelationshipsXMLFile(self, xml_data):\n    xml_root = ElementTree.fromstring(xml_data)\n\n    property_files = []\n    for xml_element in xml_root.iter():\n      type_attribute = xml_element.get('Type')\n      if 'properties' in repr(type_attribute):\n        target_attribute = xml_element.get('Target')\n        property_files.append(target_attribute)\n\n    return property_files", "response": "Parses the relationships XML file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _ProduceEvent(\n      self, parser_mediator, event_data, properties, property_name,\n      timestamp_description, error_description):\n    \"\"\"Produces an event.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      event_data (OpenXMLEventData): event data.\n      properties (dict[str, object]): properties.\n      property_name (str): name of the date and time property.\n      timestamp_description (str): description of the meaning of the timestamp\n          value.\n      error_description (str): description of the meaning of the timestamp\n          value for error reporting purposes.\n    \"\"\"\n    time_string = properties.get(property_name, None)\n    if not time_string:\n      return\n\n    # Date and time strings are in ISO 8601 format either with 1 second\n    # or 100th nano second precision. For example:\n    # 2012-11-07T23:29:00Z\n    # 2012-03-05T20:40:00.0000000Z\n    date_time = dfdatetime_time_elements.TimeElements()\n\n    try:\n      date_time.CopyFromStringISO8601(time_string)\n\n      event = time_events.DateTimeValuesEvent(date_time, timestamp_description)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n    except ValueError as exception:\n      parser_mediator.ProduceExtractionWarning(\n          'unsupported {0:s}: {1:s} with error: {2!s}'.format(\n              error_description, time_string, exception))", "response": "Produce an event from the event data."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef InspectZipFile(self, parser_mediator, zip_file):\n    try:\n      xml_data = zip_file.read('_rels/.rels')\n      property_files = self._ParseRelationshipsXMLFile(xml_data)\n    except (IndexError, IOError, KeyError, OverflowError, ValueError,\n            zipfile.BadZipfile) as exception:\n      parser_mediator.ProduceExtractionWarning((\n          'Unable to parse relationships XML file: _rels/.rels with error: '\n          '{0!s}').format(exception))\n      return\n\n    metadata = {}\n\n    for path in property_files:\n      try:\n        xml_data = zip_file.read(path)\n        properties = self._ParsePropertiesXMLFile(xml_data)\n      except (IndexError, IOError, KeyError, OverflowError, ValueError,\n              zipfile.BadZipfile) as exception:\n        parser_mediator.ProduceExtractionWarning((\n            'Unable to parse properties XML file: {0:s} with error: '\n            '{1!s}').format(path, exception))\n        continue\n\n      metadata.update(properties)\n\n    event_data = OpenXMLEventData()\n    event_data.app_version = self._GetPropertyValue(\n        parser_mediator, metadata, 'app_version')\n    event_data.app_version = self._GetPropertyValue(\n        parser_mediator, metadata, 'app_version')\n    event_data.author = self._GetPropertyValue(\n        parser_mediator, metadata, 'author')\n    event_data.creating_app = self._GetPropertyValue(\n        parser_mediator, metadata, 'creating_app')\n    event_data.doc_security = self._GetPropertyValue(\n        parser_mediator, metadata, 'doc_security')\n    event_data.hyperlinks_changed = self._GetPropertyValue(\n        parser_mediator, metadata, 'hyperlinks_changed')\n    event_data.i4 = self._GetPropertyValue(\n        parser_mediator, metadata, 'i4')\n    event_data.last_saved_by = self._GetPropertyValue(\n        parser_mediator, metadata, 'last_saved_by')\n    event_data.links_up_to_date = self._GetPropertyValue(\n        parser_mediator, metadata, 'links_up_to_date')\n    event_data.number_of_characters = self._GetPropertyValue(\n        parser_mediator, metadata, 'number_of_characters')\n    event_data.number_of_characters_with_spaces = self._GetPropertyValue(\n        parser_mediator, metadata, 'number_of_characters_with_spaces')\n    event_data.number_of_lines = self._GetPropertyValue(\n        parser_mediator, metadata, 'number_of_lines')\n    event_data.number_of_pages = self._GetPropertyValue(\n        parser_mediator, metadata, 'number_of_pages')\n    event_data.number_of_paragraphs = self._GetPropertyValue(\n        parser_mediator, metadata, 'number_of_paragraphs')\n    event_data.number_of_words = self._GetPropertyValue(\n        parser_mediator, metadata, 'number_of_words')\n    event_data.revision_number = self._GetPropertyValue(\n        parser_mediator, metadata, 'revision_number')\n    event_data.scale_crop = self._GetPropertyValue(\n        parser_mediator, metadata, 'scale_crop')\n    event_data.shared_doc = self._GetPropertyValue(\n        parser_mediator, metadata, 'shared_doc')\n    event_data.template = self._GetPropertyValue(\n        parser_mediator, metadata, 'template')\n    event_data.total_time = self._GetPropertyValue(\n        parser_mediator, metadata, 'total_time')\n\n    self._ProduceEvent(\n        parser_mediator, event_data, metadata, 'created',\n        definitions.TIME_DESCRIPTION_CREATION, 'creation time')\n    self._ProduceEvent(\n        parser_mediator, event_data, metadata, 'modified',\n        definitions.TIME_DESCRIPTION_MODIFICATION, 'modification time')\n    self._ProduceEvent(\n        parser_mediator, event_data, metadata, 'last_printed',\n        definitions.TIME_DESCRIPTION_LAST_PRINTED, 'last printed time')", "response": "Extracts properties from a zip file - like object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncopying the attribute container from a dictionary.", "response": "def CopyFromDict(self, attributes):\n    \"\"\"Copies the attribute container from a dictionary.\n\n    Args:\n      attributes (dict[str, object]): attribute values per name.\n    \"\"\"\n    for attribute_name, attribute_value in attributes.items():\n      # Not using startswith to improve performance.\n      if attribute_name[0] == '_':\n        continue\n      setattr(self, attribute_name, attribute_value)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nretrieve the names of all attributes.", "response": "def GetAttributeNames(self):\n    \"\"\"Retrieves the names of all attributes.\n\n    Returns:\n      list[str]: attribute names.\n    \"\"\"\n    attribute_names = []\n    for attribute_name in iter(self.__dict__.keys()):\n      # Not using startswith to improve performance.\n      if attribute_name[0] == '_':\n        continue\n      attribute_names.append(attribute_name)\n\n    return attribute_names"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef GetAttributes(self):\n    for attribute_name, attribute_value in iter(self.__dict__.items()):\n      # Not using startswith to improve performance.\n      if attribute_name[0] == '_' or attribute_value is None:\n        continue\n\n      yield attribute_name, attribute_value", "response": "Retrieves the attribute names and values."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nretrieve a comparable string of the attribute values.", "response": "def GetAttributeValuesString(self):\n    \"\"\"Retrieves a comparable string of the attribute values.\n\n    Returns:\n      str: comparable string of the attribute values.\n    \"\"\"\n    attributes = []\n    for attribute_name, attribute_value in sorted(self.__dict__.items()):\n      # Not using startswith to improve performance.\n      if attribute_name[0] == '_' or attribute_value is None:\n        continue\n\n      if isinstance(attribute_value, dict):\n        attribute_value = sorted(attribute_value.items())\n\n      elif isinstance(attribute_value, py2to3.BYTES_TYPE):\n        attribute_value = repr(attribute_value)\n\n      attribute_string = '{0:s}: {1!s}'.format(attribute_name, attribute_value)\n      attributes.append(attribute_string)\n\n    return ', '.join(attributes)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ExtractEvents(self, parser_mediator, registry_key, **kwargs):\n    installation_value = None\n    string_values = {}\n    for registry_value in registry_key.GetValues():\n      # Ignore the default value.\n      if not registry_value.name:\n        continue\n\n      if (registry_value.name == 'InstallDate' and\n          registry_value.DataIsInteger()):\n        installation_value = registry_value\n        continue\n\n      # Ignore any value that is empty or that does not contain a string.\n      if not registry_value.data or not registry_value.DataIsString():\n        continue\n\n      string_value_name = self._STRING_VALUE_NAME_STRINGS.get(\n          registry_value.name, None)\n      if not string_value_name:\n        continue\n\n      string_values[string_value_name] = registry_value.GetDataAsObject()\n\n    values_dict = {}\n    values_dict['Owner'] = string_values.get('owner', '')\n    values_dict['Product name'] = string_values.get('product_name', '')\n    values_dict['Service pack'] = string_values.get('service_pack', '')\n    values_dict['Windows Version Information'] = string_values.get(\n        'version', '')\n\n    event_data = windows_events.WindowsRegistryEventData()\n    event_data.key_path = registry_key.path\n    event_data.offset = registry_key.offset\n    event_data.regvalue = values_dict\n\n    event = time_events.DateTimeValuesEvent(\n        registry_key.last_written_time, definitions.TIME_DESCRIPTION_WRITTEN)\n    parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    # TODO: if not present indicate anomaly of missing installation\n    # date and time.\n    if installation_value:\n      event_data = windows_events.WindowsRegistryInstallationEventData()\n      event_data.key_path = registry_key.path\n      event_data.offset = registry_key.offset\n      event_data.owner = string_values.get('owner', None)\n      event_data.product_name = string_values.get('product_name', None)\n      event_data.service_pack = string_values.get('service_pack', None)\n      event_data.version = string_values.get('version', None)\n\n      installation_time = installation_value.GetDataAsObject()\n      date_time = dfdatetime_posix_time.PosixTime(timestamp=installation_time)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_INSTALLATION)\n      parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Extracts events from a Windows Registry key."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ParseRecord(self, parser_mediator, key, structure):\n    if key not in self._SUPPORTED_KEYS:\n      raise errors.ParseError(\n          'Unable to parse record, unknown structure: {0:s}'.format(key))\n\n    if key == 'quota_exceeded_line':\n      # skip this line\n      return\n\n    date_time = dfdatetime_time_elements.TimeElementsInMilliseconds()\n    try:\n      date_time.CopyFromStringISO8601(structure.date)\n    except ValueError:\n      parser_mediator.ProduceExtractionWarning(\n          'invalid date time value: {0:s}'.format(structure.date))\n      return\n\n    if key == 'execution_line':\n      event_data = SantaExecutionEventData()\n      event_data.action = structure.action\n      event_data.decision = structure.decision\n      event_data.reason = structure.reason\n      event_data.process_hash = structure.sha256\n      event_data.certificate_hash = structure.get('cert_sha256', None)\n      event_data.certificate_common_name = structure.get('cert_cn', None)\n      event_data.quarantine_url = structure.get('quarantine_url', None)\n      event_data.pid = structure.pid\n      event_data.ppid = structure.ppid\n      event_data.uid = structure.uid\n      event_data.user = structure.user\n      event_data.gid = structure.gid\n      event_data.group = structure.group\n      event_data.mode = structure.mode\n      event_data.process_path = structure.path\n      event_data.process_arguments = structure.get('args', None)\n\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_LAST_RUN)\n\n    if key == 'file_system_event_line':\n      event_data = SantaFileSystemEventData()\n      event_data.action = structure.action\n      event_data.file_path = structure.path\n      event_data.file_new_path = structure.get('newpath', None)\n      event_data.pid = structure.pid\n      event_data.ppid = structure.ppid\n      event_data.process = structure.process\n      event_data.process_path = structure.processpath\n      event_data.uid = structure.uid\n      event_data.user = structure.user\n      event_data.gid = structure.gid\n      event_data.group = structure.group\n\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_WRITTEN)\n\n    if key == 'umount_line':\n      event_data = SantaMountEventData()\n      event_data.action = structure.action\n      event_data.mount = structure.mount\n      event_data.volume = structure.volume\n      event_data.bsd_name = structure.bsd_name\n\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_WRITTEN)\n\n    if key == 'mount_line':\n      event_data = SantaMountEventData()\n      event_data.action = structure.action\n      event_data.mount = structure.mount\n      event_data.volume = structure.volume\n      event_data.bsd_name = structure.bsd_name\n      event_data.fs = structure.fs\n      event_data.model = structure.model\n      event_data.serial = structure.serial\n      event_data.bus = structure.bus\n      event_data.dmg_path = structure.dmg_path\n      event_data.appearance = structure.appearance\n\n      if event_data.appearance:\n        new_date_time = dfdatetime_time_elements.TimeElementsInMilliseconds()\n\n        try:\n          new_date_time.CopyFromStringISO8601(event_data.appearance)\n          new_event = time_events.DateTimeValuesEvent(\n              new_date_time, definitions.TIME_DESCRIPTION_FIRST_CONNECTED)\n          parser_mediator.ProduceEventWithEventData(new_event, event_data)\n        except ValueError:\n          parser_mediator.ProduceExtractionWarning(\n              'invalid date time value: {0:s}'.format(event_data.appearance))\n\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_WRITTEN)\n\n    parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Parses a matching entry.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n        and other components, such as storage and dfvfs.\n      key (str): name of the parsed structure.\n      structure (pyparsing.ParseResults): elements parsed from the file.\n\n    Raises:\n      ParseError: when the structure type is unknown."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _SendItem(self, zmq_socket, item, block=True):\n    try:\n      logger.debug('{0:s} sending item'.format(self.name))\n      if block:\n        zmq_socket.send_pyobj(item)\n      else:\n        zmq_socket.send_pyobj(item, zmq.DONTWAIT)\n      logger.debug('{0:s} sent item'.format(self.name))\n      return True\n\n    except zmq.error.Again:\n      logger.debug('{0:s} could not send an item'.format(self.name))\n\n    except zmq.error.ZMQError as exception:\n      if exception.errno == errno.EINTR:\n        logger.error(\n            'ZMQ syscall interrupted in {0:s}.'.format(\n                self.name))\n\n    return False", "response": "Attempts to send an item to a ZeroMQ socket."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nattempt to receive an item from a ZeroMQ socket.", "response": "def _ReceiveItemOnActivity(self, zmq_socket):\n    \"\"\"Attempts to receive an item from a ZeroMQ socket.\n\n    Args:\n      zmq_socket (zmq.Socket): used to the receive the item.\n\n    Returns:\n      object: item from the socket.\n\n    Raises:\n      QueueEmpty: if no item could be received within the timeout.\n      zmq.error.ZMQError: if an error occurs in ZeroMQ\n    \"\"\"\n    events = zmq_socket.poll(\n        self._ZMQ_SOCKET_RECEIVE_TIMEOUT_MILLISECONDS)\n    if events:\n      try:\n        received_object = self._zmq_socket.recv_pyobj()\n        return received_object\n\n      except zmq.error.Again:\n        logger.error(\n            '{0:s}. Failed to receive item in time.'.format(\n                self.name))\n        raise\n\n      except zmq.error.ZMQError as exception:\n        if exception.errno == errno.EINTR:\n          logger.error(\n              'ZMQ syscall interrupted in {0:s}. Queue aborting.'.format(\n                  self.name))\n        raise\n\n    raise errors.QueueEmpty"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _SetSocketTimeouts(self):\n    # Note that timeout must be an integer value. If timeout is a float\n    # it appears that zmq will not enforce the timeout.\n    timeout = int(self.timeout_seconds * 1000)\n    receive_timeout = min(\n        self._ZMQ_SOCKET_RECEIVE_TIMEOUT_MILLISECONDS, timeout)\n    send_timeout = min(self._ZMQ_SOCKET_SEND_TIMEOUT_MILLISECONDS, timeout)\n\n    self._zmq_socket.setsockopt(zmq.RCVTIMEO, receive_timeout)\n    self._zmq_socket.setsockopt(zmq.SNDTIMEO, send_timeout)", "response": "Sets the timeouts for socket send and receive."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a ZeroMQ socket.", "response": "def _CreateZMQSocket(self):\n    \"\"\"Creates a ZeroMQ socket.\"\"\"\n    logger.debug('Creating socket for {0:s}'.format(self.name))\n\n    if not self._zmq_context:\n      self._zmq_context = zmq.Context()\n\n    # The terminate and close threading events need to be created when the\n    # socket is opened. Threading events are unpickleable objects and cannot\n    # passed in multiprocessing on Windows.\n\n    if not self._terminate_event:\n      self._terminate_event = threading.Event()\n\n    if not self._closed_event:\n      self._closed_event = threading.Event()\n\n    if self._zmq_socket:\n      logger.debug('Closing old socket for {0:s}'.format(self.name))\n      self._zmq_socket.close()\n      self._zmq_socket = None\n\n    self._zmq_socket = self._zmq_context.socket(self._SOCKET_TYPE)\n    self._SetSocketTimeouts()\n    self._SetSocketHighWaterMark()\n\n    if self.port:\n      address = '{0:s}:{1:d}'.format(self._SOCKET_ADDRESS, self.port)\n      if self.SOCKET_CONNECTION_TYPE == self.SOCKET_CONNECTION_CONNECT:\n        self._zmq_socket.connect(address)\n        logger.debug('{0:s} connected to {1:s}'.format(self.name, address))\n      else:\n        self._zmq_socket.bind(address)\n        logger.debug(\n            '{0:s} bound to specified port {1:s}'.format(self.name, address))\n    else:\n      self.port = self._zmq_socket.bind_to_random_port(self._SOCKET_ADDRESS)\n      logger.debug(\n          '{0:s} bound to random port {1:d}'.format(self.name, self.port))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef Close(self, abort=False):\n    if not self._closed_event or not self._terminate_event:\n      raise RuntimeError('Missing closed or terminate event.')\n\n    if not abort and self._closed_event.is_set():\n      raise errors.QueueAlreadyClosed()\n\n    self._closed_event.set()\n\n    if abort:\n      if not self._closed_event.is_set():\n        logger.warning(\n            '{0:s} queue aborting. Contents may be lost.'.format(self.name))\n\n      self._linger_seconds = 0\n\n      # We can't determine whether a there might be an operation being performed\n      # on the socket in a separate method or thread, so we'll signal that any\n      # such operation should cease.\n      self._terminate_event.set()\n\n    else:\n      logger.debug(\n          '{0:s} queue closing, will linger for up to {1:d} seconds'.format(\n              self.name, self._linger_seconds))", "response": "Closes the queue.\n\n    Args:\n      abort (Optional[bool]): whether the Close is the result of an abort\n          condition. If True, queue contents may be lost.\n\n    Raises:\n      QueueAlreadyClosed: if the queue is not started, or has already been\n          closed.\n      RuntimeError: if closed or terminate event is missing."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\npopping an item off the queue.", "response": "def PopItem(self):\n    \"\"\"Pops an item off the queue.\n\n    If no ZeroMQ socket has been created, one will be created the first\n    time this method is called.\n\n    Returns:\n      object: item from the queue.\n\n    Raises:\n      KeyboardInterrupt: if the process is sent a KeyboardInterrupt while\n          popping an item.\n      QueueEmpty: if the queue is empty, and no item could be popped within the\n          queue timeout.\n      RuntimeError: if closed or terminate event is missing.\n      zmq.error.ZMQError: if a ZeroMQ error occurs.\n    \"\"\"\n    if not self._zmq_socket:\n      self._CreateZMQSocket()\n\n    if not self._closed_event or not self._terminate_event:\n      raise RuntimeError('Missing closed or terminate event.')\n\n    logger.debug(\n        'Pop on {0:s} queue, port {1:d}'.format(self.name, self.port))\n\n    last_retry_timestamp = time.time() + self.timeout_seconds\n    while not self._closed_event.is_set() or not self._terminate_event.is_set():\n      try:\n        return self._ReceiveItemOnActivity(self._zmq_socket)\n\n      except errors.QueueEmpty:\n        if time.time() > last_retry_timestamp:\n          raise\n\n      except KeyboardInterrupt:\n        self.Close(abort=True)\n        raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef PushItem(self, item, block=True):\n    if not self._zmq_socket:\n      self._CreateZMQSocket()\n\n    if not self._terminate_event:\n      raise RuntimeError('Missing terminate event.')\n\n    logger.debug(\n        'Push on {0:s} queue, port {1:d}'.format(self.name, self.port))\n\n    last_retry_timestamp = time.time() + self.timeout_seconds\n    while not self._terminate_event.is_set():\n      try:\n        send_successful = self._SendItem(self._zmq_socket, item, block)\n        if send_successful:\n          break\n\n        if time.time() > last_retry_timestamp:\n          logger.error('{0:s} unable to push item, raising.'.format(\n              self.name))\n          raise errors.QueueFull\n\n      except KeyboardInterrupt:\n        self.Close(abort=True)\n        raise", "response": "Push an item on to the queue."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef PopItem(self):\n    if not self._zmq_socket:\n      self._CreateZMQSocket()\n\n    if not self._terminate_event:\n      raise RuntimeError('Missing terminate event.')\n\n    logger.debug('Pop on {0:s} queue, port {1:d}'.format(\n        self.name, self.port))\n\n    last_retry_time = time.time() + self.timeout_seconds\n    while not self._terminate_event.is_set():\n      try:\n        self._zmq_socket.send_pyobj(None)\n        break\n\n      except zmq.error.Again:\n        # The existing socket is now out of sync, so we need to open a new one.\n        self._CreateZMQSocket()\n        if time.time() > last_retry_time:\n          logger.warning('{0:s} timeout requesting item'.format(self.name))\n          raise errors.QueueEmpty\n\n        continue\n\n    while not self._terminate_event.is_set():\n      try:\n        return self._ReceiveItemOnActivity(self._zmq_socket)\n      except errors.QueueEmpty:\n        continue\n\n      except KeyboardInterrupt:\n        self.Close(abort=True)\n        raise", "response": "Pops an item off the queue."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _CreateZMQSocket(self):\n    super(ZeroMQBufferedQueue, self)._CreateZMQSocket()\n    if not self._zmq_thread:\n      thread_name = '{0:s}_zmq_responder'.format(self.name)\n      self._zmq_thread = threading.Thread(\n          target=self._ZeroMQResponder, args=[self._queue], name=thread_name)\n      self._zmq_thread.start()", "response": "Creates a ZeroMQ socket as well as a regular queue and a thread."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nclosing the queue. Args: abort (Optional[bool]): whether the Close is the result of an abort condition. If True, queue contents may be lost. Raises: QueueAlreadyClosed: if the queue is not started, or has already been closed. RuntimeError: if closed or terminate event is missing.", "response": "def Close(self, abort=False):\n    \"\"\"Closes the queue.\n\n    Args:\n      abort (Optional[bool]): whether the Close is the result of an abort\n          condition. If True, queue contents may be lost.\n\n    Raises:\n      QueueAlreadyClosed: if the queue is not started, or has already been\n          closed.\n      RuntimeError: if closed or terminate event is missing.\n    \"\"\"\n    if not self._closed_event or not self._terminate_event:\n      raise RuntimeError('Missing closed or terminate event.')\n\n    if not abort and self._closed_event.is_set():\n      raise errors.QueueAlreadyClosed()\n\n    self._closed_event.set()\n\n    if abort:\n      if not self._closed_event.is_set():\n        logger.warning(\n            '{0:s} queue aborting. Contents may be lost.'.format(self.name))\n\n      # We can't determine whether a there might be an operation being performed\n      # on the socket in a separate method or thread, so we'll signal that any\n      # such operation should cease.\n      self._terminate_event.set()\n\n      self._linger_seconds = 0\n\n      if self._zmq_thread:\n        logger.debug('[{0:s}] Waiting for thread to exit.'.format(self.name))\n        self._zmq_thread.join(timeout=self.timeout_seconds)\n        if self._zmq_thread.isAlive():\n          logger.error((\n              '{0:s} ZMQ responder thread did not exit within timeout').format(\n                  self.name))\n    else:\n      logger.debug(\n          '{0:s} queue closing, will linger for up to {1:d} seconds'.format(\n              self.name, self._linger_seconds))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _ZeroMQResponder(self, source_queue):\n    if not self._closed_event or not self._terminate_event:\n      raise RuntimeError('Missing closed or terminate event.')\n\n    logger.debug('{0:s} responder thread started'.format(self.name))\n\n    item = None\n    while not self._terminate_event.is_set():\n      if not item:\n        try:\n          if self._closed_event.is_set():\n            item = source_queue.get_nowait()\n          else:\n            item = source_queue.get(True, self._buffer_timeout_seconds)\n\n        except Queue.Empty:\n          if self._closed_event.is_set():\n            break\n\n          continue\n\n      try:\n        # We need to receive a request before we can reply with the item.\n        self._ReceiveItemOnActivity(self._zmq_socket)\n\n      except errors.QueueEmpty:\n        if self._closed_event.is_set() and self._queue.empty():\n          break\n\n        continue\n\n      sent_successfully = self._SendItem(self._zmq_socket, item)\n      item = None\n      if not sent_successfully:\n        logger.error('Queue {0:s} unable to send item.'.format(self.name))\n        break\n\n    logger.info('Queue {0:s} responder exiting.'.format(self.name))\n    self._zmq_socket.close(self._linger_seconds)", "response": "Starts a thread that handles all the requests and replies."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef PushItem(self, item, block=True):\n    if not self._closed_event:\n      raise RuntimeError('Missing closed event.')\n\n    if self._closed_event.is_set():\n      raise errors.QueueAlreadyClosed()\n\n    if not self._zmq_socket:\n      self._CreateZMQSocket()\n\n    try:\n      if block:\n        self._queue.put(item, timeout=self.timeout_seconds)\n      else:\n        self._queue.put(item, block=False)\n    except Queue.Full as exception:\n      raise errors.QueueFull(exception)", "response": "Push an item on to the queue."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef DeregisterOutput(cls, output_class):\n    output_class_name = output_class.NAME.lower()\n\n    if output_class_name in cls._disabled_output_classes:\n      class_dict = cls._disabled_output_classes\n    else:\n      class_dict = cls._output_classes\n\n    if output_class_name not in class_dict:\n      raise KeyError(\n          'Output class not set for name: {0:s}.'.format(\n              output_class.NAME))\n\n    del class_dict[output_class_name]", "response": "Deregisters an output class."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves the disabled output classes and its associated name.", "response": "def GetDisabledOutputClasses(cls):\n    \"\"\"Retrieves the disabled output classes and its associated name.\n\n    Yields:\n      tuple[str, type]: output module name and class.\n    \"\"\"\n    for _, output_class in iter(cls._disabled_output_classes.items()):\n      yield output_class.NAME, output_class"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving the output class with the given name.", "response": "def GetOutputClass(cls, name):\n    \"\"\"Retrieves the output class for a specific name.\n\n    Args:\n      name (str): name of the output module.\n\n    Returns:\n      type: output module class.\n\n    Raises:\n      KeyError: if there is no output class found with the supplied name.\n      ValueError: if name is not a string.\n    \"\"\"\n    if not isinstance(name, py2to3.STRING_TYPES):\n      raise ValueError('Name attribute is not a string.')\n\n    name = name.lower()\n    if name not in cls._output_classes:\n      raise KeyError(\n          'Name: [{0:s}] not registered as an output module.'.format(name))\n\n    return cls._output_classes[name]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretrieves the available output classes its associated name and type object.", "response": "def GetOutputClasses(cls):\n    \"\"\"Retrieves the available output classes its associated name.\n\n    Yields:\n      tuple[str, type]: output class name and type object.\n    \"\"\"\n    for _, output_class in iter(cls._output_classes.items()):\n      yield output_class.NAME, output_class"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndetermine if a specific output class is registered with the manager.", "response": "def HasOutputClass(cls, name):\n    \"\"\"Determines if a specific output class is registered with the manager.\n\n    Args:\n      name (str): name of the output module.\n\n    Returns:\n      bool: True if the output class is registered.\n    \"\"\"\n    if not isinstance(name, py2to3.STRING_TYPES):\n      return False\n\n    return name.lower() in cls._output_classes"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndetermining if a specific output class is a linear output module.", "response": "def IsLinearOutputModule(cls, name):\n    \"\"\"Determines if a specific output class is a linear output module.\n\n    Args:\n      name (str): name of the output module.\n\n    Returns:\n      True: if the output module is linear.\n    \"\"\"\n    name = name.lower()\n\n    output_class = cls._output_classes.get(name, None)\n    if not output_class:\n      output_class = cls._disabled_output_classes.get(name, None)\n\n    if output_class:\n      return issubclass(output_class, interface.LinearOutputModule)\n\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef NewOutputModule(cls, name, output_mediator):\n    output_class = cls.GetOutputClass(name)\n    return output_class(output_mediator)", "response": "Creates a new output module object for the specified output format."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nregister an output class.", "response": "def RegisterOutput(cls, output_class, disabled=False):\n    \"\"\"Registers an output class.\n\n    The output classes are identified based on their NAME attribute.\n\n    Args:\n      output_class (type): output module class.\n      disabled (Optional[bool]): True if the output module is disabled due to\n          the module not loading correctly or not.\n\n    Raises:\n      KeyError: if output class is already set for the corresponding name.\n    \"\"\"\n    output_name = output_class.NAME.lower()\n\n    if disabled:\n      class_dict = cls._disabled_output_classes\n    else:\n      class_dict = cls._output_classes\n\n    if output_name in class_dict:\n      raise KeyError((\n          'Output class already set for name: {0:s}.').format(\n              output_class.NAME))\n\n    class_dict[output_name] = output_class"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nregistering output classes. The output classes are identified based on their NAME attribute. Args: output_classes (list[type]): output module classes. disabled (Optional[bool]): True if the output module is disabled due to the module not loading correctly or not. Raises: KeyError: if output class is already set for the corresponding name.", "response": "def RegisterOutputs(cls, output_classes, disabled=False):\n    \"\"\"Registers output classes.\n\n    The output classes are identified based on their NAME attribute.\n\n    Args:\n      output_classes (list[type]): output module classes.\n      disabled (Optional[bool]): True if the output module is disabled due to\n          the module not loading correctly or not.\n\n    Raises:\n      KeyError: if output class is already set for the corresponding name.\n    \"\"\"\n    for output_class in output_classes:\n      cls.RegisterOutput(output_class, disabled)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef SampleStart(self):\n    self._start_cpu_time = time.clock()\n    self.start_sample_time = time.time()\n    self.total_cpu_time = 0", "response": "Starts measuring the CPU time."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstopping measuring the CPU time.", "response": "def SampleStop(self):\n    \"\"\"Stops measuring the CPU time.\"\"\"\n    if self._start_cpu_time is not None:\n      self.total_cpu_time += time.clock() - self._start_cpu_time"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _WritesString(self, content):\n    content_bytes = codecs.encode(content, 'utf-8')\n    self._sample_file.write(content_bytes)", "response": "Writes a string to the sample file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstarting timing CPU time.", "response": "def StartTiming(self, profile_name):\n    \"\"\"Starts timing CPU time.\n\n    Args:\n      profile_name (str): name of the profile to sample.\n    \"\"\"\n    if profile_name not in self._profile_measurements:\n      self._profile_measurements[profile_name] = CPUTimeMeasurement()\n\n    self._profile_measurements[profile_name].SampleStart()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef StopTiming(self, profile_name):\n    measurements = self._profile_measurements.get(profile_name)\n    if measurements:\n      measurements.SampleStop()\n\n      sample = '{0:f}\\t{1:s}\\t{2:f}\\n'.format(\n          measurements.start_sample_time, profile_name,\n          measurements.total_cpu_time)\n      self._WritesString(sample)", "response": "Stops timing CPU time."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntaking a sample for profiling.", "response": "def Sample(self):\n    \"\"\"Takes a sample for profiling.\"\"\"\n    self._profiling_sample += 1\n\n    if self._profiling_sample >= self._profiling_sample_rate:\n      if self._heapy:\n        heap = self._heapy.heap()\n        heap.dump(self._sample_file)\n\n      self._profiling_sample = 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef Sample(self, profile_name, used_memory):\n    sample_time = time.time()\n    sample = '{0:f}\\t{1:s}\\t{2:d}\\n'.format(\n        sample_time, profile_name, used_memory)\n    self._WritesString(sample)", "response": "Takes a sample for profiling.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef Sample(self, operation, description, data_size, compressed_data_size):\n    sample_time = time.time()\n    sample = '{0:f}\\t{1:s}\\t{2:s}\\t{3:d}\\t{4:d}\\n'.format(\n        sample_time, operation, description, data_size, compressed_data_size)\n    self._WritesString(sample)", "response": "Takes a sample of data read or written for profiling."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntakes a sample of the status of queued tasks for profiling.", "response": "def Sample(self, tasks_status):\n    \"\"\"Takes a sample of the status of queued tasks for profiling.\n\n    Args:\n      tasks_status (TasksStatus): status information about tasks.\n    \"\"\"\n    sample_time = time.time()\n    sample = '{0:f}\\t{1:d}\\t{2:d}\\t{3:d}\\t{4:d}\\t{5:d}\\n'.format(\n        sample_time, tasks_status.number_of_queued_tasks,\n        tasks_status.number_of_tasks_processing,\n        tasks_status.number_of_tasks_pending_merge,\n        tasks_status.number_of_abandoned_tasks,\n        tasks_status.total_number_of_tasks)\n    self._WritesString(sample)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef Sample(self, task, status):\n    sample_time = time.time()\n    sample = '{0:f}\\t{1:s}\\t{2:s}\\n'.format(\n        sample_time, task.identifier, status)\n    self._WritesString(sample)", "response": "Takes a sample of the status of a task for profiling."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef Close(self):\n    super(TimesketchOutputModule, self).Close()\n\n    with self._timesketch.app_context():\n      search_index = timesketch_sketch.SearchIndex.query.filter_by(\n          index_name=self._index_name).first()\n      search_index.status.remove(search_index.status[0])\n      timesketch_db_session.add(search_index)\n      timesketch_db_session.commit()", "response": "Closes the connection to Elasticsearch and removes the processing status on the Timesketch search index object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the timeline name.", "response": "def SetTimelineName(self, timeline_name):\n    \"\"\"Sets the timeline name.\n\n    Args:\n      timeline_name (str): timeline name.\n    \"\"\"\n    self._timeline_name = timeline_name\n    logger.info('Timeline name: {0:s}'.format(self._timeline_name))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the username of the user that should own the timeline.", "response": "def SetTimelineOwner(self, username):\n    \"\"\"Sets the username of the user that should own the timeline.\n\n    Args:\n      username (str): username.\n    \"\"\"\n    self._timeline_owner = username\n    logger.info('Owner of the timeline: {0!s}'.format(self._timeline_owner))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef WriteHeader(self):\n    # This cannot be static because we use the value of self._document_type\n    # from arguments.\n    mappings = {\n        self._document_type: {\n            'properties': {\n                'timesketch_label': {\n                    'type': 'nested'\n                }\n            }\n        }\n    }\n\n    # Get Elasticsearch host and port from Timesketch configuration.\n    with self._timesketch.app_context():\n      self._host = current_app.config['ELASTIC_HOST']\n      self._port = current_app.config['ELASTIC_PORT']\n\n    self._Connect()\n\n    self._CreateIndexIfNotExists(self._index_name, mappings)\n\n    user = None\n    if self._timeline_owner:\n      user = timesketch_user.User.query.filter_by(\n          username=self._timeline_owner).first()\n      if not user:\n        raise RuntimeError(\n            'Unknown Timesketch user: {0:s}'.format(self._timeline_owner))\n    else:\n      logger.warning('Timeline will be visible to all Timesketch users')\n\n    with self._timesketch.app_context():\n      search_index = timesketch_sketch.SearchIndex.get_or_create(\n          name=self._timeline_name, description=self._timeline_name, user=user,\n          index_name=self._index_name)\n\n      # Grant the user read permission on the mapping object and set status.\n      # If user is None the timeline becomes visible to all users.\n      search_index.grant_permission(user=user, permission='read')\n\n      # In case we have a user grant additional permissions.\n      if user:\n        search_index.grant_permission(user=user, permission='write')\n        search_index.grant_permission(user=user, permission='delete')\n\n      # Let the Timesketch UI know that the timeline is processing.\n      search_index.set_status('processing')\n\n      # Save the mapping object to the Timesketch database.\n      timesketch_db_session.add(search_index)\n      timesketch_db_session.commit()\n\n    logger.debug('Adding events to Timesketch.')", "response": "Sets up the Elasticsearch index and the Timesketch database object."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses the HTTP headers and returns the request method and response code.", "response": "def _ParseHTTPHeaders(self, header_data, offset, display_name):\n    \"\"\"Extract relevant information from HTTP header.\n\n    Args:\n      header_data (bytes): HTTP header data.\n      offset (int): offset of the cache record, relative to the start of\n          the Firefox cache file.\n      display_name (str): display name of the Firefox cache file.\n\n    Returns:\n      tuple: containing:\n\n        str: HTTP request method or None if the value cannot be extracted.\n        str: HTTP response code or None if the value cannot be extracted.\n    \"\"\"\n    header_string = header_data.decode('ascii', errors='replace')\n\n    try:\n      http_header_start = header_string.index('request-method')\n    except ValueError:\n      logger.debug('No request method in header: \"{0:s}\"'.format(header_string))\n      return None, None\n\n    # HTTP request and response headers.\n    http_headers = header_string[http_header_start::]\n\n    header_parts = http_headers.split('\\x00')\n\n    # TODO: check len(header_parts).\n    request_method = header_parts[1]\n\n    if request_method not in self._REQUEST_METHODS:\n      logger.debug((\n          '[{0:s}] {1:s}:{2:d}: Unknown HTTP method \\'{3:s}\\'. Response '\n          'headers: \\'{4:s}\\'').format(\n              self.NAME, display_name, offset, request_method, header_string))\n\n    try:\n      response_head_start = http_headers.index('response-head')\n    except ValueError:\n      logger.debug('No response head in header: \"{0:s}\"'.format(header_string))\n      return request_method, None\n\n    # HTTP response headers.\n    response_head = http_headers[response_head_start::]\n\n    response_head_parts = response_head.split('\\x00')\n\n    # Response code, followed by other response header key-value pairs,\n    # separated by newline.\n    # TODO: check len(response_head_parts).\n    response_head_text = response_head_parts[1]\n    response_head_text_parts = response_head_text.split('\\r\\n')\n\n    # The first line contains response code.\n    # TODO: check len(response_head_text_parts).\n    response_code = response_head_text_parts[0]\n\n    if not response_code.startswith('HTTP'):\n      logger.debug((\n          '[{0:s}] {1:s}:{2:d}: Could not determine HTTP response code. '\n          'Response headers: \\'{3:s}\\'.').format(\n              self.NAME, display_name, offset, header_string))\n\n    return request_method, response_code"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _GetFirefoxConfig(self, file_object, display_name):\n    # There ought to be a valid record within the first 4 MiB. We use this\n    # limit to prevent reading large invalid files.\n    to_read = min(file_object.get_size(), self._INITIAL_CACHE_FILE_SIZE)\n\n    while file_object.get_offset() < to_read:\n      offset = file_object.get_offset()\n\n      try:\n        cache_entry, _ = self._ReadCacheEntry(\n            file_object, display_name, self._MINIMUM_BLOCK_SIZE)\n\n        # We have not yet determined the block size, so we use the smallest\n        # possible size.\n        record_size = (\n            self._CACHE_ENTRY_HEADER_SIZE + cache_entry.request_size +\n            cache_entry.information_size)\n\n        if record_size >= 4096:\n          # _CACHE_003_\n          block_size = 4096\n        elif record_size >= 1024:\n          # _CACHE_002_\n          block_size = 1024\n        else:\n          # _CACHE_001_\n          block_size = 256\n\n        return self.FIREFOX_CACHE_CONFIG(block_size, offset)\n\n      except IOError:\n        logger.debug('[{0:s}] {1:s}:{2:d}: Invalid record.'.format(\n            self.NAME, display_name, offset))\n\n    raise errors.UnableToParseFile(\n        'Could not find a valid cache record. Not a Firefox cache file.')", "response": "Returns the Firefox cache config."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing a cache entry.", "response": "def _ParseCacheEntry(\n      self, parser_mediator, file_object, display_name, block_size):\n    \"\"\"Parses a cache entry.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      file_object (dfvfs.FileIO): a file-like object.\n      display_name (str): display name.\n      block_size (int): block size.\n    \"\"\"\n    cache_entry, event_data = self._ReadCacheEntry(\n        file_object, display_name, block_size)\n\n    date_time = dfdatetime_posix_time.PosixTime(\n        timestamp=cache_entry.last_fetched_time)\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_LAST_VISITED)\n    parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    if cache_entry.last_modified_time:\n      date_time = dfdatetime_posix_time.PosixTime(\n          timestamp=cache_entry.last_modified_time)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_WRITTEN)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    if cache_entry.expiration_time:\n      date_time = dfdatetime_posix_time.PosixTime(\n          timestamp=cache_entry.expiration_time)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_EXPIRATION)\n      parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreads a cache entry from a file - like object.", "response": "def _ReadCacheEntry(self, file_object, display_name, block_size):\n    \"\"\"Reads a cache entry.\n\n    Args:\n      file_object (dfvfs.FileIO): a file-like object.\n      display_name (str): display name.\n      block_size (int): block size.\n\n    Returns:\n      tuple: containing:\n\n        firefox_cache1_entry_header: cache record header structure.\n        FirefoxCacheEventData: event data.\n\n    Raises:\n      IOError: if the cache record header cannot be validated.\n      OSError: if the cache record header cannot be validated.\n      ParseError: if the cache record header cannot be parsed.\n    \"\"\"\n    file_offset = file_object.get_offset()\n\n    # TODO: merge reading the cache entry header and body by having dtFabric\n    # implement the sanity checks done in _ValidateCacheEntryHeader.\n\n    # Seeing that this parser tries to read each block for a possible\n    # cache entry, we read the fixed-size values first.\n    cache_entry_header_map = self._GetDataTypeMap('firefox_cache1_entry_header')\n\n    try:\n      cache_entry_header, header_data_size = self._ReadStructureFromFileObject(\n          file_object, file_offset, cache_entry_header_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.ParseError(\n          'Unable to parse Firefox cache entry header with error: {0!s}'.format(\n              exception))\n\n    if not self._ValidateCacheEntryHeader(cache_entry_header):\n      # Skip to the next block potentially containing a cache entry.\n      file_offset = block_size - header_data_size\n      file_object.seek(file_offset, os.SEEK_CUR)\n      raise IOError('Not a valid Firefox cache record.')\n\n    body_data_size = (\n        cache_entry_header.request_size + cache_entry_header.information_size)\n\n    cache_entry_body_data = self._ReadData(\n        file_object, file_offset + header_data_size, body_data_size)\n\n    url = cache_entry_body_data[:cache_entry_header.request_size].decode(\n        'ascii').rstrip('\\x00')\n    request_method, response_code = self._ParseHTTPHeaders(\n        cache_entry_body_data[cache_entry_header.request_size:], file_offset,\n        display_name)\n\n    # A request can span multiple blocks, so we use modulo.\n    cache_entry_data_size = header_data_size + body_data_size\n    _, remaining_data_size = divmod(cache_entry_data_size, block_size)\n    if remaining_data_size > 0:\n      file_object.seek(block_size - remaining_data_size, os.SEEK_CUR)\n\n    event_data = FirefoxCacheEventData()\n    event_data.data_size = cache_entry_header.cached_data_size\n    event_data.fetch_count = cache_entry_header.fetch_count\n    event_data.info_size = cache_entry_header.information_size\n    event_data.location = cache_entry_header.location\n    event_data.request_method = request_method\n    event_data.request_size = cache_entry_header.request_size\n    event_data.response_code = response_code\n    event_data.url = url\n    event_data.version = '{0:d}.{1:d}'.format(\n        cache_entry_header.major_format_version,\n        cache_entry_header.minor_format_version)\n\n    return cache_entry_header, event_data"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _ValidateCacheEntryHeader(self, cache_entry_header):\n    return (\n        cache_entry_header.request_size > 0 and\n        cache_entry_header.request_size < self._MAXIMUM_URL_LENGTH and\n        cache_entry_header.major_format_version == 1 and\n        cache_entry_header.last_fetched_time > 0 and\n        cache_entry_header.fetch_count > 0)", "response": "Determines whether the values in the cache entry header are valid."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ParseFileObject(self, parser_mediator, file_object):\n    filename = parser_mediator.GetFilename()\n\n    if (not self._CACHE_FILENAME_RE.match(filename) and\n        not filename.startswith('_CACHE_00')):\n      raise errors.UnableToParseFile('Not a Firefox cache1 file.')\n\n    display_name = parser_mediator.GetDisplayName()\n    firefox_config = self._GetFirefoxConfig(file_object, display_name)\n\n    file_object.seek(firefox_config.first_record_offset)\n\n    while file_object.get_offset() < file_object.get_size():\n      try:\n        self._ParseCacheEntry(\n            parser_mediator, file_object, display_name,\n            firefox_config.block_size)\n\n      except IOError:\n        file_offset = file_object.get_offset() - self._MINIMUM_BLOCK_SIZE\n        logger.debug((\n            '[{0:s}] Invalid cache record in file: {1:s} at offset: '\n            '{2:d}.').format(self.NAME, display_name, file_offset))", "response": "Parses a Firefox cache file - like object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndetermining the offset of the cache file metadata header relative to the start of the file.", "response": "def _GetCacheFileMetadataHeaderOffset(self, file_object):\n    \"\"\"Determines the offset of the cache file metadata header.\n\n     This method is inspired by the work of James Habben:\n     https://github.com/JamesHabben/FirefoxCache2\n\n    Args:\n      file_object (dfvfs.FileIO): a file-like object.\n\n    Returns:\n      int: offset of the file cache metadata header relative to the start\n        of the file.\n\n    Raises:\n      IOError: if the start of the cache file metadata could not be determined.\n    \"\"\"\n    file_object.seek(-4, os.SEEK_END)\n    file_offset = file_object.tell()\n\n    metadata_size_map = self._GetDataTypeMap('uint32be')\n\n    try:\n      metadata_size, _ = self._ReadStructureFromFileObject(\n          file_object, file_offset, metadata_size_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.UnableToParseFile(\n          'Unable to parse cache file metadata size with error: {0!s}'.format(\n              exception))\n\n\n    # Firefox splits the content into chunks.\n    number_of_chunks, remainder = divmod(metadata_size, self._CHUNK_SIZE)\n    if remainder != 0:\n      number_of_chunks += 1\n\n    # Each chunk in the cached record is padded with two bytes.\n    # Skip the first 4 bytes which contains a hash value of the cached content.\n    return metadata_size + (number_of_chunks * 2) + 4"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nvalidates the cache file metadata header.", "response": "def _ValidateCacheFileMetadataHeader(self, cache_file_metadata_header):\n    \"\"\"Determines whether the cache file metadata header is valid.\n\n    Args:\n      cache_file_metadata_header (firefox_cache2_file_metadata_header): cache\n          file metadata header.\n\n    Returns:\n      bool: True if the cache file metadata header is valid.\n    \"\"\"\n    # TODO: add support for format version 2 and 3\n    return (\n        cache_file_metadata_header.key_size > 0 and\n        cache_file_metadata_header.key_size < self._MAXIMUM_URL_LENGTH and\n        cache_file_metadata_header.format_version == 1 and\n        cache_file_metadata_header.last_fetched_time > 0 and\n        cache_file_metadata_header.fetch_count > 0)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing a Firefox cache file - like object.", "response": "def ParseFileObject(self, parser_mediator, file_object):\n    \"\"\"Parses a Firefox cache file-like object.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      file_object (dfvfs.FileIO): a file-like object.\n\n    Raises:\n      UnableToParseFile: when the file cannot be parsed.\n    \"\"\"\n    filename = parser_mediator.GetFilename()\n    if not self._CACHE_FILENAME_RE.match(filename):\n      raise errors.UnableToParseFile('Not a Firefox cache2 file.')\n\n    # The file needs to be at least 36 bytes in size for it to contain\n    # a cache2 file metadata header and a 4-byte offset that points to its\n    # location in the file.\n    file_size = file_object.get_size()\n    if file_size < 36:\n      raise errors.UnableToParseFile(\n          'File size too small for Firefox cache2 file.')\n\n    file_offset = self._GetCacheFileMetadataHeaderOffset(file_object)\n    file_metadata_header_map = self._GetDataTypeMap(\n        'firefox_cache2_file_metadata_header')\n\n    try:\n      file_metadata_header, _ = self._ReadStructureFromFileObject(\n          file_object, file_offset, file_metadata_header_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.UnableToParseFile((\n          'Unable to parse Firefox cache2 file metadata header with error: '\n          '{0!s}').format(exception))\n\n    if not self._ValidateCacheFileMetadataHeader(file_metadata_header):\n      raise errors.UnableToParseFile('Not a valid Firefox cache2 record.')\n\n    url = file_object.read(file_metadata_header.key_size)\n\n    header_data = file_object.read()\n\n    display_name = parser_mediator.GetDisplayName()\n    request_method, response_code = self._ParseHTTPHeaders(\n        header_data[:-4], file_offset, display_name)\n\n    event_data = FirefoxCacheEventData()\n    event_data.fetch_count = file_metadata_header.fetch_count\n    event_data.frequency = file_metadata_header.frequency\n    event_data.request_method = request_method\n    event_data.request_size = file_metadata_header.key_size\n    event_data.response_code = response_code\n    event_data.version = self._CACHE_VERSION\n    event_data.url = url.decode('ascii', errors='replace')\n\n    date_time = dfdatetime_posix_time.PosixTime(\n        timestamp=file_metadata_header.last_fetched_time)\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_LAST_VISITED)\n    parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    if file_metadata_header.last_modified_time:\n      date_time = dfdatetime_posix_time.PosixTime(\n          timestamp=file_metadata_header.last_modified_time)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_WRITTEN)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    if file_metadata_header.expiration_time:\n      date_time = dfdatetime_posix_time.PosixTime(\n          timestamp=file_metadata_header.expiration_time)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_EXPIRATION)\n      parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _CheckStorageFile(self, storage_file_path):  # pylint: disable=arguments-differ\n    if os.path.exists(storage_file_path):\n      if not os.path.isfile(storage_file_path):\n        raise errors.BadConfigOption(\n            'Storage file: {0:s} already exists and is not a file.'.format(\n                storage_file_path))\n      logger.warning('Appending to an already existing storage file.')\n\n    dirname = os.path.dirname(storage_file_path)\n    if not dirname:\n      dirname = '.'\n\n    # TODO: add a more thorough check to see if the storage file really is\n    # a plaso storage file.\n\n    if not os.access(dirname, os.W_OK):\n      raise errors.BadConfigOption(\n          'Unable to write to storage file: {0:s}'.format(storage_file_path))", "response": "Checks if the storage file path is valid."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nretrieving the analysis plugins.", "response": "def _GetAnalysisPlugins(self, analysis_plugins_string):\n    \"\"\"Retrieves analysis plugins.\n\n    Args:\n      analysis_plugins_string (str): comma separated names of analysis plugins\n          to enable.\n\n    Returns:\n      list[AnalysisPlugin]: analysis plugins.\n    \"\"\"\n    if not analysis_plugins_string:\n      return []\n\n    analysis_plugins_list = [\n        name.strip() for name in analysis_plugins_string.split(',')]\n\n    analysis_plugins = self._analysis_manager.GetPluginObjects(\n        analysis_plugins_list)\n    return analysis_plugins.values()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _ParseAnalysisPluginOptions(self, options):\n    # Get a list of all available plugins.\n    analysis_plugin_info = self._analysis_manager.GetAllPluginInformation()\n    # Use set-comprehension to create a set of the analysis plugin names.\n    analysis_plugin_names = {\n        name.lower() for name, _, _ in analysis_plugin_info}\n\n    analysis_plugins = self.ParseStringOption(options, 'analysis_plugins')\n    if not analysis_plugins:\n      return\n\n    # Use set-comprehension to create a set of the requested plugin names.\n    requested_plugin_names = {\n        name.strip().lower() for name in analysis_plugins.split(',')}\n\n    # Check to see if we are trying to load plugins that do not exist.\n    difference = requested_plugin_names.difference(analysis_plugin_names)\n    if difference:\n      raise errors.BadConfigOption(\n          'Non-existent analysis plugins specified: {0:s}'.format(\n              ' '.join(difference)))\n\n    self._analysis_plugins = self._GetAnalysisPlugins(analysis_plugins)\n\n    for analysis_plugin in self._analysis_plugins:\n      helpers_manager.ArgumentHelperManager.ParseOptions(\n          options, analysis_plugin)", "response": "Parses the analysis plugin options."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _ParseFilterOptions(self, options):\n    self._event_filter_expression = self.ParseStringOption(options, 'filter')\n    if self._event_filter_expression:\n      self._event_filter = event_filter.EventObjectFilter()\n\n      try:\n        self._event_filter.CompileFilter(self._event_filter_expression)\n      except errors.ParseError as exception:\n        raise errors.BadConfigOption((\n            'Unable to compile filter expression with error: '\n            '{0!s}').format(exception))\n\n    time_slice_event_time_string = getattr(options, 'slice', None)\n    time_slice_duration = getattr(options, 'slice_size', 5)\n    self._use_time_slicer = getattr(options, 'slicer', False)\n\n    # The slice and slicer cannot be set at the same time.\n    if time_slice_event_time_string and self._use_time_slicer:\n      raise errors.BadConfigOption(\n          'Time slice and slicer cannot be used at the same time.')\n\n    time_slice_event_timestamp = None\n    if time_slice_event_time_string:\n      # Note self._preferred_time_zone is None when not set but represents UTC.\n      preferred_time_zone = self._preferred_time_zone or 'UTC'\n      timezone = pytz.timezone(preferred_time_zone)\n      time_slice_event_timestamp = timelib.Timestamp.FromTimeString(\n          time_slice_event_time_string, timezone=timezone)\n      if time_slice_event_timestamp is None:\n        raise errors.BadConfigOption(\n            'Unsupported time slice event date and time: {0:s}'.format(\n                time_slice_event_time_string))\n\n    if time_slice_event_timestamp is not None or self._use_time_slicer:\n      # Note that time slicer uses the time slice to determine the duration.\n      self._time_slice = time_slices.TimeSlice(\n          time_slice_event_timestamp, duration=time_slice_duration)", "response": "Parses the filter options."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _ParseInformationalOptions(self, options):\n    super(PsortTool, self)._ParseInformationalOptions(options)\n\n    self._quiet_mode = getattr(options, 'quiet', False)\n\n    helpers_manager.ArgumentHelperManager.ParseOptions(\n        options, self, names=['status_view'])", "response": "Parses the informational options."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing the processing options.", "response": "def _ParseProcessingOptions(self, options):\n    \"\"\"Parses the processing options.\n\n    Args:\n      options (argparse.Namespace): command line arguments.\n\n    Raises:\n      BadConfigOption: if the options are invalid.\n    \"\"\"\n    argument_helper_names = [\n        'process_resources', 'temporary_directory', 'zeromq']\n    helpers_manager.ArgumentHelperManager.ParseOptions(\n        options, self, names=argument_helper_names)\n\n    worker_memory_limit = getattr(options, 'worker_memory_limit', None)\n\n    if worker_memory_limit and worker_memory_limit < 0:\n      raise errors.BadConfigOption(\n          'Invalid worker memory limit value cannot be negative.')\n\n    self._worker_memory_limit = worker_memory_limit"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds processing options to the argument group.", "response": "def AddProcessingOptions(self, argument_group):\n    \"\"\"Adds processing options to the argument group\n\n    Args:\n      argument_group (argparse._ArgumentGroup): argparse argument group.\n    \"\"\"\n    argument_helper_names = ['temporary_directory', 'zeromq']\n    if self._CanEnforceProcessMemoryLimit():\n      argument_helper_names.append('process_resources')\n    helpers_manager.ArgumentHelperManager.AddCommandLineArguments(\n        argument_group, names=argument_helper_names)\n\n    argument_group.add_argument(\n        '--worker-memory-limit', '--worker_memory_limit',\n        dest='worker_memory_limit', action='store', type=int,\n        metavar='SIZE', help=(\n            'Maximum amount of memory (data segment and shared memory) '\n            'a worker process is allowed to consume in bytes, where 0 '\n            'represents no limit. The default limit is 2147483648 (2 GiB). '\n            'If a worker process exceeds this limit is is killed by the main '\n            '(foreman) process.'))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses the command line arguments.", "response": "def ParseArguments(self):\n    \"\"\"Parses the command line arguments.\n\n    Returns:\n      bool: True if the arguments were successfully parsed.\n    \"\"\"\n    loggers.ConfigureLogging()\n\n    argument_parser = argparse.ArgumentParser(\n        description=self.DESCRIPTION, add_help=False,\n        conflict_handler='resolve',\n        formatter_class=argparse.RawDescriptionHelpFormatter)\n\n    self.AddBasicOptions(argument_parser)\n\n    helpers_manager.ArgumentHelperManager.AddCommandLineArguments(\n        argument_parser, names=['storage_file'])\n\n    analysis_group = argument_parser.add_argument_group('Analysis Arguments')\n\n    helpers_manager.ArgumentHelperManager.AddCommandLineArguments(\n        analysis_group, names=['analysis_plugins'])\n\n    processing_group = argument_parser.add_argument_group('Processing')\n    self.AddProcessingOptions(processing_group)\n\n    info_group = argument_parser.add_argument_group('Informational Arguments')\n\n    self.AddLogFileOptions(info_group)\n    self.AddInformationalOptions(info_group)\n\n    helpers_manager.ArgumentHelperManager.AddCommandLineArguments(\n        info_group, names=['status_view'])\n\n    filter_group = argument_parser.add_argument_group('Filter Arguments')\n\n    helpers_manager.ArgumentHelperManager.AddCommandLineArguments(\n        filter_group, names=['event_filters'])\n\n    input_group = argument_parser.add_argument_group('Input Arguments')\n\n    helpers_manager.ArgumentHelperManager.AddCommandLineArguments(\n        input_group, names=['data_location'])\n\n    output_group = argument_parser.add_argument_group('Output Arguments')\n\n    output_group.add_argument(\n        '-a', '--include_all', '--include-all', action='store_false',\n        dest='dedup', default=True, help=(\n            'By default the psort removes duplicate entries from the '\n            'output. This parameter changes that behavior so all events '\n            'are included.'))\n\n    helpers_manager.ArgumentHelperManager.AddCommandLineArguments(\n        output_group, names=['language'])\n\n    self.AddTimeZoneOption(output_group)\n\n    output_format_group = argument_parser.add_argument_group(\n        'Output Format Arguments')\n\n    helpers_manager.ArgumentHelperManager.AddCommandLineArguments(\n        output_format_group, names=['output_modules'])\n\n    profiling_group = argument_parser.add_argument_group('profiling arguments')\n\n    helpers_manager.ArgumentHelperManager.AddCommandLineArguments(\n        profiling_group, names=['profiling'])\n\n    try:\n      # TODO: refactor how arguments is used in a more argparse way.\n      options = argument_parser.parse_args()\n    except UnicodeEncodeError:\n      # If we get here we are attempting to print help in a non-Unicode\n      # terminal.\n      self._output_writer.Write('\\n')\n      self._output_writer.Write(argument_parser.format_help())\n      return False\n\n    # Properly prepare the attributes according to local encoding.\n    if self.preferred_encoding == 'ascii':\n      logger.warning(\n          'The preferred encoding of your system is ASCII, which is not '\n          'optimal for the typically non-ASCII characters that need to be '\n          'parsed and processed. The tool will most likely crash and die, '\n          'perhaps in a way that may not be recoverable. A five second delay '\n          'is introduced to give you time to cancel the runtime and '\n          'reconfigure your preferred encoding, otherwise continue at own '\n          'risk.')\n      time.sleep(5)\n\n    try:\n      self.ParseOptions(options)\n    except errors.BadConfigOption as exception:\n      self._output_writer.Write('ERROR: {0!s}\\n'.format(exception))\n      self._output_writer.Write('\\n')\n      self._output_writer.Write(argument_parser.format_usage())\n\n      return False\n\n    loggers.ConfigureLogging(\n        debug_output=self._debug_mode, filename=self._log_file,\n        quiet_mode=self._quiet_mode)\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses the options. Args: options (argparse.Namespace): command line arguments. Raises: BadConfigOption: if the options are invalid.", "response": "def ParseOptions(self, options):\n    \"\"\"Parses the options.\n\n    Args:\n      options (argparse.Namespace): command line arguments.\n\n    Raises:\n      BadConfigOption: if the options are invalid.\n    \"\"\"\n    # The output modules options are dependent on the preferred language\n    # and preferred time zone options.\n    self._ParseTimezoneOption(options)\n\n    names = ['analysis_plugins', 'language', 'profiling']\n    helpers_manager.ArgumentHelperManager.ParseOptions(\n        options, self, names=names)\n\n    self.list_analysis_plugins = self._analysis_plugins == 'list'\n    self.list_language_identifiers = self._preferred_language == 'list'\n    self.list_profilers = self._profilers == 'list'\n\n    if (self.list_analysis_plugins or self.list_language_identifiers or\n        self.list_profilers or self.list_timezones):\n      return\n\n    # Check output modules after the other listable options, otherwise\n    # it could raise with \"requires an output file\".\n    helpers_manager.ArgumentHelperManager.ParseOptions(\n        options, self, names=['output_modules'])\n\n    self.list_output_modules = self._output_format == 'list'\n    if self.list_output_modules:\n      return\n\n    self._ParseInformationalOptions(options)\n\n    helpers_manager.ArgumentHelperManager.ParseOptions(\n        options, self, names=['data_location'])\n\n    self._ParseLogFileOptions(options)\n\n    self._ParseProcessingOptions(options)\n\n    helpers_manager.ArgumentHelperManager.ParseOptions(\n        options, self, names=['event_filters'])\n\n    self._deduplicate_events = getattr(options, 'dedup', True)\n\n    if self._data_location:\n      # Update the data location with the calculated value.\n      options.data_location = self._data_location\n    else:\n      logger.warning('Unable to automatically determine data location.')\n\n    self._command_line_arguments = self.GetCommandLineArguments()\n\n    helpers_manager.ArgumentHelperManager.ParseOptions(\n        options, self, names=['storage_file'])\n\n    # TODO: move check into _CheckStorageFile.\n    if not self._storage_file_path:\n      raise errors.BadConfigOption('Missing storage file option.')\n\n    if not os.path.isfile(self._storage_file_path):\n      raise errors.BadConfigOption(\n          'No such storage file: {0:s}.'.format(self._storage_file_path))\n\n    self._EnforceProcessMemoryLimit(self._process_memory_limit)\n\n    self._analysis_plugins = self._CreateAnalysisPlugins(options)\n    self._output_module = self._CreateOutputModule(options)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprocess a plaso storage file.", "response": "def ProcessStorage(self):\n    \"\"\"Processes a plaso storage file.\n\n    Raises:\n      BadConfigOption: when a configuration parameter fails validation.\n      RuntimeError: if a non-recoverable situation is encountered.\n    \"\"\"\n    self._CheckStorageFile(self._storage_file_path)\n\n    self._status_view.SetMode(self._status_view_mode)\n    self._status_view.SetStorageFileInformation(self._storage_file_path)\n\n    status_update_callback = (\n        self._status_view.GetAnalysisStatusUpdateCallback())\n\n    session = engine.BaseEngine.CreateSession(\n        command_line_arguments=self._command_line_arguments,\n        preferred_encoding=self.preferred_encoding)\n\n    storage_reader = storage_factory.StorageFactory.CreateStorageReaderForFile(\n        self._storage_file_path)\n    if not storage_reader:\n      logger.error('Format of storage file: {0:s} not supported'.format(\n          self._storage_file_path))\n      return\n\n    self._number_of_analysis_reports = (\n        storage_reader.GetNumberOfAnalysisReports())\n    storage_reader.Close()\n\n    configuration = configurations.ProcessingConfiguration()\n    configuration.data_location = self._data_location\n    configuration.profiling.directory = self._profiling_directory\n    configuration.profiling.sample_rate = self._profiling_sample_rate\n    configuration.profiling.profilers = self._profilers\n\n    analysis_counter = None\n    if self._analysis_plugins:\n      storage_writer = (\n          storage_factory.StorageFactory.CreateStorageWriterForFile(\n              session, self._storage_file_path))\n\n      # TODO: add single processing support.\n      analysis_engine = psort.PsortMultiProcessEngine(\n          use_zeromq=self._use_zeromq)\n\n      analysis_engine.AnalyzeEvents(\n          self._knowledge_base, storage_writer, self._data_location,\n          self._analysis_plugins, configuration,\n          event_filter=self._event_filter,\n          event_filter_expression=self._event_filter_expression,\n          status_update_callback=status_update_callback,\n          worker_memory_limit=self._worker_memory_limit)\n\n      analysis_counter = collections.Counter()\n      for item, value in iter(session.analysis_reports_counter.items()):\n        analysis_counter[item] = value\n\n    if self._output_format != 'null':\n      storage_reader = (\n          storage_factory.StorageFactory.CreateStorageReaderForFile(\n              self._storage_file_path))\n\n      # TODO: add single processing support.\n      analysis_engine = psort.PsortMultiProcessEngine(\n          use_zeromq=self._use_zeromq)\n\n      analysis_engine.ExportEvents(\n          self._knowledge_base, storage_reader, self._output_module,\n          configuration, deduplicate_events=self._deduplicate_events,\n          event_filter=self._event_filter,\n          status_update_callback=status_update_callback,\n          time_slice=self._time_slice, use_time_slicer=self._use_time_slicer)\n\n    if self._quiet_mode:\n      return\n\n    self._output_writer.Write('Processing completed.\\n')\n\n    if analysis_counter:\n      table_view = views.ViewsFactory.GetTableView(\n          self._views_format_type, title='Analysis reports generated')\n      for element, count in analysis_counter.most_common():\n        if element != 'total':\n          table_view.AddRow([element, count])\n\n      table_view.AddRow(['Total', analysis_counter['total']])\n      table_view.Write(self._output_writer)\n\n    storage_reader = storage_factory.StorageFactory.CreateStorageReaderForFile(\n        self._storage_file_path)\n    self._PrintAnalysisReportsDetails(storage_reader)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _GetKeys(self, data, keys, depth=1):\n    keys = set(keys)\n    match = {}\n\n    if depth == 1:\n      for key in keys:\n        match[key] = data[key]\n    else:\n      for _, parsed_key, parsed_value in self._RecurseKey(\n          data, depth=depth):\n        if parsed_key in keys:\n          match[parsed_key] = parsed_value\n          if set(match.keys()) == keys:\n            return match\n    return match", "response": "Helper function to return keys nested in a bencode dict."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nflatten nested dictionaries and lists by yielding their values. The hierarchy of a bencode file is a series of nested dictionaries and lists. This is a helper function helps plugins navigate the structure without having to reimplement their own recursive methods. This method implements an overridable depth limit to prevent processing extremely deeply nested dictionaries. If the limit is reached a debug message is logged indicating which key processing stopped on. Args: recur_item (object): object to be checked for additional nested items. root (str): the pathname of the current working key. depth (int): a counter to ensure we stop at the maximum recursion depth. Yields: tuple: containing: str: root str: key str: value", "response": "def _RecurseKey(self, recur_item, root='', depth=15):\n    \"\"\"Flattens nested dictionaries and lists by yielding their values.\n\n    The hierarchy of a bencode file is a series of nested dictionaries and\n    lists. This is a helper function helps plugins navigate the structure\n    without having to reimplement their own recursive methods.\n\n    This method implements an overridable depth limit to prevent processing\n    extremely deeply nested dictionaries. If the limit is reached a debug\n    message is logged indicating which key processing stopped on.\n\n    Args:\n      recur_item (object): object to be checked for additional nested items.\n      root (str): the pathname of the current working key.\n      depth (int): a counter to ensure we stop at the maximum recursion depth.\n\n    Yields:\n      tuple: containing:\n          str: root\n          str: key\n          str: value\n    \"\"\"\n    if depth < 1:\n      logger.debug('Recursion limit hit for key: {0:s}'.format(root))\n      return\n\n    if isinstance(recur_item, (list, tuple)):\n      for recur in recur_item:\n        for key in self._RecurseKey(recur, root, depth):\n          yield key\n      return\n\n    if not hasattr(recur_item, 'iteritems'):\n      return\n\n    for key, value in iter(recur_item.items()):\n      yield root, key, value\n      if isinstance(value, dict):\n        value = [value]\n      if isinstance(value, list):\n        for item in value:\n          if isinstance(item, dict):\n            for keyval in self._RecurseKey(\n                item, root=root + '/' + key, depth=depth - 1):\n              yield keyval"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef Process(self, parser_mediator, data, **kwargs):\n    if data is None:\n      raise ValueError('Data is not set.')\n\n    if not set(data.keys()).issuperset(self.BENCODE_KEYS):\n      raise errors.WrongBencodePlugin(self.NAME)\n\n    # This will raise if unhandled keyword arguments are passed.\n    super(BencodePlugin, self).Process(parser_mediator)\n\n    logger.debug('Bencode Plugin Used: {0:s}'.format(self.NAME))\n\n    self.GetEntries(parser_mediator, data=data)", "response": "Process the bencode file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse and validates the options.", "response": "def ParseOptions(cls, options, configuration_object):\n    \"\"\"Parses and validates options.\n\n    Args:\n      options (argparse.Namespace): parser options.\n      configuration_object (CLITool): object to be configured by the argument\n          helper.\n\n    Raises:\n      BadConfigObject: when the configuration object is of the wrong type.\n    \"\"\"\n    if not isinstance(configuration_object, tools.CLITool):\n      raise errors.BadConfigObject(\n          'Configuration object is not an instance of CLITool')\n\n    yara_rules_string = None\n\n    path = getattr(options, 'yara_rules_path', None)\n    if path:\n      try:\n        with io.open(path, 'rt', encoding='utf-8') as rules_file:\n          yara_rules_string = rules_file.read()\n\n      except IOError as exception:\n        raise errors.BadConfigObject(\n            'Unable to read Yara rules file: {0:s} with error: {1!s}'.format(\n                path, exception))\n\n      try:\n        # We try to parse the rules here, to check that the definitions are\n        # valid. We then pass the string definitions along to the workers, so\n        # that they don't need read access to the rules file.\n        yara.compile(source=yara_rules_string)\n\n      except yara.Error as exception:\n        raise errors.BadConfigObject(\n            'Unable to parse Yara rules in: {0:s} with error: {1!s}'.format(\n                path, exception))\n\n    setattr(configuration_object, '_yara_rules_string', yara_rules_string)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd an event to the internal storage.", "response": "def _AddEvent(self, event):\n    \"\"\"Adds an event.\n\n    Args:\n      event (EventObject): event.\n    \"\"\"\n    if hasattr(event, 'event_data_row_identifier'):\n      event_data_identifier = identifiers.SQLTableIdentifier(\n          self._CONTAINER_TYPE_EVENT_DATA,\n          event.event_data_row_identifier)\n      lookup_key = event_data_identifier.CopyToString()\n\n      event_data_identifier = self._event_data_identifier_mappings[lookup_key]\n      event.SetEventDataIdentifier(event_data_identifier)\n\n    # TODO: add event identifier mappings for event tags.\n\n    self._storage_writer.AddEvent(event)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds event data. Args: event_data (EventData): event data.", "response": "def _AddEventData(self, event_data):\n    \"\"\"Adds event data.\n\n    Args:\n      event_data (EventData): event data.\n    \"\"\"\n    identifier = event_data.GetIdentifier()\n    lookup_key = identifier.CopyToString()\n\n    self._storage_writer.AddEventData(event_data)\n\n    identifier = event_data.GetIdentifier()\n    self._event_data_identifier_mappings[lookup_key] = identifier"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves the container types to merge.", "response": "def _GetContainerTypes(self):\n    \"\"\"Retrieves the container types to merge.\n\n    Container types not defined in _CONTAINER_TYPES are ignored and not merged.\n\n    Specific container types reference other container types, such\n    as event referencing event data. The names are ordered to ensure the\n    attribute containers are merged in the correct order.\n\n    Returns:\n      list[str]: names of the container types to merge.\n    \"\"\"\n    self._cursor.execute(self._TABLE_NAMES_QUERY)\n    table_names = [row[0] for row in self._cursor.fetchall()]\n\n    return [\n        table_name for table_name in self._CONTAINER_TYPES\n        if table_name in table_names]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nopening the task storage for reading.", "response": "def _Open(self):\n    \"\"\"Opens the task storage for reading.\"\"\"\n    self._connection = sqlite3.connect(\n        self._path, detect_types=sqlite3.PARSE_DECLTYPES|sqlite3.PARSE_COLNAMES)\n    self._cursor = self._connection.cursor()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _ReadStorageMetadata(self):\n    query = 'SELECT key, value FROM metadata'\n    self._cursor.execute(query)\n\n    metadata_values = {row[0]: row[1] for row in self._cursor.fetchall()}\n\n    self._compression_format = metadata_values['compression_format']", "response": "Reads the task storage metadata."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprepare the task storage for merging the next container type.", "response": "def _PrepareForNextContainerType(self):\n    \"\"\"Prepares for the next container type.\n\n    This method prepares the task storage for merging the next container type.\n    It set the active container type, its add method and active cursor\n    accordingly.\n    \"\"\"\n    self._active_container_type = self._container_types.pop(0)\n\n    self._add_active_container_method = self._add_container_type_methods.get(\n        self._active_container_type)\n\n    query = 'SELECT _identifier, _data FROM {0:s}'.format(\n        self._active_container_type)\n    self._cursor.execute(query)\n\n    self._active_cursor = self._cursor"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef MergeAttributeContainers(\n      self, callback=None, maximum_number_of_containers=0):\n    \"\"\"Reads attribute containers from a task storage file into the writer.\n\n    Args:\n      callback (function[StorageWriter, AttributeContainer]): function to call\n          after each attribute container is deserialized.\n      maximum_number_of_containers (Optional[int]): maximum number of\n          containers to merge, where 0 represent no limit.\n\n    Returns:\n      bool: True if the entire task storage file has been merged.\n\n    Raises:\n      RuntimeError: if the add method for the active attribute container\n          type is missing.\n      OSError: if the task storage file cannot be deleted.\n      ValueError: if the maximum number of containers is a negative value.\n    \"\"\"\n    if maximum_number_of_containers < 0:\n      raise ValueError('Invalid maximum number of containers')\n\n    if not self._cursor:\n      self._Open()\n      self._ReadStorageMetadata()\n      self._container_types = self._GetContainerTypes()\n\n    number_of_containers = 0\n    while self._active_cursor or self._container_types:\n      if not self._active_cursor:\n        self._PrepareForNextContainerType()\n\n      if maximum_number_of_containers == 0:\n        rows = self._active_cursor.fetchall()\n      else:\n        number_of_rows = maximum_number_of_containers - number_of_containers\n        rows = self._active_cursor.fetchmany(size=number_of_rows)\n\n      if not rows:\n        self._active_cursor = None\n        continue\n\n      for row in rows:\n        identifier = identifiers.SQLTableIdentifier(\n            self._active_container_type, row[0])\n\n        if self._compression_format == definitions.COMPRESSION_FORMAT_ZLIB:\n          serialized_data = zlib.decompress(row[1])\n        else:\n          serialized_data = row[1]\n\n        attribute_container = self._DeserializeAttributeContainer(\n            self._active_container_type, serialized_data)\n        attribute_container.SetIdentifier(identifier)\n\n        if self._active_container_type == self._CONTAINER_TYPE_EVENT_TAG:\n          event_identifier = identifiers.SQLTableIdentifier(\n              self._CONTAINER_TYPE_EVENT,\n              attribute_container.event_row_identifier)\n          attribute_container.SetEventIdentifier(event_identifier)\n\n          del attribute_container.event_row_identifier\n\n        if callback:\n          callback(self._storage_writer, attribute_container)\n\n        self._add_active_container_method(attribute_container)\n\n        number_of_containers += 1\n\n      if (maximum_number_of_containers != 0 and\n          number_of_containers >= maximum_number_of_containers):\n        return False\n\n    self._Close()\n\n    os.remove(self._path)\n\n    return True", "response": "Merges the attribute containers from a task storage file into a single attribute container."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _GetAction(self, action, text):\n    # TODO: replace \"x in y\" checks by startswith if possible.\n    if 'airportdProcessDLILEvent' in action:\n      interface = text.split()[0]\n      return 'Interface {0:s} turn up.'.format(interface)\n\n    if 'doAutoJoin' in action:\n      match = self._CONNECTED_RE.match(text)\n      if match:\n        ssid = match.group(1)[1:-1]\n      else:\n        ssid = 'Unknown'\n      return 'Wifi connected to SSID {0:s}'.format(ssid)\n\n    if 'processSystemPSKAssoc' in action:\n      wifi_parameters = self._WIFI_PARAMETERS_RE.search(text)\n      if wifi_parameters:\n        ssid = wifi_parameters.group(1)\n        bssid = wifi_parameters.group(2)\n        security = wifi_parameters.group(3)\n        if not ssid:\n          ssid = 'Unknown'\n        if not bssid:\n          bssid = 'Unknown'\n        if not security:\n          security = 'Unknown'\n\n        return (\n            'New wifi configured. BSSID: {0:s}, SSID: {1:s}, '\n            'Security: {2:s}.').format(bssid, ssid, security)\n\n    return text", "response": "Parse the well known actions for easy reading."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _GetTimeElementsTuple(self, key, structure):\n    if key == 'turned_over_header':\n      month, day, hours, minutes, seconds = structure.date_time\n\n      milliseconds = 0\n    else:\n      _, month, day, hours, minutes, seconds, milliseconds = structure.date_time\n\n    # Note that dfdatetime_time_elements.TimeElements will raise ValueError\n    # for an invalid month.\n    month = timelib.MONTH_DICT.get(month.lower(), 0)\n\n    if month != 0 and month < self._last_month:\n      # Gap detected between years.\n      self._year_use += 1\n\n    return self._year_use, month, day, hours, minutes, seconds, milliseconds", "response": "Retrieves a time elements tuple from the structure."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _ParseLogLine(self, parser_mediator, key, structure):\n    time_elements_tuple = self._GetTimeElementsTuple(key, structure)\n\n    try:\n      date_time = dfdatetime_time_elements.TimeElementsInMilliseconds(\n          time_elements_tuple=time_elements_tuple)\n    except ValueError:\n      parser_mediator.ProduceExtractionWarning(\n          'invalid date time value: {0!s}'.format(structure.date_time))\n      return\n\n    self._last_month = time_elements_tuple[1]\n\n    event_data = MacWifiLogEventData()\n    event_data.agent = structure.agent\n    # Due to the use of CharsNotIn pyparsing structure contains whitespaces\n    # that need to be removed.\n    event_data.function = structure.function.strip()\n    event_data.text = structure.text\n\n    if key == 'known_function_logline':\n      event_data.action = self._GetAction(\n          event_data.function, event_data.text)\n\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_ADDED)\n    parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Parses a single log line and produce an event object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing a log record and produces events.", "response": "def ParseRecord(self, parser_mediator, key, structure):\n    \"\"\"Parses a log record structure and produces events.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      key (str): name of the parsed structure.\n      structure (pyparsing.ParseResults): structure of tokens derived from\n          a line of a text file.\n\n    Raises:\n      ParseError: when the structure type is unknown.\n    \"\"\"\n    if key not in self._SUPPORTED_KEYS:\n      raise errors.ParseError(\n          'Unable to parse record, unknown structure: {0:s}'.format(key))\n\n    self._ParseLogLine(parser_mediator, key, structure)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef VerifyStructure(self, parser_mediator, line):\n    self._last_month = 0\n    self._year_use = parser_mediator.GetEstimatedYear()\n\n    key = 'header'\n\n    try:\n      structure = self._MAC_WIFI_HEADER.parseString(line)\n    except pyparsing.ParseException:\n      structure = None\n\n    if not structure:\n      key = 'turned_over_header'\n\n      try:\n        structure = self._MAC_WIFI_TURNED_OVER_HEADER.parseString(line)\n      except pyparsing.ParseException:\n        structure = None\n\n    if not structure:\n      logger.debug('Not a Mac Wifi log file')\n      return False\n\n    time_elements_tuple = self._GetTimeElementsTuple(key, structure)\n\n    try:\n      dfdatetime_time_elements.TimeElementsInMilliseconds(\n          time_elements_tuple=time_elements_tuple)\n    except ValueError:\n      logger.debug(\n          'Not a Mac Wifi log file, invalid date and time: {0!s}'.format(\n              structure.date_time))\n      return False\n\n    self._last_month = time_elements_tuple[1]\n\n    return True", "response": "Verify that this file is a Mac Wifi log file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef GetMessages(self, formatter_mediator, event):\n    if self.DATA_TYPE != event.data_type:\n      raise errors.WrongFormatter('Unsupported data type: {0:s}.'.format(\n          event.data_type))\n\n    event_values = event.CopyToDict()\n\n    trigger_type = event_values.get('trigger_type', None)\n    if trigger_type is not None:\n      event_values['trigger_type'] = self._TRIGGER_TYPES.get(\n          trigger_type, '0x{0:04x}'.format(trigger_type))\n\n    return self._ConditionalFormatMessages(event_values)", "response": "Determines the formatted message strings for an event object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing a Distributed Tracking identifier.", "response": "def _ParseDistributedTrackingIdentifier(\n      self, parser_mediator, uuid_object, origin):\n    \"\"\"Extracts data from a Distributed Tracking identifier.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      uuid_object (uuid.UUID): UUID of the Distributed Tracking identifier.\n      origin (str): origin of the event (event source).\n\n    Returns:\n      str: UUID string of the Distributed Tracking identifier.\n    \"\"\"\n    if uuid_object.version == 1:\n      event_data = windows_events.WindowsDistributedLinkTrackingEventData(\n          uuid_object, origin)\n      date_time = dfdatetime_uuid_time.UUIDTime(timestamp=uuid_object.time)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_CREATION)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    return '{{{0!s}}}'.format(uuid_object)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ParseDestList(self, parser_mediator, olecf_item):\n    header_map = self._GetDataTypeMap('dest_list_header')\n\n    try:\n      header, entry_offset = self._ReadStructureFromFileObject(\n          olecf_item, 0, header_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.UnableToParseFile(\n          'Unable to parse DestList header with error: {0!s}'.format(\n              exception))\n\n    if header.format_version == 1:\n      entry_map = self._GetDataTypeMap('dest_list_entry_v1')\n    elif header.format_version in (3, 4):\n      entry_map = self._GetDataTypeMap('dest_list_entry_v3')\n    else:\n      parser_mediator.ProduceExtractionWarning(\n          'unsupported format version: {0:d}.'.format(header.format_version))\n      return\n\n    while entry_offset < olecf_item.size:\n      try:\n        entry, entry_data_size = self._ReadStructureFromFileObject(\n            olecf_item, entry_offset, entry_map)\n      except (ValueError, errors.ParseError) as exception:\n        raise errors.UnableToParseFile(\n            'Unable to parse DestList entry with error: {0!s}'.format(\n                exception))\n\n      display_name = 'DestList entry at offset: 0x{0:08x}'.format(entry_offset)\n\n      try:\n        droid_volume_identifier = self._ParseDistributedTrackingIdentifier(\n            parser_mediator, entry.droid_volume_identifier, display_name)\n\n      except (TypeError, ValueError) as exception:\n        droid_volume_identifier = ''\n        parser_mediator.ProduceExtractionWarning(\n            'unable to read droid volume identifier with error: {0!s}'.format(\n                exception))\n\n      try:\n        droid_file_identifier = self._ParseDistributedTrackingIdentifier(\n            parser_mediator, entry.droid_file_identifier, display_name)\n\n      except (TypeError, ValueError) as exception:\n        droid_file_identifier = ''\n        parser_mediator.ProduceExtractionWarning(\n            'unable to read droid file identifier with error: {0!s}'.format(\n                exception))\n\n      try:\n        birth_droid_volume_identifier = (\n            self._ParseDistributedTrackingIdentifier(\n                parser_mediator, entry.birth_droid_volume_identifier,\n                display_name))\n\n      except (TypeError, ValueError) as exception:\n        birth_droid_volume_identifier = ''\n        parser_mediator.ProduceExtractionWarning((\n            'unable to read birth droid volume identifier with error: '\n            '{0:s}').format(\n                exception))\n\n      try:\n        birth_droid_file_identifier = self._ParseDistributedTrackingIdentifier(\n            parser_mediator, entry.birth_droid_file_identifier, display_name)\n\n      except (TypeError, ValueError) as exception:\n        birth_droid_file_identifier = ''\n        parser_mediator.ProduceExtractionWarning((\n            'unable to read birth droid file identifier with error: '\n            '{0:s}').format(\n                exception))\n\n      if entry.last_modification_time == 0:\n        date_time = dfdatetime_semantic_time.SemanticTime('Not set')\n      else:\n        date_time = dfdatetime_filetime.Filetime(\n            timestamp=entry.last_modification_time)\n\n      event_data = AutomaticDestinationsDestListEntryEventData()\n      event_data.birth_droid_file_identifier = birth_droid_file_identifier\n      event_data.birth_droid_volume_identifier = birth_droid_volume_identifier\n      event_data.droid_file_identifier = droid_file_identifier\n      event_data.droid_volume_identifier = droid_volume_identifier\n      event_data.entry_number = entry.entry_number\n      event_data.hostname = entry.hostname.rstrip('\\x00')\n      event_data.offset = entry_offset\n      event_data.path = entry.path.rstrip('\\x00')\n      event_data.pin_status = entry.pin_status\n\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_MODIFICATION)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n      entry_offset += entry_data_size", "response": "Parses a DestList OLECF item."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing an OLECF file.", "response": "def Process(self, parser_mediator, root_item=None, **kwargs):\n    \"\"\"Parses an OLECF file.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      root_item (Optional[pyolecf.item]): root item of the OLECF file.\n\n    Raises:\n      ValueError: If the root_item is not set.\n    \"\"\"\n    # This will raise if unhandled keyword arguments are passed.\n    super(AutomaticDestinationsOLECFPlugin, self).Process(\n        parser_mediator, **kwargs)\n\n    if not root_item:\n      raise ValueError('Root item not set.')\n\n    for item in root_item.sub_items:\n      if item.name == 'DestList':\n        self.ParseDestList(parser_mediator, item)\n\n      elif self._RE_LNK_ITEM_NAME.match(item.name):\n        display_name = parser_mediator.GetDisplayName()\n        if display_name:\n          display_name = '{0:s} # {1:s}'.format(display_name, item.name)\n        else:\n          display_name = '# {0:s}'.format(item.name)\n\n        parser_mediator.AppendToParserChain(self._WINLNK_PARSER)\n        try:\n          item.seek(0, os.SEEK_SET)\n          self._WINLNK_PARSER.ParseFileLNKFile(\n              parser_mediator, item, display_name)\n        finally:\n          parser_mediator.PopFromParserChain()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _GetEarliestYearFromFileEntry(self):\n    file_entry = self.GetFileEntry()\n    if not file_entry:\n      return None\n\n    stat_object = file_entry.GetStat()\n\n    posix_time = getattr(stat_object, 'crtime', None)\n    if posix_time is None:\n      posix_time = getattr(stat_object, 'ctime', None)\n\n    # Gzip files don't store the creation or metadata modification times,\n    # but the modification time stored in the file is a good proxy.\n    if file_entry.TYPE_INDICATOR == dfvfs_definitions.TYPE_INDICATOR_GZIP:\n      posix_time = getattr(stat_object, 'mtime', None)\n\n    if posix_time is None:\n      logger.warning(\n          'Unable to determine earliest year from file stat information.')\n      return None\n\n    try:\n      year = timelib.GetYearFromPosixTime(\n          posix_time, timezone=self._knowledge_base.timezone)\n      return year\n    except ValueError as exception:\n      logger.error((\n          'Unable to determine earliest year from file stat information with '\n          'error: {0!s}').format(exception))\n      return None", "response": "Retrieves the year from the file entry date and time values."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nretrieve the inode from the inode value.", "response": "def _GetInode(self, inode_value):\n    \"\"\"Retrieves the inode from the inode value.\n\n    Args:\n      inode_value (int|str): inode, such as 1 or '27-128-1'.\n\n    Returns:\n      int: inode or -1 if the inode value cannot be converted to an integer.\n    \"\"\"\n    if isinstance(inode_value, py2to3.INTEGER_TYPES):\n      return inode_value\n\n    if isinstance(inode_value, float):\n      return int(inode_value)\n\n    if not isinstance(inode_value, py2to3.STRING_TYPES):\n      return -1\n\n    if b'-' in inode_value:\n      inode_value, _, _ = inode_value.partition(b'-')\n\n    try:\n      return int(inode_value, 10)\n    except ValueError:\n      return -1"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds an attribute that will be set on all events produced by this object.", "response": "def AddEventAttribute(self, attribute_name, attribute_value):\n    \"\"\"Adds an attribute that will be set on all events produced.\n\n    Setting attributes using this method will cause events produced via this\n    mediator to have an attribute with the provided name set with the\n    provided value.\n\n    Args:\n      attribute_name (str): name of the attribute to add.\n      attribute_value (str): value of the attribute to add.\n\n    Raises:\n      KeyError: if the event attribute is already set.\n    \"\"\"\n    if attribute_name in self._extra_event_attributes:\n      raise KeyError('Event attribute {0:s} already set'.format(\n          attribute_name))\n\n    self._extra_event_attributes[attribute_name] = attribute_value"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef GetDisplayName(self, file_entry=None):\n    if file_entry is None:\n      file_entry = self._file_entry\n\n    if file_entry is None:\n      raise ValueError('Missing file entry')\n\n    path_spec = getattr(file_entry, 'path_spec', None)\n\n    relative_path = path_helper.PathHelper.GetRelativePathForPathSpec(\n        path_spec, mount_path=self._mount_path)\n    if not relative_path:\n      return file_entry.name\n\n    return self.GetDisplayNameForPathSpec(path_spec)", "response": "Retrieves the display name for a file entry."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretrieves an estimate of the year for the current file entry.", "response": "def GetEstimatedYear(self):\n    \"\"\"Retrieves an estimate of the year.\n\n    This function determines the year in the following manner:\n    * see if the user provided a preferred year;\n    * see if knowledge base defines a year e.g. derived from preprocessing;\n    * determine the year based on the file entry metadata;\n    * default to the current year;\n\n    Returns:\n      int: estimated year.\n    \"\"\"\n    # TODO: improve this method to get a more reliable estimate.\n    # Preserve the year-less date and sort this out in the psort phase.\n    if self._preferred_year:\n      return self._preferred_year\n\n    if self._knowledge_base.year:\n      return self._knowledge_base.year\n\n    # TODO: Find a decent way to actually calculate the correct year\n    # instead of relying on stats object.\n    year = self._GetEarliestYearFromFileEntry()\n    if not year:\n      year = self._GetLatestYearFromFileEntry()\n\n    if not year:\n      year = timelib.GetCurrentYear()\n    return year"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nretrieves the name of the active file entry.", "response": "def GetFilename(self):\n    \"\"\"Retrieves the name of the active file entry.\n\n    Returns:\n      str: name of the active file entry or None.\n    \"\"\"\n    if not self._file_entry:\n      return None\n\n    data_stream = getattr(self._file_entry.path_spec, 'data_stream', None)\n    if data_stream:\n      return '{0:s}:{1:s}'.format(self._file_entry.name, data_stream)\n\n    return self._file_entry.name"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprocess an event before it written to the storage.", "response": "def ProcessEvent(\n      self, event, parser_chain=None, file_entry=None, query=None):\n    \"\"\"Processes an event before it written to the storage.\n\n    Args:\n      event (EventObject|EventData): event or event data.\n      parser_chain (Optional[str]): parsing chain up to this point.\n      file_entry (Optional[dfvfs.FileEntry]): file entry, where None will\n          use the current file entry set in the mediator.\n      query (Optional[str]): query that was used to obtain the event.\n\n    Raises:\n      KeyError: if there's an attempt to add a duplicate attribute value to the\n          event.\n    \"\"\"\n    # TODO: rename this to event.parser_chain or equivalent.\n    if not getattr(event, 'parser', None) and parser_chain:\n      event.parser = parser_chain\n\n    # TODO: deprecate text_prepend in favor of an event tag.\n    if not getattr(event, 'text_prepend', None) and self._text_prepend:\n      event.text_prepend = self._text_prepend\n\n    if file_entry is None:\n      file_entry = self._file_entry\n\n    display_name = None\n    if file_entry:\n      event.pathspec = file_entry.path_spec\n\n      if not getattr(event, 'filename', None):\n        path_spec = getattr(file_entry, 'path_spec', None)\n        event.filename = path_helper.PathHelper.GetRelativePathForPathSpec(\n            path_spec, mount_path=self._mount_path)\n\n      if not display_name:\n        # TODO: dfVFS refactor: move display name to output since the path\n        # specification contains the full information.\n        display_name = self.GetDisplayName(file_entry)\n\n      stat_object = file_entry.GetStat()\n      inode_value = getattr(stat_object, 'ino', None)\n      # TODO: refactor to ProcessEventData.\n      # Note that we use getattr here since event can be either EventObject\n      # or EventData.\n      if getattr(event, 'inode', None) is None and inode_value is not None:\n        event.inode = self._GetInode(inode_value)\n\n    if not getattr(event, 'display_name', None) and display_name:\n      event.display_name = display_name\n\n    if not getattr(event, 'hostname', None) and self.hostname:\n      event.hostname = self.hostname\n\n    if not getattr(event, 'username', None):\n      user_sid = getattr(event, 'user_sid', None)\n      username = self._knowledge_base.GetUsernameByIdentifier(user_sid)\n      if username:\n        event.username = username\n\n    if not getattr(event, 'query', None) and query:\n      event.query = query\n\n    for attribute, value in iter(self._extra_event_attributes.items()):\n      if hasattr(event, attribute):\n        raise KeyError('Event already has a value for {0:s}'.format(attribute))\n\n      setattr(event, attribute, value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nproduces an event source.", "response": "def ProduceEventSource(self, event_source):\n    \"\"\"Produces an event source.\n\n    Args:\n      event_source (EventSource): an event source.\n\n    Raises:\n      RuntimeError: when storage writer is not set.\n    \"\"\"\n    if not self._storage_writer:\n      raise RuntimeError('Storage writer not set.')\n\n    self._storage_writer.AddEventSource(event_source)\n    self._number_of_event_sources += 1\n\n    self.last_activity_timestamp = time.time()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ProduceEventWithEventData(self, event, event_data):\n    if event.timestamp is None:\n      raise errors.InvalidEvent('Event timestamp value not set.')\n\n    if event.timestamp < self._INT64_MIN or event.timestamp > self._INT64_MAX:\n      raise errors.InvalidEvent('Event timestamp value out of bounds.')\n\n    event_data_hash = event_data.GetAttributeValuesHash()\n    if event_data_hash != self._last_event_data_hash:\n      # Make a copy of the event data before adding additional values.\n      event_data = copy.deepcopy(event_data)\n\n      # TODO: refactor to ProcessEventData.\n      self.ProcessEvent(\n          event_data, parser_chain=self.GetParserChain(),\n          file_entry=self._file_entry)\n\n      self._storage_writer.AddEventData(event_data)\n\n      self._last_event_data_hash = event_data_hash\n      self._last_event_data_identifier = event_data.GetIdentifier()\n\n    if self._last_event_data_identifier:\n      event.SetEventDataIdentifier(self._last_event_data_identifier)\n\n    # TODO: remove this after structural fix is in place\n    # https://github.com/log2timeline/plaso/issues/1691\n    event.parser = self.GetParserChain()\n\n    self._storage_writer.AddEvent(event)\n    self._number_of_events += 1\n\n    self.last_activity_timestamp = time.time()", "response": "Produces an event with the given event data."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ProduceExtractionWarning(self, message, path_spec=None):\n    if not self._storage_writer:\n      raise RuntimeError('Storage writer not set.')\n\n    if not path_spec and self._file_entry:\n      path_spec = self._file_entry.path_spec\n\n    parser_chain = self.GetParserChain()\n    warning = warnings.ExtractionWarning(\n        message=message, parser_chain=parser_chain, path_spec=path_spec)\n    self._storage_writer.AddWarning(warning)\n    self._number_of_warnings += 1\n\n    self.last_activity_timestamp = time.time()", "response": "Produces an extraction warning."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nremoving an attribute from being set on all events produced by this event.", "response": "def RemoveEventAttribute(self, attribute_name):\n    \"\"\"Removes an attribute from being set on all events produced.\n\n    Args:\n      attribute_name (str): name of the attribute to remove.\n\n    Raises:\n      KeyError: if the event attribute is not set.\n    \"\"\"\n    if attribute_name not in self._extra_event_attributes:\n      raise KeyError('Event attribute: {0:s} not set'.format(attribute_name))\n\n    del self._extra_event_attributes[attribute_name]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef SampleMemoryUsage(self, parser_name):\n    if self._memory_profiler:\n      used_memory = self._process_information.GetUsedMemory() or 0\n      self._memory_profiler.Sample(parser_name, used_memory)", "response": "Takes a sample of the memory usage for profiling."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef SetInputSourceConfiguration(self, configuration):\n    mount_path = configuration.mount_path\n\n    # Remove a trailing path separator from the mount path so the relative\n    # paths will start with a path separator.\n    if mount_path and mount_path.endswith(os.sep):\n      mount_path = mount_path[:-1]\n\n    self._mount_path = mount_path", "response": "Sets the input source configuration settings."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the storage writer.", "response": "def SetStorageWriter(self, storage_writer):\n    \"\"\"Sets the storage writer.\n\n    Args:\n      storage_writer (StorageWriter): storage writer.\n    \"\"\"\n    self._storage_writer = storage_writer\n\n    # Reset the last event data information. Each storage file should\n    # contain event data for their events.\n    self._last_event_data_hash = None\n    self._last_event_data_identifier = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef StartProfiling(self, configuration, identifier, process_information):\n    if not configuration:\n      return\n\n    if configuration.HaveProfileParsers():\n      identifier = '{0:s}-parsers'.format(identifier)\n\n      self._cpu_time_profiler = profilers.CPUTimeProfiler(\n          identifier, configuration)\n      self._cpu_time_profiler.Start()\n\n      self._memory_profiler = profilers.MemoryProfiler(\n          identifier, configuration)\n      self._memory_profiler.Start()\n\n    self._process_information = process_information", "response": "Starts profiling.\n\n    Args:\n      configuration (ProfilingConfiguration): profiling configuration.\n      identifier (str): identifier of the profiling session used to create\n          the sample filename.\n      process_information (ProcessInfo): process information."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef CompileFilter(self, filter_expression):\n    filter_parser = pfilter.BaseParser(filter_expression).Parse()\n    matcher = filter_parser.Compile(pfilter.PlasoAttributeFilterImplementation)\n\n    self._filter_expression = filter_expression\n    self._matcher = matcher", "response": "Compiles the filter expression."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef Match(self, event):\n    if not self._matcher:\n      return True\n\n    self._decision = self._matcher.Matches(event)\n    return self._decision", "response": "Determines if an event matches the filter."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing a structure of tokens derived from a line of a text file. Args: parser_mediator (ParserMediator): mediates interactions between parsers and other components, such as storage and dfvfs. key (str): name of the parsed structure. structure (pyparsing.ParseResults): structure of tokens derived from a line of a text file. Raises: ParseError: when the structure type is unknown.", "response": "def ParseRecord(self, parser_mediator, key, structure):\n    \"\"\"Parses a structure of tokens derived from a line of a text file.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      key (str): name of the parsed structure.\n      structure (pyparsing.ParseResults): structure of tokens derived from\n          a line of a text file.\n\n    Raises:\n      ParseError: when the structure type is unknown.\n    \"\"\"\n    if key != 'line':\n      raise errors.ParseError(\n          'Unable to parse record, unknown structure: {0:s}'.format(key))\n\n    msg_value = structure.get('msg')\n    if not msg_value:\n      parser_mediator.ProduceExtractionWarning(\n          'missing msg value: {0!s}'.format(structure))\n      return\n\n    try:\n      seconds = int(msg_value[0], 10)\n    except ValueError:\n      parser_mediator.ProduceExtractionWarning(\n          'unsupported number of seconds in msg value: {0!s}'.format(\n              structure))\n      return\n\n    try:\n      milliseconds = int(msg_value[1], 10)\n    except ValueError:\n      parser_mediator.ProduceExtractionWarning(\n          'unsupported number of milliseconds in msg value: {0!s}'.format(\n              structure))\n      return\n\n    timestamp = ((seconds * 1000) + milliseconds) * 1000\n    body_text = structure[2][0]\n\n    try:\n      # Try to parse the body text as key value pairs. Note that not\n      # all log lines will be properly formatted key value pairs.\n      key_value_dict = self._SELINUX_KEY_VALUE_DICT.parseString(body_text)\n    except pyparsing.ParseException:\n      key_value_dict = {}\n\n    event_data = SELinuxLogEventData()\n    event_data.audit_type = structure.get('type', None)\n    event_data.body = body_text\n    event_data.pid = key_value_dict.get('pid', None)\n    # TODO: pass line number to offset or remove.\n    event_data.offset = 0\n\n    event = time_events.TimestampEvent(\n        timestamp, definitions.TIME_DESCRIPTION_WRITTEN)\n    parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _AddsAnalysisProcessStatusTableRow(self, process_status, table_view):\n    used_memory = self._FormatSizeInUnitsOf1024(process_status.used_memory)\n\n    events = ''\n    if (process_status.number_of_consumed_events is not None and\n        process_status.number_of_consumed_events_delta is not None):\n      events = '{0:d} ({1:d})'.format(\n          process_status.number_of_consumed_events,\n          process_status.number_of_consumed_events_delta)\n\n    event_tags = ''\n    if (process_status.number_of_produced_event_tags is not None and\n        process_status.number_of_produced_event_tags_delta is not None):\n      event_tags = '{0:d} ({1:d})'.format(\n          process_status.number_of_produced_event_tags,\n          process_status.number_of_produced_event_tags_delta)\n\n    reports = ''\n    if (process_status.number_of_produced_reports is not None and\n        process_status.number_of_produced_reports_delta is not None):\n      reports = '{0:d} ({1:d})'.format(\n          process_status.number_of_produced_reports,\n          process_status.number_of_produced_reports_delta)\n\n    table_view.AddRow([\n        process_status.identifier, process_status.pid, process_status.status,\n        used_memory, events, event_tags, reports])", "response": "Adds an analysis process status table row."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding an extraction process status table row.", "response": "def _AddExtractionProcessStatusTableRow(self, process_status, table_view):\n    \"\"\"Adds an extraction process status table row.\n\n    Args:\n      process_status (ProcessStatus): processing status.\n      table_view (CLITabularTableView): table view.\n    \"\"\"\n    used_memory = self._FormatSizeInUnitsOf1024(process_status.used_memory)\n\n    sources = ''\n    if (process_status.number_of_produced_sources is not None and\n        process_status.number_of_produced_sources_delta is not None):\n      sources = '{0:d} ({1:d})'.format(\n          process_status.number_of_produced_sources,\n          process_status.number_of_produced_sources_delta)\n\n    events = ''\n    if (process_status.number_of_produced_events is not None and\n        process_status.number_of_produced_events_delta is not None):\n      events = '{0:d} ({1:d})'.format(\n          process_status.number_of_produced_events,\n          process_status.number_of_produced_events_delta)\n\n    # TODO: shorten display name to fit in 80 chars and show the filename.\n\n    table_view.AddRow([\n        process_status.identifier, process_status.pid, process_status.status,\n        used_memory, sources, events, process_status.display_name])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _ClearScreen(self):\n    if self._have_ansi_support:\n      # ANSI escape sequence to clear screen.\n      self._output_writer.Write('\\033[2J')\n      # ANSI escape sequence to move cursor to top left.\n      self._output_writer.Write('\\033[H')\n\n    elif win32console:\n      # This version of Windows cmd.exe does not support ANSI escape codes, thus\n      # instead we fill the console screen buffer with spaces. The downside of\n      # this approach is an annoying flicker.\n      top_left_coordinate = win32console.PyCOORDType(0, 0)\n      screen_buffer = win32console.GetStdHandle(win32api.STD_OUTPUT_HANDLE)\n      screen_buffer_information = screen_buffer.GetConsoleScreenBufferInfo()\n\n      screen_buffer_attributes = screen_buffer_information['Attributes']\n      screen_buffer_size = screen_buffer_information['Size']\n      console_size = screen_buffer_size.X * screen_buffer_size.Y\n\n      screen_buffer.FillConsoleOutputCharacter(\n          ' ', console_size, top_left_coordinate)\n      screen_buffer.FillConsoleOutputAttribute(\n          screen_buffer_attributes, console_size, top_left_coordinate)\n      screen_buffer.SetConsoleCursorPosition(top_left_coordinate)", "response": "Clears the terminal screen."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _FormatSizeInUnitsOf1024(self, size):\n    magnitude_1024 = 0\n    used_memory_1024 = float(size)\n    while used_memory_1024 >= 1024:\n      used_memory_1024 /= 1024\n      magnitude_1024 += 1\n\n    if 0 < magnitude_1024 <= 7:\n      return '{0:.1f} {1:s}'.format(\n          used_memory_1024, self._UNITS_1024[magnitude_1024])\n\n    return '{0:d} B'.format(size)", "response": "Formats a number of bytes in units of 1024."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprint the analysis status header.", "response": "def _PrintAnalysisStatusHeader(self, processing_status):\n    \"\"\"Prints the analysis status header.\n\n    Args:\n      processing_status (ProcessingStatus): processing status.\n    \"\"\"\n    self._output_writer.Write(\n        'Storage file\\t\\t: {0:s}\\n'.format(self._storage_file_path))\n\n    self._PrintProcessingTime(processing_status)\n\n    if processing_status and processing_status.events_status:\n      self._PrintEventsStatus(processing_status.events_status)\n\n    self._output_writer.Write('\\n')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _PrintAnalysisStatusUpdateLinear(self, processing_status):\n    for worker_status in processing_status.workers_status:\n      status_line = (\n          '{0:s} (PID: {1:d}) - events consumed: {2:d} - running: '\n          '{3!s}\\n').format(\n              worker_status.identifier, worker_status.pid,\n              worker_status.number_of_consumed_events,\n              worker_status.status not in definitions.ERROR_STATUS_INDICATORS)\n      self._output_writer.Write(status_line)", "response": "Prints an analysis status update in linear mode."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprinting an analysis status update in window mode.", "response": "def _PrintAnalysisStatusUpdateWindow(self, processing_status):\n    \"\"\"Prints an analysis status update in window mode.\n\n    Args:\n      processing_status (ProcessingStatus): processing status.\n    \"\"\"\n    if self._stdout_output_writer:\n      self._ClearScreen()\n\n    output_text = 'plaso - {0:s} version {1:s}\\n\\n'.format(\n        self._tool_name, plaso.__version__)\n    self._output_writer.Write(output_text)\n\n    self._PrintAnalysisStatusHeader(processing_status)\n\n    table_view = views.CLITabularTableView(column_names=[\n        'Identifier', 'PID', 'Status', 'Memory', 'Events', 'Tags',\n        'Reports'], column_sizes=[23, 7, 15, 15, 15, 15, 0])\n\n    self._AddsAnalysisProcessStatusTableRow(\n        processing_status.foreman_status, table_view)\n\n    for worker_status in processing_status.workers_status:\n      self._AddsAnalysisProcessStatusTableRow(worker_status, table_view)\n\n    table_view.Write(self._output_writer)\n    self._output_writer.Write('\\n')\n\n    if processing_status.aborted:\n      self._output_writer.Write(\n          'Processing aborted - waiting for clean up.\\n\\n')\n\n    if self._stdout_output_writer:\n      # We need to explicitly flush stdout to prevent partial status updates.\n      sys.stdout.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprinting an extraction status update in linear mode.", "response": "def _PrintExtractionStatusUpdateLinear(self, processing_status):\n    \"\"\"Prints an extraction status update in linear mode.\n\n    Args:\n      processing_status (ProcessingStatus): processing status.\n    \"\"\"\n    for worker_status in processing_status.workers_status:\n      status_line = (\n          '{0:s} (PID: {1:d}) - events produced: {2:d} - file: {3:s} '\n          '- running: {4!s}\\n').format(\n              worker_status.identifier, worker_status.pid,\n              worker_status.number_of_produced_events,\n              worker_status.display_name,\n              worker_status.status not in definitions.ERROR_STATUS_INDICATORS)\n      self._output_writer.Write(status_line)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprint an extraction status update in window mode.", "response": "def _PrintExtractionStatusUpdateWindow(self, processing_status):\n    \"\"\"Prints an extraction status update in window mode.\n\n    Args:\n      processing_status (ProcessingStatus): processing status.\n    \"\"\"\n    if self._stdout_output_writer:\n      self._ClearScreen()\n\n    output_text = 'plaso - {0:s} version {1:s}\\n\\n'.format(\n        self._tool_name, plaso.__version__)\n    self._output_writer.Write(output_text)\n\n    self.PrintExtractionStatusHeader(processing_status)\n\n    table_view = views.CLITabularTableView(column_names=[\n        'Identifier', 'PID', 'Status', 'Memory', 'Sources', 'Events',\n        'File'], column_sizes=[15, 7, 15, 15, 15, 15, 0])\n\n    self._AddExtractionProcessStatusTableRow(\n        processing_status.foreman_status, table_view)\n\n    for worker_status in processing_status.workers_status:\n      self._AddExtractionProcessStatusTableRow(worker_status, table_view)\n\n    table_view.Write(self._output_writer)\n    self._output_writer.Write('\\n')\n\n    if processing_status.aborted:\n      self._output_writer.Write(\n          'Processing aborted - waiting for clean up.\\n\\n')\n\n    # TODO: remove update flicker. For win32console we could set the cursor\n    # top left, write the table, clean the remainder of the screen buffer\n    # and set the cursor at the end of the table.\n    if self._stdout_output_writer:\n      # We need to explicitly flush stdout to prevent partial status updates.\n      sys.stdout.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprint the status of the events.", "response": "def _PrintEventsStatus(self, events_status):\n    \"\"\"Prints the status of the events.\n\n    Args:\n      events_status (EventsStatus): events status.\n    \"\"\"\n    if events_status:\n      table_view = views.CLITabularTableView(\n          column_names=['Events:', 'Filtered', 'In time slice', 'Duplicates',\n                        'MACB grouped', 'Total'],\n          column_sizes=[15, 15, 15, 15, 15, 0])\n\n      table_view.AddRow([\n          '', events_status.number_of_filtered_events,\n          events_status.number_of_events_from_time_slice,\n          events_status.number_of_duplicate_events,\n          events_status.number_of_macb_grouped_events,\n          events_status.total_number_of_events])\n\n      self._output_writer.Write('\\n')\n      table_view.Write(self._output_writer)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprint the processing time.", "response": "def _PrintProcessingTime(self, processing_status):\n    \"\"\"Prints the processing time.\n\n    Args:\n      processing_status (ProcessingStatus): processing status.\n    \"\"\"\n    if not processing_status:\n      processing_time = '00:00:00'\n    else:\n      processing_time = time.time() - processing_status.start_time\n      time_struct = time.gmtime(processing_time)\n      processing_time = time.strftime('%H:%M:%S', time_struct)\n\n    self._output_writer.Write(\n        'Processing time\\t\\t: {0:s}\\n'.format(processing_time))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprint the status of the tasks.", "response": "def _PrintTasksStatus(self, processing_status):\n    \"\"\"Prints the status of the tasks.\n\n    Args:\n      processing_status (ProcessingStatus): processing status.\n    \"\"\"\n    if processing_status and processing_status.tasks_status:\n      tasks_status = processing_status.tasks_status\n\n      table_view = views.CLITabularTableView(\n          column_names=['Tasks:', 'Queued', 'Processing', 'Merging',\n                        'Abandoned', 'Total'],\n          column_sizes=[15, 7, 15, 15, 15, 0])\n\n      table_view.AddRow([\n          '', tasks_status.number_of_queued_tasks,\n          tasks_status.number_of_tasks_processing,\n          tasks_status.number_of_tasks_pending_merge,\n          tasks_status.number_of_abandoned_tasks,\n          tasks_status.total_number_of_tasks])\n\n      self._output_writer.Write('\\n')\n      table_view.Write(self._output_writer)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef GetAnalysisStatusUpdateCallback(self):\n    if self._mode == self.MODE_LINEAR:\n      return self._PrintAnalysisStatusUpdateLinear\n\n    if self._mode == self.MODE_WINDOW:\n      return self._PrintAnalysisStatusUpdateWindow\n\n    return None", "response": "Retrieves the analysis status update callback function."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef GetExtractionStatusUpdateCallback(self):\n    if self._mode == self.MODE_LINEAR:\n      return self._PrintExtractionStatusUpdateLinear\n\n    if self._mode == self.MODE_WINDOW:\n      return self._PrintExtractionStatusUpdateWindow\n\n    return None", "response": "Retrieves the extraction status update callback function."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprinting the extraction status header.", "response": "def PrintExtractionStatusHeader(self, processing_status):\n    \"\"\"Prints the extraction status header.\n\n    Args:\n      processing_status (ProcessingStatus): processing status.\n    \"\"\"\n    self._output_writer.Write(\n        'Source path\\t\\t: {0:s}\\n'.format(self._source_path))\n    self._output_writer.Write(\n        'Source type\\t\\t: {0:s}\\n'.format(self._source_type))\n\n    if self._artifact_filters:\n      artifacts_string = ', '.join(self._artifact_filters)\n      self._output_writer.Write('Artifact filters\\t: {0:s}\\n'.format(\n          artifacts_string))\n    if self._filter_file:\n      self._output_writer.Write('Filter file\\t\\t: {0:s}\\n'.format(\n          self._filter_file))\n\n    self._PrintProcessingTime(processing_status)\n    self._PrintTasksStatus(processing_status)\n    self._output_writer.Write('\\n')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef PrintExtractionSummary(self, processing_status):\n    if not processing_status:\n      self._output_writer.Write(\n          'WARNING: missing processing status information.\\n')\n\n    elif not processing_status.aborted:\n      if processing_status.error_path_specs:\n        self._output_writer.Write('Processing completed with errors.\\n')\n      else:\n        self._output_writer.Write('Processing completed.\\n')\n\n      number_of_warnings = (\n          processing_status.foreman_status.number_of_produced_warnings)\n      if number_of_warnings:\n        output_text = '\\n'.join([\n            '',\n            ('Number of warnings generated while extracting events: '\n             '{0:d}.').format(number_of_warnings),\n            '',\n            'Use pinfo to inspect warnings in more detail.',\n            ''])\n        self._output_writer.Write(output_text)\n\n      if processing_status.error_path_specs:\n        output_text = '\\n'.join([\n            '',\n            'Path specifications that could not be processed:',\n            ''])\n        self._output_writer.Write(output_text)\n        for path_spec in processing_status.error_path_specs:\n          self._output_writer.Write(path_spec.comparable)\n          self._output_writer.Write('\\n')\n\n    self._output_writer.Write('\\n')", "response": "Prints a summary of the extraction."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the source information.", "response": "def SetSourceInformation(\n      self, source_path, source_type, artifact_filters=None, filter_file=None):\n    \"\"\"Sets the source information.\n\n    Args:\n      source_path (str): path of the source.\n      source_type (str): source type.\n      artifact_filters (Optional[list[str]]): names of artifact definitions to\n          use as filters.\n      filter_file (Optional[str]): filter file.\n    \"\"\"\n    self._artifact_filters = artifact_filters\n    self._filter_file = filter_file\n    self._source_path = source_path\n    self._source_type = self._SOURCE_TYPES.get(source_type, 'UNKNOWN')"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse a cookie row.", "response": "def ParseCookieRow(self, parser_mediator, query, row, **unused_kwargs):\n    \"\"\"Parses a cookie row.\n\n    Args:\n      parser_mediator (ParserMediator): parser mediator.\n      query (str): query that created the row.\n      row (sqlite3.Row): row resulting from the query.\n    \"\"\"\n    query_hash = hash(query)\n\n    cookie_name = self._GetRowValue(query_hash, row, 'name')\n    cookie_data = self._GetRowValue(query_hash, row, 'value')\n\n    hostname = self._GetRowValue(query_hash, row, 'host_key')\n    if hostname.startswith('.'):\n      hostname = hostname[1:]\n\n    httponly = self._GetRowValue(query_hash, row, 'httponly')\n    path = self._GetRowValue(query_hash, row, 'path')\n    persistent = self._GetRowValue(query_hash, row, 'persistent')\n    secure = self._GetRowValue(query_hash, row, 'secure')\n\n    if secure:\n      scheme = 'https'\n    else:\n      scheme = 'http'\n\n    url = '{0:s}://{1:s}{2:s}'.format(scheme, hostname, path)\n\n    event_data = ChromeCookieEventData()\n    event_data.cookie_name = cookie_name\n    event_data.data = cookie_data\n    event_data.host = hostname\n    event_data.httponly = bool(httponly)\n    event_data.path = path\n    event_data.persistent = bool(persistent)\n    event_data.query = query\n    event_data.secure = bool(secure)\n    event_data.url = url\n\n    timestamp = self._GetRowValue(query_hash, row, 'creation_utc')\n    date_time = dfdatetime_webkit_time.WebKitTime(timestamp=timestamp)\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_CREATION)\n    parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    timestamp = self._GetRowValue(query_hash, row, 'last_access_utc')\n    date_time = dfdatetime_webkit_time.WebKitTime(timestamp=timestamp)\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_LAST_ACCESS)\n    parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    timestamp = self._GetRowValue(query_hash, row, 'expires_utc')\n    if timestamp:\n      date_time = dfdatetime_webkit_time.WebKitTime(timestamp=timestamp)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_EXPIRATION)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    for plugin in self._cookie_plugins:\n      if cookie_name != plugin.COOKIE_NAME:\n        continue\n\n      try:\n        plugin.UpdateChainAndProcess(\n            parser_mediator, cookie_data=cookie_data, cookie_name=cookie_name,\n            url=url)\n\n      except Exception as exception:  # pylint: disable=broad-except\n        parser_mediator.ProduceExtractionWarning(\n            'plugin: {0:s} unable to parse cookie with error: {1!s}'.format(\n                plugin.NAME, exception))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd command line arguments to an argument group.", "response": "def AddArguments(cls, argument_group):\n    \"\"\"Adds command line arguments the helper supports to an argument group.\n\n    This function takes an argument parser or an argument group object and adds\n    to it all the command line arguments this helper supports.\n\n    Args:\n      argument_group (argparse._ArgumentGroup|argparse.ArgumentParser):\n          argparse group.\n    \"\"\"\n    default_fields = ','.join(cls._DEFAULT_FIELDS)\n    argument_group.add_argument(\n        '--fields', dest='fields', type=str, action='store',\n        default=default_fields, help=(\n            'Defines which fields should be included in the output.'))\n\n    default_fields = ', '.join(cls._DEFAULT_FIELDS)\n    argument_group.add_argument(\n        '--additional_fields', dest='additional_fields', type=str,\n        action='store', default='', help=(\n            'Defines extra fields to be included in the output, in addition to'\n            ' the default fields, which are {0:s}.'.format(default_fields)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses and validates the options.", "response": "def ParseOptions(cls, options, output_module):  # pylint: disable=arguments-differ\n    \"\"\"Parses and validates options.\n\n    Args:\n      options (argparse.Namespace): parser options.\n      output_module (OutputModule): output module to configure.\n\n    Raises:\n      BadConfigObject: when the output module object is of the wrong type.\n      BadConfigOption: when the output filename was not provided.\n    \"\"\"\n    if not isinstance(output_module, dynamic.DynamicOutputModule):\n      raise errors.BadConfigObject(\n          'Output module is not an instance of DynamicOutputModule')\n\n    default_fields = ','.join(cls._DEFAULT_FIELDS)\n    fields = cls._ParseStringOption(\n        options, 'fields', default_value=default_fields)\n\n    additional_fields = cls._ParseStringOption(\n        options, 'additional_fields')\n\n    if additional_fields:\n      fields = '{0:s},{1:s}'.format(fields, additional_fields)\n\n    output_module.SetFields([\n        field_name.strip() for field_name in fields.split(',')])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ExtractEvents(self, parser_mediator, registry_key, **kwargs):\n    for subkey in registry_key.GetSubkeys():\n      drive_letter = subkey.name\n      if not drive_letter:\n        continue\n\n      values_dict = {\n          'DriveLetter': drive_letter,\n          'Type': 'Mapped Drive'}\n\n      # Get the remote path if it exists.\n      remote_path_value = subkey.GetValueByName('RemotePath')\n      if remote_path_value:\n        remote_path = remote_path_value.GetDataAsObject()\n\n        if remote_path.startswith('\\\\\\\\'):\n          server_name, _, share_name = remote_path[2:].partition('\\\\')\n          values_dict['RemoteServer'] = server_name\n          values_dict['ShareName'] = '\\\\{0:s}'.format(\n              share_name.replace('#', '\\\\'))\n\n      event_data = windows_events.WindowsRegistryEventData()\n      event_data.key_path = registry_key.path\n      event_data.offset = subkey.offset\n      event_data.regvalue = values_dict\n      event_data.source_append = self._SOURCE_APPEND\n      event_data.urls = self.URLS\n\n      event = time_events.DateTimeValuesEvent(\n          subkey.last_written_time, definitions.TIME_DESCRIPTION_WRITTEN)\n      parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Extracts events from a Windows Registry key."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nbuilds the event tag index.", "response": "def _Build(self, storage_file):\n    \"\"\"Builds the event tag index.\n\n    Args:\n      storage_file (BaseStorageFile): storage file.\n    \"\"\"\n    self._index = {}\n    for event_tag in storage_file.GetEventTags():\n      self.SetEventTag(event_tag)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nretrieve the most recently updated event tag for an event.", "response": "def GetEventTagByIdentifier(self, storage_file, event_identifier):\n    \"\"\"Retrieves the most recently updated event tag for an event.\n\n    Args:\n      storage_file (BaseStorageFile): storage file.\n      event_identifier (AttributeContainerIdentifier): event attribute\n          container identifier.\n\n    Returns:\n      EventTag: event tag or None if the event has no event tag.\n    \"\"\"\n    if not self._index:\n      self._Build(storage_file)\n\n    lookup_key = event_identifier.CopyToString()\n    event_tag_identifier = self._index.get(lookup_key, None)\n    if not event_tag_identifier:\n      return None\n\n    return storage_file.GetEventTagByIdentifier(event_tag_identifier)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef SetEventTag(self, event_tag):\n    event_identifier = event_tag.GetEventIdentifier()\n\n    lookup_key = event_identifier.CopyToString()\n    self._index[lookup_key] = event_tag.GetIdentifier()", "response": "Sets an event tag in the index."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _ExpandUsersHomeDirectoryPathSegments(\n      cls, path_segments, path_separator, user_accounts):\n    \"\"\"Expands a path to contain all users home or profile directories.\n\n    Expands the artifacts path variable \"%%users.homedir%%\" or\n    \"%%users.userprofile%%\".\n\n    Args:\n      path_segments (list[str]): path segments.\n      path_separator (str): path segment separator.\n      user_accounts (list[UserAccountArtifact]): user accounts.\n\n    Returns:\n      list[str]: paths returned for user accounts without a drive indicator.\n    \"\"\"\n    if not path_segments:\n      return []\n\n    user_paths = []\n\n    first_path_segment = path_segments[0].lower()\n    if first_path_segment not in ('%%users.homedir%%', '%%users.userprofile%%'):\n      if cls._IsWindowsDrivePathSegment(path_segments[0]):\n        path_segments[0] = ''\n\n      user_path = path_separator.join(path_segments)\n      user_paths.append(user_path)\n\n    else:\n      for user_account in user_accounts:\n        user_path_segments = user_account.GetUserDirectoryPathSegments()\n\n        if not user_path_segments:\n          continue\n\n        if cls._IsWindowsDrivePathSegment(user_path_segments[0]):\n          user_path_segments[0] = ''\n\n        # Prevent concatenating two consecutive path segment separators.\n        if not user_path_segments[-1]:\n          user_path_segments.pop()\n\n        user_path_segments.extend(path_segments[1:])\n\n        user_path = path_separator.join(user_path_segments)\n        user_paths.append(user_path)\n\n    return user_paths", "response": "Expands a path to contain all users home or profile directories."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _ExpandUsersVariablePathSegments(\n      cls, path_segments, path_separator, user_accounts):\n    \"\"\"Expands path segments with a users variable, e.g. %%users.homedir%%.\n\n    Args:\n      path_segments (list[str]): path segments.\n      path_separator (str): path segment separator.\n      user_accounts (list[UserAccountArtifact]): user accounts.\n\n    Returns:\n      list[str]: paths for which the users variables have been expanded.\n    \"\"\"\n    if not path_segments:\n      return []\n\n    path_segments_lower = [\n        path_segment.lower() for path_segment in path_segments]\n\n    if path_segments_lower[0] in ('%%users.homedir%%', '%%users.userprofile%%'):\n      return cls._ExpandUsersHomeDirectoryPathSegments(\n          path_segments, path_separator, user_accounts)\n\n    path_expansions = cls._PATH_EXPANSIONS_PER_USERS_VARIABLE.get(\n        path_segments[0], None)\n\n    if path_expansions:\n      expanded_paths = []\n\n      for path_expansion in path_expansions:\n        expanded_path_segments = list(path_expansion)\n        expanded_path_segments.extend(path_segments[1:])\n\n        paths = cls._ExpandUsersVariablePathSegments(\n            expanded_path_segments, path_separator, user_accounts)\n        expanded_paths.extend(paths)\n\n      return expanded_paths\n\n    if cls._IsWindowsDrivePathSegment(path_segments[0]):\n      path_segments[0] = ''\n\n    # TODO: add support for %%users.username%%\n    path = path_separator.join(path_segments)\n    return [path]", "response": "Expands a list of path segments with a users variable."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndetermines if the path segment contains a Windows Drive indicator.", "response": "def _IsWindowsDrivePathSegment(cls, path_segment):\n    \"\"\"Determines if the path segment contains a Windows Drive indicator.\n\n    A drive indicator can be a drive letter or %SystemDrive%.\n\n    Args:\n      path_segment (str): path segment.\n\n    Returns:\n      bool: True if the path segment contains a Windows Drive indicator.\n    \"\"\"\n    if (len(path_segment) == 2 and path_segment[1] == ':' and\n        path_segment[0].isalpha()):\n      return True\n\n    path_segment = path_segment.upper()\n    return path_segment in ('%%ENVIRON_SYSTEMDRIVE%%', '%SYSTEMDRIVE%')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef AppendPathEntries(\n      cls, path, path_separator, number_of_wildcards, skip_first):\n    \"\"\"Appends glob wildcards to a path.\n\n    This function will append glob wildcards \"*\" to a path, returning paths\n    with an additional glob wildcard up to the specified number. E.g. given\n    the path \"/tmp\" and a number of 2 wildcards, this function will return\n    \"tmp/*\", \"tmp/*/*\". When skip_first is true the path with the first\n    wildcard is not returned as a result.\n\n    Args:\n      path (str): path to append glob wildcards to.\n      path_separator (str): path segment separator.\n      number_of_wildcards (int): number of glob wildcards to append.\n      skip_first (bool): True if the the first path with glob wildcard should\n          be skipped as a result.\n\n    Returns:\n      list[str]: paths with glob wildcards.\n    \"\"\"\n    if path[-1] == path_separator:\n      path = path[:-1]\n\n    if skip_first:\n      path = ''.join([path, path_separator, '*'])\n      number_of_wildcards -= 1\n\n    paths = []\n    for _ in range(0, number_of_wildcards):\n      path = ''.join([path, path_separator, '*'])\n      paths.append(path)\n\n    return paths", "response": "Appends a number of glob wildcards to a path."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ExpandRecursiveGlobs(cls, path, path_separator):\n    glob_regex = r'(.*)?{0:s}\\*\\*(\\d{{1,2}})?({0:s})?$'.format(\n        re.escape(path_separator))\n\n    match = re.search(glob_regex, path)\n    if not match:\n      return [path]\n\n    skip_first = False\n    if match.group(3):\n      skip_first = True\n    if match.group(2):\n      iterations = int(match.group(2))\n    else:\n      iterations = cls._RECURSIVE_GLOB_LIMIT\n      logger.warning((\n          'Path \"{0:s}\" contains fully recursive glob, limiting to 10 '\n          'levels').format(path))\n\n    return cls.AppendPathEntries(\n        match.group(1), path_separator, iterations, skip_first)", "response": "Expands recursive like globs present in an artifact path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ExpandUsersVariablePath(cls, path, path_separator, user_accounts):\n    path_segments = path.split(path_separator)\n    return cls._ExpandUsersVariablePathSegments(\n        path_segments, path_separator, user_accounts)", "response": "Expands a path with a users variable."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nexpand a Windows path with environment variables.", "response": "def ExpandWindowsPath(cls, path, environment_variables):\n    \"\"\"Expands a Windows path containing environment variables.\n\n    Args:\n      path (str): Windows path with environment variables.\n      environment_variables (list[EnvironmentVariableArtifact]): environment\n          variables.\n\n    Returns:\n      str: expanded Windows path.\n    \"\"\"\n    if environment_variables is None:\n      environment_variables = []\n\n    lookup_table = {}\n    if environment_variables:\n      for environment_variable in environment_variables:\n        attribute_name = environment_variable.name.upper()\n        attribute_value = environment_variable.value\n        if not isinstance(attribute_value, py2to3.STRING_TYPES):\n          continue\n\n        lookup_table[attribute_name] = attribute_value\n\n    path_segments = path.split('\\\\')\n    # Make a copy of path_segments since this loop can change it.\n    for index, path_segment in enumerate(list(path_segments)):\n      if (len(path_segment) <= 2 or not path_segment.startswith('%') or\n          not path_segment.endswith('%')):\n        continue\n\n      path_segment_upper_case = path_segment.upper()\n      if path_segment_upper_case.startswith('%%ENVIRON_'):\n        lookup_key = path_segment_upper_case[10:-2]\n      else:\n        lookup_key = path_segment_upper_case[1:-1]\n      path_segment = lookup_table.get(lookup_key, path_segment)\n      path_segment = path_segment.split('\\\\')\n\n      expanded_path_segments = list(path_segments[:index])\n      expanded_path_segments.extend(path_segment)\n      expanded_path_segments.extend(path_segments[index + 1:])\n\n      path_segments = expanded_path_segments\n\n    if cls._IsWindowsDrivePathSegment(path_segments[0]):\n      path_segments[0] = ''\n\n    return '\\\\'.join(path_segments)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef GetDisplayNameForPathSpec(\n      cls, path_spec, mount_path=None, text_prepend=None):\n    \"\"\"Retrieves the display name of a path specification.\n\n    Args:\n      path_spec (dfvfs.PathSpec): path specification.\n      mount_path (Optional[str]): path where the file system that is used\n          by the path specification is mounted, such as \"/mnt/image\". The\n          mount path will be stripped from the absolute path defined by\n          the path specification.\n      text_prepend (Optional[str]): text to prepend.\n\n    Returns:\n      str: human readable version of the path specification or None.\n    \"\"\"\n    if not path_spec:\n      return None\n\n    relative_path = cls.GetRelativePathForPathSpec(\n        path_spec, mount_path=mount_path)\n    if not relative_path:\n      return path_spec.type_indicator\n\n    if text_prepend:\n      relative_path = '{0:s}{1:s}'.format(text_prepend, relative_path)\n\n    parent_path_spec = path_spec.parent\n    if parent_path_spec and path_spec.type_indicator in (\n        dfvfs_definitions.TYPE_INDICATOR_BZIP2,\n        dfvfs_definitions.TYPE_INDICATOR_GZIP):\n      parent_path_spec = parent_path_spec.parent\n\n    if parent_path_spec and parent_path_spec.type_indicator == (\n        dfvfs_definitions.TYPE_INDICATOR_VSHADOW):\n      store_index = getattr(path_spec.parent, 'store_index', None)\n      if store_index is not None:\n        return 'VSS{0:d}:{1:s}:{2:s}'.format(\n            store_index + 1, path_spec.type_indicator, relative_path)\n\n    return '{0:s}:{1:s}'.format(path_spec.type_indicator, relative_path)", "response": "Retrieves the display name for a path specification."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving the relative path of a path specification.", "response": "def GetRelativePathForPathSpec(cls, path_spec, mount_path=None):\n    \"\"\"Retrieves the relative path of a path specification.\n\n    If a mount path is defined the path will be relative to the mount point,\n    otherwise the path is relative to the root of the file system that is used\n    by the path specification.\n\n    Args:\n      path_spec (dfvfs.PathSpec): path specification.\n      mount_path (Optional[str]): path where the file system that is used\n          by the path specification is mounted, such as \"/mnt/image\". The\n          mount path will be stripped from the absolute path defined by\n          the path specification.\n\n    Returns:\n      str: relative path or None.\n    \"\"\"\n    if not path_spec:\n      return None\n\n    # TODO: Solve this differently, quite possibly inside dfVFS using mount\n    # path spec.\n    location = getattr(path_spec, 'location', None)\n    if not location and path_spec.HasParent():\n      location = getattr(path_spec.parent, 'location', None)\n\n    if not location:\n      return None\n\n    data_stream = getattr(path_spec, 'data_stream', None)\n    if data_stream:\n      location = '{0:s}:{1:s}'.format(location, data_stream)\n\n    if path_spec.type_indicator != dfvfs_definitions.TYPE_INDICATOR_OS:\n      return location\n\n    # If we are parsing a mount point we don't want to include the full\n    # path to file's location here, we are only interested in the path\n    # relative to the mount point.\n    if mount_path and location.startswith(mount_path):\n      location = location[len(mount_path):]\n\n    return location"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef GetUnicodeString(value):\n  if isinstance(value, list):\n    value = [GetUnicodeString(item) for item in value]\n    return ''.join(value)\n\n  if isinstance(value, py2to3.INTEGER_TYPES):\n    value = '{0:d}'.format(value)\n\n  if not isinstance(value, py2to3.UNICODE_TYPE):\n    return codecs.decode(value, 'utf8', 'ignore')\n  return value", "response": "Attempts to convert the argument to a Unicode string."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntake a list of values and if at least one matches returns True.", "response": "def Operate(self, values):\n    \"\"\"Takes a list of values and if at least one matches, returns True.\"\"\"\n    for val in values:\n      try:\n        if self.Operation(val, self.right_operand):\n          return True\n      except (TypeError, ValueError):\n        pass\n\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalls when at a non - leaf value. Should recurse and yield values.", "response": "def _AtNonLeaf(self, attr_value, path):\n    \"\"\"Called when at a non-leaf value. Should recurse and yield values.\"\"\"\n    try:\n      # Check first for iterables\n      # If it's a dictionary, we yield it\n      if isinstance(attr_value, dict):\n        yield attr_value\n      else:\n        # If it's an iterable, we recurse on each value.\n        for sub_obj in attr_value:\n          for value in self.Expand(sub_obj, path[1:]):\n            yield value\n    except TypeError:  # This is then not iterable, we recurse with the value\n      for value in self.Expand(attr_value, path[1:]):\n        yield value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef Expand(self, obj, path):\n    if isinstance(path, py2to3.STRING_TYPES):\n      path = path.split(self.FIELD_SEPARATOR)\n\n    attr_name = self._GetAttributeName(path)\n    attr_value = self._GetValue(obj, attr_name)\n    if attr_value is None:\n      return\n\n    if len(path) == 1:\n      for value in self._AtLeaf(attr_value):\n        yield value\n    else:\n      for value in self._AtNonLeaf(attr_value, path):\n        yield value", "response": "Expand the object obj at the given path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef Compile(self, filter_implementation):\n    operator = self.operator.lower()\n    if operator in ('and', '&&'):\n      method = 'AndFilter'\n    elif operator in ('or', '||'):\n      method = 'OrFilter'\n    else:\n      raise errors.ParseError(\n          'Invalid binary operator {0:s}.'.format(operator))\n\n    args = [x.Compile(filter_implementation) for x in self.args]\n    return filter_implementation.FILTERS[method](arguments=args)", "response": "Compile the binary expression into a filter object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef FlipAllowed(self):\n    if not hasattr(self, 'flipped'):\n      raise errors.ParseError('Not defined.')\n\n    if not self.flipped:\n      return\n\n    if self.current_expression.operator:\n      if not self.current_expression.operator.lower() in (\n          'is', 'contains', 'inset', 'equals'):\n        raise errors.ParseError(\n            'Keyword \\'not\\' does not work against operator: {0:s}'.format(\n                self.current_expression.operator))", "response": "Raise an error if the not keyword is used where it is not allowed."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nflip the boolean logic of the current expression.", "response": "def FlipLogic(self, **unused_kwargs):\n    \"\"\"Flip the boolean logic of the expression.\n\n    If an expression is configured to return True when the condition\n    is met this logic will flip that to False, and vice versa.\n    \"\"\"\n    if hasattr(self, 'flipped') and self.flipped:\n      raise errors.ParseError(\n          'The operator \\'not\\' can only be expressed once.')\n\n    if self.current_expression.args:\n      raise errors.ParseError(\n          'Unable to place the keyword \\'not\\' after an argument.')\n\n    self.flipped = True\n\n    # Check if this flip operation should be allowed.\n    self.FlipAllowed()\n\n    if hasattr(self.current_expression, 'FlipBool'):\n      self.current_expression.FlipBool()\n      logging.debug('Negative matching [flipping boolean logic].')\n    else:\n      logging.warning(\n          'Unable to perform a negative match, issuing a positive one.')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef InsertFloatArg(self, string='', **unused_kwargs):\n    try:\n      float_value = float(string)\n    except (TypeError, ValueError):\n      raise errors.ParseError('{0:s} is not a valid float.'.format(string))\n    return self.InsertArg(float_value)", "response": "Inserts a Float argument."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninserts an Integer argument.", "response": "def InsertIntArg(self, string='', **unused_kwargs):\n    \"\"\"Inserts an Integer argument.\"\"\"\n    try:\n      int_value = int(string)\n    except (TypeError, ValueError):\n      raise errors.ParseError('{0:s} is not a valid integer.'.format(string))\n    return self.InsertArg(int_value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nescapes backslashes found inside a string quote.", "response": "def StringEscape(self, string, match, **unused_kwargs):\n    \"\"\"Escape backslashes found inside a string quote.\n\n    Backslashes followed by anything other than [\\'\"rnbt.ws] will raise\n    an Error.\n\n    Args:\n      string: The string that matched.\n      match: the match object (instance of re.MatchObject).\n             Where match.group(1) contains the escaped code.\n\n    Raises:\n      ParseError: When the escaped string is not one of [\\'\"rnbt]\n    \"\"\"\n    if match.group(1) in '\\\\\\'\"rnbt\\\\.ws':\n      self.string += codecs.decode(string, 'unicode_escape')\n    else:\n      raise errors.ParseError('Invalid escape character {0:s}.'.format(string))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef HexEscape(self, string, match, **unused_kwargs):\n    logging.debug('HexEscape matched {0:s}.'.format(string))\n    hex_string = match.group(1)\n    try:\n      hex_string = binascii.unhexlify(hex_string)\n      hex_string = codecs.decode(hex_string, 'utf-8')\n      self.string += hex_string\n    except (TypeError, binascii.Error):\n      raise errors.ParseError('Invalid hex escape {0!s}.'.format(hex_string))", "response": "Converts a hex escaped string."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef Reduce(self):\n    # Check for sanity\n    if self.state != 'INITIAL' and self.state != 'BINARY':\n      self.Error('Premature end of expression')\n\n    length = len(self.stack)\n    while length > 1:\n      # Precedence order\n      self._CombineParenthesis()\n      self._CombineBinaryExpressions('and')\n      self._CombineBinaryExpressions('or')\n      self._CombineContext()\n\n      # No change\n      if len(self.stack) == length:\n        break\n      length = len(self.stack)\n\n    if length != 1:\n      self.Error('Illegal query expression')\n\n    return self.stack[0]", "response": "Reduce the token stack into an AST."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef GetMessages(self, formatter_mediator, event):\n    if self.DATA_TYPE != event.data_type:\n      raise errors.WrongFormatter('Unsupported data type: {0:s}.'.format(\n          event.data_type))\n\n    event_values = event.CopyToDict()\n\n    page_transition_type = event_values.get('page_transition_type', None)\n    if page_transition_type is not None:\n      page_transition, page_transition_long = self._PAGE_TRANSITIONS.get(\n          page_transition_type, self._UNKNOWN_PAGE_TRANSITION)\n\n      if page_transition_long:\n        event_values['page_transition'] = '{0:s} - {1:s}'.format(\n            page_transition, page_transition_long)\n      else:\n        event_values['page_transition'] = page_transition\n\n    visit_source = event_values.get('visit_source', None)\n    if visit_source is not None:\n      event_values['visit_source'] = self._VISIT_SOURCE.get(\n          visit_source, 'UNKNOWN')\n\n    extras = []\n\n    url_hidden = event_values.get('url_hidden', False)\n    if url_hidden:\n      extras.append('(url hidden)')\n\n    typed_count = event_values.get('typed_count', 0)\n    if typed_count == 0:\n      extras.append('(URL not typed directly - no typed count)')\n    elif typed_count == 1:\n      extras.append('(type count {0:d} time)'.format(typed_count))\n    else:\n      extras.append('(type count {0:d} times)'.format(typed_count))\n\n    event_values['extra'] = ' '.join(extras)\n\n    return self._ConditionalFormatMessages(event_values)", "response": "Determines the formatted message strings for an event object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _ParseLeak(\n      self, parser_mediator, cache_directories, msiecf_item, recovered=False):\n    \"\"\"Extract data from a MSIE Cache Files (MSIECF) leak item.\n\n    Every item is stored as an event object, one for each timestamp.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      cache_directories (list[str]): cache directory names.\n      msiecf_item (pymsiecf.leak): MSIECF leak item.\n      recovered (Optional[bool]): True if the item was recovered.\n    \"\"\"\n    # TODO: add support for possible last cache synchronization date and time.\n    date_time = dfdatetime_semantic_time.SemanticTime('Not set')\n\n    event_data = MSIECFLeakEventData()\n    event_data.cached_filename = msiecf_item.filename\n    event_data.cached_file_size = msiecf_item.cached_file_size\n    event_data.cache_directory_index = msiecf_item.cache_directory_index\n    event_data.offset = msiecf_item.offset\n    event_data.recovered = recovered\n\n    if (event_data.cache_directory_index >= 0 and\n        event_data.cache_directory_index < len(cache_directories)):\n      event_data.cache_directory_name = (\n          cache_directories[event_data.cache_directory_index])\n\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_NOT_A_TIME)\n    parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Parses a MSIE Cache Files leak item."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _ParseItems(self, parser_mediator, msiecf_file):\n    format_version = msiecf_file.format_version\n\n    decode_error = False\n    cache_directories = []\n    for cache_directory_name in iter(msiecf_file.cache_directories):\n      try:\n        cache_directory_name = cache_directory_name.decode('ascii')\n      except UnicodeDecodeError:\n        decode_error = True\n        cache_directory_name = cache_directory_name.decode(\n            'ascii', errors='replace')\n\n      cache_directories.append(cache_directory_name)\n\n    if decode_error:\n      parser_mediator.ProduceExtractionWarning((\n          'unable to decode cache directory names. Characters that cannot '\n          'be decoded will be replaced with \"?\" or \"\\\\ufffd\".'))\n\n    for item_index in range(0, msiecf_file.number_of_items):\n      try:\n        msiecf_item = msiecf_file.get_item(item_index)\n        if isinstance(msiecf_item, pymsiecf.leak):\n          self._ParseLeak(parser_mediator, cache_directories, msiecf_item)\n\n        elif isinstance(msiecf_item, pymsiecf.redirected):\n          self._ParseRedirected(parser_mediator, msiecf_item)\n\n        elif isinstance(msiecf_item, pymsiecf.url):\n          self._ParseUrl(\n              parser_mediator, format_version, cache_directories, msiecf_item)\n\n      except IOError as exception:\n        parser_mediator.ProduceExtractionWarning(\n            'Unable to parse item: {0:d} with error: {1!s}'.format(\n                item_index, exception))\n\n    for item_index in range(0, msiecf_file.number_of_recovered_items):\n      try:\n        msiecf_item = msiecf_file.get_recovered_item(item_index)\n        if isinstance(msiecf_item, pymsiecf.leak):\n          self._ParseLeak(\n              parser_mediator, cache_directories, msiecf_item, recovered=True)\n\n        elif isinstance(msiecf_item, pymsiecf.redirected):\n          self._ParseRedirected(parser_mediator, msiecf_item, recovered=True)\n\n        elif isinstance(msiecf_item, pymsiecf.url):\n          self._ParseUrl(\n              parser_mediator, format_version, cache_directories, msiecf_item,\n              recovered=True)\n\n      except IOError as exception:\n        parser_mediator.ProduceExtractionWarning(\n            'Unable to parse recovered item: {0:d} with error: {1!s}'.format(\n                item_index, exception))", "response": "Parses a MSIE Cache File items."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _ParseRedirected(\n      self, parser_mediator, msiecf_item, recovered=False):\n    \"\"\"Extract data from a MSIE Cache Files (MSIECF) redirected item.\n\n    Every item is stored as an event object, one for each timestamp.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      msiecf_item (pymsiecf.redirected): MSIECF redirected item.\n      recovered (Optional[bool]): True if the item was recovered.\n    \"\"\"\n    date_time = dfdatetime_semantic_time.SemanticTime('Not set')\n\n    event_data = MSIECFRedirectedEventData()\n    event_data.offset = msiecf_item.offset\n    event_data.recovered = recovered\n    event_data.url = msiecf_item.location\n\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_NOT_A_TIME)\n    parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Parses a redirected item."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _ParseUrl(\n      self, parser_mediator, format_version, cache_directories, msiecf_item,\n      recovered=False):\n    \"\"\"Extract data from a MSIE Cache Files (MSIECF) URL item.\n\n    Every item is stored as an event object, one for each timestamp.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      format_version (str): MSIECF format version.\n      cache_directories (list[str]): cache directory names.\n      msiecf_item (pymsiecf.url): MSIECF URL item.\n      recovered (Optional[bool]): True if the item was recovered.\n    \"\"\"\n    # The secondary time can be stored in either UTC or local time\n    # this is dependent on what the index.dat file is used for.\n    # Either the file path or location string can be used to distinguish\n    # between the different type of files.\n    timestamp = msiecf_item.get_primary_time_as_integer()\n    if not timestamp:\n      primary_date_time = dfdatetime_semantic_time.SemanticTime('Not set')\n    else:\n      primary_date_time = dfdatetime_filetime.Filetime(timestamp=timestamp)\n    primary_date_time_description = 'Primary Time'\n\n    timestamp = msiecf_item.get_secondary_time_as_integer()\n    secondary_date_time = dfdatetime_filetime.Filetime(timestamp=timestamp)\n    secondary_date_time_description = 'Secondary Time'\n\n    if msiecf_item.type:\n      if msiecf_item.type == 'cache':\n        primary_date_time_description = definitions.TIME_DESCRIPTION_LAST_ACCESS\n        secondary_date_time_description = (\n            definitions.TIME_DESCRIPTION_MODIFICATION)\n\n      elif msiecf_item.type == 'cookie':\n        primary_date_time_description = definitions.TIME_DESCRIPTION_LAST_ACCESS\n        secondary_date_time_description = (\n            definitions.TIME_DESCRIPTION_MODIFICATION)\n\n      elif msiecf_item.type == 'history':\n        primary_date_time_description = (\n            definitions.TIME_DESCRIPTION_LAST_VISITED)\n        secondary_date_time_description = (\n            definitions.TIME_DESCRIPTION_LAST_VISITED)\n\n      elif msiecf_item.type == 'history-daily':\n        primary_date_time_description = (\n            definitions.TIME_DESCRIPTION_LAST_VISITED)\n        secondary_date_time_description = (\n            definitions.TIME_DESCRIPTION_LAST_VISITED)\n        # The secondary_date_time is in localtime normalize it to be in UTC.\n        secondary_date_time.is_local_time = True\n\n      elif msiecf_item.type == 'history-weekly':\n        primary_date_time_description = definitions.TIME_DESCRIPTION_CREATION\n        secondary_date_time_description = (\n            definitions.TIME_DESCRIPTION_LAST_VISITED)\n        # The secondary_date_time is in localtime normalize it to be in UTC.\n        secondary_date_time.is_local_time = True\n\n    http_headers = ''\n    if msiecf_item.type and msiecf_item.data:\n      if msiecf_item.type == 'cache':\n        if msiecf_item.data[:4] == b'HTTP':\n          # Make sure the HTTP headers are ASCII encoded.\n          # TODO: determine correct encoding currently indications that\n          # this could be the system narrow string codepage.\n          try:\n            http_headers = msiecf_item.data[:-1].decode('ascii')\n          except UnicodeDecodeError:\n            parser_mediator.ProduceExtractionWarning((\n                'unable to decode HTTP headers of URL record at offset: '\n                '0x{0:08x}. Characters that cannot be decoded will be '\n                'replaced with \"?\" or \"\\\\ufffd\".').format(msiecf_item.offset))\n            http_headers = msiecf_item.data[:-1].decode(\n                'ascii', errors='replace')\n\n      # TODO: parse data of other URL item type like history which requires\n      # OLE VT parsing.\n\n    event_data = MSIECFURLEventData()\n    event_data.cached_filename = msiecf_item.filename\n    event_data.cached_file_size = msiecf_item.cached_file_size\n    event_data.cache_directory_index = msiecf_item.cache_directory_index\n    event_data.http_headers = http_headers\n    event_data.number_of_hits = msiecf_item.number_of_hits\n    event_data.offset = msiecf_item.offset\n    event_data.recovered = recovered\n    event_data.url = msiecf_item.location\n\n    if (event_data.cache_directory_index >= 0 and\n        event_data.cache_directory_index < len(cache_directories)):\n      event_data.cache_directory_name = (\n          cache_directories[event_data.cache_directory_index])\n\n    event = time_events.DateTimeValuesEvent(\n        primary_date_time, primary_date_time_description)\n    parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    if secondary_date_time.timestamp != 0:\n      event = time_events.DateTimeValuesEvent(\n          secondary_date_time, secondary_date_time_description,\n          time_zone=parser_mediator.timezone)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    expiration_timestamp = msiecf_item.get_expiration_time_as_integer()\n    if expiration_timestamp != 0:\n      # The expiration time in MSIECF version 4.7 is stored as a FILETIME value\n      # in version 5.2 it is stored as a FAT date time value.\n      # Since the as_integer function returns the raw integer value we need to\n      # apply the right conversion here.\n      if format_version == '4.7':\n        if expiration_timestamp == 0x7fffffffffffffff:\n          expiration_date_time = dfdatetime_semantic_time.SemanticTime('Never')\n        else:\n          expiration_date_time = dfdatetime_filetime.Filetime(\n              timestamp=expiration_timestamp)\n      else:\n        if expiration_timestamp == 0xffffffff:\n          expiration_date_time = dfdatetime_semantic_time.SemanticTime('Never')\n        else:\n          expiration_date_time = dfdatetime_fat_date_time.FATDateTime(\n              fat_date_time=expiration_timestamp)\n\n      event = time_events.DateTimeValuesEvent(\n          expiration_date_time, definitions.TIME_DESCRIPTION_EXPIRATION)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    last_checked_timestamp = msiecf_item.get_last_checked_time_as_integer()\n    if last_checked_timestamp != 0:\n      last_checked_date_time = dfdatetime_fat_date_time.FATDateTime(\n          fat_date_time=last_checked_timestamp)\n\n      event = time_events.DateTimeValuesEvent(\n          last_checked_date_time, definitions.TIME_DESCRIPTION_LAST_CHECKED)\n      parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Parses a MSIE Cache Files URL item."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ParseFileObject(self, parser_mediator, file_object):\n    msiecf_file = pymsiecf.file()\n    msiecf_file.set_ascii_codepage(parser_mediator.codepage)\n\n    try:\n      msiecf_file.open_file_object(file_object)\n    except IOError as exception:\n      parser_mediator.ProduceExtractionWarning(\n          'unable to open file with error: {0!s}'.format(exception))\n      return\n\n    try:\n      self._ParseItems(parser_mediator, msiecf_file)\n    finally:\n      msiecf_file.close()", "response": "Parses a MSIE Cache File - like object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef GetEntries(self, parser_mediator, match=None, **unused_kwargs):\n    shortcuts = match.get('UserShortcuts', {})\n    for search_text, data in iter(shortcuts.items()):\n      datetime_value = data.get('LAST_USED', None)\n      if not datetime_value:\n        continue\n\n      display_name = data.get('DISPLAY_NAME', '<DISPLAY_NAME>')\n      path = data.get('PATH', '<PATH>')\n\n      event_data = plist_event.PlistTimeEventData()\n      event_data.desc = (\n          'Spotlight term searched \"{0:s}\" associate to {1:s} ({2:s})').format(\n              search_text, display_name, path)\n      event_data.key = search_text\n      event_data.root = '/UserShortcuts'\n\n      event = time_events.PythonDatetimeEvent(\n          datetime_value, definitions.TIME_DESCRIPTION_WRITTEN)\n      parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Extracts relevant Spotlight entries."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nretrieve a value from the row.", "response": "def _GetRowValue(self, query_hash, row, value_name):\n    \"\"\"Retrieves a value from the row.\n\n    Args:\n      query_hash (int): hash of the query, that uniquely identifies the query\n          that produced the row.\n      row (sqlite3.Row): row.\n      value_name (str): name of the value.\n\n    Returns:\n      object: value.\n    \"\"\"\n    keys_name_to_index_map = self._keys_per_query.get(query_hash, None)\n    if not keys_name_to_index_map:\n      keys_name_to_index_map = {\n          name: index for index, name in enumerate(row.keys())}\n      self._keys_per_query[query_hash] = keys_name_to_index_map\n\n    value_index = keys_name_to_index_map.get(value_name)\n\n    # Note that pysqlite does not accept a Unicode string in row['string'] and\n    # will raise \"IndexError: Index must be int or string\".\n    return row[value_index]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _HashRow(cls, row):\n    values = []\n    for value in row:\n      try:\n        value = '{0!s}'.format(value)\n      except UnicodeDecodeError:\n        # In Python 2, blobs are \"read-write buffer\" and will cause a\n        # UnicodeDecodeError exception if we try format it as a string.\n        # Since Python 3 does not support the buffer type we cannot check\n        # the type of value.\n        value = repr(value)\n\n      values.append(value)\n\n    return hash(' '.join(values))", "response": "Hashes the given row."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nquery a database and parses the results.", "response": "def _ParseQuery(self, parser_mediator, database, query, callback, cache):\n    \"\"\"Queries a database and parses the results.\n\n    Args:\n      parser_mediator (ParserMediator): parser mediator.\n      database (SQLiteDatabase): database.\n      query (str): query.\n      callback (function): function to invoke to parse an individual row.\n      cache (SQLiteCache): cache.\n    \"\"\"\n    row_cache = cache.GetRowCache(query)\n\n    try:\n      rows = database.Query(query)\n\n    except sqlite3.DatabaseError as exception:\n      parser_mediator.ProduceExtractionWarning(\n          'unable to run query: {0:s} on database with error: {1!s}'.format(\n              query, exception))\n      return\n\n    for index, row in enumerate(rows):\n      if parser_mediator.abort:\n        break\n\n      row_hash = self._HashRow(row)\n      if row_hash in row_cache:\n        continue\n\n      try:\n        callback(parser_mediator, query, row, cache=cache, database=database)\n\n      except Exception as exception:  # pylint: disable=broad-except\n        parser_mediator.ProduceExtractionWarning((\n            'unable to parse row: {0:d} with callback: {1:s} on database '\n            'with error: {2!s}').format(\n                index, callback.__name__, exception))\n        # TODO: consider removing return.\n        return\n\n      row_cache.add(row_hash)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking the schema of a database with that defined in the plugin.", "response": "def CheckSchema(self, database):\n    \"\"\"Checks the schema of a database with that defined in the plugin.\n\n    Args:\n      database (SQLiteDatabase): database.\n\n    Returns:\n      bool: True if the schema of the database matches that defined by\n          the plugin, or False if the schemas do not match or no schema\n          is defined by the plugin.\n    \"\"\"\n    schema_match = False\n    if self.SCHEMAS:\n      for schema in self.SCHEMAS:\n        if database and database.schema == schema:\n          schema_match = True\n\n    return schema_match"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef Process(\n      self, parser_mediator, cache=None, database=None, **unused_kwargs):\n    \"\"\"Determine if this is the right plugin for this database.\n\n    This function takes a SQLiteDatabase object and compares the list\n    of required tables against the available tables in the database.\n    If all the tables defined in REQUIRED_TABLES are present in the\n    database then this plugin is considered to be the correct plugin\n    and the function will return back a generator that yields event\n    objects.\n\n    Args:\n      parser_mediator (ParserMediator): parser mediator.\n      cache (Optional[SQLiteCache]): cache.\n      database (Optional[SQLiteDatabase]): database.\n\n    Raises:\n      ValueError: If the database or cache value are missing.\n    \"\"\"\n    if cache is None:\n      raise ValueError('Missing cache value.')\n\n    if database is None:\n      raise ValueError('Missing database value.')\n\n    # This will raise if unhandled keyword arguments are passed.\n    super(SQLitePlugin, self).Process(parser_mediator)\n\n    for query, callback_method in self.QUERIES:\n      if parser_mediator.abort:\n        break\n\n      callback = getattr(self, callback_method, None)\n      if callback is None:\n        logger.warning(\n            '[{0:s}] missing callback method: {1:s} for query: {2:s}'.format(\n                self.NAME, callback_method, query))\n        continue\n\n      self._ParseQuery(parser_mediator, database, query, callback, cache)", "response": "Processes the SQLite database and returns a generator that yields EventResources objects."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ParseCookieRow(self, parser_mediator, query, row, **unused_kwargs):\n    query_hash = hash(query)\n\n    cookie_name = self._GetRowValue(query_hash, row, 'name')\n    cookie_value = self._GetRowValue(query_hash, row, 'value')\n    path = self._GetRowValue(query_hash, row, 'path')\n\n    hostname = self._GetRowValue(query_hash, row, 'domain')\n    if hostname.startswith('.'):\n      hostname = hostname[1:]\n\n    secure = self._GetRowValue(query_hash, row, 'secure')\n    # The WebView database stores the secure flag as a integer type,\n    # but we represent it as a boolean.\n    secure = secure != 0\n\n    if secure:\n      scheme = 'https'\n    else:\n      scheme = 'http'\n\n    url = '{0:s}://{1:s}{2:s}'.format(scheme, hostname, path)\n\n    event_data = WebViewCookieEventData()\n    event_data.cookie_name = cookie_name\n    event_data.data = cookie_value\n    event_data.host = hostname\n    event_data.offset = self._GetRowValue(query_hash, row, '_id')\n    event_data.path = path\n    event_data.query = query\n    event_data.secure = secure\n    event_data.url = url\n\n    timestamp = self._GetRowValue(query_hash, row, 'expires')\n    if timestamp:\n      date_time = dfdatetime_java_time.JavaTime(timestamp=timestamp)\n    else:\n      date_time = dfdatetime_semantic_time.SemanticTime('Infinity')\n\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_EXPIRATION)\n    parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    # Go through all cookie plugins to see if there are is any specific parsing\n    # needed.\n    for cookie_plugin in self._cookie_plugins:\n      try:\n        cookie_plugin.UpdateChainAndProcess(\n            parser_mediator, cookie_name=cookie_name,\n            cookie_data=cookie_value, url=url)\n      except errors.WrongPlugin:\n        pass", "response": "Parses a cookie row from the database."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse a Windows Restore Point log file - like object.", "response": "def ParseFileObject(self, parser_mediator, file_object):\n    \"\"\"Parses a Windows Restore Point (rp.log) log file-like object.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      file_object (dfvfs.FileIO): file-like object.\n\n    Raises:\n      UnableToParseFile: when the file cannot be parsed.\n    \"\"\"\n    file_size = file_object.get_size()\n\n    file_header_map = self._GetDataTypeMap('rp_log_file_header')\n\n    try:\n      file_header, _ = self._ReadStructureFromFileObject(\n          file_object, 0, file_header_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.UnableToParseFile(\n          'Unable to parse file header with error: {0!s}'.format(\n              exception))\n\n    file_footer_map = self._GetDataTypeMap('rp_log_file_footer')\n\n    file_footer_offset = file_size - file_footer_map.GetByteSize()\n\n    try:\n      file_footer, _ = self._ReadStructureFromFileObject(\n          file_object, file_footer_offset, file_footer_map)\n    except (ValueError, errors.ParseError) as exception:\n      parser_mediator.ProduceExtractionWarning(\n          'unable to parse file footer with error: {0!s}'.format(exception))\n      return\n\n    # The description in the file header includes the end-of-string character\n    # that we need to strip off.\n    description = file_header.description.rstrip('\\0')\n\n    if file_footer.creation_time == 0:\n      date_time = dfdatetime_semantic_time.SemanticTime('Not set')\n    else:\n      date_time = dfdatetime_filetime.Filetime(\n          timestamp=file_footer.creation_time)\n\n    event_data = RestorePointEventData()\n    event_data.description = description\n    event_data.restore_point_event_type = file_header.event_type\n    event_data.restore_point_type = file_header.restore_point_type\n    event_data.sequence_number = file_header.sequence_number\n\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_CREATION)\n    parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _AddAttributeContainer(self, container_type, attribute_container):\n    container_list = self._GetSerializedAttributeContainerList(container_type)\n\n    identifier = identifiers.SQLTableIdentifier(\n        container_type, container_list.next_sequence_number + 1)\n    attribute_container.SetIdentifier(identifier)\n\n    serialized_data = self._SerializeAttributeContainer(attribute_container)\n\n    container_list.PushAttributeContainer(serialized_data)\n\n    if container_list.data_size > self._maximum_buffer_size:\n      self._WriteSerializedAttributeContainerList(container_type)", "response": "Adds an attribute container."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _AddSerializedEvent(self, event):\n    identifier = identifiers.SQLTableIdentifier(\n        self._CONTAINER_TYPE_EVENT,\n        self._serialized_event_heap.number_of_events + 1)\n    event.SetIdentifier(identifier)\n\n    serialized_data = self._SerializeAttributeContainer(event)\n\n    self._serialized_event_heap.PushEvent(event.timestamp, serialized_data)\n\n    if self._serialized_event_heap.data_size > self._maximum_buffer_size:\n      self._WriteSerializedAttributeContainerList(self._CONTAINER_TYPE_EVENT)", "response": "Adds an event to the serialized event list."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck the storage metadata for consistency.", "response": "def _CheckStorageMetadata(cls, metadata_values, check_readable_only=False):\n    \"\"\"Checks the storage metadata.\n\n    Args:\n      metadata_values (dict[str, str]): metadata values per key.\n      check_readable_only (Optional[bool]): whether the store should only be\n          checked to see if it can be read. If False, the store will be checked\n          to see if it can be read and written to.\n\n    Raises:\n      IOError: if the format version or the serializer format is not supported.\n      OSError: if the format version or the serializer format is not supported.\n    \"\"\"\n    format_version = metadata_values.get('format_version', None)\n\n    if not format_version:\n      raise IOError('Missing format version.')\n\n    try:\n      format_version = int(format_version, 10)\n    except (TypeError, ValueError):\n      raise IOError('Invalid format version: {0!s}.'.format(format_version))\n\n    if not check_readable_only and format_version != cls._FORMAT_VERSION:\n      raise IOError('Format version: {0:d} is not supported.'.format(\n          format_version))\n\n    if format_version < cls._COMPATIBLE_FORMAT_VERSION:\n      raise IOError(\n          'Format version: {0:d} is too old and no longer supported.'.format(\n              format_version))\n\n    if format_version > cls._FORMAT_VERSION:\n      raise IOError(\n          'Format version: {0:d} is too new and not yet supported.'.format(\n              format_version))\n\n    metadata_values['format_version'] = format_version\n\n    compression_format = metadata_values.get('compression_format', None)\n    if compression_format not in definitions.COMPRESSION_FORMATS:\n      raise IOError('Unsupported compression format: {0:s}'.format(\n          compression_format))\n\n    serialization_format = metadata_values.get('serialization_format', None)\n    if serialization_format != definitions.SERIALIZER_FORMAT_JSON:\n      raise IOError('Unsupported serialization format: {0:s}'.format(\n          serialization_format))\n\n    storage_type = metadata_values.get('storage_type', None)\n    if storage_type not in definitions.STORAGE_TYPES:\n      raise IOError('Unsupported storage type: {0:s}'.format(\n          storage_type))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncounts the number of stored attribute containers of the given type.", "response": "def _CountStoredAttributeContainers(self, container_type):\n    \"\"\"Counts the number of attribute containers of the given type.\n\n    Args:\n      container_type (str): attribute container type.\n\n    Returns:\n      int: number of attribute containers of the given type.\n\n    Raises:\n      ValueError: if an unsupported container_type is provided.\n    \"\"\"\n    if not container_type in self._CONTAINER_TYPES:\n      raise ValueError('Attribute container type {0:s} is not supported'.format(\n          container_type))\n\n    if not self._HasTable(container_type):\n      return 0\n\n    # Note that this is SQLite specific, and will give inaccurate results if\n    # there are DELETE commands run on the table. The Plaso SQLite storage\n    # implementation does not run any DELETE commands.\n    query = 'SELECT MAX(_ROWID_) FROM {0:s} LIMIT 1'.format(container_type)\n    self._cursor.execute(query)\n    row = self._cursor.fetchone()\n    if not row:\n      return 0\n\n    return row[0] or 0"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nretrieving a specific attribute container by index.", "response": "def _GetAttributeContainerByIndex(self, container_type, index):\n    \"\"\"Retrieves a specific attribute container.\n\n    Args:\n      container_type (str): attribute container type.\n      index (int): attribute container index.\n\n    Returns:\n      AttributeContainer: attribute container or None if not available.\n\n    Raises:\n      IOError: when there is an error querying the storage file.\n      OSError: when there is an error querying the storage file.\n    \"\"\"\n    sequence_number = index + 1\n    query = 'SELECT _data FROM {0:s} WHERE rowid = {1:d}'.format(\n        container_type, sequence_number)\n\n    try:\n      self._cursor.execute(query)\n    except sqlite3.OperationalError as exception:\n      raise IOError('Unable to query storage file with error: {0!s}'.format(\n          exception))\n\n    row = self._cursor.fetchone()\n    if row:\n      identifier = identifiers.SQLTableIdentifier(\n          container_type, sequence_number)\n\n      if self.compression_format == definitions.COMPRESSION_FORMAT_ZLIB:\n        serialized_data = zlib.decompress(row[0])\n      else:\n        serialized_data = row[0]\n\n      if self._storage_profiler:\n        self._storage_profiler.Sample(\n            'read', container_type, len(serialized_data), len(row[0]))\n\n      attribute_container = self._DeserializeAttributeContainer(\n          container_type, serialized_data)\n      attribute_container.SetIdentifier(identifier)\n      return attribute_container\n\n    count = self._CountStoredAttributeContainers(container_type)\n    index -= count\n\n    serialized_data = self._GetSerializedAttributeContainerByIndex(\n        container_type, index)\n    attribute_container = self._DeserializeAttributeContainer(\n        container_type, serialized_data)\n\n    if attribute_container:\n      identifier = identifiers.SQLTableIdentifier(\n          container_type, sequence_number)\n      attribute_container.SetIdentifier(identifier)\n    return attribute_container"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _GetAttributeContainers(\n      self, container_type, filter_expression=None, order_by=None):\n    \"\"\"Retrieves a specific type of stored attribute containers.\n\n    Args:\n      container_type (str): attribute container type.\n      filter_expression (Optional[str]): expression to filter results by.\n      order_by (Optional[str]): name of a column to order the results by.\n\n    Yields:\n      AttributeContainer: attribute container.\n\n    Raises:\n      IOError: when there is an error querying the storage file.\n      OSError: when there is an error querying the storage file.\n    \"\"\"\n    query = 'SELECT _identifier, _data FROM {0:s}'.format(container_type)\n    if filter_expression:\n      query = '{0:s} WHERE {1:s}'.format(query, filter_expression)\n    if order_by:\n      query = '{0:s} ORDER BY {1:s}'.format(query, order_by)\n\n    # Use a local cursor to prevent another query interrupting the generator.\n    cursor = self._connection.cursor()\n\n    try:\n      cursor.execute(query)\n    except sqlite3.OperationalError as exception:\n      raise IOError('Unable to query storage file with error: {0!s}'.format(\n          exception))\n\n    row = cursor.fetchone()\n    while row:\n      identifier = identifiers.SQLTableIdentifier(container_type, row[0])\n\n      if self.compression_format == definitions.COMPRESSION_FORMAT_ZLIB:\n        serialized_data = zlib.decompress(row[1])\n      else:\n        serialized_data = row[1]\n\n      if self._storage_profiler:\n        self._storage_profiler.Sample(\n            'read', container_type, len(serialized_data), len(row[1]))\n\n      attribute_container = self._DeserializeAttributeContainer(\n          container_type, serialized_data)\n      attribute_container.SetIdentifier(identifier)\n      yield attribute_container\n\n      row = cursor.fetchone()", "response": "Retrieves a specific type of stored attribute containers."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _HasTable(self, table_name):\n    query = self._HAS_TABLE_QUERY.format(table_name)\n\n    self._cursor.execute(query)\n    return bool(self._cursor.fetchone())", "response": "Determines if a specific table exists."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _ReadAndCheckStorageMetadata(self, check_readable_only=False):\n    query = 'SELECT key, value FROM metadata'\n    self._cursor.execute(query)\n\n    metadata_values = {row[0]: row[1] for row in self._cursor.fetchall()}\n\n    self._CheckStorageMetadata(\n        metadata_values, check_readable_only=check_readable_only)\n\n    self.format_version = metadata_values['format_version']\n    self.compression_format = metadata_values['compression_format']\n    self.serialization_format = metadata_values['serialization_format']\n    self.storage_type = metadata_values['storage_type']", "response": "Reads the storage metadata and checks that the values are valid."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _WriteAttributeContainer(self, attribute_container):\n    if attribute_container.CONTAINER_TYPE == self._CONTAINER_TYPE_EVENT:\n      timestamp, serialized_data = self._serialized_event_heap.PopEvent()\n    else:\n      serialized_data = self._SerializeAttributeContainer(attribute_container)\n\n    if self.compression_format == definitions.COMPRESSION_FORMAT_ZLIB:\n      compressed_data = zlib.compress(serialized_data)\n      serialized_data = sqlite3.Binary(compressed_data)\n    else:\n      compressed_data = ''\n\n    if self._storage_profiler:\n      self._storage_profiler.Sample(\n          'write', attribute_container.CONTAINER_TYPE, len(serialized_data),\n          len(compressed_data))\n\n    if attribute_container.CONTAINER_TYPE == self._CONTAINER_TYPE_EVENT:\n      query = 'INSERT INTO event (_timestamp, _data) VALUES (?, ?)'\n      self._cursor.execute(query, (timestamp, serialized_data))\n    else:\n      query = 'INSERT INTO {0:s} (_data) VALUES (?)'.format(\n          attribute_container.CONTAINER_TYPE)\n      self._cursor.execute(query, (serialized_data, ))\n\n    identifier = identifiers.SQLTableIdentifier(\n        attribute_container.CONTAINER_TYPE, self._cursor.lastrowid)\n    attribute_container.SetIdentifier(identifier)", "response": "Writes an attribute container."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _WriteSerializedAttributeContainerList(self, container_type):\n    if container_type == self._CONTAINER_TYPE_EVENT:\n      if not self._serialized_event_heap.data_size:\n        return\n\n      number_of_attribute_containers = (\n          self._serialized_event_heap.number_of_events)\n\n    else:\n      container_list = self._GetSerializedAttributeContainerList(container_type)\n      if not container_list.data_size:\n        return\n\n      number_of_attribute_containers = (\n          container_list.number_of_attribute_containers)\n\n    if self._serializers_profiler:\n      self._serializers_profiler.StartTiming('write')\n\n    if container_type == self._CONTAINER_TYPE_EVENT:\n      query = 'INSERT INTO event (_timestamp, _data) VALUES (?, ?)'\n    else:\n      query = 'INSERT INTO {0:s} (_data) VALUES (?)'.format(container_type)\n\n    # TODO: directly use container_list instead of values_tuple_list.\n    values_tuple_list = []\n    for _ in range(number_of_attribute_containers):\n      if container_type == self._CONTAINER_TYPE_EVENT:\n        timestamp, serialized_data = self._serialized_event_heap.PopEvent()\n      else:\n        serialized_data = container_list.PopAttributeContainer()\n\n      if self.compression_format == definitions.COMPRESSION_FORMAT_ZLIB:\n        compressed_data = zlib.compress(serialized_data)\n        serialized_data = sqlite3.Binary(compressed_data)\n      else:\n        compressed_data = ''\n\n      if self._storage_profiler:\n        self._storage_profiler.Sample(\n            'write', container_type, len(serialized_data), len(compressed_data))\n\n      if container_type == self._CONTAINER_TYPE_EVENT:\n        values_tuple_list.append((timestamp, serialized_data))\n      else:\n        values_tuple_list.append((serialized_data, ))\n\n    self._cursor.executemany(query, values_tuple_list)\n\n    if self._serializers_profiler:\n      self._serializers_profiler.StopTiming('write')\n\n    if container_type == self._CONTAINER_TYPE_EVENT:\n      self._serialized_event_heap.Empty()\n    else:\n      container_list.Empty()", "response": "Writes a serialized attribute container list."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _WriteStorageMetadata(self):\n    self._cursor.execute(self._CREATE_METADATA_TABLE_QUERY)\n\n    query = 'INSERT INTO metadata (key, value) VALUES (?, ?)'\n\n    key = 'format_version'\n    value = '{0:d}'.format(self._FORMAT_VERSION)\n    self._cursor.execute(query, (key, value))\n\n    key = 'compression_format'\n    value = self.compression_format\n    self._cursor.execute(query, (key, value))\n\n    key = 'serialization_format'\n    value = self.serialization_format\n    self._cursor.execute(query, (key, value))\n\n    key = 'storage_type'\n    value = self.storage_type\n    self._cursor.execute(query, (key, value))", "response": "Writes the storage metadata."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding an extraction warning to the archive.", "response": "def AddWarning(self, warning):\n    \"\"\"Adds an warning.\n\n    Args:\n      warning (ExtractionWarning): warning.\n\n    Raises:\n      IOError: when the storage file is closed or read-only.\n      OSError: when the storage file is closed or read-only.\n    \"\"\"\n    self._RaiseIfNotWritable()\n\n    self._AddAttributeContainer(\n        self._CONTAINER_TYPE_EXTRACTION_WARNING, warning)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding an event to the internal list of events.", "response": "def AddEvent(self, event):\n    \"\"\"Adds an event.\n\n    Args:\n      event (EventObject): event.\n\n    Raises:\n      IOError: when the storage file is closed or read-only or\n          if the event data identifier type is not supported.\n      OSError: when the storage file is closed or read-only or\n          if the event data identifier type is not supported.\n    \"\"\"\n    self._RaiseIfNotWritable()\n\n    # TODO: change to no longer allow event_data_identifier is None\n    # after refactoring every parser to generate event data.\n    event_data_identifier = event.GetEventDataIdentifier()\n    if event_data_identifier:\n      if not isinstance(event_data_identifier, identifiers.SQLTableIdentifier):\n        raise IOError('Unsupported event data identifier type: {0:s}'.format(\n            type(event_data_identifier)))\n\n      event.event_data_row_identifier = event_data_identifier.row_identifier\n\n    self._AddSerializedEvent(event)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef AddEventData(self, event_data):\n    self._RaiseIfNotWritable()\n\n    self._AddAttributeContainer(self._CONTAINER_TYPE_EVENT_DATA, event_data)", "response": "Adds an event data to the event data list."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds an event source to the internal event source list.", "response": "def AddEventSource(self, event_source):\n    \"\"\"Adds an event source.\n\n    Args:\n      event_source (EventSource): event source.\n\n    Raises:\n      IOError: when the storage file is closed or read-only.\n      OSError: when the storage file is closed or read-only.\n    \"\"\"\n    self._RaiseIfNotWritable()\n\n    self._AddAttributeContainer(\n        self._CONTAINER_TYPE_EVENT_SOURCE, event_source)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef AddEventTag(self, event_tag):\n    self._RaiseIfNotWritable()\n\n    event_identifier = event_tag.GetEventIdentifier()\n    if not isinstance(event_identifier, identifiers.SQLTableIdentifier):\n      raise IOError('Unsupported event identifier type: {0:s}'.format(\n          type(event_identifier)))\n\n    event_tag.event_row_identifier = event_identifier.row_identifier\n\n    self._AddAttributeContainer(self._CONTAINER_TYPE_EVENT_TAG, event_tag)", "response": "Adds an event tag to the internal list of event tags."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef AddEventTags(self, event_tags):\n    self._RaiseIfNotWritable()\n\n    for event_tag in event_tags:\n      self.AddEventTag(event_tag)", "response": "Adds event tags to the internal event list."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef CheckSupportedFormat(cls, path, check_readable_only=False):\n    try:\n      connection = sqlite3.connect(\n          path, detect_types=sqlite3.PARSE_DECLTYPES|sqlite3.PARSE_COLNAMES)\n\n      cursor = connection.cursor()\n\n      query = 'SELECT * FROM metadata'\n      cursor.execute(query)\n\n      metadata_values = {row[0]: row[1] for row in cursor.fetchall()}\n\n      cls._CheckStorageMetadata(\n          metadata_values, check_readable_only=check_readable_only)\n\n      connection.close()\n      result = True\n\n    except (IOError, sqlite3.DatabaseError):\n      result = False\n\n    return result", "response": "Checks if the storage file format is supported."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef Close(self):\n    if not self._is_open:\n      raise IOError('Storage file already closed.')\n\n    if not self._read_only:\n      self._WriteSerializedAttributeContainerList(\n          self._CONTAINER_TYPE_EVENT_SOURCE)\n      self._WriteSerializedAttributeContainerList(\n          self._CONTAINER_TYPE_EVENT_DATA)\n      self._WriteSerializedAttributeContainerList(self._CONTAINER_TYPE_EVENT)\n      self._WriteSerializedAttributeContainerList(\n          self._CONTAINER_TYPE_EVENT_TAG)\n      self._WriteSerializedAttributeContainerList(\n          self._CONTAINER_TYPE_EXTRACTION_WARNING)\n\n    if self._connection:\n      # We need to run commit or not all data is stored in the database.\n      self._connection.commit()\n      self._connection.close()\n\n      self._connection = None\n      self._cursor = None\n\n    self._is_open = False", "response": "Closes the storage file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef GetWarnings(self):\n    # For backwards compatibility with pre-20190309 stores.\n    # Note that stores cannot contain both ExtractionErrors and\n    # ExtractionWarnings\n    if self._HasAttributeContainers(self._CONTAINER_TYPE_EXTRACTION_ERROR):\n      return self._GetExtractionErrorsAsWarnings()\n\n    return self._GetAttributeContainers(self._CONTAINER_TYPE_EXTRACTION_WARNING)", "response": "Retrieves the warnings.\n\n    Returns:\n      generator(ExtractionWarning): warning generator."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _GetExtractionErrorsAsWarnings(self):\n    for extraction_error in self._GetAttributeContainers(\n        self._CONTAINER_TYPE_EXTRACTION_ERROR):\n      error_attributes = extraction_error.CopyToDict()\n      warning = warnings.ExtractionWarning()\n      warning.CopyFromDict(error_attributes)\n      yield warning", "response": "Retrieves errors from the store and converts them to warnings."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving the events. Yield: EventObject: event.", "response": "def GetEvents(self):\n    \"\"\"Retrieves the events.\n\n    Yield:\n      EventObject: event.\n    \"\"\"\n    for event in self._GetAttributeContainers('event'):\n      if hasattr(event, 'event_data_row_identifier'):\n        event_data_identifier = identifiers.SQLTableIdentifier(\n            'event_data', event.event_data_row_identifier)\n        event.SetEventDataIdentifier(event_data_identifier)\n\n        del event.event_data_row_identifier\n\n      yield event"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef GetEventTagByIdentifier(self, identifier):\n    event_tag = self._GetAttributeContainerByIndex(\n        self._CONTAINER_TYPE_EVENT_TAG, identifier.row_identifier - 1)\n    if event_tag:\n      event_identifier = identifiers.SQLTableIdentifier(\n          self._CONTAINER_TYPE_EVENT, event_tag.event_row_identifier)\n      event_tag.SetEventIdentifier(event_identifier)\n\n      del event_tag.event_row_identifier\n\n    return event_tag", "response": "Retrieves an event tag by identifier."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving the event tags.", "response": "def GetEventTags(self):\n    \"\"\"Retrieves the event tags.\n\n    Yields:\n      EventTag: event tag.\n    \"\"\"\n    for event_tag in self._GetAttributeContainers(\n        self._CONTAINER_TYPE_EVENT_TAG):\n      event_identifier = identifiers.SQLTableIdentifier(\n          self._CONTAINER_TYPE_EVENT, event_tag.event_row_identifier)\n      event_tag.SetEventIdentifier(event_identifier)\n\n      del event_tag.event_row_identifier\n\n      yield event_tag"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef GetNumberOfEventSources(self):\n    number_of_event_sources = self._CountStoredAttributeContainers(\n        self._CONTAINER_TYPE_EVENT_SOURCE)\n\n    number_of_event_sources += self._GetNumberOfSerializedAttributeContainers(\n        self._CONTAINER_TYPE_EVENT_SOURCE)\n    return number_of_event_sources", "response": "Retrieves the number of event sources."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nretrieve the sessions. Yields: Session: session attribute container. Raises: IOError: if there is a mismatch in session identifiers between the session start and completion attribute containers. OSError: if there is a mismatch in session identifiers between the session start and completion attribute containers.", "response": "def GetSessions(self):\n    \"\"\"Retrieves the sessions.\n\n    Yields:\n      Session: session attribute container.\n\n    Raises:\n      IOError: if there is a mismatch in session identifiers between the\n          session start and completion attribute containers.\n      OSError: if there is a mismatch in session identifiers between the\n          session start and completion attribute containers.\n    \"\"\"\n    session_start_generator = self._GetAttributeContainers(\n        self._CONTAINER_TYPE_SESSION_START)\n    session_completion_generator = self._GetAttributeContainers(\n        self._CONTAINER_TYPE_SESSION_COMPLETION)\n\n    for session_index in range(0, self._last_session):\n      session_start = next(session_start_generator)  # pylint: disable=stop-iteration-return\n      session_completion = next(session_completion_generator)  # pylint: disable=stop-iteration-return\n\n      session = sessions.Session()\n      session.CopyAttributesFromSessionStart(session_start)\n      if session_completion:\n        try:\n          session.CopyAttributesFromSessionCompletion(session_completion)\n        except ValueError:\n          raise IOError(\n              'Session identifier mismatch for session: {0:d}'.format(\n                  session_index))\n\n      yield session"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef GetSortedEvents(self, time_range=None):\n    filter_expression = None\n    if time_range:\n      filter_expression = []\n\n      if time_range.start_timestamp:\n        filter_expression.append(\n            '_timestamp >= {0:d}'.format(time_range.start_timestamp))\n\n      if time_range.end_timestamp:\n        filter_expression.append(\n            '_timestamp <= {0:d}'.format(time_range.end_timestamp))\n\n      filter_expression = ' AND '.join(filter_expression)\n\n    event_generator = self._GetAttributeContainers(\n        self._CONTAINER_TYPE_EVENT, filter_expression=filter_expression,\n        order_by='_timestamp')\n\n    for event in event_generator:\n      if hasattr(event, 'event_data_row_identifier'):\n        event_data_identifier = identifiers.SQLTableIdentifier(\n            'event_data', event.event_data_row_identifier)\n        event.SetEventDataIdentifier(event_data_identifier)\n\n        del event.event_data_row_identifier\n\n      yield event", "response": "Retrieves the events in increasing chronological order."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef HasWarnings(self):\n    # To support older storage versions, check for the now deprecated\n    # extraction errors.\n    has_errors = self._HasAttributeContainers(\n        self._CONTAINER_TYPE_EXTRACTION_ERROR)\n    if has_errors:\n      return True\n\n    return self._HasAttributeContainers(self._CONTAINER_TYPE_EXTRACTION_WARNING)", "response": "Determines if a store contains extraction warnings."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef Open(self, path=None, read_only=True, **unused_kwargs):\n    if self._is_open:\n      raise IOError('Storage file already opened.')\n\n    if not path:\n      raise ValueError('Missing path.')\n\n    path = os.path.abspath(path)\n\n    connection = sqlite3.connect(\n        path, detect_types=sqlite3.PARSE_DECLTYPES|sqlite3.PARSE_COLNAMES)\n\n    cursor = connection.cursor()\n    if not cursor:\n      return\n\n    self._connection = connection\n    self._cursor = cursor\n    self._is_open = True\n    self._read_only = read_only\n\n    if read_only:\n      self._ReadAndCheckStorageMetadata(check_readable_only=True)\n    else:\n      # self._cursor.execute('PRAGMA journal_mode=MEMORY')\n\n      # Turn off insert transaction integrity since we want to do bulk insert.\n      self._cursor.execute('PRAGMA synchronous=OFF')\n\n      if not self._HasTable('metadata'):\n        self._WriteStorageMetadata()\n      else:\n        self._ReadAndCheckStorageMetadata()\n\n      if self.compression_format == definitions.COMPRESSION_FORMAT_ZLIB:\n        data_column_type = 'BLOB'\n      else:\n        data_column_type = 'TEXT'\n\n      for container_type in self._CONTAINER_TYPES:\n        if not self._HasTable(container_type):\n          if container_type == self._CONTAINER_TYPE_EVENT:\n            query = self._CREATE_EVENT_TABLE_QUERY.format(\n                container_type, data_column_type)\n          else:\n            query = self._CREATE_TABLE_QUERY.format(\n                container_type, data_column_type)\n          self._cursor.execute(query)\n\n      self._connection.commit()\n\n    last_session_start = self._CountStoredAttributeContainers(\n        self._CONTAINER_TYPE_SESSION_START)\n\n    last_session_completion = self._CountStoredAttributeContainers(\n        self._CONTAINER_TYPE_SESSION_COMPLETION)\n\n    # Initialize next_sequence_number based on the file contents so that\n    # SQLTableIdentifier points to the correct attribute container.\n    for container_type in self._REFERENCED_CONTAINER_TYPES:\n      container_list = self._GetSerializedAttributeContainerList(container_type)\n      container_list.next_sequence_number = (\n          self._CountStoredAttributeContainers(container_type))\n\n    # TODO: handle open sessions.\n    if last_session_start != last_session_completion:\n      logger.warning('Detected unclosed session.')\n\n    self._last_session = last_session_completion", "response": "Opens the database and returns the object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ReadPreprocessingInformation(self, knowledge_base):\n    generator = self._GetAttributeContainers(\n        self._CONTAINER_TYPE_SYSTEM_CONFIGURATION)\n    for stream_number, system_configuration in enumerate(generator):\n      # TODO: replace stream_number by session_identifier.\n      knowledge_base.ReadSystemConfigurationArtifact(\n          system_configuration, session_identifier=stream_number)", "response": "Reads the preprocessing information for the given knowledge base."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef WritePreprocessingInformation(self, knowledge_base):\n    self._RaiseIfNotWritable()\n\n    if self.storage_type != definitions.STORAGE_TYPE_SESSION:\n      raise IOError('Preprocess information not supported by storage type.')\n\n    system_configuration = knowledge_base.GetSystemConfigurationArtifact()\n\n    self._WriteAttributeContainer(system_configuration)", "response": "Writes the preprocessing information."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd command line arguments to an argument group.", "response": "def AddArguments(cls, argument_group):\n    \"\"\"Adds command line arguments the helper supports to an argument group.\n\n    This function takes an argument parser or an argument group object and adds\n    to it all the command line arguments this helper supports.\n\n    Args:\n      argument_group (argparse._ArgumentGroup|argparse.ArgumentParser):\n          argparse group.\n    \"\"\"\n    argument_group.add_argument(\n        '--viper-hash', '--viper_hash', dest='viper_hash', type=str,\n        action='store', choices=viper.ViperAnalyzer.SUPPORTED_HASHES,\n        default=cls._DEFAULT_HASH, metavar='HASH', help=(\n            'Type of hash to use to query the Viper server, the default is: '\n            '{0:s}. Supported options: {1:s}').format(\n                cls._DEFAULT_HASH, ', '.join(\n                    viper.ViperAnalyzer.SUPPORTED_HASHES)))\n\n    argument_group.add_argument(\n        '--viper-host', '--viper_host', dest='viper_host', type=str,\n        action='store', default=cls._DEFAULT_HOST, metavar='HOST',\n        help=(\n            'Hostname of the Viper server to query, the default is: '\n            '{0:s}'.format(cls._DEFAULT_HOST)))\n\n    argument_group.add_argument(\n        '--viper-port', '--viper_port', dest='viper_port', type=int,\n        action='store', default=cls._DEFAULT_PORT, metavar='PORT', help=(\n            'Port of the Viper server to query, the default is: {0:d}.'.format(\n                cls._DEFAULT_PORT)))\n\n    argument_group.add_argument(\n        '--viper-protocol', '--viper_protocol', dest='viper_protocol',\n        type=str, choices=viper.ViperAnalyzer.SUPPORTED_PROTOCOLS,\n        action='store', default=cls._DEFAULT_PROTOCOL, metavar='PROTOCOL',\n        help=(\n            'Protocol to use to query Viper, the default is: {0:s}. '\n            'Supported options: {1:s}').format(\n                cls._DEFAULT_PROTOCOL, ', '.join(\n                    viper.ViperAnalyzer.SUPPORTED_PROTOCOLS)))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ParseOptions(cls, options, analysis_plugin):\n    if not isinstance(analysis_plugin, viper.ViperAnalysisPlugin):\n      raise errors.BadConfigObject(\n          'Analysis plugin is not an instance of ViperAnalysisPlugin')\n\n    lookup_hash = cls._ParseStringOption(\n        options, 'viper_hash', default_value=cls._DEFAULT_HASH)\n    analysis_plugin.SetLookupHash(lookup_hash)\n\n    host = cls._ParseStringOption(\n        options, 'viper_host', default_value=cls._DEFAULT_HOST)\n    analysis_plugin.SetHost(host)\n\n    port = cls._ParseNumericOption(\n        options, 'viper_port', default_value=cls._DEFAULT_PORT)\n    analysis_plugin.SetPort(port)\n\n    protocol = cls._ParseStringOption(\n        options, 'viper_protocol', default_value=cls._DEFAULT_PROTOCOL)\n    protocol = protocol.lower().strip()\n    analysis_plugin.SetProtocol(protocol)\n\n    if not analysis_plugin.TestConnection():\n      raise errors.BadConfigOption(\n          'Unable to connect to Viper {0:s}:{1:d}'.format(host, port))", "response": "Parses and validates options."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse a bookmark annotation row.", "response": "def ParseBookmarkAnnotationRow(\n      self, parser_mediator, query, row, **unused_kwargs):\n    \"\"\"Parses a bookmark annotation row.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      query (str): query that created the row.\n      row (sqlite3.Row): row.\n    \"\"\"\n    query_hash = hash(query)\n\n    event_data = FirefoxPlacesBookmarkAnnotationEventData()\n    event_data.content = self._GetRowValue(query_hash, row, 'content')\n    event_data.offset = self._GetRowValue(query_hash, row, 'id')\n    event_data.query = query\n    event_data.title = self._GetRowValue(query_hash, row, 'title')\n    event_data.url = self._GetRowValue(query_hash, row, 'url')\n\n    timestamp = self._GetRowValue(query_hash, row, 'dateAdded')\n    if timestamp:\n      date_time = dfdatetime_posix_time.PosixTimeInMicroseconds(\n          timestamp=timestamp)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_ADDED)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    timestamp = self._GetRowValue(query_hash, row, 'lastModified')\n    if timestamp:\n      date_time = dfdatetime_posix_time.PosixTimeInMicroseconds(\n          timestamp=timestamp)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_MODIFICATION)\n      parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ParseBookmarkFolderRow(\n      self, parser_mediator, query, row, **unused_kwargs):\n    \"\"\"Parses a bookmark folder row.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      query (str): query that created the row.\n      row (sqlite3.Row): row.\n    \"\"\"\n    query_hash = hash(query)\n\n    title = self._GetRowValue(query_hash, row, 'title')\n\n    event_data = FirefoxPlacesBookmarkFolderEventData()\n    event_data.offset = self._GetRowValue(query_hash, row, 'id')\n    event_data.query = query\n    event_data.title = title or 'N/A'\n\n    timestamp = self._GetRowValue(query_hash, row, 'dateAdded')\n    if timestamp:\n      date_time = dfdatetime_posix_time.PosixTimeInMicroseconds(\n          timestamp=timestamp)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_ADDED)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    timestamp = self._GetRowValue(query_hash, row, 'lastModified')\n    if timestamp:\n      date_time = dfdatetime_posix_time.PosixTimeInMicroseconds(\n          timestamp=timestamp)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_MODIFICATION)\n      parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Parses a bookmark folder row."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ParseBookmarkRow(\n      self, parser_mediator, query, row, **unused_kwargs):\n    \"\"\"Parses a bookmark row.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      query (str): query that created the row.\n      row (sqlite3.Row): row.\n    \"\"\"\n    query_hash = hash(query)\n\n    rev_host = self._GetRowValue(query_hash, row, 'rev_host')\n    bookmark_type = self._GetRowValue(query_hash, row, 'type')\n\n    event_data = FirefoxPlacesBookmarkEventData()\n    event_data.host = rev_host or 'N/A'\n    event_data.offset = self._GetRowValue(query_hash, row, 'id')\n    event_data.places_title = self._GetRowValue(query_hash, row, 'places_title')\n    event_data.query = query\n    event_data.title = self._GetRowValue(query_hash, row, 'bookmark_title')\n    event_data.type = self._BOOKMARK_TYPES.get(bookmark_type, 'N/A')\n    event_data.url = self._GetRowValue(query_hash, row, 'url')\n    event_data.visit_count = self._GetRowValue(query_hash, row, 'visit_count')\n\n    timestamp = self._GetRowValue(query_hash, row, 'dateAdded')\n    if timestamp:\n      date_time = dfdatetime_posix_time.PosixTimeInMicroseconds(\n          timestamp=timestamp)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_ADDED)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    timestamp = self._GetRowValue(query_hash, row, 'lastModified')\n    if timestamp:\n      date_time = dfdatetime_posix_time.PosixTimeInMicroseconds(\n          timestamp=timestamp)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_MODIFICATION)\n      parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Parses a bookmark row."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses a page visited row.", "response": "def ParsePageVisitedRow(\n      self, parser_mediator, query, row, cache=None, database=None,\n      **unused_kwargs):\n    \"\"\"Parses a page visited row.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      query (str): query that created the row.\n      row (sqlite3.Row): row.\n      cache (Optional[SQLiteCache]): cache.\n      database (Optional[SQLiteDatabase]): database.\n    \"\"\"\n    query_hash = hash(query)\n\n    from_visit = self._GetRowValue(query_hash, row, 'from_visit')\n    hidden = self._GetRowValue(query_hash, row, 'hidden')\n    rev_host = self._GetRowValue(query_hash, row, 'rev_host')\n    typed = self._GetRowValue(query_hash, row, 'typed')\n\n    # TODO: make extra conditional formatting.\n    extras = []\n    if from_visit:\n      extras.append('visited from: {0:s}'.format(\n          self._GetUrl(from_visit, cache, database)))\n\n    if hidden == '1':\n      extras.append('(url hidden)')\n\n    if typed == '1':\n      extras.append('(directly typed)')\n    else:\n      extras.append('(URL not typed directly)')\n\n    event_data = FirefoxPlacesPageVisitedEventData()\n    event_data.host = self._ReverseHostname(rev_host)\n    event_data.offset = self._GetRowValue(query_hash, row, 'id')\n    event_data.query = query\n    event_data.title = self._GetRowValue(query_hash, row, 'title')\n    event_data.url = self._GetRowValue(query_hash, row, 'url')\n    event_data.visit_count = self._GetRowValue(query_hash, row, 'visit_count')\n    event_data.visit_type = self._GetRowValue(query_hash, row, 'visit_type')\n\n    if extras:\n      event_data.extra = extras\n\n    timestamp = self._GetRowValue(query_hash, row, 'visit_date')\n    if timestamp:\n      date_time = dfdatetime_posix_time.PosixTimeInMicroseconds(\n          timestamp=timestamp)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_LAST_VISITED)\n      parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _ReverseHostname(self, hostname):\n    if not hostname:\n      return ''\n\n    if len(hostname) <= 1:\n      return hostname\n\n    if hostname[-1] == '.':\n      return hostname[::-1][1:]\n\n    return hostname[::-1][0:]", "response": "Reverses the hostname and strips the leading dot."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _GetUrl(self, url_id, cache, database):\n    url_cache_results = cache.GetResults('url')\n    if not url_cache_results:\n      result_set = database.Query(self.URL_CACHE_QUERY)\n\n      cache.CacheQueryResults(\n          result_set, 'url', 'id', ('url', 'rev_host'))\n      url_cache_results = cache.GetResults('url')\n\n    url, reverse_host = url_cache_results.get(url_id, ['', ''])\n\n    if not url:\n      return ''\n\n    hostname = self._ReverseHostname(reverse_host)\n    return '{0:s} ({1:s})'.format(url, hostname)", "response": "Retrieves an URL from a reference to an entry in the from_visit table."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ParseDownloadsRow(\n      self, parser_mediator, query, row, **unused_kwargs):\n    \"\"\"Parses a downloads row.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      query (str): query that created the row.\n      row (sqlite3.Row): row.\n    \"\"\"\n    query_hash = hash(query)\n\n    event_data = FirefoxDownloadEventData()\n    event_data.full_path = self._GetRowValue(query_hash, row, 'target')\n    event_data.mime_type = self._GetRowValue(query_hash, row, 'mimeType')\n    event_data.name = self._GetRowValue(query_hash, row, 'name')\n    event_data.offset = self._GetRowValue(query_hash, row, 'id')\n    event_data.query = query\n    event_data.received_bytes = self._GetRowValue(query_hash, row, 'currBytes')\n    event_data.referrer = self._GetRowValue(query_hash, row, 'referrer')\n    event_data.temporary_location = self._GetRowValue(\n        query_hash, row, 'tempPath')\n    event_data.total_bytes = self._GetRowValue(query_hash, row, 'maxBytes')\n    event_data.url = self._GetRowValue(query_hash, row, 'source')\n\n    timestamp = self._GetRowValue(query_hash, row, 'startTime')\n    if timestamp:\n      date_time = dfdatetime_posix_time.PosixTimeInMicroseconds(\n          timestamp=timestamp)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_START)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    timestamp = self._GetRowValue(query_hash, row, 'endTime')\n    if timestamp:\n      date_time = dfdatetime_posix_time.PosixTimeInMicroseconds(\n          timestamp=timestamp)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_END)\n      parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Parses a downloads row."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsignaling handler for the SIGSEGV signal.", "response": "def _SigSegvHandler(self, signal_number, stack_frame):\n    \"\"\"Signal handler for the SIGSEGV signal.\n\n    Args:\n      signal_number (int): numeric representation of the signal.\n      stack_frame (frame): current stack frame or None.\n    \"\"\"\n    self._OnCriticalError()\n\n    # Note that the original SIGSEGV handler can be 0.\n    if self._original_sigsegv_handler is not None:\n      # Let the original SIGSEGV handler take over.\n      signal.signal(signal.SIGSEGV, self._original_sigsegv_handler)\n      os.kill(self._pid, signal.SIGSEGV)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _StartProcessStatusRPCServer(self):\n    if self._rpc_server:\n      return\n\n    self._rpc_server = plaso_xmlrpc.XMLProcessStatusRPCServer(self._GetStatus)\n\n    hostname = 'localhost'\n\n    # Try the PID as port number first otherwise pick something random\n    # between 1024 and 60000.\n    if self._pid < 1024 or self._pid > 60000:\n      port = random.randint(1024, 60000)\n    else:\n      port = self._pid\n\n    if not self._rpc_server.Start(hostname, port):\n      port = 0\n      for _ in range(self._NUMBER_OF_RPC_SERVER_START_ATTEMPTS):\n        port = random.randint(1024, 60000)\n        if self._rpc_server.Start(hostname, port):\n          break\n\n        port = 0\n\n    if not port:\n      logger.error((\n          'Unable to start a process status RPC server for {0!s} '\n          '(PID: {1:d})').format(self._name, self._pid))\n      self._rpc_server = None\n      return\n\n    self.rpc_port.value = port\n\n    logger.debug(\n        'Process: {0!s} process status RPC server started'.format(self._name))", "response": "Starts the process status RPC server."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _StartProfiling(self, configuration):\n    if not configuration:\n      return\n\n    if configuration.HaveProfileMemoryGuppy():\n      self._guppy_memory_profiler = profilers.GuppyMemoryProfiler(\n          self._name, configuration)\n      self._guppy_memory_profiler.Start()\n\n    if configuration.HaveProfileMemory():\n      self._memory_profiler = profilers.MemoryProfiler(\n          self._name, configuration)\n      self._memory_profiler.Start()\n\n    if configuration.HaveProfileProcessing():\n      identifier = '{0:s}-processing'.format(self._name)\n      self._processing_profiler = profilers.ProcessingProfiler(\n          identifier, configuration)\n      self._processing_profiler.Start()\n\n    if configuration.HaveProfileSerializers():\n      identifier = '{0:s}-serializers'.format(self._name)\n      self._serializers_profiler = profilers.SerializersProfiler(\n          identifier, configuration)\n      self._serializers_profiler.Start()\n\n    if configuration.HaveProfileStorage():\n      self._storage_profiler = profilers.StorageProfiler(\n          self._name, configuration)\n      self._storage_profiler.Start()\n\n    if configuration.HaveProfileTasks():\n      self._tasks_profiler = profilers.TasksProfiler(self._name, configuration)\n      self._tasks_profiler.Start()", "response": "Starts profiling.\n\n    Args:\n      configuration (ProfilingConfiguration): profiling configuration."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _StopProcessStatusRPCServer(self):\n    if not self._rpc_server:\n      return\n\n    # Make sure the engine gets one more status update so it knows\n    # the worker has completed.\n    self._WaitForStatusNotRunning()\n\n    self._rpc_server.Stop()\n    self._rpc_server = None\n    self.rpc_port.value = 0\n\n    logger.debug(\n        'Process: {0!s} process status RPC server stopped'.format(self._name))", "response": "Stops the process status RPC server."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwaits for the status is not running.", "response": "def _WaitForStatusNotRunning(self):\n    \"\"\"Waits for the status is running to change to false.\"\"\"\n    # We wait slightly longer than the status check sleep time.\n    time.sleep(2.0)\n    time_slept = 2.0\n    while self._status_is_running:\n      time.sleep(0.5)\n      time_slept += 0.5\n      if time_slept >= self._PROCESS_JOIN_TIMEOUT:\n        break"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse a data object.", "response": "def _ParseDataObject(self, file_object, file_offset):\n    \"\"\"Parses a data object.\n\n    Args:\n      file_object (dfvfs.FileIO): a file-like object.\n      file_offset (int): offset of the data object relative to the start\n          of the file-like object.\n\n    Returns:\n      bytes: data.\n\n    Raises:\n      ParseError: if the data object cannot be parsed.\n    \"\"\"\n    data_object_map = self._GetDataTypeMap('systemd_journal_data_object')\n\n    try:\n      data_object, _ = self._ReadStructureFromFileObject(\n          file_object, file_offset, data_object_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.ParseError((\n          'Unable to parse data object at offset: 0x{0:08x} with error: '\n          '{1!s}').format(file_offset, exception))\n\n    if data_object.object_type != self._OBJECT_TYPE_DATA:\n      raise errors.ParseError('Unsupported object type: {0:d}.'.format(\n          data_object.object_type))\n\n    if data_object.object_flags not in (\n        0, self._OBJECT_COMPRESSED_FLAG_XZ, self._OBJECT_COMPRESSED_FLAG_LZ4):\n      raise errors.ParseError('Unsupported object flags: 0x{0:02x}.'.format(\n          data_object.object_flags))\n\n    # The data is read separately for performance reasons.\n    data_size = data_object.data_size - 64\n    data = file_object.read(data_size)\n\n    if data_object.object_flags & self._OBJECT_COMPRESSED_FLAG_XZ:\n      data = lzma.decompress(data)\n\n    elif data_object.object_flags & self._OBJECT_COMPRESSED_FLAG_LZ4:\n      uncompressed_size_map = self._GetDataTypeMap('uint32le')\n\n      try:\n        uncompressed_size = self._ReadStructureFromByteStream(\n            data, file_offset + 64, uncompressed_size_map)\n      except (ValueError, errors.ParseError) as exception:\n        raise errors.ParseError((\n            'Unable to parse LZ4 uncompressed size at offset: 0x{0:08x} with '\n            'error: {1!s}').format(file_offset + 64, exception))\n\n      data = lz4.block.decompress(\n          data[8:], uncompressed_size=uncompressed_size)\n\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _ParseEntryArrayObject(self, file_object, file_offset):\n    entry_array_object_map = self._GetDataTypeMap(\n        'systemd_journal_entry_array_object')\n\n    try:\n      entry_array_object, _ = self._ReadStructureFromFileObject(\n          file_object, file_offset, entry_array_object_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.ParseError((\n          'Unable to parse entry array object at offset: 0x{0:08x} with error: '\n          '{1!s}').format(file_offset, exception))\n\n    if entry_array_object.object_type != self._OBJECT_TYPE_ENTRY_ARRAY:\n      raise errors.ParseError('Unsupported object type: {0:d}.'.format(\n          entry_array_object.object_type))\n\n    if entry_array_object.object_flags != 0:\n      raise errors.ParseError('Unsupported object flags: 0x{0:02x}.'.format(\n          entry_array_object.object_flags))\n\n    return entry_array_object", "response": "Parses an entry array object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing an entry object.", "response": "def _ParseEntryObject(self, file_object, file_offset):\n    \"\"\"Parses an entry object.\n\n    Args:\n      file_object (dfvfs.FileIO): a file-like object.\n      file_offset (int): offset of the entry object relative to the start\n          of the file-like object.\n\n    Returns:\n      systemd_journal_entry_object: entry object.\n\n    Raises:\n      ParseError: if the entry object cannot be parsed.\n    \"\"\"\n    entry_object_map = self._GetDataTypeMap('systemd_journal_entry_object')\n\n    try:\n      entry_object, _ = self._ReadStructureFromFileObject(\n          file_object, file_offset, entry_object_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.ParseError((\n          'Unable to parse entry object at offset: 0x{0:08x} with error: '\n          '{1!s}').format(file_offset, exception))\n\n    if entry_object.object_type != self._OBJECT_TYPE_ENTRY:\n      raise errors.ParseError('Unsupported object type: {0:d}.'.format(\n          entry_object.object_type))\n\n    if entry_object.object_flags != 0:\n      raise errors.ParseError('Unsupported object flags: 0x{0:02x}.'.format(\n          entry_object.object_flags))\n\n    return entry_object"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _ParseEntryObjectOffsets(self, file_object, file_offset):\n    entry_array_object = self._ParseEntryArrayObject(file_object, file_offset)\n\n    entry_object_offsets = list(entry_array_object.entry_object_offsets)\n    while entry_array_object.next_entry_array_offset != 0:\n      entry_array_object = self._ParseEntryArrayObject(\n          file_object, entry_array_object.next_entry_array_offset)\n      entry_object_offsets.extend(entry_array_object.entry_object_offsets)\n\n    return entry_object_offsets", "response": "Parses the entry array objects for the offset of the entry objects."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing a journal entry.", "response": "def _ParseJournalEntry(self, file_object, file_offset):\n    \"\"\"Parses a journal entry.\n\n    This method will generate an event per ENTRY object.\n\n    Args:\n      file_object (dfvfs.FileIO): a file-like object.\n      file_offset (int): offset of the entry object relative to the start\n          of the file-like object.\n\n    Returns:\n      dict[str, objects]: entry items per key.\n\n    Raises:\n      ParseError: when an object offset is out of bounds.\n    \"\"\"\n    entry_object = self._ParseEntryObject(file_object, file_offset)\n\n    # The data is read separately for performance reasons.\n    entry_item_map = self._GetDataTypeMap('systemd_journal_entry_item')\n\n    file_offset += 64\n    data_end_offset = file_offset + entry_object.data_size - 64\n\n    fields = {'real_time': entry_object.real_time}\n\n    while file_offset < data_end_offset:\n      try:\n        entry_item, entry_item_data_size = self._ReadStructureFromFileObject(\n            file_object, file_offset, entry_item_map)\n      except (ValueError, errors.ParseError) as exception:\n        raise errors.ParseError((\n            'Unable to parse entry item at offset: 0x{0:08x} with error: '\n            '{1!s}').format(file_offset, exception))\n\n      file_offset += entry_item_data_size\n\n      if entry_item.object_offset < self._maximum_journal_file_offset:\n        raise errors.ParseError(\n            'object offset should be after hash tables ({0:d} < {1:d})'.format(\n                entry_item.object_offset, self._maximum_journal_file_offset))\n\n      event_data = self._ParseDataObject(file_object, entry_item.object_offset)\n      event_string = event_data.decode('utf-8')\n      key, value = event_string.split('=', 1)\n      fields[key] = value\n\n    return fields"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ParseFileObject(self, parser_mediator, file_object):\n    file_header_map = self._GetDataTypeMap('systemd_journal_file_header')\n\n    try:\n      file_header, _ = self._ReadStructureFromFileObject(\n          file_object, 0, file_header_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.UnableToParseFile(\n          'Unable to parse file header with error: {0!s}'.format(\n              exception))\n\n    if file_header.signature != self._FILE_SIGNATURE:\n      raise errors.UnableToParseFile('Invalid file signature.')\n\n    if file_header.header_size not in self._SUPPORTED_FILE_HEADER_SIZES:\n      raise errors.UnableToParseFile(\n          'Unsupported file header size: {0:d}.'.format(\n              file_header.header_size))\n\n    data_hash_table_end_offset = (\n        file_header.data_hash_table_offset +\n        file_header.data_hash_table_size)\n    field_hash_table_end_offset = (\n        file_header.field_hash_table_offset +\n        file_header.field_hash_table_size)\n    self._maximum_journal_file_offset = max(\n        data_hash_table_end_offset, field_hash_table_end_offset)\n\n    entry_object_offsets = self._ParseEntryObjectOffsets(\n        file_object, file_header.entry_array_offset)\n\n    for entry_object_offset in entry_object_offsets:\n      if entry_object_offset == 0:\n        continue\n\n      try:\n        fields = self._ParseJournalEntry(file_object, entry_object_offset)\n      except errors.ParseError as exception:\n        parser_mediator.ProduceExtractionWarning((\n            'Unable to parse journal entry at offset: 0x{0:08x} with '\n            'error: {1!s}').format(entry_object_offset, exception))\n        return\n\n      event_data = SystemdJournalEventData()\n\n      event_data.body = fields.get('MESSAGE', None)\n      event_data.hostname = fields.get('_HOSTNAME', None)\n      event_data.reporter = fields.get('SYSLOG_IDENTIFIER', None)\n\n      if event_data.reporter and event_data.reporter != 'kernel':\n        event_data.pid = fields.get('_PID', fields.get('SYSLOG_PID', None))\n\n      date_time = dfdatetime_posix_time.PosixTimeInMicroseconds(\n          timestamp=fields['real_time'])\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_WRITTEN)\n      parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Parses a Systemd journal file - like object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef GetEventTypeString(self, event_type):\n    if 0 <= event_type < len(self._EVENT_TYPES):\n      return self._EVENT_TYPES[event_type]\n    return 'Unknown {0:d}'.format(event_type)", "response": "Retrieves a string representation of the event type."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves a string representation of the severity.", "response": "def GetSeverityString(self, severity):\n    \"\"\"Retrieves a string representation of the severity.\n\n    Args:\n      severity (int): severity.\n\n    Returns:\n      str: description of the event severity.\n    \"\"\"\n    if 0 <= severity < len(self._SEVERITY):\n      return self._SEVERITY[severity]\n    return 'Unknown {0:d}'.format(severity)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef GetMessages(self, formatter_mediator, event):\n    if self.DATA_TYPE != event.data_type:\n      raise errors.WrongFormatter('Unsupported data type: {0:s}.'.format(\n          event.data_type))\n\n    event_values = event.CopyToDict()\n\n    event_type = event_values.get('event_type', None)\n    if event_type is not None:\n      event_values['event_type'] = self.GetEventTypeString(event_type)\n\n    # TODO: add string representation of facility.\n\n    severity = event_values.get('severity', None)\n    if severity is not None:\n      event_values['severity'] = self.GetSeverityString(severity)\n\n    source_name = event_values.get('source_name', None)\n    message_identifier = event_values.get('message_identifier', None)\n    strings = event_values.get('strings', [])\n    if source_name and message_identifier:\n      message_string = formatter_mediator.GetWindowsEventMessage(\n          source_name, message_identifier)\n      if message_string:\n        try:\n          event_values['message_string'] = message_string.format(*strings)\n        except IndexError:\n          # Unable to create the message string.\n          pass\n\n    message_strings = []\n    for string in strings:\n      message_strings.append('\\'{0:s}\\''.format(string))\n    message_string = ', '.join(message_strings)\n    event_values['strings'] = '[{0:s}]'.format(message_string)\n\n    return self._ConditionalFormatMessages(event_values)", "response": "Determines the formatted message strings for an event object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwriting the body of an event object to the output.", "response": "def WriteEventBody(self, event):\n    \"\"\"Writes the body of an event object to the output.\n\n    Args:\n      event (EventObject): event.\n    \"\"\"\n    inode = getattr(event, 'inode', None)\n    if inode is None:\n      event.inode = 0\n\n    json_dict = self._JSON_SERIALIZER.WriteSerializedDict(event)\n    json_string = json.dumps(json_dict, sort_keys=True)\n\n    if self._event_counter != 0:\n      self._output_writer.Write(', ')\n\n    line = '\"event_{0:d}\": {1:s}\\n'.format(self._event_counter, json_string)\n    self._output_writer.Write(line)\n\n    self._event_counter += 1"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbuild a dictionary object based on a SQL command. This function will take a SQL command, execute it and for each resulting row it will store a key in a dictionary. An example:: sql_results = A SQL result object after executing the SQL command: 'SELECT foo, bla, bar FROM my_table' attribute_name = 'all_the_things' key_name = 'foo' column_names = ['bla', 'bar'] Results from running this against the database: 'first', 'stuff', 'things' 'second', 'another stuff', 'another thing' This will result in a dictionary object being created in the cache, called 'all_the_things' and it will contain the following value:: all_the_things = { 'first': ['stuff', 'things'], 'second': ['another_stuff', 'another_thing'], 'third': ['single_thing']} Args: sql_results (sqlite3.Cursor): result after executing a SQL command on a database. attribute_name (str): attribute name in the cache to store results to. This will be the name of the dictionary attribute. key_name (str): name of the result field that should be used as a key in the resulting dictionary that is created. column_names (list[str]): of column names that are stored as values to the dictionary. If this list has only one value in it the value will be stored directly, otherwise the value will be a list containing the extracted results based on the names provided in this list.", "response": "def CacheQueryResults(\n      self, sql_results, attribute_name, key_name, column_names):\n    \"\"\"Build a dictionary object based on a SQL command.\n\n    This function will take a SQL command, execute it and for\n    each resulting row it will store a key in a dictionary.\n\n    An example::\n\n      sql_results = A SQL result object after executing the\n                    SQL command: 'SELECT foo, bla, bar FROM my_table'\n      attribute_name = 'all_the_things'\n      key_name = 'foo'\n      column_names = ['bla', 'bar']\n\n    Results from running this against the database:\n    'first', 'stuff', 'things'\n    'second', 'another stuff', 'another thing'\n\n    This will result in a dictionary object being created in the\n    cache, called 'all_the_things' and it will contain the following value::\n\n      all_the_things = {\n          'first': ['stuff', 'things'],\n          'second': ['another_stuff', 'another_thing'],\n          'third': ['single_thing']}\n\n    Args:\n      sql_results (sqlite3.Cursor): result after executing a SQL command\n          on a database.\n      attribute_name (str): attribute name in the cache to store results to.\n          This will be the name of the dictionary attribute.\n      key_name (str): name of the result field that should be used as a key\n          in the resulting dictionary that is created.\n      column_names (list[str]): of column names that are stored as values to\n          the dictionary. If this list has only one value in it the value will\n          be stored directly, otherwise the value will be a list containing\n          the extracted results based on the names provided in this list.\n    \"\"\"\n    row = sql_results.fetchone()\n    if not row:\n      return\n\n    # Note that pysqlite does not accept a Unicode string in row['string'] and\n    # will raise \"IndexError: Index must be int or string\".\n    keys_name_to_index_map = {\n        name: index for index, name in enumerate(row.keys())}\n\n    attribute_value = {}\n    while row:\n      value_index = keys_name_to_index_map.get(key_name)\n      key_value = row[value_index]\n\n      attribute_value[key_value] = []\n      for column_name in column_names:\n        value_index = keys_name_to_index_map.get(column_name)\n        column_value = row[value_index]\n        attribute_value[key_value].append(column_value)\n\n      row = sql_results.fetchone()\n\n    setattr(self, attribute_name, attribute_value)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef GetRowCache(self, query):\n    query_hash = hash(query)\n    if query_hash not in self._row_caches:\n      self._row_caches[query_hash] = set()\n    return self._row_caches[query_hash]", "response": "Retrieves the row cache for a specific query."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _CopyFileObjectToTemporaryFile(self, file_object, temporary_file):\n    file_object.seek(0, os.SEEK_SET)\n    data = file_object.read(self._READ_BUFFER_SIZE)\n    while data:\n      temporary_file.write(data)\n      data = file_object.read(self._READ_BUFFER_SIZE)", "response": "Copies the contents of the file - like object to a temporary file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef Close(self):\n    self.schema = {}\n\n    if self._is_open:\n      self._database.close()\n    self._database = None\n\n    if os.path.exists(self._temp_db_file_path):\n      try:\n        os.remove(self._temp_db_file_path)\n      except (OSError, IOError) as exception:\n        logger.warning((\n            'Unable to remove temporary copy: {0:s} of SQLite database: '\n            '{1:s} with error: {2!s}').format(\n                self._temp_db_file_path, self._filename, exception))\n\n    self._temp_db_file_path = ''\n\n    if os.path.exists(self._temp_wal_file_path):\n      try:\n        os.remove(self._temp_wal_file_path)\n      except (OSError, IOError) as exception:\n        logger.warning((\n            'Unable to remove temporary copy: {0:s} of SQLite database: '\n            '{1:s} with error: {2!s}').format(\n                self._temp_wal_file_path, self._filename, exception))\n\n    self._temp_wal_file_path = ''\n\n    self._is_open = False", "response": "Closes the database connection and cleans up the temporary file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nopening a SQLite database file.", "response": "def Open(self, file_object, wal_file_object=None):\n    \"\"\"Opens a SQLite database file.\n\n    Since pysqlite cannot read directly from a file-like object a temporary\n    copy of the file is made. After creating a copy the database file this\n    function sets up a connection with the database and determines the names\n    of the tables.\n\n    Args:\n      file_object (dfvfs.FileIO): file-like object.\n      wal_file_object (Optional[dfvfs.FileIO]): file-like object for the\n          Write-Ahead Log (WAL) file.\n\n    Raises:\n      IOError: if the file-like object cannot be read.\n      OSError: if the file-like object cannot be read.\n      sqlite3.DatabaseError: if the database cannot be parsed.\n      ValueError: if the file-like object is missing.\n    \"\"\"\n    if not file_object:\n      raise ValueError('Missing file object.')\n\n    # TODO: Current design copies the entire file into a buffer\n    # that is parsed by each SQLite parser. This is not very efficient,\n    # especially when many SQLite parsers are ran against a relatively\n    # large SQLite database. This temporary file that is created should\n    # be usable by all SQLite parsers so the file should only be read\n    # once in memory and then deleted when all SQLite parsers have completed.\n\n    # TODO: Change this into a proper implementation using APSW\n    # and virtual filesystems when that will be available.\n    # Info: http://apidoc.apsw.googlecode.com/hg/vfs.html#vfs and\n    # http://apidoc.apsw.googlecode.com/hg/example.html#example-vfs\n    # Until then, just copy the file into a tempfile and parse it.\n\n    temporary_file = tempfile.NamedTemporaryFile(\n        delete=False, dir=self._temporary_directory)\n\n    try:\n      self._CopyFileObjectToTemporaryFile(file_object, temporary_file)\n      self._temp_db_file_path = temporary_file.name\n\n    except IOError:\n      os.remove(temporary_file.name)\n      raise\n\n    finally:\n      temporary_file.close()\n\n    if wal_file_object:\n      # Create WAL file using same filename so it is available for\n      # sqlite3.connect()\n      temporary_filename = '{0:s}-wal'.format(self._temp_db_file_path)\n      temporary_file = open(temporary_filename, 'wb')\n      try:\n        self._CopyFileObjectToTemporaryFile(wal_file_object, temporary_file)\n        self._temp_wal_file_path = temporary_filename\n\n      except IOError:\n        os.remove(temporary_filename)\n        raise\n\n      finally:\n        temporary_file.close()\n\n    self._database = sqlite3.connect(self._temp_db_file_path)\n    try:\n      self._database.row_factory = sqlite3.Row\n      cursor = self._database.cursor()\n\n      sql_results = cursor.execute(self.SCHEMA_QUERY)\n\n      self.schema = {\n          table_name: ' '.join(query.split())\n          for table_name, query in sql_results}\n\n    except sqlite3.DatabaseError as exception:\n      self._database.close()\n      self._database = None\n\n      os.remove(self._temp_db_file_path)\n      self._temp_db_file_path = ''\n      if self._temp_wal_file_path:\n        os.remove(self._temp_wal_file_path)\n        self._temp_wal_file_path = ''\n\n      logger.debug(\n          'Unable to parse SQLite database: {0:s} with error: {1!s}'.format(\n              self._filename, exception))\n      raise\n\n    self._is_open = True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef Query(self, query):\n    cursor = self._database.cursor()\n    cursor.execute(query)\n    return cursor", "response": "Queries the database.\n\n    Args:\n      query (str): SQL query.\n\n    Returns:\n      sqlite3.Cursor: results.\n\n    Raises:\n      sqlite3.DatabaseError: if querying the database fails."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nopens a SQLite database with its Write - Averaging Log committed.", "response": "def _OpenDatabaseWithWAL(\n      self, parser_mediator, database_file_entry, database_file_object,\n      filename):\n    \"\"\"Opens a database with its Write-Ahead Log (WAL) committed.\n\n    Args:\n      parser_mediator (ParserMediator): parser mediator.\n      database_file_entry (dfvfs.FileEntry): file entry of the database.\n      database_file_object (dfvfs.FileIO): file-like object of the database.\n      filename (str): name of the database file entry.\n\n    Returns:\n      tuple: contains:\n\n        SQLiteDatabase: a database object with WAL file committed or None\n        dfvfs.FileEntry: a file entry object of WAL file or None\n    \"\"\"\n    path_spec = database_file_entry.path_spec\n    location = getattr(path_spec, 'location', None)\n    if not path_spec or not location:\n      return None, None\n\n    location_wal = '{0:s}-wal'.format(location)\n    file_system = database_file_entry.GetFileSystem()\n    wal_path_spec = dfvfs_factory.Factory.NewPathSpec(\n        file_system.type_indicator, parent=path_spec.parent,\n        location=location_wal)\n\n    wal_file_entry = file_system.GetFileEntryByPathSpec(wal_path_spec)\n    if not wal_file_entry:\n      return None, None\n\n    wal_file_object = wal_file_entry.GetFileObject()\n    if not wal_file_object:\n      return None, None\n\n    database_wal = SQLiteDatabase(\n        filename, temporary_directory=parser_mediator.temporary_directory)\n\n    try:\n      database_wal.Open(database_file_object, wal_file_object=wal_file_object)\n\n    except (IOError, ValueError, sqlite3.DatabaseError) as exception:\n      parser_mediator.ProduceExtractionWarning((\n          'unable to open SQLite database and WAL with error: '\n          '{0!s}').format(exception))\n\n      return None, None\n\n    finally:\n      wal_file_object.close()\n\n    return database_wal, wal_file_entry"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing a SQLite database file entry.", "response": "def ParseFileEntry(self, parser_mediator, file_entry):\n    \"\"\"Parses a SQLite database file entry.\n\n    Args:\n      parser_mediator (ParserMediator): parser mediator.\n      file_entry (dfvfs.FileEntry): file entry to be parsed.\n\n    Raises:\n      UnableToParseFile: when the file cannot be parsed.\n    \"\"\"\n    filename = parser_mediator.GetFilename()\n    database = SQLiteDatabase(\n        filename, temporary_directory=parser_mediator.temporary_directory)\n\n    file_object = file_entry.GetFileObject()\n    try:\n      database.Open(file_object)\n\n    except (IOError, ValueError, sqlite3.DatabaseError) as exception:\n      parser_mediator.ProduceExtractionWarning(\n          'unable to open SQLite database with error: {0!s}'.format(exception))\n      file_object.close()\n      return\n\n    database_wal, wal_file_entry = self._OpenDatabaseWithWAL(\n        parser_mediator, file_entry, file_object, filename)\n\n    file_object.close()\n\n    # Create a cache in which the resulting tables are cached.\n    cache = SQLiteCache()\n    try:\n      table_names = frozenset(database.tables)\n\n      for plugin in self._plugins:\n        if not plugin.REQUIRED_TABLES.issubset(table_names):\n          continue\n\n        schema_match = plugin.CheckSchema(database)\n        if plugin.REQUIRES_SCHEMA_MATCH and not schema_match:\n          parser_mediator.ProduceExtractionWarning((\n              'plugin: {0:s} found required tables but not a matching '\n              'schema').format(plugin.NAME))\n          continue\n\n        parser_mediator.SetFileEntry(file_entry)\n        parser_mediator.AddEventAttribute('schema_match', schema_match)\n\n        try:\n          plugin.UpdateChainAndProcess(\n              parser_mediator, cache=cache, database=database,\n              database_wal=database_wal, wal_file_entry=wal_file_entry)\n\n        except Exception as exception:  # pylint: disable=broad-except\n          parser_mediator.ProduceExtractionWarning((\n              'plugin: {0:s} unable to parse SQLite database with error: '\n              '{1!s}').format(plugin.NAME, exception))\n\n        finally:\n          parser_mediator.RemoveEventAttribute('schema_match')\n\n        if not database_wal:\n          continue\n\n        schema_match = plugin.CheckSchema(database)\n\n        parser_mediator.SetFileEntry(wal_file_entry)\n        parser_mediator.AddEventAttribute('schema_match', schema_match)\n\n        try:\n          plugin.UpdateChainAndProcess(\n              parser_mediator, cache=cache, database=database,\n              database_wal=database_wal, wal_file_entry=wal_file_entry)\n\n        except Exception as exception:  # pylint: disable=broad-except\n          parser_mediator.ProduceExtractionWarning((\n              'plugin: {0:s} unable to parse SQLite database and WAL with '\n              'error: {1!s}').format(plugin.NAME, exception))\n\n        finally:\n          parser_mediator.RemoveEventAttribute('schema_match')\n\n    finally:\n      database.Close()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nextract events from a Windows Registry key.", "response": "def ExtractEvents(self, parser_mediator, registry_key, **kwargs):\n    \"\"\"Extracts events from a Windows Registry key.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      registry_key (dfwinreg.WinRegistryKey): Windows Registry key.\n    \"\"\"\n    for subkey in registry_key.GetSubkeys():\n      values_dict = {}\n      values_dict['subkey_name'] = subkey.name\n\n      name_values = subkey.name.split('&')\n      number_of_name_values = len(name_values)\n\n      # Normally we expect 4 fields here however that is not always the case.\n      if number_of_name_values != 4:\n        logger.warning(\n            'Expected 4 &-separated values in: {0:s}'.format(subkey.name))\n\n      if number_of_name_values >= 1:\n        values_dict['device_type'] = name_values[0]\n      if number_of_name_values >= 2:\n        values_dict['vendor'] = name_values[1]\n      if number_of_name_values >= 3:\n        values_dict['product'] = name_values[2]\n      if number_of_name_values >= 4:\n        values_dict['revision'] = name_values[3]\n\n      event_data = windows_events.WindowsRegistryEventData()\n      event_data.key_path = registry_key.path\n      event_data.offset = registry_key.offset\n      event_data.regvalue = values_dict\n      event_data.source_append = self._SOURCE_APPEND\n\n      if subkey.number_of_subkeys == 0:\n        # Time last USB device of this class was first inserted.\n        event = time_events.DateTimeValuesEvent(\n            subkey.last_written_time, definitions.TIME_DESCRIPTION_WRITTEN)\n        parser_mediator.ProduceEventWithEventData(event, event_data)\n        continue\n\n      for device_key in subkey.GetSubkeys():\n        values_dict['serial'] = device_key.name\n\n        friendly_name_value = device_key.GetValueByName('FriendlyName')\n        if friendly_name_value:\n          values_dict['friendly_name'] = friendly_name_value.GetDataAsObject()\n        else:\n          values_dict.pop('friendly_name', None)\n\n        # ParentIdPrefix applies to Windows XP Only.\n        parent_id_prefix_value = device_key.GetValueByName('ParentIdPrefix')\n        if parent_id_prefix_value:\n          values_dict['parent_id_prefix'] = (\n              parent_id_prefix_value.GetDataAsObject())\n        else:\n          values_dict.pop('parent_id_prefix', None)\n\n        # Time last USB device of this class was first inserted.\n        event = time_events.DateTimeValuesEvent(\n            subkey.last_written_time, definitions.TIME_DESCRIPTION_WRITTEN)\n        parser_mediator.ProduceEventWithEventData(event, event_data)\n\n        # Win7 - Last Connection.\n        # Vista/XP - Time of an insert.\n        event = time_events.DateTimeValuesEvent(\n            device_key.last_written_time, definitions.TIME_DESCRIPTION_WRITTEN)\n        parser_mediator.ProduceEventWithEventData(event, event_data)\n\n        device_parameter_key = device_key.GetSubkeyByName('Device Parameters')\n        if device_parameter_key:\n          event = time_events.DateTimeValuesEvent(\n              device_parameter_key.last_written_time,\n              definitions.TIME_DESCRIPTION_WRITTEN)\n          parser_mediator.ProduceEventWithEventData(event, event_data)\n\n        log_configuration_key = device_key.GetSubkeyByName('LogConf')\n        if log_configuration_key:\n          event = time_events.DateTimeValuesEvent(\n              log_configuration_key.last_written_time,\n              definitions.TIME_DESCRIPTION_WRITTEN)\n          parser_mediator.ProduceEventWithEventData(event, event_data)\n\n        properties_key = device_key.GetSubkeyByName('Properties')\n        if properties_key:\n          event = time_events.DateTimeValuesEvent(\n              properties_key.last_written_time,\n              definitions.TIME_DESCRIPTION_WRITTEN)\n          parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _ParseCString(self, page_data, string_offset):\n    cstring_map = self._GetDataTypeMap('cstring')\n\n    try:\n      value_string = self._ReadStructureFromByteStream(\n          page_data[string_offset:], string_offset, cstring_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.ParseError((\n          'Unable to map string data at offset: 0x{0:08x} with error: '\n          '{1!s}').format(string_offset, exception))\n\n    return value_string.rstrip('\\x00')", "response": "Parses a C string from the page data."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses a page. Args: parser_mediator (ParserMediator): parser mediator. file_offset (int): offset of the data relative from the start of the file-like object. page_data (bytes): page data. Raises: ParseError: when the page cannot be parsed.", "response": "def _ParsePage(self, parser_mediator, file_offset, page_data):\n    \"\"\"Parses a page.\n\n    Args:\n      parser_mediator (ParserMediator): parser mediator.\n      file_offset (int): offset of the data relative from the start of\n          the file-like object.\n      page_data (bytes): page data.\n\n    Raises:\n      ParseError: when the page cannot be parsed.\n    \"\"\"\n    page_header_map = self._GetDataTypeMap('binarycookies_page_header')\n\n    try:\n      page_header = self._ReadStructureFromByteStream(\n          page_data, file_offset, page_header_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.ParseError((\n          'Unable to map page header data at offset: 0x{0:08x} with error: '\n          '{1!s}').format(file_offset, exception))\n\n    for record_offset in page_header.offsets:\n      if parser_mediator.abort:\n        break\n\n      self._ParseRecord(parser_mediator, page_data, record_offset)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse a record from the page data.", "response": "def _ParseRecord(self, parser_mediator, page_data, record_offset):\n    \"\"\"Parses a record from the page data.\n\n    Args:\n      parser_mediator (ParserMediator): parser mediator.\n      page_data (bytes): page data.\n      record_offset (int): offset of the record relative to the start\n          of the page.\n\n    Raises:\n      ParseError: when the record cannot be parsed.\n    \"\"\"\n    record_header_map = self._GetDataTypeMap('binarycookies_record_header')\n\n    try:\n      record_header = self._ReadStructureFromByteStream(\n          page_data[record_offset:], record_offset, record_header_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.ParseError((\n          'Unable to map record header data at offset: 0x{0:08x} with error: '\n          '{1!s}').format(record_offset, exception))\n\n    event_data = SafariBinaryCookieEventData()\n    event_data.flags = record_header.flags\n\n    if record_header.url_offset:\n      data_offset = record_offset + record_header.url_offset\n      event_data.url = self._ParseCString(page_data, data_offset)\n\n    if record_header.name_offset:\n      data_offset = record_offset + record_header.name_offset\n      event_data.cookie_name = self._ParseCString(page_data, data_offset)\n\n    if record_header.path_offset:\n      data_offset = record_offset + record_header.path_offset\n      event_data.path = self._ParseCString(page_data, data_offset)\n\n    if record_header.value_offset:\n      data_offset = record_offset + record_header.value_offset\n      event_data.cookie_value = self._ParseCString(page_data, data_offset)\n\n    if record_header.creation_time:\n      date_time = dfdatetime_cocoa_time.CocoaTime(\n          timestamp=record_header.creation_time)\n      event = time_events.DateTimeValuesEvent(\n          date_time, definitions.TIME_DESCRIPTION_CREATION)\n      parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    if record_header.expiration_time:\n      date_time = dfdatetime_cocoa_time.CocoaTime(\n          timestamp=record_header.expiration_time)\n    else:\n      date_time = dfdatetime_semantic_time.SemanticTime('Not set')\n\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_EXPIRATION)\n    parser_mediator.ProduceEventWithEventData(event, event_data)\n\n    for plugin in self._cookie_plugins:\n      if parser_mediator.abort:\n        break\n\n      if event_data.cookie_name != plugin.COOKIE_NAME:\n        continue\n\n      try:\n        plugin.UpdateChainAndProcess(\n            parser_mediator, cookie_name=event_data.cookie_name,\n            cookie_data=event_data.cookie_value, url=event_data.url)\n\n      except Exception as exception:  # pylint: disable=broad-except\n        parser_mediator.ProduceExtractionWarning(\n            'plugin: {0:s} unable to parse cookie with error: {1!s}'.format(\n                plugin.NAME, exception))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ParseFileObject(self, parser_mediator, file_object):\n    file_header_map = self._GetDataTypeMap('binarycookies_file_header')\n\n    try:\n      file_header, file_header_data_size = self._ReadStructureFromFileObject(\n          file_object, 0, file_header_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.UnableToParseFile(\n          'Unable to read file header with error: {0!s}.'.format(exception))\n\n    if file_header.signature != self._SIGNATURE:\n      raise errors.UnableToParseFile('Unsupported file signature.')\n\n    file_offset = file_header_data_size\n\n    # TODO: move page sizes array into file header, this will require dtFabric\n    # to compare signature as part of data map.\n    page_sizes_data_size = file_header.number_of_pages * 4\n\n    page_sizes_data = file_object.read(page_sizes_data_size)\n\n    context = dtfabric_data_maps.DataTypeMapContext(values={\n        'binarycookies_file_header': file_header})\n\n    page_sizes_map = self._GetDataTypeMap('binarycookies_page_sizes')\n\n    try:\n      page_sizes_array = self._ReadStructureFromByteStream(\n          page_sizes_data, file_offset, page_sizes_map, context=context)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.ParseError((\n          'Unable to map page sizes data at offset: 0x{0:08x} with error: '\n          '{1!s}').format(file_offset, exception))\n\n    file_offset += page_sizes_data_size\n\n    for page_number, page_size in enumerate(page_sizes_array):\n      if parser_mediator.abort:\n        break\n\n      page_data = file_object.read(page_size)\n      if len(page_data) != page_size:\n        parser_mediator.ProduceExtractionWarning(\n            'unable to read page: {0:d}'.format(page_number))\n        break\n\n      self._ParsePage(parser_mediator, file_offset, page_data)\n\n      file_offset += page_size", "response": "Parses a Safari binary cookie file - like object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef AddArguments(cls, argument_group):\n    argument_group.add_argument(\n        '--disable_zeromq', '--disable-zeromq', action='store_false',\n        dest='use_zeromq', default=True, help=(\n            'Disable queueing using ZeroMQ. A Multiprocessing queue will be '\n            'used instead.'))", "response": "Adds command line arguments to an argument group."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ParseOptions(cls, options, configuration_object):\n    if not isinstance(configuration_object, tools.CLITool):\n      raise errors.BadConfigObject(\n          'Configuration object is not an instance of CLITool')\n\n    use_zeromq = getattr(options, 'use_zeromq', True)\n\n    setattr(configuration_object, '_use_zeromq', use_zeromq)", "response": "Parses and validates options."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nformats the date field.", "response": "def _FormatDate(self, event):\n    \"\"\"Formats the date.\n\n    Args:\n      event (EventObject): event.\n\n    Returns:\n      str: date field.\n    \"\"\"\n    # TODO: preserve dfdatetime as an object.\n    # TODO: add support for self._output_mediator.timezone\n    date_time = dfdatetime_posix_time.PosixTimeInMicroseconds(\n        timestamp=event.timestamp)\n\n    year, month, day_of_month = date_time.GetDate()\n    try:\n      return '{0:04d}-{1:02d}-{2:02d}'.format(year, month, day_of_month)\n    except (TypeError, ValueError):\n      self._ReportEventError(event, (\n          'unable to copy timestamp: {0!s} to a human readable date. '\n          'Defaulting to: \"0000-00-00\"').format(event.timestamp))\n\n      return '0000-00-00'"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _FormatDateTime(self, event):\n    try:\n      return timelib.Timestamp.CopyToIsoFormat(\n          event.timestamp, timezone=self._output_mediator.timezone,\n          raise_error=True)\n\n    except (OverflowError, ValueError) as exception:\n      self._ReportEventError(event, (\n          'unable to copy timestamp: {0!s} to a human readable date and time '\n          'with error: {1!s}. Defaulting to: \"0000-00-00T00:00:00\"').format(\n              event.timestamp, exception))\n\n      return '0000-00-00T00:00:00'", "response": "Formats the date and time field of an event object as ISO 8601 format."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _FormatInode(self, event):\n    inode = event.inode\n    if inode is None:\n      if hasattr(event, 'pathspec') and hasattr(event.pathspec, 'image_inode'):\n        inode = event.pathspec.image_inode\n    if inode is None:\n      inode = '-'\n\n    return inode", "response": "Formats the inode.\n\n    Args:\n      event (EventObject): event.\n\n    Returns:\n      str: inode field."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nformats the message. Args: event (EventObject): event. Returns: str: message field. Raises: NoFormatterFound: if no event formatter can be found to match the data type in the event.", "response": "def _FormatMessage(self, event):\n    \"\"\"Formats the message.\n\n    Args:\n      event (EventObject): event.\n\n    Returns:\n      str: message field.\n\n    Raises:\n      NoFormatterFound: if no event formatter can be found to match the data\n          type in the event.\n    \"\"\"\n    message, _ = self._output_mediator.GetFormattedMessages(event)\n    if message is None:\n      data_type = getattr(event, 'data_type', 'UNKNOWN')\n      raise errors.NoFormatterFound(\n          'Unable to find event formatter for: {0:s}.'.format(data_type))\n\n    return message"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _FormatMessageShort(self, event):\n    _, message_short = self._output_mediator.GetFormattedMessages(event)\n    if message_short is None:\n      data_type = getattr(event, 'data_type', 'UNKNOWN')\n      raise errors.NoFormatterFound(\n          'Unable to find event formatter for: {0:s}.'.format(data_type))\n\n    return message_short", "response": "Formats the short message field."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nformats the source field.", "response": "def _FormatSource(self, event):\n    \"\"\"Formats the source.\n\n    Args:\n      event (EventObject): event.\n\n    Returns:\n      str: source field.\n\n    Raises:\n      NoFormatterFound: if no event formatter can be found to match the data\n          type in the event.\n    \"\"\"\n    _, source = self._output_mediator.GetFormattedSources(event)\n    if source is None:\n      data_type = getattr(event, 'data_type', 'UNKNOWN')\n      raise errors.NoFormatterFound(\n          'Unable to find event formatter for: {0:s}.'.format(data_type))\n\n    return source"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nformat the short source field.", "response": "def _FormatSourceShort(self, event):\n    \"\"\"Formats the short source.\n\n    Args:\n      event (EventObject): event.\n\n    Returns:\n      str: short source field.\n\n    Raises:\n      NoFormatterFound: If no event formatter can be found to match the data\n          type in the event.\n    \"\"\"\n    source_short, _ = self._output_mediator.GetFormattedSources(event)\n    if source_short is None:\n      data_type = getattr(event, 'data_type', 'UNKNOWN')\n      raise errors.NoFormatterFound(\n          'Unable to find event formatter for: {0:s}.'.format(data_type))\n\n    return source_short"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nformats the event tag.", "response": "def _FormatTag(self, event):\n    \"\"\"Formats the event tag.\n\n    Args:\n      event (EventObject): event.\n\n    Returns:\n      str: event tag field.\n    \"\"\"\n    tag = getattr(event, 'tag', None)\n\n    if not tag:\n      return '-'\n\n    return ' '.join(tag.labels)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef GetFormattedField(self, event, field_name):\n    callback_name = self._FIELD_FORMAT_CALLBACKS.get(field_name, None)\n    callback_function = None\n    if callback_name:\n      callback_function = getattr(self, callback_name, None)\n\n    if callback_function:\n      output_value = callback_function(event)\n    else:\n      output_value = getattr(event, field_name, '-')\n\n    if output_value is None:\n      output_value = '-'\n\n    elif not isinstance(output_value, py2to3.STRING_TYPES):\n      output_value = '{0!s}'.format(output_value)\n\n    return output_value", "response": "Formats the specified field."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _SanitizeField(self, field):\n    if self._field_delimiter and isinstance(field, py2to3.STRING_TYPES):\n      return field.replace(self._field_delimiter, ' ')\n    return field", "response": "Sanitize a field for output."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwrites the body of an event to the output.", "response": "def WriteEventBody(self, event):\n    \"\"\"Writes the body of an event to the output.\n\n    Args:\n      event (EventObject): event.\n    \"\"\"\n    output_values = []\n    for field_name in self._fields:\n      output_value = self._dynamic_fields_helper.GetFormattedField(\n          event, field_name)\n\n      output_value = self._SanitizeField(output_value)\n      output_values.append(output_value)\n\n    output_line = '{0:s}\\n'.format(self._field_delimiter.join(output_values))\n    self._output_writer.Write(output_line)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef WriteHeader(self):\n    output_text = self._field_delimiter.join(self._fields)\n    output_text = '{0:s}\\n'.format(output_text)\n    self._output_writer.Write(output_text)", "response": "Writes the header to the output."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving a specific serialized attribute container from the list.", "response": "def GetAttributeContainerByIndex(self, index):\n    \"\"\"Retrieves a specific serialized attribute container from the list.\n\n    Args:\n      index (int): attribute container index.\n\n    Returns:\n      bytes: serialized attribute container data or None if not available.\n\n    Raises:\n      IndexError: if the index is less than zero.\n    \"\"\"\n    if index < 0:\n      raise IndexError(\n          'Unsupported negative index value: {0:d}.'.format(index))\n\n    if index < len(self._list):\n      return self._list[index]\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef PopAttributeContainer(self):\n    try:\n      serialized_data = self._list.pop(0)\n      self.data_size -= len(serialized_data)\n      return serialized_data\n\n    except IndexError:\n      return None", "response": "Pops a serialized attribute container from the list."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\npushes a serialized attribute container onto the list.", "response": "def PushAttributeContainer(self, serialized_data):\n    \"\"\"Pushes a serialized attribute container onto the list.\n\n    Args:\n      serialized_data (bytes): serialized attribute container data.\n    \"\"\"\n    self._list.append(serialized_data)\n    self.data_size += len(serialized_data)\n    self.next_sequence_number += 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _DeserializeAttributeContainer(self, container_type, serialized_data):\n    if not serialized_data:\n      return None\n\n    if self._serializers_profiler:\n      self._serializers_profiler.StartTiming(container_type)\n\n    try:\n      serialized_string = serialized_data.decode('utf-8')\n    except UnicodeDecodeError as exception:\n      raise IOError('Unable to decode serialized data: {0!s}'.format(\n          exception))\n    attribute_container = self._serializer.ReadSerialized(serialized_string)\n\n    if self._serializers_profiler:\n      self._serializers_profiler.StopTiming(container_type)\n\n    return attribute_container", "response": "Deserializes an attribute container."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving a specific serialized attribute container.", "response": "def _GetSerializedAttributeContainerByIndex(self, container_type, index):\n    \"\"\"Retrieves a specific serialized attribute container.\n\n    Args:\n      container_type (str): attribute container type.\n      index (int): attribute container index.\n\n    Returns:\n      bytes: serialized attribute container data or None if not available.\n    \"\"\"\n    container_list = self._GetSerializedAttributeContainerList(container_type)\n    return container_list.GetAttributeContainerByIndex(index)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _GetSerializedAttributeContainerList(self, container_type):\n    container_list = self._serialized_attribute_containers.get(\n        container_type, None)\n    if not container_list:\n      container_list = SerializedAttributeContainerList()\n      self._serialized_attribute_containers[container_type] = container_list\n\n    return container_list", "response": "Retrieves a serialized attribute container list."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _SerializeAttributeContainer(self, attribute_container):\n    if self._serializers_profiler:\n      self._serializers_profiler.StartTiming(\n          attribute_container.CONTAINER_TYPE)\n\n    try:\n      attribute_container_data = self._serializer.WriteSerialized(\n          attribute_container)\n      if not attribute_container_data:\n        raise IOError(\n            'Unable to serialize attribute container: {0:s}.'.format(\n                attribute_container.CONTAINER_TYPE))\n\n      attribute_container_data = attribute_container_data.encode('utf-8')\n\n    finally:\n      if self._serializers_profiler:\n        self._serializers_profiler.StopTiming(\n            attribute_container.CONTAINER_TYPE)\n\n    return attribute_container_data", "response": "Serializes an attribute container."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves the path of a task storage file in the merge directory.", "response": "def _GetMergeTaskStorageFilePath(self, task):\n    \"\"\"Retrieves the path of a task storage file in the merge directory.\n\n    Args:\n      task (Task): task.\n\n    Returns:\n      str: path of a task storage file file in the merge directory.\n    \"\"\"\n    filename = '{0:s}.plaso'.format(task.identifier)\n    return os.path.join(self._merge_task_storage_path, filename)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves the path of a task storage file in the processed directory.", "response": "def _GetProcessedStorageFilePath(self, task):\n    \"\"\"Retrieves the path of a task storage file in the processed directory.\n\n    Args:\n      task (Task): task.\n\n    Returns:\n      str: path of a task storage file in the processed directory.\n    \"\"\"\n    filename = '{0:s}.plaso'.format(task.identifier)\n    return os.path.join(self._processed_task_storage_path, filename)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nretrieve the path of a task storage file in the temporary directory.", "response": "def _GetTaskStorageFilePath(self, task):\n    \"\"\"Retrieves the path of a task storage file in the temporary directory.\n\n    Args:\n      task (Task): task.\n\n    Returns:\n      str: path of a task storage file in the temporary directory.\n    \"\"\"\n    filename = '{0:s}.plaso'.format(task.identifier)\n    return os.path.join(self._task_storage_path, filename)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _UpdateCounters(self, event):\n    self._session.parsers_counter['total'] += 1\n\n    # Here we want the name of the parser or plugin not the parser chain.\n    parser_name = getattr(event, 'parser', '')\n    _, _, parser_name = parser_name.rpartition('/')\n    if not parser_name:\n      parser_name = 'N/A'\n    self._session.parsers_counter[parser_name] += 1", "response": "Updates the counters.\n\n    Args:\n      event (EventObject): event."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd an analysis report to the internal storage file.", "response": "def AddAnalysisReport(self, analysis_report):\n    \"\"\"Adds an analysis report.\n\n    Args:\n      analysis_report (AnalysisReport): analysis report.\n\n    Raises:\n      IOError: when the storage writer is closed.\n      OSError: when the storage writer is closed.\n    \"\"\"\n    self._RaiseIfNotWritable()\n\n    self._storage_file.AddAnalysisReport(analysis_report)\n\n    report_identifier = analysis_report.plugin_name\n    self._session.analysis_reports_counter['total'] += 1\n    self._session.analysis_reports_counter[report_identifier] += 1\n    self.number_of_analysis_reports += 1"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a new warning to the archive.", "response": "def AddWarning(self, warning):\n    \"\"\"Adds an warning.\n\n    Args:\n      warning (ExtractionWarning): an extraction warning.\n\n    Raises:\n      IOError: when the storage writer is closed.\n      OSError: when the storage writer is closed.\n    \"\"\"\n    self._RaiseIfNotWritable()\n\n    self._storage_file.AddWarning(warning)\n    self.number_of_warnings += 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef AddEvent(self, event):\n    self._RaiseIfNotWritable()\n\n    self._storage_file.AddEvent(event)\n    self.number_of_events += 1\n\n    self._UpdateCounters(event)", "response": "Adds an event to the internal storage file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef AddEventSource(self, event_source):\n    self._RaiseIfNotWritable()\n\n    self._storage_file.AddEventSource(event_source)\n    self.number_of_event_sources += 1", "response": "Adds an event source to the internal storage file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef AddEventTag(self, event_tag):\n    self._RaiseIfNotWritable()\n\n    self._storage_file.AddEventTag(event_tag)\n\n    self._session.event_labels_counter['total'] += 1\n    for label in event_tag.labels:\n      self._session.event_labels_counter[label] += 1\n    self.number_of_event_tags += 1", "response": "Adds an event tag to the internal event list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck if a task is ready for merging with this session storage.", "response": "def CheckTaskReadyForMerge(self, task):\n    \"\"\"Checks if a task is ready for merging with this session storage.\n\n    If the task is ready to be merged, this method also sets the task's\n    storage file size.\n\n    Args:\n      task (Task): task.\n\n    Returns:\n      bool: True if the task is ready to be merged.\n\n    Raises:\n      IOError: if the storage type is not supported or\n      OSError: if the storage type is not supported or\n          if the temporary path for the task storage does not exist.\n    \"\"\"\n    if self._storage_type != definitions.STORAGE_TYPE_SESSION:\n      raise IOError('Unsupported storage type.')\n\n    if not self._processed_task_storage_path:\n      raise IOError('Missing processed task storage path.')\n\n    processed_storage_file_path = self._GetProcessedStorageFilePath(task)\n\n    try:\n      stat_info = os.stat(processed_storage_file_path)\n    except (IOError, OSError):\n      return False\n\n    task.storage_file_size = stat_info.st_size\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a task storage.", "response": "def CreateTaskStorage(self, task):\n    \"\"\"Creates a task storage.\n\n    The task storage is used to store attributes created by the task.\n\n    Args:\n      task(Task): task.\n\n    Returns:\n      StorageWriter: storage writer.\n\n    Raises:\n      IOError: if the storage type is not supported.\n      OSError: if the storage type is not supported.\n    \"\"\"\n    if self._storage_type != definitions.STORAGE_TYPE_SESSION:\n      raise IOError('Unsupported storage type.')\n\n    storage_file_path = self._GetTaskStorageFilePath(task)\n    return self._CreateTaskStorageWriter(storage_file_path, task)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef GetFirstWrittenEventSource(self):\n    if not self._storage_file:\n      raise IOError('Unable to read from closed storage writer.')\n\n    event_source = self._storage_file.GetEventSourceByIndex(\n        self._first_written_event_source_index)\n\n    if event_source:\n      self._written_event_source_index = (\n          self._first_written_event_source_index + 1)\n    return event_source", "response": "Retrieves the first event source that was written after open."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving the next event source that was written after open.", "response": "def GetNextWrittenEventSource(self):\n    \"\"\"Retrieves the next event source that was written after open.\n\n    Returns:\n      EventSource: event source or None if there are no newly written ones.\n\n    Raises:\n      IOError: when the storage writer is closed.\n      OSError: when the storage writer is closed.\n    \"\"\"\n    if not self._storage_file:\n      raise IOError('Unable to read from closed storage writer.')\n\n    event_source = self._storage_file.GetEventSourceByIndex(\n        self._written_event_source_index)\n    if event_source:\n      self._written_event_source_index += 1\n    return event_source"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef GetProcessedTaskIdentifiers(self):\n    if self._storage_type != definitions.STORAGE_TYPE_SESSION:\n      raise IOError('Unsupported storage type.')\n\n    if not self._processed_task_storage_path:\n      raise IOError('Missing processed task storage path.')\n\n    return [\n        path.replace('.plaso', '')\n        for path in os.listdir(self._processed_task_storage_path)]", "response": "Retrieves the list of task identifiers that have been processed."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving the events in increasing chronological order.", "response": "def GetSortedEvents(self, time_range=None):\n    \"\"\"Retrieves the events in increasing chronological order.\n\n    This includes all events written to the storage including those pending\n    being flushed (written) to the storage.\n\n    Args:\n      time_range (Optional[TimeRange]): time range used to filter events\n          that fall in a specific period.\n\n    Returns:\n      generator(EventObject): event generator.\n\n    Raises:\n      IOError: when the storage writer is closed.\n      OSError: when the storage writer is closed.\n    \"\"\"\n    if not self._storage_file:\n      raise IOError('Unable to read from closed storage writer.')\n\n    return self._storage_file.GetSortedEvents(time_range=time_range)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef FinalizeTaskStorage(self, task):\n    if self._storage_type != definitions.STORAGE_TYPE_SESSION:\n      raise IOError('Unsupported storage type.')\n\n    storage_file_path = self._GetTaskStorageFilePath(task)\n    processed_storage_file_path = self._GetProcessedStorageFilePath(task)\n\n    try:\n      os.rename(storage_file_path, processed_storage_file_path)\n    except OSError as exception:\n      raise IOError((\n          'Unable to rename task storage file: {0:s} with error: '\n          '{1!s}').format(storage_file_path, exception))", "response": "Finalizes a task storage."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef Open(self):\n    if self._storage_file:\n      raise IOError('Storage writer already opened.')\n\n    self._storage_file = self._CreateStorageFile()\n\n    if self._serializers_profiler:\n      self._storage_file.SetSerializersProfiler(self._serializers_profiler)\n\n    if self._storage_profiler:\n      self._storage_file.SetStorageProfiler(self._storage_profiler)\n\n    self._storage_file.Open(path=self._output_file, read_only=False)\n\n    self._first_written_event_source_index = (\n        self._storage_file.GetNumberOfEventSources())\n    self._written_event_source_index = self._first_written_event_source_index", "response": "Opens the storage writer."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\npreparing a task storage for merging.", "response": "def PrepareMergeTaskStorage(self, task):\n    \"\"\"Prepares a task storage for merging.\n\n    Moves the task storage file from the processed directory to the merge\n    directory.\n\n    Args:\n      task (Task): task.\n\n    Raises:\n      IOError: if the storage type is not supported or\n          if the storage file cannot be renamed.\n      OSError: if the storage type is not supported or\n          if the storage file cannot be renamed.\n    \"\"\"\n    if self._storage_type != definitions.STORAGE_TYPE_SESSION:\n      raise IOError('Unsupported storage type.')\n\n    merge_storage_file_path = self._GetMergeTaskStorageFilePath(task)\n    processed_storage_file_path = self._GetProcessedStorageFilePath(task)\n\n    task.storage_file_size = os.path.getsize(processed_storage_file_path)\n\n    try:\n      os.rename(processed_storage_file_path, merge_storage_file_path)\n    except OSError as exception:\n      raise IOError((\n          'Unable to rename task storage file: {0:s} with error: '\n          '{1!s}').format(processed_storage_file_path, exception))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ReadPreprocessingInformation(self, knowledge_base):\n    if not self._storage_file:\n      raise IOError('Unable to read from closed storage writer.')\n\n    self._storage_file.ReadPreprocessingInformation(knowledge_base)", "response": "Reads preprocessing information.\n\n    The preprocessing information contains the system configuration which\n    contains information about various system specific configuration data,\n    for example the user accounts.\n\n    Args:\n      knowledge_base (KnowledgeBase): is used to store the preprocessing\n          information.\n\n    Raises:\n      IOError: when the storage writer is closed.\n      OSError: when the storage writer is closed."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove a processed task storage.", "response": "def RemoveProcessedTaskStorage(self, task):\n    \"\"\"Removes a processed task storage.\n\n    Args:\n      task (Task): task.\n\n    Raises:\n      IOError: if the storage type is not supported or\n          if the storage file cannot be removed.\n      OSError: if the storage type is not supported or\n          if the storage file cannot be removed.\n    \"\"\"\n    if self._storage_type != definitions.STORAGE_TYPE_SESSION:\n      raise IOError('Unsupported storage type.')\n\n    processed_storage_file_path = self._GetProcessedStorageFilePath(task)\n\n    try:\n      os.remove(processed_storage_file_path)\n    except OSError as exception:\n      raise IOError((\n          'Unable to remove task storage file: {0:s} with error: '\n          '{1!s}').format(processed_storage_file_path, exception))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the serializers profiler.", "response": "def SetSerializersProfiler(self, serializers_profiler):\n    \"\"\"Sets the serializers profiler.\n\n    Args:\n      serializers_profiler (SerializersProfiler): serializers profiler.\n    \"\"\"\n    self._serializers_profiler = serializers_profiler\n    if self._storage_file:\n      self._storage_file.SetSerializersProfiler(serializers_profiler)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef SetStorageProfiler(self, storage_profiler):\n    self._storage_profiler = storage_profiler\n    if self._storage_file:\n      self._storage_file.SetStorageProfiler(storage_profiler)", "response": "Sets the storage profiler."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef StartMergeTaskStorage(self, task):\n    if self._storage_type != definitions.STORAGE_TYPE_SESSION:\n      raise IOError('Unsupported storage type.')\n\n    if not self._merge_task_storage_path:\n      raise IOError('Missing merge task storage path.')\n\n    merge_storage_file_path = self._GetMergeTaskStorageFilePath(task)\n\n    if not os.path.isfile(merge_storage_file_path):\n      raise IOError('Merge task storage path is not a file.')\n\n    return self._CreateTaskStorageMergeReader(merge_storage_file_path)", "response": "Starts a merge of a task storage with the session storage."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nstart the task storage.", "response": "def StartTaskStorage(self):\n    \"\"\"Creates a temporary path for the task storage.\n\n    Raises:\n      IOError: if the storage type is not supported or\n          if the temporary path for the task storage already exists.\n      OSError: if the storage type is not supported or\n          if the temporary path for the task storage already exists.\n    \"\"\"\n    if self._storage_type != definitions.STORAGE_TYPE_SESSION:\n      raise IOError('Unsupported storage type.')\n\n    if self._task_storage_path:\n      raise IOError('Task storage path already exists.')\n\n    output_directory = os.path.dirname(self._output_file)\n    self._task_storage_path = tempfile.mkdtemp(dir=output_directory)\n\n    self._merge_task_storage_path = os.path.join(\n        self._task_storage_path, 'merge')\n    os.mkdir(self._merge_task_storage_path)\n\n    self._processed_task_storage_path = os.path.join(\n        self._task_storage_path, 'processed')\n    os.mkdir(self._processed_task_storage_path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef StopTaskStorage(self, abort=False):\n    if self._storage_type != definitions.STORAGE_TYPE_SESSION:\n      raise IOError('Unsupported storage type.')\n\n    if os.path.isdir(self._merge_task_storage_path):\n      if abort:\n        shutil.rmtree(self._merge_task_storage_path)\n      else:\n        os.rmdir(self._merge_task_storage_path)\n\n    if os.path.isdir(self._processed_task_storage_path):\n      if abort:\n        shutil.rmtree(self._processed_task_storage_path)\n      else:\n        os.rmdir(self._processed_task_storage_path)\n\n    if os.path.isdir(self._task_storage_path):\n      if abort:\n        shutil.rmtree(self._task_storage_path)\n      else:\n        os.rmdir(self._task_storage_path)\n\n    self._merge_task_storage_path = None\n    self._processed_task_storage_path = None\n    self._task_storage_path = None", "response": "Stops the task storage."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwrites session completion information.", "response": "def WriteSessionCompletion(self, aborted=False):\n    \"\"\"Writes session completion information.\n\n    Args:\n      aborted (Optional[bool]): True if the session was aborted.\n\n    Raises:\n      IOError: if the storage type is not supported or\n          when the storage writer is closed.\n      OSError: if the storage type is not supported or\n          when the storage writer is closed.\n    \"\"\"\n    self._RaiseIfNotWritable()\n\n    if self._storage_type != definitions.STORAGE_TYPE_SESSION:\n      raise IOError('Unsupported storage type.')\n\n    self._session.aborted = aborted\n    session_completion = self._session.CreateSessionCompletion()\n    self._storage_file.WriteSessionCompletion(session_completion)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwriting session start information.", "response": "def WriteSessionStart(self):\n    \"\"\"Writes session start information.\n\n    Raises:\n      IOError: if the storage type is not supported or\n          when the storage writer is closed.\n      OSError: if the storage type is not supported or\n          when the storage writer is closed.\n    \"\"\"\n    self._RaiseIfNotWritable()\n\n    if self._storage_type != definitions.STORAGE_TYPE_SESSION:\n      raise IOError('Unsupported storage type.')\n\n    session_start = self._session.CreateSessionStart()\n    self._storage_file.WriteSessionStart(session_start)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef WriteTaskCompletion(self, aborted=False):\n    self._RaiseIfNotWritable()\n\n    if self._storage_type != definitions.STORAGE_TYPE_TASK:\n      raise IOError('Unsupported storage type.')\n\n    self._task.aborted = aborted\n    task_completion = self._task.CreateTaskCompletion()\n    self._storage_file.WriteTaskCompletion(task_completion)", "response": "Writes the task completion information."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef WriteTaskStart(self):\n    self._RaiseIfNotWritable()\n\n    if self._storage_type != definitions.STORAGE_TYPE_TASK:\n      raise IOError('Unsupported storage type.')\n\n    task_start = self._task.CreateTaskStart()\n    self._storage_file.WriteTaskStart(task_start)", "response": "Writes the task start information."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _ConvertToTimestamp(self, date, time, timezone):\n    # TODO: check if this is correct, likely not date or not time\n    # is more accurate.\n    if not date and not time:\n      raise errors.TimestampError(\n          'Unable to extract timestamp from McAfee AV logline.')\n\n    # TODO: Figure out how McAfee sets Day First and use that here.\n    # The in-file time format is '07/30/2013\\t10:22:48 AM'.\n    try:\n      time_string = '{0:s} {1:s}'.format(date, time)\n    except UnicodeDecodeError:\n      raise errors.TimestampError('Unable to form a timestamp string.')\n\n    return timelib.Timestamp.FromTimeString(time_string, timezone=timezone)", "response": "Converts date and time values into a timestamp."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ParseRow(self, parser_mediator, row_offset, row):\n    try:\n      timestamp = self._ConvertToTimestamp(\n          row['date'], row['time'], parser_mediator.timezone)\n    except errors.TimestampError as exception:\n      parser_mediator.ProduceExtractionWarning(\n          'Unable to parse time string: [{0:s} {1:s}] with error {2:s}'.format(\n              repr(row['date']), repr(row['time']), exception))\n      return\n\n    if timestamp is None:\n      return\n\n    event_data = McafeeAVEventData()\n    event_data.action = row['action']\n    event_data.filename = row['filename']\n    event_data.offset = row_offset\n    event_data.rule = row['rule']\n    event_data.status = row['status']\n    event_data.trigger_location = row['trigger_location']\n    event_data.username = row['username']\n\n    event = time_events.TimestampEvent(\n        timestamp, definitions.TIME_DESCRIPTION_WRITTEN)\n    parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Parses a single row of the log file and produces events."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nverifying if a line of the file is in the expected format.", "response": "def VerifyRow(self, parser_mediator, row):\n    \"\"\"Verifies if a line of the file is in the expected format.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      row (dict[str, str]): fields of a single row, as specified in COLUMNS.\n\n    Returns:\n      bool: True if this is the correct parser, False otherwise.\n    \"\"\"\n    if len(row) != 8:\n      return False\n\n    # This file can have a UTF-8 byte-order-marker at the beginning of\n    # the first row.\n    # TODO: Find out all the code pages this can have.  Asked McAfee 10/31.\n    row_bytes = codecs.encode(row['date'], parser_mediator.codepage)\n    if row_bytes.startswith(b'\\xef\\xbb\\xbf'):\n      row['date'] = row['date'][3:]\n      self._encoding = 'utf-8'\n\n    # Check the date format!\n    # If it doesn't parse, then this isn't a McAfee AV Access Protection Log\n    try:\n      timestamp = self._ConvertToTimestamp(\n          row['date'], row['time'], parser_mediator.timezone)\n    except errors.TimestampError:\n      return False\n\n    if timestamp is None:\n      return False\n\n    # Use the presence of these strings as a backup or in case of partial file.\n    if (not 'Access Protection' in row['status'] and\n        not 'Would be blocked' in row['status']):\n      return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef CompileReport(self, mediator):\n    report_text = [\n        'Sessionize plugin identified {0:d} sessions and '\n        'applied {1:d} tags.'.format(\n            len(self._events_per_session), self._number_of_event_tags)]\n    for session, event_count in enumerate(self._events_per_session):\n      report_text.append('\\tSession {0:d}: {1:d} events'.format(\n          session, event_count))\n    report_text = '\\n'.join(report_text)\n    return reports.AnalysisReport(plugin_name=self.NAME, text=report_text)", "response": "Compiles an analysis report."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ExamineEvent(self, mediator, event):\n    if self._session_end_timestamp is None:\n      self._session_end_timestamp = (\n          event.timestamp + self._maximum_pause_microseconds)\n      self._events_per_session.append(0)\n\n    if event.timestamp > self._session_end_timestamp:\n      self._session_counter += 1\n      self._events_per_session.append(0)\n\n    self._session_end_timestamp = (\n        event.timestamp + self._maximum_pause_microseconds)\n    # The counter for the current session is the always the last item in\n    # the list.\n    self._events_per_session[-1] += 1\n\n    label = 'session_{0:d}'.format(self._session_counter)\n    event_tag = self._CreateEventTag(event, self._EVENT_TAG_COMMENT, [label])\n    mediator.ProduceEventTag(event_tag)\n    self._number_of_event_tags += 1", "response": "Analyzes an EventObject and tags it as part of a session."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndetermining the formatted message strings for an event object.", "response": "def GetMessages(self, formatter_mediator, event):\n    \"\"\"Determines the formatted message strings for an event object.\n\n    Args:\n      formatter_mediator (FormatterMediator): mediates the interactions\n          between formatters and other components, such as storage and Windows\n          EventLog resources.\n      event (EventObject): event.\n\n    Returns:\n      tuple(str, str): formatted message string and short message string.\n\n    Raises:\n      WrongFormatter: if the event object cannot be formatted by the formatter.\n    \"\"\"\n    if self.DATA_TYPE != event.data_type:\n      raise errors.WrongFormatter('Unsupported data type: {0:s}.'.format(\n          event.data_type))\n\n    event_values = event.CopyToDict()\n\n    file_entry_type = event_values.get('file_entry_type', None)\n    if file_entry_type is not None:\n      event_values['file_entry_type'] = self._FILE_ENTRY_TYPES.get(\n          file_entry_type, 'UNKNOWN')\n\n    # The usage of allocated is deprecated in favor of is_allocated but\n    # is kept here to be backwards compatible.\n    if (not event_values.get('allocated', False) and\n        not event_values.get('is_allocated', False)):\n      event_values['unallocated'] = 'unallocated'\n\n    return self._ConditionalFormatMessages(event_values)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef GetSources(self, event):\n    if self.DATA_TYPE != event.data_type:\n      raise errors.WrongFormatter('Unsupported data type: {0:s}.'.format(\n          event.data_type))\n\n    file_system_type = getattr(event, 'file_system_type', 'UNKNOWN')\n    timestamp_desc = getattr(event, 'timestamp_desc', 'Time')\n    source_long = '{0:s} {1:s}'.format(file_system_type, timestamp_desc)\n\n    return self.SOURCE_SHORT, source_long", "response": "Determines the short and long source for an event object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef GetMessages(self, formatter_mediator, event):\n    if self.DATA_TYPE != event.data_type:\n      raise errors.WrongFormatter('Unsupported data type: {0:s}.'.format(\n          event.data_type))\n\n    event_values = event.CopyToDict()\n\n    attribute_type = event_values.get('attribute_type', 0)\n    event_values['attribute_name'] = self._ATTRIBUTE_NAMES.get(\n        attribute_type, 'UNKNOWN')\n\n    file_reference = event_values.get('file_reference', None)\n    if file_reference:\n      event_values['file_reference'] = '{0:d}-{1:d}'.format(\n          file_reference & 0xffffffffffff, file_reference >> 48)\n\n    parent_file_reference = event_values.get('parent_file_reference', None)\n    if parent_file_reference:\n      event_values['parent_file_reference'] = '{0:d}-{1:d}'.format(\n          parent_file_reference & 0xffffffffffff, parent_file_reference >> 48)\n\n    if not event_values.get('is_allocated', False):\n      event_values['unallocated'] = 'unallocated'\n\n    return self._ConditionalFormatMessages(event_values)", "response": "Determines the formatted message strings for an event object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndetermining the formatted message strings for an event object.", "response": "def GetMessages(self, formatter_mediator, event):\n    \"\"\"Determines the formatted message strings for an event object.\n\n    Args:\n      formatter_mediator (FormatterMediator): mediates the interactions\n          between formatters and other components, such as storage and Windows\n          EventLog resources.\n      event (EventObject): event.\n\n    Returns:\n      tuple(str, str): formatted message string and short message string.\n\n    Raises:\n      WrongFormatter: if the event object cannot be formatted by the formatter.\n    \"\"\"\n    if self.DATA_TYPE != event.data_type:\n      raise errors.WrongFormatter('Unsupported data type: {0:s}.'.format(\n          event.data_type))\n\n    event_values = event.CopyToDict()\n\n    file_reference = event_values.get('file_reference', None)\n    if file_reference:\n      event_values['file_reference'] = '{0:d}-{1:d}'.format(\n          file_reference & 0xffffffffffff, file_reference >> 48)\n\n    parent_file_reference = event_values.get('parent_file_reference', None)\n    if parent_file_reference:\n      event_values['parent_file_reference'] = '{0:d}-{1:d}'.format(\n          parent_file_reference & 0xffffffffffff, parent_file_reference >> 48)\n\n    update_reason_flags = event_values.get('update_reason_flags', 0)\n    update_reasons = []\n    for bitmask, description in sorted(self._USN_REASON_FLAGS.items()):\n      if bitmask & update_reason_flags:\n        update_reasons.append(description)\n\n    event_values['update_reason'] = ', '.join(update_reasons)\n\n    update_source_flags = event_values.get('update_source_flags', 0)\n    update_sources = []\n    for bitmask, description in sorted(self._USN_SOURCE_FLAGS.items()):\n      if bitmask & update_source_flags:\n        update_sources.append(description)\n\n    event_values['update_source'] = ', '.join(update_sources)\n\n    return self._ConditionalFormatMessages(event_values)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nclose the queue. This needs to be called from any process or thread putting items onto the queue. Args: abort (Optional[bool]): True if the close was issued on abort.", "response": "def Close(self, abort=False):\n    \"\"\"Closes the queue.\n\n    This needs to be called from any process or thread putting items onto\n    the queue.\n\n    Args:\n      abort (Optional[bool]): True if the close was issued on abort.\n    \"\"\"\n    if abort:\n      # Prevent join_thread() from blocking.\n      self._queue.cancel_join_thread()\n\n    self._queue.close()\n    self._queue.join_thread()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\npushes an item onto the queue.", "response": "def PushItem(self, item, block=True):\n    \"\"\"Pushes an item onto the queue.\n\n    Args:\n      item (object): item to add.\n      block (Optional[bool]): True to block the process when the queue is full.\n\n    Raises:\n      QueueFull: if the item could not be pushed the queue because it's full.\n    \"\"\"\n    try:\n      self._queue.put(item, block=block)\n    except Queue.Full as exception:\n      raise errors.QueueFull(exception)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npops an item off the queue.", "response": "def PopItem(self):\n    \"\"\"Pops an item off the queue.\n\n    Returns:\n      object: item from the queue.\n\n    Raises:\n      QueueClose: if the queue has already been closed.\n      QueueEmpty: if no item could be retrieved from the queue within the\n          specified timeout.\n    \"\"\"\n    try:\n      # If no timeout is specified the queue will block if empty otherwise\n      # a Queue.Empty exception is raised.\n      return self._queue.get(timeout=self._timeout)\n\n    except KeyboardInterrupt:\n      raise errors.QueueClose\n\n    # If close() is called on the multiprocessing.Queue while it is blocking\n    # on get() it will raise IOError.\n    except IOError:\n      raise errors.QueueClose\n\n    except Queue.Empty:\n      raise errors.QueueEmpty"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprepare an attribute container for storage.", "response": "def _PrepareAttributeContainer(self, attribute_container):\n    \"\"\"Prepares an attribute container for storage.\n\n    Args:\n      attribute_container (AttributeContainer): attribute container.\n\n    Returns:\n      AttributeContainer: copy of the attribute container to store in\n          the fake storage.\n    \"\"\"\n    attribute_values_hash = hash(attribute_container.GetAttributeValuesString())\n    identifier = identifiers.FakeIdentifier(attribute_values_hash)\n    attribute_container.SetIdentifier(identifier)\n\n    # Make sure the fake storage preserves the state of the attribute container.\n    return copy.deepcopy(attribute_container)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading the data into the event.", "response": "def _ReadEventDataIntoEvent(self, event):\n    \"\"\"Reads the data into the event.\n\n    This function is intended to offer backwards compatible event behavior.\n\n    Args:\n      event (EventObject): event.\n    \"\"\"\n    if self._storage_type != definitions.STORAGE_TYPE_SESSION:\n      return\n\n    event_data_identifier = event.GetEventDataIdentifier()\n    if event_data_identifier:\n      lookup_key = event_data_identifier.CopyToString()\n      event_data = self._event_data[lookup_key]\n\n      for attribute_name, attribute_value in event_data.GetAttributes():\n        setattr(event, attribute_name, attribute_value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef AddAnalysisReport(self, analysis_report):\n    self._RaiseIfNotWritable()\n\n    analysis_report = self._PrepareAttributeContainer(analysis_report)\n\n    self.analysis_reports.append(analysis_report)", "response": "Adds an analysis report to the storage."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd an event to the internal list of events.", "response": "def AddEvent(self, event):\n    \"\"\"Adds an event.\n\n    Args:\n      event (EventObject): event.\n\n    Raises:\n      IOError: when the storage writer is closed or\n          if the event data identifier type is not supported.\n      OSError: when the storage writer is closed or\n          if the event data identifier type is not supported.\n    \"\"\"\n    self._RaiseIfNotWritable()\n\n    # TODO: change to no longer allow event_data_identifier is None\n    # after refactoring every parser to generate event data.\n    event_data_identifier = event.GetEventDataIdentifier()\n    if event_data_identifier:\n      if not isinstance(event_data_identifier, identifiers.FakeIdentifier):\n        raise IOError('Unsupported event data identifier type: {0:s}'.format(\n            type(event_data_identifier)))\n\n    event = self._PrepareAttributeContainer(event)\n\n    self._events.append(event)\n    self.number_of_events += 1"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding an event data.", "response": "def AddEventData(self, event_data):\n    \"\"\"Adds event data.\n\n    Args:\n      event_data (EventData): event data.\n\n    Raises:\n      IOError: when the storage writer is closed.\n      OSError: when the storage writer is closed.\n    \"\"\"\n    self._RaiseIfNotWritable()\n\n    event_data = self._PrepareAttributeContainer(event_data)\n\n    identifier = event_data.GetIdentifier()\n    lookup_key = identifier.CopyToString()\n    self._event_data[lookup_key] = event_data"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef AddEventSource(self, event_source):\n    self._RaiseIfNotWritable()\n\n    event_source = self._PrepareAttributeContainer(event_source)\n\n    self._event_sources.append(event_source)\n    self.number_of_event_sources += 1", "response": "Adds an event source to the internal list of event sources."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef AddEventTag(self, event_tag):\n    self._RaiseIfNotWritable()\n\n    event_identifier = event_tag.GetEventIdentifier()\n    if not isinstance(event_identifier, identifiers.FakeIdentifier):\n      raise IOError('Unsupported event identifier type: {0:s}'.format(\n          type(event_identifier)))\n\n    event_tag = self._PrepareAttributeContainer(event_tag)\n\n    self._event_tags.append(event_tag)\n    self.number_of_event_tags += 1", "response": "Adds an event tag to the internal list of event tags."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef AddWarning(self, warning):\n    self._RaiseIfNotWritable()\n\n    warning = self._PrepareAttributeContainer(warning)\n\n    self._warnings.append(warning)\n    self.number_of_warnings += 1", "response": "Adds a warning to the internal list of warnings."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef CreateTaskStorage(self, task):\n    if task.identifier in self._task_storage_writers:\n      raise IOError('Storage writer for task: {0:s} already exists.'.format(\n          task.identifier))\n\n    storage_writer = FakeStorageWriter(\n        self._session, storage_type=definitions.STORAGE_TYPE_TASK, task=task)\n    self._task_storage_writers[task.identifier] = storage_writer\n    return storage_writer", "response": "Creates a task storage."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves the first event source that was written after open.", "response": "def GetFirstWrittenEventSource(self):\n    \"\"\"Retrieves the first event source that was written after open.\n\n    Using GetFirstWrittenEventSource and GetNextWrittenEventSource newly\n    added event sources can be retrieved in order of addition.\n\n    Returns:\n      EventSource: event source or None if there are no newly written ones.\n\n    Raises:\n      IOError: when the storage writer is closed.\n      OSError: when the storage writer is closed.\n    \"\"\"\n    if not self._is_open:\n      raise IOError('Unable to read from closed storage writer.')\n\n    if self._written_event_source_index >= len(self._event_sources):\n      return None\n\n    event_source = self._event_sources[self._first_written_event_source_index]\n    self._written_event_source_index = (\n        self._first_written_event_source_index + 1)\n    return event_source"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nretrieves the next event source that was written after open.", "response": "def GetNextWrittenEventSource(self):\n    \"\"\"Retrieves the next event source that was written after open.\n\n    Returns:\n      EventSource: event source or None if there are no newly written ones.\n\n    Raises:\n      IOError: when the storage writer is closed.\n      OSError: when the storage writer is closed.\n    \"\"\"\n    if not self._is_open:\n      raise IOError('Unable to read from closed storage writer.')\n\n    if self._written_event_source_index >= len(self._event_sources):\n      return None\n\n    event_source = self._event_sources[self._written_event_source_index]\n    self._written_event_source_index += 1\n    return event_source"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving the events in increasing chronological order.", "response": "def GetSortedEvents(self, time_range=None):\n    \"\"\"Retrieves the events in increasing chronological order.\n\n    Args:\n      time_range (Optional[TimeRange]): time range used to filter events\n          that fall in a specific period.\n\n    Returns:\n      generator(EventObject): event generator.\n\n    Raises:\n      IOError: when the storage writer is closed.\n      OSError: when the storage writer is closed.\n    \"\"\"\n    if not self._is_open:\n      raise IOError('Unable to read from closed storage writer.')\n\n    event_heap = event_heaps.EventHeap()\n\n    for event in self._events:\n      if (time_range and (\n          event.timestamp < time_range.start_timestamp or\n          event.timestamp > time_range.end_timestamp)):\n        continue\n\n      # Make a copy of the event before adding the event data.\n      event = copy.deepcopy(event)\n      # TODO: refactor this into psort.\n      self._ReadEventDataIntoEvent(event)\n\n      event_heap.PushEvent(event)\n\n    return iter(event_heap.PopEvents())"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef FinalizeTaskStorage(self, task):\n    if task.identifier not in self._task_storage_writers:\n      raise IOError('Storage writer for task: {0:s} does not exist.'.format(\n          task.identifier))", "response": "Finalizes a processed task storage."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef Open(self):\n    if self._is_open:\n      raise IOError('Storage writer already opened.')\n\n    self._is_open = True\n\n    self._first_written_event_source_index = len(self._event_sources)\n    self._written_event_source_index = self._first_written_event_source_index", "response": "Opens the storage writer."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprepare a task storage for merging.", "response": "def PrepareMergeTaskStorage(self, task):\n    \"\"\"Prepares a task storage for merging.\n\n    Args:\n      task (Task): task.\n\n    Raises:\n      IOError: if the task storage does not exist.\n      OSError: if the task storage does not exist.\n    \"\"\"\n    if task.identifier not in self._task_storage_writers:\n      raise IOError('Storage writer for task: {0:s} does not exist.'.format(\n          task.identifier))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading preprocessing information. The preprocessing information contains the system configuration which contains information about various system specific configuration data, for example the user accounts. Args: knowledge_base (KnowledgeBase): is used to store the preprocessing information. Raises: IOError: if the storage type does not support writing preprocessing information or when the storage writer is closed. OSError: if the storage type does not support writing preprocessing information or when the storage writer is closed.", "response": "def ReadPreprocessingInformation(self, knowledge_base):\n    \"\"\"Reads preprocessing information.\n\n    The preprocessing information contains the system configuration which\n    contains information about various system specific configuration data,\n    for example the user accounts.\n\n    Args:\n      knowledge_base (KnowledgeBase): is used to store the preprocessing\n          information.\n\n    Raises:\n      IOError: if the storage type does not support writing preprocessing\n          information or when the storage writer is closed.\n      OSError: if the storage type does not support writing preprocessing\n          information or when the storage writer is closed.\n    \"\"\"\n    self._RaiseIfNotWritable()\n\n    if self._storage_type != definitions.STORAGE_TYPE_SESSION:\n      raise IOError('Preprocessing information not supported by storage type.')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nremove a processed task storage.", "response": "def RemoveProcessedTaskStorage(self, task):\n    \"\"\"Removes a processed task storage.\n\n    Args:\n      task (Task): task.\n\n    Raises:\n      IOError: if the task storage does not exist.\n      OSError: if the task storage does not exist.\n    \"\"\"\n    if task.identifier not in self._task_storage_writers:\n      raise IOError('Storage writer for task: {0:s} does not exist.'.format(\n          task.identifier))\n\n    del self._task_storage_writers[task.identifier]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrite preprocessing information. Args: knowledge_base (KnowledgeBase): used to store the preprocessing information. Raises: IOError: if the storage type does not support writing preprocessing information or when the storage writer is closed. OSError: if the storage type does not support writing preprocessing information or when the storage writer is closed.", "response": "def WritePreprocessingInformation(self, knowledge_base):\n    \"\"\"Writes preprocessing information.\n\n    Args:\n      knowledge_base (KnowledgeBase): used to store the preprocessing\n          information.\n\n    Raises:\n      IOError: if the storage type does not support writing preprocessing\n          information or when the storage writer is closed.\n      OSError: if the storage type does not support writing preprocessing\n          information or when the storage writer is closed.\n    \"\"\"\n    self._RaiseIfNotWritable()\n\n    if self._storage_type != definitions.STORAGE_TYPE_SESSION:\n      raise IOError('Preprocessing information not supported by storage type.')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nretrieve a value from a parsed log line removing empty results.", "response": "def _GetStructureValue(self, structure, key):\n    \"\"\"Retrieves a value from a parsed log line, removing empty results.\n\n    Args:\n      structure (pyparsing.ParseResults): parsed log line.\n      key (str): results key to retrieve from the parsed log line.\n\n    Returns:\n      type or None: the value of the named key in the parsed log line, or None\n        if the value is a ParseResults object.\n    \"\"\"\n    value = structure.get(key)\n    return value if not isinstance(value, pyparsing.ParseResults) else None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing a comment record and stores appropriate attributes.", "response": "def _ParseCommentRecord(self, structure):\n    \"\"\"Parse a comment and store appropriate attributes.\n\n    Args:\n      structure (pyparsing.ParseResults): parsed log line.\n    \"\"\"\n    comment = structure[1]\n    if comment.startswith('Version'):\n      _, _, self._version = comment.partition(':')\n    elif comment.startswith('Software'):\n      _, _, self._software = comment.partition(':')\n    elif comment.startswith('Time'):\n      _, _, time_format = comment.partition(':')\n      if 'local' in time_format.lower():\n        self._use_local_timezone = True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse a single log line and produce an event object.", "response": "def _ParseLogLine(self, parser_mediator, structure):\n    \"\"\"Parse a single log line and and produce an event object.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      structure (pyparsing.ParseResults): structure of tokens derived from\n          a line of a text file.\n    \"\"\"\n    try:\n      date_time = dfdatetime_time_elements.TimeElements(\n          time_elements_tuple=structure.date_time)\n      date_time.is_local_time = True\n    except ValueError:\n      parser_mediator.ProduceExtractionWarning(\n          'invalid date time value: {0!s}'.format(structure.date_time))\n      return\n\n    event_data = WinFirewallEventData()\n    event_data.action = self._GetStructureValue(structure, 'action')\n    event_data.dest_ip = self._GetStructureValue(structure, 'dest_ip')\n    event_data.dest_port = self._GetStructureValue(structure, 'dest_port')\n    event_data.flags = self._GetStructureValue(structure, 'flags')\n    event_data.icmp_code = self._GetStructureValue(structure, 'icmp_code')\n    event_data.icmp_type = self._GetStructureValue(structure, 'icmp_type')\n    event_data.info = self._GetStructureValue(structure, 'info')\n    event_data.path = self._GetStructureValue(structure, 'path')\n    event_data.protocol = self._GetStructureValue(structure, 'protocol')\n    event_data.size = self._GetStructureValue(structure, 'size')\n    event_data.source_ip = self._GetStructureValue(structure, 'source_ip')\n    event_data.source_port = self._GetStructureValue(structure, 'source_port')\n    event_data.tcp_ack = self._GetStructureValue(structure, 'tcp_ack')\n    event_data.tcp_seq = self._GetStructureValue(structure, 'tcp_seq')\n    event_data.tcp_win = self._GetStructureValue(structure, 'tcp_win')\n\n    if self._use_local_timezone:\n      time_zone = parser_mediator.timezone\n    else:\n      time_zone = pytz.UTC\n\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_WRITTEN, time_zone=time_zone)\n    parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a new service object from an event object.", "response": "def FromEvent(cls, service_event):\n    \"\"\"Creates a service object from an event.\n\n    Args:\n      service_event (EventObject): event to create a new service object from.\n\n    Returns:\n      WindowsService: service.\n    \"\"\"\n    _, _, name = service_event.key_path.rpartition(\n        WindowsService._REGISTRY_KEY_PATH_SEPARATOR)\n    service_type = service_event.regvalue.get('Type', '')\n    image_path = service_event.regvalue.get('ImagePath', '')\n    start_type = service_event.regvalue.get('Start', '')\n    service_dll = service_event.regvalue.get('ServiceDll', '')\n    object_name = service_event.regvalue.get('ObjectName', '')\n\n    if service_event.pathspec:\n      source = (service_event.pathspec.location, service_event.key_path)\n    else:\n      source = ('Unknown', 'Unknown')\n    return cls(\n        name=name, service_type=service_type, image_path=image_path,\n        start_type=start_type, object_name=object_name,\n        source=source, service_dll=service_dll)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a human readable string describing the type value.", "response": "def HumanReadableType(self):\n    \"\"\"Return a human readable string describing the type value.\n\n    Returns:\n      str: human readable description of the type value.\n    \"\"\"\n    if isinstance(self.service_type, py2to3.STRING_TYPES):\n      return self.service_type\n    return human_readable_service_enums.SERVICE_ENUMS['Type'].get(\n        self.service_type, '{0:d}'.format(self.service_type))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a human readable string describing the start type value.", "response": "def HumanReadableStartType(self):\n    \"\"\"Return a human readable string describing the start type value.\n\n    Returns:\n      str: human readable description of the start type value.\n    \"\"\"\n    if isinstance(self.start_type, py2to3.STRING_TYPES):\n      return self.start_type\n    return human_readable_service_enums.SERVICE_ENUMS['Start'].get(\n        self.start_type, '{0:d}'.format(self.start_type))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef AddService(self, new_service):\n    for service in self._services:\n      if new_service == service:\n        # If this service is the same as one we already know about, we\n        # just want to add where it came from.\n        service.sources.append(new_service.sources[0])\n        return\n\n    # We only add a new object to our list if we don't have\n    # an identical one already.\n    self._services.append(new_service)", "response": "Adds a new service to the list of ones we know about."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _FormatServiceText(self, service):\n    string_segments = [\n        service.name,\n        '\\tImage Path    = {0:s}'.format(service.image_path),\n        '\\tService Type  = {0:s}'.format(service.HumanReadableType()),\n        '\\tStart Type    = {0:s}'.format(service.HumanReadableStartType()),\n        '\\tService Dll   = {0:s}'.format(service.service_dll),\n        '\\tObject Name   = {0:s}'.format(service.object_name),\n        '\\tSources:']\n\n    for source in service.sources:\n      string_segments.append('\\t\\t{0:s}:{1:s}'.format(source[0], source[1]))\n    return '\\n'.join(string_segments)", "response": "Formats a Windows Service into a human readable multi - line string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef CompileReport(self, mediator):\n    # TODO: move YAML representation out of plugin and into serialization.\n    lines_of_text = []\n    if self._output_format == 'yaml':\n      lines_of_text.append(\n          yaml.safe_dump_all(self._service_collection.services))\n    else:\n      lines_of_text.append('Listing Windows Services')\n      for service in self._service_collection.services:\n        lines_of_text.append(self._FormatServiceText(service))\n        lines_of_text.append('')\n\n    lines_of_text.append('')\n    report_text = '\\n'.join(lines_of_text)\n    return reports.AnalysisReport(plugin_name=self.NAME, text=report_text)", "response": "Compiles an analysis report."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nanalyze an event and creates Windows Services as required.", "response": "def ExamineEvent(self, mediator, event):\n    \"\"\"Analyzes an event and creates Windows Services as required.\n\n      At present, this method only handles events extracted from the Registry.\n\n    Args:\n      mediator (AnalysisMediator): mediates interactions between analysis\n          plugins and other components, such as storage and dfvfs.\n      event (EventObject): event to examine.\n    \"\"\"\n    # TODO: Handle event log entries here also (ie, event id 4697).\n    event_data_type = getattr(event, 'data_type', '')\n    if event_data_type == 'windows:registry:service':\n      # Create and store the service.\n      service = WindowsService.FromEvent(event)\n      self._service_collection.AddService(service)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndetermine the formatted message strings for an event object.", "response": "def GetMessages(self, formatter_mediator, event):\n    \"\"\"Determines the formatted message strings for an event object.\n\n    Args:\n      formatter_mediator (FormatterMediator): mediates the interactions\n          between formatters and other components, such as storage and Windows\n          EventLog resources.\n      event (EventObject): event.\n\n    Returns:\n      tuple(str, str): formatted message string and short message string.\n\n    Raises:\n      WrongFormatter: if the event object cannot be formatted by the formatter.\n    \"\"\"\n    if self.DATA_TYPE != event.data_type:\n      raise errors.WrongFormatter('Unsupported data type: {0:s}.'.format(\n          event.data_type))\n\n    event_values = event.CopyToDict()\n\n    cookie_flags = event_values.get('flags', None)\n    if cookie_flags == 0:\n      del event_values['flags']\n    elif cookie_flags:\n      flags = []\n      for flag_value, flag_description in iter(self._COOKIE_FLAGS.items()):\n        if cookie_flags & flag_value:\n          flags.append(flag_description)\n\n      event_values['flags'] = '|'.join(flags)\n\n    return self._ConditionalFormatMessages(event_values)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nextracting relevant install history entries.", "response": "def GetEntries(self, parser_mediator, top_level=None, **unused_kwargs):\n    \"\"\"Extracts relevant install history entries.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      top_level (dict[str, object]): plist top-level key.\n    \"\"\"\n    for entry in top_level:\n      datetime_value = entry.get('date', None)\n      package_identifiers = entry.get('packageIdentifiers', [])\n\n      if not datetime_value or not package_identifiers:\n        continue\n\n      display_name = entry.get('displayName', '<UNKNOWN>')\n      display_version = entry.get('displayVersion', '<DISPLAY_VERSION>')\n      process_name = entry.get('processName', '<PROCESS_NAME>')\n      package_identifiers = ', '.join(package_identifiers)\n\n      event_data = plist_event.PlistTimeEventData()\n      event_data.desc = (\n          'Installation of [{0:s} {1:s}] using [{2:s}]. Packages: '\n          '{3:s}.').format(\n              display_name, display_version, process_name, package_identifiers)\n      event_data.key = ''\n      event_data.root = '/item'\n\n      event = time_events.PythonDatetimeEvent(\n          datetime_value, definitions.TIME_DESCRIPTION_WRITTEN)\n      parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nattempt to detect a tag file based on the operating system.", "response": "def _AttemptAutoDetectTagFile(self, analysis_mediator):\n    \"\"\"Detects which tag file is most appropriate.\n\n    Args:\n      analysis_mediator (AnalysisMediator): analysis mediator.\n\n    Returns:\n      bool: True if a tag file is autodetected.\n    \"\"\"\n    self._autodetect_tag_file_attempt = True\n    if not analysis_mediator.data_location:\n      return False\n\n    operating_system = analysis_mediator.operating_system.lower()\n    filename = self._OS_TAG_FILES.get(operating_system, None)\n    if not filename:\n      return False\n\n    logger.info('Using auto detected tag file: {0:s}'.format(filename))\n    tag_file_path = os.path.join(analysis_mediator.data_location, filename)\n    self.SetAndLoadTagFile(tag_file_path)\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef CompileReport(self, mediator):\n    report_text = 'Tagging plugin produced {0:d} tags.\\n'.format(\n        self._number_of_event_tags)\n    self._number_of_event_tags = 0\n    return reports.AnalysisReport(plugin_name=self.NAME, text=report_text)", "response": "Compiles an analysis report."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ExamineEvent(self, mediator, event):\n    if self._tagging_rules is None:\n      if self._autodetect_tag_file_attempt:\n        # There's nothing to tag with, and we've already tried to find a good\n        # tag file, so there's nothing we can do with this event (or any other).\n        return\n\n      if not self._AttemptAutoDetectTagFile(mediator):\n        logger.info(\n            'No tag definition file specified, and plaso was not able to '\n            'autoselect a tagging file. As no definitions were specified, '\n            'no events will be tagged.')\n        return\n\n    matched_label_names = []\n    for label_name, filter_objects in iter(self._tagging_rules.items()):\n      for filter_object in filter_objects:\n        if filter_object.Match(event):\n          matched_label_names.append(label_name)\n          break\n\n    if matched_label_names:\n      event_tag = self._CreateEventTag(\n          event, self._EVENT_TAG_COMMENT, matched_label_names)\n\n      mediator.ProduceEventTag(event_tag)\n      self._number_of_event_tags += 1", "response": "Analyzes an EventObject and tags it according to the rules in the tag file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the tag file to be used by the plugin.", "response": "def SetAndLoadTagFile(self, tagging_file_path):\n    \"\"\"Sets the tag file to be used by the plugin.\n\n    Args:\n      tagging_file_path (str): path of the tagging file.\n    \"\"\"\n    tag_file = tagging_file.TaggingFile(tagging_file_path)\n    self._tagging_rules = tag_file.GetEventTaggingRules()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef WriteEventBody(self, event):\n    latitude = getattr(event, 'latitude', None)\n    longitude = getattr(event, 'longitude', None)\n    if latitude is not None and longitude is not None:\n      placemark_xml_element = ElementTree.Element('Placemark')\n\n      name_xml_element = ElementTree.SubElement(placemark_xml_element, 'name')\n\n      name_xml_element.text = 'PLACEHOLDER FOR EVENT IDENTIFIER'\n\n      description_xml_element = ElementTree.SubElement(\n          placemark_xml_element, 'description')\n      # TODO: move the description formatting into this output module.\n      description_xml_element.text = (\n          rawpy.NativePythonFormatterHelper.GetFormattedEventObject(event))\n\n      point_xml_element = ElementTree.SubElement(\n          placemark_xml_element, 'Point')\n\n      coordinates_xml_element = ElementTree.SubElement(\n          point_xml_element, 'coordinates')\n      coordinates_xml_element.text = '{0!s},{1!s}'.format(longitude, latitude)\n\n      # Note that ElementTree.tostring() will appropriately escape\n      # the input data.\n      xml_string = ElementTree.tostring(placemark_xml_element)\n\n      output_text = codecs.decode(xml_string, self._output_mediator.encoding)\n      self._output_writer.Write(output_text)", "response": "Writes the body of an event to the output."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef WriteHeader(self):\n    xml_string = (\n        '<?xml version=\"1.0\" encoding=\"{0:s}\"?>'\n        '<kml xmlns=\"http://www.opengis.net/kml/2.2\"><Document>'.format(\n            self._output_mediator.encoding))\n    self._output_writer.Write(xml_string)", "response": "Writes the header to the output."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ParseMessage(self, parser_mediator, key, date_time, tokens):\n    if key != 'task_run':\n      raise ValueError('Unknown grammar key: {0:s}'.format(key))\n\n    event_data = CronTaskRunEventData()\n    event_data.body = tokens.get('body', None)\n    event_data.command = tokens.get('command', None)\n    event_data.hostname = tokens.get('hostname', None)\n    # TODO: pass line number to offset or remove.\n    event_data.offset = 0\n    event_data.pid = tokens.get('pid', None)\n    event_data.reporter = tokens.get('reporter', None)\n    event_data.severity = tokens.get('severity', None)\n    event_data.username = tokens.get('username', None)\n\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_WRITTEN)\n    parser_mediator.ProduceEventWithEventData(event, event_data)", "response": "Parses a syslog message that matched one of the defined grammars."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nanalyzes a block of data attempting to match Yara rules to it.", "response": "def Analyze(self, data):\n    \"\"\"Analyzes a block of data, attempting to match Yara rules to it.\n\n    Args:\n      data(bytes): a block of data.\n    \"\"\"\n    if not self._rules:\n      return\n    try:\n      self._matches = self._rules.match(data=data, timeout=self._MATCH_TIMEOUT)\n    except yara.YaraTimeoutError:\n      logger.error('Could not process file within timeout: {0:d}'.format(\n          self._MATCH_TIMEOUT))\n    except yara.YaraError as exception:\n      logger.error('Error processing file with Yara: {0!s}.'.format(\n          exception))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef GetResults(self):\n    result = analyzer_result.AnalyzerResult()\n    result.analyzer_name = self.NAME\n    result.attribute_name = self._ATTRIBUTE_NAME\n    rule_names = [match.rule for match in self._matches]\n    result.attribute_value = ','.join(rule_names)\n    return [result]", "response": "Retrieves the results of the most recent analysis."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _GetTableNames(self, database):\n    table_names = []\n    for esedb_table in database.tables:\n      table_names.append(esedb_table.name)\n\n    return table_names", "response": "Retrieves the table names in a database."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ParseFileObject(self, parser_mediator, file_object):\n    esedb_file = pyesedb.file()\n\n    try:\n      esedb_file.open_file_object(file_object)\n    except IOError as exception:\n      parser_mediator.ProduceExtractionWarning(\n          'unable to open file with error: {0!s}'.format(exception))\n      return\n\n    # Compare the list of available plugin objects.\n    cache = ESEDBCache()\n    try:\n      table_names = frozenset(self._GetTableNames(esedb_file))\n\n      for plugin in self._plugins:\n        if parser_mediator.abort:\n          break\n\n        if not plugin.required_tables.issubset(table_names):\n          continue\n\n        try:\n          plugin.UpdateChainAndProcess(\n              parser_mediator, cache=cache, database=esedb_file)\n\n        except Exception as exception:  # pylint: disable=broad-except\n          parser_mediator.ProduceExtractionWarning((\n              'plugin: {0:s} unable to parse ESE database with error: '\n              '{1!s}').format(plugin.NAME, exception))\n\n    finally:\n      # TODO: explicitly clean up cache.\n\n      esedb_file.close()", "response": "Parses an ESE database file - like object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing an activity log row.", "response": "def ParseActivityLogUncompressedRow(\n      self, parser_mediator, query, row, **unused_kwargs):\n    \"\"\"Parses an activity log row.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      query (str): query that created the row.\n      row (sqlite3.Row): row.\n    \"\"\"\n    query_hash = hash(query)\n\n    event_data = ChromeExtensionActivityEventData()\n    event_data.action_type = self._GetRowValue(query_hash, row, 'action_type')\n    event_data.activity_id = self._GetRowValue(query_hash, row, 'activity_id')\n    event_data.api_name = self._GetRowValue(query_hash, row, 'api_name')\n    event_data.arg_url = self._GetRowValue(query_hash, row, 'arg_url')\n    event_data.args = self._GetRowValue(query_hash, row, 'args')\n    event_data.extension_id = self._GetRowValue(query_hash, row, 'extension_id')\n    event_data.other = self._GetRowValue(query_hash, row, 'other')\n    event_data.page_title = self._GetRowValue(query_hash, row, 'page_title')\n    event_data.page_url = self._GetRowValue(query_hash, row, 'page_url')\n    event_data.query = query\n\n    timestamp = self._GetRowValue(query_hash, row, 'time')\n    date_time = dfdatetime_webkit_time.WebKitTime(timestamp=timestamp)\n    event = time_events.DateTimeValuesEvent(\n        date_time, definitions.TIME_DESCRIPTION_UNKNOWN)\n    parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef Close(self):\n    if not self._connection:\n      raise RuntimeError('Cannot close database not opened.')\n\n    # We need to run commit or not all data is stored in the database.\n    self._connection.commit()\n    self._connection.close()\n\n    self._connection = None\n    self._cursor = None\n    self.filename = None\n    self.read_only = None", "response": "Closes the database file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef HasTable(self, table_name):\n    if not self._connection:\n      raise RuntimeError(\n          'Cannot determine if table exists database not opened.')\n\n    sql_query = self._HAS_TABLE_QUERY.format(table_name)\n\n    self._cursor.execute(sql_query)\n    if self._cursor.fetchone():\n      return True\n\n    return False", "response": "Determines if a specific table exists."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef GetValues(self, table_names, column_names, condition):\n    if not self._connection:\n      raise RuntimeError('Cannot retrieve values database not opened.')\n\n    if condition:\n      condition = ' WHERE {0:s}'.format(condition)\n\n    sql_query = 'SELECT {1:s} FROM {0:s}{2:s}'.format(\n        ', '.join(table_names), ', '.join(column_names), condition)\n\n    self._cursor.execute(sql_query)\n\n    # TODO: have a look at https://docs.python.org/2/library/\n    # sqlite3.html#sqlite3.Row.\n    for row in self._cursor:\n      yield {\n          column_name: row[column_index]\n          for column_index, column_name in enumerate(column_names)}", "response": "Retrieves values from a table."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nopening the database file.", "response": "def Open(self, filename, read_only=False):\n    \"\"\"Opens the database file.\n\n    Args:\n      filename (str): filename of the database.\n      read_only (Optional[bool]): True if the database should be opened in\n          read-only mode. Since sqlite3 does not support a real read-only\n          mode we fake it by only permitting SELECT queries.\n\n    Returns:\n      bool: True if successful.\n\n    Raises:\n      RuntimeError: if the database is already opened.\n    \"\"\"\n    if self._connection:\n      raise RuntimeError('Cannot open database already opened.')\n\n    self.filename = filename\n    self.read_only = read_only\n\n    try:\n      self._connection = sqlite3.connect(filename)\n    except sqlite3.OperationalError:\n      return False\n\n    if not self._connection:\n      return False\n\n    self._cursor = self._connection.cursor()\n    if not self._cursor:\n      return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving the Event Log provider key from the database.", "response": "def _GetEventLogProviderKey(self, log_source):\n    \"\"\"Retrieves the Event Log provider key.\n\n    Args:\n      log_source (str): Event Log source.\n\n    Returns:\n      str: Event Log provider key or None if not available.\n\n    Raises:\n      RuntimeError: if more than one value is found in the database.\n    \"\"\"\n    table_names = ['event_log_providers']\n    column_names = ['event_log_provider_key']\n    condition = 'log_source == \"{0:s}\"'.format(log_source)\n\n    values_list = list(self._database_file.GetValues(\n        table_names, column_names, condition))\n\n    number_of_values = len(values_list)\n    if number_of_values == 0:\n      return None\n\n    if number_of_values == 1:\n      values = values_list[0]\n      return values['event_log_provider_key']\n\n    raise RuntimeError('More than one value found in database.')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nretrieve a specific message from a specific message table.", "response": "def _GetMessage(self, message_file_key, lcid, message_identifier):\n    \"\"\"Retrieves a specific message from a specific message table.\n\n    Args:\n      message_file_key (int): message file key.\n      lcid (int): language code identifier (LCID).\n      message_identifier (int): message identifier.\n\n    Returns:\n      str: message string or None if not available.\n\n    Raises:\n      RuntimeError: if more than one value is found in the database.\n    \"\"\"\n    table_name = 'message_table_{0:d}_0x{1:08x}'.format(message_file_key, lcid)\n\n    has_table = self._database_file.HasTable(table_name)\n    if not has_table:\n      return None\n\n    column_names = ['message_string']\n    condition = 'message_identifier == \"0x{0:08x}\"'.format(message_identifier)\n\n    values = list(self._database_file.GetValues(\n        [table_name], column_names, condition))\n\n    number_of_values = len(values)\n    if number_of_values == 0:\n      return None\n\n    if number_of_values == 1:\n      return values[0]['message_string']\n\n    raise RuntimeError('More than one value found in database.')"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nretrieves the message file keys.", "response": "def _GetMessageFileKeys(self, event_log_provider_key):\n    \"\"\"Retrieves the message file keys.\n\n    Args:\n      event_log_provider_key (int): Event Log provider key.\n\n    Yields:\n      int: message file key.\n    \"\"\"\n    table_names = ['message_file_per_event_log_provider']\n    column_names = ['message_file_key']\n    condition = 'event_log_provider_key == {0:d}'.format(\n        event_log_provider_key)\n\n    generator = self._database_file.GetValues(\n        table_names, column_names, condition)\n    for values in generator:\n      yield values['message_file_key']"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreformat the message string into a new Python format string.", "response": "def _ReformatMessageString(self, message_string):\n    \"\"\"Reformats the message string.\n\n    Args:\n      message_string (str): message string.\n\n    Returns:\n      str: message string in Python format() (PEP 3101) style.\n    \"\"\"\n    def _PlaceHolderSpecifierReplacer(match_object):\n      \"\"\"Replaces message string place holders into Python format() style.\"\"\"\n      expanded_groups = []\n      for group in match_object.groups():\n        try:\n          place_holder_number = int(group, 10) - 1\n          expanded_group = '{{{0:d}:s}}'.format(place_holder_number)\n        except ValueError:\n          expanded_group = group\n\n        expanded_groups.append(expanded_group)\n\n      return ''.join(expanded_groups)\n\n    if not message_string:\n      return None\n\n    message_string = self._WHITE_SPACE_SPECIFIER_RE.sub(r'', message_string)\n    message_string = self._TEXT_SPECIFIER_RE.sub(r'\\\\\\1', message_string)\n    message_string = self._CURLY_BRACKETS.sub(r'\\1\\1', message_string)\n    return self._PLACE_HOLDER_SPECIFIER_RE.sub(\n        _PlaceHolderSpecifierReplacer, message_string)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef GetMessage(self, log_source, lcid, message_identifier):\n    event_log_provider_key = self._GetEventLogProviderKey(log_source)\n    if not event_log_provider_key:\n      return None\n\n    generator = self._GetMessageFileKeys(event_log_provider_key)\n    if not generator:\n      return None\n\n    # TODO: cache a number of message strings.\n    message_string = None\n    for message_file_key in generator:\n      message_string = self._GetMessage(\n          message_file_key, lcid, message_identifier)\n\n      if message_string:\n        break\n\n    if self._string_format == 'wrc':\n      message_string = self._ReformatMessageString(message_string)\n\n    return message_string", "response": "Retrieves a specific message for a specific Event Log source."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving the metadata attribute from the database.", "response": "def GetMetadataAttribute(self, attribute_name):\n    \"\"\"Retrieves the metadata attribute.\n\n    Args:\n      attribute_name (str): name of the metadata attribute.\n\n    Returns:\n      str: the metadata attribute or None.\n\n    Raises:\n      RuntimeError: if more than one value is found in the database.\n    \"\"\"\n    table_name = 'metadata'\n\n    has_table = self._database_file.HasTable(table_name)\n    if not has_table:\n      return None\n\n    column_names = ['value']\n    condition = 'name == \"{0:s}\"'.format(attribute_name)\n\n    values = list(self._database_file.GetValues(\n        [table_name], column_names, condition))\n\n    number_of_values = len(values)\n    if number_of_values == 0:\n      return None\n\n    if number_of_values == 1:\n      return values[0]['value']\n\n    raise RuntimeError('More than one value found in database.')"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nopen the database reader object.", "response": "def Open(self, filename):\n    \"\"\"Opens the database reader object.\n\n    Args:\n      filename (str): filename of the database.\n\n    Returns:\n      bool: True if successful.\n\n    Raises:\n      RuntimeError: if the version or string format of the database\n                    is not supported.\n    \"\"\"\n    if not super(WinevtResourcesSqlite3DatabaseReader, self).Open(filename):\n      return False\n\n    version = self.GetMetadataAttribute('version')\n    if not version or version != '20150315':\n      raise RuntimeError('Unsupported version: {0:s}'.format(version))\n\n    string_format = self.GetMetadataAttribute('string_format')\n    if not string_format:\n      string_format = 'wrc'\n\n    if string_format not in ('pep3101', 'wrc'):\n      raise RuntimeError('Unsupported string format: {0:s}'.format(\n          string_format))\n\n    self._string_format = string_format\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef CopyToDict(self):\n    dictionary = {}\n    for attribute_name, attribute_value in self.GetAttributes():\n      if attribute_value is None:\n        continue\n\n      dictionary[attribute_name] = attribute_value\n\n    return dictionary", "response": "Copies the attribute container to a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef GetString(self):\n    string_list = []\n    string_list.append('Report generated from: {0:s}'.format(self.plugin_name))\n\n    time_compiled = getattr(self, 'time_compiled', 0)\n    if time_compiled:\n      time_compiled = timelib.Timestamp.CopyToIsoFormat(time_compiled)\n      string_list.append('Generated on: {0:s}'.format(time_compiled))\n\n    filter_string = getattr(self, 'filter_string', '')\n    if filter_string:\n      string_list.append('Filter String: {0:s}'.format(filter_string))\n\n    string_list.append('')\n    string_list.append('Report text:')\n    string_list.append(self.text)\n\n    return '\\n'.join(string_list)", "response": "Retrieves a string representation of the report."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing and validates the options.", "response": "def ParseOptions(cls, options, configuration_object):\n    \"\"\"Parses and validates options.\n\n    Args:\n      options (argparse.Namespace): parser options.\n      configuration_object (CLITool): object to be configured by the argument\n          helper.\n\n    Raises:\n      BadConfigObject: when the configuration object is of the wrong type.\n    \"\"\"\n    if not isinstance(configuration_object, tools.CLITool):\n      raise errors.BadConfigObject(\n          'Configuration object is not an instance of CLITool')\n\n    profilers = cls._ParseStringOption(options, 'profilers')\n\n    if not profilers:\n      profilers = set()\n    elif profilers.lower() != 'list':\n      profilers = set(profilers.split(','))\n\n      supported_profilers = set(cls.PROFILERS_INFORMATION.keys())\n      unsupported_profilers = profilers.difference(supported_profilers)\n      if unsupported_profilers:\n        unsupported_profilers = ', '.join(unsupported_profilers)\n        raise errors.BadConfigOption(\n            'Unsupported profilers: {0:s}'.format(unsupported_profilers))\n\n    profiling_directory = getattr(options, 'profiling_directory', None)\n    if profiling_directory and not os.path.isdir(profiling_directory):\n      raise errors.BadConfigOption(\n          'No such profiling directory: {0:s}'.format(profiling_directory))\n\n    profiling_sample_rate = getattr(options, 'profiling_sample_rate', None)\n    if not profiling_sample_rate:\n      profiling_sample_rate = cls.DEFAULT_PROFILING_SAMPLE_RATE\n    else:\n      try:\n        profiling_sample_rate = int(profiling_sample_rate, 10)\n      except (TypeError, ValueError):\n        raise errors.BadConfigOption(\n            'Invalid profile sample rate: {0!s}.'.format(profiling_sample_rate))\n\n    setattr(configuration_object, '_profilers', profilers)\n    setattr(configuration_object, '_profiling_directory', profiling_directory)\n    setattr(\n        configuration_object, '_profiling_sample_rate', profiling_sample_rate)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nevaluates if this is the correct plugin and processes data accordingly.", "response": "def Process(self, parser_mediator, **kwargs):\n    \"\"\"Evaluates if this is the correct plugin and processes data accordingly.\n\n    The purpose of the process function is to evaluate if this particular\n    plugin is the correct one for the particular data structure at hand.\n    This function accepts one value to use for evaluation, that could be\n    a registry key, list of table names for a database or any other criteria\n    that can be used to evaluate if the plugin should be run or not.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between\n          parsers and other components, such as storage and dfvfs.\n      kwargs (dict[str, object]): Depending on the plugin they may require\n          different sets of arguments to be able to evaluate whether or not\n          this is the correct plugin.\n\n    Raises:\n      ValueError: when there are unused keyword arguments.\n    \"\"\"\n    if kwargs:\n      raise ValueError('Unused keyword arguments: {0:s}.'.format(\n          ', '.join(kwargs.keys())))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse and validates options.", "response": "def ParseOptions(cls, options, configuration_object):\n    \"\"\"Parses and validates options.\n\n    Args:\n      options (argparse.Namespace): parser options.\n      configuration_object (CLITool): object to be configured by the argument\n          helper.\n\n    Raises:\n      BadConfigObject: when the configuration object is of the wrong type.\n    \"\"\"\n    if not isinstance(configuration_object, tools.CLITool):\n      raise errors.BadConfigObject(\n          'Configuration object is not an instance of CLITool')\n\n    data_location = cls._ParseStringOption(options, 'data_location')\n    if not data_location:\n      # Determine the source root path, which is 3 directories up from\n      # the location of the script.\n      data_location = os.path.dirname(cls._PATH)\n      data_location = os.path.dirname(data_location)\n      data_location = os.path.dirname(data_location)\n      data_location = os.path.dirname(data_location)\n\n      # There are multiple options to run a tool e.g. running from source or\n      # from an egg file.\n      data_location_egg = os.path.join(data_location, 'share', 'plaso')\n      data_location_source = os.path.join(data_location, 'data')\n\n      data_location = None\n      if os.path.exists(data_location_egg) and os.path.isfile(os.path.join(\n          data_location_egg, 'plaso-data.README')):\n        data_location = data_location_egg\n      elif os.path.exists(data_location_source) and os.path.isfile(os.path.join(\n          data_location_source, 'plaso-data.README')):\n        data_location = data_location_source\n\n      if not data_location or not os.path.exists(data_location):\n        data_location = os.path.join(sys.prefix, 'share', 'plaso')\n      if not os.path.exists(data_location):\n        data_location = os.path.join(sys.prefix, 'local', 'share', 'plaso')\n\n      if sys.prefix != '/usr':\n        if not os.path.exists(data_location):\n          data_location = os.path.join('/usr', 'share', 'plaso')\n        if not os.path.exists(data_location):\n          data_location = os.path.join('/usr', 'local', 'share', 'plaso')\n\n      if not os.path.exists(data_location) or not os.path.isfile(os.path.join(\n          data_location, 'plaso-data.README')):\n        data_location = None\n\n    if not data_location:\n      raise errors.BadConfigOption(\n          'Unable to determine location of data files.')\n\n    logger.info('Determined data location: {0:s}'.format(data_location))\n\n    setattr(configuration_object, '_data_location', data_location)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds command line arguments to an argument group.", "response": "def AddArguments(cls, argument_group):\n    \"\"\"Adds command line arguments the helper supports to an argument group.\n\n    This function takes an argument parser or an argument group object and adds\n    to it all the command line arguments this helper supports.\n\n    Args:\n      argument_group (argparse._ArgumentGroup|argparse.ArgumentParser): group\n          to append arguments to.\n    \"\"\"\n    argument_group.add_argument(\n        '--nsrlsvr-hash', '--nsrlsvr_hash', dest='nsrlsvr_hash', type=str,\n        action='store', choices=nsrlsvr.NsrlsvrAnalyzer.SUPPORTED_HASHES,\n        default=cls._DEFAULT_HASH, metavar='HASH', help=(\n            'Type of hash to use to query nsrlsvr instance, the default is: '\n            '{0:s}. Supported options: {1:s}'.format(\n                cls._DEFAULT_HASH, ', '.join(\n                    nsrlsvr.NsrlsvrAnalyzer.SUPPORTED_HASHES))))\n\n    argument_group.add_argument(\n        '--nsrlsvr-host', '--nsrlsvr_host', dest='nsrlsvr_host', type=str,\n        action='store', default=cls._DEFAULT_HOST, metavar='HOST',\n        help=(\n            'Hostname or IP address of the nsrlsvr instance to query, the '\n            'default is: {0:s}').format(cls._DEFAULT_HOST))\n\n    argument_group.add_argument(\n        '--nsrlsvr-label', '--nsrlsvr_label', dest='nsrlsvr_label', type=str,\n        action='store', default=cls._DEFAULT_LABEL, metavar='LABEL', help=(\n            'Label to apply to events, the default is: '\n            '{0:s}.').format(cls._DEFAULT_LABEL))\n\n    argument_group.add_argument(\n        '--nsrlsvr-port', '--nsrlsvr_port', dest='nsrlsvr_port', type=int,\n        action='store', default=cls._DEFAULT_PORT, metavar='PORT', help=(\n            'Port number of the nsrlsvr instance to query, the default is: '\n            '{0:d}.').format(cls._DEFAULT_PORT))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ParseOptions(cls, options, analysis_plugin):\n    if not isinstance(analysis_plugin, nsrlsvr.NsrlsvrAnalysisPlugin):\n      raise errors.BadConfigObject(\n          'Analysis plugin is not an instance of NsrlsvrAnalysisPlugin')\n\n    label = cls._ParseStringOption(\n        options, 'nsrlsvr_label', default_value=cls._DEFAULT_LABEL)\n    analysis_plugin.SetLabel(label)\n\n    lookup_hash = cls._ParseStringOption(\n        options, 'nsrlsvr_hash', default_value=cls._DEFAULT_HASH)\n    analysis_plugin.SetLookupHash(lookup_hash)\n\n    host = cls._ParseStringOption(\n        options, 'nsrlsvr_host', default_value=cls._DEFAULT_HOST)\n    analysis_plugin.SetHost(host)\n\n    port = cls._ParseNumericOption(\n        options, 'nsrlsvr_port', default_value=cls._DEFAULT_PORT)\n    analysis_plugin.SetPort(port)\n\n    if not analysis_plugin.TestConnection():\n      raise errors.BadConfigOption(\n          'Unable to connect to nsrlsvr {0:s}:{1:d}'.format(host, port))", "response": "Parses and validates the options."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef AddArguments(cls, argument_group):\n    storage_formats = sorted(definitions.STORAGE_FORMATS)\n\n    argument_group.add_argument(\n        '--storage_format', '--storage-format', action='store',\n        choices=storage_formats, dest='storage_format', type=str,\n        metavar='FORMAT', default=definitions.DEFAULT_STORAGE_FORMAT, help=(\n            'Format of the storage file, the default is: {0:s}. Supported '\n            'options: {1:s}'.format(\n                definitions.DEFAULT_STORAGE_FORMAT,\n                ', '.join(storage_formats))))", "response": "Adds command line arguments to an argument group."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ParseOptions(cls, options, configuration_object):\n    if not isinstance(configuration_object, tools.CLITool):\n      raise errors.BadConfigObject(\n          'Configuration object is not an instance of CLITool')\n\n    storage_format = cls._ParseStringOption(options, 'storage_format')\n    if not storage_format:\n      raise errors.BadConfigOption('Unable to determine storage format.')\n\n    if storage_format not in definitions.STORAGE_FORMATS:\n      raise errors.BadConfigOption(\n          'Unsupported storage format: {0:s}'.format(storage_format))\n\n    setattr(configuration_object, '_storage_format', storage_format)", "response": "Parses and validates the options."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ConfigureLogging(\n    debug_output=False, filename=None, mode='w', quiet_mode=False):\n  \"\"\"Configures the logging root logger.\n\n  Args:\n    debug_output (Optional[bool]): True if the logging should include debug\n        output.\n    filename (Optional[str]): log filename.\n    mode (Optional[str]): log file access mode.\n    quiet_mode (Optional[bool]): True if the logging should not include\n        information output. Note that debug_output takes precedence over\n        quiet_mode.\n  \"\"\"\n  # Remove all possible log handlers. The log handlers cannot be reconfigured\n  # and therefore must be recreated.\n  for handler in logging.root.handlers:\n    logging.root.removeHandler(handler)\n\n  logger = logging.getLogger()\n\n  if filename and filename.endswith('.gz'):\n    handler = CompressedFileHandler(filename, mode=mode)\n  elif filename:\n    handler = logging.FileHandler(filename, mode=mode)\n  else:\n    handler = logging.StreamHandler()\n\n  format_string = (\n      '%(asctime)s [%(levelname)s] (%(processName)-10s) PID:%(process)d '\n      '<%(module)s> %(message)s')\n\n  formatter = logging.Formatter(format_string)\n  handler.setFormatter(formatter)\n\n  if debug_output:\n    level = logging.DEBUG\n  elif quiet_mode:\n    level = logging.WARNING\n  else:\n    level = logging.INFO\n\n  logger.setLevel(level)\n  handler.setLevel(level)\n\n  logger.addHandler(handler)", "response": "Configures the logging root logger."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nopens the compressed log file.", "response": "def _open(self):\n    \"\"\"Opens the compressed log file.\n\n    Returns\n      file: file-like object of the resulting stream.\n    \"\"\"\n    # The gzip module supports directly setting encoding as of Python 3.3.\n    # pylint: disable=unexpected-keyword-arg\n    if py2to3.PY_3:\n      return gzip.open(\n          self.baseFilename, mode=self.mode, encoding=self.encoding)\n\n    return gzip.open(self.baseFilename, self.mode)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsend a SUBSCRIBE packet to the broker.", "response": "def mqtt_subscribe(self, topics, packet_id):\n        \"\"\"\n        :param topics: array of topics [{'filter':'/a/b', 'qos': 0x00}, ...]\n        :return:\n        \"\"\"\n\n        # Build and send SUBSCRIBE message\n        subscribe = SubscribePacket.build(topics, packet_id)\n        yield from self._send_packet(subscribe)\n\n        # Wait for SUBACK is received\n        waiter = futures.Future(loop=self._loop)\n        self._subscriptions_waiter[subscribe.variable_header.packet_id] = waiter\n        return_codes = yield from waiter\n\n        del self._subscriptions_waiter[subscribe.variable_header.packet_id]\n        return return_codes"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef start(self):\n        try:\n            self._sessions = dict()\n            self._subscriptions = dict()\n            self._retained_messages = dict()\n            self.transitions.start()\n            self.logger.debug(\"Broker starting\")\n        except (MachineError, ValueError) as exc:\n            # Backwards compat: MachineError is raised by transitions < 0.5.0.\n            self.logger.warning(\"[WARN-0001] Invalid method call at this moment: %s\" % exc)\n            raise BrokerException(\"Broker instance can't be started: %s\" % exc)\n\n        yield from self.plugins_manager.fire_event(EVENT_BROKER_PRE_START)\n        try:\n            # Start network listeners\n            for listener_name in self.listeners_config:\n                listener = self.listeners_config[listener_name]\n\n                if 'bind' not in listener:\n                    self.logger.debug(\"Listener configuration '%s' is not bound\" % listener_name)\n                else:\n                    # Max connections\n                    try:\n                        max_connections = listener['max_connections']\n                    except KeyError:\n                        max_connections = -1\n\n                    # SSL Context\n                    sc = None\n\n                    # accept string \"on\" / \"off\" or boolean\n                    ssl_active = listener.get('ssl', False)\n                    if isinstance(ssl_active, str):\n                        ssl_active = ssl_active.upper() == 'ON'\n\n                    if ssl_active:\n                        try:\n                            sc = ssl.create_default_context(\n                                ssl.Purpose.CLIENT_AUTH,\n                                cafile=listener.get('cafile'),\n                                capath=listener.get('capath'),\n                                cadata=listener.get('cadata')\n                            )\n                            sc.load_cert_chain(listener['certfile'], listener['keyfile'])\n                            sc.verify_mode = ssl.CERT_OPTIONAL\n                        except KeyError as ke:\n                            raise BrokerException(\"'certfile' or 'keyfile' configuration parameter missing: %s\" % ke)\n                        except FileNotFoundError as fnfe:\n                            raise BrokerException(\"Can't read cert files '%s' or '%s' : %s\" %\n                                                  (listener['certfile'], listener['keyfile'], fnfe))\n\n                    address, s_port = listener['bind'].split(':')\n                    port = 0\n                    try:\n                        port = int(s_port)\n                    except ValueError as ve:\n                        raise BrokerException(\"Invalid port value in bind value: %s\" % listener['bind'])\n\n                    if listener['type'] == 'tcp':\n                        cb_partial = partial(self.stream_connected, listener_name=listener_name)\n                        instance = yield from asyncio.start_server(cb_partial,\n                                                                   address,\n                                                                   port,\n                                                                   ssl=sc,\n                                                                   loop=self._loop)\n                        self._servers[listener_name] = Server(listener_name, instance, max_connections, self._loop)\n                    elif listener['type'] == 'ws':\n                        cb_partial = partial(self.ws_connected, listener_name=listener_name)\n                        instance = yield from websockets.serve(cb_partial, address, port, ssl=sc, loop=self._loop,\n                                                               subprotocols=['mqtt'])\n                        self._servers[listener_name] = Server(listener_name, instance, max_connections, self._loop)\n\n                    self.logger.info(\"Listener '%s' bind to %s (max_connections=%d)\" %\n                                     (listener_name, listener['bind'], max_connections))\n\n            self.transitions.starting_success()\n            yield from self.plugins_manager.fire_event(EVENT_BROKER_POST_START)\n\n            #Start broadcast loop\n            self._broadcast_task = asyncio.ensure_future(self._broadcast_loop(), loop=self._loop)\n\n            self.logger.debug(\"Broker started\")\n        except Exception as e:\n            self.logger.error(\"Broker startup failed: %s\" % e)\n            self.transitions.starting_fail()\n            raise BrokerException(\"Broker instance can't be started: %s\" % e)", "response": "Start the broker instance with the given configuration."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef shutdown(self):\n        try:\n            self._sessions = dict()\n            self._subscriptions = dict()\n            self._retained_messages = dict()\n            self.transitions.shutdown()\n        except (MachineError, ValueError) as exc:\n            # Backwards compat: MachineError is raised by transitions < 0.5.0.\n            self.logger.debug(\"Invalid method call at this moment: %s\" % exc)\n            raise BrokerException(\"Broker instance can't be stopped: %s\" % exc)\n\n        # Fire broker_shutdown event to plugins\n        yield from self.plugins_manager.fire_event(EVENT_BROKER_PRE_SHUTDOWN)\n\n        # Stop broadcast loop\n        if self._broadcast_task:\n            self._broadcast_task.cancel()\n        if self._broadcast_queue.qsize() > 0:\n            self.logger.warning(\"%d messages not broadcasted\" % self._broadcast_queue.qsize())\n\n        for listener_name in self._servers:\n            server = self._servers[listener_name]\n            yield from server.close_instance()\n        self.logger.debug(\"Broker closing\")\n        self.logger.info(\"Broker closed\")\n        yield from self.plugins_manager.fire_event(EVENT_BROKER_POST_SHUTDOWN)\n        self.transitions.stopping_success()", "response": "Stop broker instance.\n\n            Closes all connected session, stop listening on network socket and free resources."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _init_handler(self, session, reader, writer):\n        handler = BrokerProtocolHandler(self.plugins_manager, self._loop)\n        handler.attach(session, reader, writer)\n        return handler", "response": "Create a BrokerProtocolHandler and attach to a session"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nstops a running handler and detach if from the session", "response": "def _stop_handler(self, handler):\n        \"\"\"\n        Stop a running handler and detach if from the session\n        :param handler:\n        :return:\n        \"\"\"\n        try:\n            yield from handler.stop()\n        except Exception as e:\n            self.logger.error(e)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef topic_filtering(self, session: Session, topic):\n        topic_plugins = None\n        topic_config = self.config.get('topic-check', None)\n        if topic_config and topic_config.get('enabled', False):\n            topic_plugins = topic_config.get('plugins', None)\n        returns = yield from self.plugins_manager.map_plugin_coro(\n            \"topic_filtering\",\n            session=session,\n            topic=topic,\n            filter_plugins=topic_plugins)\n        topic_result = True\n        if returns:\n            for plugin in returns:\n                res = returns[plugin]\n                if res is False:\n                    topic_result = False\n                    self.logger.debug(\"Topic filtering failed due to '%s' plugin result: %s\" % (plugin.name, res))\n                else:\n                    self.logger.debug(\"'%s' plugin result: %s\" % (plugin.name, res))\n        # If all plugins returned True, authentication is success\n        return topic_result", "response": "This method checks that the subscription is allowed for the topic in which the client wants to subscribe to the topic."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndeleting a subscription on a given topic", "response": "def _del_subscription(self, a_filter, session):\n        \"\"\"\n        Delete a session subscription on a given topic\n        :param a_filter:\n        :param session:\n        :return:\n        \"\"\"\n        deleted = 0\n        try:\n            subscriptions = self._subscriptions[a_filter]\n            for index, (sub_session, qos) in enumerate(subscriptions):\n                if sub_session.client_id == session.client_id:\n                    self.logger.debug(\"Removing subscription on topic '%s' for client %s\" %\n                                      (a_filter, format_client_message(session=session)))\n                    subscriptions.pop(index)\n                    deleted += 1\n                    break\n        except KeyError:\n            # Unsubscribe topic not found in current subscribed topics\n            pass\n        finally:\n            return deleted"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndeletes all topic subscriptions for a given session.", "response": "def _del_all_subscriptions(self, session):\n        \"\"\"\n        Delete all topic subscriptions for a given session\n        :param session:\n        :return:\n        \"\"\"\n        filter_queue = deque()\n        for topic in self._subscriptions:\n            if self._del_subscription(topic, session):\n                filter_queue.append(topic)\n        for topic in filter_queue:\n            if not self._subscriptions[topic]:\n                del self._subscriptions[topic]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndelete an existing session data for example due to clean session set in CONNECT .", "response": "def delete_session(self, client_id):\n        \"\"\"\n        Delete an existing session data, for example due to clean session set in CONNECT\n        :param client_id:\n        :return:\n        \"\"\"\n        try:\n            session = self._sessions[client_id][0]\n        except KeyError:\n            session = None\n        if session is None:\n            self.logger.debug(\"Delete session : session %s doesn't exist\" % client_id)\n            return\n\n        # Delete subscriptions\n        self.logger.debug(\"deleting session %s subscriptions\" % repr(session))\n        self._del_all_subscriptions(session)\n\n        self.logger.debug(\"deleting existing session %s\" % repr(self._sessions[client_id]))\n        del self._sessions[client_id]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef connect(self,\n                uri=None,\n                cleansession=None,\n                cafile=None,\n                capath=None,\n                cadata=None,\n                extra_headers={}):\n        \"\"\"\n            Connect to a remote broker.\n\n            At first, a network connection is established with the server using the given protocol (``mqtt``, ``mqtts``, ``ws`` or ``wss``). Once the socket is connected, a `CONNECT <http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/os/mqtt-v3.1.1-os.html#_Toc398718028>`_ message is sent with the requested informations.\n\n            This method is a *coroutine*.\n\n            :param uri: Broker URI connection, conforming to `MQTT URI scheme <https://github.com/mqtt/mqtt.github.io/wiki/URI-Scheme>`_. Uses ``uri`` config attribute by default.\n            :param cleansession: MQTT CONNECT clean session flag\n            :param cafile: server certificate authority file (optional, used for secured connection)\n            :param capath: server certificate authority path (optional, used for secured connection)\n            :param cadata: server certificate authority data (optional, used for secured connection)\n            :param extra_headers: a dictionary with additional http headers that should be sent on the initial connection (optional, used only with websocket connections)\n            :return: `CONNACK <http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/os/mqtt-v3.1.1-os.html#_Toc398718033>`_ return code\n            :raise: :class:`hbmqtt.client.ConnectException` if connection fails\n        \"\"\"\n\n        self.session = self._initsession(uri, cleansession, cafile, capath, cadata)\n        self.extra_headers = extra_headers;\n        self.logger.debug(\"Connect to: %s\" % uri)\n\n        try:\n            return (yield from self._do_connect())\n        except BaseException as be:\n            self.logger.warning(\"Connection failed: %r\" % be)\n            auto_reconnect = self.config.get('auto_reconnect', False)\n            if not auto_reconnect:\n                raise\n            else:\n                return (yield from self.reconnect())", "response": "Connect to a remote broker."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef disconnect(self):\n\n        if self.session.transitions.is_connected():\n            if not self._disconnect_task.done():\n                self._disconnect_task.cancel()\n            yield from self._handler.mqtt_disconnect()\n            self._connected_state.clear()\n            yield from self._handler.stop()\n            self.session.transitions.disconnect()\n        else:\n            self.logger.warning(\"Client session is not currently connected, ignoring call\")", "response": "Disconnect from the broker."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef reconnect(self, cleansession=None):\n\n        if self.session.transitions.is_connected():\n            self.logger.warning(\"Client already connected\")\n            return CONNECTION_ACCEPTED\n\n        if cleansession:\n            self.session.clean_session = cleansession\n        self.logger.debug(\"Reconnecting with session parameters: %s\" % self.session)\n        reconnect_max_interval = self.config.get('reconnect_max_interval', 10)\n        reconnect_retries = self.config.get('reconnect_retries', 5)\n        nb_attempt = 1\n        yield from asyncio.sleep(1, loop=self._loop)\n        while True:\n            try:\n                self.logger.debug(\"Reconnect attempt %d ...\" % nb_attempt)\n                return (yield from self._do_connect())\n            except BaseException as e:\n                self.logger.warning(\"Reconnection attempt failed: %r\" % e)\n                if reconnect_retries >= 0 and nb_attempt > reconnect_retries:\n                    self.logger.error(\"Maximum number of connection attempts reached. Reconnection aborted\")\n                    raise ConnectException(\"Too many connection attempts failed\")\n                exp = 2 ** nb_attempt\n                delay = exp if exp < reconnect_max_interval else reconnect_max_interval\n                self.logger.debug(\"Waiting %d second before next attempt\" % delay)\n                yield from asyncio.sleep(delay, loop=self._loop)\n                nb_attempt += 1", "response": "Reconnect a previously connected broker."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nping the broker. Send a MQTT `PINGREQ <http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/os/mqtt-v3.1.1-os.html#_Toc398718081>`_ message for response. This method is a *coroutine*.", "response": "def ping(self):\n        \"\"\"\n            Ping the broker.\n\n            Send a MQTT `PINGREQ <http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/os/mqtt-v3.1.1-os.html#_Toc398718081>`_ message for response.\n\n            This method is a *coroutine*.\n        \"\"\"\n\n        if self.session.transitions.is_connected():\n            yield from self._handler.mqtt_ping()\n        else:\n            self.logger.warning(\"MQTT PING request incompatible with current session state '%s'\" %\n                                self.session.transitions.state)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\npublish a message to the broker.", "response": "def publish(self, topic, message, qos=None, retain=None, ack_timeout=None):\n        \"\"\"\n            Publish a message to the broker.\n\n            Send a MQTT `PUBLISH <http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/os/mqtt-v3.1.1-os.html#_Toc398718037>`_ message and wait for acknowledgment depending on Quality Of Service\n\n            This method is a *coroutine*.\n\n            :param topic: topic name to which message data is published\n            :param message: payload message (as bytes) to send.\n            :param qos: requested publish quality of service : QOS_0, QOS_1 or QOS_2. Defaults to ``default_qos`` config parameter or QOS_0.\n            :param retain: retain flag. Defaults to ``default_retain`` config parameter or False.\n        \"\"\"\n\n        def get_retain_and_qos():\n            if qos:\n                assert qos in (QOS_0, QOS_1, QOS_2)\n                _qos = qos\n            else:\n                _qos = self.config['default_qos']\n                try:\n                    _qos = self.config['topics'][topic]['qos']\n                except KeyError:\n                    pass\n            if retain:\n                _retain = retain\n            else:\n                _retain = self.config['default_retain']\n                try:\n                    _retain = self.config['topics'][topic]['retain']\n                except KeyError:\n                    pass\n            return _qos, _retain\n        (app_qos, app_retain) = get_retain_and_qos()\n        return (yield from self._handler.mqtt_publish(topic, message, app_qos, app_retain, ack_timeout))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef subscribe(self, topics):\n        return (yield from self._handler.mqtt_subscribe(topics, self.session.next_packet_id))", "response": "Subscribe to some topics."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndelivers next received message from the broker.", "response": "def deliver_message(self, timeout=None):\n        \"\"\"\n            Deliver next received message.\n\n            Deliver next message received from the broker. If no message is available, this methods waits until next message arrives or ``timeout`` occurs.\n\n            This method is a *coroutine*.\n\n            :param timeout: maximum number of seconds to wait before returning. If timeout is not specified or None, there is no limit to the wait time until next message arrives.\n            :return: instance of :class:`hbmqtt.session.ApplicationMessage` containing received message information flow.\n            :raises: :class:`asyncio.TimeoutError` if timeout occurs before a message is delivered\n        \"\"\"\n        deliver_task = asyncio.ensure_future(self._handler.mqtt_deliver_next_message(), loop=self._loop)\n        self.client_tasks.append(deliver_task)\n        self.logger.debug(\"Waiting message delivery\")\n        done, pending = yield from asyncio.wait([deliver_task], loop=self._loop, return_when=asyncio.FIRST_EXCEPTION, timeout=timeout)\n        if deliver_task in done:\n            if deliver_task.exception() is not None:\n                # deliver_task raised an exception, pass it on to our caller\n                raise deliver_task.exception()\n            self.client_tasks.pop()\n            return deliver_task.result()\n        else:\n            #timeout occured before message received\n            deliver_task.cancel()\n            raise asyncio.TimeoutError"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbuild a PublishPacket from this ApplicationMessage instance.", "response": "def build_publish_packet(self, dup=False):\n        \"\"\"\n            Build :class:`hbmqtt.mqtt.publish.PublishPacket` from attributes\n\n        :param dup: force dup flag\n        :return: :class:`hbmqtt.mqtt.publish.PublishPacket` built from ApplicationMessage instance attributes\n        \"\"\"\n        return PublishPacket.build(self.topic, self.data, self.packet_id, dup, self.qos, self.retain)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef int_to_bytes(int_value: int, length: int) -> bytes:\n    if length == 1:\n        fmt = \"!B\"\n    elif length == 2:\n        fmt = \"!H\"\n    return pack(fmt, int_value)", "response": "convert an integer to a sequence of bytes using big endian byte ordering"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef read_or_raise(reader, n=-1):\n    data = yield from reader.read(n)\n    if not data:\n        raise NoDataException(\"No more data\")\n    return data", "response": "Read a given number of bytes from a stream."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef decode_string(reader) -> bytes:\n    length_bytes = yield from read_or_raise(reader, 2)\n    str_length = unpack(\"!H\", length_bytes)\n    if str_length[0]:\n        byte_str = yield from read_or_raise(reader, str_length[0])\n        try:\n            return byte_str.decode(encoding='utf-8')\n        except:\n            return str(byte_str)\n    else:\n        return ''", "response": "Read a string from a reader and decode it according to MQTT string specification."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef decode_data_with_length(reader) -> bytes:\n    length_bytes = yield from read_or_raise(reader, 2)\n    bytes_length = unpack(\"!H\", length_bytes)\n    data = yield from read_or_raise(reader, bytes_length[0])\n    return data", "response": "Read data from a reader."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef decode_packet_id(reader) -> int:\n    packet_id_bytes = yield from read_or_raise(reader, 2)\n    packet_id = unpack(\"!H\", packet_id_bytes)\n    return packet_id[0]", "response": "Reads a packet ID from the stream according to MQTT specification 2. 3. 1."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_plugin(self, name):\n        for p in self._plugins:\n            if p.name == name:\n                return p\n        return None", "response": "Get a plugin by its name from the loaded plugins."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfiring an event to plugins.", "response": "def fire_event(self, event_name, wait=False, *args, **kwargs):\n        \"\"\"\n        Fire an event to plugins.\n        PluginManager schedule @asyncio.coroutinecalls for each plugin on method called \"on_\" + event_name\n        For example, on_connect will be called on event 'connect'\n        Method calls are schedule in the asyn loop. wait parameter must be set to true to wait until all\n        mehtods are completed.\n        :param event_name:\n        :param args:\n        :param kwargs:\n        :param wait: indicates if fire_event should wait for plugin calls completion (True), or not\n        :return:\n        \"\"\"\n        tasks = []\n        event_method_name = \"on_\" + event_name\n        for plugin in self._plugins:\n            event_method = getattr(plugin.object, event_method_name, None)\n            if event_method:\n                try:\n                    task = self._schedule_coro(event_method(*args, **kwargs))\n                    tasks.append(task)\n\n                    def clean_fired_events(future):\n                        try:\n                            self._fired_events.remove(task)\n                        except (KeyError, ValueError):\n                            pass\n\n                    task.add_done_callback(clean_fired_events)\n                except AssertionError:\n                    self.logger.error(\"Method '%s' on plugin '%s' is not a coroutine\" %\n                                      (event_method_name, plugin.name))\n\n        self._fired_events.extend(tasks)\n        if wait:\n            if tasks:\n                yield from asyncio.wait(tasks, loop=self._loop)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef map(self, coro, *args, **kwargs):\n        p_list = kwargs.pop('filter_plugins', None)\n        if p_list is None:\n            p_list = [p.name for p in self.plugins]\n        tasks = []\n        plugins_list = []\n        for plugin in self._plugins:\n            if plugin.name in p_list:\n                coro_instance = coro(plugin, *args, **kwargs)\n                if coro_instance:\n                    try:\n                        tasks.append(self._schedule_coro(coro_instance))\n                        plugins_list.append(plugin)\n                    except AssertionError:\n                        self.logger.error(\"Method '%r' on plugin '%s' is not a coroutine\" %\n                                          (coro, plugin.name))\n        if tasks:\n            ret_list = yield from asyncio.gather(*tasks, loop=self._loop)\n            # Create result map plugin=>ret\n            ret_dict = {k: v for k, v in zip(plugins_list, ret_list)}\n        else:\n            ret_dict = {}\n        return ret_dict", "response": "Schedule a given coroutine call for each plugin."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef map_plugin_coro(self, coro_name, *args, **kwargs):\n        return (yield from self.map(self._call_coro, coro_name, *args, **kwargs))", "response": "Map a plugin by its name by its coroutine."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nclear the broker statistics data structures", "response": "def _clear_stats(self):\n        \"\"\"\n        Initializes broker statistics data structures\n        \"\"\"\n        for stat in (STAT_BYTES_RECEIVED,\n                     STAT_BYTES_SENT,\n                     STAT_MSG_RECEIVED,\n                     STAT_MSG_SENT,\n                     STAT_CLIENTS_MAXIMUM,\n                     STAT_CLIENTS_CONNECTED,\n                     STAT_CLIENTS_DISCONNECTED,\n                     STAT_PUBLISH_RECEIVED,\n                     STAT_PUBLISH_SENT):\n            self._stats[stat] = 0"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef broadcast_dollar_sys_topics(self):\n\n        # Update stats\n        uptime = datetime.now() - self._stats[STAT_START_TIME]\n        client_connected = self._stats[STAT_CLIENTS_CONNECTED]\n        client_disconnected = self._stats[STAT_CLIENTS_DISCONNECTED]\n        inflight_in = 0\n        inflight_out = 0\n        messages_stored = 0\n        for session in self.context.sessions:\n            inflight_in += session.inflight_in_count\n            inflight_out += session.inflight_out_count\n            messages_stored += session.retained_messages_count\n        messages_stored += len(self.context.retained_messages)\n        subscriptions_count = 0\n        for topic in self.context.subscriptions:\n            subscriptions_count += len(self.context.subscriptions[topic])\n\n        # Broadcast updates\n        tasks = deque()\n        tasks.append(self.schedule_broadcast_sys_topic('load/bytes/received', int_to_bytes_str(self._stats[STAT_BYTES_RECEIVED])))\n        tasks.append(self.schedule_broadcast_sys_topic('load/bytes/sent', int_to_bytes_str(self._stats[STAT_BYTES_SENT])))\n        tasks.append(self.schedule_broadcast_sys_topic('messages/received', int_to_bytes_str(self._stats[STAT_MSG_RECEIVED])))\n        tasks.append(self.schedule_broadcast_sys_topic('messages/sent', int_to_bytes_str(self._stats[STAT_MSG_SENT])))\n        tasks.append(self.schedule_broadcast_sys_topic('time', str(datetime.now()).encode('utf-8')))\n        tasks.append(self.schedule_broadcast_sys_topic('uptime', int_to_bytes_str(int(uptime.total_seconds()))))\n        tasks.append(self.schedule_broadcast_sys_topic('uptime/formated', str(uptime).encode('utf-8')))\n        tasks.append(self.schedule_broadcast_sys_topic('clients/connected', int_to_bytes_str(client_connected)))\n        tasks.append(self.schedule_broadcast_sys_topic('clients/disconnected', int_to_bytes_str(client_disconnected)))\n        tasks.append(self.schedule_broadcast_sys_topic('clients/maximum', int_to_bytes_str(self._stats[STAT_CLIENTS_MAXIMUM])))\n        tasks.append(self.schedule_broadcast_sys_topic('clients/total', int_to_bytes_str(client_connected + client_disconnected)))\n        tasks.append(self.schedule_broadcast_sys_topic('messages/inflight', int_to_bytes_str(inflight_in + inflight_out)))\n        tasks.append(self.schedule_broadcast_sys_topic('messages/inflight/in', int_to_bytes_str(inflight_in)))\n        tasks.append(self.schedule_broadcast_sys_topic('messages/inflight/out', int_to_bytes_str(inflight_out)))\n        tasks.append(self.schedule_broadcast_sys_topic('messages/inflight/stored', int_to_bytes_str(messages_stored)))\n        tasks.append(self.schedule_broadcast_sys_topic('messages/publish/received', int_to_bytes_str(self._stats[STAT_PUBLISH_RECEIVED])))\n        tasks.append(self.schedule_broadcast_sys_topic('messages/publish/sent', int_to_bytes_str(self._stats[STAT_PUBLISH_SENT])))\n        tasks.append(self.schedule_broadcast_sys_topic('messages/retained/count', int_to_bytes_str(len(self.context.retained_messages))))\n        tasks.append(self.schedule_broadcast_sys_topic('messages/subscriptions/count', int_to_bytes_str(subscriptions_count)))\n\n        # Wait until broadcasting tasks end\n        while tasks and tasks[0].done():\n            tasks.popleft()\n        # Reschedule\n        sys_interval = int(self.context.config['sys_interval'])\n        self.context.logger.debug(\"Broadcasting $SYS topics\")\n        self.sys_handle = self.context.loop.call_later(sys_interval, self.broadcast_dollar_sys_topics)", "response": "Broadcast dynamic $SYS topics updates and reschedule next execution depending on sys_interval config."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef gen_client_id():\n    import random\n    gen_id = 'hbmqtt/'\n\n    for i in range(7, 23):\n        gen_id += chr(random.randint(0, 74) + 48)\n    return gen_id", "response": "Generates random client ID"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_stream(cls, reader: ReaderAdapter):\n        @asyncio.coroutine\n        def decode_remaining_length():\n            \"\"\"\n            Decode message length according to MQTT specifications\n            :return:\n            \"\"\"\n            multiplier = 1\n            value = 0\n            buffer = bytearray()\n            while True:\n                encoded_byte = yield from reader.read(1)\n                int_byte = unpack('!B', encoded_byte)\n                buffer.append(int_byte[0])\n                value += (int_byte[0] & 0x7f) * multiplier\n                if (int_byte[0] & 0x80) == 0:\n                    break\n                else:\n                    multiplier *= 128\n                    if multiplier > 128 * 128 * 128:\n                        raise MQTTException(\"Invalid remaining length bytes:%s, packet_type=%d\" % (bytes_to_hex_str(buffer), msg_type))\n            return value\n\n        try:\n            byte1 = yield from read_or_raise(reader, 1)\n            int1 = unpack('!B', byte1)\n            msg_type = (int1[0] & 0xf0) >> 4\n            flags = int1[0] & 0x0f\n            remain_length = yield from decode_remaining_length()\n\n            return cls(msg_type, flags, remain_length)\n        except NoDataException:\n            return None", "response": "Read and decode a fixed header from a stream."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _feed_buffer(self, n=1):\n        buffer = bytearray(self._stream.read())\n        while len(buffer) < n:\n            try:\n                message = yield from self._protocol.recv()\n            except ConnectionClosed:\n                message = None\n            if message is None:\n                break\n            if not isinstance(message, bytes):\n                raise TypeError(\"message must be bytes\")\n            buffer.extend(message)\n        self._stream = io.BytesIO(buffer)", "response": "Feed the data buffer by reading a Websocket message."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef drain(self):\n        data = self._stream.getvalue()\n        if len(data):\n            yield from self._protocol.send(data)\n        self._stream = io.BytesIO(b'')", "response": "Drain the write buffer of the underlying transport."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _retry_deliveries(self):\n        self.logger.debug(\"Begin messages delivery retries\")\n        tasks = []\n        for message in itertools.chain(self.session.inflight_in.values(), self.session.inflight_out.values()):\n            tasks.append(asyncio.wait_for(self._handle_message_flow(message), 10, loop=self._loop))\n        if tasks:\n            done, pending = yield from asyncio.wait(tasks, loop=self._loop)\n            self.logger.debug(\"%d messages redelivered\" % len(done))\n            self.logger.debug(\"%d messages not redelivered due to timeout\" % len(pending))\n        self.logger.debug(\"End messages delivery retries\")", "response": "Handle messages that are not in flight and send them to the broker."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef mqtt_publish(self, topic, data, qos, retain, ack_timeout=None):\n        if qos in (QOS_1, QOS_2):\n            packet_id = self.session.next_packet_id\n            if packet_id in self.session.inflight_out:\n                raise HBMQTTException(\"A message with the same packet ID '%d' is already in flight\" % packet_id)\n        else:\n            packet_id = None\n\n        message = OutgoingApplicationMessage(packet_id, topic, qos, data, retain)\n        # Handle message flow\n        if ack_timeout is not None and ack_timeout > 0:\n            yield from asyncio.wait_for(self._handle_message_flow(message), ack_timeout, loop=self._loop)\n        else:\n            yield from self._handle_message_flow(message)\n\n        return message", "response": "Send a MQTT publish message and manages messages flows."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _handle_message_flow(self, app_message):\n        if app_message.qos == QOS_0:\n            yield from self._handle_qos0_message_flow(app_message)\n        elif app_message.qos == QOS_1:\n            yield from self._handle_qos1_message_flow(app_message)\n        elif app_message.qos == QOS_2:\n            yield from self._handle_qos2_message_flow(app_message)\n        else:\n            raise HBMQTTException(\"Unexcepted QOS value '%d\" % str(app_message.qos))", "response": "Handle protocol flow for incoming and outgoing messages."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nhandles a QOS 0 application message.", "response": "def _handle_qos0_message_flow(self, app_message):\n        \"\"\"\n        Handle QOS_0 application message acknowledgment\n        For incoming messages, this method stores the message\n        For outgoing messages, this methods sends PUBLISH\n        :param app_message:\n        :return:\n        \"\"\"\n        assert app_message.qos == QOS_0\n        if app_message.direction == OUTGOING:\n            packet = app_message.build_publish_packet()\n            # Send PUBLISH packet\n            yield from self._send_packet(packet)\n            app_message.publish_packet = packet\n        elif app_message.direction == INCOMING:\n            if app_message.publish_packet.dup_flag:\n                self.logger.warning(\"[MQTT-3.3.1-2] DUP flag must set to 0 for QOS 0 message. Message ignored: %s\" %\n                                    repr(app_message.publish_packet))\n            else:\n                try:\n                    self.session.delivered_message_queue.put_nowait(app_message)\n                except:\n                    self.logger.warning(\"delivered messages queue full. QOS_0 message discarded\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nhandles a QOS_1 application message flow.", "response": "def _handle_qos1_message_flow(self, app_message):\n        \"\"\"\n        Handle QOS_1 application message acknowledgment\n        For incoming messages, this method stores the message and reply with PUBACK\n        For outgoing messages, this methods sends PUBLISH and waits for the corresponding PUBACK\n        :param app_message:\n        :return:\n        \"\"\"\n        assert app_message.qos == QOS_1\n        if app_message.puback_packet:\n            raise HBMQTTException(\"Message '%d' has already been acknowledged\" % app_message.packet_id)\n        if app_message.direction == OUTGOING:\n            if app_message.packet_id not in self.session.inflight_out:\n                # Store message in session\n                self.session.inflight_out[app_message.packet_id] = app_message\n            if app_message.publish_packet is not None:\n                # A Publish packet has already been sent, this is a retry\n                publish_packet = app_message.build_publish_packet(dup=True)\n            else:\n                publish_packet = app_message.build_publish_packet()\n            # Send PUBLISH packet\n            yield from self._send_packet(publish_packet)\n            app_message.publish_packet = publish_packet\n\n            # Wait for puback\n            waiter = asyncio.Future(loop=self._loop)\n            self._puback_waiters[app_message.packet_id] = waiter\n            yield from waiter\n            del self._puback_waiters[app_message.packet_id]\n            app_message.puback_packet = waiter.result()\n\n            # Discard inflight message\n            del self.session.inflight_out[app_message.packet_id]\n        elif app_message.direction == INCOMING:\n            # Initiate delivery\n            self.logger.debug(\"Add message to delivery\")\n            yield from self.session.delivered_message_queue.put(app_message)\n            # Send PUBACK\n            puback = PubackPacket.build(app_message.packet_id)\n            yield from self._send_packet(puback)\n            app_message.puback_packet = puback"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nhandles a QOS 2 application message flow.", "response": "def _handle_qos2_message_flow(self, app_message):\n        \"\"\"\n        Handle QOS_2 application message acknowledgment\n        For incoming messages, this method stores the message, sends PUBREC, waits for PUBREL, initiate delivery\n        and send PUBCOMP\n        For outgoing messages, this methods sends PUBLISH, waits for PUBREC, discards messages and wait for PUBCOMP\n        :param app_message:\n        :return:\n        \"\"\"\n        assert app_message.qos == QOS_2\n        if app_message.direction == OUTGOING:\n            if app_message.pubrel_packet and app_message.pubcomp_packet:\n                raise HBMQTTException(\"Message '%d' has already been acknowledged\" % app_message.packet_id)\n            if not app_message.pubrel_packet:\n                # Store message\n                if app_message.publish_packet is not None:\n                    # This is a retry flow, no need to store just check the message exists in session\n                    if app_message.packet_id not in self.session.inflight_out:\n                        raise HBMQTTException(\"Unknown inflight message '%d' in session\" % app_message.packet_id)\n                    publish_packet = app_message.build_publish_packet(dup=True)\n                else:\n                    # Store message in session\n                    self.session.inflight_out[app_message.packet_id] = app_message\n                    publish_packet = app_message.build_publish_packet()\n                # Send PUBLISH packet\n                yield from self._send_packet(publish_packet)\n                app_message.publish_packet = publish_packet\n                # Wait PUBREC\n                if app_message.packet_id in self._pubrec_waiters:\n                    # PUBREC waiter already exists for this packet ID\n                    message = \"Can't add PUBREC waiter, a waiter already exists for message Id '%s'\" \\\n                              % app_message.packet_id\n                    self.logger.warning(message)\n                    raise HBMQTTException(message)\n                waiter = asyncio.Future(loop=self._loop)\n                self._pubrec_waiters[app_message.packet_id] = waiter\n                yield from waiter\n                del self._pubrec_waiters[app_message.packet_id]\n                app_message.pubrec_packet = waiter.result()\n            if not app_message.pubcomp_packet:\n                # Send pubrel\n                app_message.pubrel_packet = PubrelPacket.build(app_message.packet_id)\n                yield from self._send_packet(app_message.pubrel_packet)\n                # Wait for PUBCOMP\n                waiter = asyncio.Future(loop=self._loop)\n                self._pubcomp_waiters[app_message.packet_id] = waiter\n                yield from waiter\n                del self._pubcomp_waiters[app_message.packet_id]\n                app_message.pubcomp_packet = waiter.result()\n            # Discard inflight message\n            del self.session.inflight_out[app_message.packet_id]\n        elif app_message.direction == INCOMING:\n            self.session.inflight_in[app_message.packet_id] = app_message\n            # Send pubrec\n            pubrec_packet = PubrecPacket.build(app_message.packet_id)\n            yield from self._send_packet(pubrec_packet)\n            app_message.pubrec_packet = pubrec_packet\n            # Wait PUBREL\n            if app_message.packet_id in self._pubrel_waiters and not self._pubrel_waiters[app_message.packet_id].done():\n                # PUBREL waiter already exists for this packet ID\n                message = \"A waiter already exists for message Id '%s', canceling it\" \\\n                          % app_message.packet_id\n                self.logger.warning(message)\n                self._pubrel_waiters[app_message.packet_id].cancel()\n            try:\n                waiter = asyncio.Future(loop=self._loop)\n                self._pubrel_waiters[app_message.packet_id] = waiter\n                yield from waiter\n                del self._pubrel_waiters[app_message.packet_id]\n                app_message.pubrel_packet = waiter.result()\n                # Initiate delivery and discard message\n                yield from self.session.delivered_message_queue.put(app_message)\n                del self.session.inflight_in[app_message.packet_id]\n                # Send pubcomp\n                pubcomp_packet = PubcompPacket.build(app_message.packet_id)\n                yield from self._send_packet(pubcomp_packet)\n                app_message.pubcomp_packet = pubcomp_packet\n            except asyncio.CancelledError:\n                self.logger.debug(\"Message flow cancelled\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the language of the API being requested.", "response": "def set_lang(prefix):\n  '''\n  Change the language of the API being requested.\n  Set `prefix` to one of the two letter prefixes found on the `list of all Wikipedias <http://meta.wikimedia.org/wiki/List_of_Wikipedias>`_.\n\n  After setting the language, the cache for ``search``, ``suggest``, and ``summary`` will be cleared.\n\n  .. note:: Make sure you search for page titles in the language that you have set.\n  '''\n  global API_URL\n  API_URL = 'http://' + prefix.lower() + '.wikipedia.org/w/api.php'\n\n  for cached_func in (search, suggest, summary):\n    cached_func.clear_cache()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_rate_limiting(rate_limit, min_wait=timedelta(milliseconds=50)):\n  '''\n  Enable or disable rate limiting on requests to the Mediawiki servers.\n  If rate limiting is not enabled, under some circumstances (depending on\n  load on Wikipedia, the number of requests you and other `wikipedia` users\n  are making, and other factors), Wikipedia may return an HTTP timeout error.\n\n  Enabling rate limiting generally prevents that issue, but please note that\n  HTTPTimeoutError still might be raised.\n\n  Arguments:\n\n  * rate_limit - (Boolean) whether to enable rate limiting or not\n\n  Keyword arguments:\n\n  * min_wait - if rate limiting is enabled, `min_wait` is a timedelta describing the minimum time to wait before requests.\n         Defaults to timedelta(milliseconds=50)\n  '''\n  global RATE_LIMIT\n  global RATE_LIMIT_MIN_WAIT\n  global RATE_LIMIT_LAST_CALL\n\n  RATE_LIMIT = rate_limit\n  if not rate_limit:\n    RATE_LIMIT_MIN_WAIT = None\n  else:\n    RATE_LIMIT_MIN_WAIT = min_wait\n\n  RATE_LIMIT_LAST_CALL = None", "response": "Enables or disables rate limiting on requests to Mediawiki servers."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef search(query, results=10, suggestion=False):\n  '''\n  Do a Wikipedia search for `query`.\n\n  Keyword arguments:\n\n  * results - the maxmimum number of results returned\n  * suggestion - if True, return results and suggestion (if any) in a tuple\n  '''\n\n  search_params = {\n    'list': 'search',\n    'srprop': '',\n    'srlimit': results,\n    'limit': results,\n    'srsearch': query\n  }\n  if suggestion:\n    search_params['srinfo'] = 'suggestion'\n\n  raw_results = _wiki_request(search_params)\n\n  if 'error' in raw_results:\n    if raw_results['error']['info'] in ('HTTP request timed out.', 'Pool queue is full'):\n      raise HTTPTimeoutError(query)\n    else:\n      raise WikipediaException(raw_results['error']['info'])\n\n  search_results = (d['title'] for d in raw_results['query']['search'])\n\n  if suggestion:\n    if raw_results['query'].get('searchinfo'):\n      return list(search_results), raw_results['query']['searchinfo']['suggestion']\n    else:\n      return list(search_results), None\n\n  return list(search_results)", "response": "Do a Wikipedia search for query."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef geosearch(latitude, longitude, title=None, results=10, radius=1000):\n  '''\n  Do a wikipedia geo search for `latitude` and `longitude`\n  using HTTP API described in http://www.mediawiki.org/wiki/Extension:GeoData\n\n  Arguments:\n\n  * latitude (float or decimal.Decimal)\n  * longitude (float or decimal.Decimal)\n\n  Keyword arguments:\n\n  * title - The title of an article to search for\n  * results - the maximum number of results returned\n  * radius - Search radius in meters. The value must be between 10 and 10000\n  '''\n\n  search_params = {\n    'list': 'geosearch',\n    'gsradius': radius,\n    'gscoord': '{0}|{1}'.format(latitude, longitude),\n    'gslimit': results\n  }\n  if title:\n    search_params['titles'] = title\n\n  raw_results = _wiki_request(search_params)\n\n  if 'error' in raw_results:\n    if raw_results['error']['info'] in ('HTTP request timed out.', 'Pool queue is full'):\n      raise HTTPTimeoutError('{0}|{1}'.format(latitude, longitude))\n    else:\n      raise WikipediaException(raw_results['error']['info'])\n\n  search_pages = raw_results['query'].get('pages', None)\n  if search_pages:\n    search_results = (v['title'] for k, v in search_pages.items() if k != '-1')\n  else:\n    search_results = (d['title'] for d in raw_results['query']['geosearch'])\n\n  return list(search_results)", "response": "Do a wikipedia geo search for latitude and longitude."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef suggest(query):\n  '''\n  Get a Wikipedia search suggestion for `query`.\n  Returns a string or None if no suggestion was found.\n  '''\n\n  search_params = {\n    'list': 'search',\n    'srinfo': 'suggestion',\n    'srprop': '',\n  }\n  search_params['srsearch'] = query\n\n  raw_result = _wiki_request(search_params)\n\n  if raw_result['query'].get('searchinfo'):\n    return raw_result['query']['searchinfo']['suggestion']\n\n  return None", "response": "Get a Wikipedia search suggestion for query.\n Returns a string or None."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets a list of random Wikipedia article titles.", "response": "def random(pages=1):\n  '''\n  Get a list of random Wikipedia article titles.\n\n  .. note:: Random only gets articles from namespace 0, meaning no Category, User talk, or other meta-Wikipedia pages.\n\n  Keyword arguments:\n\n  * pages - the number of random pages returned (max of 10)\n  '''\n  #http://en.wikipedia.org/w/api.php?action=query&list=random&rnlimit=5000&format=jsonfm\n  query_params = {\n    'list': 'random',\n    'rnnamespace': 0,\n    'rnlimit': pages,\n  }\n\n  request = _wiki_request(query_params)\n  titles = [page['title'] for page in request['query']['random']]\n\n  if len(titles) == 1:\n    return titles[0]\n\n  return titles"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the plain text summary of the page.", "response": "def summary(title, sentences=0, chars=0, auto_suggest=True, redirect=True):\n  '''\n  Plain text summary of the page.\n\n  .. note:: This is a convenience wrapper - auto_suggest and redirect are enabled by default\n\n  Keyword arguments:\n\n  * sentences - if set, return the first `sentences` sentences (can be no greater than 10).\n  * chars - if set, return only the first `chars` characters (actual text returned may be slightly longer).\n  * auto_suggest - let Wikipedia find a valid page title for the query\n  * redirect - allow redirection without raising RedirectError\n  '''\n\n  # use auto_suggest and redirect to get the correct article\n  # also, use page's error checking to raise DisambiguationError if necessary\n  page_info = page(title, auto_suggest=auto_suggest, redirect=redirect)\n  title = page_info.title\n  pageid = page_info.pageid\n\n  query_params = {\n    'prop': 'extracts',\n    'explaintext': '',\n    'titles': title\n  }\n\n  if sentences:\n    query_params['exsentences'] = sentences\n  elif chars:\n    query_params['exchars'] = chars\n  else:\n    query_params['exintro'] = ''\n\n  request = _wiki_request(query_params)\n  summary = request['query']['pages'][pageid]['extract']\n\n  return summary"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef page(title=None, pageid=None, auto_suggest=True, redirect=True, preload=False):\n  '''\n  Get a WikipediaPage object for the page with title `title` or the pageid\n  `pageid` (mutually exclusive).\n\n  Keyword arguments:\n\n  * title - the title of the page to load\n  * pageid - the numeric pageid of the page to load\n  * auto_suggest - let Wikipedia find a valid page title for the query\n  * redirect - allow redirection without raising RedirectError\n  * preload - load content, summary, images, references, and links during initialization\n  '''\n\n  if title is not None:\n    if auto_suggest:\n      results, suggestion = search(title, results=1, suggestion=True)\n      try:\n        title = suggestion or results[0]\n      except IndexError:\n        # if there is no suggestion or search results, the page doesn't exist\n        raise PageError(title)\n    return WikipediaPage(title, redirect=redirect, preload=preload)\n  elif pageid is not None:\n    return WikipediaPage(pageid=pageid, preload=preload)\n  else:\n    raise ValueError(\"Either a title or a pageid must be specified\")", "response": "Returns a WikipediaPage object for the page with title or pageid."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _wiki_request(params):\n  '''\n  Make a request to the Wikipedia API using the given search parameters.\n  Returns a parsed dict of the JSON response.\n  '''\n  global RATE_LIMIT_LAST_CALL\n  global USER_AGENT\n\n  params['format'] = 'json'\n  if not 'action' in params:\n    params['action'] = 'query'\n\n  headers = {\n    'User-Agent': USER_AGENT\n  }\n\n  if RATE_LIMIT and RATE_LIMIT_LAST_CALL and \\\n    RATE_LIMIT_LAST_CALL + RATE_LIMIT_MIN_WAIT > datetime.now():\n\n    # it hasn't been long enough since the last API call\n    # so wait until we're in the clear to make the request\n\n    wait_time = (RATE_LIMIT_LAST_CALL + RATE_LIMIT_MIN_WAIT) - datetime.now()\n    time.sleep(int(wait_time.total_seconds()))\n\n  r = requests.get(API_URL, params=params, headers=headers)\n\n  if RATE_LIMIT:\n    RATE_LIMIT_LAST_CALL = datetime.now()\n\n  return r.json()", "response": "Make a request to the Wikipedia API using the given search parameters."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading basic information from Wikipedia.", "response": "def __load(self, redirect=True, preload=False):\n    '''\n    Load basic information from Wikipedia.\n    Confirm that page exists and is not a disambiguation/redirect.\n\n    Does not need to be called manually, should be called automatically during __init__.\n    '''\n    query_params = {\n      'prop': 'info|pageprops',\n      'inprop': 'url',\n      'ppprop': 'disambiguation',\n      'redirects': '',\n    }\n    if not getattr(self, 'pageid', None):\n      query_params['titles'] = self.title\n    else:\n      query_params['pageids'] = self.pageid\n\n    request = _wiki_request(query_params)\n\n    query = request['query']\n    pageid = list(query['pages'].keys())[0]\n    page = query['pages'][pageid]\n\n    # missing is present if the page is missing\n    if 'missing' in page:\n      if hasattr(self, 'title'):\n        raise PageError(self.title)\n      else:\n        raise PageError(pageid=self.pageid)\n\n    # same thing for redirect, except it shows up in query instead of page for\n    # whatever silly reason\n    elif 'redirects' in query:\n      if redirect:\n        redirects = query['redirects'][0]\n\n        if 'normalized' in query:\n          normalized = query['normalized'][0]\n          assert normalized['from'] == self.title, ODD_ERROR_MESSAGE\n\n          from_title = normalized['to']\n\n        else:\n          from_title = self.title\n\n        assert redirects['from'] == from_title, ODD_ERROR_MESSAGE\n\n        # change the title and reload the whole object\n        self.__init__(redirects['to'], redirect=redirect, preload=preload)\n\n      else:\n        raise RedirectError(getattr(self, 'title', page['title']))\n\n    # since we only asked for disambiguation in ppprop,\n    # if a pageprop is returned,\n    # then the page must be a disambiguation page\n    elif 'pageprops' in page:\n      query_params = {\n        'prop': 'revisions',\n        'rvprop': 'content',\n        'rvparse': '',\n        'rvlimit': 1\n      }\n      if hasattr(self, 'pageid'):\n        query_params['pageids'] = self.pageid\n      else:\n        query_params['titles'] = self.title\n      request = _wiki_request(query_params)\n      html = request['query']['pages'][pageid]['revisions'][0]['*']\n\n      lis = BeautifulSoup(html, 'html.parser').find_all('li')\n      filtered_lis = [li for li in lis if not 'tocsection' in ''.join(li.get('class', []))]\n      may_refer_to = [li.a.get_text() for li in filtered_lis if li.a]\n\n      raise DisambiguationError(getattr(self, 'title', page['title']), may_refer_to)\n\n    else:\n      self.pageid = pageid\n      self.title = page['title']\n      self.url = page['fullurl']"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef __continued_query(self, query_params):\n    '''\n    Based on https://www.mediawiki.org/wiki/API:Query#Continuing_queries\n    '''\n    query_params.update(self.__title_query_param)\n\n    last_continue = {}\n    prop = query_params.get('prop', None)\n\n    while True:\n      params = query_params.copy()\n      params.update(last_continue)\n\n      request = _wiki_request(params)\n\n      if 'query' not in request:\n        break\n\n      pages = request['query']['pages']\n      if 'generator' in query_params:\n        for datum in pages.values():  # in python 3.3+: \"yield from pages.values()\"\n          yield datum\n      else:\n        for datum in pages[self.pageid][prop]:\n          yield datum\n\n      if 'continue' not in request:\n        break\n\n      last_continue = request['continue']", "response": "Yields the pages that have been executed in the next query."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef html(self):\n    '''\n    Get full page HTML.\n\n    .. warning:: This can get pretty slow on long pages.\n    '''\n\n    if not getattr(self, '_html', False):\n      query_params = {\n        'prop': 'revisions',\n        'rvprop': 'content',\n        'rvlimit': 1,\n        'rvparse': '',\n        'titles': self.title\n      }\n\n      request = _wiki_request(query_params)\n      self._html = request['query']['pages'][self.pageid]['revisions'][0]['*']\n\n    return self._html", "response": "Get full page HTML."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef summary(self):\n    '''\n    Plain text summary of the page.\n    '''\n\n    if not getattr(self, '_summary', False):\n      query_params = {\n        'prop': 'extracts',\n        'explaintext': '',\n        'exintro': '',\n      }\n      if not getattr(self, 'title', None) is None:\n         query_params['titles'] = self.title\n      else:\n         query_params['pageids'] = self.pageid\n\n      request = _wiki_request(query_params)\n      self._summary = request['query']['pages'][self.pageid]['extract']\n\n    return self._summary", "response": "Plain text summary of the page."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of URLs of images on the page.", "response": "def images(self):\n    '''\n    List of URLs of images on the page.\n    '''\n\n    if not getattr(self, '_images', False):\n      self._images = [\n        page['imageinfo'][0]['url']\n        for page in self.__continued_query({\n          'generator': 'images',\n          'gimlimit': 'max',\n          'prop': 'imageinfo',\n          'iiprop': 'url',\n        })\n        if 'imageinfo' in page\n      ]\n\n    return self._images"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef coordinates(self):\n    '''\n    Tuple of Decimals in the form of (lat, lon) or None\n    '''\n    if not getattr(self, '_coordinates', False):\n      query_params = {\n        'prop': 'coordinates',\n        'colimit': 'max',\n        'titles': self.title,\n      }\n\n      request = _wiki_request(query_params)\n\n      if 'query' in request:\n        coordinates = request['query']['pages'][self.pageid]['coordinates']\n        self._coordinates = (Decimal(coordinates[0]['lat']), Decimal(coordinates[0]['lon']))\n      else:\n        self._coordinates = None\n\n    return self._coordinates", "response": "Tuple of Decimals in the form of lat lon or None"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of URLs of external links on a page.", "response": "def references(self):\n    '''\n    List of URLs of external links on a page.\n    May include external links within page that aren't technically cited anywhere.\n    '''\n\n    if not getattr(self, '_references', False):\n      def add_protocol(url):\n        return url if url.startswith('http') else 'http:' + url\n\n      self._references = [\n        add_protocol(link['*'])\n        for link in self.__continued_query({\n          'prop': 'extlinks',\n          'ellimit': 'max'\n        })\n      ]\n\n    return self._references"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef links(self):\n    '''\n    List of titles of Wikipedia page links on a page.\n\n    .. note:: Only includes articles from namespace 0, meaning no Category, User talk, or other meta-Wikipedia pages.\n    '''\n\n    if not getattr(self, '_links', False):\n      self._links = [\n        link['title']\n        for link in self.__continued_query({\n          'prop': 'links',\n          'plnamespace': 0,\n          'pllimit': 'max'\n        })\n      ]\n\n    return self._links", "response": "Returns a list of titles of Wikipedia page links on a page."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of categories of a page.", "response": "def categories(self):\n    '''\n    List of categories of a page.\n    '''\n\n    if not getattr(self, '_categories', False):\n      self._categories = [re.sub(r'^Category:', '', x) for x in\n        [link['title']\n        for link in self.__continued_query({\n          'prop': 'categories',\n          'cllimit': 'max'\n        })\n      ]]\n\n    return self._categories"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sections(self):\n    '''\n    List of section titles from the table of contents on the page.\n    '''\n\n    if not getattr(self, '_sections', False):\n      query_params = {\n        'action': 'parse',\n        'prop': 'sections',\n      }\n      query_params.update(self.__title_query_param)\n\n      request = _wiki_request(query_params)\n      self._sections = [section['line'] for section in request['parse']['sections']]\n\n    return self._sections", "response": "List of section titles from the table of contents on the page."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the plain text content of a section from self. sections.", "response": "def section(self, section_title):\n    '''\n    Get the plain text content of a section from `self.sections`.\n    Returns None if `section_title` isn't found, otherwise returns a whitespace stripped string.\n\n    This is a convenience method that wraps self.content.\n\n    .. warning:: Calling `section` on a section that has subheadings will NOT return\n           the full text of all of the subsections. It only gets the text between\n           `section_title` and the next subheading, which is often empty.\n    '''\n\n    section = u\"== {} ==\".format(section_title)\n    try:\n      index = self.content.index(section) + len(section)\n    except ValueError:\n      return None\n\n    try:\n      next_index = self.content.index(\"==\", index)\n    except ValueError:\n      next_index = len(self.content)\n\n    return self.content[index:next_index].lstrip(\"=\").strip()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update(self, render, force = False):\n        if not force and not self._needUpdate:\n            return\n        self._needUpdate = False\n        \n        i = 0\n        drawArea = QtGui.QImage(self._width, self._height, render.getImageFormat())\n        #fill with background Color\n        drawArea.fill(self._backgroudColor)\n        with QtGui.QPainter(drawArea) as qp:\n            for label in self._labels:\n                rect = QtCore.QRect(0, i * self._cellHeight, self._width - 2, self._cellHeight)\n                if i == self._current:\n                    qp.setPen(QtCore.Qt.darkGreen)\n                    qp.drawRoundedRect(rect, 5.0, 5.0)\n                qp.setPen(QtCore.Qt.white)  \n                qp.setFont(QtGui.QFont('arial', self._fontSize, QtGui.QFont.Bold))\n                qp.drawText(rect, QtCore.Qt.AlignCenter, label)\n                i += 1\n        render.drawImage(drawArea)", "response": "Draw GUI that list active session"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update(self, render, force = False):\n        if not force:\n            return;\n        drawArea = QtGui.QImage(self._width, self._height, render.getImageFormat())\n        drawArea.fill(self._backgroundColor)\n        with QtGui.QPainter(drawArea) as qp:\n            qp.setFont(self._font)\n            qp.setPen(self._fontColor) \n            qp.drawText(drawArea.rect(), QtCore.Qt.AlignCenter, self._label)\n        render.drawImage(drawArea)", "response": "Update the view of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef drawImage(self, image):\n        padding = image.width() % 4\n        for i in range(0, image.height()):\n            tmp = image.copy(0, i, image.width() + padding, 1)\n            #in RDP image or bottom top encoded\n            ptr = tmp.bits()\n            ptr.setsize(tmp.byteCount())\n            self._controller.sendUpdate(self._dx, i + self._dy, image.width() + self._dx - 1, i + self._dy, tmp.width(), tmp.height(), self._colorDepth, False, ptr.asstring())", "response": "Render the image of the current widget."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads header and wait header value to call next state", "response": "def expectedBody(self, data):\n        \"\"\"\n        Read header and wait header value to call next state\n        @param data: Stream that length are to header length (1|2|4 bytes)\n        set next state to callBack body when length read from header\n        are received\n        \"\"\"\n        bodyLen = None\n        if data.len == 1:\n            bodyLen = UInt8()\n        elif data.len == 2:\n            bodyLen = UInt16Be()\n        elif data.len == 4:\n            bodyLen = UInt32Be()\n        else:\n            log.error(\"invalid header length\")\n            return\n        data.readType(bodyLen)\n        self.expect(bodyLen.value, self._callbackBody)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef readProtocolVersion(self, data):\n        data.readType(self._version)\n        if not self._version.value in [ProtocolVersion.RFB003003, ProtocolVersion.RFB003007, ProtocolVersion.RFB003008]:\n            self._version.value = ProtocolVersion.UNKNOWN", "response": "Read the protocol version from the data stream."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads protocol version from client.", "response": "def recvProtocolVersion(self, data):\n        \"\"\"\n        Read handshake packet \n        If protocol receive from client is unknown\n        try best version of protocol version (ProtocolVersion.RFB003008)\n        @param data: Stream\n        \"\"\"\n        self.readProtocolVersion(data)\n        if self._version.value == ProtocolVersion.UNKNOWN:\n            log.info(\"Unknown protocol version %s send 003.008\"%data.getvalue())\n            #protocol version is unknown try best version we can handle\n            self._version.value = ProtocolVersion.RFB003008\n        #send same version of \n        self.send(self._version)\n        \n        #next state read security\n        if self._version.value == ProtocolVersion.RFB003003:\n            self.expect(4, self.recvSecurityServer)\n        else:\n            self.expectWithHeader(1, self.recvSecurityList)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading security list packet send to server to client to check if security level is high or low", "response": "def recvSecurityList(self, data):\n        \"\"\"\n        Read security list packet send from server to client\n        @param data: Stream that contains well formed packet\n        \"\"\"\n        securityList = []\n        while data.dataLen() > 0:\n            securityElement = UInt8()\n            data.readType(securityElement)\n            securityList.append(securityElement)\n        #select high security level\n        for s in securityList:\n            if s.value in [SecurityType.NONE, SecurityType.VNC] and s > self._securityLevel:\n                self._securityLevel = s\n                break\n        #send back security level choosen\n        self.send(self._securityLevel)\n        if self._securityLevel.value == SecurityType.VNC:\n            self.expect(16, self.recvVNCChallenge)\n        else:\n            self.expect(4, self.recvSecurityResult)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads security result packet from server", "response": "def recvSecurityResult(self, data):\n        \"\"\"\n        Read security result packet\n        Use by server to inform connection status of client\n        @param data: Stream that contain well formed packet \n        \"\"\"\n        result = UInt32Be()\n        data.readType(result)\n        if result == UInt32Be(1):\n            log.info(\"Authentification failed\")\n            if self._version.value == ProtocolVersion.RFB003008:\n                self.expectWithHeader(4, self.recvSecurityFailed)\n        else:\n            log.debug(\"Authentification OK\")\n            self.sendClientInit()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread server init packet", "response": "def recvServerInit(self, data):\n        \"\"\"\n        Read server init packet\n        @param data: Stream that contains well formed packet\n        \"\"\"\n        data.readType(self._serverInit)\n        self.expectWithHeader(4, self.recvServerName)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert a datetime to Microsoft filetime format.", "response": "def dt_to_filetime(dt):\n    \"\"\"Converts a datetime to Microsoft filetime format. If the object is\n    time zone-naive, it is forced to UTC before conversion.\n\n    >>> \"%.0f\" % dt_to_filetime(datetime(2009, 7, 25, 23, 0))\n    '128930364000000000'\n\n    >>> \"%.0f\" % dt_to_filetime(datetime(1970, 1, 1, 0, 0, tzinfo=utc))\n    '116444736000000000'\n\n    >>> \"%.0f\" % dt_to_filetime(datetime(1970, 1, 1, 0, 0))\n    '116444736000000000'\n    \n    >>> dt_to_filetime(datetime(2009, 7, 25, 23, 0, 0, 100))\n    128930364000001000\n    \"\"\"\n    if (dt.tzinfo is None) or (dt.tzinfo.utcoffset(dt) is None):\n        dt = dt.replace(tzinfo=utc)\n    ft = EPOCH_AS_FILETIME + (timegm(dt.timetuple()) * HUNDREDS_OF_NANOSECONDS)\n    return ft + (dt.microsecond * 10)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a Microsoft filetime number to a Python datetime object.", "response": "def filetime_to_dt(ft):\n    \"\"\"Converts a Microsoft filetime number to a Python datetime. The new\n    datetime object is time zone-naive but is equivalent to tzinfo=utc.\n\n    >>> filetime_to_dt(116444736000000000)\n    datetime.datetime(1970, 1, 1, 0, 0)\n\n    >>> filetime_to_dt(128930364000000000)\n    datetime.datetime(2009, 7, 25, 23, 0)\n    \n    >>> filetime_to_dt(128930364000001000)\n    datetime.datetime(2009, 7, 25, 23, 0, 0, 100)\n    \"\"\"\n    # Get seconds and remainder in terms of Unix epoch\n    (s, ns100) = divmod(ft - EPOCH_AS_FILETIME, HUNDREDS_OF_NANOSECONDS)\n    # Convert to datetime object\n    dt = datetime.utcfromtimestamp(s)\n    # Add remainder in as microseconds. Python 3.2 requires an integer\n    dt = dt.replace(microsecond=(ns100 // 10))\n    return dt"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if some arguments are incompatible with each other.", "response": "def check_arg_compatibility(args: argparse.Namespace):\n    \"\"\"\n    Check if some arguments are incompatible with each other.\n\n    :param args: Arguments as returned by argparse.\n    \"\"\"\n\n    if args.lhuc is not None:\n        # Actually this check is a bit too strict\n        check_condition(args.encoder != C.CONVOLUTION_TYPE or args.decoder != C.CONVOLUTION_TYPE,\n                        \"LHUC is not supported for convolutional models yet.\")\n        check_condition(args.decoder != C.TRANSFORMER_TYPE or C.LHUC_STATE_INIT not in args.lhuc,\n                        \"The %s options only applies to RNN models\" % C.LHUC_STATE_INIT)\n\n    if args.decoder_only:\n        check_condition(args.decoder != C.TRANSFORMER_TYPE and args.decoder != C.CONVOLUTION_TYPE,\n                        \"Decoder pre-training currently supports RNN decoders only.\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_resume(args: argparse.Namespace, output_folder: str) -> bool:\n    resume_training = False\n    training_state_dir = os.path.join(output_folder, C.TRAINING_STATE_DIRNAME)\n    if os.path.exists(output_folder):\n        if args.overwrite_output:\n            logger.info(\"Removing existing output folder %s.\", output_folder)\n            shutil.rmtree(output_folder)\n            os.makedirs(output_folder)\n        elif os.path.exists(training_state_dir):\n            old_args = vars(arguments.load_args(os.path.join(output_folder, C.ARGS_STATE_NAME)))\n            arg_diffs = _dict_difference(vars(args), old_args) | _dict_difference(old_args, vars(args))\n            # Remove args that may differ without affecting the training.\n            arg_diffs -= set(C.ARGS_MAY_DIFFER)\n            # allow different device-ids provided their total count is the same\n            if 'device_ids' in arg_diffs and len(old_args['device_ids']) == len(vars(args)['device_ids']):\n                arg_diffs.discard('device_ids')\n            if not arg_diffs:\n                resume_training = True\n            else:\n                # We do not have the logger yet\n                logger.error(\"Mismatch in arguments for training continuation.\")\n                logger.error(\"Differing arguments: %s.\", \", \".join(arg_diffs))\n                sys.exit(1)\n        elif os.path.exists(os.path.join(output_folder, C.PARAMS_BEST_NAME)):\n            logger.error(\"Refusing to overwrite model folder %s as it seems to contain a trained model.\", output_folder)\n            sys.exit(1)\n        else:\n            logger.info(\"The output folder %s already exists, but no training state or parameter file was found. \"\n                        \"Will start training from scratch.\", output_folder)\n    else:\n        os.makedirs(output_folder)\n\n    return resume_training", "response": "Check if we should resume a broken training run."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns True if arguments entail a shared source and target vocabulary.", "response": "def use_shared_vocab(args: argparse.Namespace) -> bool:\n    \"\"\"\n    True if arguments entail a shared source and target vocabulary.\n\n    :param: args: Arguments as returned by argparse.\n    \"\"\"\n    weight_tying = args.weight_tying\n    weight_tying_type = args.weight_tying_type\n    shared_vocab = args.shared_vocab\n    decoder_only = args.decoder_only\n    if weight_tying and C.WEIGHT_TYING_SRC in weight_tying_type and C.WEIGHT_TYING_TRG in weight_tying_type:\n        if not shared_vocab:\n            logger.info(\"A shared source/target vocabulary will be used as weight tying source/target weight tying \"\n                        \"is enabled\")\n        shared_vocab = True\n    if decoder_only:\n        if not shared_vocab:\n            logger.info(\"A shared source/target vocabulary will be used for pre-training the decoder.\")\n        shared_vocab = True\n    return shared_vocab"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate the data iterators and vocabularies.", "response": "def create_data_iters_and_vocabs(args: argparse.Namespace,\n                                 max_seq_len_source: int,\n                                 max_seq_len_target: int,\n                                 shared_vocab: bool,\n                                 resume_training: bool,\n                                 output_folder: str) -> Tuple['data_io.BaseParallelSampleIter',\n                                                              'data_io.BaseParallelSampleIter',\n                                                              'data_io.DataConfig',\n                                                              List[vocab.Vocab], vocab.Vocab]:\n    \"\"\"\n    Create the data iterators and the vocabularies.\n\n    :param args: Arguments as returned by argparse.\n    :param max_seq_len_source: Source maximum sequence length.\n    :param max_seq_len_target: Target maximum sequence length.\n    :param shared_vocab: Whether to create a shared vocabulary.\n    :param resume_training: Whether to resume training.\n    :param output_folder: Output folder.\n    :return: The data iterators (train, validation, config_data) as well as the source and target vocabularies.\n    \"\"\"\n    num_words_source, num_words_target = args.num_words\n    num_words_source = num_words_source if num_words_source > 0 else None\n    num_words_target = num_words_target if num_words_target > 0 else None\n\n    word_min_count_source, word_min_count_target = args.word_min_count\n    batch_num_devices = 1 if args.use_cpu else sum(-di if di < 0 else 1 for di in args.device_ids)\n    batch_by_words = args.batch_type == C.BATCH_TYPE_WORD\n\n    validation_sources = [args.validation_source] + args.validation_source_factors\n    validation_sources = [str(os.path.abspath(source)) for source in validation_sources]\n    validation_target = str(os.path.abspath(args.validation_target))\n\n    either_raw_or_prepared_error_msg = \"Either specify a raw training corpus with %s and %s or a preprocessed corpus \" \\\n                                       \"with %s.\" % (C.TRAINING_ARG_SOURCE,\n                                                     C.TRAINING_ARG_TARGET,\n                                                     C.TRAINING_ARG_PREPARED_DATA)\n    if args.prepared_data is not None:\n        utils.check_condition(args.source is None and args.target is None, either_raw_or_prepared_error_msg)\n        if not resume_training:\n            utils.check_condition(args.source_vocab is None and args.target_vocab is None,\n                                  \"You are using a prepared data folder, which is tied to a vocabulary. \"\n                                  \"To change it you need to rerun data preparation with a different vocabulary.\")\n        train_iter, validation_iter, data_config, source_vocabs, target_vocab = data_io.get_prepared_data_iters(\n            prepared_data_dir=args.prepared_data,\n            validation_sources=validation_sources,\n            validation_target=validation_target,\n            shared_vocab=shared_vocab,\n            batch_size=args.batch_size,\n            batch_by_words=batch_by_words,\n            batch_num_devices=batch_num_devices)\n\n        check_condition(args.source_factors_combine == C.SOURCE_FACTORS_COMBINE_SUM \\\n                        or len(source_vocabs) == len(args.source_factors_num_embed) + 1,\n                        \"Data was prepared with %d source factors, but only provided %d source factor dimensions.\" % (\n                            len(source_vocabs), len(args.source_factors_num_embed) + 1))\n\n        if resume_training:\n            # resuming training. Making sure the vocabs in the model and in the prepared data match up\n            model_source_vocabs = vocab.load_source_vocabs(output_folder)\n            for i, (v, mv) in enumerate(zip(source_vocabs, model_source_vocabs)):\n                utils.check_condition(vocab.are_identical(v, mv),\n                                      \"Prepared data and resumed model source vocab %d do not match.\" % i)\n            model_target_vocab = vocab.load_target_vocab(output_folder)\n            utils.check_condition(vocab.are_identical(target_vocab, model_target_vocab),\n                                  \"Prepared data and resumed model target vocabs do not match.\")\n\n        check_condition(data_config.num_source_factors == len(validation_sources),\n                        'Training and validation data must have the same number of factors, but found %d and %d.' % (\n                            data_config.num_source_factors, len(validation_sources)))\n\n        return train_iter, validation_iter, data_config, source_vocabs, target_vocab\n\n    else:\n        utils.check_condition(args.prepared_data is None and args.source is not None and args.target is not None,\n                              either_raw_or_prepared_error_msg)\n\n        if resume_training:\n            # Load the existing vocabs created when starting the training run.\n            source_vocabs = vocab.load_source_vocabs(output_folder)\n            target_vocab = vocab.load_target_vocab(output_folder)\n\n            # Recover the vocabulary path from the data info file:\n            data_info = cast(data_io.DataInfo, Config.load(os.path.join(output_folder, C.DATA_INFO)))\n            source_vocab_paths = data_info.source_vocabs\n            target_vocab_path = data_info.target_vocab\n\n        else:\n            # Load or create vocabs\n            source_factor_vocab_paths = [args.source_factor_vocabs[i] if i < len(args.source_factor_vocabs)\n                                         else None for i in range(len(args.source_factors))]\n            source_vocab_paths = [args.source_vocab] + source_factor_vocab_paths\n            target_vocab_path = args.target_vocab\n            source_vocabs, target_vocab = vocab.load_or_create_vocabs(\n                source_paths=[args.source] + args.source_factors,\n                target_path=args.target,\n                source_vocab_paths=source_vocab_paths,\n                target_vocab_path=target_vocab_path,\n                shared_vocab=shared_vocab,\n                num_words_source=num_words_source,\n                num_words_target=num_words_target,\n                word_min_count_source=word_min_count_source,\n                word_min_count_target=word_min_count_target,\n                pad_to_multiple_of=args.pad_vocab_to_multiple_of)\n\n        check_condition(args.source_factors_combine == C.SOURCE_FACTORS_COMBINE_SUM \\\n                        or len(args.source_factors) == len(args.source_factors_num_embed),\n                        \"Number of source factor data (%d) differs from provided source factor dimensions (%d)\" % (\n                            len(args.source_factors), len(args.source_factors_num_embed)))\n\n        sources = [args.source] + args.source_factors\n        sources = [str(os.path.abspath(source)) for source in sources]\n\n        check_condition(len(sources) == len(validation_sources),\n                        'Training and validation data must have the same number of factors, but found %d and %d.' % (\n                            len(source_vocabs), len(validation_sources)))\n\n        train_iter, validation_iter, config_data, data_info = data_io.get_training_data_iters(\n            sources=sources,\n            target=os.path.abspath(args.target),\n            validation_sources=validation_sources,\n            validation_target=validation_target,\n            source_vocabs=source_vocabs,\n            target_vocab=target_vocab,\n            source_vocab_paths=source_vocab_paths,\n            target_vocab_path=target_vocab_path,\n            shared_vocab=shared_vocab,\n            batch_size=args.batch_size,\n            batch_by_words=batch_by_words,\n            batch_num_devices=batch_num_devices,\n            max_seq_len_source=max_seq_len_source,\n            max_seq_len_target=max_seq_len_target,\n            bucketing=not args.no_bucketing,\n            bucket_width=args.bucket_width)\n\n        data_info_fname = os.path.join(output_folder, C.DATA_INFO)\n        logger.info(\"Writing data config to '%s'\", data_info_fname)\n        data_info.save(data_info_fname)\n\n        return train_iter, validation_iter, config_data, source_vocabs, target_vocab"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates the encoder config.", "response": "def create_encoder_config(args: argparse.Namespace,\n                          max_seq_len_source: int,\n                          max_seq_len_target: int,\n                          config_conv: Optional[encoder.ConvolutionalEmbeddingConfig],\n                          num_embed_source: int) -> Tuple[encoder.EncoderConfig, int]:\n    \"\"\"\n    Create the encoder config.\n\n    :param args: Arguments as returned by argparse.\n    :param max_seq_len_source: Maximum source sequence length.\n    :param max_seq_len_target: Maximum target sequence length.\n    :param config_conv: The config for the convolutional encoder (optional).\n    :param num_embed_source: The size of the source embedding.\n    :return: The encoder config and the number of hidden units of the encoder.\n    \"\"\"\n    encoder_num_layers, _ = args.num_layers\n    config_encoder = None  # type: Optional[Config]\n\n    if args.decoder_only:\n        if args.encoder in (C.TRANSFORMER_TYPE, C.TRANSFORMER_WITH_CONV_EMBED_TYPE):\n            encoder_num_hidden = args.transformer_model_size[0]\n        elif args.encoder == C.CONVOLUTION_TYPE:\n            encoder_num_hidden = args.cnn_num_hidden\n        else:\n            encoder_num_hidden = args.rnn_num_hidden\n        config_encoder = encoder.EmptyEncoderConfig(num_embed=num_embed_source,\n                                                    num_hidden=encoder_num_hidden)\n    elif args.encoder in (C.TRANSFORMER_TYPE, C.TRANSFORMER_WITH_CONV_EMBED_TYPE):\n        encoder_transformer_preprocess, _ = args.transformer_preprocess\n        encoder_transformer_postprocess, _ = args.transformer_postprocess\n        encoder_transformer_model_size = args.transformer_model_size[0]\n\n        total_source_factor_size = sum(args.source_factors_num_embed)\n        if args.source_factors_combine == C.SOURCE_FACTORS_COMBINE_CONCAT and total_source_factor_size > 0:\n            logger.info(\"Encoder transformer-model-size adjusted to account for source factor embeddings: %d -> %d\" % (\n                encoder_transformer_model_size, num_embed_source + total_source_factor_size))\n            encoder_transformer_model_size = num_embed_source + total_source_factor_size\n        config_encoder = transformer.TransformerConfig(\n            model_size=encoder_transformer_model_size,\n            attention_heads=args.transformer_attention_heads[0],\n            feed_forward_num_hidden=args.transformer_feed_forward_num_hidden[0],\n            act_type=args.transformer_activation_type,\n            num_layers=encoder_num_layers,\n            dropout_attention=args.transformer_dropout_attention,\n            dropout_act=args.transformer_dropout_act,\n            dropout_prepost=args.transformer_dropout_prepost,\n            positional_embedding_type=args.transformer_positional_embedding_type,\n            preprocess_sequence=encoder_transformer_preprocess,\n            postprocess_sequence=encoder_transformer_postprocess,\n            max_seq_len_source=max_seq_len_source,\n            max_seq_len_target=max_seq_len_target,\n            conv_config=config_conv,\n            lhuc=args.lhuc is not None and (C.LHUC_ENCODER in args.lhuc or C.LHUC_ALL in args.lhuc))\n        encoder_num_hidden = encoder_transformer_model_size\n    elif args.encoder == C.CONVOLUTION_TYPE:\n        cnn_kernel_width_encoder, _ = args.cnn_kernel_width\n        cnn_config = convolution.ConvolutionConfig(kernel_width=cnn_kernel_width_encoder,\n                                                   num_hidden=args.cnn_num_hidden,\n                                                   act_type=args.cnn_activation_type,\n                                                   weight_normalization=args.weight_normalization)\n        cnn_num_embed = num_embed_source\n        if args.source_factors_combine == C.SOURCE_FACTORS_COMBINE_CONCAT:\n            cnn_num_embed += sum(args.source_factors_num_embed)\n        config_encoder = encoder.ConvolutionalEncoderConfig(num_embed=cnn_num_embed,\n                                                            max_seq_len_source=max_seq_len_source,\n                                                            cnn_config=cnn_config,\n                                                            num_layers=encoder_num_layers,\n                                                            positional_embedding_type=args.cnn_positional_embedding_type)\n\n        encoder_num_hidden = args.cnn_num_hidden\n    else:\n        encoder_rnn_dropout_inputs, _ = args.rnn_dropout_inputs\n        encoder_rnn_dropout_states, _ = args.rnn_dropout_states\n        encoder_rnn_dropout_recurrent, _ = args.rnn_dropout_recurrent\n        config_encoder = encoder.RecurrentEncoderConfig(\n            rnn_config=rnn.RNNConfig(cell_type=args.rnn_cell_type,\n                                     num_hidden=args.rnn_num_hidden,\n                                     num_layers=encoder_num_layers,\n                                     dropout_inputs=encoder_rnn_dropout_inputs,\n                                     dropout_states=encoder_rnn_dropout_states,\n                                     dropout_recurrent=encoder_rnn_dropout_recurrent,\n                                     residual=args.rnn_residual_connections,\n                                     first_residual_layer=args.rnn_first_residual_layer,\n                                     forget_bias=args.rnn_forget_bias,\n                                     lhuc=args.lhuc is not None and (C.LHUC_ENCODER in args.lhuc or C.LHUC_ALL in args.lhuc)),\n            conv_config=config_conv,\n            reverse_input=args.rnn_encoder_reverse_input)\n        encoder_num_hidden = args.rnn_num_hidden\n\n    return config_encoder, encoder_num_hidden"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_decoder_config(args: argparse.Namespace, encoder_num_hidden: int,\n                          max_seq_len_source: int, max_seq_len_target: int,\n                          num_embed_target: int) -> decoder.DecoderConfig:\n    \"\"\"\n    Create the config for the decoder.\n\n    :param args: Arguments as returned by argparse.\n    :param encoder_num_hidden: Number of hidden units of the Encoder.\n    :param max_seq_len_source: Maximum source sequence length.\n    :param max_seq_len_target: Maximum target sequence length.\n    :param num_embed_target: The size of the source embedding.\n    :return: The config for the decoder.\n    \"\"\"\n    _, decoder_num_layers = args.num_layers\n\n    config_decoder = None  # type: Optional[Config]\n\n    if args.decoder == C.TRANSFORMER_TYPE:\n        if args.decoder_only:\n            raise NotImplementedError()\n        _, decoder_transformer_preprocess = args.transformer_preprocess\n        _, decoder_transformer_postprocess = args.transformer_postprocess\n        config_decoder = transformer.TransformerConfig(\n            model_size=args.transformer_model_size[1],\n            attention_heads=args.transformer_attention_heads[1],\n            feed_forward_num_hidden=args.transformer_feed_forward_num_hidden[1],\n            act_type=args.transformer_activation_type,\n            num_layers=decoder_num_layers,\n            dropout_attention=args.transformer_dropout_attention,\n            dropout_act=args.transformer_dropout_act,\n            dropout_prepost=args.transformer_dropout_prepost,\n            positional_embedding_type=args.transformer_positional_embedding_type,\n            preprocess_sequence=decoder_transformer_preprocess,\n            postprocess_sequence=decoder_transformer_postprocess,\n            max_seq_len_source=max_seq_len_source,\n            max_seq_len_target=max_seq_len_target,\n            conv_config=None,\n            lhuc=args.lhuc is not None and (C.LHUC_DECODER in args.lhuc or C.LHUC_ALL in args.lhuc))\n\n    elif args.decoder == C.CONVOLUTION_TYPE:\n        if args.decoder_only:\n            raise NotImplementedError()\n        _, cnn_kernel_width_decoder = args.cnn_kernel_width\n        convolution_config = convolution.ConvolutionConfig(kernel_width=cnn_kernel_width_decoder,\n                                                           num_hidden=args.cnn_num_hidden,\n                                                           act_type=args.cnn_activation_type,\n                                                           weight_normalization=args.weight_normalization)\n        config_decoder = decoder.ConvolutionalDecoderConfig(cnn_config=convolution_config,\n                                                            max_seq_len_target=max_seq_len_target,\n                                                            num_embed=num_embed_target,\n                                                            encoder_num_hidden=encoder_num_hidden,\n                                                            num_layers=decoder_num_layers,\n                                                            positional_embedding_type=args.cnn_positional_embedding_type,\n                                                            project_qkv=args.cnn_project_qkv,\n                                                            hidden_dropout=args.cnn_hidden_dropout)\n\n    else:\n        if args.decoder_only:\n            args.rnn_decoder_state_init = C.RNN_DEC_INIT_ZERO\n            args.rnn_context_gating = False\n            args.rnn_attention_type = C.ATT_FIXED\n            args.rnn_attention_in_upper_layers = False\n            args.lhuc = None\n            args.rnn_enc_last_hidden_concat_to_embedding = False\n\n        rnn_attention_num_hidden = args.rnn_num_hidden if args.rnn_attention_num_hidden is None else args.rnn_attention_num_hidden\n        config_coverage = None\n        if args.rnn_attention_type == C.ATT_COV:\n            config_coverage = coverage.CoverageConfig(type=args.rnn_attention_coverage_type,\n                                                      max_fertility=args.rnn_attention_coverage_max_fertility,\n                                                      num_hidden=args.rnn_attention_coverage_num_hidden,\n                                                      layer_normalization=args.layer_normalization)\n        config_attention = rnn_attention.AttentionConfig(type=args.rnn_attention_type,\n                                                         num_hidden=rnn_attention_num_hidden,\n                                                         input_previous_word=args.rnn_attention_use_prev_word,\n                                                         source_num_hidden=encoder_num_hidden,\n                                                         query_num_hidden=args.rnn_num_hidden,\n                                                         layer_normalization=args.layer_normalization,\n                                                         config_coverage=config_coverage,\n                                                         num_heads=args.rnn_attention_mhdot_heads,\n                                                         is_scaled=args.rnn_scale_dot_attention)\n\n        _, decoder_rnn_dropout_inputs = args.rnn_dropout_inputs\n        _, decoder_rnn_dropout_states = args.rnn_dropout_states\n        _, decoder_rnn_dropout_recurrent = args.rnn_dropout_recurrent\n\n        config_decoder = decoder.RecurrentDecoderConfig(\n            max_seq_len_source=max_seq_len_source,\n            rnn_config=rnn.RNNConfig(cell_type=args.rnn_cell_type,\n                                     num_hidden=args.rnn_num_hidden,\n                                     num_layers=decoder_num_layers,\n                                     dropout_inputs=decoder_rnn_dropout_inputs,\n                                     dropout_states=decoder_rnn_dropout_states,\n                                     dropout_recurrent=decoder_rnn_dropout_recurrent,\n                                     residual=args.rnn_residual_connections,\n                                     first_residual_layer=args.rnn_first_residual_layer,\n                                     forget_bias=args.rnn_forget_bias,\n                                     lhuc=args.lhuc is not None and (C.LHUC_DECODER in args.lhuc or C.LHUC_ALL in args.lhuc)),\n            attention_config=config_attention,\n            hidden_dropout=args.rnn_decoder_hidden_dropout,\n            state_init=args.rnn_decoder_state_init,\n            context_gating=args.rnn_context_gating,\n            layer_normalization=args.layer_normalization,\n            attention_in_upper_layers=args.rnn_attention_in_upper_layers,\n            state_init_lhuc=args.lhuc is not None and (C.LHUC_STATE_INIT in args.lhuc or C.LHUC_ALL in args.lhuc),\n            enc_last_hidden_concat_to_embedding=args.rnn_enc_last_hidden_concat_to_embedding)\n\n    return config_decoder", "response": "Create the decoder config."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck possible encoder - decoder argument conflicts.", "response": "def check_encoder_decoder_args(args) -> None:\n    \"\"\"\n    Check possible encoder-decoder argument conflicts.\n\n    :param args: Arguments as returned by argparse.\n    \"\"\"\n    encoder_embed_dropout, decoder_embed_dropout = args.embed_dropout\n    encoder_rnn_dropout_inputs, decoder_rnn_dropout_inputs = args.rnn_dropout_inputs\n    encoder_rnn_dropout_states, decoder_rnn_dropout_states = args.rnn_dropout_states\n    if encoder_embed_dropout > 0 and encoder_rnn_dropout_inputs > 0:\n        logger.warning(\"Setting encoder RNN AND source embedding dropout > 0 leads to \"\n                       \"two dropout layers on top of each other.\")\n    if decoder_embed_dropout > 0 and decoder_rnn_dropout_inputs > 0:\n        logger.warning(\"Setting encoder RNN AND source embedding dropout > 0 leads to \"\n                       \"two dropout layers on top of each other.\")\n    encoder_rnn_dropout_recurrent, decoder_rnn_dropout_recurrent = args.rnn_dropout_recurrent\n    if encoder_rnn_dropout_recurrent > 0 or decoder_rnn_dropout_recurrent > 0:\n        check_condition(args.rnn_cell_type == C.LSTM_TYPE,\n                        \"Recurrent dropout without memory loss only supported for LSTMs right now.\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a ModelConfig from the arguments given in the command line.", "response": "def create_model_config(args: argparse.Namespace,\n                        source_vocab_sizes: List[int],\n                        target_vocab_size: int,\n                        max_seq_len_source: int,\n                        max_seq_len_target: int,\n                        config_data: data_io.DataConfig) -> model.ModelConfig:\n    \"\"\"\n    Create a ModelConfig from the argument given in the command line.\n\n    :param args: Arguments as returned by argparse.\n    :param source_vocab_sizes: The size of the source vocabulary (and source factors).\n    :param target_vocab_size: The size of the target vocabulary.\n    :param max_seq_len_source: Maximum source sequence length.\n    :param max_seq_len_target: Maximum target sequence length.\n    :param config_data: Data config.\n    :return: The model configuration.\n    \"\"\"\n    num_embed_source, num_embed_target = get_num_embed(args)\n\n    embed_dropout_source, embed_dropout_target = args.embed_dropout\n    source_vocab_size, *source_factor_vocab_sizes = source_vocab_sizes\n\n    check_encoder_decoder_args(args)\n\n    config_conv = None\n    if args.encoder == C.RNN_WITH_CONV_EMBED_NAME:\n        config_conv = encoder.ConvolutionalEmbeddingConfig(num_embed=num_embed_source,\n                                                           max_filter_width=args.conv_embed_max_filter_width,\n                                                           num_filters=args.conv_embed_num_filters,\n                                                           pool_stride=args.conv_embed_pool_stride,\n                                                           num_highway_layers=args.conv_embed_num_highway_layers,\n                                                           dropout=args.conv_embed_dropout)\n    if args.encoder == C.TRANSFORMER_WITH_CONV_EMBED_TYPE:\n        config_conv = encoder.ConvolutionalEmbeddingConfig(num_embed=num_embed_source,\n                                                           output_dim=num_embed_source,\n                                                           max_filter_width=args.conv_embed_max_filter_width,\n                                                           num_filters=args.conv_embed_num_filters,\n                                                           pool_stride=args.conv_embed_pool_stride,\n                                                           num_highway_layers=args.conv_embed_num_highway_layers,\n                                                           dropout=args.conv_embed_dropout)\n\n    config_encoder, encoder_num_hidden = create_encoder_config(args, max_seq_len_source, max_seq_len_target,\n                                                               config_conv, num_embed_source)\n    config_decoder = create_decoder_config(args, encoder_num_hidden, max_seq_len_source, max_seq_len_target,\n                                           num_embed_target)\n\n    source_factor_configs = None\n    if len(source_vocab_sizes) > 1:\n        source_factors_num_embed = args.source_factors_num_embed\n        if args.source_factors_combine == C.SOURCE_FACTORS_COMBINE_SUM:\n            # If factors are being added instead of concatenated, set all dimensions to the embedding dimensions\n            logger.info(\"Setting all source factor embedding sizes to `num_embed` ('%d') for summing\",\n                        num_embed_source)\n            source_factors_num_embed = [num_embed_source] * len(source_factor_vocab_sizes)\n\n        source_factor_configs = [encoder.FactorConfig(size, dim) for size, dim in zip(source_factor_vocab_sizes,\n                                                                                      source_factors_num_embed)]\n\n    config_embed_source = encoder.EmbeddingConfig(vocab_size=source_vocab_size,\n                                                  num_embed=num_embed_source,\n                                                  dropout=embed_dropout_source,\n                                                  factor_configs=source_factor_configs,\n                                                  source_factors_combine=args.source_factors_combine)\n\n    config_embed_target = encoder.EmbeddingConfig(vocab_size=target_vocab_size,\n                                                  num_embed=num_embed_target,\n                                                  dropout=embed_dropout_target)\n\n    config_loss = loss.LossConfig(name=args.loss,\n                                  vocab_size=target_vocab_size,\n                                  normalization_type=args.loss_normalization_type,\n                                  label_smoothing=args.label_smoothing)\n\n    if args.length_task is not None:\n        config_length_task = layers.LengthRatioConfig(num_layers=args.length_task_layers, weight=args.length_task_weight)\n        link = C.LINK_NORMAL if args.length_task == C.LENGTH_TASK_RATIO else C.LINK_POISSON\n        config_length_task_loss = loss.LossConfig(name=C.LENRATIO_REGRESSION,\n                                                   length_task_link=link,\n                                                   length_task_weight=args.length_task_weight)\n    else:\n        config_length_task = None\n        config_length_task_loss = None\n\n    model_config = model.ModelConfig(config_data=config_data,\n                                     vocab_source_size=source_vocab_size,\n                                     vocab_target_size=target_vocab_size,\n                                     config_embed_source=config_embed_source,\n                                     config_embed_target=config_embed_target,\n                                     config_encoder=config_encoder,\n                                     config_decoder=config_decoder,\n                                     config_loss=config_loss,\n                                     config_length_task_loss=config_length_task_loss,\n                                     config_length_task=config_length_task,\n                                     weight_tying=args.weight_tying,\n                                     weight_tying_type=args.weight_tying_type if args.weight_tying else None,\n                                     weight_normalization=args.weight_normalization,\n                                     lhuc=args.lhuc is not None)\n    return model_config"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_training_model(config: model.ModelConfig,\n                          context: List[mx.Context],\n                          output_dir: str,\n                          train_iter: data_io.BaseParallelSampleIter,\n                          args: argparse.Namespace) -> training.TrainingModel:\n    \"\"\"\n    Create a training model and load the parameters from disk if needed.\n\n    :param config: The configuration for the model.\n    :param context: The context(s) to run on.\n    :param output_dir: Output folder.\n    :param train_iter: The training data iterator.\n    :param args: Arguments as returned by argparse.\n    :return: The training model.\n    \"\"\"\n    training_model = training.TrainingModel(config=config,\n                                            context=context,\n                                            output_dir=output_dir,\n                                            provide_data=train_iter.provide_data,\n                                            provide_label=train_iter.provide_label,\n                                            default_bucket_key=train_iter.default_bucket_key,\n                                            bucketing=not args.no_bucketing,\n                                            gradient_compression_params=gradient_compression_params(args),\n                                            gradient_accumulation=args.update_interval > 1,\n                                            fixed_param_names=args.fixed_param_names,\n                                            fixed_param_strategy=args.fixed_param_strategy)\n\n    return training_model", "response": "Create a training model and load the parameters from disk if needed."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef gradient_compression_params(args: argparse.Namespace) -> Optional[Dict[str, Any]]:\n    if args.gradient_compression_type is None:\n        return None\n    else:\n        return {'type': args.gradient_compression_type, 'threshold': args.gradient_compression_threshold}", "response": "Returns gradient compression parameters as a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_optimizer_config(args: argparse.Namespace, source_vocab_sizes: List[int],\n                            extra_initializers: List[Tuple[str, mx.initializer.Initializer]] = None) -> OptimizerConfig:\n    \"\"\"\n    Returns an OptimizerConfig.\n\n    :param args: Arguments as returned by argparse.\n    :param source_vocab_sizes: Source vocabulary sizes.\n    :param extra_initializers: extra initializer to pass to `get_initializer`.\n    :return: The optimizer type and its parameters as well as the kvstore.\n    \"\"\"\n    optimizer_params = {'wd': args.weight_decay,\n                        \"learning_rate\": args.initial_learning_rate}\n\n    gradient_clipping_threshold = none_if_negative(args.gradient_clipping_threshold)\n    if gradient_clipping_threshold is None:\n        logger.info(\"Gradient clipping threshold set to negative value. Will not perform gradient clipping.\")\n        gradient_clipping_type = C.GRADIENT_CLIPPING_TYPE_NONE\n    else:\n        gradient_clipping_type = args.gradient_clipping_type\n\n    effective_batch_size = args.batch_size * args.update_interval\n\n    # Note: for 'abs' we use the implementation inside of MXNet's optimizer and 'norm_*' we implement ourselves\n    # inside the TrainingModel.\n    if gradient_clipping_threshold is not None and gradient_clipping_type == C.GRADIENT_CLIPPING_TYPE_ABS:\n        optimizer_params[\"clip_gradient\"] = gradient_clipping_threshold\n    if args.momentum is not None:\n        optimizer_params[\"momentum\"] = args.momentum\n    if args.loss_normalization_type == C.LOSS_NORM_VALID:\n        # When we normalize by the number of non-PAD symbols in a batch we need to disable rescale_grad.\n        optimizer_params[\"rescale_grad\"] = 1.0 / args.update_interval\n    elif args.loss_normalization_type == C.LOSS_NORM_BATCH:\n        # Making MXNet module API's default scaling factor explicit\n        optimizer_params[\"rescale_grad\"] = 1.0 / effective_batch_size\n    # Manually specified params\n    if args.optimizer_params:\n        optimizer_params.update(args.optimizer_params)\n\n    weight_init = initializer.get_initializer(default_init_type=args.weight_init,\n                                              default_init_scale=args.weight_init_scale,\n                                              default_init_xavier_rand_type=args.weight_init_xavier_rand_type,\n                                              default_init_xavier_factor_type=args.weight_init_xavier_factor_type,\n                                              embed_init_type=args.embed_weight_init,\n                                              embed_init_sigma=source_vocab_sizes[0] ** -0.5,\n                                              rnn_init_type=args.rnn_h2h_init,\n                                              extra_initializers=extra_initializers)\n\n    lr_sched = lr_scheduler.get_lr_scheduler(args.learning_rate_scheduler_type,\n                                             args.checkpoint_interval,\n                                             none_if_negative(args.learning_rate_half_life),\n                                             args.learning_rate_reduce_factor,\n                                             args.learning_rate_reduce_num_not_improved,\n                                             args.learning_rate_schedule,\n                                             args.learning_rate_warmup)\n\n    config = OptimizerConfig(name=args.optimizer,\n                             params=optimizer_params,\n                             kvstore=args.kvstore,\n                             initializer=weight_init,\n                             gradient_clipping_type=gradient_clipping_type,\n                             gradient_clipping_threshold=gradient_clipping_threshold,\n                             update_interval=args.update_interval)\n    config.set_lr_scheduler(lr_sched)\n    logger.info(\"Optimizer: %s\", config)\n    logger.info(\"Gradient Compression: %s\", gradient_compression_params(args))\n    if args.update_interval > 1:\n        logger.info(\"Gradient accumulation over %d batches. Effective batch size: %d\",\n                    args.update_interval, effective_batch_size)\n    return config", "response": "Create an optimizer configuration object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfreeze this Config object disallowing modification or addition of any parameters.", "response": "def freeze(self):\n        \"\"\"\n        Freezes this Config object, disallowing modification or addition of any parameters.\n        \"\"\"\n        if getattr(self, '_frozen'):\n            return\n        object.__setattr__(self, \"_frozen\", True)\n        for k, v in self.__dict__.items():\n            if isinstance(v, Config) and k != \"self\":\n                v.freeze()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef __del_frozen(self):\n        self.__delattr__('_frozen')\n        for attr, val in self.__dict__.items():\n            if isinstance(val, Config) and hasattr(val, '_frozen'):\n                val.__del_frozen()", "response": "Removes _frozen attribute from this instance and all its child configurations."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __add_frozen(self):\n        setattr(self, \"_frozen\", False)\n        for attr, val in self.__dict__.items():\n            if isinstance(val, Config):\n                val.__add_frozen()", "response": "Adds _frozen attribute to this instance and all its child configurations."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef save(self, fname: str):\n        obj = copy.deepcopy(self)\n        obj.__del_frozen()\n        with open(fname, 'w') as out:\n            yaml.dump(obj, out, default_flow_style=False)", "response": "Saves this Config to a file called fname."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load(fname: str) -> 'Config':\n        with open(fname) as inp:\n            obj = yaml.load(inp)\n            obj.__add_frozen()\n            return obj", "response": "Load a configuration from a file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef copy(self, **kwargs):\n        copy_obj = copy.deepcopy(self)\n        for name, value in kwargs.items():\n            object.__setattr__(copy_obj, name, value)\n        return copy_obj", "response": "Create a deep copy of the config object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndisable dropout for all float - valued attributes in this config or any of its children.", "response": "def disable_dropout(self):\n        \"\"\"\n        Sets the value of all float-valued attributes in this config (or any of its children) that contain 'dropout'\n        in their name to 0.0.\n        \"\"\"\n        for attr, val in self.__dict__.items():\n            if isinstance(val, Config):\n                val.disable_dropout()\n            elif 'dropout' in attr and isinstance(val, float):\n                logger.debug(\"Setting %s to 0.0\", attr)\n                setattr(self, attr, 0.0)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef compute_sims(inputs: mx.nd.NDArray, normalize: bool) -> mx.nd.NDArray:\n    if normalize:\n        logger.info(\"Normalizing embeddings to unit length\")\n        inputs = mx.nd.L2Normalization(inputs, mode='instance')\n    sims = mx.nd.dot(inputs, inputs, transpose_b=True)\n    sims_np = sims.asnumpy()\n    np.fill_diagonal(sims_np, -9999999.)\n    sims = mx.nd.array(sims_np)\n    return sims", "response": "Computes pairwise similarity scores between the embeddings."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef nearest_k(similarity_matrix: mx.nd.NDArray,\n              query_word_id: int,\n              k: int,\n              gamma: float = 1.0) -> Iterable[Tuple[int, float]]:\n    \"\"\"\n    Returns values and indices of k items with largest similarity.\n\n    :param similarity_matrix: Similarity matrix.\n    :param query_word_id: Query word id.\n    :param k: Number of closest items to retrieve.\n    :param gamma: Parameter to control distribution steepness.\n    :return: List of indices and values of k nearest elements.\n    \"\"\"\n    # pylint: disable=unbalanced-tuple-unpacking\n    values, indices = mx.nd.topk(mx.nd.softmax(similarity_matrix[query_word_id] / gamma), k=k, ret_typ='both')\n    return zip(indices.asnumpy(), values.asnumpy())", "response": "Returns values and indices of k nearest items."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef main():\n    setup_main_logger(file_logging=False)\n    params = argparse.ArgumentParser(description='Shows nearest neighbours of input tokens in the embedding space.')\n    params.add_argument('--model', '-m', required=True,\n                        help='Model folder to load config from.')\n    params.add_argument('--checkpoint', '-c', required=False, type=int, default=None,\n                        help='Optional specific checkpoint to load parameters from. Best params otherwise.')\n    params.add_argument('--side', '-s', required=True, choices=['source', 'target'], help='what embeddings to look at')\n    params.add_argument('--norm', '-n', action='store_true', help='normalize embeddings to unit length')\n    params.add_argument('-k', type=int, default=5, help='Number of neighbours to print')\n    params.add_argument('--gamma', '-g', type=float, default=1.0, help='Softmax distribution steepness.')\n    args = params.parse_args()\n    embeddings(args)", "response": "Command - line tool to inspect model embeddings."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating word n - grams for multiple sentences.", "response": "def _get_word_ngrams(n, sentences):\n    \"\"\"Calculates word n-grams for multiple sentences.\n    \"\"\"\n    assert len(sentences) > 0\n    assert n > 0\n\n    words = _split_into_words(sentences)\n    return _get_ngrams(n, words)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _recon_lcs(x, y):\n    i, j = len(x), len(y)\n    table = _lcs(x, y)\n\n    def _recon(i, j):\n        \"\"\"private recon calculation\"\"\"\n        if i == 0 or j == 0:\n            return []\n        elif x[i - 1] == y[j - 1]:\n            return _recon(i - 1, j - 1) + [(x[i - 1], i)]\n        elif table[i - 1, j] > table[i, j - 1]:\n            return _recon(i - 1, j)\n        else:\n            return _recon(i, j - 1)\n\n    recon_tuple = tuple(map(lambda x: x[0], _recon(i, j)))\n    return recon_tuple", "response": "Returns the Longest Common Subsequence between x and y."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute ROUGE - N of two text collections of sentences.", "response": "def rouge_n(evaluated_sentences, reference_sentences, n=2):\n    \"\"\"\n    Computes ROUGE-N of two text collections of sentences.\n    Sourece: http://research.microsoft.com/en-us/um/people/cyl/download/\n    papers/rouge-working-note-v1.3.1.pdf\n\n    Args:\n      evaluated_sentences: The sentences that have been picked by the summarizer\n      reference_sentences: The sentences from the referene set\n      n: Size of ngram.  Defaults to 2.\n\n    Returns:\n      A tuple (f1, precision, recall) for ROUGE-N\n\n    Raises:\n      ValueError: raises exception if a param has len <= 0\n    \"\"\"\n    if len(evaluated_sentences) <= 0 or len(reference_sentences) <= 0:\n        raise ValueError(\"Collections must contain at least 1 sentence.\")\n\n    evaluated_ngrams = _get_word_ngrams(n, evaluated_sentences)\n    reference_ngrams = _get_word_ngrams(n, reference_sentences)\n    reference_count = len(reference_ngrams)\n    evaluated_count = len(evaluated_ngrams)\n\n    # Gets the overlapping ngrams between evaluated and reference\n    overlapping_ngrams = evaluated_ngrams.intersection(reference_ngrams)\n    overlapping_count = len(overlapping_ngrams)\n\n    # Handle edge case. This isn't mathematically correct, but it's good enough\n    if evaluated_count == 0:\n        precision = 0.0\n    else:\n        precision = overlapping_count / evaluated_count\n\n    # Handle edge case for recall, same as for precision\n    if reference_count == 0:\n        recall = 0.0\n    else:\n        recall = overlapping_count / reference_count\n    f1_score = 2.0 * ((precision * recall) / (precision + recall + 1e-8))\n\n    # return overlapping_count / reference_count\n    return f1_score, precision, recall"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef rouge_l_summary_level(evaluated_sentences, reference_sentences):\n    if len(evaluated_sentences) <= 0 or len(reference_sentences) <= 0:\n        raise ValueError(\"Collections must contain at least 1 sentence.\")\n\n    # total number of words in reference sentences\n    m = len(_split_into_words(reference_sentences))\n\n    # total number of words in evaluated sentences\n    n = len(_split_into_words(evaluated_sentences))\n\n    union_lcs_sum_across_all_references = 0\n    for ref_s in reference_sentences:\n        union_lcs_sum_across_all_references += _union_lcs(evaluated_sentences,\n                                                          ref_s)\n    return _f_p_r_lcs(union_lcs_sum_across_all_references, m, n)", "response": "Compute ROUGE - L summary level of two text collections of sentences."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates average ROUGE scores for a list of hypotheses and references", "response": "def rouge(hypotheses, references):\n    \"\"\"Calculates average rouge scores for a list of hypotheses and\n    references\"\"\"\n\n    # Filter out hyps that are of 0 length\n    # hyps_and_refs = zip(hypotheses, references)\n    # hyps_and_refs = [_ for _ in hyps_and_refs if len(_[0]) > 0]\n    # hypotheses, references = zip(*hyps_and_refs)\n\n    # Calculate ROUGE-1 F1, precision, recall scores\n    rouge_1 = [\n        rouge_n([hyp], [ref], 1) for hyp, ref in zip(hypotheses, references)\n    ]\n    rouge_1_f, rouge_1_p, rouge_1_r = map(np.mean, zip(*rouge_1))\n\n    # Calculate ROUGE-2 F1, precision, recall scores\n    rouge_2 = [\n        rouge_n([hyp], [ref], 2) for hyp, ref in zip(hypotheses, references)\n    ]\n    rouge_2_f, rouge_2_p, rouge_2_r = map(np.mean, zip(*rouge_2))\n\n    # Calculate ROUGE-L F1, precision, recall scores\n    rouge_l = [\n        rouge_l_sentence_level([hyp], [ref])\n        for hyp, ref in zip(hypotheses, references)\n    ]\n    rouge_l_f, rouge_l_p, rouge_l_r = map(np.mean, zip(*rouge_l))\n\n    return {\n        \"rouge_1/f_score\": rouge_1_f,\n        \"rouge_1/r_score\": rouge_1_r,\n        \"rouge_1/p_score\": rouge_1_p,\n        \"rouge_2/f_score\": rouge_2_f,\n        \"rouge_2/r_score\": rouge_2_r,\n        \"rouge_2/p_score\": rouge_2_p,\n        \"rouge_l/f_score\": rouge_l_f,\n        \"rouge_l/r_score\": rouge_l_r,\n        \"rouge_l/p_score\": rouge_l_p,\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates ROUGE - 1 F1 precision recall scores", "response": "def rouge_1(hypotheses, references):\n    \"\"\"\n    Calculate ROUGE-1 F1, precision, recall scores\n    \"\"\"\n    rouge_1 = [\n        rouge_n([hyp], [ref], 1) for hyp, ref in zip(hypotheses, references)\n    ]\n    rouge_1_f, _, _ = map(np.mean, zip(*rouge_1))\n    return rouge_1_f"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate ROUGE - 2 F1 precision recall scores", "response": "def rouge_2(hypotheses, references):\n    \"\"\"\n    Calculate ROUGE-2 F1, precision, recall scores\n    \"\"\"\n    rouge_2 = [\n        rouge_n([hyp], [ref], 2) for hyp, ref in zip(hypotheses, references)\n    ]\n    rouge_2_f, _, _ = map(np.mean, zip(*rouge_2))\n    return rouge_2_f"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate ROUGE - L F1 precision recall scores", "response": "def rouge_l(hypotheses, references):\n    \"\"\"\n    Calculate ROUGE-L F1, precision, recall scores\n    \"\"\"\n    rouge_l = [\n        rouge_l_sentence_level([hyp], [ref])\n        for hyp, ref in zip(hypotheses, references)\n    ]\n    rouge_l_f, _, _ = map(np.mean, zip(*rouge_l))\n    return rouge_l_f"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn an iterator for the image text validation data.", "response": "def get_validation_image_text_data_iter(data_loader: RawParallelDatasetLoader,\n                                        validation_source_root: str,\n                                        validation_source: str,\n                                        validation_target: str,\n                                        buckets: List[Tuple[int, int]],\n                                        bucket_batch_sizes: List[BucketBatchSize],\n                                        source_image_size: tuple,\n                                        vocab_target: vocab.Vocab,\n                                        max_seq_len_target: int,\n                                        batch_size: int,\n                                        use_feature_loader: bool = False,\n                                        preload_features: bool = False) -> 'ParallelSampleIter':\n    \"\"\"\n    Returns a ParallelSampleIter for the validation data.\n    \"\"\"\n    logger.info(\"=================================\")\n    logger.info(\"Creating validation data iterator\")\n    logger.info(\"=================================\")\n\n    validation_source_images = [FileListReader(validation_source, validation_source_root)]\n    validation_target_sentences = SequenceReader(validation_target, vocab_target, add_bos=True, limit=None)\n\n    validation_data_statistics = get_data_statistics(source_readers=None,\n                                                     target_reader=validation_target_sentences,\n                                                     buckets=buckets,\n                                                     length_ratio_mean=1.0,\n                                                     length_ratio_std=1.0,\n                                                     source_vocabs=None,\n                                                     target_vocab=vocab_target)\n    validation_data_statistics.log(bucket_batch_sizes)\n\n    validation_data = data_loader.load(validation_source_images[0],\n                                       validation_target_sentences,\n                                       validation_data_statistics.num_sents_per_bucket).fill_up(bucket_batch_sizes)\n    return ImageTextSampleIter(data=validation_data,\n                               buckets=buckets,\n                               batch_size=batch_size,\n                               bucket_batch_sizes=bucket_batch_sizes,\n                               image_size=source_image_size,\n                               use_feature_loader=use_feature_loader,\n                               preload_features=preload_features)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns data iterators for training and validation images.", "response": "def get_training_image_text_data_iters(source_root: str,\n                                       source: str, target: str,\n                                       validation_source_root: str,\n                                       validation_source: str, validation_target: str,\n                                       vocab_target: vocab.Vocab,\n                                       vocab_target_path: Optional[str],\n                                       batch_size: int,\n                                       batch_by_words: bool,\n                                       batch_num_devices: int,\n                                       source_image_size: tuple,\n                                       max_seq_len_target: int,\n                                       bucketing: bool,\n                                       bucket_width: int,\n                                       use_feature_loader: bool = False,\n                                       preload_features: bool = False) -> Tuple['ParallelSampleIter',\n                                                                                'ParallelSampleIter',\n                                                                                'DataConfig', 'DataInfo']:\n    \"\"\"\n    Returns data iterators for training and validation data.\n\n    :param source_root: Path to source images since the file in source contains relative paths.\n    :param source: Path to source training data.\n    :param target: Path to target training data.\n    :param validation_source_root: Path to validation source images since the file in validation_source contains relative paths.\n    :param validation_source: Path to source validation data.\n    :param validation_target: Path to target validation data.\n    :param vocab_target: Target vocabulary.\n    :param vocab_target_path: Path to target vocabulary.\n    :param batch_size: Batch size.\n    :param batch_by_words: Size batches by words rather than sentences.\n    :param batch_num_devices: Number of devices batches will be parallelized across.\n    :param source_image_size: size to resize the image to (for iterator)\n    :param max_seq_len_target: Maximum target sequence length.\n    :param bucketing: Whether to use bucketing.\n    :param bucket_width: Size of buckets.\n    :param use_feature_loader: If True, features are loaded instead of images.\n    :param preload_features: If use_feature_loader si True, this enables load all the feature to memory\n    :return: Tuple of (training data iterator, validation data iterator, data config).\n    \"\"\"\n    logger.info(\"===============================\")\n    logger.info(\"Creating training data iterator\")\n    logger.info(\"===============================\")\n\n    # define buckets\n    buckets = define_empty_source_parallel_buckets(max_seq_len_target, bucket_width) if bucketing else [\n        (0, max_seq_len_target)]\n\n    source_images = [FileListReader(source, source_root)]\n    target_sentences = SequenceReader(target, vocab_target, add_bos=True)\n\n    # 2. pass: Get data statistics only on target (source not considered)\n    data_statistics = get_data_statistics(source_readers=None,\n                                          target_reader=target_sentences,\n                                          buckets=buckets,\n                                          length_ratio_mean=1.0,\n                                          length_ratio_std=1.0,\n                                          source_vocabs=None,\n                                          target_vocab=vocab_target)\n\n    bucket_batch_sizes = define_bucket_batch_sizes(buckets,\n                                                   batch_size,\n                                                   batch_by_words,\n                                                   batch_num_devices,\n                                                   data_statistics.average_len_target_per_bucket)\n\n    data_statistics.log(bucket_batch_sizes)\n\n    data_loader = RawListTextDatasetLoader(buckets=buckets,\n                                           eos_id=vocab_target[C.EOS_SYMBOL],\n                                           pad_id=C.PAD_ID)\n\n    training_data = data_loader.load(source_images[0], target_sentences,\n                                     data_statistics.num_sents_per_bucket).fill_up(bucket_batch_sizes)\n\n    data_info = DataInfo(sources=source_images,\n                         target=target,\n                         source_vocabs=None,\n                         target_vocab=vocab_target_path,\n                         shared_vocab=False,\n                         num_shards=1)\n\n    config_data = DataConfig(data_statistics=data_statistics,\n                             max_seq_len_source=0,\n                             max_seq_len_target=max_seq_len_target,\n                             num_source_factors=len(source_images))\n\n    # Add useful stuff to config_data\n    config_data.source_root = source_root\n    config_data.validation_source_root = validation_source_root\n    config_data.use_feature_loader = use_feature_loader\n\n    train_iter = ImageTextSampleIter(data=training_data,\n                                     buckets=buckets,\n                                     batch_size=batch_size,\n                                     bucket_batch_sizes=bucket_batch_sizes,\n                                     image_size=source_image_size,\n                                     use_feature_loader=use_feature_loader,\n                                     preload_features=preload_features)\n\n    validation_iter = get_validation_image_text_data_iter(data_loader=data_loader,\n                                                          validation_source_root=validation_source_root,\n                                                          validation_source=validation_source,\n                                                          validation_target=validation_target,\n                                                          buckets=buckets,\n                                                          bucket_batch_sizes=bucket_batch_sizes,\n                                                          source_image_size=source_image_size,\n                                                          vocab_target=vocab_target,\n                                                          max_seq_len_target=max_seq_len_target,\n                                                          batch_size=batch_size,\n                                                          use_feature_loader=use_feature_loader,\n                                                          preload_features=preload_features)\n\n    return train_iter, validation_iter, config_data, data_info"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load(self,\n             source_list: Iterable[List[str]],\n             target_sentences: Iterable[List[Any]],\n             num_samples_per_bucket: List[int]) -> 'ParallelDataSet':\n        \"\"\"\n        Creates a parallel dataset base on source list of strings and target sentences.\n        Returns a `sockeye.data_io.ParallelDataSet`.\n\n        :param source_list: Source list of strings (e.g., filenames).\n        :param target_sentences: Target sentences used to do bucketing.\n        :param num_samples_per_bucket: Number of samples per bucket.\n        :return: Returns a parallel dataset `sockeye.data_io.ParallelDataSet`.\n        \"\"\"\n        assert len(num_samples_per_bucket) == len(self.buckets)\n\n        data_source = [np.full((num_samples,), self.pad_id, dtype=object)\n                       for num_samples in num_samples_per_bucket]\n        # data_source is a List[numpy.array[str]] which semantic is bucket, index, str\n        # Its loading to memory is deferred to the iterator, since the full data\n        # is supposed to not fit in memory.\n        data_target = [np.full((num_samples, target_len), self.pad_id, dtype=self.dtype)\n                       for (source_len, target_len), num_samples in zip(self.buckets, num_samples_per_bucket)]\n        data_label = [np.full((num_samples, target_len), self.pad_id, dtype=self.dtype)\n                      for (source_len, target_len), num_samples in zip(self.buckets, num_samples_per_bucket)]\n\n        bucket_sample_index = [0 for buck in self.buckets]\n\n        # track amount of padding introduced through bucketing\n        num_tokens_target = 0\n        num_pad_target = 0\n\n        # Bucket sentences as padded np arrays\n        for source, target in zip(source_list, target_sentences):\n            target_len = len(target)\n            buck_index, buck = get_target_bucket(self.buckets, target_len)\n            if buck is None:\n                continue  # skip this sentence pair\n\n            num_tokens_target += buck[1]\n            num_pad_target += buck[1] - target_len\n\n            sample_index = bucket_sample_index[buck_index]\n            data_source[buck_index][sample_index] = source\n            data_target[buck_index][sample_index, :target_len] = target\n            # NOTE(fhieber): while this is wasteful w.r.t memory, we need to explicitly create the label sequence\n            # with the EOS symbol here sentence-wise and not per-batch due to variable sequence length within a batch.\n            # Once MXNet allows item assignments given a list of indices (probably MXNet 1.0): e.g a[[0,1,5,2]] = x,\n            # we can try again to compute the label sequence on the fly in next().\n            data_label[buck_index][sample_index, :target_len] = target[1:] + [self.eos_id]\n\n            bucket_sample_index[buck_index] += 1\n\n        for i in range(len(data_source)):\n            data_target[i] = mx.nd.array(data_target[i], dtype=self.dtype)\n            data_label[i] = mx.nd.array(data_label[i], dtype=self.dtype)\n\n        if num_tokens_target > 0:\n            logger.info(\"Created bucketed parallel data set. Introduced padding: target=%.1f%%)\",\n                        num_pad_target / num_tokens_target * 100)\n\n        return ParallelDataSet(data_source, data_target, data_label)", "response": "Creates a parallel dataset from source list of strings and target sentences."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef next(self) -> mx.io.DataBatch:\n        if not self.iter_next():\n            raise StopIteration\n\n        i, j = self.batch_indices[self.curr_batch_index]\n        self.curr_batch_index += 1\n\n        batch_size = self.bucket_batch_sizes[i].batch_size\n        source = self.data.source[i][j:j + batch_size]\n        target = self.data.target[i][j:j + batch_size]\n        if self.preload_features:\n            loaded_source = []  # type: List[np.ndarray]\n            for k in source:\n                loaded_source.append(self.loaded_source[k])\n        else:\n            loaded_source = self.data_loader(source)\n        # zero pad features if not agree with expected shape\n        loaded_source = zero_pad_features(loaded_source, self.image_size)\n        loaded_source = mx.nd.array(loaded_source)\n\n        label = [self.data.label[i][j:j + batch_size]]\n\n        provide_data = [mx.io.DataDesc(name=self.source_data_name, shape=loaded_source.shape, layout=C.BATCH_MAJOR_IMAGE)]\n        if self.with_text:\n            provide_data += [mx.io.DataDesc(name=self.target_data_name, shape=target.shape, layout=C.BATCH_MAJOR)]\n        provide_label = [mx.io.DataDesc(name=n, shape=x.shape, layout=C.BATCH_MAJOR) for n, x in\n                         zip(self.label_names, label)]\n\n        data = [loaded_source]\n        if self.with_text:\n            data += [target]\n        return mx.io.DataBatch(data, label,\n                               pad=0, index=None, bucket_key=self.buckets[i],\n                               provide_data=provide_data, provide_label=provide_label)", "response": "Returns the next batch from the data iterator."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_stacked_rnn(config: RNNConfig, prefix: str,\n                    parallel_inputs: bool = False,\n                    layers: Optional[Iterable[int]] = None) -> mx.rnn.SequentialRNNCell:\n    \"\"\"\n    Returns (stacked) RNN cell given parameters.\n\n    :param config: rnn configuration.\n    :param prefix: Symbol prefix for RNN.\n    :param parallel_inputs: Support parallel inputs for the stacked RNN cells.\n    :param layers: Specify which layers to create as a list of layer indexes.\n\n    :return: RNN cell.\n    \"\"\"\n\n    rnn = mx.rnn.SequentialRNNCell() if not parallel_inputs else SequentialRNNCellParallelInput()\n    if not layers:\n        layers = range(config.num_layers)\n    for layer_idx in layers:\n        # fhieber: the 'l' in the prefix does NOT stand for 'layer' but for the direction 'l' as in mx.rnn.rnn_cell::517\n        # this ensures parameter name compatibility of training w/ FusedRNN and decoding with 'unfused' RNN.\n        cell_prefix = \"%sl%d_\" % (prefix, layer_idx)\n        if config.cell_type == C.LSTM_TYPE:\n            if config.dropout_recurrent > 0.0:\n                cell = RecurrentDropoutLSTMCell(num_hidden=config.num_hidden, prefix=cell_prefix,\n                                                forget_bias=config.forget_bias, dropout=config.dropout_recurrent)\n            else:\n                cell = mx.rnn.LSTMCell(num_hidden=config.num_hidden, prefix=cell_prefix, forget_bias=config.forget_bias)\n        elif config.cell_type == C.LNLSTM_TYPE:\n            cell = LayerNormLSTMCell(num_hidden=config.num_hidden, prefix=cell_prefix, forget_bias=config.forget_bias)\n        elif config.cell_type == C.LNGLSTM_TYPE:\n            cell = LayerNormPerGateLSTMCell(num_hidden=config.num_hidden, prefix=cell_prefix,\n                                            forget_bias=config.forget_bias)\n        elif config.cell_type == C.GRU_TYPE:\n            cell = mx.rnn.GRUCell(num_hidden=config.num_hidden, prefix=cell_prefix)\n        elif config.cell_type == C.LNGRU_TYPE:\n            cell = LayerNormGRUCell(num_hidden=config.num_hidden, prefix=cell_prefix)\n        elif config.cell_type == C.LNGGRU_TYPE:\n            cell = LayerNormPerGateGRUCell(num_hidden=config.num_hidden, prefix=cell_prefix)\n        else:\n            raise NotImplementedError()\n\n        if config.dropout_inputs > 0 or config.dropout_states > 0:\n            cell = VariationalDropoutCell(cell,\n                                          dropout_inputs=config.dropout_inputs,\n                                          dropout_states=config.dropout_states)\n\n        if config.lhuc:\n            cell = LHUCCell(cell, config.num_hidden, config.dtype)\n\n        # layer_idx is 0 based, whereas first_residual_layer is 1-based\n        if config.residual and layer_idx + 1 >= config.first_residual_layer:\n            cell = mx.rnn.ResidualCell(cell) if not parallel_inputs else ResidualCellParallelInput(cell)\n        elif parallel_inputs:\n            cell = ParallelInputCell(cell)\n\n        rnn.add(cell)\n\n    return rnn", "response": "Returns a stacked RNN cell given parameters."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef init_batch(raw_constraints: List[Optional[RawConstraintList]],\n               beam_size: int,\n               start_id: int,\n               eos_id: int) -> List[Optional[ConstrainedHypothesis]]:\n    \"\"\"\n    :param raw_constraints: The list of raw constraints (list of list of IDs).\n    :param beam_size: The beam size.\n    :param start_id: The target-language vocabulary ID of the SOS symbol.\n    :param eos_id: The target-language vocabulary ID of the EOS symbol.\n    :return: A list of ConstrainedHypothesis objects (shape: (batch_size * beam_size,)).\n    \"\"\"\n    constraints = [None] * (len(raw_constraints) * beam_size)  # type: List[Optional[ConstrainedHypothesis]]\n    if any(raw_constraints):\n        for i, raw_list in enumerate(raw_constraints):\n            num_constraints = sum([len(phrase) for phrase in raw_list]) if raw_list is not None else 0\n            if num_constraints > 0:\n                hyp = ConstrainedHypothesis(raw_list, eos_id)\n                idx = i * beam_size\n                constraints[idx:idx + beam_size] = [hyp.advance(start_id) for x in range(beam_size)]\n\n    return constraints", "response": "Initializes a batch of ConstrainedHypothesis objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_bank_sizes(num_constraints: int,\n                   beam_size: int,\n                   candidate_counts: List[int]) -> List[int]:\n    \"\"\"\n    Evenly distributes the beam across the banks, where each bank is a portion of the beam devoted\n    to hypotheses having met the same number of constraints, 0..num_constraints.\n    After the assignment, banks with more slots than candidates are adjusted.\n\n    :param num_constraints: The number of constraints.\n    :param beam_size: The beam size.\n    :param candidate_counts: The empirical counts of number of candidates in each bank.\n    :return: A distribution over banks.\n    \"\"\"\n\n    num_banks = num_constraints + 1\n    bank_size = beam_size // num_banks\n    remainder = beam_size - bank_size * num_banks\n\n    # Distribute any remainder to the end\n    assigned = [bank_size for x in range(num_banks)]\n    assigned[-1] += remainder\n\n    # Now, moving right to left, push extra allocation to earlier buckets.\n    # This encodes a bias for higher buckets, but if no candidates are found, space\n    # will be made in lower buckets. This may not be the best strategy, but it is important\n    # that you start pushing from the bucket that is assigned the remainder, for cases where\n    # num_constraints >= beam_size.\n    for i in reversed(range(num_banks)):\n        overfill = assigned[i] - candidate_counts[i]\n        if overfill > 0:\n            assigned[i] -= overfill\n            assigned[(i - 1) % num_banks] += overfill\n\n    return assigned", "response": "This function returns the number of banks that are assigned to the beam."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbuilds a topk list from the given beam and batch size.", "response": "def topk(timestep: int,\n         batch_size: int,\n         beam_size: int,\n         inactive: mx.nd.NDArray,\n         scores: mx.nd.NDArray,\n         hypotheses: List[ConstrainedHypothesis],\n         best_ids: mx.nd.NDArray,\n         best_word_ids: mx.nd.NDArray,\n         seq_scores: mx.nd.NDArray) -> Tuple[np.array, np.array, np.array, List[ConstrainedHypothesis], mx.nd.NDArray]:\n    \"\"\"\n    Builds a new topk list such that the beam contains hypotheses having completed different numbers of constraints.\n    These items are built from three different types: (1) the best items across the whole\n    scores matrix, (2) the set of words that must follow existing constraints, and (3) k-best items from each row.\n\n    :param timestep: The current decoder timestep.\n    :param batch_size: The number of segments in the batch.\n    :param beam_size: The length of the beam for each segment.\n    :param inactive: Array listing inactive rows (shape: (beam_size,)).\n    :param scores: The scores array (shape: (batch_size if t==1 else beam_size, target_vocab_size)).\n    :param hypotheses: The list of hypothesis objects.\n    :param best_ids: The current list of best hypotheses (shape: (beam_size,)).\n    :param best_word_ids: The parallel list of best word IDs (shape: (beam_size,)).\n    :param seq_scores: (shape: (beam_size, 1)).\n    :return: A tuple containing the best hypothesis rows, the best hypothesis words, the scores,\n        the updated constrained hypotheses, and the updated set of inactive hypotheses.\n    \"\"\"\n\n    for sentno in range(batch_size):\n        rows = slice(sentno * beam_size, sentno * beam_size + beam_size)\n        if hypotheses[rows.start] is not None and hypotheses[rows.start].size() > 0:\n            best_ids[rows], best_word_ids[rows], seq_scores[rows], \\\n                hypotheses[rows], inactive[rows] = _sequential_topk(timestep,\n                                                                    beam_size,\n                                                                    inactive[rows],\n                                                                    scores[rows],\n                                                                    hypotheses[rows],\n                                                                    best_ids[rows] - rows.start,\n                                                                    best_word_ids[rows],\n                                                                    seq_scores[rows])\n\n            # offsetting since the returned smallest_k() indices were slice-relative\n            best_ids[rows] += rows.start\n        else:\n            # If there are no constraints for this sentence in the batch, everything stays\n            # the same, except we need to mark all hypotheses as active\n            inactive[rows] = 0\n\n    return best_ids, best_word_ids, seq_scores, hypotheses, inactive"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _sequential_topk(timestep: int,\n                     beam_size: int,\n                     inactive: mx.nd.NDArray,\n                     scores: mx.nd.NDArray,\n                     hypotheses: List[ConstrainedHypothesis],\n                     best_ids: mx.nd.NDArray,\n                     best_word_ids: mx.nd.NDArray,\n                     sequence_scores: mx.nd.NDArray) -> Tuple[np.array, np.array, np.array,\n                                                              List[ConstrainedHypothesis], mx.nd.NDArray]:\n    \"\"\"\n    Builds a new topk list such that the beam contains hypotheses having completed different numbers of constraints.\n    These items are built from three different types: (1) the best items across the whole\n    scores matrix, (2) the set of words that must follow existing constraints, and (3) k-best items from each row.\n\n    :param timestep: The current decoder timestep.\n    :param beam_size: The length of the beam for each segment.\n    :param inactive: Array listing inactive rows (shape: (beam_size,)).\n    :param scores: The scores array (shape: (beam_size, target_vocab_size)).\n    :param hypotheses: The list of hypothesis objects.\n    :param best_ids: The current list of best hypotheses (shape: (beam_size,)).\n    :param best_word_ids: The parallel list of best word IDs (shape: (beam_size,)).\n    :param sequence_scores: (shape: (beam_size, 1)).\n    :return: A tuple containing the best hypothesis rows, the best hypothesis words, the scores,\n        the updated constrained hypotheses, and the updated set of inactive hypotheses.\n    \"\"\"\n\n    num_constraints = hypotheses[0].size()\n\n    candidates = set()\n    # (1) Add all of the top-k items (which were passed) in as long as they pass the constraints\n    for row, col, seq_score in zip(best_ids, best_word_ids, sequence_scores):\n        row = int(row.asscalar())\n        col = int(col.asscalar())\n        if hypotheses[row] is not None and hypotheses[row].is_valid(col):\n            seq_score = float(seq_score.asscalar())\n            new_item = hypotheses[row].advance(col)\n            cand = ConstrainedCandidate(row, col, seq_score, new_item)\n            candidates.add(cand)\n\n    # For each hypothesis, we add (2) all the constraints that could follow it and\n    # (3) the best item (constrained or not) in that row\n    best_next = mx.nd.argmin(scores, axis=1)\n    for row in range(beam_size):\n        if inactive[row]:\n            continue\n\n        hyp = hypotheses[row]\n\n        # (2) add all the constraints that could extend this\n        nextones = hyp.allowed()\n\n        # (3) add the single-best item after this (if it's valid)\n        col = int(best_next[row].asscalar())\n        if hyp.is_valid(col):\n            nextones.add(col)\n\n        # Now, create new candidates for each of these items\n        for col in nextones:\n            new_item = hyp.advance(col)\n            score = scores[row, col].asscalar()\n            cand = ConstrainedCandidate(row, col, score, new_item)\n            candidates.add(cand)\n\n    # Sort the candidates. After allocating the beam across the banks, we will pick the top items\n    # for each bank from this list\n    sorted_candidates = sorted(candidates, key=attrgetter('score'))\n\n    # The number of hypotheses in each bank\n    counts = [0 for _ in range(num_constraints + 1)]\n    for cand in sorted_candidates:\n        counts[cand.hypothesis.num_met()] += 1\n\n    # Adjust allocated bank sizes if there are too few candidates in any of them\n    bank_sizes = get_bank_sizes(num_constraints, beam_size, counts)\n\n    # Sort the candidates into the allocated banks\n    pruned_candidates = []  # type: List[ConstrainedCandidate]\n    for i, cand in enumerate(sorted_candidates):\n        bank = cand.hypothesis.num_met()\n\n        if bank_sizes[bank] > 0:\n            pruned_candidates.append(cand)\n            bank_sizes[bank] -= 1\n\n    num_pruned_candidates = len(pruned_candidates)\n\n    inactive[:num_pruned_candidates] = 0\n\n    # Pad the beam so array assignment still works\n    if num_pruned_candidates < beam_size:\n        inactive[num_pruned_candidates:] = 1\n        pruned_candidates += [pruned_candidates[num_pruned_candidates - 1]] * (beam_size - num_pruned_candidates)\n\n    return (np.array([x.row for x in pruned_candidates]),\n            np.array([x.col for x in pruned_candidates]),\n            np.array([[x.score] for x in pruned_candidates]),\n            [x.hypothesis for x in pruned_candidates],\n            inactive)", "response": "Builds a sequential topk list from the current beam and the current set of inactive hypotheses and the scores of the current set of inactive hypotheses."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef consume(self, word_id: int) -> 'AvoidState':\n        if word_id in self.state.children:\n            return AvoidState(self.root, self.state.step(word_id))\n        elif word_id in self.root.children:\n            return AvoidState(self.root, self.root.step(word_id))\n        elif self.state != self.root:\n            return AvoidState(self.root, self.root)\n        else:\n            return self", "response": "Consumes a word and updates the state based on it."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a set of word IDs that should be avoided.", "response": "def avoid(self) -> Set[int]:\n        \"\"\"\n        Returns a set of word IDs that should be avoided. This includes the set of final states from the\n        root node, which are single tokens that must never be generated.\n\n        :return: A set of integers representing words that must not be generated next by this hypothesis.\n        \"\"\"\n        return self.root.final() | self.state.final()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreorders the avoid list according to the selected row indices.", "response": "def reorder(self, indices: mx.nd.NDArray) -> None:\n        \"\"\"\n        Reorders the avoid list according to the selected row indices.\n        This can produce duplicates, but this is fixed if state changes occur in consume().\n\n        :param indices: An mx.nd.NDArray containing indices of hypotheses to select.\n        \"\"\"\n        if self.global_avoid_states:\n            self.global_avoid_states = [self.global_avoid_states[x] for x in indices.asnumpy()]\n\n        if self.local_avoid_states:\n            self.local_avoid_states = [self.local_avoid_states[x] for x in indices.asnumpy()]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconsume a word for each trie updating respective states.", "response": "def consume(self, word_ids: mx.nd.NDArray) -> None:\n        \"\"\"\n        Consumes a word for each trie, updating respective states.\n\n        :param word_ids: The set of word IDs.\n        \"\"\"\n        word_ids = word_ids.asnumpy().tolist()\n        for i, word_id in enumerate(word_ids):\n            if self.global_avoid_states:\n                self.global_avoid_states[i] = self.global_avoid_states[i].consume(word_id)\n            if self.local_avoid_states:\n                self.local_avoid_states[i] = self.local_avoid_states[i].consume(word_id)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nassemble a list of per - hypothesis words to avoid.", "response": "def avoid(self) -> Tuple[Tuple[int], Tuple[int]]:\n        \"\"\"\n        Assembles a list of per-hypothesis words to avoid. The indices are (x, y) pairs into the scores\n        array, which has dimensions (beam_size, target_vocab_size). These values are then used by the caller\n        to set these items to np.inf so they won't be selected. Words to be avoided are selected by\n        consulting both the global trie of phrases and the sentence-specific one.\n\n        :return: Two lists of indices: the x coordinates and y coordinates.\n        \"\"\"\n        to_avoid = set()  # type: Set[Tuple[int, int]]\n        for i, state in enumerate(self.global_avoid_states):\n            for word_id in state.avoid():\n                if word_id > 0:\n                    to_avoid.add((i, word_id))\n        for i, state in enumerate(self.local_avoid_states):\n            for word_id in state.avoid():\n                if word_id > 0:\n                    to_avoid.add((i, word_id))\n\n        return tuple(zip(*to_avoid))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef allowed(self) -> Set[int]:\n        items = set()  # type: Set[int]\n        # Add extensions of a started-but-incomplete sequential constraint\n        if self.last_met != -1 and self.is_sequence[self.last_met] == 1:\n            word_id = self.constraints[self.last_met + 1]\n            if word_id != self.eos_id or self.num_needed() == 1:\n                items.add(word_id)\n\n        # Add all constraints that aren't non-initial sequences\n        else:\n            for i, word_id in enumerate(self.constraints):\n                if not self.met[i] and (i == 0 or not self.is_sequence[i - 1]):\n                    if word_id != self.eos_id or self.num_needed() == 1:\n                        items.add(word_id)\n\n        return items", "response": "Returns the set of constrained words that can follow this one."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if the word ID is valid.", "response": "def is_valid(self, wordid) -> bool:\n        \"\"\"\n        Ensures </s> is only generated when the hypothesis is completed.\n\n        :param wordid: The wordid to validate.\n        :return: True if all constraints are already met or the word ID is not the EOS id.\n        \"\"\"\n        return self.finished() or wordid != self.eos_id or (self.num_needed() == 1 and self.eos_id in self.allowed())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadvancing the internal state of the object based on word_id.", "response": "def advance(self, word_id: int) -> 'ConstrainedHypothesis':\n        \"\"\"\n        Updates the constraints object based on advancing on word_id.\n        There is a complication, in that we may have started but not\n        yet completed a multi-word constraint.  We need to allow constraints\n        to be added as unconstrained words, so if the next word is\n        invalid, we must \"back out\" of the current (incomplete) phrase,\n        re-setting all of its words as unmet.\n\n        :param word_id: The word ID to advance on.\n        :return: A deep copy of the object, advanced on word_id.\n        \"\"\"\n\n        obj = copy.deepcopy(self)\n\n        # First, check if we're updating a sequential constraint.\n        if obj.last_met != -1 and obj.is_sequence[obj.last_met] == 1:\n            if word_id == obj.constraints[obj.last_met + 1]:\n                # Here, the word matches what we expect next in the constraint, so we update everything\n                obj.met[obj.last_met + 1] = True\n                obj.last_met += 1\n            else:\n                # Here, the word is not the expected next word of the constraint, so we back out of the constraint.\n                index = obj.last_met\n                while obj.is_sequence[index]:\n                    obj.met[index] = False\n                    index -= 1\n                obj.last_met = -1\n\n        # If not, check whether we're meeting a single-word constraint\n        else:\n            # Build a list from all constraints of tuples of the\n            # form (constraint, whether it's a non-initial sequential, whether it's been met)\n            constraint_tuples = list(zip(obj.constraints, [False] + obj.is_sequence[:-1], obj.met))\n            # We are searching for an unmet constraint (word_id) that is not the middle of a phrase and is not met\n            query = (word_id, False, False)\n            try:\n                pos = constraint_tuples.index(query)\n                obj.met[pos] = True\n                obj.last_met = pos\n            except ValueError:\n                # query not found; identical but duplicated object will be returned\n                pass\n\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef temporarily_write_git_hash(git_hash, filename=os.path.join('sockeye', 'git_version.py')):\n    content = \"\"\"\n# This file is automatically generated in setup.py\ngit_hash = \"%s\"\n\"\"\" % git_hash\n    if os.path.exists(filename):\n        raise RuntimeError(\"%s already exists, will not overwrite\" % filename)\n    with open(filename, \"w\") as out:\n        out.write(content)\n    try:\n        yield\n    except:\n        raise\n    finally:\n        os.remove(filename)", "response": "Temporarily create a module git_version in sockeye so that it will be included when installing and packaging."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsave the current configuration to the specified folder.", "response": "def save_config(self, folder: str):\n        \"\"\"\n        Saves model configuration to <folder>/config\n\n        :param folder: Destination folder.\n        \"\"\"\n        fname = os.path.join(folder, C.CONFIG_NAME)\n        self.config.save(fname)\n        logger.info('Saved config to \"%s\"', fname)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load_config(fname: str) -> ModelConfig:\n        config = ModelConfig.load(fname)\n        logger.info('ModelConfig loaded from \"%s\"', fname)\n        return cast(ModelConfig, config)", "response": "Loads model configuration from file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef save_params_to_file(self, fname: str):\n        if self.aux_params is not None:\n            utils.save_params(self.params.copy(), fname, self.aux_params.copy())\n        else:\n            utils.save_params(self.params.copy(), fname)\n        logging.info('Saved params to \"%s\"', fname)", "response": "Saves model parameters to file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_params_from_file(self, fname: str):\n        utils.check_condition(os.path.exists(fname), \"No model parameter file found under %s. \"\n                                                     \"This is either not a model directory or the first training \"\n                                                     \"checkpoint has not happened yet.\" % fname)\n        self.params, self.aux_params = utils.load_params(fname)\n        utils.check_condition(all(name.startswith(self.prefix) for name in self.params.keys()),\n                              \"Not all parameter names start with model prefix '%s'\" % self.prefix)\n        utils.check_condition(all(name.startswith(self.prefix) for name in self.aux_params.keys()),\n                              \"Not all auxiliary parameter names start with model prefix '%s'\" % self.prefix)\n        logger.info('Loaded params from \"%s\"', fname)", "response": "Loads and sets model parameters from a file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef save_version(folder: str):\n        fname = os.path.join(folder, C.VERSION_NAME)\n        with open(fname, \"w\") as out:\n            out.write(__version__)", "response": "Saves version to <folder >."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_embed_weights(self, prefix: str) -> Tuple[mx.sym.Symbol, mx.sym.Symbol, mx.sym.Symbol]:\n        w_embed_source = mx.sym.Variable(prefix + C.SOURCE_EMBEDDING_PREFIX + \"weight\",\n                                         shape=(self.config.config_embed_source.vocab_size,\n                                                self.config.config_embed_source.num_embed))\n        w_embed_target = mx.sym.Variable(prefix + C.TARGET_EMBEDDING_PREFIX + \"weight\",\n                                         shape=(self.config.config_embed_target.vocab_size,\n                                                self.config.config_embed_target.num_embed))\n\n        w_out_target = mx.sym.Variable(prefix + \"target_output_weight\", dtype='float32',\n                                       shape=(self.config.vocab_target_size, self.decoder.get_num_hidden()))\n\n        if self.config.weight_tying:\n            if C.WEIGHT_TYING_SRC in self.config.weight_tying_type \\\n                    and C.WEIGHT_TYING_TRG in self.config.weight_tying_type:\n                logger.info(\"Tying the source and target embeddings.\")\n                w_embed_source = w_embed_target = mx.sym.Variable(prefix + C.SHARED_EMBEDDING_PREFIX + \"weight\",\n                                                                  shape=(self.config.config_embed_source.vocab_size,\n                                                                         self.config.config_embed_source.num_embed))\n\n            if C.WEIGHT_TYING_SOFTMAX in self.config.weight_tying_type:\n                logger.info(\"Tying the target embeddings and output layer parameters.\")\n                utils.check_condition(self.config.config_embed_target.num_embed == self.decoder.get_num_hidden(),\n                                      \"Weight tying requires target embedding size and decoder hidden size \" +\n                                      \"to be equal: %d vs. %d\" % (self.config.config_embed_target.num_embed,\n                                                                  self.decoder.get_num_hidden()))\n                w_out_target = w_embed_target\n\n        self._embed_weight_source_name = None\n        if w_embed_source is not None:\n            self._embed_weight_source_name = w_embed_source.name\n        self._embed_weight_target_name = w_embed_target.name\n        self._out_weight_target_name = w_out_target.name\n        return w_embed_source, w_embed_target, w_out_target", "response": "Returns the source and target embeddings and outputs layer parameters for source and target embeddings."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_models(context: mx.context.Context,\n                max_input_len: Optional[int],\n                beam_size: int,\n                batch_size: int,\n                model_folders: List[str],\n                checkpoints: Optional[List[int]] = None,\n                softmax_temperature: Optional[float] = None,\n                max_output_length_num_stds: int = C.DEFAULT_NUM_STD_MAX_OUTPUT_LENGTH,\n                decoder_return_logit_inputs: bool = False,\n                cache_output_layer_w_b: bool = False,\n                forced_max_output_len: Optional[int] = None,\n                override_dtype: Optional[str] = None,\n                output_scores: bool = False,\n                sampling: bool = False) -> Tuple[List[InferenceModel],\n                                                 List[vocab.Vocab],\n                                                 vocab.Vocab]:\n    \"\"\"\n    Loads a list of models for inference.\n\n    :param context: MXNet context to bind modules to.\n    :param max_input_len: Maximum input length.\n    :param beam_size: Beam size.\n    :param batch_size: Batch size.\n    :param model_folders: List of model folders to load models from.\n    :param checkpoints: List of checkpoints to use for each model in model_folders. Use None to load best checkpoint.\n    :param softmax_temperature: Optional parameter to control steepness of softmax distribution.\n    :param max_output_length_num_stds: Number of standard deviations to add to mean target-source length ratio\n           to compute maximum output length.\n    :param decoder_return_logit_inputs: Model decoders return inputs to logit computation instead of softmax over target\n                                        vocabulary.  Used when logits/softmax are handled separately.\n    :param cache_output_layer_w_b: Models cache weights and biases for logit computation as NumPy arrays (used with\n                                   restrict lexicon).\n    :param forced_max_output_len: An optional overwrite of the maximum output length.\n    :param override_dtype: Overrides dtype of encoder and decoder defined at training time to a different one.\n    :param output_scores: Whether the scores will be needed as outputs. If True, scores will be normalized, negative\n           log probabilities. If False, scores will be negative, raw logit activations if decoding with beam size 1\n           and a single model.\n    :param sampling: True if the model is sampling instead of doing normal topk().\n    :return: List of models, source vocabulary, target vocabulary, source factor vocabularies.\n    \"\"\"\n    logger.info(\"Loading %d model(s) from %s ...\", len(model_folders), model_folders)\n    load_time_start = time.time()\n    models = []  # type: List[InferenceModel]\n    source_vocabs = []  # type: List[List[vocab.Vocab]]\n    target_vocabs = []  # type: List[vocab.Vocab]\n\n    if checkpoints is None:\n        checkpoints = [None] * len(model_folders)\n    else:\n        utils.check_condition(len(checkpoints) == len(model_folders), \"Must provide checkpoints for each model\")\n\n    skip_softmax = False\n    # performance tweak: skip softmax for a single model, decoding with beam size 1, when not sampling and no scores are required in output.\n    if len(model_folders) == 1 and beam_size == 1 and not output_scores and not sampling:\n        skip_softmax = True\n        logger.info(\"Enabled skipping softmax for a single model and greedy decoding.\")\n\n    for model_folder, checkpoint in zip(model_folders, checkpoints):\n        model_source_vocabs = vocab.load_source_vocabs(model_folder)\n        model_target_vocab = vocab.load_target_vocab(model_folder)\n        source_vocabs.append(model_source_vocabs)\n        target_vocabs.append(model_target_vocab)\n\n        model_version = utils.load_version(os.path.join(model_folder, C.VERSION_NAME))\n        logger.info(\"Model version: %s\", model_version)\n        utils.check_version(model_version)\n        model_config = model.SockeyeModel.load_config(os.path.join(model_folder, C.CONFIG_NAME))\n\n        logger.info(\"Disabling dropout layers for performance reasons\")\n        model_config.disable_dropout()\n\n        if override_dtype is not None:\n            model_config.config_encoder.dtype = override_dtype\n            model_config.config_decoder.dtype = override_dtype\n            if override_dtype == C.DTYPE_FP16:\n                logger.warning('Experimental feature \\'override_dtype=float16\\' has been used. '\n                               'This feature may be removed or change its behaviour in future. '\n                               'DO NOT USE IT IN PRODUCTION!')\n\n        if checkpoint is None:\n            params_fname = os.path.join(model_folder, C.PARAMS_BEST_NAME)\n        else:\n            params_fname = os.path.join(model_folder, C.PARAMS_NAME % checkpoint)\n\n        inference_model = InferenceModel(config=model_config,\n                                         params_fname=params_fname,\n                                         context=context,\n                                         beam_size=beam_size,\n                                         softmax_temperature=softmax_temperature,\n                                         decoder_return_logit_inputs=decoder_return_logit_inputs,\n                                         cache_output_layer_w_b=cache_output_layer_w_b,\n                                         skip_softmax=skip_softmax)\n        utils.check_condition(inference_model.num_source_factors == len(model_source_vocabs),\n                              \"Number of loaded source vocabularies (%d) does not match \"\n                              \"number of source factors for model '%s' (%d)\" % (len(model_source_vocabs), model_folder,\n                                                                                inference_model.num_source_factors))\n        models.append(inference_model)\n\n    utils.check_condition(vocab.are_identical(*target_vocabs), \"Target vocabulary ids do not match\")\n    first_model_vocabs = source_vocabs[0]\n    for fi in range(len(first_model_vocabs)):\n        utils.check_condition(vocab.are_identical(*[source_vocabs[i][fi] for i in range(len(source_vocabs))]),\n                              \"Source vocabulary ids do not match. Factor %d\" % fi)\n\n    source_with_eos = models[0].source_with_eos\n    utils.check_condition(all(source_with_eos == m.source_with_eos for m in models),\n                          \"All models must agree on using source-side EOS symbols or not. \"\n                          \"Did you try combining models trained with different versions?\")\n\n    # set a common max_output length for all models.\n    max_input_len, get_max_output_length = models_max_input_output_length(models,\n                                                                          max_output_length_num_stds,\n                                                                          max_input_len,\n                                                                          forced_max_output_len=forced_max_output_len)\n\n    for inference_model in models:\n        inference_model.initialize(batch_size, max_input_len, get_max_output_length)\n\n    load_time = time.time() - load_time_start\n    logger.info(\"%d model(s) loaded in %.4fs\", len(models), load_time)\n    return models, source_vocabs[0], target_vocabs[0]", "response": "Loads a list of inference models from the specified model folders."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef models_max_input_output_length(models: List[InferenceModel],\n                                   num_stds: int,\n                                   forced_max_input_len: Optional[int] = None,\n                                   forced_max_output_len: Optional[int] = None) -> Tuple[int, Callable]:\n    \"\"\"\n    Returns a function to compute maximum output length given a fixed number of standard deviations as a\n    safety margin, and the current input length.\n    Mean and std are taken from the model with the largest values to allow proper ensembling of models\n    trained on different data sets.\n\n    :param models: List of models.\n    :param num_stds: Number of standard deviations to add as a safety margin. If -1, returned maximum output lengths\n                     will always be 2 * input_length.\n    :param forced_max_input_len: An optional overwrite of the maximum input length.\n    :param forced_max_output_len: An optional overwrite of the maximum output length.\n    :return: The maximum input length and a function to get the output length given the input length.\n    \"\"\"\n    max_mean = max(model.length_ratio_mean for model in models)\n    max_std = max(model.length_ratio_std for model in models)\n\n    supported_max_seq_len_source = min((model.max_supported_seq_len_source for model in models\n                                        if model.max_supported_seq_len_source is not None),\n                                       default=None)\n    supported_max_seq_len_target = min((model.max_supported_seq_len_target for model in models\n                                        if model.max_supported_seq_len_target is not None),\n                                       default=None)\n    training_max_seq_len_source = min(model.training_max_seq_len_source for model in models)\n\n    return get_max_input_output_length(supported_max_seq_len_source,\n                                       supported_max_seq_len_target,\n                                       training_max_seq_len_source,\n                                       length_ratio_mean=max_mean,\n                                       length_ratio_std=max_std,\n                                       num_stds=num_stds,\n                                       forced_max_input_len=forced_max_input_len,\n                                       forced_max_output_len=forced_max_output_len)", "response": "Returns a function to compute the maximum output length given a list of models."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_max_input_output_length(supported_max_seq_len_source: Optional[int],\n                                supported_max_seq_len_target: Optional[int],\n                                training_max_seq_len_source: Optional[int],\n                                length_ratio_mean: float,\n                                length_ratio_std: float,\n                                num_stds: int,\n                                forced_max_input_len: Optional[int] = None,\n                                forced_max_output_len: Optional[int] = None) -> Tuple[int, Callable]:\n    \"\"\"\n    Returns a function to compute maximum output length given a fixed number of standard deviations as a\n    safety margin, and the current input length. It takes into account optional maximum source and target lengths.\n\n    :param supported_max_seq_len_source: The maximum source length supported by the models.\n    :param supported_max_seq_len_target: The maximum target length supported by the models.\n    :param training_max_seq_len_source: The maximum source length observed during training.\n    :param length_ratio_mean: The mean of the length ratio that was calculated on the raw sequences with special\n           symbols such as EOS or BOS.\n    :param length_ratio_std: The standard deviation of the length ratio.\n    :param num_stds: The number of standard deviations the target length may exceed the mean target length (as long as\n           the supported maximum length allows for this).\n    :param forced_max_input_len: An optional overwrite of the maximum input length.\n    :param forced_max_output_len: An optional overwrite of the maximum out length.\n    :return: The maximum input length and a function to get the output length given the input length.\n    \"\"\"\n    space_for_bos = 1\n    space_for_eos = 1\n\n    if num_stds < 0:\n        factor = C.TARGET_MAX_LENGTH_FACTOR  # type: float\n    else:\n        factor = length_ratio_mean + (length_ratio_std * num_stds)\n\n    if forced_max_input_len is None:\n        # Make sure that if there is a hard constraint on the maximum source or target length we never exceed this\n        # constraint. This is for example the case for learned positional embeddings, which are only defined for the\n        # maximum source and target sequence length observed during training.\n        if supported_max_seq_len_source is not None and supported_max_seq_len_target is None:\n            max_input_len = supported_max_seq_len_source\n        elif supported_max_seq_len_source is None and supported_max_seq_len_target is not None:\n            max_output_len = supported_max_seq_len_target - space_for_bos - space_for_eos\n            if np.ceil(factor * training_max_seq_len_source) > max_output_len:\n                max_input_len = int(np.floor(max_output_len / factor))\n            else:\n                max_input_len = training_max_seq_len_source\n        elif supported_max_seq_len_source is not None or supported_max_seq_len_target is not None:\n            max_output_len = supported_max_seq_len_target - space_for_bos - space_for_eos\n            if np.ceil(factor * supported_max_seq_len_source) > max_output_len:\n                max_input_len = int(np.floor(max_output_len / factor))\n            else:\n                max_input_len = supported_max_seq_len_source\n        else:\n            # Any source/target length is supported and max_input_len was not manually set, therefore we use the\n            # maximum length from training.\n            max_input_len = training_max_seq_len_source\n    else:\n        max_input_len = forced_max_input_len\n\n    def get_max_output_length(input_length: int):\n        \"\"\"\n        Returns the maximum output length for inference given the input length.\n        Explicitly includes space for BOS and EOS sentence symbols in the target sequence, because we assume\n        that the mean length ratio computed on the training data do not include these special symbols.\n        (see data_io.analyze_sequence_lengths)\n        \"\"\"\n        if forced_max_output_len is not None:\n            return forced_max_output_len\n        else:\n            return int(np.ceil(factor * input_length)) + space_for_bos + space_for_eos\n\n    return max_input_len, get_max_output_length", "response": "This function returns a function to compute the maximum input length given the input length."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef make_input_from_plain_string(sentence_id: SentenceId, string: str) -> TranslatorInput:\n    return TranslatorInput(sentence_id, tokens=list(data_io.get_tokens(string)), factors=None)", "response": "Creates a TranslatorInput object from a plain string."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef make_input_from_json_string(sentence_id: SentenceId,\n                                json_string: str,\n                                translator: 'Translator') -> TranslatorInput:\n    \"\"\"\n    Returns a TranslatorInput object from a JSON object, serialized as a string.\n\n    :param sentence_id: Sentence id.\n    :param json_string: A JSON object serialized as a string that must contain a key \"text\", mapping to the input text,\n           and optionally a key \"factors\" that maps to a list of strings, each of which representing a factor sequence\n           for the input text. Constraints and an avoid list can also be added through the \"constraints\" and \"avoid\"\n           keys.\n    :param translator: A translator object.\n    :return: A TranslatorInput.\n    \"\"\"\n    try:\n        jobj = json.loads(json_string, encoding=C.JSON_ENCODING)\n        return make_input_from_dict(sentence_id, jobj, translator)\n\n    except Exception as e:\n        logger.exception(e, exc_info=True) if not is_python34() else logger.error(e)  # type: ignore\n        return _bad_input(sentence_id, reason=json_string)", "response": "Returns a TranslatorInput object from a JSON string."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef make_input_from_dict(sentence_id: SentenceId,\n                         input_dict: Dict,\n                         translator: 'Translator') -> TranslatorInput:\n    \"\"\"\n    Returns a TranslatorInput object from a JSON object, serialized as a string.\n\n    :param sentence_id: Sentence id.\n    :param input_dict: A dict that must contain a key \"text\", mapping to the input text, and optionally a key \"factors\"\n           that maps to a list of strings, each of which representing a factor sequence for the input text.\n           Constraints and an avoid list can also be added through the \"constraints\" and \"avoid\" keys.\n    :param translator: A translator object.\n    :return: A TranslatorInput.\n    \"\"\"\n    try:\n        tokens = input_dict[C.JSON_TEXT_KEY]\n        tokens = list(data_io.get_tokens(tokens))\n        factors = input_dict.get(C.JSON_FACTORS_KEY)\n        if isinstance(factors, list):\n            factors = [list(data_io.get_tokens(factor)) for factor in factors]\n            lengths = [len(f) for f in factors]\n            if not all(length == len(tokens) for length in lengths):\n                logger.error(\"Factors have different length than input text: %d vs. %s\", len(tokens), str(lengths))\n                return _bad_input(sentence_id, reason=str(input_dict))\n\n        # Lexicon for vocabulary selection/restriction:\n        # This is only populated when using multiple lexicons, in which case the\n        # restrict_lexicon key must exist and the value (name) must map to one\n        # of the translator's known lexicons.\n        restrict_lexicon = None\n        restrict_lexicon_name = input_dict.get(C.JSON_RESTRICT_LEXICON_KEY)\n        if isinstance(translator.restrict_lexicon, dict):\n            if restrict_lexicon_name is None:\n                logger.error(\"Must specify restrict_lexicon when using multiple lexicons. Choices: %s\"\n                             % ' '.join(sorted(translator.restrict_lexicon)))\n                return _bad_input(sentence_id, reason=str(input_dict))\n            restrict_lexicon = translator.restrict_lexicon.get(restrict_lexicon_name, None)\n            if restrict_lexicon is None:\n                logger.error(\"Unknown restrict_lexicon '%s'. Choices: %s\"\n                             % (restrict_lexicon_name, ' '.join(sorted(translator.restrict_lexicon))))\n                return _bad_input(sentence_id, reason=str(input_dict))\n\n        # List of phrases to prevent from occuring in the output\n        avoid_list = input_dict.get(C.JSON_AVOID_KEY)\n\n        # List of phrases that must appear in the output\n        constraints = input_dict.get(C.JSON_CONSTRAINTS_KEY)\n\n        # If there is overlap between positive and negative constraints, assume the user wanted\n        # the words, and so remove them from the avoid_list (negative constraints)\n        if constraints is not None and avoid_list is not None:\n            avoid_set = set(avoid_list)\n            overlap = set(constraints).intersection(avoid_set)\n            if len(overlap) > 0:\n                logger.warning(\"Overlap between constraints and avoid set, dropping the overlapping avoids\")\n                avoid_list = list(avoid_set.difference(overlap))\n\n        # Convert to a list of tokens\n        if isinstance(avoid_list, list):\n            avoid_list = [list(data_io.get_tokens(phrase)) for phrase in avoid_list]\n        if isinstance(constraints, list):\n            constraints = [list(data_io.get_tokens(constraint)) for constraint in constraints]\n\n        return TranslatorInput(sentence_id=sentence_id, tokens=tokens, factors=factors,\n                               restrict_lexicon=restrict_lexicon, constraints=constraints,\n                               avoid_list=avoid_list, pass_through_dict=input_dict)\n\n    except Exception as e:\n        logger.exception(e, exc_info=True) if not is_python34() else logger.error(e)  # type: ignore\n        return _bad_input(sentence_id, reason=str(input_dict))", "response": "Creates a TranslatorInput object from a JSON object."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a TranslatorInput object from a string with additional factors on a token level separated by delimiter.", "response": "def make_input_from_factored_string(sentence_id: SentenceId,\n                                    factored_string: str,\n                                    translator: 'Translator',\n                                    delimiter: str = C.DEFAULT_FACTOR_DELIMITER) -> TranslatorInput:\n    \"\"\"\n    Returns a TranslatorInput object from a string with factor annotations on a token level, separated by delimiter.\n    If translator does not require any source factors, the string is parsed as a plain token string.\n\n    :param sentence_id: Sentence id.\n    :param factored_string: An input string with additional factors per token, separated by delimiter.\n    :param translator: A translator object.\n    :param delimiter: A factor delimiter. Default: '|'.\n    :return: A TranslatorInput.\n    \"\"\"\n    utils.check_condition(bool(delimiter) and not delimiter.isspace(),\n                          \"Factor delimiter can not be whitespace or empty.\")\n\n    model_num_source_factors = translator.num_source_factors\n\n    if model_num_source_factors == 1:\n        return make_input_from_plain_string(sentence_id=sentence_id, string=factored_string)\n\n    tokens = []  # type: Tokens\n    factors = [[] for _ in range(model_num_source_factors - 1)]  # type: List[Tokens]\n    for token_id, token in enumerate(data_io.get_tokens(factored_string)):\n        pieces = token.split(delimiter)\n\n        if not all(pieces) or len(pieces) != model_num_source_factors:\n            logger.error(\"Failed to parse %d factors at position %d ('%s') in '%s'\" % (model_num_source_factors,\n                                                                                       token_id, token,\n                                                                                       factored_string.strip()))\n            return _bad_input(sentence_id, reason=factored_string)\n\n        tokens.append(pieces[0])\n        for i, factor in enumerate(factors):\n            factors[i].append(pieces[i + 1])\n\n    return TranslatorInput(sentence_id=sentence_id, tokens=tokens, factors=factors)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a TranslatorInput object from multiple strings.", "response": "def make_input_from_multiple_strings(sentence_id: SentenceId, strings: List[str]) -> TranslatorInput:\n    \"\"\"\n    Returns a TranslatorInput object from multiple strings, where the first element corresponds to the surface tokens\n    and the remaining elements to additional factors. All strings must parse into token sequences of the same length.\n\n    :param sentence_id: Sentence id.\n    :param strings: A list of strings representing a factored input sequence.\n    :return: A TranslatorInput.\n    \"\"\"\n    if not bool(strings):\n        return TranslatorInput(sentence_id=sentence_id, tokens=[], factors=None)\n\n    tokens = list(data_io.get_tokens(strings[0]))\n    factors = [list(data_io.get_tokens(factor)) for factor in strings[1:]]\n    if not all(len(factor) == len(tokens) for factor in factors):\n        logger.error(\"Length of string sequences do not match: '%s'\", strings)\n        return _bad_input(sentence_id, reason=str(strings))\n    return TranslatorInput(sentence_id=sentence_id, tokens=tokens, factors=factors)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn an empty translation.", "response": "def empty_translation(add_nbest: bool = False) -> Translation:\n    \"\"\"\n    Return an empty translation.\n\n    :param add_nbest: Include (empty) nbest_translations in the translation object.\n    \"\"\"\n    return Translation(target_ids=[],\n                       attention_matrix=np.asarray([[0]]),\n                       score=-np.inf,\n                       nbest_translations=NBestTranslations([], [], []) if add_nbest else None)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncombining nbest translations through concatenation.", "response": "def _concat_nbest_translations(translations: List[Translation], stop_ids: Set[int],\n                               length_penalty: LengthPenalty,\n                               brevity_penalty: Optional[BrevityPenalty] = None) -> Translation:\n    \"\"\"\n    Combines nbest translations through concatenation.\n\n    :param translations: A list of translations (sequence starting with BOS symbol,\n        attention_matrix), score and length.\n    :param stop_ids: The EOS symbols.\n    :param length_penalty: LengthPenalty.\n    :param brevity_penalty: Optional BrevityPenalty.\n    :return: A concatenation of the translations with a score.\n    \"\"\"\n    expanded_translations = (_expand_nbest_translation(translation) for translation in translations)\n\n    concatenated_translations = []  # type: List[Translation]\n\n    for translations_to_concat in zip(*expanded_translations):\n        concatenated_translations.append(_concat_translations(translations=list(translations_to_concat),\n                                                              stop_ids=stop_ids,\n                                                              length_penalty=length_penalty,\n                                                              brevity_penalty=brevity_penalty))\n\n    return _reduce_nbest_translations(concatenated_translations)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _reduce_nbest_translations(nbest_translations_list: List[Translation]) -> Translation:\n    best_translation = nbest_translations_list[0]\n\n    sequences = [translation.target_ids for translation in nbest_translations_list]\n    attention_matrices = [translation.attention_matrix for translation in nbest_translations_list]\n    scores = [translation.score for translation in nbest_translations_list]\n\n    nbest_translations = NBestTranslations(sequences, attention_matrices, scores)\n\n    return Translation(best_translation.target_ids,\n                       best_translation.attention_matrix,\n                       best_translation.score,\n                       best_translation.beam_histories,\n                       nbest_translations,\n                       best_translation.estimated_reference_length)", "response": "Reduces nbest translations into a single Translation object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nexpanding nbest translations in a single Translation object to one Translation object per nbest translation.", "response": "def _expand_nbest_translation(translation: Translation) -> List[Translation]:\n    \"\"\"\n    Expand nbest translations in a single Translation object to one Translation\n        object per nbest translation.\n\n    :param translation: A Translation object.\n    :return: A list of Translation objects.\n    \"\"\"\n    nbest_list = []  # type = List[Translation]\n    for target_ids, attention_matrix, score in zip(translation.nbest_translations.target_ids_list,\n                                                   translation.nbest_translations.attention_matrices,\n                                                   translation.nbest_translations.scores):\n        nbest_list.append(Translation(target_ids, attention_matrix, score, translation.beam_histories,\n                                      estimated_reference_length=translation.estimated_reference_length))\n\n    return nbest_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _concat_translations(translations: List[Translation],\n                         stop_ids: Set[int],\n                         length_penalty: LengthPenalty,\n                         brevity_penalty: Optional[BrevityPenalty] = None) -> Translation:\n    \"\"\"\n    Combines translations through concatenation.\n\n    :param translations: A list of translations (sequence starting with BOS symbol, attention_matrix), score and length.\n    :param stop_ids: The EOS symbols.\n    :param length_penalty: Instance of the LengthPenalty class initialized with alpha and beta.\n    :param brevity_penalty: Optional Instance of the BrevityPenalty class initialized with a brevity weight.\n    :return: A concatenation of the translations with a score.\n    \"\"\"\n    # Concatenation of all target ids without BOS and EOS\n    target_ids = []\n    attention_matrices = []\n    beam_histories = []  # type: List[BeamHistory]\n    estimated_reference_length = None  # type: float\n\n    for idx, translation in enumerate(translations):\n        if idx == len(translations) - 1:\n            target_ids.extend(translation.target_ids)\n            attention_matrices.append(translation.attention_matrix)\n        else:\n            if translation.target_ids[-1] in stop_ids:\n                target_ids.extend(translation.target_ids[:-1])\n                attention_matrices.append(translation.attention_matrix[:-1, :])\n            else:\n                target_ids.extend(translation.target_ids)\n                attention_matrices.append(translation.attention_matrix)\n        beam_histories.extend(translation.beam_histories)\n        if translation.estimated_reference_length is not None:\n            if estimated_reference_length is None:\n                estimated_reference_length = translation.estimated_reference_length\n            else:\n                estimated_reference_length += translation.estimated_reference_length\n    # Combine attention matrices:\n    attention_shapes = [attention_matrix.shape for attention_matrix in attention_matrices]\n    attention_matrix_combined = np.zeros(np.sum(np.asarray(attention_shapes), axis=0))\n    pos_t, pos_s = 0, 0\n    for attention_matrix, (len_t, len_s) in zip(attention_matrices, attention_shapes):\n        attention_matrix_combined[pos_t:pos_t + len_t, pos_s:pos_s + len_s] = attention_matrix\n        pos_t += len_t\n        pos_s += len_s\n\n    def _brevity_penalty(hypothesis_length, reference_length):\n        return 0.0 if brevity_penalty is None else brevity_penalty.get(hypothesis_length, reference_length)\n\n    # Unnormalize + sum and renormalize the score:\n    score = sum((translation.score + _brevity_penalty(len(translation.target_ids), translation.estimated_reference_length)) \\\n                    * length_penalty.get(len(translation.target_ids))\n                 for translation in translations)\n    score = score / length_penalty.get(len(target_ids)) - _brevity_penalty(len(target_ids), estimated_reference_length)\n    return Translation(target_ids, attention_matrix_combined, score, beam_histories,\n                       estimated_reference_length=estimated_reference_length)", "response": "Combines the translations into a single object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef initialize(self, max_batch_size: int, max_input_length: int, get_max_output_length_function: Callable):\n        self.max_batch_size = max_batch_size\n        self.max_input_length = max_input_length\n        if self.max_input_length > self.training_max_seq_len_source:\n            logger.warning(\"Model was only trained with sentences up to a length of %d, \"\n                           \"but a max_input_len of %d is used.\",\n                           self.training_max_seq_len_source, self.max_input_length)\n        self.get_max_output_length = get_max_output_length_function\n\n        # check the maximum supported length of the encoder & decoder:\n        if self.max_supported_seq_len_source is not None:\n            utils.check_condition(self.max_input_length <= self.max_supported_seq_len_source,\n                                  \"Encoder only supports a maximum length of %d\" % self.max_supported_seq_len_source)\n        if self.max_supported_seq_len_target is not None:\n            decoder_max_len = self.get_max_output_length(max_input_length)\n            utils.check_condition(decoder_max_len <= self.max_supported_seq_len_target,\n                                  \"Decoder only supports a maximum length of %d, but %d was requested. Note that the \"\n                                  \"maximum output length depends on the input length and the source/target length \"\n                                  \"ratio observed during training.\" % (self.max_supported_seq_len_target,\n                                                                       decoder_max_len))\n\n        self.encoder_module, self.encoder_default_bucket_key = self._get_encoder_module()\n        self.decoder_module, self.decoder_default_bucket_key = self._get_decoder_module()\n\n        max_encoder_data_shapes = self._get_encoder_data_shapes(self.encoder_default_bucket_key,\n                                                                self.max_batch_size)\n        max_decoder_data_shapes = self._get_decoder_data_shapes(self.decoder_default_bucket_key,\n                                                                self.max_batch_size * self.beam_size)\n        self.encoder_module.bind(data_shapes=max_encoder_data_shapes, for_training=False, grad_req=\"null\")\n        self.decoder_module.bind(data_shapes=max_decoder_data_shapes, for_training=False, grad_req=\"null\")\n\n        self.load_params_from_file(self.params_fname)\n        self.encoder_module.init_params(arg_params=self.params, aux_params=self.aux_params, allow_missing=False)\n        self.decoder_module.init_params(arg_params=self.params, aux_params=self.aux_params, allow_missing=False)\n\n        if self.cache_output_layer_w_b:\n            if self.output_layer.weight_normalization:\n                # precompute normalized output layer weight imperatively\n                assert self.output_layer.weight_norm is not None\n                weight = self.params[self.output_layer.weight_norm.weight.name].as_in_context(self.context)\n                scale = self.params[self.output_layer.weight_norm.scale.name].as_in_context(self.context)\n                self.output_layer_w = self.output_layer.weight_norm(weight, scale)\n            else:\n                self.output_layer_w = self.params[self.output_layer.w.name].as_in_context(self.context)\n            self.output_layer_b = self.params[self.output_layer.b.name].as_in_context(self.context)", "response": "Initializes the internal state of the InferenceModule class."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a BucketingModule for the encoder. Given a source sequence it returns a BucketingModule that returns the initial decoder states and the default decoder states.", "response": "def _get_encoder_module(self) -> Tuple[mx.mod.BucketingModule, int]:\n        \"\"\"\n        Returns a BucketingModule for the encoder. Given a source sequence, it returns\n        the initial decoder states of the model.\n        The bucket key for this module is the length of the source sequence.\n\n        :return: Tuple of encoder module and default bucket key.\n        \"\"\"\n\n        def sym_gen(source_seq_len: int):\n            source = mx.sym.Variable(C.SOURCE_NAME)\n            source_words = source.split(num_outputs=self.num_source_factors, axis=2, squeeze_axis=True)[0]\n            source_length = utils.compute_lengths(source_words)\n\n            # source embedding\n            (source_embed,\n             source_embed_length,\n             source_embed_seq_len) = self.embedding_source.encode(source, source_length, source_seq_len)\n\n            # encoder\n            # source_encoded: (source_encoded_length, batch_size, encoder_depth)\n            (source_encoded,\n             source_encoded_length,\n             source_encoded_seq_len) = self.encoder.encode(source_embed,\n                                                           source_embed_length,\n                                                           source_embed_seq_len)\n\n            # initial decoder states\n            decoder_init_states = self.decoder.init_states(source_encoded,\n                                                           source_encoded_length,\n                                                           source_encoded_seq_len)\n\n            data_names = [C.SOURCE_NAME]\n            label_names = []  # type: List[str]\n\n            # predict length ratios\n            predicted_length_ratios = []  # type: List[mx.nd.NDArray]\n            if self.length_ratio is not None:\n                # predicted_length_ratios: List[(n, 1)]\n                predicted_length_ratios = [self.length_ratio(source_encoded, source_encoded_length)]\n\n            return mx.sym.Group(decoder_init_states + predicted_length_ratios), data_names, label_names\n\n        default_bucket_key = self.max_input_length\n        module = mx.mod.BucketingModule(sym_gen=sym_gen,\n                                        default_bucket_key=default_bucket_key,\n                                        context=self.context)\n        return module, default_bucket_key"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a BucketingModule for a single decoder step.", "response": "def _get_decoder_module(self) -> Tuple[mx.mod.BucketingModule, Tuple[int, int]]:\n        \"\"\"\n        Returns a BucketingModule for a single decoder step.\n        Given previously predicted word and previous decoder states, it returns\n        a distribution over the next predicted word and the next decoder states.\n        The bucket key for this module is the length of the source sequence\n        and the current time-step in the inference procedure (e.g. beam search).\n        The latter corresponds to the current length of the target sequences.\n\n        :return: Tuple of decoder module and default bucket key.\n        \"\"\"\n\n        def sym_gen(bucket_key: Tuple[int, int]):\n            \"\"\"\n            Returns either softmax output (probs over target vocabulary) or inputs to logit\n            computation, controlled by decoder_return_logit_inputs\n            \"\"\"\n            source_seq_len, decode_step = bucket_key\n            source_embed_seq_len = self.embedding_source.get_encoded_seq_len(source_seq_len)\n            source_encoded_seq_len = self.encoder.get_encoded_seq_len(source_embed_seq_len)\n\n            self.decoder.reset()\n            target_prev = mx.sym.Variable(C.TARGET_NAME)\n            states = self.decoder.state_variables(decode_step)\n            state_names = [state.name for state in states]\n\n            # embedding for previous word\n            # (batch_size, num_embed)\n            target_embed_prev, _, _ = self.embedding_target.encode(data=target_prev, data_length=None, seq_len=1)\n\n            # decoder\n            # target_decoded: (batch_size, decoder_depth)\n            (target_decoded,\n             attention_probs,\n             states) = self.decoder.decode_step(decode_step,\n                                                target_embed_prev,\n                                                source_encoded_seq_len,\n                                                *states)\n\n            if self.decoder_return_logit_inputs:\n                # skip output layer in graph\n                outputs = mx.sym.identity(target_decoded, name=C.LOGIT_INPUTS_NAME)\n            else:\n                # logits: (batch_size, target_vocab_size)\n                logits = self.output_layer(target_decoded)\n                if self.softmax_temperature is not None:\n                    logits = logits / self.softmax_temperature\n                if self.skip_softmax:\n                    # skip softmax for greedy decoding\n                    outputs = logits\n                else:\n                    outputs = mx.sym.softmax(data=logits, name=C.SOFTMAX_NAME)\n\n            data_names = [C.TARGET_NAME] + state_names\n            label_names = []  # type: List[str]\n            return mx.sym.Group([outputs, attention_probs] + states), data_names, label_names\n\n        # pylint: disable=not-callable\n        default_bucket_key = (self.max_input_length, self.get_max_output_length(self.max_input_length))\n        module = mx.mod.BucketingModule(sym_gen=sym_gen,\n                                        default_bucket_key=default_bucket_key,\n                                        context=self.context)\n        return module, default_bucket_key"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_encoder_data_shapes(self, bucket_key: int, batch_size: int) -> List[mx.io.DataDesc]:\n        return [mx.io.DataDesc(name=C.SOURCE_NAME,\n                               shape=(batch_size, bucket_key, self.num_source_factors),\n                               layout=C.BATCH_MAJOR)]", "response": "Returns the data shapes of the encoder module."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the data shapes of the decoder module.", "response": "def _get_decoder_data_shapes(self, bucket_key: Tuple[int, int], batch_beam_size: int) -> List[mx.io.DataDesc]:\n        \"\"\"\n        Returns data shapes of the decoder module.\n\n        :param bucket_key: Tuple of (maximum input length, maximum target length).\n        :param batch_beam_size: Batch size * beam size.\n        :return: List of data descriptions.\n        \"\"\"\n        source_max_length, target_max_length = bucket_key\n        return [mx.io.DataDesc(name=C.TARGET_NAME, shape=(batch_beam_size,),\n                               layout=\"NT\")] + self.decoder.state_shapes(batch_beam_size,\n                                                                         target_max_length,\n                                                                         self.encoder.get_encoded_seq_len(\n                                                                             source_max_length),\n                                                                         self.encoder.get_num_hidden())"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns forward pass of the encoder. Encodes source given source length and bucket key. Returns encoder representation of the source, source_length, initial hidden state of decoder RNN, and initial decoder states tiled to beam size. :param source: Integer-coded input tokens. Shape (batch_size, source length, num_source_factors). :param source_max_length: Bucket key. :return: Initial model state.", "response": "def run_encoder(self,\n                    source: mx.nd.NDArray,\n                    source_max_length: int) -> Tuple['ModelState', mx.nd.NDArray]:\n        \"\"\"\n        Runs forward pass of the encoder.\n        Encodes source given source length and bucket key.\n        Returns encoder representation of the source, source_length, initial hidden state of decoder RNN,\n        and initial decoder states tiled to beam size.\n\n        :param source: Integer-coded input tokens. Shape (batch_size, source length, num_source_factors).\n        :param source_max_length: Bucket key.\n        :return: Initial model state.\n        \"\"\"\n        batch_size = source.shape[0]\n        batch = mx.io.DataBatch(data=[source],\n                                label=None,\n                                bucket_key=source_max_length,\n                                provide_data=self._get_encoder_data_shapes(source_max_length, batch_size))\n\n        self.encoder_module.forward(data_batch=batch, is_train=False)\n        decoder_init_states = self.encoder_module.get_outputs()\n\n        if self.length_ratio is not None:\n            estimated_length_ratio = decoder_init_states[-1]\n            estimated_length_ratio = mx.nd.repeat(estimated_length_ratio, repeats=self.beam_size, axis=0)\n            decoder_init_states = decoder_init_states[:-1]\n        else:\n            estimated_length_ratio = None\n            decoder_init_states = decoder_init_states\n        # replicate encoder/init module results beam size times\n        decoder_init_states = [mx.nd.repeat(s, repeats=self.beam_size, axis=0) for s in decoder_init_states]\n        return ModelState(decoder_init_states), estimated_length_ratio"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run_decoder(self,\n                    prev_word: mx.nd.NDArray,\n                    bucket_key: Tuple[int, int],\n                    model_state: 'ModelState') -> Tuple[mx.nd.NDArray, mx.nd.NDArray, 'ModelState']:\n        \"\"\"\n        Runs forward pass of the single-step decoder.\n\n        :param prev_word: Previous word ids. Shape: (batch*beam,).\n        :param bucket_key: Bucket key.\n        :param model_state: Model states.\n        :return: Decoder stack output (logit inputs or probability distribution), attention scores, updated model state.\n        \"\"\"\n        batch_beam_size = prev_word.shape[0]\n        batch = mx.io.DataBatch(\n            data=[prev_word.as_in_context(self.context)] + model_state.states,\n            label=None,\n            bucket_key=bucket_key,\n            provide_data=self._get_decoder_data_shapes(bucket_key, batch_beam_size))\n        self.decoder_module.forward(data_batch=batch, is_train=False)\n        out, attention_probs, *model_state.states = self.decoder_module.get_outputs()\n        return out, attention_probs, model_state", "response": "Runs the decoder forward pass of the single - step decoder."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef chunks(self, chunk_size: int) -> Generator['TranslatorInput', None, None]:\n\n        if len(self.tokens) > chunk_size and self.constraints is not None:\n            logger.warning(\n                'Input %s has length (%d) that exceeds max input length (%d), '\n                'triggering internal splitting. Placing all target-side constraints '\n                'with the first chunk, which is probably wrong.',\n                self.sentence_id, len(self.tokens), chunk_size)\n\n        for chunk_id, i in enumerate(range(0, len(self), chunk_size)):\n            factors = [factor[i:i + chunk_size] for factor in self.factors] if self.factors is not None else None\n            # Constrained decoding is not supported for chunked TranslatorInputs. As a fall-back, constraints are\n            # assigned to the first chunk\n            constraints = self.constraints if chunk_id == 0 else None\n            pass_through_dict = self.pass_through_dict if chunk_id == 0 else None\n            yield TranslatorInput(sentence_id=self.sentence_id,\n                                  tokens=self.tokens[i:i + chunk_size],\n                                  factors=factors,\n                                  restrict_lexicon=self.restrict_lexicon,\n                                  constraints=constraints,\n                                  avoid_list=self.avoid_list,\n                                  pass_through_dict=pass_through_dict)", "response": "Takes a TranslatorInput and yields TranslatorInputs for each chunk of size chunk_size."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef with_eos(self) -> 'TranslatorInput':\n        return TranslatorInput(sentence_id=self.sentence_id,\n                               tokens=self.tokens + [C.EOS_SYMBOL],\n                               factors=[factor + [C.EOS_SYMBOL] for factor in\n                                        self.factors] if self.factors is not None else None,\n                               restrict_lexicon=self.restrict_lexicon,\n                               constraints=self.constraints,\n                               avoid_list=self.avoid_list,\n                               pass_through_dict=self.pass_through_dict)", "response": "Returns a new TranslatorInput object with EOS appended to the tokens and factors."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef json(self, align_threshold: float = 0.0) -> Dict:\n        _d = self.pass_through_dict  # type: Dict[str, Any]\n        _d['sentence_id'] = self.sentence_id\n        _d['translation'] = self.translation\n        _d['score'] = self.score\n\n        if self.nbest_translations is not None and len(self.nbest_translations) > 1:\n            _d['translations'] = self.nbest_translations\n            _d['scores'] = self.nbest_scores\n            if self.nbest_attention_matrices:\n                extracted_alignments = []\n                for alignment_matrix in self.nbest_attention_matrices:\n                    extracted_alignments.append(list(utils.get_alignments(alignment_matrix, threshold=align_threshold)))\n                _d['alignments'] = extracted_alignments\n\n        return _d", "response": "Returns a dictionary suitable for json. dumps() representing all the keys in the class."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsorting states according to k - best order from last step in beam search.", "response": "def sort_state(self, best_hyp_indices: mx.nd.NDArray):\n        \"\"\"\n        Sorts states according to k-best order from last step in beam search.\n        \"\"\"\n        self.states = [mx.nd.take(ds, best_hyp_indices) for ds in self.states]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get(self, lengths: Union[mx.nd.NDArray, int, float]) -> Union[mx.nd.NDArray, float]:\n        return self.hybrid_forward(None, lengths)", "response": "Calculate the length penalty for the given vector of lengths."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get(self,\n            hyp_lengths: Union[mx.nd.NDArray, int, float],\n            reference_lengths: Optional[Union[mx.nd.NDArray, int, float]]) -> Union[mx.nd.NDArray, float]:\n        \"\"\"\n        Calculate the length penalty for the given vector of lengths.\n\n        :param hyp_lengths: Hypotheses lengths.\n        :param reference_lengths: Reference lengths.\n        :return: The length penalty. A scalar or a matrix (batch_size, 1) depending on the input.\n        \"\"\"\n        if reference_lengths is None:\n            return 0.0\n        else:\n            return self.hybrid_forward(None, hyp_lengths, reference_lengths)", "response": "Calculate the length penalty for the given vector of lengths."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef max_input_length(self) -> int:\n        if self.source_with_eos:\n            return self._max_input_length - C.SPACE_FOR_XOS\n        else:\n            return self._max_input_length", "response": "Returns the maximum input length for the TranslatorInput objects passed to translate."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn averaged and re-normalized log probabilities", "response": "def _log_linear_interpolation(predictions):\n        \"\"\"\n        Returns averaged and re-normalized log probabilities\n        \"\"\"\n        log_probs = utils.average_arrays([p.log() for p in predictions])\n        # pylint: disable=invalid-unary-operand-type\n        return -log_probs.log_softmax()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef translate(self, trans_inputs: List[TranslatorInput], fill_up_batches: bool = True) -> List[TranslatorOutput]:\n        num_inputs = len(trans_inputs)\n        translated_chunks = []  # type: List[IndexedTranslation]\n\n        # split into chunks\n        input_chunks = []  # type: List[IndexedTranslatorInput]\n        for trans_input_idx, trans_input in enumerate(trans_inputs):\n            # bad input\n            if isinstance(trans_input, BadTranslatorInput):\n                translated_chunks.append(IndexedTranslation(input_idx=trans_input_idx, chunk_idx=0,\n                                                            translation=empty_translation(add_nbest=(self.nbest_size > 1))))\n            # empty input\n            elif len(trans_input.tokens) == 0:\n                translated_chunks.append(IndexedTranslation(input_idx=trans_input_idx, chunk_idx=0,\n                                                            translation=empty_translation(add_nbest=(self.nbest_size > 1))))\n            else:\n                # TODO(tdomhan): Remove branch without EOS with next major version bump, as future models will always be trained with source side EOS symbols\n                if self.source_with_eos:\n                    max_input_length_without_eos = self.max_input_length\n                    # oversized input\n                    if len(trans_input.tokens) > max_input_length_without_eos:\n                        logger.debug(\n                            \"Input %s has length (%d) that exceeds max input length (%d). \"\n                            \"Splitting into chunks of size %d.\",\n                            trans_input.sentence_id, len(trans_input.tokens),\n                            self.buckets_source[-1], max_input_length_without_eos)\n                        chunks = [trans_input_chunk.with_eos()\n                                  for trans_input_chunk in trans_input.chunks(max_input_length_without_eos)]\n                        input_chunks.extend([IndexedTranslatorInput(trans_input_idx, chunk_idx, chunk_input)\n                                             for chunk_idx, chunk_input in enumerate(chunks)])\n                    # regular input\n                    else:\n                        input_chunks.append(IndexedTranslatorInput(trans_input_idx,\n                                                                   chunk_idx=0,\n                                                                   translator_input=trans_input.with_eos()))\n                else:\n                    if len(trans_input.tokens) > self.max_input_length:\n                        # oversized input\n                        logger.debug(\n                            \"Input %s has length (%d) that exceeds max input length (%d). \"\n                            \"Splitting into chunks of size %d.\",\n                            trans_input.sentence_id, len(trans_input.tokens),\n                            self.buckets_source[-1], self.max_input_length)\n                        chunks = [trans_input_chunk\n                                  for trans_input_chunk in\n                                  trans_input.chunks(self.max_input_length)]\n                        input_chunks.extend([IndexedTranslatorInput(trans_input_idx, chunk_idx, chunk_input)\n                                             for chunk_idx, chunk_input in enumerate(chunks)])\n                    else:\n                        # regular input\n                        input_chunks.append(IndexedTranslatorInput(trans_input_idx,\n                                                                   chunk_idx=0,\n                                                                   translator_input=trans_input))\n\n            if trans_input.constraints is not None:\n                logger.info(\"Input %s has %d %s: %s\", trans_input.sentence_id,\n                            len(trans_input.constraints),\n                            \"constraint\" if len(trans_input.constraints) == 1 else \"constraints\",\n                            \", \".join(\" \".join(x) for x in trans_input.constraints))\n\n        num_bad_empty = len(translated_chunks)\n\n        # Sort longest to shortest (to rather fill batches of shorter than longer sequences)\n        input_chunks = sorted(input_chunks, key=lambda chunk: len(chunk.translator_input.tokens), reverse=True)\n        # translate in batch-sized blocks over input chunks\n        batch_size = self.max_batch_size if fill_up_batches else min(len(input_chunks), self.max_batch_size)\n\n        num_batches = 0\n        for batch_id, batch in enumerate(utils.grouper(input_chunks, batch_size)):\n            logger.debug(\"Translating batch %d\", batch_id)\n\n            rest = batch_size - len(batch)\n            if fill_up_batches and rest > 0:\n                logger.debug(\"Padding batch of size %d to full batch size (%d)\", len(batch), batch_size)\n                batch = batch + [batch[0]] * rest\n\n            translator_inputs = [indexed_translator_input.translator_input for indexed_translator_input in batch]\n            batch_translations = self._translate_nd(*self._get_inference_input(translator_inputs))\n\n            # truncate to remove filler translations\n            if fill_up_batches and rest > 0:\n                batch_translations = batch_translations[:-rest]\n\n            for chunk, translation in zip(batch, batch_translations):\n                translated_chunks.append(IndexedTranslation(chunk.input_idx, chunk.chunk_idx, translation))\n            num_batches += 1\n        # Sort by input idx and then chunk id\n        translated_chunks = sorted(translated_chunks)\n        num_chunks = len(translated_chunks)\n\n        # Concatenate results\n        results = []  # type: List[TranslatorOutput]\n        chunks_by_input_idx = itertools.groupby(translated_chunks, key=lambda translation: translation.input_idx)\n        for trans_input, (input_idx, translations_for_input_idx) in zip(trans_inputs, chunks_by_input_idx):\n            translations_for_input_idx = list(translations_for_input_idx)  # type: ignore\n            if len(translations_for_input_idx) == 1:  # type: ignore\n                translation = translations_for_input_idx[0].translation  # type: ignore\n            else:\n                translations_to_concat = [translated_chunk.translation\n                                          for translated_chunk in translations_for_input_idx]\n                translation = self._concat_translations(translations_to_concat)\n\n            results.append(self._make_result(trans_input, translation))\n\n        num_outputs = len(results)\n\n        logger.debug(\"Translated %d inputs (%d chunks) in %d batches to %d outputs. %d empty/bad inputs.\",\n                     num_inputs, num_chunks, num_batches, num_outputs, num_bad_empty)\n\n        return results", "response": "Translate a list of TranslatorInputs into a list of TranslatorOutputs."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_inference_input(self,\n                             trans_inputs: List[TranslatorInput]) -> Tuple[mx.nd.NDArray,\n                                                                           int,\n                                                                           Optional[lexicon.TopKLexicon],\n                                                                           List[Optional[constrained.RawConstraintList]],\n                                                                           List[Optional[constrained.RawConstraintList]],\n                                                                           mx.nd.NDArray]:\n        \"\"\"\n        Assembles the numerical data for the batch. This comprises an NDArray for the source sentences,\n        the bucket key (padded source length), and a list of raw constraint lists, one for each sentence in the batch,\n        an NDArray of maximum output lengths for each sentence in the batch.\n        Each raw constraint list contains phrases in the form of lists of integers in the target language vocabulary.\n\n        :param trans_inputs: List of TranslatorInputs.\n        :return NDArray of source ids (shape=(batch_size, bucket_key, num_factors)),\n                bucket key, lexicon for vocabulary restriction, list of raw constraint\n                lists, and list of phrases to avoid, and an NDArray of maximum output\n                lengths.\n        \"\"\"\n        batch_size = len(trans_inputs)\n        bucket_key = data_io.get_bucket(max(len(inp.tokens) for inp in trans_inputs), self.buckets_source)\n        source = mx.nd.zeros((batch_size, bucket_key, self.num_source_factors), ctx=self.context)\n        restrict_lexicon = None  # type: Optional[lexicon.TopKLexicon]\n        raw_constraints = [None] * batch_size  # type: List[Optional[constrained.RawConstraintList]]\n        raw_avoid_list = [None] * batch_size  # type: List[Optional[constrained.RawConstraintList]]\n\n        max_output_lengths = []  # type: List[int]\n        for j, trans_input in enumerate(trans_inputs):\n            num_tokens = len(trans_input)\n            max_output_lengths.append(self.models[0].get_max_output_length(data_io.get_bucket(num_tokens, self.buckets_source)))\n            source[j, :num_tokens, 0] = data_io.tokens2ids(trans_input.tokens, self.source_vocabs[0])\n\n            factors = trans_input.factors if trans_input.factors is not None else []\n            num_factors = 1 + len(factors)\n            if num_factors != self.num_source_factors:\n                logger.warning(\"Input %d factors, but model(s) expect %d\", num_factors,\n                               self.num_source_factors)\n            for i, factor in enumerate(factors[:self.num_source_factors - 1], start=1):\n                # fill in as many factors as there are tokens\n\n                source[j, :num_tokens, i] = data_io.tokens2ids(factor, self.source_vocabs[i])[:num_tokens]\n\n            # Check if vocabulary selection/restriction is enabled:\n            # - First, see if the translator input provides a lexicon (used for multiple lexicons)\n            # - If not, see if the translator itself provides a lexicon (used for single lexicon)\n            # - The same lexicon must be used for all inputs in the batch.\n            if trans_input.restrict_lexicon is not None:\n                if restrict_lexicon is not None and restrict_lexicon is not trans_input.restrict_lexicon:\n                    logger.warning(\"Sentence %s: different restrict_lexicon specified, will overrule previous. \"\n                                   \"All inputs in batch must use same lexicon.\" % trans_input.sentence_id)\n                restrict_lexicon = trans_input.restrict_lexicon\n            elif self.restrict_lexicon is not None:\n                if isinstance(self.restrict_lexicon, dict):\n                    # This code should not be reachable since the case is checked when creating\n                    # translator inputs. It is included here to guarantee that the translator can\n                    # handle any valid input regardless of whether it was checked at creation time.\n                    logger.warning(\"Sentence %s: no restrict_lexicon specified for input when using multiple lexicons, \"\n                                   \"defaulting to first lexicon for entire batch.\" % trans_input.sentence_id)\n                    restrict_lexicon = list(self.restrict_lexicon.values())[0]\n                else:\n                    restrict_lexicon = self.restrict_lexicon\n\n            if trans_input.constraints is not None:\n                raw_constraints[j] = [data_io.tokens2ids(phrase, self.vocab_target) for phrase in\n                                      trans_input.constraints]\n\n            if trans_input.avoid_list is not None:\n                raw_avoid_list[j] = [data_io.tokens2ids(phrase, self.vocab_target) for phrase in\n                                     trans_input.avoid_list]\n                if any(self.unk_id in phrase for phrase in raw_avoid_list[j]):\n                    logger.warning(\"Sentence %s: %s was found in the list of phrases to avoid; \"\n                                   \"this may indicate improper preprocessing.\", trans_input.sentence_id, C.UNK_SYMBOL)\n\n        return source, bucket_key, restrict_lexicon, raw_constraints, raw_avoid_list, \\\n                mx.nd.array(max_output_lengths, ctx=self.context, dtype='int32')", "response": "Returns the inference input for the given list of TranslatorInputs."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a TranslatorOutput object from the input and the translation.", "response": "def _make_result(self,\n                     trans_input: TranslatorInput,\n                     translation: Translation) -> TranslatorOutput:\n        \"\"\"\n        Returns a translator result from generated target-side word ids, attention matrices and scores.\n        Strips stop ids from translation string.\n\n        :param trans_input: Translator input.\n        :param translation: The translation + attention and score.\n        :return: TranslatorOutput.\n        \"\"\"\n        target_ids = translation.target_ids\n        target_tokens = [self.vocab_target_inv[target_id] for target_id in target_ids]\n        target_string = C.TOKEN_SEPARATOR.join(data_io.ids2tokens(target_ids, self.vocab_target_inv, self.strip_ids))\n\n        attention_matrix = translation.attention_matrix\n        attention_matrix = attention_matrix[:, :len(trans_input.tokens)]\n\n        if translation.nbest_translations is None:\n            return TranslatorOutput(sentence_id=trans_input.sentence_id,\n                                    translation=target_string,\n                                    tokens=target_tokens,\n                                    attention_matrix=attention_matrix,\n                                    score=translation.score,\n                                    pass_through_dict=trans_input.pass_through_dict,\n                                    beam_histories=translation.beam_histories)\n        else:\n            nbest_target_ids = translation.nbest_translations.target_ids_list\n            target_tokens_list = [[self.vocab_target_inv[id] for id in ids] for ids in nbest_target_ids]\n            target_strings = [C.TOKEN_SEPARATOR.join(\n                                data_io.ids2tokens(target_ids,\n                                                   self.vocab_target_inv,\n                                                   self.strip_ids)) for target_ids in nbest_target_ids]\n\n            attention_matrices = [matrix[:, :len(trans_input.tokens)] for matrix in\n                                  translation.nbest_translations.attention_matrices]\n\n            scores = translation.nbest_translations.scores\n\n            return TranslatorOutput(sentence_id=trans_input.sentence_id,\n                                    translation=target_string,\n                                    tokens=target_tokens,\n                                    attention_matrix=attention_matrix,\n                                    score=translation.score,\n                                    pass_through_dict=trans_input.pass_through_dict,\n                                    beam_histories=translation.beam_histories,\n                                    nbest_translations=target_strings,\n                                    nbest_tokens=target_tokens_list,\n                                    nbest_attention_matrices=attention_matrices,\n                                    nbest_scores=scores)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntranslates a source of source_length into a list of translations.", "response": "def _translate_nd(self,\n                      source: mx.nd.NDArray,\n                      source_length: int,\n                      restrict_lexicon: Optional[lexicon.TopKLexicon],\n                      raw_constraints: List[Optional[constrained.RawConstraintList]],\n                      raw_avoid_list: List[Optional[constrained.RawConstraintList]],\n                      max_output_lengths: mx.nd.NDArray) -> List[Translation]:\n        \"\"\"\n        Translates source of source_length, given a bucket_key.\n\n        :param source: Source ids. Shape: (batch_size, bucket_key, num_factors).\n        :param source_length: Bucket key.\n        :param restrict_lexicon: Lexicon to use for vocabulary restriction.\n        :param raw_constraints: A list of optional constraint lists.\n\n        :return: Sequence of translations.\n        \"\"\"\n        return self._get_best_from_beam(*self._beam_search(source,\n                                                           source_length,\n                                                           restrict_lexicon,\n                                                           raw_constraints,\n                                                           raw_avoid_list,\n                                                           max_output_lengths))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nencodes the source into a list of state arrays and the estimated length of the model.", "response": "def _encode(self, sources: mx.nd.NDArray, source_length: int) -> Tuple[List[ModelState], mx.nd.NDArray]:\n        \"\"\"\n        Returns a ModelState for each model representing the state of the model after encoding the source.\n\n        :param sources: Source ids. Shape: (batch_size, bucket_key, num_factors).\n        :param source_length: Bucket key.\n        :return: List of ModelStates and the estimated reference length based on ratios averaged over models.\n        \"\"\"\n        model_states = []\n        ratios = []\n        for model in self.models:\n            state, ratio = model.run_encoder(sources, source_length)\n            model_states.append(state)\n            if ratio is not None:\n                ratios.append(ratio)\n\n        # num_seq takes batch_size and beam_size into account\n        num_seq = model_states[0].states[0].shape[0]\n        if self.constant_length_ratio > 0.0:\n            # override all ratios with the constant value\n            length_ratios = mx.nd.full(val=self.constant_length_ratio, shape=(num_seq, 1), ctx=self.context)\n        else:\n            if len(ratios) > 0:  # some model predicted a ratio?\n                # average the ratios over the models that actually we able to predict them\n                length_ratios = mx.nd.mean(mx.nd.stack(*ratios, axis=1), axis=1)\n            else:\n                length_ratios = mx.nd.zeros((num_seq, 1), ctx=self.context)\n\n        encoded_source_length=self.models[0].encoder.get_encoded_seq_len(source_length)\n        return model_states, length_ratios * encoded_source_length"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn decoder predictions (combined from all models), attention scores, and updated states. :param prev_word: Previous words of hypotheses. Shape: (batch_size * beam_size,). :param step: Beam search iteration. :param source_length: Length of the input sequence. :param states: List of model states. :param models_output_layer_w: Custom model weights for logit computation (empty for none). :param models_output_layer_b: Custom model biases for logit computation (empty for none). :return: (scores, attention scores, list of model states)", "response": "def _decode_step(self,\n                     prev_word: mx.nd.NDArray,\n                     step: int,\n                     source_length: int,\n                     states: List[ModelState],\n                     models_output_layer_w: List[mx.nd.NDArray],\n                     models_output_layer_b: List[mx.nd.NDArray]) \\\n            -> Tuple[mx.nd.NDArray, mx.nd.NDArray, List[ModelState]]:\n        \"\"\"\n        Returns decoder predictions (combined from all models), attention scores, and updated states.\n\n        :param prev_word: Previous words of hypotheses. Shape: (batch_size * beam_size,).\n        :param step: Beam search iteration.\n        :param source_length: Length of the input sequence.\n        :param states: List of model states.\n        :param models_output_layer_w: Custom model weights for logit computation (empty for none).\n        :param models_output_layer_b: Custom model biases for logit computation (empty for none).\n        :return: (scores, attention scores, list of model states)\n        \"\"\"\n        bucket_key = (source_length, step)\n\n        model_outs, model_attention_probs, model_states = [], [], []\n        # We use zip_longest here since we'll have empty lists when not using restrict_lexicon\n        for model, out_w, out_b, state in itertools.zip_longest(\n                self.models, models_output_layer_w, models_output_layer_b, states):\n            decoder_out, attention_probs, state = model.run_decoder(prev_word, bucket_key, state)\n            # Compute logits and softmax with restricted vocabulary\n            if self.restrict_lexicon:\n                # Apply output layer outside decoder module.\n                logits = model.output_layer(decoder_out, out_w, out_b)\n                if model.skip_softmax:\n                    model_out = logits  # raw logits\n                else:\n                    model_out = mx.nd.softmax(logits)  # normalized probabilities\n            else:\n                # Output layer is applied inside decoder module.\n                # if model.skip_softmax decoder_out represents logits, normalized probabilities else.\n                model_out = decoder_out\n            model_outs.append(model_out)\n            model_attention_probs.append(attention_probs)\n            model_states.append(state)\n        scores, attention_probs = self._combine_predictions(model_outs, model_attention_probs)\n        return scores, attention_probs, model_states"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncombines predictions of models and averaged attention prob scores.", "response": "def _combine_predictions(self,\n                             model_outputs: List[mx.nd.NDArray],\n                             attention_probs: List[mx.nd.NDArray]) -> Tuple[mx.nd.NDArray, mx.nd.NDArray]:\n        \"\"\"\n        Returns combined predictions of models and averaged attention prob scores.\n        If model_outputs are probabilities, they are converted to negative log probabilities before combination.\n        If model_outputs are logits (and no ensembling is used),\n        no combination is applied and logits are converted to negative logits.\n\n        :param model_outputs: List of Shape(beam_size, target_vocab_size).\n        :param attention_probs: List of Shape(beam_size, bucket_key).\n        :return: Combined scores, averaged attention scores.\n        \"\"\"\n        # average attention prob scores. TODO: is there a smarter way to do this?\n        attention_prob_score = utils.average_arrays(attention_probs)\n\n        # combine model predictions and convert to neg log probs\n        if len(self.models) == 1:\n            if self.models[0].skip_softmax:\n                scores = -model_outputs[0]\n            else:\n                scores = -mx.nd.log(model_outputs[0])  # pylint: disable=invalid-unary-operand-type\n        else:\n            scores = self.interpolation_func(model_outputs)\n        return scores, attention_prob_score"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntranslates multiple sentences using beam search. :param source: Source ids. Shape: (batch_size, bucket_key, num_factors). :param source_length: Max source length. :param restrict_lexicon: Lexicon to use for vocabulary restriction. :param raw_constraint_list: A list of optional lists containing phrases (as lists of target word IDs) that must appear in each output. :param raw_avoid_list: A list of optional lists containing phrases (as lists of target word IDs) that must NOT appear in each output. :return List of best hypotheses indices, list of best word indices, list of attentions, array of accumulated length-normalized negative log-probs, hypotheses lengths, predicted lengths of references (if any), constraints (if any), beam histories (if any).", "response": "def _beam_search(self,\n                     source: mx.nd.NDArray,\n                     source_length: int,\n                     restrict_lexicon: Optional[lexicon.TopKLexicon],\n                     raw_constraint_list: List[Optional[constrained.RawConstraintList]],\n                     raw_avoid_list: List[Optional[constrained.RawConstraintList]],\n                     max_output_lengths: mx.nd.NDArray) -> Tuple[np.ndarray,\n                                                                 np.ndarray,\n                                                                 np.ndarray,\n                                                                 np.ndarray,\n                                                                 np.ndarray,\n                                                                 List[Optional[np.ndarray]],\n                                                                 List[Optional[constrained.ConstrainedHypothesis]],\n                                                                 Optional[List[BeamHistory]]]:\n        \"\"\"\n        Translates multiple sentences using beam search.\n\n        :param source: Source ids. Shape: (batch_size, bucket_key, num_factors).\n        :param source_length: Max source length.\n        :param restrict_lexicon: Lexicon to use for vocabulary restriction.\n        :param raw_constraint_list: A list of optional lists containing phrases (as lists of target word IDs)\n               that must appear in each output.\n        :param raw_avoid_list: A list of optional lists containing phrases (as lists of target word IDs)\n               that must NOT appear in each output.\n        :return List of best hypotheses indices, list of best word indices, list of attentions,\n                array of accumulated length-normalized negative log-probs, hypotheses lengths,\n                predicted lengths of references (if any), constraints (if any), beam histories (if any).\n        \"\"\"\n        batch_size = source.shape[0]\n        logger.debug(\"_beam_search batch size: %d\", batch_size)\n\n        # Length of encoded sequence (may differ from initial input length)\n        encoded_source_length = self.models[0].encoder.get_encoded_seq_len(source_length)\n        utils.check_condition(all(encoded_source_length ==\n                                  model.encoder.get_encoded_seq_len(source_length) for model in self.models),\n                              \"Models must agree on encoded sequence length\")\n        # Maximum output length\n        max_output_length = self.models[0].get_max_output_length(source_length)\n\n        # General data structure: batch_size * beam_size blocks in total;\n        # a full beam for each sentence, folloed by the next beam-block for the next sentence and so on\n\n        best_word_indices = mx.nd.full((batch_size * self.beam_size,), val=self.start_id, ctx=self.context,\n                                       dtype='int32')\n\n        # offset for hypothesis indices in batch decoding\n        offset = mx.nd.repeat(mx.nd.arange(0, batch_size * self.beam_size, self.beam_size,\n                                           dtype='int32', ctx=self.context), self.beam_size)\n\n        # locations of each batch item when first dimension is (batch * beam)\n        batch_indices = mx.nd.arange(0, batch_size * self.beam_size, self.beam_size, dtype='int32', ctx=self.context)\n        first_step_mask = mx.nd.full((batch_size * self.beam_size, 1), val=np.inf, ctx=self.context)\n        first_step_mask[batch_indices] = 1.0\n        pad_dist = mx.nd.full((batch_size * self.beam_size, len(self.vocab_target) - 1), val=np.inf,\n                              ctx=self.context)\n\n        # Best word and hypotheses indices across beam search steps from topk operation.\n        best_hyp_indices_list = []  # type: List[mx.nd.NDArray]\n        best_word_indices_list = []  # type: List[mx.nd.NDArray]\n\n        # Beam history\n        beam_histories = None  # type: Optional[List[BeamHistory]]\n        if self.store_beam:\n            beam_histories = [defaultdict(list) for _ in range(batch_size)]\n\n        lengths = mx.nd.zeros((batch_size * self.beam_size, 1), ctx=self.context)\n        finished = mx.nd.zeros((batch_size * self.beam_size,), ctx=self.context, dtype='int32')\n\n        # Extending max_output_lengths to shape (batch_size * beam_size,)\n        max_output_lengths = mx.nd.repeat(max_output_lengths, self.beam_size)\n\n        # Attention distributions across beam search steps\n        attentions = []  # type: List[mx.nd.NDArray]\n\n        # scores_accumulated: chosen smallest scores in scores (ascending).\n        scores_accumulated = mx.nd.zeros((batch_size * self.beam_size, 1), ctx=self.context)\n\n        # If using a top-k lexicon, select param rows for logit computation that correspond to the\n        # target vocab for this sentence.\n        models_output_layer_w = list()\n        models_output_layer_b = list()\n        vocab_slice_ids = None  # type: mx.nd.NDArray\n        if restrict_lexicon:\n            source_words = utils.split(source, num_outputs=self.num_source_factors, axis=2, squeeze_axis=True)[0]\n            # TODO: See note in method about migrating to pure MXNet when set operations are supported.\n            #       We currently convert source to NumPy and target ids back to NDArray.\n            vocab_slice_ids = restrict_lexicon.get_trg_ids(source_words.astype(\"int32\").asnumpy())\n            if any(raw_constraint_list):\n                # Add the constraint IDs to the list of permissibled IDs, and then project them into the reduced space\n                constraint_ids = np.array([word_id for sent in raw_constraint_list for phr in sent for word_id in phr])\n                vocab_slice_ids = np.lib.arraysetops.union1d(vocab_slice_ids, constraint_ids)\n                full_to_reduced = dict((val, i) for i, val in enumerate(vocab_slice_ids))\n                raw_constraint_list = [[[full_to_reduced[x] for x in phr] for phr in sent] for sent in\n                                       raw_constraint_list]\n\n            vocab_slice_ids = mx.nd.array(vocab_slice_ids, ctx=self.context, dtype='int32')\n\n            if vocab_slice_ids.shape[0] < self.beam_size + 1:\n                # This fixes an edge case for toy models, where the number of vocab ids from the lexicon is\n                # smaller than the beam size.\n                logger.warning(\"Padding vocab_slice_ids (%d) with EOS to have at least %d+1 elements to expand\",\n                               vocab_slice_ids.shape[0], self.beam_size)\n                n = self.beam_size - vocab_slice_ids.shape[0] + 1\n                vocab_slice_ids = mx.nd.concat(vocab_slice_ids,\n                                               mx.nd.full((n,), val=self.vocab_target[C.EOS_SYMBOL],\n                                                          ctx=self.context, dtype='int32'),\n                                               dim=0)\n\n            pad_dist = mx.nd.full((batch_size * self.beam_size, vocab_slice_ids.shape[0] - 1),\n                                  val=np.inf, ctx=self.context)\n            for m in self.models:\n                models_output_layer_w.append(m.output_layer_w.take(vocab_slice_ids))\n                models_output_layer_b.append(m.output_layer_b.take(vocab_slice_ids))\n\n        # (0) encode source sentence, returns a list\n        model_states, estimated_reference_lengths = self._encode(source, source_length)\n\n        # Initialize the beam to track constraint sets, where target-side lexical constraints are present\n        constraints = constrained.init_batch(raw_constraint_list, self.beam_size, self.start_id,\n                                             self.vocab_target[C.EOS_SYMBOL])\n\n        if self.global_avoid_trie or any(raw_avoid_list):\n            avoid_states = constrained.AvoidBatch(batch_size, self.beam_size,\n                                                  avoid_list=raw_avoid_list,\n                                                  global_avoid_trie=self.global_avoid_trie)\n            avoid_states.consume(best_word_indices)\n\n        # Records items in the beam that are inactive. At the beginning (t==1), there is only one valid or active\n        # item on the beam for each sentence\n        inactive = mx.nd.zeros((batch_size * self.beam_size), dtype='int32', ctx=self.context)\n        t = 1\n        for t in range(1, max_output_length):\n            # (1) obtain next predictions and advance models' state\n            # target_dists: (batch_size * beam_size, target_vocab_size)\n            # attention_scores: (batch_size * beam_size, bucket_key)\n            target_dists, attention_scores, model_states = self._decode_step(prev_word=best_word_indices,\n                                                                             step=t,\n                                                                             source_length=source_length,\n                                                                             states=model_states,\n                                                                             models_output_layer_w=models_output_layer_w,\n                                                                             models_output_layer_b=models_output_layer_b)\n\n            # (2) Produces the accumulated cost of target words in each row.\n            # There is special treatment for finished and inactive rows: inactive rows are inf everywhere;\n            # finished rows are inf everywhere except column zero, which holds the accumulated model score\n            scores = self._update_scores.forward(target_dists, finished, inactive, scores_accumulated, pad_dist)\n\n            # Mark entries that should be blocked as having a score of np.inf\n            if self.global_avoid_trie or any(raw_avoid_list):\n                block_indices = avoid_states.avoid()\n                if len(block_indices) > 0:\n                    scores[block_indices] = np.inf\n                    if self.sample is not None:\n                        target_dists[block_indices] = np.inf\n\n            # (3) Get beam_size winning hypotheses for each sentence block separately. Only look as\n            # far as the active beam size for each sentence.\n\n            if self.sample is not None:\n                best_hyp_indices, best_word_indices, scores_accumulated = self._top(scores, target_dists, finished)\n            else:\n                # On the first timestep, all hypotheses have identical histories, so force topk() to choose extensions\n                # of the first row only by setting all other rows to inf\n                if t == 1 and not self.skip_topk:\n                    scores *= first_step_mask\n\n                best_hyp_indices, best_word_indices, scores_accumulated = self._top(scores, offset)\n\n            # Constraints for constrained decoding are processed sentence by sentence\n            if any(raw_constraint_list):\n                best_hyp_indices, best_word_indices, scores_accumulated, constraints, inactive = constrained.topk(\n                    t,\n                    batch_size,\n                    self.beam_size,\n                    inactive,\n                    scores,\n                    constraints,\n                    best_hyp_indices,\n                    best_word_indices,\n                    scores_accumulated)\n\n            # Map from restricted to full vocab ids if needed\n            if restrict_lexicon:\n                best_word_indices = vocab_slice_ids.take(best_word_indices)\n\n            # (4) Reorder fixed-size beam data according to best_hyp_indices (ascending)\n            finished, lengths, attention_scores, estimated_reference_lengths \\\n                                                = self._sort_by_index.forward(best_hyp_indices,\n                                                                              finished,\n                                                                              lengths,\n                                                                              attention_scores,\n                                                                              estimated_reference_lengths)\n\n            # (5) Normalize the scores of newly finished hypotheses. Note that after this until the\n            # next call to topk(), hypotheses may not be in sorted order.\n            finished, scores_accumulated, lengths = self._update_finished.forward(best_word_indices,\n                                                                                  max_output_lengths,\n                                                                                  finished,\n                                                                                  scores_accumulated,\n                                                                                  lengths,\n                                                                                  estimated_reference_lengths)\n\n            # (6) Prune out low-probability hypotheses. Pruning works by setting entries `inactive`.\n            if self.beam_prune > 0.0:\n                inactive, best_word_indices, scores_accumulated = self._prune_hyps.forward(best_word_indices,\n                                                                                           scores_accumulated,\n                                                                                           finished)\n\n            # (7) update negative constraints\n            if self.global_avoid_trie or any(raw_avoid_list):\n                avoid_states.reorder(best_hyp_indices)\n                avoid_states.consume(best_word_indices)\n\n            # (8) optionally save beam history\n            if self.store_beam:\n                finished_or_inactive = mx.nd.clip(data=finished + inactive, a_min=0, a_max=1)\n                unnormalized_scores = mx.nd.where(finished_or_inactive,\n                                                  scores_accumulated * self.length_penalty(lengths),\n                                                  scores_accumulated)\n                normalized_scores = mx.nd.where(finished_or_inactive,\n                                                scores_accumulated,\n                                                scores_accumulated / self.length_penalty(lengths))\n                for sent in range(batch_size):\n                    rows = slice(sent * self.beam_size, (sent + 1) * self.beam_size)\n\n                    best_word_indices_sent = best_word_indices[rows].asnumpy().tolist()\n                    # avoid adding columns for finished sentences\n                    if any(x for x in best_word_indices_sent if x != C.PAD_ID):\n                        beam_histories[sent][\"predicted_ids\"].append(best_word_indices_sent)\n                        beam_histories[sent][\"predicted_tokens\"].append([self.vocab_target_inv[x] for x in\n                                                                         best_word_indices_sent])\n                        # for later sentences in the matrix, shift from e.g. [5, 6, 7, 8, 6] to [0, 1, 3, 4, 1]\n                        shifted_parents = best_hyp_indices[rows] - (sent * self.beam_size)\n                        beam_histories[sent][\"parent_ids\"].append(shifted_parents.asnumpy().tolist())\n\n                        beam_histories[sent][\"scores\"].append(unnormalized_scores[rows].asnumpy().flatten().tolist())\n                        beam_histories[sent][\"normalized_scores\"].append(\n                            normalized_scores[rows].asnumpy().flatten().tolist())\n\n            # Collect best hypotheses, best word indices, and attention scores\n            best_hyp_indices_list.append(best_hyp_indices)\n            best_word_indices_list.append(best_word_indices)\n            attentions.append(attention_scores)\n\n            if self.beam_search_stop == C.BEAM_SEARCH_STOP_FIRST:\n                at_least_one_finished = finished.reshape((batch_size, self.beam_size)).sum(axis=1) > 0\n                if at_least_one_finished.sum().asscalar() == batch_size:\n                    break\n            else:\n                if finished.sum().asscalar() == batch_size * self.beam_size:  # all finished\n                    break\n\n            # (9) update models' state with winning hypotheses (ascending)\n            for ms in model_states:\n                ms.sort_state(best_hyp_indices)\n\n        logger.debug(\"Finished after %d / %d steps.\", t + 1, max_output_length)\n\n        # (9) Sort the hypotheses within each sentence (normalization for finished hyps may have unsorted them).\n        folded_accumulated_scores = scores_accumulated.reshape((batch_size,\n                                                                self.beam_size * scores_accumulated.shape[-1]))\n        indices = mx.nd.cast(mx.nd.argsort(folded_accumulated_scores, axis=1), dtype='int32').reshape((-1,))\n        best_hyp_indices, _ = mx.nd.unravel_index(indices, scores_accumulated.shape) + offset\n        best_hyp_indices_list.append(best_hyp_indices)\n        lengths = lengths.take(best_hyp_indices)\n        scores_accumulated = scores_accumulated.take(best_hyp_indices)\n        constraints = [constraints[x] for x in best_hyp_indices.asnumpy()]\n\n        all_best_hyp_indices = mx.nd.stack(*best_hyp_indices_list, axis=1)\n        all_best_word_indices = mx.nd.stack(*best_word_indices_list, axis=1)\n        all_attentions = mx.nd.stack(*attentions, axis=1)\n\n        return all_best_hyp_indices.asnumpy(), \\\n               all_best_word_indices.asnumpy(), \\\n               all_attentions.asnumpy(), \\\n               scores_accumulated.asnumpy(), \\\n               lengths.asnumpy().astype('int32'), \\\n               estimated_reference_lengths.asnumpy(), \\\n               constraints, \\\n               beam_histories"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_best_from_beam(self,\n                            best_hyp_indices: np.ndarray,\n                            best_word_indices: np.ndarray,\n                            attentions: np.ndarray,\n                            seq_scores: np.ndarray,\n                            lengths: np.ndarray,\n                            estimated_reference_lengths: Optional[mx.nd.NDArray],\n                            constraints: List[Optional[constrained.ConstrainedHypothesis]],\n                            beam_histories: Optional[List[BeamHistory]] = None) -> List[Translation]:\n        \"\"\"\n        Return the nbest (aka n top) entries from the n-best list.\n\n        :param best_hyp_indices: Array of best hypotheses indices ids. Shape: (batch * beam, num_beam_search_steps + 1).\n        :param best_word_indices: Array of best hypotheses indices ids. Shape: (batch * beam, num_beam_search_steps).\n        :param attentions: Array of attentions over source words.\n                           Shape: (batch * beam, num_beam_search_steps, encoded_source_length).\n        :param seq_scores: Array of length-normalized negative log-probs. Shape: (batch * beam, 1)\n        :param lengths: The lengths of all items in the beam. Shape: (batch * beam). Dtype: int32.\n        :param estimated_reference_lengths: Predicted reference lengths.\n        :param constraints: The constraints for all items in the beam. Shape: (batch * beam).\n        :param beam_histories: The beam histories for each sentence in the batch.\n        :return: List of Translation objects containing all relevant information.\n        \"\"\"\n        batch_size = best_hyp_indices.shape[0] // self.beam_size\n        nbest_translations = []  # type: List[List[Translation]]\n        histories = beam_histories if beam_histories is not None else [None] * self.batch_size  # type: List\n        reference_lengths = estimated_reference_lengths if estimated_reference_lengths is not None \\\n                                                        else np.full(self.batch_size * self.beam_size, None)\n        for n in range(0, self.nbest_size):\n\n            # Initialize the best_ids to the first item in each batch, plus current nbest index\n            best_ids = np.arange(n, batch_size * self.beam_size, self.beam_size, dtype='int32')\n\n            # only check for constraints for 1-best translation for each sequence in batch\n            if n == 0 and any(constraints):\n                # For constrained decoding, select from items that have met all constraints (might not be finished)\n                unmet = np.array([c.num_needed() if c is not None else 0 for c in constraints])\n                filtered = np.where(unmet == 0, seq_scores.flatten(), np.inf)\n                filtered = filtered.reshape((batch_size, self.beam_size))\n                best_ids += np.argmin(filtered, axis=1).astype('int32')\n\n            # Obtain sequences for all best hypotheses in the batch\n            indices = self._get_best_word_indices_for_kth_hypotheses(best_ids, best_hyp_indices)\n            nbest_translations.append([self._assemble_translation(*x) for x in zip(best_word_indices[indices, np.arange(indices.shape[1])],\n                                                                                   lengths[best_ids],\n                                                                                   attentions[best_ids],\n                                                                                   seq_scores[best_ids],\n                                                                                   histories,\n                                                                                   reference_lengths[best_ids])])\n        # reorder and regroup lists\n        reduced_translations = [_reduce_nbest_translations(grouped_nbest) for grouped_nbest in zip(*nbest_translations)]\n        return reduced_translations", "response": "Return the nbest entries from the n - best list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntraversing the matrix of best hypotheses indices collected during beam search in reversed order by using the kth hypotheses index as a backpointer. Returns an array containing the indices into the best_word_indices collected during beam search to extract the kth hypotheses. :param ks: The kth-best hypotheses to extract. Supports multiple for batch_size > 1. Shape: (batch,). :param all_hyp_indices: All best hypotheses indices list collected in beam search. Shape: (batch * beam, steps). :return: Array of indices into the best_word_indices collected in beam search that extract the kth-best hypothesis. Shape: (batch,).", "response": "def _get_best_word_indices_for_kth_hypotheses(ks: np.ndarray, all_hyp_indices: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Traverses the matrix of best hypotheses indices collected during beam search in reversed order by\n        using the kth hypotheses index as a backpointer.\n        Returns an array containing the indices into the best_word_indices collected during beam search to extract\n        the kth hypotheses.\n\n        :param ks: The kth-best hypotheses to extract. Supports multiple for batch_size > 1. Shape: (batch,).\n        :param all_hyp_indices: All best hypotheses indices list collected in beam search. Shape: (batch * beam, steps).\n        :return: Array of indices into the best_word_indices collected in beam search\n            that extract the kth-best hypothesis. Shape: (batch,).\n        \"\"\"\n        batch_size = ks.shape[0]\n        num_steps = all_hyp_indices.shape[1]\n        result = np.zeros((batch_size, num_steps - 1), dtype=all_hyp_indices.dtype)\n        # first index into the history of the desired hypotheses.\n        pointer = all_hyp_indices[ks, -1]\n        # for each column/step follow the pointer, starting from the penultimate column/step\n        num_steps = all_hyp_indices.shape[1]\n        for step in range(num_steps - 2, -1, -1):\n            result[:, step] = pointer\n            pointer = all_hyp_indices[pointer, step]\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _assemble_translation(sequence: np.ndarray,\n                              length: np.ndarray,\n                              attention_lists: np.ndarray,\n                              seq_score: np.ndarray,\n                              beam_history: Optional[BeamHistory],\n                              estimated_reference_length: Optional[float]) -> Translation:\n        \"\"\"\n        Takes a set of data pertaining to a single translated item, performs slightly different\n        processing on each, and merges it into a Translation object.\n        :param sequence: Array of word ids. Shape: (batch_size, bucket_key).\n        :param length: The length of the translated segment.\n        :param attention_lists: Array of attentions over source words.\n                                Shape: (batch_size * self.beam_size, max_output_length, encoded_source_length).\n        :param seq_score: Array of length-normalized negative log-probs.\n        :param estimated_reference_length: Estimated reference length (if any).\n        :param beam_history: The optional beam histories for each sentence in the batch.\n        :return: A Translation object.\n        \"\"\"\n        length = int(length)\n        sequence = sequence[:length].tolist()\n        attention_matrix = attention_lists[:length, :]\n        score = float(seq_score)\n        estimated_reference_length=float(estimated_reference_length) if estimated_reference_length else None\n        beam_history_list = [beam_history] if beam_history is not None else []\n        return Translation(sequence, attention_matrix, score, beam_history_list,\n                           nbest_translations=None,\n                           estimated_reference_length=estimated_reference_length)", "response": "Assemble a Translation object from a set of data pertaining to a single translated item."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _print_beam(self,\n                    sequences: mx.nd.NDArray,\n                    accumulated_scores: mx.nd.NDArray,\n                    finished: mx.nd.NDArray,\n                    inactive: mx.nd.NDArray,\n                    constraints: List[Optional[constrained.ConstrainedHypothesis]],\n                    timestep: int) -> None:\n        \"\"\"\n        Prints the beam for debugging purposes.\n\n        :param sequences: The beam histories (shape: batch_size * beam_size, max_output_len).\n        :param accumulated_scores: The accumulated scores for each item in the beam.\n               Shape: (batch_size * beam_size, target_vocab_size).\n        :param finished: Indicates which items are finished (shape: batch_size * beam_size).\n        :param inactive: Indicates any inactive items (shape: batch_size * beam_size).\n        :param timestep: The current timestep.\n        \"\"\"\n        logger.info('BEAM AT TIMESTEP %d', timestep)\n        batch_beam_size = sequences.shape[0]\n        for i in range(batch_beam_size):\n            # for each hypothesis, print its entire history\n            score = accumulated_scores[i].asscalar()\n            word_ids = [int(x.asscalar()) for x in sequences[i]]\n            unmet = constraints[i].num_needed() if constraints[i] is not None else -1\n            hypothesis = '----------' if inactive[i] else ' '.join(\n                [self.vocab_target_inv[x] for x in word_ids if x != 0])\n            logger.info('%d %d %d %d %.2f %s', i + 1, finished[i].asscalar(), inactive[i].asscalar(), unmet, score,\n                        hypothesis)", "response": "Prints the beam for debugging purposes."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef hybrid_forward(self, F, scores, offset):\n        # Shape: (batch size, beam_size * vocab_size)\n        folded_scores = F.reshape(scores, shape=(-1, self.k * self.vocab_size))\n\n        values, indices = F.topk(folded_scores, axis=1, k=self.k, ret_typ='both', is_ascend=True)\n\n        # Project indices back into original shape (which is different for t==1 and t>1)\n        indices = F.reshape(F.cast(indices, 'int32'), shape=(-1,))\n        # TODO: we currently exploit a bug in the implementation of unravel_index to not require knowing the first shape\n        # value. See https://github.com/apache/incubator-mxnet/issues/13862\n        unraveled = F.unravel_index(indices, shape=(C.LARGEST_INT, self.vocab_size))\n\n        best_hyp_indices, best_word_indices = F.split(unraveled, axis=0, num_outputs=2, squeeze_axis=True)\n        best_hyp_indices = best_hyp_indices + offset\n        values = F.reshape(values, shape=(-1, 1))\n        return best_hyp_indices, best_word_indices, values", "response": "This method is used to get the k elements per sentence from a scores matrix."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nchoose an extension of each hypothesis from its softmax distribution. :param scores: Vocabulary scores for the next beam step. (batch_size * beam_size, target_vocabulary_size) :param target_dists: The non-cumulative target distributions (ignored). :param finished: The list of finished hypotheses. :param best_hyp_indices: Best hypothesis indices constant. :return: The row indices, column indices, and values of the sampled words.", "response": "def hybrid_forward(self, F, scores, target_dists, finished, best_hyp_indices):\n        \"\"\"\n        Choose an extension of each hypothesis from its softmax distribution.\n\n        :param scores: Vocabulary scores for the next beam step. (batch_size * beam_size, target_vocabulary_size)\n        :param target_dists: The non-cumulative target distributions (ignored).\n        :param finished: The list of finished hypotheses.\n        :param best_hyp_indices: Best hypothesis indices constant.\n        :return: The row indices, column indices, and values of the sampled words.\n        \"\"\"\n        # Map the negative logprobs to probabilities so as to have a distribution\n        target_dists = F.exp(-target_dists)\n\n        # n == 0 means sample from the full vocabulary. Otherwise, we sample from the top n.\n        if self.n != 0:\n            # select the top n in each row, via a mask\n            masked_items = F.topk(target_dists, k=self.n, ret_typ='mask', axis=1, is_ascend=False)\n            # set unmasked items to 0\n            masked_items = F.where(masked_items, target_dists, masked_items)\n            # renormalize\n            target_dists = F.broadcast_div(masked_items, F.sum(masked_items, axis=1, keepdims=True))\n\n        # Sample from the target distributions over words, then get the corresponding values from the cumulative scores\n        best_word_indices = F.random.multinomial(target_dists, get_prob=False)\n        # Zeroes for finished hypotheses.\n        best_word_indices = F.where(finished, F.zeros_like(best_word_indices), best_word_indices)\n        values = F.pick(scores, best_word_indices, axis=1, keepdims=True)\n\n        best_hyp_indices = F.slice_like(best_hyp_indices, best_word_indices, axes=(0,))\n\n        return best_hyp_indices, best_word_indices, values"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef hybrid_forward(self, F, scores, offset):\n        best_word_indices = F.cast(F.argmin(scores, axis=1), dtype='int32')\n        values = F.pick(scores, best_word_indices, axis=1)\n        values = F.reshape(values, shape=(-1, 1))\n\n        # for top1, the best hyp indices are equal to the plain offset\n        best_hyp_indices = offset\n\n        return best_hyp_indices, best_word_indices, values", "response": "This method is used to get the smallest word indices and values from a scores matrix."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _decode_and_evaluate(decoder: checkpoint_decoder.CheckpointDecoder,\n                         checkpoint: int,\n                         output_name: str,\n                         queue: multiprocessing.Queue):\n    \"\"\"\n    Decodes and evaluates using given checkpoint_decoder and puts result in the queue,\n    indexed by the checkpoint.\n    \"\"\"\n    metrics = decoder.decode_and_evaluate(checkpoint, output_name)\n    queue.put((checkpoint, metrics))", "response": "Decodes and evaluates using given checkpoint_decoder and puts result in queue."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _initialize(self,\n                    provide_data: List[mx.io.DataDesc],\n                    provide_label: List[mx.io.DataDesc],\n                    default_bucket_key: Tuple[int, int]):\n        \"\"\"\n        Initializes model components, creates training symbol and module, and binds it.\n        \"\"\"\n        source = mx.sym.Variable(C.SOURCE_NAME)\n        source_words = source.split(num_outputs=self.config.config_embed_source.num_factors,\n                                    axis=2, squeeze_axis=True)[0]\n        source_length = utils.compute_lengths(source_words)\n        target = mx.sym.Variable(C.TARGET_NAME)\n        target_length = utils.compute_lengths(target)\n        labels = mx.sym.reshape(data=mx.sym.Variable(C.TARGET_LABEL_NAME), shape=(-1,))\n\n        self.model_loss = loss.get_loss(self.config.config_loss)\n        logger.info(\"Using model loss: %s\", self.model_loss)\n        if self.config.config_length_task_loss is not None:\n            self.length_task_loss = loss.get_length_task_loss(self.config.config_length_task_loss)\n            logger.info(\"Using length task loss: %s\", self.length_task_loss)\n        else:\n            self.length_task_loss = None\n\n        data_names = [C.SOURCE_NAME, C.TARGET_NAME]\n        label_names = [C.TARGET_LABEL_NAME]\n\n        # length_ratio: (batch_size, ). Will be pruned if not used\n        length_ratio = mx.sym.broadcast_div(target_length, source_length, name=C.LENRATIO_LABEL_NAME)\n\n        # check provide_{data,label} names\n        provide_data_names = [d[0] for d in provide_data]\n        utils.check_condition(provide_data_names == data_names,\n                              \"incompatible provide_data: %s, names should be %s\" % (provide_data_names, data_names))\n        provide_label_names = [d[0] for d in provide_label]\n        utils.check_condition(provide_label_names == label_names,\n                              \"incompatible provide_label: %s, names should be %s\" % (provide_label_names, label_names))\n\n        def sym_gen(seq_lens):\n            \"\"\"\n            Returns a (grouped) loss symbol given source & target input lengths.\n            Also returns data and label names for the BucketingModule.\n            \"\"\"\n            source_seq_len, target_seq_len = seq_lens\n\n            # source embedding\n            (source_embed,\n             source_embed_length,\n             source_embed_seq_len) = self.embedding_source.encode(source, source_length, source_seq_len)\n\n            # target embedding\n            (target_embed,\n             target_embed_length,\n             target_embed_seq_len) = self.embedding_target.encode(target, target_length, target_seq_len)\n\n            # encoder\n            # source_encoded: (batch_size, source_encoded_length, encoder_depth)\n            (source_encoded,\n             source_encoded_length,\n             source_encoded_seq_len) = self.encoder.encode(source_embed,\n                                                           source_embed_length,\n                                                           source_embed_seq_len)\n            # decoder\n            # target_decoded: (batch-size, target_len, decoder_depth)\n            target_decoded = self.decoder.decode_sequence(source_encoded, source_encoded_length, source_encoded_seq_len,\n                                                          target_embed, target_embed_length, target_embed_seq_len)\n\n            # target_decoded: (batch_size * target_seq_len, decoder_depth)\n            target_decoded = mx.sym.reshape(data=target_decoded, shape=(-3, 0))\n\n            # output layer\n            # logits: (batch_size * target_seq_len, target_vocab_size)\n            logits = self.output_layer(target_decoded)\n\n            # 1) standard cross-entropy loss\n            net_outputs = [self.model_loss.get_loss(logits, labels)]\n            # 2) length task losses\n            if self.length_task_loss is not None:\n                # predicted_length_ratios: (batch_size, 1)\n                predicted_length_ratio = self.length_ratio(source_encoded, source_encoded_length)\n                if isinstance(self.length_task_loss, loss.MSELoss):\n                    loss_symbol = self.length_task_loss.get_loss(predicted_length_ratio, length_ratio)\n                elif isinstance(self.length_task_loss, loss.PoissonLoss):\n                    # convert ratios to (expected) length estimations for the Poisson loss\n                    predicted_reference_length = predicted_length_ratio * source_encoded_length.reshape((-1, 1))\n                    loss_symbol = self.length_task_loss.get_loss(predicted_reference_length, target_length)\n                # return both the loss symbol, prediction and the computed length_ratio to be used in metrics\n                net_outputs.extend([loss_symbol,\n                                    mx.sym.BlockGrad(predicted_length_ratio, name=C.LENRATIO_NAME),\n                                    mx.sym.BlockGrad(length_ratio, name=C.LENRATIO_LABEL_NAME)])\n\n            return mx.sym.Group(net_outputs), data_names, label_names\n\n        # Fix model parameters as needed for different training options.\n        utils.check_condition(not self.config.lhuc or self.fixed_param_strategy is None,\n                \"LHUC fixes all other parameters and is thus not compatible with other fixing strategies.\")\n        if self.config.lhuc:\n            arguments = sym_gen(default_bucket_key)[0].list_arguments()\n            fixed_param_names = [a for a in arguments if not a.endswith(C.LHUC_NAME)]\n        elif self.fixed_param_strategy is not None:\n            arguments = sym_gen(default_bucket_key)[0].list_arguments()\n            fixed_param_names = self._generate_fixed_param_names(arguments, self.fixed_param_strategy)\n        else:\n            fixed_param_names = self.fixed_param_names\n\n        if self._bucketing:\n            logger.info(\"Using bucketing. Default max_seq_len=%s\", default_bucket_key)\n            self.module = mx.mod.BucketingModule(sym_gen=sym_gen,\n                                                 logger=logger,\n                                                 default_bucket_key=default_bucket_key,\n                                                 context=self.context,\n                                                 compression_params=self._gradient_compression_params,\n                                                 fixed_param_names=fixed_param_names)\n        else:\n            logger.info(\"No bucketing. Unrolled to (%d,%d)\",\n                        self.config.config_data.max_seq_len_source, self.config.config_data.max_seq_len_target)\n            symbol, _, __ = sym_gen(default_bucket_key)\n            self.module = mx.mod.Module(symbol=symbol,\n                                        data_names=data_names,\n                                        label_names=label_names,\n                                        logger=logger,\n                                        context=self.context,\n                                        compression_params=self._gradient_compression_params,\n                                        fixed_param_names=fixed_param_names)\n\n        self.module.bind(data_shapes=provide_data,\n                         label_shapes=provide_label,\n                         for_training=True,\n                         force_rebind=True,\n                         grad_req='add' if self._gradient_accumulation else 'write')\n\n        self.module.symbol.save(os.path.join(self.output_dir, C.SYMBOL_NAME))\n\n        self.save_version(self.output_dir)\n        self.save_config(self.output_dir)", "response": "Initializes the training symbol and module."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating a list of all parameter names and a strategy.", "response": "def _generate_fixed_param_names(self, param_names: List[str], strategy: str) -> List[str]:\n        \"\"\"\n        Generate a fixed parameter list given a list of all parameter names and\n        a strategy.\n        \"\"\"\n        # Number of encoder/decoder layers in model.\n        if isinstance(self.config.config_encoder, EmptyEncoderConfig):\n            num_encoder_layers = 1\n        elif isinstance(self.config.config_encoder, RecurrentEncoderConfig):\n            num_encoder_layers = self.config.config_encoder.rnn_config.num_layers\n        else:\n            num_encoder_layers = self.config.config_encoder.num_layers\n        if isinstance(self.config.config_decoder, RecurrentDecoderConfig):\n            num_decoder_layers = self.config.config_decoder.rnn_config.num_layers\n        else:\n            num_decoder_layers = self.config.config_decoder.num_layers\n\n        def is_fixed(name: str) -> bool:\n            if strategy == C.FIXED_PARAM_STRATEGY_ALL_EXCEPT_DECODER:\n                # Any decoder layer.\n                return not name.startswith(C.DECODER_PREFIX)\n            if strategy == C.FIXED_PARAM_STRATEGY_ALL_EXCEPT_OUTER_LAYERS:\n                # First and last encoder and decoder layers for RNN,\n                # Transformer, and CNN models.\n                return not (name.startswith(\"{}{}l{}\".format(C.BIDIRECTIONALRNN_PREFIX, C.FORWARD_PREFIX, 0)) or\n                            name.startswith(\"{}{}l{}\".format(C.BIDIRECTIONALRNN_PREFIX, C.REVERSE_PREFIX, 0)) or\n                            name.startswith(\"{}l{}\".format(C.STACKEDRNN_PREFIX, num_encoder_layers - 2)) or\n                            name.startswith(\"{}l{}\".format(C.RNN_DECODER_PREFIX, 0)) or\n                            name.startswith(\"{}l{}\".format(C.RNN_DECODER_PREFIX, num_decoder_layers - 1)) or\n                            name.startswith(\"{}{}\".format(C.TRANSFORMER_ENCODER_PREFIX, 0)) or\n                            name.startswith(\"{}{}\".format(C.TRANSFORMER_ENCODER_PREFIX, num_encoder_layers - 1)) or\n                            name.startswith(\"{}{}\".format(C.TRANSFORMER_DECODER_PREFIX, 0)) or\n                            name.startswith(\"{}{}\".format(C.TRANSFORMER_DECODER_PREFIX, num_decoder_layers - 1)) or\n                            name.startswith(\"{}{}\".format(C.CNN_ENCODER_PREFIX, 0)) or\n                            name.startswith(\"{}{}\".format(C.CNN_ENCODER_PREFIX, num_encoder_layers - 1)) or\n                            name.startswith(\"{}{}\".format(C.CNN_DECODER_PREFIX, 0)) or\n                            name.startswith(\"{}{}\".format(C.CNN_DECODER_PREFIX, num_decoder_layers - 1)))\n            if strategy == C.FIXED_PARAM_STRATEGY_ALL_EXCEPT_EMBEDDINGS:\n                # Any type of learned embedding.\n                return not (name.startswith(C.SOURCE_EMBEDDING_PREFIX) or\n                            name.startswith(C.SOURCE_POSITIONAL_EMBEDDING_PREFIX) or\n                            name.startswith(C.TARGET_EMBEDDING_PREFIX) or\n                            name.startswith(C.TARGET_POSITIONAL_EMBEDDING_PREFIX) or\n                            name.startswith(C.SHARED_EMBEDDING_PREFIX))\n            if strategy == C.FIXED_PARAM_STRATEGY_ALL_EXCEPT_OUTPUT_PROJ:\n                # Target output projection.\n                return not name.startswith(C.DEFAULT_OUTPUT_LAYER_PREFIX)\n            raise ValueError(\"Unknown fixed parameter strategy: %s\" % strategy)\n\n        return [name for name in param_names if is_fixed(name)]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nruns forward and backward pass and updates training metric.", "response": "def run_forward_backward(self, batch: mx.io.DataBatch, metric: mx.metric.EvalMetric):\n        \"\"\"\n        Runs forward/backward pass and updates training metric(s).\n        \"\"\"\n        self.module.forward_backward(batch)\n        self.module.update_metric(metric, batch.label)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_gradients(self) -> Dict[str, List[mx.nd.NDArray]]:\n        # We may have None if not all parameters are optimized\n        return {\"dev_%d_%s\" % (i, name): exe.grad_arrays[j] for i, exe in enumerate(self.executors) for j, name in\n                enumerate(self.executor_group.arg_names)\n                if name in self.executor_group.param_names and self.executors[0].grad_arrays[j] is not None}", "response": "Returns a mapping of parameters names to gradient arrays."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_global_gradient_norm(self) -> float:\n        # average norm across executors:\n        exec_norms = [global_norm([arr for arr in exe.grad_arrays if arr is not None]) for exe in self.executors]\n        norm_val = sum(exec_norms) / float(len(exec_norms))\n        norm_val *= self.optimizer.rescale_grad\n        return norm_val", "response": "Returns global gradient norm."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef evaluate(self, eval_iter: data_io.BaseParallelSampleIter, eval_metric: mx.metric.EvalMetric):\n        for eval_batch in eval_iter:\n            self.module.forward(eval_batch, is_train=False)\n            self.module.update_metric(eval_metric, eval_batch.label)", "response": "Evaluate the evaluation metric on the given data iterator."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef optimizer(self) -> Union[mx.optimizer.Optimizer, SockeyeOptimizer]:\n        # TODO: Push update to MXNet to expose the optimizer (Module should have a get_optimizer method)\n        return self.current_module._optimizer", "response": "Returns the underlying optimizer of the underlying module."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninitializes the optimizer of the underlying module with an optimizer config.", "response": "def initialize_optimizer(self, config: OptimizerConfig):\n        \"\"\"\n        Initializes the optimizer of the underlying module with an optimizer config.\n        \"\"\"\n        self.module.init_optimizer(kvstore=config.kvstore,\n                                   optimizer=config.name,\n                                   optimizer_params=config.params,\n                                   force_init=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef initialize_parameters(self, initializer: mx.init.Initializer, allow_missing_params: bool):\n        self.module.init_params(initializer=initializer,\n                                arg_params=self.params,\n                                aux_params=self.aux_params,\n                                allow_missing=allow_missing_params,\n                                force_init=False)", "response": "Initializes the parameters of the underlying module."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef log_parameters(self):\n        arg_params, aux_params = self.module.get_params()\n        total_parameters = 0\n        fixed_parameters = 0\n        learned_parameters = 0\n        info = []  # type: List[str]\n        for name, array in sorted(arg_params.items()):\n            info.append(\"%s: %s\" % (name, array.shape))\n            num_parameters = reduce(lambda x, y: x * y, array.shape)\n            total_parameters += num_parameters\n            if name in self.module._fixed_param_names:\n                fixed_parameters += num_parameters\n            else:\n                learned_parameters += num_parameters\n        percent_fixed = 100 * (fixed_parameters / max(1, total_parameters))\n        percent_learned = 100 * (learned_parameters / max(1, total_parameters))\n        logger.info(\"Model parameters: %s\", \", \".join(info))\n        logger.info(\"Fixed model parameters: %s\", \", \".join(self.module._fixed_param_names))\n        logger.info(\"Fixing %d parameters (%0.2f%%)\", fixed_parameters, percent_fixed)\n        logger.info(\"Learning %d parameters (%0.2f%%)\", learned_parameters, percent_learned)\n        logger.info(\"Total # of parameters: %d\", total_parameters)", "response": "Logs information about model parameters."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef save_params_to_file(self, fname: str):\n        arg_params, aux_params = self.module.get_params()\n        self.module.set_params(arg_params, aux_params)\n        self.params = arg_params\n        self.aux_params = aux_params\n        super().save_params_to_file(fname)", "response": "Synchronizes parameters across devices saves the parameters to disk and updates self. params and self. aux_params."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading parameters from a file and sets the parameters of the underlying module and this model instance.", "response": "def load_params_from_file(self, fname: str, allow_missing_params: bool = False):\n        \"\"\"\n        Loads parameters from a file and sets the parameters of the underlying module and this model instance.\n\n        :param fname: File name to load parameters from.\n        :param allow_missing_params: If set, the given parameters are allowed to be a subset of the Module parameters.\n        \"\"\"\n        super().load_params_from_file(fname)  # sets self.params & self.aux_params\n        self.module.set_params(arg_params=self.params,\n                               aux_params=self.aux_params,\n                               allow_missing=allow_missing_params)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef install_monitor(self, monitor_pattern: str, monitor_stat_func_name: str):\n        self._monitor = mx.monitor.Monitor(interval=C.MEASURE_SPEED_EVERY,\n                                           stat_func=C.MONITOR_STAT_FUNCS.get(monitor_stat_func_name),\n                                           pattern=monitor_pattern,\n                                           sort=True)\n        self.module.install_monitor(self._monitor)\n        logger.info(\"Installed MXNet monitor; pattern='%s'; statistics_func='%s'\",\n                    monitor_pattern, monitor_stat_func_name)", "response": "Installs an MXNet monitor on the underlying module."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsaves this training state to fname.", "response": "def save(self, fname: str):\n        \"\"\"\n        Saves this training state to fname.\n        \"\"\"\n        with open(fname, \"wb\") as fp:\n            pickle.dump(self, fp)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fit(self,\n            train_iter: data_io.BaseParallelSampleIter,\n            validation_iter: data_io.BaseParallelSampleIter,\n            early_stopping_metric,\n            metrics: List[str],\n            checkpoint_interval: int,\n            max_num_not_improved: int,\n            max_checkpoints: Optional[int] = None,\n            min_samples: Optional[int] = None,\n            max_samples: Optional[int] = None,\n            min_updates: Optional[int] = None,\n            max_updates: Optional[int] = None,\n            min_epochs: Optional[int] = None,\n            max_epochs: Optional[int] = None,\n            lr_decay_param_reset: bool = False,\n            lr_decay_opt_states_reset: str = C.LR_DECAY_OPT_STATES_RESET_OFF,\n            decoder: Optional[checkpoint_decoder.CheckpointDecoder] = None,\n            mxmonitor_pattern: Optional[str] = None,\n            mxmonitor_stat_func: Optional[str] = None,\n            allow_missing_parameters: bool = False,\n            existing_parameters: Optional[str] = None) -> TrainState:\n        \"\"\"\n        Fits model to data given by train_iter using early-stopping w.r.t data given by val_iter.\n        Saves all intermediate and final output to output_folder.\n\n        :param train_iter: The training data iterator.\n        :param validation_iter: The data iterator for held-out data.\n\n        :param early_stopping_metric: The metric that is evaluated on held-out data and optimized.\n        :param metrics: List of metrics that will be tracked during training.\n        :param checkpoint_interval: Frequency of checkpoints in number of update steps.\n\n        :param max_num_not_improved: Stop training if early_stopping_metric did not improve for this many checkpoints.\n               Use -1 to disable stopping based on early_stopping_metric.\n        :param max_checkpoints: Stop training after this many checkpoints.\n               Use None to disable.\n\n        :param min_samples: Optional minimum number of samples.\n        :param max_samples: Optional maximum number of samples.\n        :param min_updates: Optional minimum number of update steps.\n        :param max_updates: Optional maximum number of update steps.\n        :param min_epochs: Optional minimum number of epochs to train, overrides early stopping.\n        :param max_epochs: Optional maximum number of epochs to train, overrides early stopping.\n\n        :param lr_decay_param_reset: Reset parameters to previous best after a learning rate decay.\n        :param lr_decay_opt_states_reset: How to reset optimizer states after a learning rate decay.\n\n        :param decoder: Optional CheckpointDecoder instance to decode and compute evaluation metrics.\n        :param mxmonitor_pattern: Optional pattern to match to monitor weights/gradients/outputs\n               with MXNet's monitor. Default is None which means no monitoring.\n        :param mxmonitor_stat_func: Choice of statistics function to run on monitored weights/gradients/outputs\n               when using MXNEt's monitor.\n\n        :param allow_missing_parameters: Allow missing parameters when initializing model parameters from file.\n        :param existing_parameters: Optional filename of existing/pre-trained parameters to initialize from.\n\n        :return: Training state.\n        \"\"\"\n        self._check_args(metrics, early_stopping_metric, lr_decay_opt_states_reset, lr_decay_param_reset, decoder)\n        logger.info(\"Early stopping by optimizing '%s'\", early_stopping_metric)\n\n        self._initialize_parameters(existing_parameters, allow_missing_parameters)\n        self._initialize_optimizer()\n\n        resume_training = os.path.exists(self.training_state_dirname)\n        if resume_training:\n            logger.info(\"Found partial training in '%s'. Resuming from saved state.\", self.training_state_dirname)\n            utils.check_condition('dist' not in self.optimizer_config.kvstore,\n                                  \"Training continuation not supported with distributed training.\")\n            self._load_training_state(train_iter)\n        else:\n            self.state = TrainState(early_stopping_metric)\n            self._save_params()\n            self._update_best_params_link()\n            self._save_training_state(train_iter)\n            self._save_initial_optimizer_states(lr_decay_opt_states_reset)\n            self._update_best_optimizer_states(lr_decay_opt_states_reset)\n            self.tflogger.log_graph(self.model.current_module.symbol)\n            logger.info(\"Training started.\")\n\n        metric_train, metric_val, metric_loss = self._create_metrics(metrics, self.model.optimizer, self.model.loss)\n\n        process_manager = None\n        if decoder is not None:\n            process_manager = DecoderProcessManager(self.model.output_dir, decoder=decoder)\n\n            if self.stop_training_on_decoder_failure:\n                # Start an initial decoder process to fail early in case we run out of memory\n                process_manager.start_decoder(checkpoint=0)\n\n        if mxmonitor_pattern is not None:\n            self.model.install_monitor(mxmonitor_pattern, mxmonitor_stat_func)\n\n        speedometer = Speedometer(frequency=C.MEASURE_SPEED_EVERY, auto_reset=False)\n        tic = time.time()\n\n        if max_checkpoints is not None:\n            max_updates = self.state.updates + max_checkpoints * checkpoint_interval\n            logger.info((\"Resetting max_updates to %d + %d * %d = %d in order to implement stopping after (an additional) %d checkpoints.\"\n                         % (self.state.updates, max_checkpoints, checkpoint_interval, max_updates, max_checkpoints)))\n\n        next_data_batch = train_iter.next()\n        while True:\n\n            if max_epochs is not None and self.state.epoch == max_epochs:\n                logger.info(\"Maximum # of epochs (%s) reached.\", max_epochs)\n                break\n\n            if max_updates is not None and self.state.updates == max_updates:\n                logger.info(\"Maximum # of updates (%s) reached.\", max_updates)\n                break\n\n            if max_samples is not None and self.state.samples >= max_samples:\n                logger.info(\"Maximum # of samples (%s) reached\", max_samples)\n                break\n\n            ######\n            # STEP\n            ######\n            batch = next_data_batch\n            self.state.batches += 1\n            self._step(self.model, batch, checkpoint_interval, metric_train, metric_loss)\n            batch_num_samples = batch.data[0].shape[0]\n            batch_num_tokens = batch.data[0].shape[1] * batch_num_samples\n            self.state.samples += batch_num_samples\n\n            if not train_iter.iter_next():\n                self.state.epoch += 1\n                train_iter.reset()\n\n            next_data_batch = train_iter.next()\n            self.model.prepare_batch(next_data_batch)\n\n            speedometer(self.state.epoch, self.state.batches, self.state.updates,\n                        batch_num_samples, batch_num_tokens, metric_train)\n\n            ############\n            # CHECKPOINT\n            ############\n            if self.state.updates > 0 and self.state.batches % (checkpoint_interval * self.update_interval) == 0:\n                time_cost = time.time() - tic\n                self.state.checkpoint += 1\n                # (1) save parameters and evaluate on validation data\n                self._save_params()\n                logger.info(\"Checkpoint [%d]\\tUpdates=%d Epoch=%d Samples=%d Time-cost=%.3f Updates/sec=%.3f\",\n                            self.state.checkpoint, self.state.updates, self.state.epoch,\n                            self.state.samples, time_cost, checkpoint_interval / time_cost)\n                for name, val in metric_train.get_name_value():\n                    logger.info('Checkpoint [%d]\\tTrain-%s=%f', self.state.checkpoint, name, val)\n                self._evaluate(validation_iter, metric_val)\n                for name, val in metric_val.get_name_value():\n                    logger.info('Checkpoint [%d]\\tValidation-%s=%f', self.state.checkpoint, name, val)\n\n                # (2) wait for checkpoint decoder results and fill self.state.metrics\n                if process_manager is not None:\n                    result = process_manager.collect_results()\n                    if result is not None:\n                        decoded_checkpoint, decoder_metrics = result\n                        # The first checkpoint before any gradient updates is ignored\n                        if decoded_checkpoint > 0:\n                            self.state.metrics[decoded_checkpoint - 1].update(decoder_metrics)\n                            self.tflogger.log_metrics(decoder_metrics, decoded_checkpoint)\n                            utils.write_metrics_file(self.state.metrics, self.metrics_fname)\n                    # Start the decoder for the next checkpoint\n                    process_manager.start_decoder(self.state.checkpoint)\n\n                # (3) determine improvement\n                has_improved = False\n                previous_best = self.state.best_metric\n                # at this point state.self.metrics doesn't have perplexity validation results yet\n                current_checkpoint_val_metric = {\"%s-val\" % name: val for name, val in metric_val.get_name_value()}\n                for checkpoint, metric_dict in enumerate(self.state.metrics + [current_checkpoint_val_metric], 1):\n                    value = metric_dict.get(\"%s-val\" % early_stopping_metric, self.state.best_metric)\n                    if utils.metric_value_is_better(value, self.state.best_metric, early_stopping_metric):\n                        self.state.best_metric = value\n                        self.state.best_checkpoint = checkpoint\n                        has_improved = True\n\n                if has_improved:\n                    self._update_best_params_link()\n                    self._update_best_optimizer_states(lr_decay_opt_states_reset)\n                    self.state.num_not_improved = 0\n                    logger.info(\"Validation-%s improved to %f (delta=%f).\", early_stopping_metric,\n                                self.state.best_metric, abs(self.state.best_metric - previous_best))\n                else:\n                    self.state.num_not_improved += 1\n                    logger.info(\"Validation-%s has not improved for %d checkpoints, best so far: %f\",\n                                early_stopping_metric, self.state.num_not_improved, self.state.best_metric)\n\n                # (4) determine stopping\n                if 0 <= max_num_not_improved <= self.state.num_not_improved:\n                    logger.info(\"Maximum number of not improved checkpoints (%d) reached: %d\",\n                                max_num_not_improved, self.state.num_not_improved)\n                    self.state.converged = True\n\n                    if min_epochs is not None and self.state.epoch < min_epochs:\n                        logger.info(\"Minimum number of epochs (%d) not reached yet: %d\",\n                                    min_epochs, self.state.epoch)\n                        self.state.converged = False\n\n                    if min_updates is not None and self.state.updates < min_updates:\n                        logger.info(\"Minimum number of updates (%d) not reached yet: %d\",\n                                    min_updates, self.state.updates)\n                        self.state.converged = False\n\n                    if min_samples is not None and self.state.samples < min_samples:\n                        logger.info(\"Minimum number of samples (%d) not reached yet: %d\",\n                                    min_samples, self.state.samples)\n                        self.state.converged = False\n\n                # (5) detect divergence with respect to the perplexity value at the last checkpoint\n                if self.state.metrics and not has_improved:\n                    last_ppl_value = current_checkpoint_val_metric[\"%s-val\" % C.PERPLEXITY]\n                    # using a double of uniform distribution's value as a threshold\n                    if not np.isfinite(last_ppl_value) or last_ppl_value > 2 * len(self.target_vocab):\n                        logger.warning(\"Model optimization diverged. Last checkpoint's perplexity: %f\",\n                                       last_ppl_value)\n                        self.state.diverged = True\n\n                # (6) update and write training/validation metrics late to capture converged/diverged status\n                self._update_metrics(metric_train, metric_val)\n                metric_train.reset()\n\n                # If using an extended optimizer, provide extra state information about the current checkpoint\n                # Loss: optimized metric\n                if metric_loss is not None and isinstance(self.model.optimizer, SockeyeOptimizer):\n                    m_val = 0\n                    for name, val in metric_val.get_name_value():\n                        if name == early_stopping_metric:\n                            m_val = val\n                    checkpoint_state = CheckpointState(checkpoint=self.state.checkpoint, metric_val=m_val)\n                    self.model.optimizer.pre_update_checkpoint(checkpoint_state)\n\n                # (7) adjust learning rates\n                self._adjust_learning_rate(has_improved, lr_decay_param_reset, lr_decay_opt_states_reset)\n\n                # (8) save training state\n                self._save_training_state(train_iter)\n\n                if self.state.converged or self.state.diverged:\n                    break\n\n                tic = time.time()\n\n            if process_manager is not None:\n                process_manager.update_process_died_status()\n                if self.stop_training_on_decoder_failure and process_manager.any_process_died:\n                    logger.info(\"A decoder process has died, will stop training as this was requested via %s\",\n                                C.TRAIN_ARGS_STOP_ON_DECODER_FAILURE)\n                    break\n\n        self._cleanup(lr_decay_opt_states_reset, process_manager=process_manager,\n                      keep_training_state=not self.state.converged and not self.state.diverged)\n        logger.info(\"Training finished%s. Best checkpoint: %d. Best validation %s: %.6f\",\n                    \", can be continued later\" if not self.state.converged else \"\",\n                    self.state.best_checkpoint, early_stopping_metric, self.state.best_metric)\n\n        return self.state", "response": "Fits model to data given by train_iter using early - stopping w. r. t data given by validation_iter."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nperform an update to the model given a batch and updates metrics.", "response": "def _step(self,\n              model: TrainingModel,\n              batch: mx.io.DataBatch,\n              checkpoint_interval: int,\n              metric_train: mx.metric.EvalMetric,\n              metric_loss: Optional[mx.metric.EvalMetric] = None):\n        \"\"\"\n        Performs an update to model given a batch and updates metrics.\n        \"\"\"\n\n        if model.monitor is not None:\n            model.monitor.tic()\n\n        ####################\n        # Forward & Backward\n        ####################\n        model.run_forward_backward(batch, metric_train)\n\n        # If using an extended optimizer, provide extra state information about the current batch\n        optimizer = model.optimizer\n        if metric_loss is not None and isinstance(optimizer, SockeyeOptimizer):\n            # Loss for this batch\n            metric_loss.reset()\n            metric_loss.update(batch.label, model.module.get_outputs())\n            [(_, m_val)] = metric_loss.get_name_value()\n            batch_state = BatchState(metric_val=m_val)\n            optimizer.pre_update_batch(batch_state)\n\n        ########\n        # UPDATE\n        ########\n        if self.update_interval == 1 or self.state.batches % self.update_interval == 0:\n\n            # Gradient rescaling\n            gradient_norm = None\n            if self.state.updates > 0 and (self.state.updates + 1) % checkpoint_interval == 0:\n                # compute values for logging to metrics (before rescaling...)\n                gradient_norm = self.state.gradient_norm = model.get_global_gradient_norm()\n                self.state.gradients = model.get_gradients()\n\n            # note: C.GRADIENT_CLIPPING_TYPE_ABS is handled by the mxnet optimizer directly\n            if self.optimizer_config.gradient_clipping_type == C.GRADIENT_CLIPPING_TYPE_NORM:\n                if gradient_norm is None:\n                    gradient_norm = model.get_global_gradient_norm()\n                # clip gradients\n                if gradient_norm > self.optimizer_config.gradient_clipping_threshold:\n                    ratio = self.optimizer_config.gradient_clipping_threshold / gradient_norm\n                    model.rescale_gradients(ratio)\n\n            model.update()\n\n            if self.update_interval > 1:\n                model.zero_gradients()\n\n            self.state.updates += 1\n\n        if model.monitor is not None:\n            results = model.monitor.toc()\n            if results:\n                for _, k, v in results:\n                    logger.info('Monitor: Batch [{:d}] {:s} {:s}'.format(self.state.updates, k, v))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _evaluate(self, val_iter: data_io.BaseParallelSampleIter, val_metric: mx.metric.EvalMetric):\n        val_iter.reset()\n        val_metric.reset()\n        self.model.evaluate(val_iter, val_metric)", "response": "Evaluates the model on the validation data and updates the validation metric."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _update_metrics(self,\n                        metric_train: mx.metric.EvalMetric,\n                        metric_val: mx.metric.EvalMetric):\n        \"\"\"\n        Updates metrics for current checkpoint. If a process manager is given, also collects previous decoding results\n        and spawns a new decoding process.\n        Writes all metrics to the metrics file and optionally logs to tensorboard.\n        \"\"\"\n        checkpoint_metrics = {\"epoch\": self.state.epoch,\n                              \"learning-rate\": self.model.optimizer.learning_rate,\n                              \"gradient-norm\": self.state.gradient_norm,\n                              \"time-elapsed\": time.time() - self.state.start_tic}\n        gpu_memory_usage = utils.get_gpu_memory_usage(self.model.context)\n        checkpoint_metrics['used-gpu-memory'] = sum(v[0] for v in gpu_memory_usage.values())\n        checkpoint_metrics['converged'] = self.state.converged\n        checkpoint_metrics['diverged'] = self.state.diverged\n\n        for name, value in metric_train.get_name_value():\n            checkpoint_metrics[\"%s-train\" % name] = value\n        for name, value in metric_val.get_name_value():\n            checkpoint_metrics[\"%s-val\" % name] = value\n\n        self.state.metrics.append(checkpoint_metrics)\n        utils.write_metrics_file(self.state.metrics, self.metrics_fname)\n\n        tf_metrics = checkpoint_metrics.copy()\n        tf_metrics.update({\"%s_grad\" % n: v for n, v in self.state.gradients.items()})\n        tf_metrics.update(self.model.params)\n        self.tflogger.log_metrics(metrics=tf_metrics, checkpoint=self.state.checkpoint)", "response": "Updates the metrics for the current checkpoint. If a process manager is given also collects previous decoding results\n        and spawns a new decoding process."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncleans up training state files and updates the state metrics file.", "response": "def _cleanup(self, lr_decay_opt_states_reset: str, process_manager: Optional['DecoderProcessManager'] = None,\n                 keep_training_state = False):\n        \"\"\"\n        Cleans parameter files, training state directory and waits for remaining decoding processes.\n        \"\"\"\n        utils.cleanup_params_files(self.model.output_dir, self.max_params_files_to_keep,\n                                   self.state.checkpoint, self.state.best_checkpoint, self.keep_initializations)\n        if process_manager is not None:\n            result = process_manager.collect_results()\n            if result is not None:\n                decoded_checkpoint, decoder_metrics = result\n                self.state.metrics[decoded_checkpoint - 1].update(decoder_metrics)\n                self.tflogger.log_metrics(decoder_metrics, decoded_checkpoint)\n                utils.write_metrics_file(self.state.metrics, self.metrics_fname)\n                self.state.save(os.path.join(self.training_state_dirname, C.TRAINING_STATE_NAME))\n\n        if not keep_training_state:\n            final_training_state_dirname = os.path.join(self.model.output_dir, C.TRAINING_STATE_DIRNAME)\n            if os.path.exists(final_training_state_dirname):\n                shutil.rmtree(final_training_state_dirname)\n            if lr_decay_opt_states_reset == C.LR_DECAY_OPT_STATES_RESET_BEST:\n                best_opt_states_fname = os.path.join(self.model.output_dir, C.OPT_STATES_BEST)\n                if os.path.exists(best_opt_states_fname):\n                    os.remove(best_opt_states_fname)\n            if lr_decay_opt_states_reset == C.LR_DECAY_OPT_STATES_RESET_INITIAL:\n                initial_opt_states_fname = os.path.join(self.model.output_dir, C.OPT_STATES_INITIAL)\n                if os.path.exists(initial_opt_states_fname):\n                    os.remove(initial_opt_states_fname)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadjusts the learning rate of the model if required.", "response": "def _adjust_learning_rate(self, has_improved: bool, lr_decay_param_reset: bool, lr_decay_opt_states_reset: str):\n        \"\"\"\n        Adjusts the optimizer learning rate if required.\n        \"\"\"\n        if self.optimizer_config.lr_scheduler is not None:\n            if issubclass(type(self.optimizer_config.lr_scheduler), lr_scheduler.AdaptiveLearningRateScheduler):\n                lr_adjusted = self.optimizer_config.lr_scheduler.new_evaluation_result(has_improved)  # type: ignore\n            else:\n                lr_adjusted = False\n            if lr_adjusted and not has_improved:\n                if lr_decay_param_reset:\n                    logger.info(\"Loading parameters from last best checkpoint: %d\",\n                                self.state.best_checkpoint)\n                    self.model.load_params_from_file(self.best_params_fname)\n                if lr_decay_opt_states_reset == C.LR_DECAY_OPT_STATES_RESET_INITIAL:\n                    logger.info(\"Loading initial optimizer states\")\n                    self.model.load_optimizer_states(os.path.join(self.model.output_dir, C.OPT_STATES_INITIAL))\n                elif lr_decay_opt_states_reset == C.LR_DECAY_OPT_STATES_RESET_BEST:\n                    logger.info(\"Loading optimizer states from best checkpoint: %d\",\n                                self.state.best_checkpoint)\n                    self.model.load_optimizer_states(os.path.join(self.model.output_dir, C.OPT_STATES_BEST))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating an EvalMetric given a metric name.", "response": "def _create_eval_metric(metric_name: str) -> mx.metric.EvalMetric:\n        \"\"\"\n        Creates an EvalMetric given a metric names.\n        \"\"\"\n        # output_names refers to the list of outputs this metric should use to update itself, e.g. the softmax output\n        if metric_name == C.ACCURACY:\n            return utils.Accuracy(ignore_label=C.PAD_ID, output_names=[C.SOFTMAX_OUTPUT_NAME], label_names=[C.TARGET_LABEL_NAME])\n        elif metric_name == C.PERPLEXITY:\n            return mx.metric.Perplexity(ignore_label=C.PAD_ID, output_names=[C.SOFTMAX_OUTPUT_NAME], label_names=[C.TARGET_LABEL_NAME], name=C.PERPLEXITY)\n        elif metric_name == C.LENRATIO_MSE:\n            return loss.LengthRatioMSEMetric(name=C.LENRATIO_MSE,\n                                  output_names=[C.LENRATIO_OUTPUT_NAME], label_names=[C.LENRATIO_LABEL_OUTPUT_NAME])\n        else:\n            raise ValueError(\"unknown metric name\")"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _create_eval_metric_composite(metric_names: List[str]) -> mx.metric.CompositeEvalMetric:\n        metrics = [EarlyStoppingTrainer._create_eval_metric(metric_name) for metric_name in metric_names]\n        return mx.metric.create(metrics)", "response": "Creates a composite EvalMetric given a list of metric names."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate the params. best link to the latest best parameter file.", "response": "def _update_best_params_link(self):\n        \"\"\"\n        Updates the params.best link to the latest best parameter file.\n        \"\"\"\n        best_params_path = self.best_params_fname\n        actual_best_params_fname = C.PARAMS_NAME % self.state.best_checkpoint\n        if os.path.lexists(best_params_path):\n            os.remove(best_params_path)\n        os.symlink(actual_best_params_fname, best_params_path)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking the arguments for various checks.", "response": "def _check_args(self,\n                    metrics: List[str],\n                    early_stopping_metric: str,\n                    lr_decay_opt_states_reset: str,\n                    lr_decay_param_reset: bool,\n                    cp_decoder: Optional[checkpoint_decoder.CheckpointDecoder] = None):\n        \"\"\"\n        Helper function that checks various configuration compatibilities.\n        \"\"\"\n        utils.check_condition(len(metrics) > 0, \"At least one metric must be provided.\")\n        for metric in metrics:\n            utils.check_condition(metric in C.METRICS, \"Unknown metric to track during training: %s\" % metric)\n\n        if 'dist' in self.optimizer_config.kvstore:\n            # In distributed training the optimizer will run remotely. For eve we however need to pass information about\n            # the loss, which is not possible anymore by means of accessing self.module._curr_module._optimizer.\n            utils.check_condition(self.optimizer_config.name != C.OPTIMIZER_EVE,\n                                  \"Eve optimizer not supported with distributed training.\")\n            utils.check_condition(\n                not issubclass(type(self.optimizer_config.lr_scheduler),\n                               lr_scheduler.AdaptiveLearningRateScheduler),\n                \"Adaptive learning rate schedulers not supported with a dist kvstore. \"\n                \"Try a fixed schedule such as %s.\" % C.LR_SCHEDULER_FIXED_RATE_INV_SQRT_T)\n            utils.check_condition(not lr_decay_param_reset, \"Parameter reset when the learning rate decays not \"\n                                                            \"supported with distributed training.\")\n            utils.check_condition(lr_decay_opt_states_reset == C.LR_DECAY_OPT_STATES_RESET_OFF,\n                                  \"Optimizer state reset when the learning rate decays \"\n                                  \"not supported with distributed training.\")\n\n        utils.check_condition(self.optimizer_config.gradient_clipping_type in C.GRADIENT_CLIPPING_TYPES,\n                              \"Unknown gradient clipping type %s\" % self.optimizer_config.gradient_clipping_type)\n\n        utils.check_condition(early_stopping_metric in C.METRICS,\n                              \"Unsupported early-stopping metric: %s\" % early_stopping_metric)\n        if early_stopping_metric in C.METRICS_REQUIRING_DECODER:\n            utils.check_condition(cp_decoder is not None, \"%s requires CheckpointDecoder\" % early_stopping_metric)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsaving model parameters at current checkpoint and optionally cleans up older parameters files.", "response": "def _save_params(self):\n        \"\"\"\n        Saves model parameters at current checkpoint and optionally cleans up older parameter files to save disk space.\n        \"\"\"\n        self.model.save_params_to_file(self.current_params_fname)\n        utils.cleanup_params_files(self.model.output_dir, self.max_params_files_to_keep, self.state.checkpoint,\n                                   self.state.best_checkpoint, self.keep_initializations)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsave training state of AFAIK.", "response": "def _save_training_state(self, train_iter: data_io.BaseParallelSampleIter):\n        \"\"\"\n        Saves current training state.\n        \"\"\"\n        # Create temporary directory for storing the state of the optimization process\n        training_state_dirname = os.path.join(self.model.output_dir, C.TRAINING_STATE_TEMP_DIRNAME)\n        if not os.path.exists(training_state_dirname):\n            os.mkdir(training_state_dirname)\n\n        # (1) Parameters: link current file\n        params_base_fname = C.PARAMS_NAME % self.state.checkpoint\n        params_file = os.path.join(training_state_dirname, C.TRAINING_STATE_PARAMS_NAME)\n        if os.path.exists(params_file):\n            os.unlink(params_file)\n        os.symlink(os.path.join(\"..\", params_base_fname), params_file)\n\n        # (2) Optimizer states\n        opt_state_fname = os.path.join(training_state_dirname, C.OPT_STATES_LAST)\n        self.model.save_optimizer_states(opt_state_fname)\n\n        # (3) Data iterator\n        train_iter.save_state(os.path.join(training_state_dirname, C.BUCKET_ITER_STATE_NAME))\n\n        # (4) Random generators\n        # RNG states: python's random and np.random provide functions for\n        # storing the state, mxnet does not, but inside our code mxnet's RNG is\n        # not used AFAIK\n        with open(os.path.join(training_state_dirname, C.RNG_STATE_NAME), \"wb\") as fp:\n            pickle.dump(random.getstate(), fp)\n            pickle.dump(np.random.get_state(), fp)\n\n        # (5) Training state\n        self.state.save(os.path.join(training_state_dirname, C.TRAINING_STATE_NAME))\n\n        # (6) Learning rate scheduler\n        with open(os.path.join(training_state_dirname, C.SCHEDULER_STATE_NAME), \"wb\") as fp:\n            pickle.dump(self.optimizer_config.lr_scheduler, fp)\n\n        # First we rename the existing directory to minimize the risk of state\n        # loss if the process is aborted during deletion (which will be slower\n        # than directory renaming)\n        delete_training_state_dirname = os.path.join(self.model.output_dir, C.TRAINING_STATE_TEMP_DELETENAME)\n        if os.path.exists(self.training_state_dirname):\n            os.rename(self.training_state_dirname, delete_training_state_dirname)\n        os.rename(training_state_dirname, self.training_state_dirname)\n        if os.path.exists(delete_training_state_dirname):\n            shutil.rmtree(delete_training_state_dirname)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nloading the training state from disk.", "response": "def _load_training_state(self, train_iter: data_io.BaseParallelSampleIter):\n        \"\"\"\n        Loads the full training state from disk.\n\n        :param train_iter: training data iterator.\n        \"\"\"\n        # (1) Parameters\n        params_fname = os.path.join(self.training_state_dirname, C.TRAINING_STATE_PARAMS_NAME)\n        self.model.load_params_from_file(params_fname)\n\n        # (2) Optimizer states\n        opt_state_fname = os.path.join(self.training_state_dirname, C.OPT_STATES_LAST)\n        self.model.load_optimizer_states(opt_state_fname)\n\n        # (3) Data Iterator\n        train_iter.load_state(os.path.join(self.training_state_dirname, C.BUCKET_ITER_STATE_NAME))\n\n        # (4) Random generators\n        # RNG states: python's random and np.random provide functions for\n        # storing the state, mxnet does not, but inside our code mxnet's RNG is\n        # not used AFAIK\n        with open(os.path.join(self.training_state_dirname, C.RNG_STATE_NAME), \"rb\") as fp:\n            random.setstate(pickle.load(fp))\n            np.random.set_state(pickle.load(fp))\n\n        # (5) Training state\n        self.state = TrainState.load(os.path.join(self.training_state_dirname, C.TRAINING_STATE_NAME))\n\n        # (6) Learning rate scheduler\n        with open(os.path.join(self.training_state_dirname, C.SCHEDULER_STATE_NAME), \"rb\") as fp:\n            self.optimizer_config.set_lr_scheduler(pickle.load(fp))\n        # initialize optimizer again\n        self._initialize_optimizer()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef start_decoder(self, checkpoint: int):\n        assert self.decoder_process is None\n        output_name = os.path.join(self.output_folder, C.DECODE_OUT_NAME % checkpoint)\n        self.decoder_process = self.ctx.Process(target=_decode_and_evaluate,\n                                                args=(self.decoder, checkpoint, output_name, self.decoder_metric_queue))\n        self.decoder_process.name = 'Decoder-%d' % checkpoint\n        logger.info(\"Starting process: %s\", self.decoder_process.name)\n        self.decoder_process.start()\n        self._results_pending = True", "response": "Starts a new CheckpointDecoder process and returns."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncollecting the results from the decoder and return them.", "response": "def collect_results(self) -> Optional[Tuple[int, Dict[str, float]]]:\n        \"\"\"\n        Returns the decoded checkpoint and the decoder metrics or None if the queue is empty.\n        \"\"\"\n        self.wait_to_finish()\n        if self.decoder_metric_queue.empty():\n            if self._results_pending:\n                self._any_process_died = True\n            self._results_pending = False\n            return None\n        decoded_checkpoint, decoder_metrics = self.decoder_metric_queue.get()\n        assert self.decoder_metric_queue.empty()\n        self._results_pending = False\n        logger.info(\"Decoder-%d finished: %s\", decoded_checkpoint, decoder_metrics)\n        return decoded_checkpoint, decoder_metrics"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update_process_died_status(self):\n\n        # There is a result pending, the process is no longer alive, yet there is no result in the queue\n        # This means the decoder process has not succesfully produced metrics\n        queue_should_hold_result = self._results_pending and self.decoder_process is not None and not self.decoder_process.is_alive()\n        if queue_should_hold_result and self.decoder_metric_queue.empty():\n            self._any_process_died = True", "response": "Update the flag indicating whether any process exited and did not provide a result."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\naverages parameters from a list of. params file paths.", "response": "def average(param_paths: Iterable[str]) -> Dict[str, mx.nd.NDArray]:\n    \"\"\"\n    Averages parameters from a list of .params file paths.\n\n    :param param_paths: List of paths to parameter files.\n    :return: Averaged parameter dictionary.\n    \"\"\"\n    all_arg_params = []\n    all_aux_params = []\n    for path in param_paths:\n        logger.info(\"Loading parameters from '%s'\", path)\n        arg_params, aux_params = utils.load_params(path)\n        all_arg_params.append(arg_params)\n        all_aux_params.append(aux_params)\n\n    logger.info(\"%d models loaded\", len(all_arg_params))\n    utils.check_condition(all(all_arg_params[0].keys() == p.keys() for p in all_arg_params),\n                          \"arg_param names do not match across models\")\n    utils.check_condition(all(all_aux_params[0].keys() == p.keys() for p in all_aux_params),\n                          \"aux_param names do not match across models\")\n\n    avg_params = {}\n    # average arg_params\n    for k in all_arg_params[0]:\n        arrays = [p[k] for p in all_arg_params]\n        avg_params[\"arg:\" + k] = utils.average_arrays(arrays)\n    # average aux_params\n    for k in all_aux_params[0]:\n        arrays = [p[k] for p in all_aux_params]\n        avg_params[\"aux:\" + k] = utils.average_arrays(arrays)\n\n    return avg_params"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfind N best points from model file according to strategy.", "response": "def find_checkpoints(model_path: str, size=4, strategy=\"best\", metric: str = C.PERPLEXITY) -> List[str]:\n    \"\"\"\n    Finds N best points from .metrics file according to strategy.\n\n    :param model_path: Path to model.\n    :param size: Number of checkpoints to combine.\n    :param strategy: Combination strategy.\n    :param metric: Metric according to which checkpoints are selected.  Corresponds to columns in model/metrics file.\n    :return: List of paths corresponding to chosen checkpoints.\n    \"\"\"\n    maximize = C.METRIC_MAXIMIZE[metric]\n    points = utils.get_validation_metric_points(model_path=model_path, metric=metric)\n    # keep only points for which .param files exist\n    param_path = os.path.join(model_path, C.PARAMS_NAME)\n    points = [(value, checkpoint) for value, checkpoint in points if os.path.exists(param_path % checkpoint)]\n\n    if strategy == \"best\":\n        # N best scoring points\n        top_n = _strategy_best(points, size, maximize)\n\n    elif strategy == \"last\":\n        # N sequential points ending with overall best\n        top_n = _strategy_last(points, size, maximize)\n\n    elif strategy == \"lifespan\":\n        # Track lifespan of every \"new best\" point\n        # Points dominated by a previous better point have lifespan 0\n        top_n = _strategy_lifespan(points, size, maximize)\n    else:\n        raise RuntimeError(\"Unknown strategy, options: best last lifespan\")\n\n    # Assemble paths for params files corresponding to chosen checkpoints\n    # Last element in point is always the checkpoint id\n    params_paths = [\n        os.path.join(model_path, C.PARAMS_NAME % point[-1]) for point in top_n\n    ]\n\n    # Report\n    logger.info(\"Found: \" + \", \".join(str(point) for point in top_n))\n\n    return params_paths"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_lr_scheduler(scheduler_type: str,\n                     updates_per_checkpoint: int,\n                     learning_rate_half_life: int,\n                     learning_rate_reduce_factor: float,\n                     learning_rate_reduce_num_not_improved: int,\n                     learning_rate_schedule: Optional[List[Tuple[float, int]]] = None,\n                     learning_rate_warmup: Optional[int] = 0) -> Optional[LearningRateScheduler]:\n    \"\"\"\n    Returns a learning rate scheduler.\n\n    :param scheduler_type: Scheduler type.\n    :param updates_per_checkpoint: Number of batches between checkpoints.\n    :param learning_rate_half_life: Half life of the learning rate in number of checkpoints.\n    :param learning_rate_reduce_factor: Factor to reduce learning rate with.\n    :param learning_rate_reduce_num_not_improved: Number of checkpoints with no improvement after which learning rate is\n           reduced.\n    :param learning_rate_schedule: Optional fixed learning rate schedule.\n    :param learning_rate_warmup: Number of batches that the learning rate is linearly increased.\n    :raises: ValueError if unknown scheduler_type\n    :return: Learning rate scheduler.\n    \"\"\"\n    check_condition(learning_rate_schedule is None or scheduler_type == C.LR_SCHEDULER_FIXED_STEP,\n                    \"Learning rate schedule can only be used with '%s' learning rate scheduler.\"\n                    % C.LR_SCHEDULER_FIXED_STEP)\n    if scheduler_type is None:\n        return None\n    if scheduler_type == C.LR_SCHEDULER_FIXED_RATE_INV_SQRT_T:\n        return LearningRateSchedulerInvSqrtT(updates_per_checkpoint, learning_rate_half_life, learning_rate_warmup)\n    elif scheduler_type == C.LR_SCHEDULER_FIXED_RATE_INV_T:\n        return LearningRateSchedulerInvT(updates_per_checkpoint, learning_rate_half_life, learning_rate_warmup)\n    elif scheduler_type == C.LR_SCHEDULER_FIXED_STEP:\n        check_condition(learning_rate_schedule is not None,\n                        \"learning_rate_schedule needed for %s scheduler\" % C.LR_SCHEDULER_FIXED_STEP)\n        return LearningRateSchedulerFixedStep(learning_rate_schedule, updates_per_checkpoint)\n    elif scheduler_type == C.LR_SCHEDULER_PLATEAU_REDUCE:\n        check_condition(learning_rate_reduce_factor is not None,\n                        \"learning_rate_reduce_factor needed for %s scheduler\" % C.LR_SCHEDULER_PLATEAU_REDUCE)\n        check_condition(learning_rate_reduce_num_not_improved is not None,\n                        \"learning_rate_reduce_num_not_improved needed for %s scheduler\" % C.LR_SCHEDULER_PLATEAU_REDUCE)\n        if learning_rate_reduce_factor >= 1.0:\n            logger.warning(\"Not using %s learning rate scheduling: learning_rate_reduce_factor == 1.0\"\n                           % C.LR_SCHEDULER_PLATEAU_REDUCE)\n            return None\n        return LearningRateSchedulerPlateauReduce(learning_rate_reduce_factor, learning_rate_reduce_num_not_improved,\n                                                  learning_rate_warmup)\n    else:\n        raise ValueError(\"Unknown learning rate scheduler type %s.\" % scheduler_type)", "response": "Returns a learning rate scheduler."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning linearly increasing fraction of base_lr.", "response": "def _warmup(self, num_updates):\n        \"\"\"\n        Returns linearly increasing fraction of base_lr.\n        \"\"\"\n        assert self.base_lr is not None\n        if not self.warmup:\n            return self.base_lr\n        fraction = (num_updates + 1) * self.base_lr / (self.warmup + 1)\n        if num_updates > self.last_warmup_log and num_updates % self.log_warmup_every_t == 0:\n            self.last_warmup_log = num_updates\n            logger.info(\"Learning rate warmup: %3.0f%%\", fraction / self.base_lr * 100.0)\n        return fraction"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn true if the parameters should be reset to the ones with the best validation score.", "response": "def new_evaluation_result(self, has_improved: bool) -> bool:\n        \"\"\"\n        Returns true if the parameters should be reset to the ones with the best validation score.\n\n        :param has_improved: Whether the model improved on held-out validation data.\n        :return: True if parameters should be reset to the ones with best validation score.\n        \"\"\"\n        logger.info(\"Checkpoint learning rate: %1.2e (%d/%d updates)\",\n                    self.current_rate,\n                    self.latest_t - self.current_step_started_at,\n                    self.current_step_num_updates)\n        if self.latest_t >= self.next_step_at:\n            self.current_step += 1\n            self._update_rate(self.current_step)\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_schedule_str(schedule_str: str) -> List[Tuple[float, int]]:\n        schedule = list()\n        for step in schedule_str.split(\",\"):\n            rate, num_updates = step.split(\":\")\n            schedule.append((float(rate), int(num_updates)))\n        return schedule", "response": "Parse learning schedule string."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns true if the parameters should be reset to the ones with the best validation score.", "response": "def new_evaluation_result(self, has_improved: bool) -> bool:\n        \"\"\"\n        Returns true if the parameters should be reset to the ones with the best validation score.\n\n        :param has_improved: Whether the model improved on held-out validation data.\n        :return: True if parameters should be reset to the ones with best validation score.\n        \"\"\"\n        if self.lr is None:\n            assert self.base_lr is not None\n            self.lr = self.base_lr\n        if has_improved:\n            self.num_not_improved = 0\n        else:\n            self.num_not_improved += 1\n            if self.num_not_improved >= self.reduce_num_not_improved and self.reduce_factor < 1.0 and self.warmed_up:\n                old_lr = self.lr\n                self.lr *= self.reduce_factor\n                logger.info(\"%d checkpoints since improvement or rate scaling, \"\n                            \"lowering learning rate: %1.2e -> %1.2e\", self.num_not_improved, old_lr, self.lr)\n                self.num_not_improved = 0\n                return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef copy_mx_model_to(model_path, model_epoch, output_folder):\n    target_path = os.path.join(output_folder, os.path.basename(model_path))\n    logger.info(\"Copying image model from {} to {}\".format(model_path,\n                                                           target_path))\n    suffix = ['-symbol.json', '-%04d.params' % (model_epoch,)]\n    for s in suffix:\n        copyfile(model_path + s, target_path + s)\n    return target_path", "response": "Copy mxnet models to new folder."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncrop the input image to the specified size.", "response": "def crop_resize_image(image: np.ndarray, size) -> np.ndarray:\n    \"\"\"\n    Resize the input image.\n\n    :param image: Original image which is a  PIL object.\n    :param size: Tuple of height and width to resize the image to.\n    :return: Resized image which is a PIL object\n    \"\"\"\n    width, height = image.size\n    if width > height:\n        left = (width - height) / 2\n        right = width - left\n        top = 0\n        bottom = height\n    else:\n        top = (height - width) / 2\n        bottom = height - top\n        left = 0\n        right = width\n    image = image.crop((left, top, right, bottom))\n    image = image.resize(size, Image.ANTIALIAS)\n    return image"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_preprocess_images(image_paths: List[str], image_size: tuple) -> List[np.ndarray]:\n    image_size = image_size[1:]  # we do not need the number of channels\n    images = []\n    for image_path in image_paths:\n        images.append(load_preprocess_image(image_path, image_size))\n    return images", "response": "Load and pre - process the images specified with absolute paths."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_features(paths: List[str],\n                  expected_shape: Optional[tuple] = None) -> List[np.ndarray]:\n    \"\"\"\n    Load features specified with absolute paths.\n\n    :param paths: List of files specified with paths.\n    :param expected_shape: Optional expected shape.\n    :return: A list of loaded images (numpy arrays).\n    \"\"\"\n    data = []  # type: List[np.ndarray]\n    for path in paths:\n        data.append(load_feature(path, expected_shape))\n    return data", "response": "Load features specified with absolute paths."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsave features specified with absolute paths.", "response": "def save_features(paths: List[str], datas: List[np.ndarray],\n                  compressed: bool = False) -> List:\n    \"\"\"\n    Save features specified with absolute paths.\n\n    :param paths: List of files specified with paths.\n    :param datas: List of numpy ndarrays to save into the respective files\n    :param compressed: Use numpy compression\n    :return: A list of file names.\n    \"\"\"\n    fnames = []  # type: List[str]\n    for path, data in zip(paths, datas):\n        fnames.append(save_feature(path, data, compressed))\n    return fnames"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nzeroes pads the features to target_shape.", "response": "def zero_pad_features(features: List[np.ndarray],\n                      target_shape: tuple) -> List[np.ndarray]:\n    \"\"\"\n    Zero pad to numpy array.\n\n    :param features: List of numpy arrays.\n    :param target_shape: Target shape of each numpy array in the list feat. Note:\n                   target_shape should be greater that the largest shapes in feat.\n    :return: A list of padded numpy arrays.\n    \"\"\"\n    pad_features = []\n    for feature in features:\n        feature_shape = feature.shape\n        if len(feature_shape) < len(target_shape):  # add extra dimensions\n            for i in range(len(target_shape) - len(feature_shape)):\n                feature = np.expand_dims(feature, axis=len(feature.shape) + 1)\n                feature_shape = feature.shape\n        elif len(feature_shape) > len(target_shape):\n            raise ValueError(\"Provided target shape must be bigger then the original \"\n                             \"shape. (provided: {}, original {})\".format(len(target_shape), len(feature_shape)))\n        diff_shape = np.subtract(target_shape, feature_shape)  # pylint: disable=assignment-from-no-return\n        if np.any(diff_shape < 0):\n            raise ValueError(\"Provided target values must be bigger then the original \"\n                             \"values for each dimension. (provided: {}, original {})\".format(target_shape, feature_shape))\n        # pad format: ((before_1, after_1), ... (before_N, after_N))\n        diff_shape = [[0, d] for d in diff_shape]  # pylint: disable=not-an-iterable\n        p = np.pad(feature, diff_shape, 'constant', constant_values=0)\n        pad_features.append(p)\n    return pad_features"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_initializer(default_init_type: str, default_init_scale: float, default_init_xavier_rand_type: str,\n                    default_init_xavier_factor_type: str, embed_init_type: str, embed_init_sigma: float,\n                    rnn_init_type: str, extra_initializers: Optional[List[Tuple[str, mx.initializer.Initializer]]] = None) -> mx.initializer.Initializer:\n    \"\"\"\n    Returns a mixed MXNet initializer.\n\n    :param default_init_type: The default weight initializer type.\n    :param default_init_scale: The scale used for default weight initialization (only used with uniform initialization).\n    :param default_init_xavier_rand_type: Xavier random number generator type.\n    :param default_init_xavier_factor_type: Xavier factor type.\n    :param embed_init_type: Embedding matrix initialization type.\n    :param embed_init_sigma: Sigma for normal initialization of embedding matrix.\n    :param rnn_init_type: Initialization type for RNN h2h matrices.\n    :param extra_initializers: Optional initializers provided from other sources.\n    :return: Mixed initializer.\n    \"\"\"\n    # default initializer\n    if default_init_type == C.INIT_XAVIER:\n        default_init = [(C.DEFAULT_INIT_PATTERN,\n                         mx.init.Xavier(rnd_type=default_init_xavier_rand_type,\n                                        factor_type=default_init_xavier_factor_type,\n                                        magnitude=default_init_scale))]\n    elif default_init_type == C.INIT_UNIFORM:\n        default_init = [(C.DEFAULT_INIT_PATTERN, mx.init.Uniform(scale=default_init_scale))]\n    else:\n        raise ValueError(\"Unknown default initializer %s.\" % default_init_type)\n\n    # embedding initializer\n    if embed_init_type == C.EMBED_INIT_NORMAL:\n        embed_init = [(C.EMBED_INIT_PATTERN, mx.init.Normal(sigma=embed_init_sigma))]\n    elif embed_init_type == C.EMBED_INIT_DEFAULT:\n        embed_init = []\n    else:\n        raise ValueError('Unknown embedding initializer: %s' % embed_init_type)\n\n    # rnn initializer\n    if rnn_init_type == C.RNN_INIT_ORTHOGONAL:\n        rnn_init = [(C.RNN_INIT_PATTERN, mx.initializer.Orthogonal())]\n    elif rnn_init_type == C.RNN_INIT_ORTHOGONAL_STACKED:\n        rnn_init = [(C.RNN_INIT_PATTERN, StackedOrthogonalInit(scale=1.0, rand_type=\"eye\"))]\n    elif rnn_init_type == C.RNN_INIT_DEFAULT:\n        rnn_init = []\n    else:\n        raise ValueError('Unknown RNN initializer: %s' % rnn_init_type)\n\n    params_init_pairs = embed_init + rnn_init + default_init\n    if extra_initializers is not None:\n        params_init_pairs = extra_initializers + params_init_pairs\n    return mx.initializer.Mixed(*zip(*params_init_pairs))", "response": "Returns a mixed MXNet initializer."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndecoding data set and evaluates the data set.", "response": "def decode_and_evaluate(self,\n                            checkpoint: Optional[int] = None,\n                            output_name: str = os.devnull) -> Dict[str, float]:\n        \"\"\"\n        Decodes data set and evaluates given a checkpoint.\n\n        :param checkpoint: Checkpoint to load parameters from.\n        :param output_name: Filename to write translations to. Defaults to /dev/null.\n        :return: Mapping of metric names to scores.\n        \"\"\"\n        models, vocab_target = inference_image.load_models(context=self.context,\n                                                           max_input_len=self.max_input_len,\n                                                           beam_size=self.beam_size,\n                                                           batch_size=self.batch_size,\n                                                           model_folders=[self.model],\n                                                           checkpoints=[checkpoint],\n                                                           softmax_temperature=self.softmax_temperature,\n                                                           max_output_length_num_stds=self.max_output_length_num_stds,\n                                                           source_image_size=tuple(self.source_image_size),\n                                                           forced_max_output_len=self.max_output_length)\n        translator = inference_image.ImageCaptioner(context=self.context,\n                                                    ensemble_mode=self.ensemble_mode,\n                                                    bucket_source_width=0,\n                                                    length_penalty=inference.LengthPenalty(\n                                                        self.length_penalty_alpha,\n                                                        self.length_penalty_beta),\n                                                    brevity_penalty=inference.BrevityPenalty(\n                                                        weight=0.0),\n                                                    beam_prune=0.0,\n                                                    beam_search_stop='all',\n                                                    models=models,\n                                                    source_vocabs=None,\n                                                    target_vocab=vocab_target,\n                                                    restrict_lexicon=None,\n                                                    store_beam=False,\n                                                    source_image_size=tuple(\n                                                        self.source_image_size),\n                                                    source_root=self.image_root,\n                                                    use_feature_loader=self.use_feature_loader)\n\n        trans_wall_time = 0.0\n        translations = []\n        with data_io.smart_open(output_name, 'w') as output:\n            handler = output_handler.StringOutputHandler(output)\n            tic = time.time()\n            trans_inputs = []  # type: List[inference.TranslatorInput]\n            for i, inputs in enumerate(self.inputs_sentences):\n                trans_inputs.append(\n                    inference.make_input_from_multiple_strings(i, inputs))\n            trans_outputs = translator.translate(trans_inputs)\n            trans_wall_time = time.time() - tic\n            for trans_input, trans_output in zip(trans_inputs, trans_outputs):\n                handler.handle(trans_input, trans_output)\n                translations.append(trans_output.translation)\n        avg_time = trans_wall_time / len(self.target_sentences)\n\n        # TODO(fhieber): eventually add more metrics (METEOR etc.)\n        return {C.BLEU_VAL: evaluate.raw_corpus_bleu(hypotheses=translations,\n                                                     references=self.target_sentences,\n                                                     offset=0.01),\n                C.CHRF_VAL: evaluate.raw_corpus_chrf(hypotheses=translations,\n                                                     references=self.target_sentences),\n                C.AVG_TIME: avg_time,\n                C.DECODING_TIME: trans_wall_time}"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef regular_file() -> Callable:\n\n    def check_regular_file(value_to_check):\n        value_to_check = str(value_to_check)\n        if not os.path.isfile(value_to_check):\n            raise argparse.ArgumentTypeError(\"must exist and be a regular file.\")\n        return value_to_check\n\n    return check_regular_file", "response": "Returns a method that can be used in argument parsing to check the argument is a regular file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a method that can be used in argument parsing to check the argument is a directory.", "response": "def regular_folder() -> Callable:\n    \"\"\"\n    Returns a method that can be used in argument parsing to check the argument is a directory.\n\n    :return: A method that can be used as a type in argparse.\n    \"\"\"\n\n    def check_regular_directory(value_to_check):\n        value_to_check = str(value_to_check)\n        if not os.path.isdir(value_to_check):\n            raise argparse.ArgumentTypeError(\"must be a directory.\")\n        return value_to_check\n\n    return check_regular_directory"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a method that can be used in argument parsing to check that the int argument is greater or equal to threshold.", "response": "def int_greater_or_equal(threshold: int) -> Callable:\n    \"\"\"\n    Returns a method that can be used in argument parsing to check that the int argument is greater or equal to `threshold`.\n\n    :param threshold: The threshold that we assume the cli argument value is greater or equal to.\n    :return: A method that can be used as a type in argparse.\n    \"\"\"\n\n    def check_greater_equal(value: str):\n        value_to_check = int(value)\n        if value_to_check < threshold:\n            raise argparse.ArgumentTypeError(\"must be greater or equal to %d.\" % threshold)\n        return value_to_check\n\n    return check_greater_equal"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef float_greater_or_equal(threshold: float) -> Callable:\n\n    def check_greater_equal(value: str):\n        value_to_check = float(value)\n        if value_to_check < threshold:\n            raise argparse.ArgumentTypeError(\"must be greater or equal to %f.\" % threshold)\n        return value_to_check\n\n    return check_greater_equal", "response": "Returns a method that can be used in argument parsing to check that the float argument is greater or equal to threshold."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef learning_schedule() -> Callable:\n\n    def parse(schedule_str):\n        try:\n            schedule = LearningRateSchedulerFixedStep.parse_schedule_str(schedule_str)\n        except ValueError:\n            raise argparse.ArgumentTypeError(\n                \"Learning rate schedule string should have form rate1:num_updates1[,rate2:num_updates2,...]\")\n        return schedule\n\n    return parse", "response": "Returns a method that can be used in argument parsing to check that the argument is a valid learning rate schedule string."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef simple_dict() -> Callable:\n\n    def parse(dict_str: str):\n\n        def _parse(value: str):\n            if value == \"True\":\n                return True\n            if value == \"False\":\n                return False\n            if \".\" in value:\n                return float(value)\n            return int(value)\n\n        _dict = dict()\n        try:\n            for entry in dict_str.split(\",\"):\n                key, value = entry.split(\":\")\n                _dict[key] = _parse(value)\n        except ValueError:\n            raise argparse.ArgumentTypeError(\"Specify argument dictionary as key1:value1,key2:value2,...\"\n                                             \" Supported types: bool, int, float.\")\n        return _dict\n\n    return parse", "response": "A simple dictionary format that does not require spaces or quoting."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef multiple_values(num_values: int = 0,\n                    greater_or_equal: Optional[float] = None,\n                    data_type: Callable = int) -> Callable:\n    \"\"\"\n    Returns a method to be used in argument parsing to parse a string of the form \"<val>:<val>[:<val>...]\" into\n    a tuple of values of type data_type.\n\n    :param num_values: Optional number of ints required.\n    :param greater_or_equal: Optional constraint that all values should be greater or equal to this value.\n    :param data_type: Type of values. Default: int.\n    :return: Method for parsing.\n    \"\"\"\n\n    def parse(value_to_check):\n        if ':' in value_to_check:\n            expected_num_separators = num_values - 1 if num_values else 0\n            if expected_num_separators > 0 and (value_to_check.count(':') != expected_num_separators):\n                raise argparse.ArgumentTypeError(\"Expected either a single value or %d values separated by %s\" %\n                                                 (num_values, C.ARG_SEPARATOR))\n            values = tuple(map(data_type, value_to_check.split(C.ARG_SEPARATOR, num_values - 1)))\n        else:\n            values = tuple([data_type(value_to_check)] * num_values)\n        if greater_or_equal is not None:\n            if any((value < greater_or_equal for value in values)):\n                raise argparse.ArgumentTypeError(\"Must provide value greater or equal to %d\" % greater_or_equal)\n        return values\n\n    return parse", "response": "Returns a method to parse a string of the form \"<val > : <val >..."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a file descriptor from stdin or opening a file from a given path.", "response": "def file_or_stdin() -> Callable:\n    \"\"\"\n    Returns a file descriptor from stdin or opening a file from a given path.\n    \"\"\"\n\n    def parse(path):\n        if path is None or path == \"-\":\n            return sys.stdin\n        else:\n            return data_io.smart_open(path)\n\n    return parse"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _extract(param_names: List[str],\n             params: Dict[str, mx.nd.NDArray],\n             ext_params: Dict[str, np.ndarray]) -> List[str]:\n    \"\"\"\n    Extract specific parameters from a given base.\n\n    :param param_names: Names of parameters to be extracted.\n    :param params: Mapping from parameter names to the actual NDArrays parameters.\n    :param ext_params: Extracted parameter dictionary.\n    :return: Remaining names of parameters to be extracted.\n    \"\"\"\n    remaining_param_names = list(param_names)\n    for name in param_names:\n        if name in params:\n            logger.info(\"\\tFound '%s': shape=%s\", name, str(params[name].shape))\n            ext_params[name] = params[name].asnumpy()\n            remaining_param_names.remove(name)\n    return remaining_param_names", "response": "Extracts specific parameters from a given base."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nextract specific parameters given their names.", "response": "def extract(param_path: str,\n            param_names: List[str],\n            list_all: bool) -> Dict[str, np.ndarray]:\n    \"\"\"\n    Extract specific parameters given their names.\n\n    :param param_path: Path to the parameter file.\n    :param param_names: Names of parameters to be extracted.\n    :param list_all: List names of all available parameters.\n    :return: Extracted parameter dictionary.\n    \"\"\"\n    logger.info(\"Loading parameters from '%s'\", param_path)\n    arg_params, aux_params = utils.load_params(param_path)\n\n    ext_params = {}  # type: Dict[str, np.ndarray]\n    param_names = _extract(param_names, arg_params, ext_params)\n    param_names = _extract(param_names, aux_params, ext_params)\n\n    if len(param_names) > 0:\n        logger.info(\"The following parameters were not found:\")\n        for name in param_names:\n            logger.info(\"\\t%s\", name)\n        logger.info(\"Check the following availabilities\")\n        list_all = True\n\n    if list_all:\n        if arg_params:\n            logger.info(\"Available arg parameters:\")\n            for name in arg_params:\n                logger.info(\"\\t%s: shape=%s\", name, str(arg_params[name].shape))\n        if aux_params:\n            logger.info(\"Available aux parameters:\")\n            for name in aux_params:\n                logger.info(\"\\t%s: shape=%s\", name, str(aux_params[name].shape))\n\n    return ext_params"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef main():\n    setup_main_logger(console=True, file_logging=False)\n    params = argparse.ArgumentParser(description=\"Extract specific parameters.\")\n    arguments.add_extract_args(params)\n    args = params.parse_args()\n    extract_parameters(args)", "response": "Commandline interface to extract parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef register(cls, config_type: Type[DecoderConfig], suffix: str):\n        def wrapper(target_cls):\n            cls.__registry[config_type] = (target_cls, suffix)\n            return target_cls\n\n        return wrapper", "response": "Decorator for registering decoder type for configuration."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a decoder based on config type.", "response": "def get_decoder(cls, config: DecoderConfig, prefix: str) -> 'Decoder':\n        \"\"\"\n        Creates decoder based on config type.\n\n        :param config: Decoder config.\n        :param prefix: Prefix to prepend for decoder.\n\n        :return: Decoder instance.\n        \"\"\"\n        config_type = type(config)\n        if config_type not in cls.__registry:\n            raise ValueError('Unsupported decoder configuration %s' % config_type.__name__)\n        decoder_cls, suffix = cls.__registry[config_type]\n        # TODO: move final suffix/prefix construction logic into config builder\n        return decoder_cls(config=config, prefix=prefix + suffix)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef decode_sequence(self,\n                        source_encoded: mx.sym.Symbol,\n                        source_encoded_lengths: mx.sym.Symbol,\n                        source_encoded_max_length: int,\n                        target_embed: mx.sym.Symbol,\n                        target_embed_lengths: mx.sym.Symbol,\n                        target_embed_max_length: int) -> mx.sym.Symbol:\n        \"\"\"\n        Decodes a sequence of embedded target words and returns sequence of last decoder\n        representations for each time step.\n\n        :param source_encoded: Encoded source: (batch_size, source_encoded_max_length, encoder_depth).\n        :param source_encoded_lengths: Lengths of encoded source sequences. Shape: (batch_size,).\n        :param source_encoded_max_length: Size of encoder time dimension.\n        :param target_embed: Embedded target sequence. Shape: (batch_size, target_embed_max_length, target_num_embed).\n        :param target_embed_lengths: Lengths of embedded target sequences. Shape: (batch_size,).\n        :param target_embed_max_length: Dimension of the embedded target sequence.\n        :return: Decoder data. Shape: (batch_size, target_embed_max_length, decoder_depth).\n        \"\"\"\n        pass", "response": "Decodes a sequence of embedded target words and returns a sequence of last decoder\n        representations for each time step."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndecodes a single time step given the previous embedded target word embedding and the encoded source time dimension.", "response": "def decode_step(self,\n                    step: int,\n                    target_embed_prev: mx.sym.Symbol,\n                    source_encoded_max_length: int,\n                    *states: mx.sym.Symbol) -> Tuple[mx.sym.Symbol, mx.sym.Symbol, List[mx.sym.Symbol]]:\n        \"\"\"\n        Decodes a single time step given the current step, the previous embedded target word,\n        and previous decoder states.\n        Returns decoder representation for the next prediction, attention probabilities, and next decoder states.\n        Implementations can maintain an arbitrary number of states.\n\n        :param step: Global step of inference procedure, starts with 1.\n        :param target_embed_prev: Previous target word embedding. Shape: (batch_size, target_num_embed).\n        :param source_encoded_max_length: Length of encoded source time dimension.\n        :param states: Arbitrary list of decoder states.\n        :return: logit inputs, attention probabilities, next decoder states.\n        \"\"\"\n        pass"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of symbolic states that represent the initial states of this decoder.", "response": "def init_states(self,\n                    source_encoded: mx.sym.Symbol,\n                    source_encoded_lengths: mx.sym.Symbol,\n                    source_encoded_max_length: int) -> List[mx.sym.Symbol]:\n        \"\"\"\n        Returns a list of symbolic states that represent the initial states of this decoder.\n        Used for inference.\n\n        :param source_encoded: Encoded source. Shape: (batch_size, source_encoded_max_length, encoder_depth).\n        :param source_encoded_lengths: Lengths of encoded source sequences. Shape: (batch_size,).\n        :param source_encoded_max_length: Size of encoder time dimension.\n        :return: List of symbolic initial states.\n        \"\"\"\n        pass"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of shape descriptions given batch size encoded target max length and encoded source depth.", "response": "def state_shapes(self,\n                     batch_size: int,\n                     target_max_length: int,\n                     source_encoded_max_length: int,\n                     source_encoded_depth: int) -> List[mx.io.DataDesc]:\n        \"\"\"\n        Returns a list of shape descriptions given batch size, encoded source max length and encoded source depth.\n        Used for inference.\n\n        :param batch_size: Batch size during inference.\n        :param target_max_length: Current target sequence length.\n        :param source_encoded_max_length: Size of encoder time dimension.\n        :param source_encoded_depth: Depth of encoded source.\n        :return: List of shape descriptions.\n        \"\"\"\n        pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nhandles the translation of a single locale.", "response": "def handle(self,\n               t_input: inference.TranslatorInput,\n               t_output: inference.TranslatorOutput,\n               t_walltime: float = 0.):\n        \"\"\"\n        :param t_input: Translator input.\n        :param t_output: Translator output.\n        :param t_walltime: Total walltime for translation.\n        \"\"\"\n        self.stream.write(\"%s\\n\" % t_output.translation)\n        self.stream.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef handle(self,\n               t_input: inference.TranslatorInput,\n               t_output: inference.TranslatorOutput,\n               t_walltime: float = 0.):\n        \"\"\"\n        :param t_input: Translator input.\n        :param t_output: Translator output.\n        :param t_walltime: Total walltime for translation.\n        \"\"\"\n        self.stream.write(\"{:.3f}\\t{}\\t{}\\n\".format(t_output.score,\n                                                    C.TOKEN_SEPARATOR.join(t_input.tokens),\n                                                    t_output.translation))\n        self.stream.flush()", "response": "Handles the translation of a single locale."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef handle(self,\n               t_input: inference.TranslatorInput,\n               t_output: inference.TranslatorOutput,\n               t_walltime: float = 0.):\n        \"\"\"\n        :param t_input: Translator input.\n        :param t_output: Translator output.\n        :param t_walltime: Total wall-clock time for translation.\n        \"\"\"\n        line = \"{sent_id} ||| {target} ||| {score:f} ||| {source} ||| {source_len:d} ||| {target_len:d}\\n\"\n        self.stream.write(line.format(sent_id=t_input.sentence_id,\n                                      target=\" \".join(t_output.tokens),\n                                      score=t_output.score,\n                                      source=\" \".join(t_input.tokens),\n                                      source_len=len(t_input.tokens),\n                                      target_len=len(t_output.tokens)))\n        attention_matrix = t_output.attention_matrix.T\n        for i in range(0, attention_matrix.shape[0]):\n            attention_vector = attention_matrix[i]\n            self.stream.write(\" \".join([\"%f\" % value for value in attention_vector]))\n            self.stream.write(\"\\n\")\n\n        self.stream.write(\"\\n\")\n        self.stream.flush()", "response": "Handles the translation of a single language."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nhandling the translation of a single locale.", "response": "def handle(self,\n               t_input: inference.TranslatorInput,\n               t_output: inference.TranslatorOutput,\n               t_walltime: float = 0.):\n        \"\"\"\n        :param t_input: Translator input.\n        :param t_output: Translator output.\n        :param t_walltime: Total walltime for translation.\n        \"\"\"\n        self.stream.write(\"input=%s\\toutput=%s\\tinput_tokens=%d\\toutput_tokens=%d\\ttranslation_time=%0.4f\\n\" %\n                          (\" \".join(t_input.tokens),\n                           t_output.translation,\n                           len(t_input.tokens),\n                           len(t_output.tokens),\n                           t_walltime))\n        self.stream.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nhandle the translation of a single language language.", "response": "def handle(self,\n               t_input: inference.TranslatorInput,\n               t_output: inference.TranslatorOutput,\n               t_walltime: float = 0.):\n        \"\"\"\n        :param t_input: Translator input.\n        :param t_output: Translator output.\n        :param t_walltime: Total wall-clock time for translation.\n        \"\"\"\n        assert len(t_output.beam_histories) >= 1, \"Translator output should contain beam histories.\"\n        # If the sentence was max_len split, we may have more than one history\n        for h in t_output.beam_histories:\n            # Add the number of steps in each beam\n            h[\"number_steps\"] = len(h[\"predicted_tokens\"])  # type: ignore\n            # Some outputs can have more than one beam, add the id for bookkeeping\n            h[\"id\"] = t_output.sentence_id  # type: ignore\n            self.stream.write(\"%s\\n\" % json.dumps(h, sort_keys=True))\n        self.stream.flush()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef handle(self,\n               t_input: inference.TranslatorInput,\n               t_output: inference.TranslatorOutput,\n               t_walltime: float = 0.):\n        \"\"\"\n        Outputs a JSON object of the fields in the `TranslatorOutput` object.\n        \"\"\"\n\n        d_ = t_output.json(self.align_threshold)\n\n        self.stream.write(\"%s\\n\" % json.dumps(d_, sort_keys=True))\n        self.stream.flush()", "response": "Handles the translator output."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _add_graph_level(graph, level, parent_ids, names, scores, normalized_scores,\n                     include_pad):\n    \"\"\"Adds a level to the passed graph\"\"\"\n    for i, parent_id in enumerate(parent_ids):\n        if not include_pad and names[i] == PAD_TOKEN:\n            continue\n        new_node = (level, i)\n        parent_node = (level - 1, parent_id)\n        raw_score = '%.3f' % float(scores[i]) if scores[i] is not None else '-inf'\n        norm_score = '%.3f' % float(normalized_scores[i]) if normalized_scores[i] is not None else '-inf'\n\n        graph.add_node(new_node)\n        graph.node[new_node][\"name\"] = names[i]\n        graph.node[new_node][\"score\"] = \"[RAW] {}\".format(raw_score)\n        graph.node[new_node][\"norm_score\"] = \"[NORM] {}\".format(norm_score)\n        graph.node[new_node][\"size\"] = 100\n        # Add an edge to the parent\n        graph.add_edge(parent_node, new_node)", "response": "Adds a level to the passed graph"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading the data iterators and vocabularies for the current language.", "response": "def get_data_iters_and_vocabs(args: argparse.Namespace,\n                              model_folder: Optional[str]) -> Tuple['data_io.BaseParallelSampleIter',\n                                                                    List[vocab.Vocab], vocab.Vocab, model.ModelConfig]:\n    \"\"\"\n    Loads the data iterators and vocabularies.\n\n    :param args: Arguments as returned by argparse.\n    :param model_folder: Output folder.\n    :return: The scoring data iterator as well as the source and target vocabularies.\n    \"\"\"\n\n    model_config = model.SockeyeModel.load_config(os.path.join(args.model, C.CONFIG_NAME))\n\n    if args.max_seq_len is None:\n        max_seq_len_source = model_config.config_data.max_seq_len_source\n        max_seq_len_target = model_config.config_data.max_seq_len_target\n    else:\n        max_seq_len_source, max_seq_len_target = args.max_seq_len\n\n    batch_num_devices = 1 if args.use_cpu else sum(-di if di < 0 else 1 for di in args.device_ids)\n\n    # Load the existing vocabs created when starting the training run.\n    source_vocabs = vocab.load_source_vocabs(model_folder)\n    target_vocab = vocab.load_target_vocab(model_folder)\n\n    sources = [args.source] + args.source_factors\n    sources = [str(os.path.abspath(source)) for source in sources]\n\n    score_iter = data_io.get_scoring_data_iters(\n        sources=sources,\n        target=os.path.abspath(args.target),\n        source_vocabs=source_vocabs,\n        target_vocab=target_vocab,\n        batch_size=args.batch_size,\n        batch_num_devices=batch_num_devices,\n        max_seq_len_source=max_seq_len_source,\n        max_seq_len_target=max_seq_len_target)\n\n    return score_iter, source_vocabs, target_vocab, model_config"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a image encoder.", "response": "def get_image_cnn_encoder(config: ImageLoadedCnnEncoderConfig) -> 'Encoder':\n    \"\"\"\n    Creates a image encoder.\n\n    :param config: Configuration for image encoder.\n    :return: Encoder instance.\n    \"\"\"\n\n    encoders = list()  # type: List[Encoder]\n    max_seq_len = config.encoded_seq_len\n    if not config.no_global_descriptor:\n        max_seq_len += 1\n    encoders.append(get_positional_embedding(config.positional_embedding_type,\n                                             config.num_embed,\n                                             max_seq_len=max_seq_len,\n                                             fixed_pos_embed_scale_up_input=False,\n                                             fixed_pos_embed_scale_down_positions=True,\n                                             prefix=C.SOURCE_POSITIONAL_EMBEDDING_PREFIX))\n    encoders.append(ImageLoadedCnnEncoder(config=config))\n    return EncoderSequence(encoders)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef encode(self,\n               data: mx.sym.Symbol,\n               data_length: mx.sym.Symbol,\n               seq_len: int) -> Tuple[mx.sym.Symbol, mx.sym.Symbol, int]:\n        \"\"\"\n        Encodes data given sequence lengths of individual examples and maximum sequence length.\n\n        :param data: Ignored. Assume that the input is the image.\n        :param data_length: Vector with sequence lengths.\n        :param seq_len: Maximum sequence length.\n        :return: Encoded versions of input data data, data_length, seq_len.\n        \"\"\"\n\n        # (batch, n_kernels, height, width) -> (batch, width, height, n_kernels)\n        embedding = mx.sym.swapaxes(data=self.sym, dim1=1, dim2=3)\n        # (batch, width, height, n_kernels) -> (batch, height, width, n_kernels)\n        embedding = mx.sym.swapaxes(data=embedding, dim1=1, dim2=2)\n        # (batch, height, width, n_kernels) -> (batch, height*width, n_kernels)\n        embedding = mx.sym.Reshape(data=embedding, shape=(0, -3, self.n_kernels))\n        # Feature projection layer: (batch, height*width, num_embed)\n        embedding = mx.sym.FullyConnected(data=embedding, weight=self.other_weights[self.names[0]],\n                                          num_hidden=self.num_embed, no_bias=True, flatten=False)\n        embedding = mx.sym.Activation(data=embedding, act_type='relu')\n\n        # Visual global description: average pooling\n        if not self.no_global_descriptor:\n            glob_embedding = mx.sym.mean(data=embedding, axis=1)  # (batch, n_kernels)\n            glob_embedding = mx.sym.FullyConnected(data=glob_embedding, weight=self.other_weights[self.names[1]],\n                                                   num_hidden=self.num_embed, no_bias=True)\n            glob_embedding = mx.sym.Activation(data=glob_embedding, act_type='relu')\n            glob_embedding = mx.sym.expand_dims(glob_embedding, axis=1)\n            # Concatenate embeddings with global embedding: (batch, height*width+1, num_embed)\n            embedding = mx.sym.concat(embedding, glob_embedding, dim=1, name=\"local_global_image_embedding\")\n\n        # Symbol to infer axis 1 dimension\n        d = mx.sym.slice_axis(data=embedding, axis=2, begin=0, end=1)  # (batch, height*width, num_embed)\n        d = mx.sym.clip(data=d, a_min=1.0, a_max=1.0)  # matrix of all ones\n        encoded_data_length = mx.sym.sum(mx.sym.broadcast_equal(d, mx.sym.ones((1,))), axis=1)  # (batch, 1)\n        encoded_data_length = mx.sym.reshape(data=encoded_data_length, shape=(-1,))  # (batch, )\n\n        return embedding, encoded_data_length, self.encoded_seq_len", "response": "Encodes data given sequence lengths of individual examples and maximum sequence length."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the initializers of the current model considering the pretrained models.", "response": "def get_initializers(self) -> List[Tuple[str, mx.init.Initializer]]:\n        \"\"\"\n        Get the initializers of the network, considering the pretrained models.\n\n        :return: List of tuples (string name, mxnet initializer)\n        \"\"\"\n        patterns_vals = []\n        # Load from args/auxs\n        for k in self.args.keys():\n            patterns_vals.append((k, mx.init.Load({k: self.args[k]})))\n        for k in self.auxs.keys():\n            patterns_vals.append((k, mx.init.Load({k: self.auxs[k]})))\n        # Initialize\n        for k in self.names:\n            patterns_vals.append((k, mx.init.Xavier(rnd_type='uniform', factor_type='avg', magnitude=3)))\n\n        return patterns_vals"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_fixed_param_names(self) -> List[str]:\n        args = set(self.args.keys()) | set(self.auxs.keys())\n\n        return list(args & set(self.sym.list_arguments()))", "response": "Get the fixed params of the network."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of integers defining bucket boundaries.", "response": "def define_buckets(max_seq_len: int, step=10) -> List[int]:\n    \"\"\"\n    Returns a list of integers defining bucket boundaries.\n    Bucket boundaries are created according to the following policy:\n    We generate buckets with a step size of step until the final bucket fits max_seq_len.\n    We then limit that bucket to max_seq_len (difference between semi-final and final bucket may be less than step).\n\n    :param max_seq_len: Maximum bucket size.\n    :param step: Distance between buckets.\n    :return: List of bucket sizes.\n    \"\"\"\n    buckets = [bucket_len for bucket_len in range(step, max_seq_len + step, step)]\n    buckets[-1] = max_seq_len\n    return buckets"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef define_parallel_buckets(max_seq_len_source: int,\n                            max_seq_len_target: int,\n                            bucket_width: int = 10,\n                            length_ratio: float = 1.0) -> List[Tuple[int, int]]:\n    \"\"\"\n    Returns (source, target) buckets up to (max_seq_len_source, max_seq_len_target).  The longer side of the data uses\n    steps of bucket_width while the shorter side uses steps scaled down by the average target/source length ratio.  If\n    one side reaches its max_seq_len before the other, width of extra buckets on that side is fixed to that max_seq_len.\n\n    :param max_seq_len_source: Maximum source bucket size.\n    :param max_seq_len_target: Maximum target bucket size.\n    :param bucket_width: Width of buckets on longer side.\n    :param length_ratio: Length ratio of data (target/source).\n    \"\"\"\n    source_step_size = bucket_width\n    target_step_size = bucket_width\n    if length_ratio >= 1.0:\n        # target side is longer -> scale source\n        source_step_size = max(1, int(round(bucket_width / length_ratio)))\n    else:\n        # source side is longer, -> scale target\n        target_step_size = max(1, int(round(bucket_width * length_ratio)))\n    source_buckets = define_buckets(max_seq_len_source, step=source_step_size)\n    target_buckets = define_buckets(max_seq_len_target, step=target_step_size)\n    # Extra buckets\n    if len(source_buckets) < len(target_buckets):\n        source_buckets += [source_buckets[-1] for _ in range(len(target_buckets) - len(source_buckets))]\n    elif len(target_buckets) < len(source_buckets):\n        target_buckets += [target_buckets[-1] for _ in range(len(source_buckets) - len(target_buckets))]\n    # minimum bucket size is 2 (as we add BOS symbol to target side)\n    source_buckets = [max(2, b) for b in source_buckets]\n    target_buckets = [max(2, b) for b in target_buckets]\n    parallel_buckets = list(zip(source_buckets, target_buckets))\n    # deduplicate for return\n    buckets = list(OrderedDict.fromkeys(parallel_buckets))\n    buckets.sort()\n    return buckets", "response": "Define the parallel buckets of the sequence length between two source and target."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef define_empty_source_parallel_buckets(max_seq_len_target: int,\n                                         bucket_width: int = 10) -> List[Tuple[int, int]]:\n    \"\"\"\n    Returns (source, target) buckets up to (None, max_seq_len_target). The source\n    is empty since it is supposed to not contain data that can be bucketized.\n    The target is used as reference to create the buckets.\n\n    :param max_seq_len_target: Maximum target bucket size.\n    :param bucket_width: Width of buckets on longer side.\n    \"\"\"\n    target_step_size = max(1, bucket_width)\n    target_buckets = define_buckets(max_seq_len_target, step=target_step_size)\n    # source buckets are always 0 since there is no text\n    source_buckets = [0 for b in target_buckets]\n    target_buckets = [max(2, b) for b in target_buckets]\n    parallel_buckets = list(zip(source_buckets, target_buckets))\n    # deduplicate for return\n    buckets = list(OrderedDict.fromkeys(parallel_buckets))\n    buckets.sort()\n    return buckets", "response": "Returns a list of source and target buckets up to max_seq_len_target."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_bucket(seq_len: int, buckets: List[int]) -> Optional[int]:\n    bucket_idx = bisect.bisect_left(buckets, seq_len)\n    if bucket_idx == len(buckets):\n        return None\n    return buckets[bucket_idx]", "response": "Given sequence length and a list of buckets return corresponding bucket."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef define_bucket_batch_sizes(buckets: List[Tuple[int, int]],\n                              batch_size: int,\n                              batch_by_words: bool,\n                              batch_num_devices: int,\n                              data_target_average_len: List[Optional[float]]) -> List[BucketBatchSize]:\n    \"\"\"\n    Computes bucket-specific batch sizes (sentences, average_words).\n\n    If sentence-based batching: number of sentences is the same for each batch, determines the\n    number of words. Hence all batch sizes for each bucket are equal.\n\n    If word-based batching: number of sentences for each batch is set to the multiple of number\n    of devices that produces the number of words closest to the target batch size.  Average\n    target sentence length (non-padding symbols) is used for word number calculations.\n\n    :param buckets: Bucket list.\n    :param batch_size: Batch size.\n    :param batch_by_words: Batch by words.\n    :param batch_num_devices: Number of devices.\n    :param data_target_average_len: Optional average target length for each bucket.\n    \"\"\"\n    check_condition(len(data_target_average_len) == len(buckets),\n                    \"Must provide None or average target length for each bucket\")\n    data_target_average_len = list(data_target_average_len)\n    bucket_batch_sizes = []  # type: List[BucketBatchSize]\n    largest_total_num_words = 0\n    for buck_idx, bucket in enumerate(buckets):\n        # Target/label length with padding\n        padded_seq_len = bucket[1]\n        # Average target/label length excluding padding\n        if data_target_average_len[buck_idx] is None:\n            data_target_average_len[buck_idx] = padded_seq_len\n        average_seq_len = data_target_average_len[buck_idx]\n\n        # Word-based: num words determines num sentences\n        # Sentence-based: num sentences determines num words\n        if batch_by_words:\n            check_condition(padded_seq_len <= batch_size, \"Word batch size must cover sequence lengths for all\"\n                                                          \" buckets: (%d > %d)\" % (padded_seq_len, batch_size))\n            # Multiple of number of devices (int) closest to target number of words, assuming each sentence is of\n            # average length\n            batch_size_seq = batch_num_devices * max(1, round((batch_size / average_seq_len) / batch_num_devices))\n            batch_size_word = batch_size_seq * average_seq_len\n        else:\n            batch_size_seq = batch_size\n            batch_size_word = batch_size_seq * average_seq_len\n        bucket_batch_sizes.append(BucketBatchSize(bucket, batch_size_seq, batch_size_word))\n        # Track largest number of source or target word samples in a batch\n        largest_total_num_words = max(largest_total_num_words, batch_size_seq * max(*bucket))\n\n    # Final step: guarantee that largest bucket by sequence length also has a batch size so that it covers any\n    # (batch_size, len_source) and (batch_size, len_target) matrix from the data iterator to allow for memory sharing.\n    # When batching by sentences, this will already be the case.\n    if batch_by_words:\n        padded_seq_len = max(*buckets[-1])\n        average_seq_len = data_target_average_len[-1]\n        while bucket_batch_sizes[-1].batch_size * padded_seq_len < largest_total_num_words:\n            bucket_batch_sizes[-1] = BucketBatchSize(\n                bucket_batch_sizes[-1].bucket,\n                bucket_batch_sizes[-1].batch_size + batch_num_devices,\n                bucket_batch_sizes[-1].average_words_per_batch + batch_num_devices * average_seq_len)\n    return bucket_batch_sizes", "response": "Define the batch sizes for a list of buckets."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef calculate_length_statistics(source_iterables: Sequence[Iterable[Any]],\n                                target_iterable: Iterable[Any],\n                                max_seq_len_source: int,\n                                max_seq_len_target: int) -> 'LengthStatistics':\n    \"\"\"\n    Returns mean and standard deviation of target-to-source length ratios of parallel corpus.\n\n    :param source_iterables: Source sequence readers.\n    :param target_iterable: Target sequence reader.\n    :param max_seq_len_source: Maximum source sequence length.\n    :param max_seq_len_target: Maximum target sequence length.\n    :return: The number of sentences as well as the mean and standard deviation of target to source length ratios.\n    \"\"\"\n    mean_and_variance = OnlineMeanAndVariance()\n\n    for sources, target in parallel_iter(source_iterables, target_iterable):\n        source_len = len(sources[0])\n        target_len = len(target)\n        if source_len > max_seq_len_source or target_len > max_seq_len_target:\n            continue\n\n        length_ratio = target_len / source_len\n        mean_and_variance.update(length_ratio)\n\n    return LengthStatistics(mean_and_variance.count, mean_and_variance.mean, mean_and_variance.std)", "response": "Calculates the mean and standard deviation of the target - to - source length ratios of the source sequence."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef are_none(sequences: Sequence[Sized]) -> bool:\n    if not sequences:\n        return True\n    return all(s is None for s in sequences)", "response": "Returns True if all sequences are None."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef are_token_parallel(sequences: Sequence[Sized]) -> bool:\n    if not sequences or len(sequences) == 1:\n        return True\n    return all(len(s) == len(sequences[0]) for s in sequences)", "response": "Returns True if all sequences in the list have the same length."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating a shard file with the given source and target sentences.", "response": "def shard_data(source_fnames: List[str],\n               target_fname: str,\n               source_vocabs: List[vocab.Vocab],\n               target_vocab: vocab.Vocab,\n               num_shards: int,\n               buckets: List[Tuple[int, int]],\n               length_ratio_mean: float,\n               length_ratio_std: float,\n               output_prefix: str) -> Tuple[List[Tuple[List[str], str, 'DataStatistics']], 'DataStatistics']:\n    \"\"\"\n    Assign int-coded source/target sentence pairs to shards at random.\n\n    :param source_fnames: The path to the source text (and optional token-parallel factor files).\n    :param target_fname: The file name of the target file.\n    :param source_vocabs: Source vocabulary (and optional source factor vocabularies).\n    :param target_vocab: Target vocabulary.\n    :param num_shards: The total number of shards.\n    :param buckets: Bucket list.\n    :param length_ratio_mean: Mean length ratio.\n    :param length_ratio_std: Standard deviation of length ratios.\n    :param output_prefix: The prefix under which the shard files will be created.\n    :return: Tuple of source (and source factor) file names, target file names and statistics for each shard,\n             as well as global statistics.\n    \"\"\"\n    os.makedirs(output_prefix, exist_ok=True)\n    sources_shard_fnames = [[os.path.join(output_prefix, C.SHARD_SOURCE % i) + \".%d\" % f for i in range(num_shards)]\n                            for f in range(len(source_fnames))]\n    target_shard_fnames = [os.path.join(output_prefix, C.SHARD_TARGET % i)\n                           for i in range(num_shards)]  # type: List[str]\n\n    data_stats_accumulator = DataStatisticsAccumulator(buckets, source_vocabs[0], target_vocab,\n                                                       length_ratio_mean, length_ratio_std)\n    per_shard_stat_accumulators = [DataStatisticsAccumulator(buckets, source_vocabs[0], target_vocab, length_ratio_mean,\n                                                             length_ratio_std) for shard_idx in range(num_shards)]\n\n    with ExitStack() as exit_stack:\n        sources_shards = [[exit_stack.enter_context(smart_open(f, mode=\"wt\")) for f in sources_shard_fnames[i]] for i in\n                          range(len(source_fnames))]\n        target_shards = [exit_stack.enter_context(smart_open(f, mode=\"wt\")) for f in target_shard_fnames]\n\n        source_readers, target_reader = create_sequence_readers(source_fnames, target_fname,\n                                                                source_vocabs, target_vocab)\n\n        random_shard_iter = iter(lambda: random.randrange(num_shards), None)\n\n        for (sources, target), random_shard_index in zip(parallel_iter(source_readers, target_reader),\n                                                         random_shard_iter):\n            random_shard_index = cast(int, random_shard_index)\n            source_len = len(sources[0])\n            target_len = len(target)\n\n            buck_idx, buck = get_parallel_bucket(buckets, source_len, target_len)\n            data_stats_accumulator.sequence_pair(sources[0], target, buck_idx)\n            per_shard_stat_accumulators[random_shard_index].sequence_pair(sources[0], target, buck_idx)\n\n            if buck is None:\n                continue\n\n            for i, line in enumerate(sources):\n                sources_shards[i][random_shard_index].write(ids2strids(line) + \"\\n\")\n            target_shards[random_shard_index].write(ids2strids(target) + \"\\n\")\n\n    per_shard_stats = [shard_stat_accumulator.statistics for shard_stat_accumulator in per_shard_stat_accumulators]\n\n    sources_shard_fnames_by_shards = zip(*sources_shard_fnames)  # type: List[List[str]]\n\n    return list(\n        zip(sources_shard_fnames_by_shards, target_shard_fnames, per_shard_stats)), data_stats_accumulator.statistics"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_num_shards(num_samples: int, samples_per_shard: int, min_num_shards: int) -> int:\n    return max(int(math.ceil(num_samples / samples_per_shard)), min_num_shards)", "response": "Returns the number of shards."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn an iterator for the validation data.", "response": "def get_validation_data_iter(data_loader: RawParallelDatasetLoader,\n                             validation_sources: List[str],\n                             validation_target: str,\n                             buckets: List[Tuple[int, int]],\n                             bucket_batch_sizes: List[BucketBatchSize],\n                             source_vocabs: List[vocab.Vocab],\n                             target_vocab: vocab.Vocab,\n                             max_seq_len_source: int,\n                             max_seq_len_target: int,\n                             batch_size: int) -> 'ParallelSampleIter':\n    \"\"\"\n    Returns a ParallelSampleIter for the validation data.\n    \"\"\"\n    logger.info(\"=================================\")\n    logger.info(\"Creating validation data iterator\")\n    logger.info(\"=================================\")\n    validation_length_statistics = analyze_sequence_lengths(validation_sources, validation_target,\n                                                            source_vocabs, target_vocab,\n                                                            max_seq_len_source, max_seq_len_target)\n\n    check_condition(validation_length_statistics.num_sents > 0,\n                    \"No validation sequences found with length smaller or equal than the maximum sequence length.\"\n                    \"Consider increasing %s\" % C.TRAINING_ARG_MAX_SEQ_LEN)\n\n    validation_sources_sentences, validation_target_sentences = create_sequence_readers(validation_sources,\n                                                                                        validation_target,\n                                                                                        source_vocabs, target_vocab)\n\n    validation_data_statistics = get_data_statistics(validation_sources_sentences,\n                                                     validation_target_sentences,\n                                                     buckets,\n                                                     validation_length_statistics.length_ratio_mean,\n                                                     validation_length_statistics.length_ratio_std,\n                                                     source_vocabs, target_vocab)\n\n    validation_data_statistics.log(bucket_batch_sizes)\n\n    validation_data = data_loader.load(validation_sources_sentences, validation_target_sentences,\n                                       validation_data_statistics.num_sents_per_bucket).fill_up(bucket_batch_sizes)\n\n    return ParallelSampleIter(data=validation_data,\n                              buckets=buckets,\n                              batch_size=batch_size,\n                              bucket_batch_sizes=bucket_batch_sizes,\n                              num_factors=len(validation_sources))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_training_data_iters(sources: List[str],\n                            target: str,\n                            validation_sources: List[str],\n                            validation_target: str,\n                            source_vocabs: List[vocab.Vocab],\n                            target_vocab: vocab.Vocab,\n                            source_vocab_paths: List[Optional[str]],\n                            target_vocab_path: Optional[str],\n                            shared_vocab: bool,\n                            batch_size: int,\n                            batch_by_words: bool,\n                            batch_num_devices: int,\n                            max_seq_len_source: int,\n                            max_seq_len_target: int,\n                            bucketing: bool,\n                            bucket_width: int,\n                            allow_empty: bool = False) -> Tuple['BaseParallelSampleIter',\n                                                                Optional['BaseParallelSampleIter'],\n                                                                'DataConfig', 'DataInfo']:\n    \"\"\"\n    Returns data iterators for training and validation data.\n\n    :param sources: Path to source training data (with optional factor data paths).\n    :param target: Path to target training data.\n    :param validation_sources: Path to source validation data (with optional factor data paths).\n    :param validation_target: Path to target validation data.\n    :param source_vocabs: Source vocabulary and optional factor vocabularies.\n    :param target_vocab: Target vocabulary.\n    :param source_vocab_paths: Path to source vocabulary.\n    :param target_vocab_path: Path to target vocabulary.\n    :param shared_vocab: Whether the vocabularies are shared.\n    :param batch_size: Batch size.\n    :param batch_by_words: Size batches by words rather than sentences.\n    :param batch_num_devices: Number of devices batches will be parallelized across.\n    :param max_seq_len_source: Maximum source sequence length.\n    :param max_seq_len_target: Maximum target sequence length.\n    :param bucketing: Whether to use bucketing.\n    :param bucket_width: Size of buckets.\n    :param allow_empty: Unless True if no sentences are below or equal to the maximum length an exception is raised.\n    :return: Tuple of (training data iterator, validation data iterator, data config).\n    \"\"\"\n    logger.info(\"===============================\")\n    logger.info(\"Creating training data iterator\")\n    logger.info(\"===============================\")\n    # Pass 1: get target/source length ratios.\n    length_statistics = analyze_sequence_lengths(sources, target, source_vocabs, target_vocab,\n                                                 max_seq_len_source, max_seq_len_target)\n\n    if not allow_empty:\n        check_condition(length_statistics.num_sents > 0,\n                        \"No training sequences found with length smaller or equal than the maximum sequence length.\"\n                        \"Consider increasing %s\" % C.TRAINING_ARG_MAX_SEQ_LEN)\n\n    # define buckets\n    buckets = define_parallel_buckets(max_seq_len_source, max_seq_len_target, bucket_width,\n                                      length_statistics.length_ratio_mean) if bucketing else [\n        (max_seq_len_source, max_seq_len_target)]\n\n    sources_sentences, target_sentences = create_sequence_readers(sources, target, source_vocabs, target_vocab)\n\n    # Pass 2: Get data statistics and determine the number of data points for each bucket.\n    data_statistics = get_data_statistics(sources_sentences, target_sentences, buckets,\n                                          length_statistics.length_ratio_mean, length_statistics.length_ratio_std,\n                                          source_vocabs, target_vocab)\n\n    bucket_batch_sizes = define_bucket_batch_sizes(buckets,\n                                                   batch_size,\n                                                   batch_by_words,\n                                                   batch_num_devices,\n                                                   data_statistics.average_len_target_per_bucket)\n\n    data_statistics.log(bucket_batch_sizes)\n\n    # Pass 3: Load the data into memory and return the iterator.\n    data_loader = RawParallelDatasetLoader(buckets=buckets,\n                                           eos_id=target_vocab[C.EOS_SYMBOL],\n                                           pad_id=C.PAD_ID)\n\n    training_data = data_loader.load(sources_sentences, target_sentences,\n                                     data_statistics.num_sents_per_bucket).fill_up(bucket_batch_sizes)\n\n    data_info = DataInfo(sources=sources,\n                         target=target,\n                         source_vocabs=source_vocab_paths,\n                         target_vocab=target_vocab_path,\n                         shared_vocab=shared_vocab,\n                         num_shards=1)\n\n    config_data = DataConfig(data_statistics=data_statistics,\n                             max_seq_len_source=max_seq_len_source,\n                             max_seq_len_target=max_seq_len_target,\n                             num_source_factors=len(sources),\n                             source_with_eos=True)\n\n    train_iter = ParallelSampleIter(data=training_data,\n                                    buckets=buckets,\n                                    batch_size=batch_size,\n                                    bucket_batch_sizes=bucket_batch_sizes,\n                                    num_factors=len(sources),\n                                    permute=True)\n\n    validation_iter = get_validation_data_iter(data_loader=data_loader,\n                                               validation_sources=validation_sources,\n                                               validation_target=validation_target,\n                                               buckets=buckets,\n                                               bucket_batch_sizes=bucket_batch_sizes,\n                                               source_vocabs=source_vocabs,\n                                               target_vocab=target_vocab,\n                                               max_seq_len_source=max_seq_len_source,\n                                               max_seq_len_target=max_seq_len_target,\n                                               batch_size=batch_size)\n\n    return train_iter, validation_iter, config_data, data_info", "response": "Returns training data iterators for training and validation data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_scoring_data_iters(sources: List[str],\n                           target: str,\n                           source_vocabs: List[vocab.Vocab],\n                           target_vocab: vocab.Vocab,\n                           batch_size: int,\n                           batch_num_devices: int,\n                           max_seq_len_source: int,\n                           max_seq_len_target: int) -> 'BaseParallelSampleIter':\n    \"\"\"\n    Returns a data iterator for scoring. The iterator loads data on demand,\n    batch by batch, and does not skip any lines. Lines that are too long\n    are truncated.\n\n    :param sources: Path to source training data (with optional factor data paths).\n    :param target: Path to target training data.\n    :param source_vocabs: Source vocabulary and optional factor vocabularies.\n    :param target_vocab: Target vocabulary.\n    :param batch_size: Batch size.\n    :param batch_num_devices: Number of devices batches will be parallelized across.\n    :param max_seq_len_source: Maximum source sequence length.\n    :param max_seq_len_target: Maximum target sequence length.\n    :return: The scoring data iterator.\n    \"\"\"\n    logger.info(\"==============================\")\n    logger.info(\"Creating scoring data iterator\")\n    logger.info(\"==============================\")\n\n    # One bucket to hold them all,\n    bucket = (max_seq_len_source, max_seq_len_target)\n\n    # ...One loader to raise them,\n    data_loader = RawParallelDatasetLoader(buckets=[bucket],\n                                           eos_id=target_vocab[C.EOS_SYMBOL],\n                                           pad_id=C.PAD_ID,\n                                           skip_blanks=False)\n\n    # ...one iterator to traverse them all,\n    scoring_iter = BatchedRawParallelSampleIter(data_loader=data_loader,\n                                                sources=sources,\n                                                target=target,\n                                                source_vocabs=source_vocabs,\n                                                target_vocab=target_vocab,\n                                                bucket=bucket,\n                                                batch_size=batch_size,\n                                                max_lens=(max_seq_len_source, max_seq_len_target),\n                                                num_factors=len(sources))\n\n    # and with the model appraise them.\n    return scoring_iter", "response": "Returns a data iterator for scoring."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef describe_data_and_buckets(data_statistics: DataStatistics, bucket_batch_sizes: List[BucketBatchSize]):\n    check_condition(len(bucket_batch_sizes) == len(data_statistics.buckets),\n                    \"Number of bucket batch sizes (%d) does not match number of buckets in statistics (%d).\"\n                    % (len(bucket_batch_sizes), len(data_statistics.buckets)))\n    if data_statistics.length_ratio_stats_per_bucket:\n        for bucket_batch_size, num_seq, (lr_mean, lr_std) in zip(bucket_batch_sizes,\n                                                                data_statistics.num_sents_per_bucket,\n                                                                data_statistics.length_ratio_stats_per_bucket):\n            if num_seq > 0:\n                logger.info(\"Bucket %s: %d samples in %d batches of %d, ~%.1f tokens/batch, \"\n                            \"trg/src length ratio: %.2f (+-%.2f)\",\n                            bucket_batch_size.bucket,\n                            num_seq,\n                            math.ceil(num_seq / bucket_batch_size.batch_size),\n                            bucket_batch_size.batch_size,\n                            bucket_batch_size.average_words_per_batch,\n                            lr_mean, lr_std)\n    else:\n        # TODO: remove with next bump of C.PREPARED_DATA_VERSION\n        for bucket_batch_size, num_seq in zip(bucket_batch_sizes, data_statistics.num_sents_per_bucket):\n            if num_seq > 0:\n                logger.info(\"Bucket %s: %d samples in %d batches of %d, ~%.1f tokens/batch, \",\n                            bucket_batch_size.bucket,\n                            num_seq,\n                            math.ceil(num_seq / bucket_batch_size.batch_size),\n                            bucket_batch_size.batch_size,\n                            bucket_batch_size.average_words_per_batch)", "response": "Describe data statistics across buckets."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read_content(path: str, limit: Optional[int] = None) -> Iterator[List[str]]:\n    with smart_open(path) as indata:\n        for i, line in enumerate(indata):\n            if limit is not None and i == limit:\n                break\n            yield list(get_tokens(line))", "response": "Reads the content of a file into a list of tokens."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning sequence of integer ids given a sequence of tokens and vocab.", "response": "def tokens2ids(tokens: Iterable[str], vocab: Dict[str, int]) -> List[int]:\n    \"\"\"\n    Returns sequence of integer ids given a sequence of tokens and vocab.\n\n    :param tokens: List of string tokens.\n    :param vocab: Vocabulary (containing UNK symbol).\n    :return: List of word ids.\n    \"\"\"\n    return [vocab.get(w, vocab[C.UNK_SYMBOL]) for w in tokens]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef strids2ids(tokens: Iterable[str]) -> List[int]:\n    return list(map(int, tokens))", "response": "Returns a sequence of integer ids given a sequence of string ids."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ids2strids(ids: Iterable[int]) -> str:\n    return C.TOKEN_SEPARATOR.join(map(str, ids))", "response": "Returns a string representation of a sequence of integers."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntransforms a list of token IDs into a list of words excluding any IDs in exclude_set.", "response": "def ids2tokens(token_ids: Iterable[int],\n               vocab_inv: Dict[int, str],\n               exclude_set: Set[int]) -> Iterator[str]:\n    \"\"\"\n    Transforms a list of token IDs into a list of words, excluding any IDs in `exclude_set`.\n\n    :param token_ids: The list of token IDs.\n    :param vocab_inv: The inverse vocabulary.\n    :param exclude_set: The list of token IDs to exclude.\n    :return: The list of words.\n    \"\"\"\n    tokens = (vocab_inv[token] for token in token_ids)\n    return (tok for token_id, tok in zip(token_ids, tokens) if token_id not in exclude_set)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_sequence_readers(sources: List[str], target: str,\n                            vocab_sources: List[vocab.Vocab],\n                            vocab_target: vocab.Vocab) -> Tuple[List[SequenceReader], SequenceReader]:\n    \"\"\"\n    Create source readers with EOS and target readers with BOS.\n\n    :param sources: The file names of source data and factors.\n    :param target: The file name of the target data.\n    :param vocab_sources: The source vocabularies.\n    :param vocab_target: The target vocabularies.\n    :return: The source sequence readers and the target reader.\n    \"\"\"\n    source_sequence_readers = [SequenceReader(source, vocab, add_eos=True) for source, vocab in\n                               zip(sources, vocab_sources)]\n    target_sequence_reader = SequenceReader(target, vocab_target, add_bos=True)\n    return source_sequence_readers, target_sequence_reader", "response": "Create source and target sequences."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates iterators over source_iterables and target_iterables.", "response": "def parallel_iter(source_iterables: Sequence[Iterable[Optional[Any]]],\n                  target_iterable: Iterable[Optional[Any]],\n                  skip_blanks: bool = True):\n    \"\"\"\n    Creates iterators over parallel iteratables by calling iter() on the iterables\n    and chaining to parallel_iterate(). The purpose of the separation is to allow\n    the caller to save iterator state between calls, if desired.\n\n    :param source_iterables: A list of source iterables.\n    :param target_iterable: A target iterable.\n    :param skip_blanks: Whether to skip empty target lines.\n    :return: Iterators over sources and target.\n    \"\"\"\n    source_iterators = [iter(s) for s in source_iterables]\n    target_iterator = iter(target_iterable)\n    return parallel_iterate(source_iterators, target_iterator, skip_blanks)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parallel_iterate(source_iterators: Sequence[Iterator[Optional[Any]]],\n                     target_iterator: Iterator[Optional[Any]],\n                     skip_blanks: bool = True):\n    \"\"\"\n    Yields parallel source(s), target sequences from iterables.\n    Checks for token parallelism in source sequences.\n    Skips pairs where element in at least one iterable is None.\n    Checks that all iterables have the same number of elements.\n    Can optionally continue from an already-begun iterator.\n\n    :param source_iterators: A list of source iterators.\n    :param target_iterator: A target iterator.\n    :param skip_blanks: Whether to skip empty target lines.\n    :return: Iterators over sources and target.\n    \"\"\"\n    num_skipped = 0\n    while True:\n        try:\n            sources = [next(source_iter) for source_iter in source_iterators]\n            target = next(target_iterator)\n        except StopIteration:\n            break\n        if skip_blanks and (any((s is None for s in sources)) or target is None):\n            num_skipped += 1\n            continue\n        check_condition(are_none(sources) or are_token_parallel(sources), \"Source sequences are not token-parallel: %s\" % (str(sources)))\n        yield sources, target\n\n    if num_skipped > 0:\n        logger.warning(\"Parallel reading of sequences skipped %d elements\", num_skipped)\n\n    check_condition(\n        all(next(cast(Iterator, s), None) is None for s in source_iterators) and next(cast(Iterator, target_iterator),\n                                                                                      None) is None,\n        \"Different number of lines in source(s) and target iterables.\")", "response": "Yields source and target sequences from source_iterators and target_iterators."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_default_bucket_key(buckets: List[Tuple[int, int]]) -> Tuple[int, int]:\n    return max(buckets)", "response": "Returns the default bucket key from a list of buckets i. e. the largest bucket in the list."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_parallel_bucket(buckets: List[Tuple[int, int]],\n                        length_source: int,\n                        length_target: int) -> Tuple[Optional[int], Optional[Tuple[int, int]]]:\n    \"\"\"\n    Returns bucket index and bucket from a list of buckets, given source and target length.\n    Algorithm assumes buckets are sorted from shortest to longest.\n    Returns (None, None) if no bucket fits.\n\n    :param buckets: List of buckets, in sorted order, shortest to longest.\n    :param length_source: Length of source sequence.\n    :param length_target: Length of target sequence.\n    :return: Tuple of (bucket index, bucket), or (None, None) if not fitting.\n    \"\"\"\n    for j, (source_bkt, target_bkt) in enumerate(buckets):\n        if source_bkt >= length_source and target_bkt >= length_target:\n            return j, (source_bkt, target_bkt)\n    return None, None", "response": "Returns the index and bucket of the next bucket in a list of buckets given source and target length."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_target_bucket(buckets: List[Tuple[int, int]],\n                      length_target: int) -> Optional[Tuple[int, Tuple[int, int]]]:\n    \"\"\"\n    Returns bucket index and bucket from a list of buckets, given source and target length.\n    Returns (None, None) if no bucket fits.\n\n    :param buckets: List of buckets.\n    :param length_target: Length of target sequence.\n    :return: Tuple of (bucket index, bucket), or (None, None) if not fitting.\n    \"\"\"\n    bucket = None, None  # type: Tuple[int, Tuple[int, int]]\n    for j, (source_bkt, target_bkt) in enumerate(buckets):\n        if target_bkt >= length_target:\n            bucket = j, (source_bkt, target_bkt)\n            break\n    return bucket", "response": "Returns the index and bucket of the target sequence."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the indices of a random permutation for each bucket and the corresponding inverse permutations that can be applied to the data.", "response": "def get_permutations(bucket_counts: List[int]) -> Tuple[List[mx.nd.NDArray], List[mx.nd.NDArray]]:\n    \"\"\"\n    Returns the indices of a random permutation for each bucket and the corresponding inverse permutations that can\n    restore the original order of the data if applied to the permuted data.\n\n    :param bucket_counts: The number of elements per bucket.\n    :return: For each bucket a permutation and inverse permutation is returned.\n    \"\"\"\n    data_permutations = []  # type: List[mx.nd.NDArray]\n    inverse_data_permutations = []  # type: List[mx.nd.NDArray]\n    for num_samples in bucket_counts:\n        if num_samples == 0:\n            num_samples = 1\n        # new random order:\n        data_permutation = np.random.permutation(num_samples)\n        inverse_data_permutation = np.empty(num_samples, np.int32)\n        inverse_data_permutation[data_permutation] = np.arange(num_samples)\n        inverse_data_permutation = mx.nd.array(inverse_data_permutation)\n        data_permutation = mx.nd.array(data_permutation)\n\n        data_permutations.append(data_permutation)\n        inverse_data_permutations.append(inverse_data_permutation)\n    return data_permutations, inverse_data_permutations"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of indices that index into the bucket and the start index inside a bucket.", "response": "def get_batch_indices(data: ParallelDataSet,\n                      bucket_batch_sizes: List[BucketBatchSize]) -> List[Tuple[int, int]]:\n    \"\"\"\n    Returns a list of index tuples that index into the bucket and the start index inside a bucket given\n    the batch size for a bucket. These indices are valid for the given dataset.\n\n    Put another way, this returns the starting points for all batches within the dataset, across all buckets.\n\n    :param data: Data to create indices for.\n    :param bucket_batch_sizes: Bucket batch sizes.\n    :return: List of 2d indices.\n    \"\"\"\n    # create index tuples (i,j) into buckets: i := bucket index ; j := row index of bucket array\n    idxs = []  # type: List[Tuple[int, int]]\n    for buck_idx, buck in enumerate(data.source):\n        bucket = bucket_batch_sizes[buck_idx].bucket\n        batch_size = bucket_batch_sizes[buck_idx].batch_size\n        num_samples = data.source[buck_idx].shape[0]\n        rest = num_samples % batch_size\n        if rest > 0:\n            logger.info(\"Ignoring %d samples from bucket %s with %d samples due to incomplete batch\",\n                        rest, bucket, num_samples)\n        idxs.extend([(buck_idx, j) for j in range(0, num_samples - batch_size + 1, batch_size)])\n    return idxs"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef save(self, fname: str):\n        mx.nd.save(fname, self.source + self.target + self.label)", "response": "Saves the dataset to a binary. npy file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading a dataset from a binary. npy file.", "response": "def load(fname: str) -> 'ParallelDataSet':\n        \"\"\"\n        Loads a dataset from a binary .npy file.\n        \"\"\"\n        data = mx.nd.load(fname)\n        n = len(data) // 3\n        source = data[:n]\n        target = data[n:2 * n]\n        label = data[2 * n:]\n        assert len(source) == len(target) == len(label)\n        return ParallelDataSet(source, target, label)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a new dataset with buckets filled up to the next multiple of batch size.", "response": "def fill_up(self,\n                bucket_batch_sizes: List[BucketBatchSize],\n                seed: int = 42) -> 'ParallelDataSet':\n        \"\"\"\n        Returns a new dataset with buckets filled up.\n\n        :param bucket_batch_sizes: Bucket batch sizes.\n        :param seed: The random seed used for sampling sentences to fill up.\n        :return: New dataset with buckets filled up to the next multiple of batch size\n        \"\"\"\n        source = list(self.source)\n        target = list(self.target)\n        label = list(self.label)\n\n        rs = np.random.RandomState(seed)\n\n        for bucket_idx in range(len(self)):\n            bucket = bucket_batch_sizes[bucket_idx].bucket\n            bucket_batch_size = bucket_batch_sizes[bucket_idx].batch_size\n            bucket_source = self.source[bucket_idx]\n            bucket_target = self.target[bucket_idx]\n            bucket_label = self.label[bucket_idx]\n            num_samples = bucket_source.shape[0]\n\n            # Fill up the last batch by randomly sampling from the extant items.\n            if num_samples % bucket_batch_size != 0:\n                rest = bucket_batch_size - num_samples % bucket_batch_size\n                desired_indices_np = rs.randint(num_samples, size=rest)\n                desired_indices = mx.nd.array(desired_indices_np)\n\n                if isinstance(source[bucket_idx], np.ndarray):\n                    source[bucket_idx] = np.concatenate((bucket_source, bucket_source.take(desired_indices_np)), axis=0)\n                else:\n                    source[bucket_idx] = mx.nd.concat(bucket_source, bucket_source.take(desired_indices), dim=0)\n                target[bucket_idx] = mx.nd.concat(bucket_target, bucket_target.take(desired_indices), dim=0)\n                label[bucket_idx] = mx.nd.concat(bucket_label, bucket_label.take(desired_indices), dim=0)\n\n        return ParallelDataSet(source, target, label)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef permute(self, permutations: List[mx.nd.NDArray]) -> 'ParallelDataSet':\n        assert len(self) == len(permutations)\n        source = []\n        target = []\n        label = []\n        for buck_idx in range(len(self)):\n            num_samples = self.source[buck_idx].shape[0]\n            if num_samples:  # not empty bucket\n                permutation = permutations[buck_idx]\n                if isinstance(self.source[buck_idx], np.ndarray):\n                    source.append(self.source[buck_idx].take(np.int64(permutation.asnumpy())))\n                else:\n                    source.append(self.source[buck_idx].take(permutation))\n                target.append(self.target[buck_idx].take(permutation))\n                label.append(self.label[buck_idx].take(permutation))\n            else:\n                source.append(self.source[buck_idx])\n                target.append(self.target[buck_idx])\n                label.append(self.label[buck_idx])\n\n        return ParallelDataSet(source, target, label)", "response": "Permutes the data within each bucket."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef iter_next(self) -> bool:\n\n        # Read batch_size lines from the source stream\n        sources_sentences = [[] for x in self.sources_sentences]  # type: List[List[str]]\n        target_sentences = []  # type: List[str]\n        num_read = 0\n        for num_read, (sources, target) in enumerate(parallel_iterate(self.sources_iters, self.target_iter, skip_blanks=False), 1):\n            source_len = 0 if sources[0] is None else len(sources[0])\n            target_len = 0 if target is None else len(target)\n            if source_len > self.max_len_source:\n                logger.info(\"Trimming source sentence {} ({} -> {})\".format(self.sentno + num_read, source_len, self.max_len_source))\n                sources = [source[0:self.max_len_source] for source in sources]\n            if target_len > self.max_len_target:\n                logger.info(\"Trimming target sentence {} ({} -> {})\".format(self.sentno + num_read, target_len, self.max_len_target))\n                target = target[0:self.max_len_target]\n\n            for i, source in enumerate(sources):\n                sources_sentences[i].append(source)\n            target_sentences.append(target)\n            if num_read == self.batch_size:\n                break\n\n        self.sentno += num_read\n\n        if num_read == 0:\n            self.next_batch = None\n            return False\n\n        # The final batch may be underfilled, so mark it\n        num_pad = self.batch_size - num_read\n\n        dataset = self.data_loader.load(sources_sentences,\n                                        target_sentences,\n                                        [num_read]).fill_up(self.bucket_batch_sizes)\n\n        data = [dataset.source[0], dataset.target[0]]\n        label = dataset.label\n\n        provide_data = [mx.io.DataDesc(name=n, shape=x.shape, layout=C.BATCH_MAJOR) for n, x in\n                        zip(self.data_names, data)]\n        provide_label = [mx.io.DataDesc(name=n, shape=x.shape, layout=C.BATCH_MAJOR) for n, x in\n                         zip(self.label_names, label)]\n\n        self.next_batch = mx.io.DataBatch(data, label,\n                                          pad=num_pad, index=None, bucket_key=self.buckets[0],\n                                          provide_data=provide_data, provide_label=provide_label)\n\n        return True", "response": "Returns True if the iterator can return another batch."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef next(self) -> mx.io.DataBatch:\n        if self.iter_next():\n            return self.next_batch\n        raise StopIteration", "response": "Returns the next batch."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nresets and reshuffles the data.", "response": "def reset(self):\n        \"\"\"\n        Resets and reshuffles the data.\n        \"\"\"\n        self.curr_batch_index = 0\n        if self.permute:\n            # shuffle batch start indices\n            random.shuffle(self.batch_indices)\n\n            # restore the data permutation\n            self.data = self.data.permute(self.inverse_data_permutations)\n\n            # permute the data within each batch\n            self.data_permutations, self.inverse_data_permutations = get_permutations(self.data.get_bucket_counts())\n            self.data = self.data.permute(self.data_permutations)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the next batch from the data iterator.", "response": "def next(self) -> mx.io.DataBatch:\n        \"\"\"\n        Returns the next batch from the data iterator.\n        \"\"\"\n        if not self.iter_next():\n            raise StopIteration\n\n        i, j = self.batch_indices[self.curr_batch_index]\n        self.curr_batch_index += 1\n\n        batch_size = self.bucket_batch_sizes[i].batch_size\n        source = self.data.source[i][j:j + batch_size]\n        target = self.data.target[i][j:j + batch_size]\n        data = [source, target]\n        label = [self.data.label[i][j:j + batch_size]]\n\n        provide_data = [mx.io.DataDesc(name=n, shape=x.shape, layout=C.BATCH_MAJOR) for n, x in\n                        zip(self.data_names, data)]\n        provide_label = [mx.io.DataDesc(name=n, shape=x.shape, layout=C.BATCH_MAJOR) for n, x in\n                         zip(self.label_names, label)]\n\n        # TODO: num pad examples is not set here if fillup policy would be padding\n        return mx.io.DataBatch(data, label,\n                               pad=0, index=None, bucket_key=self.buckets[i],\n                               provide_data=provide_data, provide_label=provide_label)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsaves the current state of the iterator to a file.", "response": "def save_state(self, fname: str):\n        \"\"\"\n        Saves the current state of iterator to a file, so that iteration can be\n        continued. Note that the data is not saved, i.e. the iterator must be\n        initialized with the same parameters as in the first call.\n\n        :param fname: File name to save the information to.\n        \"\"\"\n        with open(fname, \"wb\") as fp:\n            pickle.dump(self.batch_indices, fp)\n            pickle.dump(self.curr_batch_index, fp)\n            np.save(fp, [a.asnumpy() for a in self.inverse_data_permutations])\n            np.save(fp, [a.asnumpy() for a in self.data_permutations])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load_state(self, fname: str):\n\n        # restore order\n        self.data = self.data.permute(self.inverse_data_permutations)\n\n        with open(fname, \"rb\") as fp:\n            self.batch_indices = pickle.load(fp)\n            self.curr_batch_index = pickle.load(fp)\n            inverse_data_permutations = np.load(fp)\n            data_permutations = np.load(fp)\n\n        # Right after loading the iterator state, next() should be called\n        self.curr_batch_index -= 1\n\n        # load previous permutations\n        self.inverse_data_permutations = []\n        self.data_permutations = []\n\n        for bucket in range(len(self.data)):\n            inverse_permutation = mx.nd.array(inverse_data_permutations[bucket])\n            self.inverse_data_permutations.append(inverse_permutation)\n\n            permutation = mx.nd.array(data_permutations[bucket])\n            self.data_permutations.append(permutation)\n\n        self.data = self.data.permute(self.data_permutations)", "response": "Loads the state of the iterator from a file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_checkpoint_decoder(args: argparse.Namespace,\n                              exit_stack: ExitStack,\n                              train_context: List[mx.Context]) -> Optional[checkpoint_decoder.CheckpointDecoder]:\n    \"\"\"\n    Returns a checkpoint decoder or None.\n\n    :param args: Arguments as returned by argparse.\n    :param exit_stack: An ExitStack from contextlib.\n    :param train_context: Context for training.\n    :return: A CheckpointDecoder if --decode-and-evaluate != 0, else None.\n    \"\"\"\n    sample_size = args.decode_and_evaluate\n    if args.optimized_metric == C.BLEU and sample_size == 0:\n        logger.info(\"You chose BLEU as the optimized metric, will turn on BLEU monitoring during training. \"\n                    \"To control how many validation sentences are used for calculating bleu use \"\n                    \"the --decode-and-evaluate argument.\")\n        sample_size = -1\n\n    if sample_size == 0:\n        return None\n\n    if args.use_cpu or args.decode_and_evaluate_use_cpu:\n        context = mx.cpu()\n    elif args.decode_and_evaluate_device_id is not None:\n        context = utils.determine_context(device_ids=args.decode_and_evaluate_device_id,\n                                          use_cpu=False,\n                                          disable_device_locking=args.disable_device_locking,\n                                          lock_dir=args.lock_dir,\n                                          exit_stack=exit_stack)[0]\n    else:\n        # default decode context is the last training device\n        context = train_context[-1]\n\n    return checkpoint_decoder.CheckpointDecoderImageModel(context=context,\n                                                          inputs=[args.validation_source] + args.validation_source_factors,\n                                                          references=args.validation_target,\n                                                          model=args.output,\n                                                          sample_size=sample_size,\n                                                          source_image_size=args.source_image_size,\n                                                          image_root=args.validation_source_root,\n                                                          max_output_length=args.max_output_length,\n                                                          use_feature_loader=args.image_preextracted_features)", "response": "Create a checkpoint decoder for the given arguments."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate the data iterators and the vocabularies.", "response": "def create_data_iters_and_vocab(args: argparse.Namespace,\n                                max_seq_len_source: int,\n                                max_seq_len_target: int,\n                                resume_training: bool,\n                                output_folder: str) -> Tuple['data_io.BaseParallelSampleIter',\n                                                             'data_io.BaseParallelSampleIter',\n                                                             'data_io.DataConfig', Dict]:\n    \"\"\"\n    Create the data iterators and the vocabularies.\n\n    :param args: Arguments as returned by argparse.\n    :param max_seq_len_source: Source maximum sequence length.\n    :param max_seq_len_target: Target maximum sequence length.\n    :param resume_training: Whether to resume training.\n    :param output_folder: Output folder.\n    :return: The data iterators (train, validation, config_data) as well as the source and target vocabularies.\n    \"\"\"\n\n    _, num_words_target = args.num_words\n    num_words_target = num_words_target if num_words_target > 0 else None\n    _, word_min_count_target = args.word_min_count\n    batch_num_devices = 1 if args.use_cpu else sum(-di if di < 0 else 1 for di in args.device_ids)\n    batch_by_words = args.batch_type == C.BATCH_TYPE_WORD\n\n    either_raw_or_prepared_error_msg = \"Either specify a raw training corpus with %s or a preprocessed corpus \" \\\n                                       \"with %s.\" % (C.TRAINING_ARG_TARGET,\n                                                     C.TRAINING_ARG_PREPARED_DATA)\n    # Note: ignore args.prepared_data for the moment\n    utils.check_condition(args.prepared_data is None and args.target is not None,\n                          either_raw_or_prepared_error_msg)\n\n    if resume_training:\n        # Load the existing vocab created when starting the training run.\n        target_vocab = vocab.vocab_from_json(os.path.join(output_folder, C.VOCAB_TRG_NAME))\n\n        # Recover the vocabulary path from the existing config file:\n        data_info = cast(data_io.DataInfo, Config.load(os.path.join(output_folder, C.DATA_INFO)))\n        target_vocab_path = data_info.target_vocab\n    else:\n        # Load vocab:\n        target_vocab_path = args.target_vocab\n        # Note: We do not care about the source vocab for images, that is why some inputs are mocked\n        target_vocab = vocab.load_or_create_vocab(data=args.target,\n                                                  vocab_path=target_vocab_path,\n                                                  num_words=num_words_target,\n                                                  word_min_count=word_min_count_target)\n\n    train_iter, validation_iter, config_data, data_info = data_io_image.get_training_image_text_data_iters(\n        source_root=args.source_root,\n        source=os.path.abspath(args.source),\n        target=os.path.abspath(args.target),\n        validation_source_root=args.validation_source_root,\n        validation_source=os.path.abspath(args.validation_source),\n        validation_target=os.path.abspath(args.validation_target),\n        vocab_target=target_vocab,\n        vocab_target_path=target_vocab_path,\n        batch_size=args.batch_size,\n        batch_by_words=batch_by_words,\n        batch_num_devices=batch_num_devices,\n        source_image_size=args.source_image_size,\n        max_seq_len_target=max_seq_len_target,\n        bucketing=not args.no_bucketing,\n        bucket_width=args.bucket_width,\n        use_feature_loader=args.image_preextracted_features,\n        preload_features=args.load_all_features_to_memory\n    )\n\n    data_info_fname = os.path.join(output_folder, C.DATA_INFO)\n    logger.info(\"Writing data config to '%s'\", data_info_fname)\n    # Removing objects that cannot be saved:\n    data_info.sources = None\n    data_info.save(data_info_fname)\n\n    return train_iter, validation_iter, config_data, target_vocab"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a ModelConfig from the given arguments.", "response": "def create_model_config(args: argparse.Namespace,\n                        vocab_target_size: int,\n                        max_seq_len_source: int,\n                        max_seq_len_target: int,\n                        config_data: data_io.DataConfig) -> model.ModelConfig:\n    \"\"\"\n    Create a ModelConfig from the argument given in the command line.\n\n    :param args: Arguments as returned by argparse.\n    :param vocab_target_size: The size of the target vocabulary.\n    :param max_seq_len_source: Maximum source sequence length.\n    :param max_seq_len_target: Maximum target sequence length.\n    :param config_data: Data config.\n    :return: The model configuration.\n    \"\"\"\n    num_embed_source, num_embed_target = get_num_embed(args)\n    _, embed_dropout_target = args.embed_dropout\n\n    config_encoder, encoder_num_hidden = create_encoder_config(args)\n    config_decoder = create_decoder_config(args, encoder_num_hidden, max_seq_len_source, max_seq_len_target,\n                                           num_embed_target)\n\n    config_embed_source = encoder.PassThroughEmbeddingConfig()\n    config_embed_target = encoder.EmbeddingConfig(vocab_size=vocab_target_size,\n                                                  num_embed=num_embed_target,\n                                                  dropout=embed_dropout_target)\n\n    config_loss = loss.LossConfig(name=args.loss,\n                                  vocab_size=vocab_target_size,\n                                  normalization_type=args.loss_normalization_type,\n                                  label_smoothing=args.label_smoothing)\n\n    model_config = model.ModelConfig(config_data=config_data,\n                                     vocab_source_size=0,\n                                     vocab_target_size=vocab_target_size,\n                                     config_embed_source=config_embed_source,\n                                     config_embed_target=config_embed_target,\n                                     config_encoder=config_encoder,\n                                     config_decoder=config_decoder,\n                                     config_loss=config_loss,\n                                     weight_tying=args.weight_tying,\n                                     weight_tying_type=args.weight_tying_type if args.weight_tying else None,\n                                     weight_normalization=args.weight_normalization,\n                                     lhuc=args.lhuc is not None)\n    return model_config"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting initializers from encoders. Some encoders might be initialized from pretrained models.", "response": "def get_preinit_encoders(encoders: List[encoder.Encoder]) -> List[Tuple[str, mx.init.Initializer]]:\n    \"\"\"\n    Get initializers from encoders. Some encoders might be initialized from pretrained models.\n\n    :param encoders: List of encoders\n    :return: The list of initializers\n    \"\"\"\n    init = []  # type: List[Tuple[str, mx.init.Initializer]]\n    for enc in encoders:\n        if hasattr(enc, \"get_initializers\"):\n            enc = cast(encoder_image.ImageLoadedCnnEncoder, enc)\n            init.extend(enc.get_initializers())\n    return init"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_recurrent_encoder(config: RecurrentEncoderConfig, prefix: str) -> 'Encoder':\n    # TODO give more control on encoder architecture\n    encoder_seq = EncoderSequence([], config.dtype)\n\n    if config.conv_config is not None:\n        encoder_seq.append(ConvolutionalEmbeddingEncoder, config=config.conv_config,\n                           prefix=prefix + C.CHAR_SEQ_ENCODER_PREFIX)\n        if config.conv_config.add_positional_encoding:\n            # If specified, add positional encodings to segment embeddings\n            encoder_seq.append(AddSinCosPositionalEmbeddings,\n                               num_embed=config.conv_config.num_embed,\n                               scale_up_input=False,\n                               scale_down_positions=False,\n                               prefix=\"%s%sadd_positional_encodings\" % (prefix, C.CHAR_SEQ_ENCODER_PREFIX))\n        encoder_seq.append(ConvertLayout, infer_hidden=True, target_layout=C.TIME_MAJOR)\n    else:\n        encoder_seq.append(ConvertLayout, target_layout=C.TIME_MAJOR, num_hidden=0)\n\n    if config.reverse_input:\n        encoder_seq.append(ReverseSequence, infer_hidden=True)\n\n    if config.rnn_config.residual:\n        utils.check_condition(config.rnn_config.first_residual_layer >= 2,\n                              \"Residual connections on the first encoder layer are not supported\")\n\n    # One layer bi-directional RNN:\n    encoder_seq.append(BiDirectionalRNNEncoder,\n                       rnn_config=config.rnn_config.copy(num_layers=1),\n                       prefix=prefix + C.BIDIRECTIONALRNN_PREFIX,\n                       layout=C.TIME_MAJOR)\n\n    if config.rnn_config.num_layers > 1:\n        # Stacked uni-directional RNN:\n        # Because we already have a one layer bi-rnn we reduce the num_layers as well as the first_residual_layer.\n        remaining_rnn_config = config.rnn_config.copy(num_layers=config.rnn_config.num_layers - 1,\n                                                      first_residual_layer=config.rnn_config.first_residual_layer - 1)\n        encoder_seq.append(RecurrentEncoder,\n                           rnn_config=remaining_rnn_config,\n                           prefix=prefix + C.STACKEDRNN_PREFIX,\n                           layout=C.TIME_MAJOR)\n\n    encoder_seq.append(ConvertLayout, infer_hidden=True, target_layout=C.BATCH_MAJOR)\n\n    return encoder_seq", "response": "Returns an encoder stack with a bi - directional RNN and a variable number of uni - directional forward RNNs."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_convolutional_encoder(config: ConvolutionalEncoderConfig, prefix: str) -> 'Encoder':\n    encoder_seq = EncoderSequence([], dtype=config.dtype)\n    cls, encoder_params = _get_positional_embedding_params(config.positional_embedding_type,\n                                                           config.num_embed,\n                                                           max_seq_len=config.max_seq_len_source,\n                                                           fixed_pos_embed_scale_up_input=False,\n                                                           fixed_pos_embed_scale_down_positions=True,\n                                                           prefix=prefix + C.SOURCE_POSITIONAL_EMBEDDING_PREFIX)\n    encoder_seq.append(cls, **encoder_params)\n    encoder_seq.append(ConvolutionalEncoder, config=config)\n    return encoder_seq", "response": "Creates a convolutional encoder."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_transformer_encoder(config: transformer.TransformerConfig, prefix: str) -> 'Encoder':\n    encoder_seq = EncoderSequence([], dtype=config.dtype)\n    cls, encoder_params = _get_positional_embedding_params(config.positional_embedding_type,\n                                                           config.model_size,\n                                                           config.max_seq_len_source,\n                                                           fixed_pos_embed_scale_up_input=True,\n                                                           fixed_pos_embed_scale_down_positions=False,\n                                                           prefix=prefix + C.SOURCE_POSITIONAL_EMBEDDING_PREFIX)\n    encoder_seq.append(cls, **encoder_params)\n    if config.conv_config is not None:\n        encoder_seq.append(ConvolutionalEmbeddingEncoder, config=config.conv_config,\n                           prefix=prefix + C.CHAR_SEQ_ENCODER_PREFIX)\n\n    encoder_seq.append(TransformerEncoder, config=config, prefix=prefix + C.TRANSFORMER_ENCODER_PREFIX)\n\n    return encoder_seq", "response": "Returns a Transformer encoder with positional encodings and TransformerEncoder instances."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nencode data given sequence lengths of individual examples and maximum sequence length.", "response": "def encode(self,\n               data: mx.sym.Symbol,\n               data_length: Optional[mx.sym.Symbol],\n               seq_len: int) -> Tuple[mx.sym.Symbol, mx.sym.Symbol, int]:\n        \"\"\"\n        Encodes data given sequence lengths of individual examples and maximum sequence length.\n\n        :param data: Input data.\n        :param data_length: Vector with sequence lengths.\n        :param seq_len: Maximum sequence length.\n        :return: Encoded versions of input data (data, data_length, seq_len).\n        \"\"\"\n        with mx.AttrScope(__layout__=self.target_layout):\n            return mx.sym.swapaxes(data=data, dim1=0, dim2=1), data_length, seq_len"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef encode(self,\n               data: mx.sym.Symbol,\n               data_length: Optional[mx.sym.Symbol],\n               seq_len: int) -> Tuple[mx.sym.Symbol, mx.sym.Symbol, int]:\n        \"\"\"\n        Encodes data given sequence lengths of individual examples and maximum sequence length.\n\n        :param data: Input data.\n        :param data_length: Vector with sequence lengths.\n        :param seq_len: Maximum sequence length.\n        :return: Encoded versions of input data (data, data_length, seq_len).\n        \"\"\"\n        factor_embeddings = []  # type: List[mx.sym.Symbol]\n        if self.is_source:\n            data, *data_factors = mx.sym.split(data=data,\n                                               num_outputs=self.config.num_factors,\n                                               axis=2,\n                                               squeeze_axis=True, name=self.prefix + \"factor_split\")\n\n            if self.config.factor_configs is not None:\n                for i, (factor_data, factor_config, factor_weight) in enumerate(zip(data_factors,\n                                                                                    self.config.factor_configs,\n                                                                                    self.embed_factor_weights)):\n                    factor_embeddings.append(mx.sym.Embedding(data=factor_data,\n                                                              input_dim=factor_config.vocab_size,\n                                                              weight=factor_weight,\n                                                              output_dim=factor_config.num_embed,\n                                                              name=self.prefix + \"factor%d_embed\" % i))\n\n        embedding = mx.sym.Embedding(data=data,\n                                     input_dim=self.config.vocab_size,\n                                     weight=self.embed_weight,\n                                     output_dim=self.config.num_embed,\n                                     name=self.prefix + \"embed\")\n\n        if self.config.factor_configs is not None:\n            if self.config.source_factors_combine == C.SOURCE_FACTORS_COMBINE_CONCAT:\n                embedding = mx.sym.concat(embedding, *factor_embeddings, dim=2, name=self.prefix + \"embed_plus_factors\")\n            else:\n                embedding = mx.sym.add_n(embedding, *factor_embeddings, name=self.prefix + \"embed_plus_factors\")\n\n        if self.config.dropout > 0:\n            embedding = mx.sym.Dropout(data=embedding, p=self.config.dropout, name=\"source_embed_dropout\")\n\n        return embedding, data_length, seq_len", "response": "Encodes data given sequence lengths of individual examples and maximum sequence length."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef encode(self,\n               data: mx.sym.Symbol,\n               data_length: Optional[mx.sym.Symbol],\n               seq_len: int) -> Tuple[mx.sym.Symbol, mx.sym.Symbol, int]:\n        \"\"\"\n        :param data: (batch_size, source_seq_len, num_embed)\n        :param data_length: (batch_size,)\n        :param seq_len: sequence length.\n        :return: (batch_size, source_seq_len, num_embed)\n        \"\"\"\n        positions = mx.sym.arange(0, seq_len)\n        embedding = self.encode_positions(positions, data)\n        return embedding, data_length, seq_len", "response": "Encodes the data into a sequence of sequence length embedding."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef encode_positions(self,\n                         positions: mx.sym.Symbol,\n                         data: mx.sym.Symbol) -> mx.sym.Symbol:\n        \"\"\"\n        :param positions: (batch_size,)\n        :param data: (batch_size, num_embed)\n        :return: (batch_size, num_embed)\n        \"\"\"\n        # (batch_size, 1)\n        positions = mx.sym.expand_dims(positions, axis=1)\n        # (num_embed,)\n        channels = mx.sym.arange(0, self.num_embed // 2)\n        # (1, num_embed,)\n        scaling = mx.sym.expand_dims(1. / mx.sym.pow(10000, (2 * channels) / self.num_embed), axis=0)\n\n        # (batch_size, num_embed/2)\n        scaled_positions = mx.sym.dot(positions, scaling)\n\n        sin = mx.sym.sin(scaled_positions)\n        cos = mx.sym.cos(scaled_positions)\n\n        # (batch_size, num_embed)\n        pos_embedding = mx.sym.concat(sin, cos, dim=1)\n\n        if self.scale_up_input:\n            data = data * (self.num_embed ** 0.5)\n\n        if self.scale_down_positions:\n            pos_embedding = pos_embedding * (self.num_embed ** -0.5)\n\n        pos_embedding = mx.sym.BlockGrad(pos_embedding)\n\n        return mx.sym.broadcast_add(data, pos_embedding, name=\"%s_add\" % self.prefix)", "response": "Encode the data for the cluster."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef encode(self,\n               data: mx.sym.Symbol,\n               data_length: Optional[mx.sym.Symbol],\n               seq_len: int) -> Tuple[mx.sym.Symbol, mx.sym.Symbol, int]:\n        \"\"\"\n        :param data: (batch_size, source_seq_len, num_embed)\n        :param data_length: (batch_size,)\n        :param seq_len: sequence length.\n        :return: (batch_size, source_seq_len, num_embed)\n        \"\"\"\n\n        # (1, source_seq_len)\n        positions = mx.sym.expand_dims(data=mx.sym.arange(start=0, stop=seq_len, step=1), axis=0)\n\n        # (1, source_seq_len, num_embed)\n        pos_embedding = mx.sym.Embedding(data=positions,\n                                         input_dim=self.max_seq_len,\n                                         weight=self.embed_weight,\n                                         output_dim=self.num_embed,\n                                         name=self.prefix + \"pos_embed\")\n        return mx.sym.broadcast_add(data, pos_embedding, name=\"%s_add\" % self.prefix), data_length, seq_len", "response": "Encode the data into a sequence of size data_length and num_embed."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nencoding the positions into a new symbol.", "response": "def encode_positions(self,\n                         positions: mx.sym.Symbol,\n                         data: mx.sym.Symbol) -> mx.sym.Symbol:\n        \"\"\"\n        :param positions: (batch_size,)\n        :param data: (batch_size, num_embed)\n        :return: (batch_size, num_embed)\n        \"\"\"\n\n        # (batch_size, source_seq_len, num_embed)\n        pos_embedding = mx.sym.Embedding(data=positions,\n                                         input_dim=self.max_seq_len,\n                                         weight=self.embed_weight,\n                                         output_dim=self.num_embed,\n                                         name=self.prefix + \"pos_embed\")\n        return mx.sym.broadcast_add(data, pos_embedding, name=\"%s_add\" % self.prefix)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef encode(self,\n               data: mx.sym.Symbol,\n               data_length: mx.sym.Symbol,\n               seq_len: int) -> Tuple[mx.sym.Symbol, mx.sym.Symbol, int]:\n        \"\"\"\n        Encodes data given sequence lengths of individual examples and maximum sequence length.\n\n        :param data: Input data.\n        :param data_length: Vector with sequence lengths.\n        :param seq_len: Maximum sequence length.\n        :return: Encoded versions of input data (data, data_length, seq_len).\n        \"\"\"\n        for encoder in self.encoders:\n            data, data_length, seq_len = encoder.encode(data, data_length, seq_len)\n        return data, data_length, seq_len", "response": "Encodes data given sequence lengths of individual examples and maximum sequence length."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_encoded_seq_len(self, seq_len: int) -> int:\n        for encoder in self.encoders:\n            seq_len = encoder.get_encoded_seq_len(seq_len)\n        return seq_len", "response": "Returns the size of the encoded sequence."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the maximum length supported by the encoder if such a restriction exists.", "response": "def get_max_seq_len(self) -> Optional[int]:\n        \"\"\"\n        :return: The maximum length supported by the encoder if such a restriction exists.\n        \"\"\"\n        max_seq_len = min((encoder.get_max_seq_len()\n                           for encoder in self.encoders if encoder.get_max_seq_len() is not None), default=None)\n        return max_seq_len"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef append(self, cls, infer_hidden: bool = False, **kwargs) -> Encoder:\n        params = dict(kwargs)\n        if infer_hidden:\n            params['num_hidden'] = self.get_num_hidden()\n\n        sig_params = inspect.signature(cls.__init__).parameters\n        if 'dtype' in sig_params and 'dtype' not in kwargs:\n            params['dtype'] = self.dtype\n        encoder = cls(**params)\n        self.encoders.append(encoder)\n        return encoder", "response": "Extends sequence with new Encoder."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nencodes data given sequence lengths of individual examples and maximum sequence length.", "response": "def encode(self,\n               data: mx.sym.Symbol,\n               data_length: Optional[mx.sym.Symbol],\n               seq_len: int) -> Tuple[mx.sym.Symbol, mx.sym.Symbol, int]:\n        \"\"\"\n        Encodes data given sequence lengths of individual examples and maximum sequence length.\n        :param data: Input data.\n        :param data_length: Vector with sequence lengths.\n        :param seq_len: Maximum sequence length.\n        :return: Expected number of empty states (zero-filled).\n        \"\"\"\n        # outputs: (batch_size, seq_len, num_hidden)\n        outputs = mx.sym.dot(data, mx.sym.zeros((self.num_embed, self.num_hidden)))\n        return outputs, data_length, seq_len"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef encode(self,\n               data: mx.sym.Symbol,\n               data_length: Optional[mx.sym.Symbol],\n               seq_len: int) -> Tuple[mx.sym.Symbol, mx.sym.Symbol, int]:\n        \"\"\"\n        Encodes data given sequence lengths of individual examples and maximum sequence length.\n\n        :param data: Input data.\n        :param data_length: Vector with sequence lengths.\n        :param seq_len: Maximum sequence length.\n        :return: Encoded versions of input data (data, data_length, seq_len).\n        \"\"\"\n        outputs, _ = self.rnn.unroll(seq_len, inputs=data, merge_outputs=True, layout=self.layout)\n\n        return outputs, data_length, seq_len", "response": "Encodes data given sequence lengths of individual examples and maximum sequence length."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef encode(self,\n               data: mx.sym.Symbol,\n               data_length: mx.sym.Symbol,\n               seq_len: int) -> Tuple[mx.sym.Symbol, mx.sym.Symbol, int]:\n        \"\"\"\n        Encodes data given sequence lengths of individual examples and maximum sequence length.\n\n        :param data: Input data.\n        :param data_length: Vector with sequence lengths.\n        :param seq_len: Maximum sequence length.\n        :return: Encoded versions of input data (data, data_length, seq_len).\n        \"\"\"\n        if self.layout[0] == 'N':\n            data = mx.sym.swapaxes(data=data, dim1=0, dim2=1)\n        data = self._encode(data, data_length, seq_len)\n        if self.layout[0] == 'N':\n            data = mx.sym.swapaxes(data=data, dim1=0, dim2=1)\n        return data, data_length, seq_len", "response": "Encodes data given sequence lengths of individual examples and maximum sequence length."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _encode(self, data: mx.sym.Symbol, data_length: mx.sym.Symbol, seq_len: int) -> mx.sym.Symbol:\n        # (seq_len, batch_size, num_embed)\n        data_reverse = mx.sym.SequenceReverse(data=data, sequence_length=data_length,\n                                              use_sequence_length=True)\n        # (seq_length, batch, cell_num_hidden)\n        hidden_forward, _, _ = self.forward_rnn.encode(data, data_length, seq_len)\n        # (seq_length, batch, cell_num_hidden)\n        hidden_reverse, _, _ = self.reverse_rnn.encode(data_reverse, data_length, seq_len)\n        # (seq_length, batch, cell_num_hidden)\n        hidden_reverse = mx.sym.SequenceReverse(data=hidden_reverse, sequence_length=data_length,\n                                                use_sequence_length=True)\n        # (seq_length, batch, 2 * cell_num_hidden)\n        hidden_concat = mx.sym.concat(hidden_forward, hidden_reverse, dim=2, name=\"%s_rnn\" % self.prefix)\n\n        return hidden_concat", "response": "Encodes time - major data."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_rnn_cells(self) -> List[mx.rnn.BaseRNNCell]:\n        return self.forward_rnn.get_rnn_cells() + self.reverse_rnn.get_rnn_cells()", "response": "Returns a list of RNNCells used by this encoder."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef encode(self,\n               data: mx.sym.Symbol,\n               data_length: mx.sym.Symbol,\n               seq_len: int) -> Tuple[mx.sym.Symbol, mx.sym.Symbol, int]:\n        \"\"\"\n        Encodes data with a stack of Convolution+GLU blocks given sequence lengths of individual examples\n        and maximum sequence length.\n\n        :param data: Input data. Shape: (batch_size, seq_len, input_num_hidden).\n        :param data_length: Vector with sequence lengths.\n        :param seq_len: Maximum sequence length.\n        :return: Encoded version of the data.\n        \"\"\"\n        # data: (batch_size, seq_len, num_hidden)\n        data = mx.sym.FullyConnected(data=data,\n                                     num_hidden=self.config.cnn_config.num_hidden,\n                                     no_bias=True,\n                                     flatten=False,\n                                     weight=self.i2h_weight)\n\n        # Multiple layers with residual connections:\n        for layer in self.layers:\n            data = data + layer(data, data_length, seq_len)\n        return data, data_length, seq_len", "response": "Encodes data with a stack of Convolution + GLU blocks given sequence lengths of individual examples\n               data_length and maximum sequence length."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef encode(self,\n               data: mx.sym.Symbol,\n               data_length: mx.sym.Symbol,\n               seq_len: int) -> Tuple[mx.sym.Symbol, mx.sym.Symbol, int]:\n        \"\"\"\n        Encodes data given sequence lengths of individual examples and maximum sequence length.\n\n        :param data: Input data.\n        :param data_length: Vector with sequence lengths.\n        :param seq_len: Maximum sequence length.\n        :return: Encoded versions of input data data, data_length, seq_len.\n        \"\"\"\n        data = utils.cast_conditionally(data, self.dtype)\n        if self.config.dropout_prepost > 0.0:\n            data = mx.sym.Dropout(data=data, p=self.config.dropout_prepost)\n\n        # (batch_size * heads, 1, max_length)\n        bias = mx.sym.expand_dims(transformer.get_valid_length_mask_for(data=data,\n                                                                        lengths=data_length,\n                                                                        num_heads=self.config.attention_heads,\n                                                                        fold_heads=True,\n                                                                        name=\"%sbias\" % self.prefix), axis=1)\n        bias = utils.cast_conditionally(bias, self.dtype)\n        for i, layer in enumerate(self.layers):\n            # (batch_size, seq_len, config.model_size)\n            data = layer(data, bias)\n        data = self.final_process(data=data, prev=None)\n        data = utils.uncast_conditionally(data, self.dtype)\n        return data, data_length, seq_len", "response": "Encodes data given sequence lengths of individual examples and maximum sequence length."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef encode(self,\n               data: mx.sym.Symbol,\n               data_length: mx.sym.Symbol,\n               seq_len: int) -> Tuple[mx.sym.Symbol, mx.sym.Symbol, int]:\n        \"\"\"\n        Encodes data given sequence lengths of individual examples and maximum sequence length.\n\n        :param data: Input data.\n        :param data_length: Vector with sequence lengths.\n        :param seq_len: Maximum sequence length.\n        :return: Encoded versions of input data data, data_length, seq_len.\n        \"\"\"\n        total_num_filters = sum(self.num_filters)\n        encoded_seq_len = self.get_encoded_seq_len(seq_len)\n\n        # (batch_size, channel=1, seq_len, num_embed)\n        data = mx.sym.Reshape(data=data, shape=(-1, 1, seq_len, self.num_embed))\n\n        # Convolution filters of width 1..N\n        conv_outputs = []\n        for filter_width, num_filter in enumerate(self.num_filters, 1):\n            # \"half\" padding: output length == input length\n            pad_before = ceil((filter_width - 1) / 2)\n            pad_after = floor((filter_width - 1) / 2)\n            # (batch_size, channel=1, seq_len + (filter_width - 1), num_embed)\n            padded = mx.sym.pad(data=data,\n                                mode=\"constant\",\n                                constant_value=0,\n                                pad_width=(0, 0, 0, 0, pad_before, pad_after, 0, 0))\n            # (batch_size, num_filter, seq_len, num_scores=1)\n            conv = mx.sym.Convolution(data=padded,\n                                      # cudnn_tune=\"off\",\n                                      kernel=(filter_width, self.num_embed),\n                                      num_filter=num_filter,\n                                      weight=self.conv_weight[filter_width],\n                                      bias=self.conv_bias[filter_width])\n            conv = mx.sym.Activation(data=conv, act_type=\"relu\")\n            conv_outputs.append(conv)\n        # (batch_size, total_num_filters, seq_len, num_scores=1)\n        conv_concat = mx.sym.concat(*conv_outputs, dim=1)\n\n        # Max pooling with stride\n        uncovered = seq_len % self.pool_stride\n        if uncovered > 0:\n            pad_after = self.pool_stride - uncovered\n            # (batch_size, total_num_filters, seq_len + pad_to_final_stride, num_scores=1)\n            conv_concat = mx.sym.pad(data=conv_concat,\n                                     mode=\"constant\",\n                                     constant_value=0,\n                                     pad_width=(0, 0, 0, 0, 0, pad_after, 0, 0))\n        # (batch_size, total_num_filters, seq_len/stride, num_scores=1)\n        pool = mx.sym.Pooling(data=conv_concat,\n                              pool_type=\"max\",\n                              kernel=(self.pool_stride, 1),\n                              stride=(self.pool_stride, 1))\n        # (batch_size, total_num_filters, seq_len/stride)\n        pool = mx.sym.reshape(data=pool,\n                              shape=(-1, total_num_filters, encoded_seq_len))\n        # (batch_size, seq_len/stride, total_num_filters)\n        pool = mx.sym.swapaxes(data=pool, dim1=1, dim2=2)\n        if self.dropout > 0:\n            pool = mx.sym.Dropout(data=pool, p=self.dropout)\n\n        # Raw segment embeddings reshaped for highway network\n        # (batch_size * seq_len/stride, total_num_filters)\n        seg_embedding = mx.sym.Reshape(data=pool, shape=(-3, total_num_filters))\n\n        # Projection layer if requested output dimension is different from total number of filters\n        # (TransformerEncoder compatibility, not in original paper)\n        if self.output_dim != total_num_filters:\n            # (batch_size * seq_len/stride, outut_dim)\n            seg_embedding = mx.sym.FullyConnected(data=seg_embedding,\n                                                  num_hidden=self.output_dim,\n                                                  weight=self.project_weight,\n                                                  bias=self.project_bias)\n            seg_embedding = mx.sym.Activation(data=seg_embedding, act_type=\"relu\")\n            if self.dropout > 0:\n                seg_embedding = mx.sym.Dropout(data=seg_embedding, p=self.dropout)\n\n        # Highway network\n        for i in range(self.num_highway_layers):\n            # Gate\n            gate = mx.sym.FullyConnected(data=seg_embedding,\n                                         num_hidden=self.output_dim,\n                                         weight=self.gate_weight[i],\n                                         bias=self.gate_bias[i])\n            gate = mx.sym.Activation(data=gate, act_type=\"sigmoid\")\n            if self.dropout > 0:\n                gate = mx.sym.Dropout(data=gate, p=self.dropout)\n            # Transform\n            transform = mx.sym.FullyConnected(data=seg_embedding,\n                                              num_hidden=self.output_dim,\n                                              weight=self.transform_weight[i],\n                                              bias=self.transform_bias[i])\n            transform = mx.sym.Activation(data=transform, act_type=\"relu\")\n            if self.dropout > 0:\n                transform = mx.sym.Dropout(data=transform, p=self.dropout)\n            # Connection\n            seg_embedding = gate * transform + (1 - gate) * seg_embedding\n        # (batch_size, seq_len/stride, output_dim) aka\n        # (batch_size, encoded_seq_len, num_segment_embed)\n        seg_embedding = mx.sym.Reshape(data=seg_embedding,\n                                       shape=(-1, encoded_seq_len, self.output_dim))\n\n        # Dropout on final segment embeddings\n        if self.dropout > 0:\n            seg_embedding = mx.sym.Dropout(data=seg_embedding, p=self.dropout)\n\n        # Ceiling function isn't differentiable so this will throw errors if we\n        # attempt to compute gradients.  Fortunately we aren't updating inputs\n        # so we can just block the backward pass here.\n        encoded_data_length = mx.sym.BlockGrad(mx.sym.ceil(data_length / self.pool_stride))\n\n        return seg_embedding, encoded_data_length, encoded_seq_len", "response": "Encodes data given sequence lengths of individual examples and maximum sequence length."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the size of the encoded sequence.", "response": "def get_encoded_seq_len(self, seq_len: int) -> int:\n        \"\"\"\n        Returns the size of the encoded sequence.\n        \"\"\"\n        return int(ceil(seq_len / self.pool_stride))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_attention(config: AttentionConfig, max_seq_len: int, prefix: str = C.ATTENTION_PREFIX) -> 'Attention':\n\n    att_cls = Attention.get_attention_cls(config.type)\n    params = config.__dict__.copy()\n    params.pop('_frozen')\n    params['max_seq_len'] = max_seq_len\n    params['prefix'] = prefix\n    return _instantiate(att_cls, params)", "response": "Returns an Attention instance based on attention_type."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_context_and_attention_probs(values: mx.sym.Symbol,\n                                    length: mx.sym.Symbol,\n                                    logits: mx.sym.Symbol,\n                                    dtype: str) -> Tuple[mx.sym.Symbol, mx.sym.Symbol]:\n    \"\"\"\n    Returns context vector and attention probabilities\n    via a weighted sum over values.\n\n    :param values: Shape: (batch_size, seq_len, encoder_num_hidden).\n    :param length: Shape: (batch_size,).\n    :param logits: Shape: (batch_size, seq_len, 1).\n    :param dtype: data type.\n    :return: context: (batch_size, encoder_num_hidden), attention_probs: (batch_size, seq_len).\n    \"\"\"\n    # masks attention scores according to sequence length.\n    # (batch_size, seq_len, 1)\n    logits = mx.sym.SequenceMask(data=logits,\n                                 axis=1,\n                                 use_sequence_length=True,\n                                 sequence_length=length,\n                                 value=-C.LARGE_VALUES[dtype])\n\n    # (batch_size, seq_len, 1)\n    probs = mx.sym.softmax(logits, axis=1, name='attention_softmax')\n\n    # batch_dot: (batch, M, K) X (batch, K, N) \u2013> (batch, M, N).\n    # (batch_size, seq_len, num_hidden) X (batch_size, seq_len, 1) -> (batch_size, num_hidden, 1)\n    context = mx.sym.batch_dot(lhs=values, rhs=probs, transpose_a=True)\n    # (batch_size, encoder_num_hidden, 1)-> (batch_size, encoder_num_hidden)\n    context = mx.sym.reshape(data=context, shape=(0, 0))\n    probs = mx.sym.reshape(data=probs, shape=(0, 0))\n\n    return context, probs", "response": "Returns context vector and attention probabilities for a single encoder."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef on(self, source: mx.sym.Symbol, source_length: mx.sym.Symbol, source_seq_len: int) -> Callable:\n\n        def attend(att_input: AttentionInput, att_state: AttentionState) -> AttentionState:\n            \"\"\"\n            Returns updated attention state given attention input and current attention state.\n\n            :param att_input: Attention input as returned by make_input().\n            :param att_state: Current attention state\n            :return: Updated attention state.\n            \"\"\"\n            raise NotImplementedError()\n\n        return attend", "response": "Returns a callable that can be used to recurrently attention in a sequence decoder."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns initial attention state. Dynamic source encoding is initialized with zeros.", "response": "def get_initial_state(self, source_length: mx.sym.Symbol, source_seq_len: int) -> AttentionState:\n        \"\"\"\n        Returns initial attention state. Dynamic source encoding is initialized with zeros.\n\n        :param source_length: Source length. Shape: (batch_size,).\n        :param source_seq_len: Maximum length of source sequences.\n        \"\"\"\n        dynamic_source = mx.sym.reshape(mx.sym.zeros_like(source_length), shape=(-1, 1, 1))\n        # dynamic_source: (batch_size, source_seq_len, num_hidden_dynamic_source)\n        dynamic_source = mx.sym.broadcast_to(dynamic_source, shape=(0, source_seq_len, self.dynamic_source_num_hidden))\n        return AttentionState(context=None, probs=None, dynamic_source=dynamic_source)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating AttentionInput object for the given sequence index.", "response": "def make_input(self,\n                   seq_idx: int,\n                   word_vec_prev: mx.sym.Symbol,\n                   decoder_state: mx.sym.Symbol) -> AttentionInput:\n        \"\"\"\n        Returns AttentionInput to be fed into the attend callable returned by the on() method.\n\n        :param seq_idx: Decoder time step.\n        :param word_vec_prev: Embedding of previously predicted ord\n        :param decoder_state: Current decoder state\n        :return: Attention input.\n        \"\"\"\n        query = decoder_state\n        if self._input_previous_word:\n            # (batch_size, num_target_embed + rnn_num_hidden)\n            query = mx.sym.concat(word_vec_prev, decoder_state, dim=1,\n                                  name='%sconcat_prev_word_%d' % (self.prefix, seq_idx))\n        return AttentionInput(seq_idx=seq_idx, query=query)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\napply custom or standard activation.", "response": "def activation(data: mx.sym.Symbol, act_type: str) -> mx.sym.Symbol:\n    \"\"\"\n    Apply custom or standard activation.\n\n    Custom activation types include:\n     - Swish-1, also called Sigmoid-Weighted Linear Unit (SiLU): Ramachandran et\n       al. (https://arxiv.org/pdf/1710.05941.pdf), Elfwing et al.\n       (https://arxiv.org/pdf/1702.03118.pdf)\n     - Gaussian Error Linear Unit (GELU): Hendrycks and Gimpel\n       (https://arxiv.org/pdf/1606.08415.pdf)\n\n    :param data: input Symbol of any shape.\n    :param act_type: Type of activation.\n    :return: output Symbol with same shape as input.\n    \"\"\"\n    # TODO: Contribute these to MXNet?  For now it appears that registered activation types must be implemented in C++.\n    if act_type == C.SWISH1:\n        return data * mx.sym.Activation(data, act_type=\"sigmoid\")\n    elif act_type == C.GELU:\n        # Approximation of x * gaussian_cdf(x) used by Hendrycks and Gimpel\n        return 0.5 * data * (1 + mx.sym.Activation((math.sqrt(2 / math.pi) * (data + (0.044715 * (data**3)))),\n                                                   act_type=\"tanh\"))\n    else:\n        return mx.sym.Activation(data, act_type=act_type)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef split_heads(x: mx.sym.Symbol, depth_per_head: int, heads: int) -> mx.sym.Symbol:\n    # (batch, length, heads, depth_per_head)\n    x = mx.sym.reshape(data=x, shape=(0, -1, heads, depth_per_head))\n    # (batch, heads, length, depth/heads)\n    x = mx.sym.transpose(data=x, axes=(0, 2, 1, 3))\n    # (batch * heads, length, depth/heads)\n    return mx.sym.reshape(data=x, shape=(-3, -1, depth_per_head))", "response": "Returns a symbol with head dimension folded into batch and depth divided by the number of heads."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbroadcasting batch - major input of shape num_heads ndim - number of heads ndim - number of dimensions in x.", "response": "def broadcast_to_heads(x: mx.sym.Symbol, num_heads: int, ndim: int, fold_heads: bool = True) -> mx.sym.Symbol:\n    \"\"\"\n    Broadcasts batch-major input of shape (batch, d1 ... dn-1) to (batch*heads, d1 ... dn-1).\n\n    :param x: Batch-major input. Shape: (batch, d1 ... dn-1).\n    :param num_heads: Number of heads.\n    :param ndim: Number of dimensions in x.\n    :param fold_heads: Whether to fold heads dimension into batch dimension.\n    :return: Tensor with each sample repeated heads-many times.\n             Shape: (batch * heads, d1 ... dn-1) if fold_heads == True, (batch, heads, d1 ... dn-1) else.\n    \"\"\"\n    dims = [0] * (ndim - 1)\n    # x: (batch, 1)\n    x = mx.sym.expand_dims(x, axis=1)\n    # x: (batch, heads, dims...)\n    x = mx.sym.broadcast_to(x, shape=[0, num_heads] + dims)\n    if fold_heads:\n        # (batch * heads, dims...)\n        return mx.sym.reshape(x, shape=[-3] + dims)\n    else:\n        # x: (batch, heads, dims...)\n        return x"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dot_attention(queries: mx.sym.Symbol,\n                  keys: mx.sym.Symbol,\n                  values: mx.sym.Symbol,\n                  lengths: Optional[mx.sym.Symbol] = None,\n                  dropout: float = 0.0,\n                  bias: Optional[mx.sym.Symbol] = None,\n                  prefix: Optional[str] = ''):\n    \"\"\"\n    Computes dot attention for a set of queries, keys, and values.\n\n    :param queries: Attention queries. Shape: (n, lq, d).\n    :param keys: Attention keys. Shape: (n, lk, d).\n    :param values: Attention values. Shape: (n, lk, dv).\n    :param lengths: Optional sequence lengths of the keys. Shape: (n,).\n    :param dropout: Dropout probability.\n    :param bias: Optional 3d bias tensor.\n    :param prefix: Optional prefix\n    :return: 'Context' vectors for each query. Shape: (n, lq, dv).\n    \"\"\"\n    utils.check_condition(lengths is not None or bias is not None,\n                          \"Must provide either length or bias argument for masking\")\n\n    # (n, lq, lk)\n    logits = mx.sym.batch_dot(lhs=queries, rhs=keys, transpose_b=True, name='%sdot' % prefix)\n\n    if lengths is not None:\n        # mask lk dimension\n        # (lk, n, lq)\n        logits = mx.sym.transpose(data=logits, axes=(2, 0, 1))\n        logits = mx.sym.SequenceMask(data=logits,\n                                     use_sequence_length=True,\n                                     sequence_length=lengths,\n                                     value=C.LARGE_NEGATIVE_VALUE)\n        # (n, lq, lk)\n        logits = mx.sym.transpose(data=logits, axes=(1, 2, 0))\n\n    if bias is not None:\n        logits = mx.sym.broadcast_add(logits, bias, name='%sbias_add' % prefix)\n\n    probs = mx.sym.softmax(logits, axis=-1)\n    probs = mx.sym.Dropout(probs, p=dropout) if dropout > 0.0 else probs\n\n    # (n, lq, lk) x (n, lk, dv) -> (n, lq, dv)\n    return mx.sym.batch_dot(lhs=probs, rhs=values, name='%scontexts' % prefix)", "response": "Computes dot attention for a set of queries keys and values."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef average_sources(source_encoded: mx.sym.Symbol, source_encoded_length: mx.sym.Symbol) -> mx.nd.NDArray:\n        # source_masked: (n, source_encoded_length, hidden_size)\n        source_masked = mx.sym.SequenceMask(data=source_encoded,\n                                            axis=1,\n                                            sequence_length=source_encoded_length,\n                                            use_sequence_length=True,\n                                            value=0.)\n        # calculate the proper means of encoded sources\n        averaged = mx.sym.broadcast_div(mx.sym.sum(source_masked, axis=1, keepdims=False),\n                                                   mx.sym.reshape(source_encoded_length, shape=(-1, 1)))\n        return averaged", "response": "Calculate the average of encoded sources taking into account their lengths."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _attend(self,\n                queries: mx.sym.Symbol,\n                keys: mx.sym.Symbol,\n                values: mx.sym.Symbol,\n                lengths: Optional[mx.sym.Symbol] = None,\n                bias: Optional[mx.sym.Symbol] = None) -> mx.sym.Symbol:\n        \"\"\"\n        Returns context vectors of multi-head dot attention.\n\n        :param queries: Query tensor. Shape: (batch_size, query_max_length, depth).\n        :param keys: Keys. Shape: (batch_size, memory_max_length, depth).\n        :param values: Values. Shape: (batch_size, memory_max_length, depth).\n        :param lengths: Optional lengths of keys. Shape: (batch_size,).\n        :param bias: Optional 3d bias.\n        :return: Context vectors. Shape: (batch_size, query_max_length, output_depth).\n        \"\"\"\n        # scale by sqrt(depth_per_head)\n        queries = queries * (self.depth_per_head ** -0.5)\n\n        # (batch*heads, length, depth/heads)\n        queries = split_heads(queries, self.depth_per_head, self.heads)\n        keys = split_heads(keys, self.depth_per_head, self.heads)\n        values = split_heads(values, self.depth_per_head, self.heads)\n        lengths = broadcast_to_heads(lengths, self.heads, ndim=1, fold_heads=True) if lengths is not None else lengths\n\n        # (batch*heads, query_max_length, depth_per_head)\n        contexts = dot_attention(queries, keys, values,\n                                 lengths=lengths, dropout=self.dropout, bias=bias, prefix=self.prefix)\n\n        # (batch, query_max_length, depth)\n        contexts = combine_heads(contexts, self.depth_per_head, self.heads)\n\n        # contexts: (batch, query_max_length, output_depth)\n        contexts = mx.sym.FullyConnected(data=contexts,\n                                         weight=self.w_h2o,\n                                         no_bias=True,\n                                         num_hidden=self.depth_out,\n                                         flatten=False)\n\n        return contexts", "response": "Multi - head dot attention."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef main():\n    log.setup_main_logger(console=True, file_logging=False)\n    log.log_sockeye_version(logger)\n\n    params = argparse.ArgumentParser(description=\"Rerank nbest lists of translations.\"\n                                                 \" Reranking sorts a list of hypotheses according\"\n                                                 \" to their score compared to a common reference.\")\n    arguments.add_rerank_args(params)\n    args = params.parse_args()\n\n    logger.info(args)\n\n    rerank(args)", "response": "Commandline interface to rerank nbest lists."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_loss(config: LossConfig) -> 'Loss':\n    if config.name == C.CROSS_ENTROPY:\n        return CrossEntropyLoss(config,\n                                output_names=[C.SOFTMAX_OUTPUT_NAME],\n                                label_names=[C.TARGET_LABEL_NAME])\n    else:\n        raise ValueError(\"unknown loss name: %s\" % config.name)", "response": "Returns a Loss instance implementing the Loss."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a Loss instance implementing the length task.", "response": "def get_length_task_loss(config: LossConfig) -> 'Loss':\n    \"\"\"\n    Returns a Loss instance.\n\n    :param config: Loss configuration.\n    :return: Instance implementing Loss.\n    \"\"\"\n    if config.length_task_link is not None:\n        if config.length_task_link == C.LINK_NORMAL:\n            return MSELoss(config,\n                           output_names=[C.LENRATIO_OUTPUT_NAME],\n                           label_names=[C.LENRATIO_LABEL_NAME])\n        elif config.length_task_link == C.LINK_POISSON:\n            return PoissonLoss(config,\n                               output_names=[C.LENRATIO_OUTPUT_NAME],\n                               label_names=[C.LENRATIO_LABEL_NAME])\n        else:\n            raise ValueError(\"unknown link function name for length task: %s\" % config.length_task_link)\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn loss and softmax output symbols given logits and integer - coded labels.", "response": "def get_loss(self, logits: mx.sym.Symbol, labels: mx.sym.Symbol) -> mx.sym.Symbol:\n        \"\"\"\n        Returns loss and softmax output symbols given logits and integer-coded labels.\n\n        :param logits: Shape: (batch_size * target_seq_len, target_vocab_size).\n        :param labels: Shape: (batch_size * target_seq_len,).\n        :return: Loss symbol.\n        \"\"\"\n        raise NotImplementedError()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn loss symbol given logits and integer - coded labels.", "response": "def get_loss(self, logits: mx.sym.Symbol, labels: mx.sym.Symbol) -> mx.sym.Symbol:\n        \"\"\"\n        Returns loss symbol given logits and integer-coded labels.\n\n        :param logits: Shape: (batch_size * target_seq_len, target_vocab_size).\n        :param labels: Shape: (batch_size * target_seq_len,).\n        :return: List of loss symbols.\n        \"\"\"\n        if self.loss_config.normalization_type == C.LOSS_NORM_VALID:\n            normalization = \"valid\"\n        elif self.loss_config.normalization_type == C.LOSS_NORM_BATCH:\n            normalization = \"null\"\n        else:\n            raise ValueError(\"Unknown loss normalization type: %s\" % self.loss_config.normalization_type)\n        return mx.sym.SoftmaxOutput(data=logits,\n                                    label=labels,\n                                    ignore_label=self.ignore_label,\n                                    use_ignore=True,\n                                    normalization=normalization,\n                                    smooth_alpha=self.loss_config.label_smoothing,\n                                    name=self.name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_loss(self, pred: mx.sym.Symbol, labels: mx.sym.Symbol) -> mx.sym.Symbol:\n        labels = mx.sym.reshape(labels, shape=(-1, 1))\n        loss_value = self.loss_config.length_task_weight / 2 * mx.sym.square(pred - labels)\n        loss_value = mx.sym.MakeLoss(data=loss_value,\n                                     normalization='batch',\n                                     name=self.name)\n        return loss_value", "response": "Returns MSE loss and output symbol given logits and expected integers as labels."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate the internal state of the logistic distribution.", "response": "def update(self, labels, preds):\n        \"\"\"\n        :param labels: List of (batch_size,)-shaped NDArrays.\n        :param preds: List of (batch_size,1)-shaped NDArrays.\n        \"\"\"\n        for label, pred in zip(labels, preds):\n            batch_size = label.shape[0]\n            # label: (batch_size, 1)\n            label = label.as_in_context(pred.context).reshape((label.size,1))\n            # mse: (batch_size,)\n            mse = mx.nd.square(label - pred)\n            # mse: (1,)\n            mse = mx.nd.sum(mse)\n            self.num_inst += batch_size\n\n            self.sum_metric += mse.asscalar()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update_dict(self, label: Dict, pred: Dict):\n        if not set(self.label_names).issubset(set(label.keys())):\n            label.update({name:pred[name] for name in self.label_names})\n        super().update_dict(label, pred)", "response": "Update the label with the prediction."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking given version against code version and determines compatibility.", "response": "def check_version(version: str):\n    \"\"\"\n    Checks given version against code version and determines compatibility.\n    Throws if versions are incompatible.\n\n    :param version: Given version.\n    \"\"\"\n    code_version = parse_version(__version__)\n    given_version = parse_version(version)\n    check_condition(code_version[0] == given_version[0],\n                    \"Given release version (%s) does not match release code version (%s)\" % (version, __version__))\n    check_condition(code_version[1] == given_version[1],\n                    \"Given major version (%s) does not match major code version (%s)\" % (version, __version__))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load_version(fname: str) -> str:\n    if not os.path.exists(fname):\n        logger.warning(\"No version file found. Defaulting to 1.0.3\")\n        return \"1.0.3\"\n    with open(fname) as inp:\n        return inp.read().strip()", "response": "Loads version from file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_version(version_string: str) -> Tuple[str, str, str]:\n    release, major, minor = version_string.split(\".\", 2)\n    return release, major, minor", "response": "Parse version string into release major minor."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef log_basic_info(args) -> None:\n    log_sockeye_version(logger)\n    log_mxnet_version(logger)\n    logger.info(\"Command: %s\", \" \".join(sys.argv))\n    logger.info(\"Arguments: %s\", args)", "response": "Log basic information like version number arguments etc."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef seed_rngs(seed: int) -> None:\n    np.random.seed(seed)\n    random.seed(seed)\n    mx.random.seed(seed)", "response": "Seed the random number generators."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndump the computation graph to. pdf and. dot file.", "response": "def save_graph(symbol: mx.sym.Symbol, filename: str, hide_weights: bool = True):\n    \"\"\"\n    Dumps computation graph visualization to .pdf and .dot file.\n\n    :param symbol: The symbol representing the computation graph.\n    :param filename: The filename to save the graphic to.\n    :param hide_weights: If true the weights will not be shown.\n    \"\"\"\n    dot = mx.viz.plot_network(symbol, hide_weights=hide_weights)\n    dot.render(filename=filename)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes sequence lengths of PAD_ID - padded data in sequence_data.", "response": "def compute_lengths(sequence_data: mx.sym.Symbol) -> mx.sym.Symbol:\n    \"\"\"\n    Computes sequence lengths of PAD_ID-padded data in sequence_data.\n\n    :param sequence_data: Input data. Shape: (batch_size, seq_len).\n    :return: Length data. Shape: (batch_size,).\n    \"\"\"\n    return mx.sym.sum(sequence_data != C.PAD_ID, axis=1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef save_params(arg_params: Mapping[str, mx.nd.NDArray], fname: str,\n                aux_params: Optional[Mapping[str, mx.nd.NDArray]] = None):\n    \"\"\"\n    Saves the parameters to a file.\n\n    :param arg_params: Mapping from parameter names to the actual parameters.\n    :param fname: The file name to store the parameters in.\n    :param aux_params: Optional mapping from parameter names to the auxiliary parameters.\n    \"\"\"\n    save_dict = {('arg:%s' % k): v.as_in_context(mx.cpu()) for k, v in arg_params.items()}\n    if aux_params is not None:\n        save_dict.update({('aux:%s' % k): v.as_in_context(mx.cpu()) for k, v in aux_params.items()})\n    mx.nd.save(fname, save_dict)", "response": "Saves the parameters to a file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload the parameters from a file.", "response": "def load_params(fname: str) -> Tuple[Dict[str, mx.nd.NDArray], Dict[str, mx.nd.NDArray]]:\n    \"\"\"\n    Loads parameters from a file.\n\n    :param fname: The file containing the parameters.\n    :return: Mapping from parameter names to the actual parameters for both the arg parameters and the aux parameters.\n    \"\"\"\n    save_dict = mx.nd.load(fname)\n    arg_params = {}\n    aux_params = {}\n    for k, v in save_dict.items():\n        tp, name = k.split(':', 1)\n        if tp == 'arg':\n            \"\"\"TODO(fhieber):\n            temporary weight split for models with combined weight for keys & values\n            in transformer source attention layers. This can be removed once with the next major version change.\"\"\"\n            if \"att_enc_kv2h_weight\" in name:\n                logger.info(\"Splitting '%s' parameters into separate k & v matrices.\", name)\n                v_split = mx.nd.split(v, axis=0, num_outputs=2)\n                arg_params[name.replace('kv2h', \"k2h\")] = v_split[0]\n                arg_params[name.replace('kv2h', \"v2h\")] = v_split[1]\n            else:\n                arg_params[name] = v\n        if tp == 'aux':\n            aux_params[name] = v\n    return arg_params, aux_params"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef top1(scores: mx.nd.NDArray,\n         offset: mx.nd.NDArray) -> Tuple[mx.nd.NDArray, mx.nd.NDArray, mx.nd.NDArray]:\n    \"\"\"\n    Get the single lowest element per sentence from a `scores` matrix. Expects that\n    beam size is 1, for greedy decoding.\n\n    NOTE(mathmu): The current implementation of argmin in MXNet much slower than topk with k=1.\n\n    :param scores: Vocabulary scores for the next beam step. (batch_size * beam_size, target_vocabulary_size)\n    :param offset: Array to add to the hypothesis indices for offsetting in batch decoding.\n    :return: The row indices, column indices and values of the smallest items in matrix.\n    \"\"\"\n    best_word_indices = mx.nd.cast(mx.nd.argmin(scores, axis=1), dtype='int32')\n    values = scores[mx.nd.arange(scores.shape[0], dtype='int32', ctx=scores.context), best_word_indices]\n\n    values = values.reshape((-1, 1))\n\n    # for top1, the best hyp indices are equal to the plain offset\n\n    return offset, best_word_indices, values", "response": "Get the smallest item per sentence from a scores matrix."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the k smallest items per sentence from a scores matrix.", "response": "def topk(scores: mx.nd.NDArray,\n         offset: mx.nd.NDArray,\n         k: int) -> Tuple[mx.nd.NDArray, mx.nd.NDArray, mx.nd.NDArray]:\n    \"\"\"\n    Get the lowest k elements per sentence from a `scores` matrix.\n    At the first timestep, the shape of scores is (batch, target_vocabulary_size).\n    At subsequent steps, the shape is (batch * k, target_vocabulary_size).\n\n    :param scores: Vocabulary scores for the next beam step. (batch_size * beam_size, target_vocabulary_size)\n    :param offset: Array (shape: batch_size * k) containing offsets to add to the hypothesis indices in batch decoding.\n    :param k: The number of smallest scores to return.\n    :return: The row indices, column indices and values of the k smallest items in matrix.\n    \"\"\"\n\n    # Compute the batch size from the offsets and k. We don't know the batch size because it is\n    # either 1 (at timestep 1) or k (at timesteps 2+).\n    # (batch_size, beam_size * target_vocab_size)\n    batch_size = int(offset.shape[-1] / k)\n    folded_scores = scores.reshape((batch_size, -1))\n\n    # pylint: disable=unbalanced-tuple-unpacking\n    values, indices = mx.nd.topk(folded_scores, axis=1, k=k, ret_typ='both', is_ascend=True)\n    indices = mx.nd.cast(indices, 'int32').reshape((-1,))\n    best_hyp_indices, best_word_indices = mx.nd.unravel_index(indices, shape=(batch_size * k, scores.shape[-1]))\n\n    if batch_size > 1:\n        # Offsetting the indices to match the shape of the scores matrix\n        best_hyp_indices += offset\n\n    values = values.reshape((-1, 1))\n    return best_hyp_indices, best_word_indices, values"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nyields successive n - sized chunks from l.", "response": "def chunks(some_list: List, n: int) -> Iterable[List]:\n    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n    for i in range(0, len(some_list), n):\n        yield some_list[i:i + n]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_tokens(line: str) -> Iterator[str]:\n    for token in line.rstrip().split():\n        if len(token) > 0:\n            yield token", "response": "Yields tokens from input string."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nopening a file with UTF - 8 encoding.", "response": "def smart_open(filename: str, mode: str = \"rt\", ftype: str = \"auto\", errors: str = 'replace'):\n    \"\"\"\n    Returns a file descriptor for filename with UTF-8 encoding.\n    If mode is \"rt\", file is opened read-only.\n    If ftype is \"auto\", uses gzip iff filename endswith .gz.\n    If ftype is {\"gzip\",\"gz\"}, uses gzip.\n    If ftype is \"auto\" and read mode requested, uses gzip iff is_gzip_file(filename).\n\n    Note: encoding error handling defaults to \"replace\"\n\n    :param filename: The filename to open.\n    :param mode: Reader mode.\n    :param ftype: File type. If 'auto' checks filename suffix for gz to try gzip.open.\n    :param errors: Encoding error handling during reading. Defaults to 'replace'.\n    :return: File descriptor.\n    \"\"\"\n    if ftype in ('gzip', 'gz') \\\n            or (ftype == 'auto' and filename.endswith(\".gz\")) \\\n            or (ftype == 'auto' and 'r' in mode and is_gzip_file(filename)):\n        return gzip.open(filename, mode=mode, encoding='utf-8', errors=errors)\n    else:\n        return open(filename, mode=mode, encoding='utf-8', errors=errors)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nplot the attention matrix.", "response": "def plot_attention(attention_matrix: np.ndarray, source_tokens: List[str], target_tokens: List[str], filename: str):\n    \"\"\"\n    Uses matplotlib for creating a visualization of the attention matrix.\n\n    :param attention_matrix: The attention matrix.\n    :param source_tokens: A list of source tokens.\n    :param target_tokens: A list of target tokens.\n    :param filename: The file to which the attention visualization will be written to.\n    \"\"\"\n    try:\n        import matplotlib\n    except ImportError:\n        raise RuntimeError(\"Please install matplotlib.\")\n    matplotlib.use(\"Agg\")\n    import matplotlib.pyplot as plt\n    assert attention_matrix.shape[0] == len(target_tokens)\n\n    plt.imshow(attention_matrix.transpose(), interpolation=\"nearest\", cmap=\"Greys\")\n    plt.xlabel(\"target\")\n    plt.ylabel(\"source\")\n    plt.gca().set_xticks([i for i in range(0, len(target_tokens))])\n    plt.gca().set_yticks([i for i in range(0, len(source_tokens))])\n    plt.gca().set_xticklabels(target_tokens, rotation='vertical')\n    plt.gca().set_yticklabels(source_tokens)\n    plt.tight_layout()\n    plt.savefig(filename)\n    logger.info(\"Saved alignment visualization to \" + filename)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef print_attention_text(attention_matrix: np.ndarray, source_tokens: List[str], target_tokens: List[str],\n                         threshold: float):\n    \"\"\"\n    Prints the attention matrix to standard out.\n\n    :param attention_matrix: The attention matrix.\n    :param source_tokens: A list of source tokens.\n    :param target_tokens: A list of target tokens.\n    :param threshold: The threshold for including an alignment link in the result.\n    \"\"\"\n    sys.stdout.write(\"  \")\n    for _ in target_tokens:\n        sys.stdout.write(\"---\")\n    sys.stdout.write(\"\\n\")\n    for i, f_i in enumerate(source_tokens):  # type: ignore\n        sys.stdout.write(\" |\")\n        for j in range(len(target_tokens)):\n            align_prob = attention_matrix[j, i]\n            if align_prob > threshold:\n                sys.stdout.write(\"(*)\")\n            elif align_prob > 0.4:\n                sys.stdout.write(\"(?)\")\n            else:\n                sys.stdout.write(\"   \")\n        sys.stdout.write(\" | %s\\n\" % f_i)\n    sys.stdout.write(\"  \")\n    for _ in target_tokens:\n        sys.stdout.write(\"---\")\n    sys.stdout.write(\"\\n\")\n    for k in range(max(map(len, target_tokens))):\n        sys.stdout.write(\"  \")\n        for word in target_tokens:\n            letter = word[k] if len(word) > k else \" \"\n            sys.stdout.write(\" %s \" % letter)\n        sys.stdout.write(\"\\n\")\n    sys.stdout.write(\"\\n\")", "response": "Prints the attention matrix to standard out."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nyielding hard alignments from an attention matrix.", "response": "def get_alignments(attention_matrix: np.ndarray, threshold: float = .9) -> Iterator[Tuple[int, int]]:\n    \"\"\"\n    Yields hard alignments from an attention_matrix (target_length, source_length)\n    given a threshold.\n\n    :param attention_matrix: The attention matrix.\n    :param threshold: The threshold for including an alignment link in the result.\n    :return: Generator yielding strings of the form 0-0, 0-1, 2-1, 2-2, 3-4...\n    \"\"\"\n    for src_idx in range(attention_matrix.shape[1]):\n        for trg_idx in range(attention_matrix.shape[0]):\n            if attention_matrix[trg_idx, src_idx] > threshold:\n                yield (src_idx, trg_idx)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntake a list of arrays of the same shape and take the element wise average.", "response": "def average_arrays(arrays: List[mx.nd.NDArray]) -> mx.nd.NDArray:\n    \"\"\"\n    Take a list of arrays of the same shape and take the element wise average.\n\n    :param arrays: A list of NDArrays with the same shape that will be averaged.\n    :return: The average of the NDArrays in the same context as arrays[0].\n    \"\"\"\n    if not arrays:\n        raise ValueError(\"arrays is empty.\")\n    if len(arrays) == 1:\n        return arrays[0]\n    check_condition(all(arrays[0].shape == a.shape for a in arrays), \"nd array shapes do not match\")\n    return mx.nd.add_n(*arrays) / len(arrays)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef query_nvidia_smi(device_ids: List[int], result_queue: multiprocessing.Queue) -> None:\n    device_id_strs = [str(device_id) for device_id in device_ids]\n    query = \"--query-gpu=index,memory.used,memory.total\"\n    format_arg = \"--format=csv,noheader,nounits\"\n    try:\n        sp = subprocess.Popen(['nvidia-smi', query, format_arg, \"-i\", \",\".join(device_id_strs)],\n                              stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        result = sp.communicate()[0].decode(\"utf-8\").rstrip().split(\"\\n\")\n    except OSError:\n        logger.exception(\"Failed calling nvidia-smi to query memory usage.\")\n        result_queue.put({})\n        return\n    try:\n        memory_data = {}\n        for line in result:\n            gpu_id, mem_used, mem_total = line.split(\",\")\n            memory_data[int(gpu_id)] = (int(mem_used), int(mem_total))\n\n        result_queue.put(memory_data)\n    except:\n        logger.exception(\"Failed parsing nvidia-smi output %s\", \"\\n\".join(result))\n        result_queue.put({})", "response": "Runs nvidia - smi to determine the memory usage."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_gpu_memory_usage(ctx: List[mx.context.Context]) -> Dict[int, Tuple[int, int]]:\n    if isinstance(ctx, mx.context.Context):\n        ctx = [ctx]\n    ctx = [c for c in ctx if c.device_type == 'gpu']\n    if not ctx:\n        return {}\n    if shutil.which(\"nvidia-smi\") is None:\n        logger.warning(\"Couldn't find nvidia-smi, therefore we assume no GPUs are available.\")\n        return {}\n\n    device_ids = [c.device_id for c in ctx]\n\n    # Run from clean forkserver process to not leak any CUDA resources\n\n    mp_context = mp_utils.get_context()\n    result_queue = mp_context.Queue()\n    nvidia_smi_process = mp_context.Process(target=query_nvidia_smi, args=(device_ids, result_queue,))\n    nvidia_smi_process.start()\n    nvidia_smi_process.join()\n\n    memory_data = result_queue.get()\n\n    log_gpu_memory_usage(memory_data)\n\n    return memory_data", "response": "Get the memory usage for the given GPU contexts."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef determine_context(device_ids: List[int],\n                      use_cpu: bool,\n                      disable_device_locking: bool,\n                      lock_dir: str,\n                      exit_stack: ExitStack) -> List[mx.Context]:\n    \"\"\"\n    Determine the MXNet context to run on (CPU or GPU).\n\n    :param device_ids: List of device as defined from the CLI.\n    :param use_cpu: Whether to use the CPU instead of GPU(s).\n    :param disable_device_locking: Disable Sockeye's device locking feature.\n    :param lock_dir: Directory to place device lock files in.\n    :param exit_stack: An ExitStack from contextlib.\n    :return: A list with the context(s) to run on.\n    \"\"\"\n    if use_cpu:\n        context = [mx.cpu()]\n    else:\n        num_gpus = get_num_gpus()\n        check_condition(num_gpus >= 1,\n                        \"No GPUs found, consider running on the CPU with --use-cpu \")\n        if disable_device_locking:\n            context = expand_requested_device_ids(device_ids)\n        else:\n            context = exit_stack.enter_context(acquire_gpus(device_ids, lock_dir=lock_dir))\n        context = [mx.gpu(gpu_id) for gpu_id in context]\n    return context", "response": "Determines the MXNet context to run on a list of GPUs."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntransforms a list of device ids requested by the Sockeye GPU to concrete device ids.", "response": "def expand_requested_device_ids(requested_device_ids: List[int]) -> List[int]:\n    \"\"\"\n    Transform a list of device id requests to concrete device ids. For example on a host with 8 GPUs when requesting\n    [-4, 3, 5] you will get [0, 1, 2, 3, 4, 5]. Namely you will get device 3 and 5, as well as 3 other available\n    device ids (starting to fill up from low to high device ids).\n\n    :param requested_device_ids: The requested device ids, each number is either negative indicating the number of GPUs\n     that will be allocated, or positive indicating we want to acquire a specific device id.\n    :return: A list of device ids.\n    \"\"\"\n    num_gpus_available = get_num_gpus()\n    if \"CUDA_VISIBLE_DEVICES\" in os.environ:\n        logger.warning(\"Sockeye currently does not respect CUDA_VISIBLE_DEVICE settings when locking GPU devices.\")\n    return _expand_requested_device_ids(requested_device_ids, num_gpus_available)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef acquire_gpus(requested_device_ids: List[int], lock_dir: str = \"/tmp\",\n                 retry_wait_min: int = 10, retry_wait_rand: int = 60,\n                 num_gpus_available: Optional[int] = None):\n    \"\"\"\n    Acquire a number of GPUs in a transactional way. This method should be used inside a `with` statement.\n    Will try to acquire all the requested number of GPUs. If currently\n    not enough GPUs are available all locks will be released and we wait until we retry. Will retry until enough\n    GPUs become available.\n\n    :param requested_device_ids: The requested device ids, each number is either negative indicating the number of GPUs\n     that will be allocated, or positive indicating we want to acquire a specific device id.\n    :param lock_dir: The directory for storing the lock file.\n    :param retry_wait_min: The minimum number of seconds to wait between retries.\n    :param retry_wait_rand: Randomly add between 0 and `retry_wait_rand` seconds to the wait time.\n    :param num_gpus_available: The number of GPUs available, if None we will call get_num_gpus().\n    :return: yields a list of GPU ids.\n    \"\"\"\n    if num_gpus_available is None:\n        num_gpus_available = get_num_gpus()\n    if num_gpus_available == 0:\n        raise RuntimeError(\"Can not acquire GPU, as no GPUs were found on this machine.\")\n\n    if not os.path.exists(lock_dir):\n        raise IOError(\"Lock directory %s does not exist.\" % lock_dir)\n\n    if not os.access(lock_dir, os.W_OK):\n        raise IOError(\"Lock directory %s is not writeable.\" % lock_dir)\n\n    # split the device ids into the specific ids requested and count up the number of arbitrary ids we want\n    # e.g. device_ids = [-3, 2, 5, 7, -5] means we want to acquire device 2, 5 and 7 plus 8 other devices.\n    specific_device_ids = set()  # type: Set[int]\n    num_arbitrary_device_ids = 0\n    for device_id in requested_device_ids:\n        if device_id < 0:\n            num_gpus = -device_id\n            num_arbitrary_device_ids += num_gpus\n        else:\n            if device_id in specific_device_ids:\n                raise ValueError(\"Requested GPU %d twice.\" % device_id)\n            specific_device_ids.add(device_id)\n\n    # make sure we have enough GPUs available\n    num_gpus_requested = len(specific_device_ids) + num_arbitrary_device_ids\n    if num_gpus_requested > num_gpus_available:\n        raise ValueError(\"Requested %d GPUs, but only %d are available.\" % (num_gpus_requested, num_gpus_available))\n    logger.info(\"Attempting to acquire %d GPUs of %d GPUs. The requested devices are: %s\",\n                num_gpus_requested, num_gpus_available, str(requested_device_ids))\n\n    # note: it's important to first allocate the specific device ids and then the others to not deadlock ourselves.\n\n    # for specific device ids we just have the device id itself as a candidate\n    candidates_to_request = [[device_id] for device_id in specific_device_ids]\n\n    # for the arbitrary device ids we take all remaining device ids as a list of candidates\n    remaining_device_ids = [device_id for device_id in range(num_gpus_available)\n                            if device_id not in specific_device_ids]\n    candidates_to_request += [remaining_device_ids for _ in range(num_arbitrary_device_ids)]\n\n    while True:\n\n        with ExitStack() as exit_stack:\n            any_failed = False\n            acquired_gpus = []  # type: List[int]\n            with GpuFileLock(candidates=[\"master_lock\"], lock_dir=lock_dir) as master_lock:  # type: str\n                # Only one process, determined by the master lock, can try acquiring gpu locks at a time.\n                # This will make sure that we use consecutive device ids whenever possible.\n                if master_lock is not None:\n                    for candidates in candidates_to_request:\n                        gpu_id = exit_stack.enter_context(GpuFileLock(candidates=candidates, lock_dir=lock_dir))\n                        if gpu_id is not None:\n                            acquired_gpus.append(cast(int, gpu_id))\n                        else:\n                            if len(candidates) == 1:\n                                logger.info(\"Could not acquire GPU %d. It's currently locked.\", candidates[0])\n                            any_failed = True\n                            break\n            if master_lock is not None and not any_failed:\n                try:\n                    yield acquired_gpus\n                except:  # pylint: disable=try-except-raise\n                    raise\n                return\n\n        # randomize so that multiple processes starting at the same time don't retry at a similar point in time\n        if retry_wait_rand > 0:\n            retry_wait_actual = retry_wait_min + random.randint(0, retry_wait_rand)\n        else:\n            retry_wait_actual = retry_wait_min\n\n        if master_lock is None:\n            logger.info(\"Another process is acquiring GPUs at the moment will try again in %ss.\" % retry_wait_actual)\n        else:\n            logger.info(\"Not enough GPUs available will try again in %ss.\" % retry_wait_actual)\n        time.sleep(retry_wait_actual)", "response": "Acquire a number of GPUs in a transactional way."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_metrics_line(line_number: int, line: str) -> Dict[str, Any]:\n    fields = line.split('\\t')\n    checkpoint = int(fields[0])\n    check_condition(line_number == checkpoint,\n                    \"Line (%d) and loaded checkpoint (%d) do not align.\" % (line_number, checkpoint))\n    metric = dict()  # type: Dict[str, Any]\n    for field in fields[1:]:\n        key, value = field.split(\"=\", 1)\n        if value == 'True' or value == 'False':\n            metric[key] = (value == 'True')\n        else:\n            metric[key] = float(value)\n    return metric", "response": "Parses a line of Sockeye metrics file into a dictionary of key and value pairs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef read_metrics_file(path: str) -> List[Dict[str, Any]]:\n    with open(path) as fin:\n        metrics = [parse_metrics_line(i, line.strip()) for i, line in enumerate(fin, 1)]\n    return metrics", "response": "Reads lines metrics file and returns list of mappings of key and values."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrite metrics data to a tab - separated file.", "response": "def write_metrics_file(metrics: List[Dict[str, Any]], path: str):\n    \"\"\"\n    Write metrics data to tab-separated file.\n\n    :param metrics: metrics data.\n    :param path: Path to write to.\n    \"\"\"\n    with open(path, 'w') as metrics_out:\n        for checkpoint, metric_dict in enumerate(metrics, 1):\n            metrics_str = \"\\t\".join([\"{}={}\".format(name, value) for name, value in sorted(metric_dict.items())])\n            metrics_out.write(\"{}\\t{}\\n\".format(checkpoint, metrics_str))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_validation_metric_points(model_path: str, metric: str):\n    metrics_path = os.path.join(model_path, C.METRICS_NAME)\n    data = read_metrics_file(metrics_path)\n    return [(d['%s-val' % metric], cp) for cp, d in enumerate(data, 1)]", "response": "Returns a list of tuples of value checkpoint for given metric from metrics file at model_path."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nyields data into fixed - length chunks or underfilled chunks without discarding underfilled chunks.", "response": "def grouper(iterable: Iterable, size: int) -> Iterable:\n    \"\"\"\n    Collect data into fixed-length chunks or blocks without discarding underfilled chunks or padding them.\n\n    :param iterable: A sequence of inputs.\n    :param size: Chunk size.\n    :return: Sequence of chunks.\n    \"\"\"\n    it = iter(iterable)\n    while True:\n        chunk = list(itertools.islice(it, size))\n        if not chunk:\n            return\n        yield chunk"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn true if new value is strictly better than old for given metric.", "response": "def metric_value_is_better(new: float, old: float, metric: str) -> bool:\n    \"\"\"\n    Returns true if new value is strictly better than old for given metric.\n    \"\"\"\n    if C.METRIC_MAXIMIZE[metric]:\n        return new > old\n    else:\n        return new < old"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndelete old parameter files from a model folder.", "response": "def cleanup_params_files(output_folder: str, max_to_keep: int, checkpoint: int, best_checkpoint: int, keep_first: bool):\n    \"\"\"\n    Deletes oldest parameter files from a model folder.\n\n    :param output_folder: Folder where param files are located.\n    :param max_to_keep: Maximum number of files to keep, negative to keep all.\n    :param checkpoint: Current checkpoint (i.e. index of last params file created).\n    :param best_checkpoint: Best checkpoint. The parameter file corresponding to this checkpoint will not be deleted.\n    :param keep_first: Don't delete the first checkpoint.\n    \"\"\"\n    if max_to_keep <= 0:\n        return\n    existing_files = glob.glob(os.path.join(output_folder, C.PARAMS_PREFIX + \"*\"))\n    params_name_with_dir = os.path.join(output_folder, C.PARAMS_NAME)\n    for n in range(1 if keep_first else 0, max(1, checkpoint - max_to_keep + 1)):\n        if n != best_checkpoint:\n            param_fname_n = params_name_with_dir % n\n            if param_fname_n in existing_files:\n                os.remove(param_fname_n)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncasting symbol to target dtype", "response": "def cast_conditionally(data: mx.sym.Symbol, dtype: str) -> mx.sym.Symbol:\n    \"\"\"\n    Workaround until no-op cast will be fixed in MXNet codebase.\n    Creates cast symbol only if dtype is different from default one, i.e. float32.\n\n    :param data: Input symbol.\n    :param dtype: Target dtype.\n    :return: Cast symbol or just data symbol.\n    \"\"\"\n    if dtype != C.DTYPE_FP32:\n        return mx.sym.cast(data=data, dtype=dtype)\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef split(data: mx.nd.NDArray,\n          num_outputs: int,\n          axis: int = 1,\n          squeeze_axis: bool = False) -> List[mx.nd.NDArray]:\n    \"\"\"\n    Version of mxnet.ndarray.split that always returns a list.  The original\n    implementation only returns a list if num_outputs > 1:\n    https://mxnet.incubator.apache.org/api/python/ndarray/ndarray.html#mxnet.ndarray.split\n\n    Splits an array along a particular axis into multiple sub-arrays.\n\n    :param data: The input.\n    :param num_outputs: Number of splits. Note that this should evenly divide\n                        the length of the axis.\n    :param axis: Axis along which to split.\n    :param squeeze_axis: If true, Removes the axis with length 1 from the shapes\n                         of the output arrays.\n    :return: List of NDArrays resulting from the split.\n    \"\"\"\n    ndarray_or_list = data.split(num_outputs=num_outputs, axis=axis, squeeze_axis=squeeze_axis)\n    if num_outputs == 1:\n        return [ndarray_or_list]\n    return ndarray_or_list", "response": "Split an array along a particular axis into multiple sub - arrays."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef inflect(word: str,\n            count: int):\n    \"\"\"\n    Minimal inflection module.\n\n    :param word: The word to inflect.\n    :param count: The count.\n    :return: The word, perhaps inflected for number.\n    \"\"\"\n    if word in ['time', 'sentence']:\n        return word if count == 1 else word + 's'\n    elif word == 'was':\n        return 'was' if count == 1 else 'were'\n    else:\n        return word + '(s)'", "response": "Inflect a word in the n - grams."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nperform an element - wise check to determine if the NDArray contains an infinite element or not.", "response": "def isfinite(data: mx.nd.NDArray) -> mx.nd.NDArray:\n    \"\"\"Performs an element-wise check to determine if the NDArray contains an infinite element or not.\n       TODO: remove this funciton after upgrade to MXNet 1.4.* in favor of mx.ndarray.contrib.isfinite()\n    \"\"\"\n    is_data_not_nan = data == data\n    is_data_not_infinite = data.abs() != np.inf\n    return mx.nd.logical_and(is_data_not_infinite, is_data_not_nan)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_coverage(config: CoverageConfig) -> 'Coverage':\n    if config.type == C.COVERAGE_COUNT or config.type == C.COVERAGE_FERTILITY:\n        utils.check_condition(config.num_hidden == 1, \"Count or fertility coverage requires coverage_num_hidden==1\")\n    if config.type == C.GRU_TYPE:\n        return GRUCoverage(config.num_hidden, config.layer_normalization)\n    elif config.type in {C.TANH, C.SIGMOID, C.RELU, C.SOFT_RELU}:\n        return ActivationCoverage(config.num_hidden, config.type, config.layer_normalization)\n    elif config.type == C.COVERAGE_COUNT:\n        return CountCoverage()\n    elif config.type == C.COVERAGE_FERTILITY:\n        return FertilityCoverage(config.max_fertility)\n    else:\n        raise ValueError(\"Unknown coverage type %s\" % config.type)", "response": "Returns a Coverage instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef mask_coverage(coverage: mx.sym.Symbol, source_length: mx.sym.Symbol) -> mx.sym.Symbol:\n    return mx.sym.SequenceMask(data=coverage, axis=1, use_sequence_length=True, sequence_length=source_length)", "response": "Masks all coverage scores that are outside the actual sequence."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef on(self, source: mx.sym.Symbol, source_length: mx.sym.Symbol, source_seq_len: int) -> Callable:\n\n        def update_coverage(prev_hidden: mx.sym.Symbol,\n                            attention_prob_scores: mx.sym.Symbol,\n                            prev_coverage: mx.sym.Symbol):\n            \"\"\"\n            :param prev_hidden: Previous hidden decoder state. Shape: (batch_size, decoder_num_hidden).\n            :param attention_prob_scores: Current attention scores. Shape: (batch_size, source_seq_len).\n            :param prev_coverage: Shape: (batch_size, source_seq_len, coverage_num_hidden).\n            :return: Updated coverage matrix . Shape: (batch_size, source_seq_len, coverage_num_hidden).\n            \"\"\"\n            raise NotImplementedError()\n\n        return update_coverage", "response": "Returns a function that returns a function that can be used to update the coverage vectors in a sequence decoder."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef on(self, source: mx.sym.Symbol, source_length: mx.sym.Symbol, source_seq_len: int) -> Callable:\n\n        # (batch_size, seq_len, 1)\n        source_fertility = mx.sym.FullyConnected(data=source,\n                                                 weight=self.cov_e2f_weight,\n                                                 no_bias=True,\n                                                 num_hidden=1,\n                                                 flatten=False,\n                                                 name=\"%ssource_fertility_fc\" % self.prefix)\n\n        # (batch_size, seq_len, 1)\n        fertility = mx.sym.Activation(data=source_fertility,\n                                      act_type=\"sigmoid\",\n                                      name=\"%sactivation\" % self.prefix)\n\n        # (batch_size, seq_len, 1)\n        scaled_fertility = 1 / (self.max_fertility * fertility)\n\n        def update_coverage(prev_hidden: mx.sym.Symbol,\n                            attention_prob_scores: mx.sym.Symbol,\n                            prev_coverage: mx.sym.Symbol):\n            \"\"\"\n            :param prev_hidden: Previous hidden decoder state. Shape: (batch_size, decoder_num_hidden).\n            :param attention_prob_scores: Current attention scores. Shape: (batch_size, source_seq_len).\n            :param prev_coverage: Shape: (batch_size, source_seq_len, coverage_num_hidden).\n            :return: Updated coverage matrix . Shape: (batch_size, source_seq_len, coverage_num_hidden).\n            \"\"\"\n\n            # (batch_size, source_seq_len, 1)\n            expanded_att_scores = mx.sym.expand_dims(data=attention_prob_scores,\n                                                     axis=2,\n                                                     name=\"%sexpand_attention_scores\" % self.prefix)\n\n            # (batch_size, source_seq_len, 1)\n            new_coverage = scaled_fertility * expanded_att_scores\n\n            return prev_coverage + new_coverage\n\n        return update_coverage", "response": "Returns a function that can be used to update the coverage vectors in a sequence decoder."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a function that updates the coverage vectors of a source word.", "response": "def on(self, source: mx.sym.Symbol, source_length: mx.sym.Symbol, source_seq_len: int) -> Callable:\n        \"\"\"\n        Returns callable to be used for updating coverage vectors in a sequence decoder.\n\n        :param source: Shape: (batch_size, seq_len, encoder_num_hidden).\n        :param source_length: Shape: (batch_size,).\n        :param source_seq_len: Maximum length of source sequences.\n        :return: Coverage callable.\n        \"\"\"\n\n        def update_coverage(prev_hidden: mx.sym.Symbol,\n                            attention_prob_scores: mx.sym.Symbol,\n                            prev_coverage: mx.sym.Symbol):\n            \"\"\"\n            :param prev_hidden: Previous hidden decoder state. Shape: (batch_size, decoder_num_hidden).\n            :param attention_prob_scores: Current attention scores. Shape: (batch_size, source_seq_len).\n            :param prev_coverage: Shape: (batch_size, source_seq_len, coverage_num_hidden).\n            :return: Updated coverage matrix . Shape: (batch_size, source_seq_len, coverage_num_hidden).\n            \"\"\"\n\n            # (batch_size, source_seq_len, decoder_num_hidden)\n            expanded_decoder = mx.sym.broadcast_axis(\n                data=mx.sym.expand_dims(data=prev_hidden, axis=1, name=\"%sexpand_decoder\" % self.prefix),\n                axis=1, size=source_seq_len, name=\"%sbroadcast_decoder\" % self.prefix)\n\n            # (batch_size, source_seq_len, 1)\n            expanded_att_scores = mx.sym.expand_dims(data=attention_prob_scores,\n                                                     axis=2,\n                                                     name=\"%sexpand_attention_scores\" % self.prefix)\n\n            # (batch_size, source_seq_len, encoder_num_hidden + decoder_num_hidden + 1)\n            # +1 for the attention_prob_score for the source word\n            concat_input = mx.sym.concat(source, expanded_decoder, expanded_att_scores, dim=2,\n                                         name=\"%sconcat_inputs\" % self.prefix)\n\n            # (batch_size * source_seq_len, encoder_num_hidden + decoder_num_hidden + 1)\n            flat_input = mx.sym.reshape(concat_input, shape=(-3, -1), name=\"%sflatten_inputs\")\n\n            # coverage: (batch_size * seq_len, coverage_num_hidden)\n            coverage = mx.sym.reshape(data=prev_coverage, shape=(-3, -1))\n            updated_coverage, _ = self.gru(flat_input, states=[coverage])\n\n            # coverage: (batch_size, seq_len, coverage_num_hidden)\n            coverage = mx.sym.reshape(updated_coverage, shape=(-1, source_seq_len, self.num_hidden))\n\n            return mask_coverage(coverage, source_length)\n\n        return update_coverage"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef on(self, source: mx.sym.Symbol, source_length: mx.sym.Symbol, source_seq_len: int) -> Callable:\n\n        # (batch_size, seq_len, coverage_hidden_num)\n        source_hidden = mx.sym.FullyConnected(data=source,\n                                              weight=self.cov_e2h_weight,\n                                              no_bias=True,\n                                              num_hidden=self.num_hidden,\n                                              flatten=False,\n                                              name=\"%ssource_hidden_fc\" % self.prefix)\n\n        def update_coverage(prev_hidden: mx.sym.Symbol,\n                            attention_prob_scores: mx.sym.Symbol,\n                            prev_coverage: mx.sym.Symbol):\n            \"\"\"\n            :param prev_hidden: Previous hidden decoder state. Shape: (batch_size, decoder_num_hidden).\n            :param attention_prob_scores: Current attention scores. Shape: (batch_size, source_seq_len).\n            :param prev_coverage: Shape: (batch_size, source_seq_len, coverage_num_hidden).\n            :return: Updated coverage matrix . Shape: (batch_size, source_seq_len, coverage_num_hidden).\n            \"\"\"\n\n            # (batch_size, seq_len, coverage_hidden_num)\n            coverage_hidden = mx.sym.FullyConnected(data=prev_coverage,\n                                                    weight=self.cov_prev2h_weight,\n                                                    no_bias=True,\n                                                    num_hidden=self.num_hidden,\n                                                    flatten=False,\n                                                    name=\"%sprevious_hidden_fc\" % self.prefix)\n\n            # (batch_size, source_seq_len, 1)\n            attention_prob_scores = mx.sym.expand_dims(attention_prob_scores, axis=2)\n\n            # (batch_size, source_seq_len, coverage_num_hidden)\n            attention_hidden = mx.sym.FullyConnected(data=attention_prob_scores,\n                                                     weight=self.cov_a2h_weight,\n                                                     no_bias=True,\n                                                     num_hidden=self.num_hidden,\n                                                     flatten=False,\n                                                     name=\"%sattention_fc\" % self.prefix)\n\n            # (batch_size, coverage_num_hidden)\n            prev_hidden = mx.sym.FullyConnected(data=prev_hidden, weight=self.cov_dec2h_weight, no_bias=True,\n                                                num_hidden=self.num_hidden, name=\"%sdecoder_hidden\")\n\n            # (batch_size, 1, coverage_num_hidden)\n            prev_hidden = mx.sym.expand_dims(data=prev_hidden, axis=1,\n                                             name=\"%sinput_decoder_hidden_expanded\" % self.prefix)\n\n            # (batch_size, source_seq_len, coverage_num_hidden)\n            intermediate = mx.sym.broadcast_add(lhs=source_hidden, rhs=prev_hidden,\n                                                name=\"%ssource_plus_hidden\" % self.prefix)\n\n            # (batch_size, source_seq_len, coverage_num_hidden)\n            updated_coverage = intermediate + attention_hidden + coverage_hidden\n\n            if self.layer_norm is not None:\n                updated_coverage = self.layer_norm(data=updated_coverage)\n\n            # (batch_size, seq_len, coverage_num_hidden)\n            coverage = mx.sym.Activation(data=updated_coverage,\n                                         act_type=self.activation,\n                                         name=\"%sactivation\" % self.prefix)\n\n            return mask_coverage(coverage, source_length)\n\n        return update_coverage", "response": "Returns a function that can be used to update the coverage vectors in a sequence decoder."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a file descriptor for a plain text or gzipped file in binary read mode.", "response": "def bin_open(fname: str):\n    \"\"\"\n    Returns a file descriptor for a plain text or gzipped file, binary read mode\n    for subprocess interaction.\n\n    :param fname: The filename to open.\n    :return: File descriptor in binary read mode.\n    \"\"\"\n    if fname.endswith(\".gz\"):\n        return gzip.open(fname, \"rb\")\n    return open(fname, \"rb\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks if git command is available.", "response": "def check_git():\n    \"\"\"Check if git command is available.\"\"\"\n    try:\n        with open(os.devnull, \"wb\") as devnull:\n            subprocess.check_call([\"git\", \"--version\"], stdout=devnull, stderr=devnull)\n    except:\n        raise RuntimeError(\"Please make sure git is installed and on your path.\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef checkout_moses_tokenizer(workspace_dir: str):\n    # Prerequisites\n    check_git()\n    check_perl()\n    # Check cache\n    dest = os.path.join(workspace_dir, DIR_THIRD_PARTY, MOSES_DEST)\n    if confirm_checkout(dest, MOSES_COMMIT):\n        logging.info(\"Usable: %s\", dest)\n        return\n    # Need to (re-)checkout\n    if os.path.exists(dest):\n        shutil.rmtree(dest)\n    logging.info(\"Checkout: %s -> %s\", MOSES_REPO, dest)\n    os.makedirs(dest)\n    log_fname = os.path.join(workspace_dir, DIR_LOGS, \"checkout.{}.{}.log\".format(MOSES_DEST, os.getpid()))\n    with open(log_fname, \"wb\") as log:\n        logging.info(\"Log: %s\", log_fname)\n        subprocess.call([\"git\", \"init\"], cwd=dest, stdout=log, stderr=log)\n        subprocess.call([\"git\", \"remote\", \"add\", \"origin\", MOSES_REPO], cwd=dest, stdout=log, stderr=log)\n        subprocess.call([\"git\", \"config\", \"core.sparsecheckout\", \"true\"], cwd=dest, stdout=log, stderr=log)\n        with open(os.path.join(dest, \".git\", \"info\", \"sparse-checkout\"), \"w\") as out:\n            for path in MOSES_SPARSE_CHECKOUT:\n                print(path, file=out)\n        subprocess.call([\"git\", \"pull\", \"origin\", \"master\"], cwd=dest, stdout=log, stderr=log)\n        subprocess.call([\"git\", \"checkout\", MOSES_COMMIT], cwd=dest, stdout=log, stderr=log)", "response": "Checkout Moses tokenizer (sparse checkout of Moses).\n\n    :param workspace_dir: Workspace directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef checkout_subword_nmt(workspace_dir: str):\n    # Prerequisites\n    check_git()\n    # Check cache\n    dest = os.path.join(workspace_dir, DIR_THIRD_PARTY, SUBWORD_NMT_DEST)\n    if confirm_checkout(dest, SUBWORD_NMT_COMMIT):\n        logging.info(\"Usable: %s\", dest)\n        return\n    # Need to (re-)checkout\n    if os.path.exists(dest):\n        shutil.rmtree(dest)\n    logging.info(\"Checkout: %s -> %s\", SUBWORD_NMT_REPO, dest)\n    log_fname = os.path.join(workspace_dir, DIR_LOGS, \"checkout.{}.{}.log\".format(SUBWORD_NMT_DEST, os.getpid()))\n    with open(log_fname, \"wb\") as log:\n        logging.info(\"Log: %s\", log_fname)\n        subprocess.call([\"git\", \"clone\", SUBWORD_NMT_REPO, dest], stdout=log, stderr=log)\n        subprocess.call([\"git\", \"checkout\", SUBWORD_NMT_COMMIT], cwd=dest, stdout=log, stderr=log)", "response": "Checkout subword - nmt implementation of byte - pair encoding."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconfirm that git repository is checked out.", "response": "def confirm_checkout(dest: str, commit: str) -> bool:\n    \"\"\"\n    Confirm that git repository is checked out.\n\n    :param dest: Local directory for checkout.\n    :param commit: Git commit.\n    :return: True if checkout is usable.\n    \"\"\"\n    usable = False\n    if os.path.exists(dest):\n        try:\n            rev = subprocess.check_output([\"git\", \"rev-parse\", \"--verify\", \"HEAD\"], cwd=dest).decode(\"utf-8\").strip()\n            usable = (rev == commit)\n        except subprocess.CalledProcessError:\n            pass\n        if not usable:\n            logging.info(\"Problem with %s, requires new checkout.\", dest)\n    return usable"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef call_moses_tokenizer(workspace_dir: str,\n                         input_fname: str,\n                         output_fname: str,\n                         lang_code: str,\n                         num_threads: int = 4):\n    \"\"\"\n    Call Moses tokenizer.\n\n    :param workspace_dir: Workspace third-party directory where Moses\n                            tokenizer is checked out.\n    :param input_fname: Path of raw input file, plain text or gzipped.\n    :param output_fname: Path of tokenized output file, gzipped.\n    :param lang_code: Language code for rules and non-breaking prefixes.\n    :param num_threads: Number of threads to use.\n    \"\"\"\n    tokenizer_fname = os.path.join(workspace_dir,\n                                   DIR_THIRD_PARTY,\n                                   MOSES_DEST,\n                                   \"scripts\",\n                                   \"tokenizer\",\n                                   \"tokenizer.perl\")\n    with bin_open(input_fname) as inp, gzip.open(output_fname, \"wb\") as out, open(os.devnull, \"wb\") as devnull:\n        tokenizer = subprocess.Popen([\"perl\", tokenizer_fname, \"-l\", lang_code, \"-threads\", str(num_threads)],\n                                     stdin=subprocess.PIPE,\n                                     stdout=subprocess.PIPE,\n                                     stderr=devnull)\n        tokenizer_thread = threading.Thread(target=copy_out, args=(tokenizer.stdout, out))\n        tokenizer_thread.start()\n        for line in inp:\n            tokenizer.stdin.write(line)\n        tokenizer.stdin.close()\n        tokenizer_thread.join()\n        tokenizer.wait()", "response": "Call Moses tokenizer.\n\n    :param workspace_dir: Workspace third-party directory where Moses\n                            tokenizer is checked out.\n    :param input_fname: Path of raw input file, plain text or gzipped.\n    :param output_fname: Path of tokenized output file, gzipped.\n    :param lang_code: Language code for rules and non-breaking prefixes.\n    :param num_threads: Number of threads to use."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalling Moses detokenizer. :param workspace_dir: Workspace third-party directory where Moses tokenizer is checked out. :param input_fname: Path of tokenized input file, plain text or gzipped. :param output_fname: Path of tokenized output file, plain text. :param lang_code: Language code for rules and non-breaking prefixes. Can be None if unknown (using pre-tokenized data), which will cause the tokenizer to default to English.", "response": "def call_moses_detokenizer(workspace_dir: str, input_fname: str, output_fname: str, lang_code: Optional[str] = None):\n    \"\"\"\n    Call Moses detokenizer.\n\n    :param workspace_dir: Workspace third-party directory where Moses\n                          tokenizer is checked out.\n    :param input_fname: Path of tokenized input file, plain text or gzipped.\n    :param output_fname: Path of tokenized output file, plain text.\n    :param lang_code: Language code for rules and non-breaking prefixes.  Can be\n                      None if unknown (using pre-tokenized data), which will\n                      cause the tokenizer to default to English.\n    \"\"\"\n    detokenizer_fname = os.path.join(workspace_dir,\n                                     DIR_THIRD_PARTY,\n                                     MOSES_DEST,\n                                     \"scripts\",\n                                     \"tokenizer\",\n                                     \"detokenizer.perl\")\n    with bin_open(input_fname) as inp, open(output_fname, \"wb\") as out, open(os.devnull, \"wb\") as devnull:\n        command = [\"perl\", detokenizer_fname]\n        if lang_code:\n            command.append(\"-l\")\n            command.append(lang_code)\n        detokenizer = subprocess.Popen(command,\n                                       stdin=subprocess.PIPE,\n                                       stdout=subprocess.PIPE,\n                                       stderr=devnull)\n        detokenizer_thread = threading.Thread(target=copy_out, args=(detokenizer.stdout, out))\n        detokenizer_thread.start()\n        for line in inp:\n            detokenizer.stdin.write(line)\n        detokenizer.stdin.close()\n        detokenizer_thread.join()\n        detokenizer.wait()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalls learn_bpe script to learn byte - pair encoding model.", "response": "def call_learn_bpe(workspace_dir: str, source_fname: str, target_fname: str, model_fname: str, num_ops: int = 32000):\n    \"\"\"\n    Call script to learn byte-pair encoding model.\n\n    :param workspace_dir: Workspace third-party directory where subword-nmt is\n                            checked out.\n    :param source_fname: Path of source corpus file, plain text or gzipped.\n    :param target_fname: Path of target corpus file, plain text or gzipped.\n    :param model_fname: Path to write out model.\n    :param num_ops: Number of operations.\n    \"\"\"\n    learn_bpe_fname = os.path.join(workspace_dir, DIR_THIRD_PARTY, SUBWORD_NMT_DEST, \"learn_bpe.py\")\n    with bin_open(source_fname) as src_in, bin_open(target_fname) as trg_in, open(model_fname, \"wb\") as out:\n        learn_bpe = subprocess.Popen([sys.executable, learn_bpe_fname, \"-s\", str(num_ops)],\n                                     stdin=subprocess.PIPE,\n                                     stdout=subprocess.PIPE)\n        learn_bpe_thread = threading.Thread(target=copy_out, args=(learn_bpe.stdout, out))\n        learn_bpe_thread.start()\n        for inp in (src_in, trg_in):\n            for line in inp:\n                learn_bpe.stdin.write(line)\n        learn_bpe.stdin.close()\n        learn_bpe_thread.join()\n        learn_bpe.wait()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef call_apply_bpe(workspace_dir: str, input_fname: str, output_fname: str, model_fname: str):\n    apply_bpe_fname = os.path.join(workspace_dir, DIR_THIRD_PARTY, SUBWORD_NMT_DEST, \"apply_bpe.py\")\n    with bin_open(input_fname) as inp, gzip.open(output_fname, \"wb\") as out:\n        apply_bpe = subprocess.Popen([sys.executable, apply_bpe_fname, \"-c\", model_fname],\n                                     stdin=subprocess.PIPE,\n                                     stdout=subprocess.PIPE)\n        apply_bpe_thread = threading.Thread(target=copy_out, args=(apply_bpe.stdout, out, True))\n        apply_bpe_thread.start()\n        for line in inp:\n            # Use an empty line placeholder to avoid blank line duplication\n            # issues with BPE script\n            if not line.strip():\n                line = PLACEHOLDER + b\"\\n\"\n            apply_bpe.stdin.write(line)\n        apply_bpe.stdin.close()\n        apply_bpe_thread.join()\n        apply_bpe.wait()", "response": "Call BPE apply script."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmerging byte - pair encoded sub - words into one single file.", "response": "def merge_bpe(input_fname: str, output_fname: str):\n    \"\"\"\n    Merge byte-pair encoded sub-words.\n\n    :param input_fname: Path of byte-pair encoded input file, plain text or\n                        gzipped.\n    :param output_fname: Path of tokenized output file, plain text.\n    \"\"\"\n    with utils.smart_open(input_fname, \"r\") as inp, open(output_fname, \"w\", encoding=\"utf-8\") as out:\n        for line in inp:\n            # Merge on special markers and strip stray markers (end of line)\n            merged = line.replace(SUBWORD_SPECIAL + \" \", \"\").replace(SUBWORD_SPECIAL, \"\")\n            out.write(merged)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncopies lines from source to destination.", "response": "def copy_out(source: Iterable[bytes], dest: io.BytesIO, use_placeholders: bool = False):\n    \"\"\"\n    Copy lines from source to destination.\n\n    :param source: Source line iterable.\n    :param dest: Destination open file.\n    :param use_placeholders: When true, convert lines containing placeholders to\n                             empty lines and drop true empty lines (assume to be\n                             spuriously generated).\n    \"\"\"\n    for line in source:\n        if use_placeholders:\n            # True empty lines are assumed to be spurious as the placeholder\n            # should be passed through\n            if not line.strip():\n                continue\n            if line.startswith(PLACEHOLDER):\n                line = b\"\\n\"\n        dest.write(line)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef raw_corpus_bleu(hypotheses: Iterable[str], references: Iterable[str], offset: Optional[float] = 0.01) -> float:\n    return sacrebleu.raw_corpus_bleu(hypotheses, [references], smooth_floor=offset).score / 100.0", "response": "Simple wrapper around sacreBLEU s BLEU with tokenization and smoothing constant."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef raw_corpus_rougel(hypotheses: Iterable[str], references: Iterable[str]) -> float:\n    return rouge.rouge_l(hypotheses, references)", "response": "Simple wrapper around ROUGE - L implementation."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef raw_corpus_length_ratio(hypotheses: Iterable[str], references: Iterable[str]) -> float:\n    ratios = [len(h.split())/len(r.split()) for h, r in zip(hypotheses, references)]\n    return sum(ratios)/len(ratios) if len(ratios) else 0.0", "response": "Simple wrapper around length ratio implementation."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads a list of image - text models for inference.", "response": "def load_models(context: mx.context.Context,\n                max_input_len: Optional[int],\n                beam_size: int,\n                batch_size: int,\n                model_folders: List[str],\n                checkpoints: Optional[List[int]] = None,\n                softmax_temperature: Optional[float] = None,\n                max_output_length_num_stds: int = C.DEFAULT_NUM_STD_MAX_OUTPUT_LENGTH,\n                decoder_return_logit_inputs: bool = False,\n                cache_output_layer_w_b: bool = False,\n                source_image_size: tuple = None,\n                forced_max_output_len: Optional[int] = None) -> Tuple[List[ImageInferenceModel], vocab.Vocab]:\n    \"\"\"\n    Loads a list of models for inference.\n\n    :param context: MXNet context to bind modules to.\n    :param max_input_len: Maximum input length.\n    :param beam_size: Beam size.\n    :param batch_size: Batch size.\n    :param model_folders: List of model folders to load models from.\n    :param checkpoints: List of checkpoints to use for each model in model_folders. Use None to load best checkpoint.\n    :param softmax_temperature: Optional parameter to control steepness of softmax distribution.\n    :param max_output_length_num_stds: Number of standard deviations to add to mean target-source length ratio\n           to compute maximum output length.\n    :param decoder_return_logit_inputs: Model decoders return inputs to logit computation instead of softmax over target\n                                        vocabulary.  Used when logits/softmax are handled separately.\n    :param cache_output_layer_w_b: Models cache weights and biases for logit computation as NumPy arrays (used with\n                                   restrict lexicon).\n    :param source_image_size: Size of the image to resize to. Used only for the image-text models\n    :param forced_max_output_len: An optional overwrite of the maximum out length.\n    :return: List of models, target vocabulary, source factor vocabularies.\n    \"\"\"\n    models = []  # type: List[ImageInferenceModel]\n    target_vocabs = []  # type: List[vocab.Vocab]\n\n    if checkpoints is None:\n        checkpoints = [None] * len(model_folders)\n\n    for model_folder, checkpoint in zip(model_folders, checkpoints):\n        target_vocabs.append(vocab.vocab_from_json(os.path.join(model_folder, C.VOCAB_TRG_NAME)))\n\n        model_version = utils.load_version(os.path.join(model_folder, C.VERSION_NAME))\n        logger.info(\"Model version: %s\", model_version)\n        utils.check_version(model_version)\n        model_config = model.SockeyeModel.load_config(os.path.join(model_folder, C.CONFIG_NAME))\n\n        if checkpoint is None:\n            params_fname = os.path.join(model_folder, C.PARAMS_BEST_NAME)\n        else:\n            params_fname = os.path.join(model_folder, C.PARAMS_NAME % checkpoint)\n\n        inference_model = ImageInferenceModel(config=model_config,\n                                              params_fname=params_fname,\n                                              context=context,\n                                              beam_size=beam_size,\n                                              softmax_temperature=softmax_temperature,\n                                              decoder_return_logit_inputs=decoder_return_logit_inputs,\n                                              cache_output_layer_w_b=cache_output_layer_w_b,\n                                              input_size=source_image_size,\n                                              forced_max_output_len=forced_max_output_len)\n\n        models.append(inference_model)\n\n    # set a common max_output length for all models.\n    max_input_len, get_max_output_length = models_max_input_output_length(models,\n                                                                          max_output_length_num_stds,\n                                                                          max_input_len,\n                                                                          forced_max_output_len=forced_max_output_len)\n\n    for inference_model in models:\n        inference_model.initialize(batch_size, max_input_len, get_max_output_length)\n\n    return models, target_vocabs[0]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_encoder_data_shapes(self, bucket_key: int, batch_size: int) -> List[mx.io.DataDesc]:\n        return [mx.io.DataDesc(name=C.SOURCE_NAME,\n                               shape=(batch_size,) + self.input_size,\n                               layout=C.BATCH_MAJOR_IMAGE)]", "response": "Returns the data shapes of the encoder module."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntranslate a list of TranslatorInputs into a list of TranslatorOutputs.", "response": "def translate(self, trans_inputs: List[TranslatorInput]) -> List[TranslatorOutput]:\n        \"\"\"\n        Batch-translates a list of TranslatorInputs, returns a list of TranslatorOutputs.\n        Splits oversized sentences to sentence chunks of size less than max_input_length.\n\n        :param trans_inputs: List of TranslatorInputs as returned by make_input().\n        :return: List of translation results.\n        \"\"\"\n        batch_size = self.max_batch_size\n        # translate in batch-sized blocks over input chunks\n        translations = []\n        for batch_id, batch in enumerate(utils.grouper(trans_inputs, batch_size)):\n            logger.debug(\"Translating batch %d\", batch_id)\n            # underfilled batch will be filled to a full batch size with copies of the 1st input\n            rest = batch_size - len(batch)\n            if rest > 0:\n                logger.debug(\"Extending the last batch to the full batch size (%d)\", batch_size)\n                batch = batch + [batch[0]] * rest\n            batch_translations = self._translate_nd(*self._get_inference_input(batch))\n            # truncate to remove filler translations\n            if rest > 0:\n                batch_translations = batch_translations[:-rest]\n            translations.extend(batch_translations)\n\n        # Concatenate results\n        results = []  # type: List[TranslatorOutput]\n        for trans_input, translation in zip(trans_inputs, translations):\n            results.append(self._make_result(trans_input, translation))\n        return results"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_inference_input(self,\n                             trans_inputs: List[TranslatorInput]) -> Tuple[mx.nd.NDArray,\n                                                                           int,\n                                                                           Optional[lexicon.TopKLexicon],\n                                                                           List[\n                                                                               Optional[constrained.RawConstraintList]],\n                                                                           List[\n                                                                               Optional[constrained.RawConstraintList]],\n                                                                           mx.nd.NDArray]:\n        \"\"\"\n        Returns NDArray of images and corresponding bucket_key and an NDArray of maximum output lengths\n        for each sentence in the batch.\n\n        :param trans_inputs: List of TranslatorInputs. The path of the image/feature is in the token field.\n        :param constraints: Optional list of constraints.\n        :return: NDArray of images paths, bucket key, a list of raw constraint lists,\n                an NDArray of maximum output lengths.\n        \"\"\"\n        batch_size = len(trans_inputs)\n        image_paths = [None for _ in range(batch_size)]  # type: List[Optional[str]]\n        restrict_lexicon = None  # type: Optional[lexicon.TopKLexicon]\n        raw_constraints = [None for _ in range(batch_size)]  # type: List[Optional[constrained.RawConstraintList]]\n        raw_avoid_list = [None for _ in range(batch_size)]  # type: List[Optional[constrained.RawConstraintList]]\n        for j, trans_input in enumerate(trans_inputs):\n            # Join relative path with absolute\n            path = trans_input.tokens[0]\n            if self.source_root is not None:\n                path = os.path.join(self.source_root, path)\n            image_paths[j] = path\n            # Preprocess constraints\n            if trans_input.constraints is not None:\n                raw_constraints[j] = [data_io.tokens2ids(phrase, self.vocab_target) for phrase in\n                                      trans_input.constraints]\n\n        # Read data and zero pad if necessary\n        images = self.data_loader(image_paths)\n        images = utils_image.zero_pad_features(images, self.source_image_size)\n\n        max_input_length = 0\n        max_output_lengths = [self.models[0].get_max_output_length(max_input_length)] * len(image_paths)\n        return mx.nd.array(images), max_input_length, restrict_lexicon, raw_constraints, raw_avoid_list, \\\n                mx.nd.array(max_output_lengths, ctx=self.context, dtype='int32')", "response": "Returns an NDArray of images paths bucket key a list of raw constraints and maximum output lengths for each sentence in the batch."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef decode_and_evaluate(self,\n                            checkpoint: Optional[int] = None,\n                            output_name: str = os.devnull) -> Dict[str, float]:\n        \"\"\"\n        Decodes data set and evaluates given a checkpoint.\n\n        :param checkpoint: Checkpoint to load parameters from.\n        :param output_name: Filename to write translations to. Defaults to /dev/null.\n        :return: Mapping of metric names to scores.\n        \"\"\"\n        models, source_vocabs, target_vocab = inference.load_models(\n            self.context,\n            self.max_input_len,\n            self.beam_size,\n            self.batch_size,\n            [self.model],\n            [checkpoint],\n            softmax_temperature=self.softmax_temperature,\n            max_output_length_num_stds=self.max_output_length_num_stds)\n        translator = inference.Translator(context=self.context,\n                                          ensemble_mode=self.ensemble_mode,\n                                          bucket_source_width=self.bucket_width_source,\n                                          length_penalty=inference.LengthPenalty(self.length_penalty_alpha, self.length_penalty_beta),\n                                          brevity_penalty=inference.BrevityPenalty(weight=0.0),\n                                          beam_prune=0.0,\n                                          beam_search_stop='all',\n                                          nbest_size=self.nbest_size,\n                                          models=models,\n                                          source_vocabs=source_vocabs,\n                                          target_vocab=target_vocab,\n                                          restrict_lexicon=None,\n                                          store_beam=False)\n        trans_wall_time = 0.0\n        translations = []\n        with data_io.smart_open(output_name, 'w') as output:\n            handler = sockeye.output_handler.StringOutputHandler(output)\n            tic = time.time()\n            trans_inputs = []  # type: List[inference.TranslatorInput]\n            for i, inputs in enumerate(self.inputs_sentences):\n                trans_inputs.append(sockeye.inference.make_input_from_multiple_strings(i, inputs))\n            trans_outputs = translator.translate(trans_inputs)\n            trans_wall_time = time.time() - tic\n            for trans_input, trans_output in zip(trans_inputs, trans_outputs):\n                handler.handle(trans_input, trans_output)\n                translations.append(trans_output.translation)\n        avg_time = trans_wall_time / len(self.target_sentences)\n\n        # TODO(fhieber): eventually add more metrics (METEOR etc.)\n        return {C.BLEU_VAL: evaluate.raw_corpus_bleu(hypotheses=translations,\n                                                     references=self.target_sentences,\n                                                     offset=0.01),\n                C.CHRF_VAL: evaluate.raw_corpus_chrf(hypotheses=translations,\n                                                     references=self.target_sentences),\n                C.ROUGE_1_VAL: evaluate.raw_corpus_rouge1(hypotheses=translations,\n                                                          references=self.target_sentences),\n                C.ROUGE_2_VAL: evaluate.raw_corpus_rouge2(hypotheses=translations,\n                                                          references=self.target_sentences),\n                C.ROUGE_L_VAL: evaluate.raw_corpus_rougel(hypotheses=translations,\n                                                          references=self.target_sentences),\n                C.LENRATIO_VAL: evaluate.raw_corpus_length_ratio(hypotheses=translations,\n                                                                 references=self.target_sentences),\n                C.AVG_TIME: avg_time,\n                C.DECODING_TIME: trans_wall_time}", "response": "Decodes the data set and evaluates the data set."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nyield lines from a translation table of format: src, trg, logprob. :param path: Path to lexicon file. :param vocab_source: Source vocabulary. :param vocab_target: Target vocabulary. :return: Generator returning tuples (src_id, trg_id, prob).", "response": "def lexicon_iterator(path: str,\n                     vocab_source: Dict[str, int],\n                     vocab_target: Dict[str, int]) -> Generator[Tuple[int, int, float], None, None]:\n    \"\"\"\n    Yields lines from a translation table of format: src, trg, logprob.\n\n    :param path: Path to lexicon file.\n    :param vocab_source: Source vocabulary.\n    :param vocab_target: Target vocabulary.\n    :return: Generator returning tuples (src_id, trg_id, prob).\n    \"\"\"\n    assert C.UNK_SYMBOL in vocab_source\n    assert C.UNK_SYMBOL in vocab_target\n    src_unk_id = vocab_source[C.UNK_SYMBOL]\n    trg_unk_id = vocab_target[C.UNK_SYMBOL]\n    with smart_open(path) as fin:\n        for line in fin:\n            src, trg, logprob = line.rstrip(\"\\n\").split(\"\\t\")\n            prob = np.exp(float(logprob))\n            src_id = vocab_source.get(src, src_unk_id)\n            trg_id = vocab_target.get(trg, trg_unk_id)\n            yield src_id, trg_id, prob"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading a lexicon from a file.", "response": "def read_lexicon(path: str, vocab_source: Dict[str, int], vocab_target: Dict[str, int]) -> np.ndarray:\n    \"\"\"\n    Loads lexical translation probabilities from a translation table of format: src, trg, logprob.\n    Source words unknown to vocab_source are discarded.\n    Target words unknown to vocab_target contribute to p(unk|source_word).\n    See Incorporating Discrete Translation Lexicons into Neural Machine Translation, Section 3.1 & Equation 5\n    (https://arxiv.org/pdf/1606.02006.pdf))\n\n    :param path: Path to lexicon file.\n    :param vocab_source: Source vocabulary.\n    :param vocab_target: Target vocabulary.\n    :return: Lexicon array. Shape: (vocab_source_size, vocab_target_size).\n    \"\"\"\n    src_unk_id = vocab_source[C.UNK_SYMBOL]\n    trg_unk_id = vocab_target[C.UNK_SYMBOL]\n    lexicon = np.zeros((len(vocab_source), len(vocab_target)))\n    n = 0\n    for src_id, trg_id, prob in lexicon_iterator(path, vocab_source, vocab_target):\n        if src_id == src_unk_id:\n            continue\n        if trg_id == trg_unk_id:\n            lexicon[src_id, trg_unk_id] += prob\n        else:\n            lexicon[src_id, trg_id] = prob\n        n += 1\n    logger.info(\"Loaded lexicon from '%s' with %d entries\", path, n)\n    return lexicon"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a top - k lexicon from a scored lexicon file.", "response": "def create(self, path: str, k: int = 20):\n        \"\"\"\n        Create from a scored lexicon file (fast_align format) using vocab from a trained Sockeye model.\n\n        :param path: Path to lexicon file.\n        :param k: Number of target entries per source to keep.\n        \"\"\"\n        self.lex = np.zeros((len(self.vocab_source), k), dtype=np.int)\n        src_unk_id = self.vocab_source[C.UNK_SYMBOL]\n        trg_unk_id = self.vocab_target[C.UNK_SYMBOL]\n        num_insufficient = 0  # number of source tokens with insufficient number of translations given k\n        for src_id, group in groupby(lexicon_iterator(path, self.vocab_source, self.vocab_target), key=itemgetter(0)):\n            # Unk token will always be part of target vocab, so no need to track it here\n            if src_id == src_unk_id:\n                continue\n\n            # filter trg_unk_id\n            filtered_group = ((trg_id, prob) for src_id, trg_id, prob in group if trg_id != trg_unk_id)\n            # sort by prob and take top k\n            top_k = [trg_id for trg_id, prob in sorted(filtered_group, key=itemgetter(1), reverse=True)[:k]]\n            if len(top_k) < k:\n                num_insufficient += 1\n\n            self.lex[src_id, :len(top_k)] = top_k\n\n        logger.info(\"Created top-k lexicon from \\\"%s\\\", k=%d. %d source tokens with fewer than %d translations\",\n                    path, k, num_insufficient, k)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef save(self, path: str):\n        with open(path, 'wb') as out:\n            np.save(out, self.lex)\n        logger.info(\"Saved top-k lexicon to \\\"%s\\\"\", path)", "response": "Save the top - k lexicon in Numpy array format."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading top - k items from Numpy array file.", "response": "def load(self, path: str, k: Optional[int] = None):\n        \"\"\"\n        Load lexicon from Numpy array file. The top-k target ids will be sorted by increasing target id.\n\n        :param path: Path to Numpy array file.\n        :param k: Optionally load less items than stored in path.\n        \"\"\"\n        load_time_start = time.time()\n        with open(path, 'rb') as inp:\n            _lex = np.load(inp)\n        loaded_k = _lex.shape[1]\n        if k is not None:\n            top_k = min(k, loaded_k)\n            if k > loaded_k:\n                logger.warning(\"Can not load top-%d translations from lexicon that \"\n                               \"contains at most %d entries per source.\", k, loaded_k)\n        else:\n            top_k = loaded_k\n        self.lex = np.zeros((len(self.vocab_source), top_k), dtype=_lex.dtype)\n        for src_id, trg_ids in enumerate(_lex):\n            self.lex[src_id, :] = np.sort(trg_ids[:top_k])\n        load_time = time.time() - load_time_start\n        logger.info(\"Loaded top-%d lexicon from \\\"%s\\\" in %.4fs.\", top_k, path, load_time)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconfigures logging for the main application. :param file_logging: Whether to log to a file. :param console: Whether to log to the console. :param path: Optional path to write logfile to. :param level: Log level. Default: INFO.", "response": "def setup_main_logger(file_logging=True, console=True, path: Optional[str] = None, level=logging.INFO):\n    \"\"\"\n    Configures logging for the main application.\n\n    :param file_logging: Whether to log to a file.\n    :param console: Whether to log to the console.\n    :param path: Optional path to write logfile to.\n    :param level: Log level. Default: INFO.\n    \"\"\"\n    if file_logging and console:\n        log_config = LOGGING_CONFIGS[\"file_console\"]  # type: ignore\n    elif file_logging:\n        log_config = LOGGING_CONFIGS[\"file_only\"]\n    elif console:\n        log_config = LOGGING_CONFIGS[\"console_only\"]\n    else:\n        log_config = LOGGING_CONFIGS[\"none\"]\n\n    if path:\n        log_config[\"handlers\"][\"rotating\"][\"filename\"] = path  # type: ignore\n\n    for _, handler_config in log_config['handlers'].items():  # type: ignore\n        handler_config['level'] = level\n\n    logging.config.dictConfig(log_config)  # type: ignore\n\n    def exception_hook(exc_type, exc_value, exc_traceback):\n        if is_python34():\n            # Python3.4 does not seem to handle logger.exception() well\n            import traceback\n            traceback = \"\".join(traceback.format_tb(exc_traceback)) + exc_type.name\n            logging.error(\"Uncaught exception\\n%s\", traceback)\n        else:\n            logging.exception(\"Uncaught exception\", exc_info=(exc_type, exc_value, exc_traceback))\n\n    sys.excepthook = exception_hook"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef identify_raw_files(task: Task, test_mode: bool = False) -> List[str]:\n    raw_files = set()\n    all_sets = [task.test,] if test_mode else [task.train, task.dev, task.test]\n    for file_sets in all_sets:\n        for file_set in file_sets:\n            for fname in file_set[:2]:\n                raw_file = fname.split(\"/\", 1)[0]\n                if raw_file not in RAW_FILES:\n                    raise RuntimeError(\"Unknown raw file %s found in path %s\" % (raw_file, fname))\n                raw_files.add(raw_file)\n    return sorted(raw_files)", "response": "Identify raw files that need to be downloaded for a given task."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndownloads and extract raw files from the given list of raw files.", "response": "def download_extract_raw_files(names: List[str], cache_dir: str, dest_dir: str):\n    \"\"\"\n    Download and extract raw files, making use of a cache directory.\n    - Downloaded files are verified by MD5 sum.\n    - Extraction overwrites existing files.\n\n    :param names: List of raw file names in RAW_FILES.\n    :param cache_dir: Cache directory for downloading raw files.\n    :param dest_dir: Destination directory for extracting raw files.\n    \"\"\"\n\n    for name in names:\n        raw_file = RAW_FILES[name]\n        local_dir = os.path.join(cache_dir, name)\n        local_fname = os.path.join(local_dir, os.path.basename(raw_file.url))\n\n        # Download file if not present\n        if not os.path.exists(local_dir):\n            logging.info(\"Create: %s\", local_dir)\n            os.makedirs(local_dir)\n        if not os.path.exists(local_fname):\n            logging.info(\"Download: %s -> %s\", raw_file.url, local_fname)\n            urllib.request.urlretrieve(raw_file.url, local_fname)\n\n        # Check MD5 sum, attempt one re-download on mismatch\n        md5 = md5sum(local_fname)\n        if not md5 == raw_file.md5:\n            logging.info(\"MD5 mismatch for %s, attempt re-download %s\", local_fname, raw_file.url)\n            urllib.request.urlretrieve(raw_file.url, local_fname)\n            md5 = md5sum(local_fname)\n            if not md5 == raw_file.md5:\n                raise RuntimeError(\"MD5 mismatch for %s after re-download.  Check validity of %s\"\n                                   % (local_fname, raw_file.url))\n        logging.info(\"Confirmed MD5: %s (%s)\", local_fname, md5)\n\n        # Extract file(s), overwriting directory if exists\n        extract_path = os.path.join(dest_dir, name)\n        if os.path.exists(extract_path):\n            shutil.rmtree(extract_path)\n        os.makedirs(extract_path)\n        logging.info(\"Extract: %s -> %s\", local_fname, extract_path)\n        if raw_file.archive_type == ARCHIVE_NONE:\n            os.symlink(local_fname, os.path.join(extract_path, os.path.basename(local_fname)))\n        elif raw_file.archive_type == ARCHIVE_TAR:\n            tar = tarfile.open(local_fname)\n            tar.extractall(path=extract_path)\n        elif raw_file.archive_type == ARCHIVE_ZIP:\n            zipf = zipfile.ZipFile(local_fname, \"r\")\n            zipf.extractall(path=extract_path)\n        else:\n            raise RuntimeError(\"Unknown archive type: %s\" % raw_file.archive_type)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef md5sum(fname: str) -> str:\n    with open(fname, \"rb\") as inp:\n        md5 = hashlib.md5(inp.read()).hexdigest()\n    return md5", "response": "Compute MD5 sum of file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef populate_parallel_text(extract_dir: str,\n                           file_sets: List[Tuple[str, str, str]],\n                           dest_prefix: str,\n                           keep_separate: bool,\n                           head_n: int = 0):\n    \"\"\"\n    Create raw parallel train, dev, or test files with a given prefix.\n\n    :param extract_dir: Directory where raw files (inputs) are extracted.\n    :param file_sets: Sets of files to use.\n    :param dest_prefix: Prefix for output files.\n    :param keep_separate: True if each file set (source-target pair) should have\n                          its own file (used for test sets).\n    :param head_n: If N>0, use only the first N lines (used in test mode).\n    \"\"\"\n    source_out = None  # type: IO[Any]\n    target_out = None  # type: IO[Any]\n    lines_written = 0\n    # Single output file for each side\n    if not keep_separate:\n        source_dest = dest_prefix + SUFFIX_SRC_GZ\n        target_dest = dest_prefix + SUFFIX_TRG_GZ\n        logging.info(\"Populate: %s %s\", source_dest, target_dest)\n        source_out = gzip.open(source_dest, \"wt\", encoding=\"utf-8\")\n        target_out = gzip.open(target_dest, \"wt\", encoding=\"utf-8\")\n    for i, (source_fname, target_fname, text_type) in enumerate(file_sets):\n        # One output file per input file for each side\n        if keep_separate:\n            if source_out:\n                source_out.close()\n            if target_out:\n                target_out.close()\n            source_dest = dest_prefix + str(i) + \".\" + SUFFIX_SRC_GZ\n            target_dest = dest_prefix + str(i) + \".\" + SUFFIX_TRG_GZ\n            logging.info(\"Populate: %s %s\", source_dest, target_dest)\n            source_out = gzip.open(source_dest, \"wt\", encoding=\"utf-8\")\n            target_out = gzip.open(target_dest, \"wt\", encoding=\"utf-8\")\n        for source_line, target_line in zip(\n                plain_text_iter(os.path.join(extract_dir, source_fname), text_type, DATA_SRC),\n                plain_text_iter(os.path.join(extract_dir, target_fname), text_type, DATA_TRG)):\n            # Only write N lines total if requested, but reset per file when\n            # keeping files separate\n            if head_n > 0 and lines_written >= head_n:\n                if keep_separate:\n                    lines_written = 0\n                break\n            source_out.write(\"{}\\n\".format(source_line))\n            target_out.write(\"{}\\n\".format(target_line))\n            lines_written += 1\n    source_out.close()\n    target_out.close()", "response": "Populate the raw text files with a given prefix."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef copy_parallel_text(file_list: List[str], dest_prefix: str):\n    # Group files into source-target pairs\n    file_sets = []\n    for i in range(0, len(file_list), 2):\n        file_sets.append((file_list[i], file_list[i + 1]))\n    multiple_sets = len(file_sets) > 1\n    for i, (source_fname, target_fname) in enumerate(file_sets):\n        if multiple_sets:\n            source_dest = dest_prefix + str(i) + \".\" + SUFFIX_SRC_GZ\n            target_dest = dest_prefix + str(i) + \".\" + SUFFIX_TRG_GZ\n        else:\n            source_dest = dest_prefix + SUFFIX_SRC_GZ\n            target_dest = dest_prefix + SUFFIX_TRG_GZ\n        logging.info(\"Populate: %s %s\", source_dest, target_dest)\n        with gzip.open(source_dest, \"wb\") as source_out, gzip.open(target_dest, \"wb\") as target_out:\n            with third_party.bin_open(source_fname) as inp:\n                for line in inp:\n                    line = (re.sub(r\"\\s\", \" \", line.decode(\"utf-8\"))).encode(\"utf-8\") + b\"\\n\"\n                    source_out.write(line)\n            with third_party.bin_open(target_fname) as inp:\n                for line in inp:\n                    line = (re.sub(r\"\\s\", \" \", line.decode(\"utf-8\"))).encode(\"utf-8\") + b\"\\n\"\n                    target_out.write(line)", "response": "Copy pre - compiled raw parallel files with a given prefix."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nextracting plain text from file as iterable.", "response": "def plain_text_iter(fname: str, text_type: str, data_side: str) -> Iterable[str]:\n    \"\"\"\n    Extract plain text from file as iterable.  Also take steps to ensure that\n    whitespace characters (including unicode newlines) are normalized and\n    outputs are line-parallel with inputs considering ASCII newlines only.\n\n    :param fname: Path of possibly gzipped input file.\n    :param text_type: One of TEXT_*, indicating data format.\n    :param data_side: DATA_SRC or DATA_TRG.\n    \"\"\"\n    if text_type in (TEXT_UTF8_RAW, TEXT_UTF8_TOKENIZED):\n        with third_party.bin_open(fname) as inp:\n            for line in inp:\n                line = re.sub(r\"\\s\", \" \", line.decode(\"utf-8\"))\n                yield line.strip()\n    elif text_type == TEXT_UTF8_RAW_SGML:\n        with third_party.bin_open(fname) as inp:\n            for line in inp:\n                line = re.sub(r\"\\s\", \" \", line.decode(\"utf-8\"))\n                if line.startswith(\"<seg \"):\n                    # Extract segment text\n                    text = re.sub(r\"<seg.*?>(.*)</seg>.*?\", \"\\\\1\", line)\n                    text = re.sub(r\"\\s+\", \" \", text.strip())\n                    # Unescape XML entities\n                    text = text.replace(\"&quot;\", \"\\\"\")\n                    text = text.replace(\"&apos;\", \"'\")\n                    text = text.replace(\"&lt;\", \"<\")\n                    text = text.replace(\"&gt;\", \">\")\n                    text = text.replace(\"&amp;\", \"&\")\n                    yield text\n    elif text_type in (TEXT_UTF8_RAW_BITEXT, TEXT_UTF8_RAW_BITEXT_REVERSE):\n        # Select source or target field, reversing if needed\n        if text_type == TEXT_UTF8_RAW_BITEXT:\n            field_id = 0 if data_side == DATA_SRC else 1\n        else:\n            field_id = 1 if data_side == DATA_SRC else 0\n        with third_party.bin_open(fname) as inp:\n            for line in inp:\n                line = re.sub(r\"\\s\", \" \", line.decode(\"utf-8\"))\n                fields = line.split(\"|||\")\n                yield fields[field_id].strip()\n    else:\n        raise RuntimeError(\"Unknown text type: %s\" % text_type)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndeleting step directory if exists and create reporting actions.", "response": "def renew_step_dir(step_dir: str):\n    \"\"\"Delete step directory if exists and create, reporting actions.\"\"\"\n    if os.path.exists(step_dir):\n        logging.info(\"Remove unfinished step %s\", step_dir)\n        shutil.rmtree(step_dir)\n    logging.info(\"Create: %s\", step_dir)\n    os.makedirs(step_dir)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalls sockeye. train with specified arguments on prepared inputs. Will resume training if model is already finished.", "response": "def call_sockeye_train(model: str,\n                       bpe_dir: str,\n                       model_dir: str,\n                       log_fname: str,\n                       num_gpus: int,\n                       test_mode: bool = False):\n    \"\"\"\n    Call sockeye.train with specified arguments on prepared inputs.  Will resume\n    partial training or skip training if model is already finished.  Record\n    command for future use.\n\n    :param model: Type of translation model to train.\n    :param bpe_dir: Directory of BPE-encoded input data.\n    :param model_dir: Model output directory.\n    :param log_fname: Location to write log file.\n    :param num_gpus: Number of GPUs to use for training (0 for CPU).\n    :param test_mode: Run in test mode, stopping after a small number of\n                      updates.\n    \"\"\"\n    # Inputs and outputs\n    fnames = [\"--source={}\".format(os.path.join(bpe_dir, PREFIX_TRAIN + SUFFIX_SRC_GZ)),\n              \"--target={}\".format(os.path.join(bpe_dir, PREFIX_TRAIN + SUFFIX_TRG_GZ)),\n              \"--validation-source={}\".format(os.path.join(bpe_dir, PREFIX_DEV + SUFFIX_SRC_GZ)),\n              \"--validation-target={}\".format(os.path.join(bpe_dir, PREFIX_DEV + SUFFIX_TRG_GZ)),\n              \"--output={}\".format(model_dir)]\n    # Assemble command\n    command = [sys.executable, \"-m\", \"sockeye.train\"] + fnames + MODELS[model]\n    # Request GPUs or specify CPU\n    if num_gpus > 0:\n        command.append(\"--device-ids=-{}\".format(num_gpus))\n    else:\n        command.append(\"--use-cpu\")\n    # Test mode trains a smaller model for a small number of steps\n    if test_mode:\n        command += MODEL_TEST_ARGS[model]\n    command_fname = os.path.join(model_dir, FILE_COMMAND.format(\"sockeye.train\"))\n    # Run unless training already finished\n    if not os.path.exists(command_fname):\n        # Call Sockeye training\n        with open(log_fname, \"wb\") as log:\n            logging.info(\"sockeye.train: %s\", model_dir)\n            logging.info(\"Log: %s\", log_fname)\n            logging.info(\"(This step can take several days. See log file or TensorBoard for progress)\")\n            subprocess.check_call(command, stderr=log)\n        # Record successful command\n        logging.info(\"Command: %s\", command_fname)\n        print_command(command, command_fname)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncall sockeye. average with reasonable defaults.", "response": "def call_sockeye_average(model_dir: str, log_fname: str):\n    \"\"\"\n    Call sockeye.average with reasonable defaults.\n\n    :param model_dir: Trained model directory.\n    :param log_fname: Location to write log file.\n    \"\"\"\n    params_best_fname = os.path.join(model_dir, C.PARAMS_BEST_NAME)\n    params_best_single_fname = os.path.join(model_dir, PARAMS_BEST_SINGLE)\n    params_average_fname = os.path.join(model_dir, PARAMS_AVERAGE)\n    command = [sys.executable,\n               \"-m\",\n               \"sockeye.average\",\n               \"--metric={}\".format(AVERAGE_METRIC),\n               \"-n\",\n               str(AVERAGE_NUM_CHECKPOINTS),\n               \"--output={}\".format(params_average_fname),\n               \"--strategy={}\".format(AVERAGE_STRATEGY),\n               model_dir]\n    command_fname = os.path.join(model_dir, FILE_COMMAND.format(\"sockeye.average\"))\n    # Run average if not previously run\n    if not os.path.exists(command_fname):\n        # Re-link best point to best single point\n        os.symlink(os.path.basename(os.path.realpath(params_best_fname)), params_best_single_fname)\n        os.remove(params_best_fname)\n        # Call Sockeye average\n        with open(log_fname, \"wb\") as log:\n            logging.info(\"sockeye.average: %s\", os.path.join(model_dir, params_best_fname))\n            logging.info(\"Log: %s\", log_fname)\n            subprocess.check_call(command, stderr=log)\n        # Link averaged point as new best\n        os.symlink(PARAMS_AVERAGE, params_best_fname)\n        # Record successful command\n        logging.info(\"Command: %s\", command_fname)\n        print_command(command, command_fname)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncall sockeye. translate with specified arguments using a trained model.", "response": "def call_sockeye_translate(args: List[str],\n                           input_fname: str,\n                           output_fname: str,\n                           model_dir: str,\n                           log_fname: str,\n                           use_cpu: bool):\n    \"\"\"\n    Call sockeye.translate with specified arguments using a trained model.\n\n    :param args: Command line arguments for sockeye.translate.\n    :param input_fname: Input file (byte-pair encoded).\n    :param output_fname: Raw decoder output file.\n    :param model_dir: Model output directory.\n    :param log_fname: Location to write log file.\n    :param use_cpu: Use CPU instead of GPU for decoding.\n    \"\"\"\n    # Inputs and outputs\n    fnames = [\"--input={}\".format(input_fname),\n              \"--output={}\".format(output_fname),\n              \"--models={}\".format(model_dir)]\n    # Assemble command\n    command = [sys.executable, \"-m\", \"sockeye.translate\"] + fnames + args\n    # Request GPUs or specify CPU\n    if use_cpu:\n        command.append(\"--use-cpu\")\n    command_fname = output_fname + \".\" + SUFFIX_COMMAND\n    # Run unless translate already finished\n    if not os.path.exists(command_fname):\n        # Call Sockeye translate\n        with open(log_fname, \"wb\") as log:\n            logging.info(\"sockeye.translate: %s -> %s\", input_fname, output_fname)\n            logging.info(\"Log: %s\", log_fname)\n            subprocess.check_call(command, stderr=log)\n        # Cleanup redundant log file\n        try:\n            os.remove(output_fname + \".log\")\n        except FileNotFoundError:\n            pass\n\n        # Record successful command\n        logging.info(\"Command: %s\", command_fname)\n        print_command(command, command_fname)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef call_sacrebleu(input_fname: str, ref_fname: str, output_fname: str, log_fname: str, tokenized: bool = False):\n    # Assemble command\n    command = [\"sacrebleu\",\n               \"--score-only\",\n               \"--input={}\".format(input_fname),\n               ref_fname]\n    # Already tokenized?\n    if tokenized:\n        command.append(\"--tokenize=none\")\n    # Call sacrebleu\n    with open(log_fname, \"wb\") as log:\n        logging.info(\"sacrebleu: %s -> %s\", input_fname, output_fname)\n        logging.info(\"Log: %s\", log_fname)\n        score = subprocess.check_output(command, stderr=log)\n    # Record successful score\n    with open(output_fname, \"wb\") as out:\n        out.write(score)", "response": "Call sacrebleu on tokenized or detokenized inputs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nformat and print command to file.", "response": "def print_command(command: List[str], fname: str):\n    \"\"\"\n    Format and print command to file.\n\n    :param command: Command in args list form.\n    :param fname: File name to write out.\n    \"\"\"\n    with open(fname, \"w\", encoding=\"utf-8\") as out:\n        print(\" \\\\\\n\".join(command), file=out)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run_steps(args: argparse.Namespace):\n\n    logging.basicConfig(level=logging.INFO, format=\"sockeye.autopilot: %(message)s\")\n\n    # (1) Establish task\n\n    logging.info(\"=== Start Autopilot ===\")\n    # Listed task\n    if args.task:\n        task = TASKS[args.task]\n        logging.info(\"Task: %s\", task.description)\n        logging.info(\"URL: %s\", task.url)\n\n        def report_data(file_sets):\n            for file_set in file_sets:\n                for fname in file_set[:2]:\n                    logging.info(\"    %s\", fname)\n\n        logging.info(\"  Train:\")\n        report_data(task.train)\n        logging.info(\"  Dev:\")\n        report_data(task.dev)\n        logging.info(\"  Test:\")\n        report_data(task.test)\n    # Custom task\n    else:\n        logging.info(\"Task: custom\")\n    # Source and target language codes\n    lang_codes = (task.src_lang, task.trg_lang) if args.task else args.custom_lang\n\n    # (2) Establish workspace and task directories\n\n    logging.info(\"=== Establish working directories ===\")\n    logging.info(\"Workspace: %s\", args.workspace)\n    special_fname = os.path.join(args.workspace, FILE_WORKSPACE)\n    if not os.path.exists(args.workspace):\n        logging.info(\"Create: %s\", args.workspace)\n        os.makedirs(args.workspace)\n        touch_file(special_fname)\n    else:\n        if not os.path.exists(special_fname):\n            raise RuntimeError(\"Directory %s exists but %s does not, stopping to avoid overwriting files in non-workspace directory\"\n                            % (args.workspace, special_fname))\n\n    dir_third_party = os.path.join(args.workspace, third_party.DIR_THIRD_PARTY)\n    dir_cache = os.path.join(args.workspace, DIR_CACHE)\n    dir_logs = os.path.join(args.workspace, DIR_LOGS)\n    dir_systems = os.path.join(args.workspace, DIR_SYSTEMS)\n    task_name = args.task if args.task else args.custom_task\n    if args.test:\n        task_name += SUFFIX_TEST\n    dir_task = os.path.join(dir_systems, task_name)\n    for dirname in (dir_third_party, dir_cache, dir_logs, dir_systems, dir_task):\n        if os.path.exists(dirname):\n            logging.info(\"Exists: %s\", dirname)\n        else:\n            logging.info(\"Create: %s\", dirname)\n            os.makedirs(dirname)\n\n    # (3) Checkout necessary tools\n\n    logging.info(\"=== Checkout third-party tools ===\")\n    # Requires tokenization?\n    if args.task or args.custom_text_type == CUSTOM_UTF8_RAW:\n        third_party.checkout_moses_tokenizer(args.workspace)\n    # Requires byte-pair encoding?\n    if args.task or args.custom_text_type in (CUSTOM_UTF8_RAW, CUSTOM_UTF8_TOK):\n        third_party.checkout_subword_nmt(args.workspace)\n\n    # (4) Populate train/dev/test data\n\n    # This step also normalizes whitespace on data population or copy, ensuring\n    # that for all input data, only ASCII newlines are considered line breaks.\n    logging.info(\"=== Populate train/dev/test data ===\")\n    step_dir_raw = os.path.join(dir_task, DIR_DATA, DIR_RAW)\n    complete_fname = os.path.join(step_dir_raw, FILE_COMPLETE)\n    if os.path.exists(complete_fname):\n        logging.info(\"Re-use completed step: %s\", step_dir_raw)\n    else:\n        # Listed task\n        if args.task:\n            raw_files = identify_raw_files(task, test_mode=args.test)\n            with tempfile.TemporaryDirectory(prefix=\"raw.\", dir=dir_task) as raw_dir:\n                # Download (or locate in cache) and extract raw files to temp directory\n                logging.info(\"=== Download and extract raw files ===\")\n                download_extract_raw_files(raw_files, dir_cache, raw_dir)\n                # Copy required files to train/dev/test\n                logging.info(\"=== Create input data files ===\")\n                renew_step_dir(step_dir_raw)\n                # Test mode uses the full test set as training data and the\n                # first line of the test set as dev and test data\n                populate_parallel_text(raw_dir,\n                                       task.test if args.test else task.train,\n                                       os.path.join(step_dir_raw, PREFIX_TRAIN),\n                                       False)\n                populate_parallel_text(raw_dir,\n                                       task.test if args.test else task.dev,\n                                       os.path.join(step_dir_raw, PREFIX_DEV),\n                                       False,\n                                       head_n=1 if args.test else 0)\n                populate_parallel_text(raw_dir,\n                                       task.test,\n                                       os.path.join(step_dir_raw, PREFIX_TEST),\n                                       True,\n                                       head_n=1 if args.test else 0)\n        # Custom task\n        else:\n            logging.info(\"=== Copy input data files ===\")\n            renew_step_dir(step_dir_raw)\n            copy_parallel_text(args.custom_train, os.path.join(step_dir_raw, PREFIX_TRAIN))\n            copy_parallel_text(args.custom_dev, os.path.join(step_dir_raw, PREFIX_DEV))\n            copy_parallel_text(args.custom_test, os.path.join(step_dir_raw, PREFIX_TEST))\n        # Record success\n        touch_file(complete_fname)\n        logging.info(\"Step complete: %s\", step_dir_raw)\n\n    # (5) Tokenize train/dev/test data\n\n    # Task requires tokenization if _any_ raw file is not already tokenized\n    requires_tokenization = False\n    if args.task:\n        for file_sets in (task.train, task.dev, task.test):\n            for _, _, text_type in file_sets:\n                if text_type in TEXT_REQUIRES_TOKENIZATION:\n                    requires_tokenization = True\n    else:\n        if args.custom_text_type == CUSTOM_UTF8_RAW:\n            requires_tokenization = True\n    logging.info(\"=== Tokenize train/dev/test data ===\")\n    step_dir_tok = os.path.join(dir_task, DIR_DATA, DIR_TOK)\n    complete_fname = os.path.join(step_dir_tok, FILE_COMPLETE)\n    if os.path.exists(complete_fname):\n        logging.info(\"Re-use completed step: %s\", step_dir_tok)\n    else:\n        renew_step_dir(step_dir_tok)\n\n        # Tokenize each data file using the appropriate language code OR link\n        # raw file if already tokenized.\n        for fname in os.listdir(step_dir_raw):\n            if fname.startswith(\".\"):\n                continue\n            input_fname = os.path.join(step_dir_raw, fname)\n            output_fname = os.path.join(step_dir_tok, fname)\n            if requires_tokenization:\n                lang_code = lang_codes[0] if fname.endswith(SUFFIX_SRC_GZ) else lang_codes[1]\n                logging.info(\"Tokenize (%s): %s -> %s\", lang_code, input_fname, output_fname)\n                third_party.call_moses_tokenizer(workspace_dir=args.workspace,\n                                                 input_fname=input_fname,\n                                                 output_fname=output_fname,\n                                                 lang_code=lang_code)\n            else:\n                logging.info(\"Link pre-tokenized: %s -> %s\", input_fname, output_fname)\n                os.symlink(os.path.join(\"..\", DIR_RAW, fname), output_fname)\n        # Record success\n        touch_file(complete_fname)\n        logging.info(\"Step complete: %s\", step_dir_tok)\n\n    # (6) Learn byte-pair encoding model\n\n    # Task requires byte-pair encoding unless using pre-encoded custom data\n    skip_bpe = (not args.task) and args.custom_text_type == CUSTOM_UTF8_BPE\n    logging.info(\"=== Learn byte-pair encoding model ===\")\n    step_dir_bpe_model = os.path.join(dir_task, DIR_BPE_MODEL)\n    complete_fname = os.path.join(step_dir_bpe_model, FILE_COMPLETE)\n    if os.path.exists(complete_fname):\n        logging.info(\"Re-use completed step: %s\", step_dir_bpe_model)\n    else:\n        renew_step_dir(step_dir_bpe_model)\n        if skip_bpe:\n            logging.info(\"BPE model not required for pre-encoded data\")\n        else:\n            source_fname = os.path.join(step_dir_tok, PREFIX_TRAIN + SUFFIX_SRC_GZ)\n            target_fname = os.path.join(step_dir_tok, PREFIX_TRAIN + SUFFIX_TRG_GZ)\n            codes_fname = os.path.join(step_dir_bpe_model, FILE_BPE_CODES)\n            num_ops = task.bpe_op if args.task else args.custom_bpe_op\n            if args.test:\n                num_ops = TEST_BPE_OPS\n            logging.info(\"BPE Learn (%s): %s + %s -> %s\", num_ops, source_fname, target_fname, codes_fname)\n            third_party.call_learn_bpe(workspace_dir=args.workspace,\n                                       source_fname=source_fname,\n                                       target_fname=target_fname,\n                                       model_fname=codes_fname,\n                                       num_ops=num_ops)\n        # Record success\n        touch_file(complete_fname)\n        logging.info(\"Step complete: %s\", step_dir_bpe_model)\n\n    # (7) Byte-pair encode data\n\n    logging.info(\"=== Byte-pair encode train/dev/test data ===\")\n    step_dir_bpe = os.path.join(dir_task, DIR_DATA, DIR_BPE)\n    complete_fname = os.path.join(step_dir_bpe, FILE_COMPLETE)\n    if os.path.exists(complete_fname):\n        logging.info(\"Re-use completed step: %s\", step_dir_bpe)\n    else:\n        renew_step_dir(step_dir_bpe)\n        # Encode each data file\n        for fname in os.listdir(step_dir_tok):\n            if fname.startswith(\".\"):\n                continue\n            input_fname = os.path.join(step_dir_tok, fname)\n            output_fname = os.path.join(step_dir_bpe, fname)\n            if skip_bpe:\n                logging.info(\"Link pre-encoded: %s -> %s\", input_fname, output_fname)\n                os.symlink(os.path.join(\"..\", DIR_TOK, fname), output_fname)\n            else:\n                codes_fname = os.path.join(step_dir_bpe_model, FILE_BPE_CODES)\n                logging.info(\"BPE: %s -> %s\", input_fname, output_fname)\n                third_party.call_apply_bpe(workspace_dir=args.workspace,\n                                           input_fname=input_fname,\n                                           output_fname=output_fname,\n                                           model_fname=codes_fname)\n        # Record success\n        touch_file(complete_fname)\n        logging.info(\"Step complete: %s\", step_dir_bpe)\n\n    # Done if only running data preparation steps\n    if args.model == MODEL_NONE:\n        return\n\n    # (8) Run Sockeye training\n\n    logging.info(\"=== Train translation model ===\")\n    logging.info(\"Model: %s\", args.model)\n    if args.model == MODEL_GNMT:\n        logging.info(\"NOTE: This is an 8 layer LSTM model similar (but not exactly identical) to the 'GNMT' architecture.\")\n    step_dir_model = os.path.join(dir_task, DIR_PREFIX_MODEL + args.model)\n    complete_fname = os.path.join(step_dir_model, FILE_COMPLETE)\n    if os.path.exists(complete_fname):\n        logging.info(\"Re-use completed step: %s\", step_dir_model)\n    else:\n        log_fname = os.path.join(args.workspace,\n                                 DIR_LOGS,\n                                 \"sockeye.{{}}.{}.{}.{}.log\".format(task_name, args.model, os.getpid()))\n        call_sockeye_train(args.model,\n                           step_dir_bpe,\n                           step_dir_model,\n                           log_fname.format(\"train\"),\n                           args.gpus,\n                           test_mode=args.test)\n        call_sockeye_average(step_dir_model, log_fname.format(\"average\"))\n        # Record success\n        touch_file(complete_fname)\n        logging.info(\"Step complete: %s\", step_dir_model)\n\n    # (9) Decode test sets\n\n    logging.info(\"=== Decode test sets ===\")\n    logging.info(\"Settings: %s\", args.decode_settings)\n    step_dir_results = os.path.join(dir_task, DIR_RESULTS)\n    if not os.path.exists(step_dir_results):\n        logging.info(\"Create: %s\", step_dir_results)\n        os.makedirs(step_dir_results)\n    # To collect BPE output names\n    output_fnames_bpe = []\n    # For each test file\n    for fname in os.listdir(step_dir_bpe):\n        if fname.startswith(PREFIX_TEST) and fname.endswith(SUFFIX_SRC_GZ):\n            input_fname = os.path.join(step_dir_bpe, fname)\n            # /path/to/results/test[.N].<model>.<settings>\n            output_fname = os.path.join(step_dir_results, \"{}.{}.{}.{}\".format(args.model,\n                                                                               args.decode_settings,\n                                                                               fname[:-len(SUFFIX_SRC_GZ) - 1],\n                                                                               SUFFIX_BPE))\n            output_fnames_bpe.append(output_fname)\n            # For the shared results directory, a command file indicates that\n            # the step has completed successfully.\n            command_fname = output_fname + \".\" + SUFFIX_COMMAND\n            if os.path.exists(command_fname):\n                logging.info(\"Re-use output: %s\", output_fname)\n            else:\n                log_fname = os.path.join(args.workspace,\n                                 DIR_LOGS,\n                                 \"sockeye.translate.{}.{}.{}.{}.log\".format(task_name,\n                                                                            args.model,\n                                                                            fname[:-len(SUFFIX_SRC_GZ) - 1],\n                                                                            os.getpid()))\n                call_sockeye_translate(args=DECODE_ARGS[args.decode_settings],\n                                       input_fname=input_fname,\n                                       output_fname=output_fname,\n                                       model_dir=step_dir_model,\n                                       log_fname=log_fname,\n                                       use_cpu=(args.gpus == 0))\n\n    # (10) Evaluate test sets (bpe/tok/detok)\n\n    lang_code = lang_codes[1] if lang_codes else None\n    logging.info(\"=== Score outputs ===\")\n    # For each output file\n    for fname_bpe in output_fnames_bpe:\n        # Score byte-pair encoded\n        fname_base = os.path.basename(fname_bpe)[:-len(SUFFIX_BPE)].split(\".\", 2)[2]\n        fname_ref_bpe = os.path.join(step_dir_bpe, fname_base + SUFFIX_TRG_GZ)\n        fname_bleu_bpe = fname_bpe + \".\" + SUFFIX_BLEU\n        if os.path.exists(fname_bleu_bpe):\n            logging.info(\"Re-use output: %s\", fname_bleu_bpe)\n        else:\n            fname_log = os.path.join(args.workspace,\n                                     DIR_LOGS,\n                                     \"sacrebleu.sacrebleu.{}.{}.{}.{}.log\".format(task_name,\n                                                                                  args.model,\n                                                                                  fname_base + SUFFIX_BPE,\n                                                                                  os.getpid()))\n            call_sacrebleu(input_fname=fname_bpe,\n                           ref_fname=fname_ref_bpe,\n                           output_fname=fname_bleu_bpe,\n                           log_fname=fname_log,\n                           tokenized=True)\n        # Score tokenized\n        fname_tok = fname_bpe[:-len(SUFFIX_BPE)] + SUFFIX_TOK\n        fname_ref_tok = os.path.join(step_dir_tok, fname_base + SUFFIX_TRG_GZ)\n        fname_bleu_tok = fname_tok + \".\" + SUFFIX_BLEU\n        if os.path.exists(fname_bleu_tok):\n            logging.info(\"Re-use output: %s\", fname_bleu_tok)\n        else:\n            # Merge BPE\n            logging.info(\"Merge BPE: %s -> %s\", fname_bpe, fname_tok)\n            third_party.merge_bpe(input_fname=fname_bpe, output_fname=fname_tok)\n            fname_log = os.path.join(args.workspace,\n                                     DIR_LOGS,\n                                     \"sacrebleu.sacrebleu.{}.{}.{}.{}.log\".format(task_name,\n                                                                                  args.model,\n                                                                                  fname_base + SUFFIX_TOK,\n                                                                                  os.getpid()))\n            call_sacrebleu(input_fname=fname_tok,\n                           ref_fname=fname_ref_tok,\n                           output_fname=fname_bleu_tok,\n                           log_fname=fname_log,\n                           tokenized=True)\n        # Score detokenized (WMT-compatible BLEU)\n        fname_detok = fname_bpe[:-len(SUFFIX_BPE)] + SUFFIX_DETOK\n        fname_ref_raw = os.path.join(step_dir_raw, fname_base + SUFFIX_TRG_GZ)\n        fname_bleu_detok = fname_detok + \".\" + SUFFIX_SACREBLEU\n        if os.path.exists(fname_bleu_detok):\n            logging.info(\"Re-use output: %s\", fname_bleu_detok)\n        else:\n            if not requires_tokenization:\n                logging.info(\n                    \"WARNING: Task uses pre-tokenized data, cannot reliably detokenize to compute WMT-compatible scores\")\n                continue\n            # Detokenize\n            logging.info(\"Detokenize (%s): %s -> %s\", lang_code, fname_tok, fname_detok)\n            third_party.call_moses_detokenizer(workspace_dir=args.workspace,\n                                               input_fname=fname_tok,\n                                               output_fname=fname_detok,\n                                               lang_code=lang_code)\n            fname_log = os.path.join(args.workspace,\n                                     DIR_LOGS,\n                                     \"sacrebleu.sacrebleu.{}.{}.{}.{}.log\".format(task_name,\n                                                                                  args.model,\n                                                                                  fname_base + SUFFIX_DETOK,\n                                                                                  os.getpid()))\n            call_sacrebleu(input_fname=fname_detok,\n                           ref_fname=fname_ref_raw,\n                           output_fname=fname_bleu_detok,\n                           log_fname=fname_log,\n                           tokenized=False)", "response": "Run all steps required to complete a single task."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninitializing vocabulary - sized weight by existing values given input and output vocabularies.", "response": "def init_weight(weight: np.ndarray,\n                vocab_in: Dict[str, int],\n                vocab_out: Dict[str, int],\n                initializer: mx.initializer.Initializer=mx.init.Constant(value=0.0)) -> mx.nd.NDArray:\n    \"\"\"\n    Initialize vocabulary-sized weight by existing values given input and output vocabularies.\n\n    :param weight: Input weight.\n    :param vocab_in: Input vocabulary.\n    :param vocab_out: Output vocabulary.\n    :param initializer: MXNet initializer.\n    :return: Initialized output weight.\n    \"\"\"\n    shape = list(weight.shape)\n    shape[0] = len(vocab_out)\n    weight_init = mx.nd.empty(tuple(shape), dtype='float32')\n    weight_desc = mx.init.InitDesc(\"vocabulary_sized_weight\")\n    initializer(weight_desc, weight_init)\n    for token in vocab_out:\n        if token in vocab_in:\n            weight_init[vocab_out[token]] = weight[vocab_in[token]]\n    return weight_init"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload a weight file or cache if it was loaded before.", "response": "def load_weight(weight_file: str,\n                weight_name: str,\n                weight_file_cache: Dict[str, Dict]) -> mx.nd.NDArray:\n    \"\"\"\n    Load wight fron a file or the cache if it was loaded before.\n\n    :param weight_file: Weight file.\n    :param weight_name: Weight name.\n    :param weight_file_cache: Cache of loaded files.\n    :return: Loaded weight.\n    \"\"\"\n    logger.info('Loading input weight file: %s', weight_file)\n    if weight_file.endswith(\".npy\"):\n        return np.load(weight_file)\n    elif weight_file.endswith(\".npz\"):\n        if weight_file not in weight_file_cache:\n            weight_file_cache[weight_file] = np.load(weight_file)\n        return weight_file_cache[weight_file][weight_name]\n    else:\n        if weight_file not in weight_file_cache:\n            weight_file_cache[weight_file] = mx.nd.load(weight_file)\n        return weight_file_cache[weight_file]['arg:%s' % weight_name].asnumpy()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef main():\n    setup_main_logger(console=True, file_logging=False)\n    params = argparse.ArgumentParser(description='Quick usage: python3 -m sockeye.init_embedding '\n                                                 '-w embed-in-src.npy embed-in-tgt.npy '\n                                                 '-i vocab-in-src.json vocab-in-tgt.json '\n                                                 '-o vocab-out-src.json vocab-out-tgt.json '\n                                                 '-n source_embed_weight target_embed_weight '\n                                                 '-f params.init')\n    arguments.add_init_embedding_args(params)\n    args = params.parse_args()\n    init_embeddings(args)", "response": "Commandline interface to initialize Sockeye embedding weights with pretrained word representations."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninitializing the model components creates the scoring symbol and module and binds it to the base class.", "response": "def _initialize(self,\n                    provide_data: List[mx.io.DataDesc],\n                    provide_label: List[mx.io.DataDesc],\n                    default_bucket_key: Tuple[int, int]) -> None:\n        \"\"\"\n        Initializes model components, creates scoring symbol and module, and binds it.\n\n        :param provide_data: List of data descriptors.\n        :param provide_label: List of label descriptors.\n        :param default_bucket_key: The default maximum (source, target) lengths.\n        \"\"\"\n        source = mx.sym.Variable(C.SOURCE_NAME)\n        source_words = source.split(num_outputs=self.config.config_embed_source.num_factors,\n                                    axis=2, squeeze_axis=True)[0]\n        source_length = utils.compute_lengths(source_words)\n        target = mx.sym.Variable(C.TARGET_NAME)\n        target_length = utils.compute_lengths(target)\n\n        # labels shape: (batch_size, target_length) (usually the maximum target sequence length)\n        labels = mx.sym.Variable(C.TARGET_LABEL_NAME)\n\n        data_names = [C.SOURCE_NAME, C.TARGET_NAME]\n        label_names = [C.TARGET_LABEL_NAME]\n\n        # check provide_{data,label} names\n        provide_data_names = [d[0] for d in provide_data]\n        utils.check_condition(provide_data_names == data_names,\n                              \"incompatible provide_data: %s, names should be %s\" % (provide_data_names, data_names))\n        provide_label_names = [d[0] for d in provide_label]\n        utils.check_condition(provide_label_names == label_names,\n                              \"incompatible provide_label: %s, names should be %s\" % (provide_label_names, label_names))\n\n        def sym_gen(seq_lens):\n            \"\"\"\n            Returns a (grouped) symbol containing the summed score for each sentence, as well as the entire target\n            distributions for each word.\n            Also returns data and label names for the BucketingModule.\n            \"\"\"\n            source_seq_len, target_seq_len = seq_lens\n\n            # source embedding\n            (source_embed,\n             source_embed_length,\n             source_embed_seq_len) = self.embedding_source.encode(source, source_length, source_seq_len)\n\n            # target embedding\n            (target_embed,\n             target_embed_length,\n             target_embed_seq_len) = self.embedding_target.encode(target, target_length, target_seq_len)\n\n            # encoder\n            # source_encoded: (batch_size, source_encoded_length, encoder_depth)\n            (source_encoded,\n             source_encoded_length,\n             source_encoded_seq_len) = self.encoder.encode(source_embed,\n                                                           source_embed_length,\n                                                           source_embed_seq_len)\n\n            # decoder\n            # target_decoded: (batch-size, target_len, decoder_depth)\n            target_decoded = self.decoder.decode_sequence(source_encoded, source_encoded_length, source_encoded_seq_len,\n                                                          target_embed, target_embed_length, target_embed_seq_len)\n\n            # output layer\n            # logits: (batch_size * target_seq_len, target_vocab_size)\n            logits = self.output_layer(mx.sym.reshape(data=target_decoded, shape=(-3, 0)))\n            # logits after reshape: (batch_size, target_seq_len, target_vocab_size)\n            logits = mx.sym.reshape(data=logits, shape=(-4, -1, target_embed_seq_len, 0))\n\n            if self.softmax_temperature is not None:\n                logits = logits / self.softmax_temperature\n\n            # Compute the softmax along the final dimension.\n            # target_dists: (batch_size, target_seq_len, target_vocab_size)\n            target_dists = mx.sym.softmax(data=logits, axis=2, name=C.SOFTMAX_NAME)\n\n            # Select the label probability, then take their logs.\n            # probs and scores: (batch_size, target_seq_len)\n            probs = mx.sym.pick(target_dists, labels)\n            scores = mx.sym.log(probs)\n            if self.score_type == C.SCORING_TYPE_NEGLOGPROB:\n                scores = -1 * scores\n\n            # Sum, then apply length penalty. The call to `mx.sym.where` masks out invalid values from scores.\n            # zeros and sums: (batch_size,)\n            zeros = mx.sym.zeros_like(scores)\n            sums = mx.sym.sum(mx.sym.where(labels != 0, scores, zeros), axis=1) / (self.length_penalty(target_length - 1))\n\n            # Deal with the potential presence of brevity penalty\n            # length_ratio: (batch_size,)\n            if self.constant_length_ratio > 0.0:\n                # override all ratios with the constant value\n                length_ratio = self.constant_length_ratio * mx.sym.ones_like(sums)\n            else:\n                # predict length ratio if supported\n                length_ratio = self.length_ratio(source_encoded, source_encoded_length).reshape((-1,)) \\\n                                    if self.length_ratio is not None else mx.sym.zeros_like(sums)\n            sums = sums - self.brevity_penalty(target_length - 1, length_ratio * source_encoded_length)\n\n            # Return the sums and the target distributions\n            # sums: (batch_size,) target_dists: (batch_size, target_seq_len, target_vocab_size)\n            return mx.sym.Group([sums, target_dists]), data_names, label_names\n\n        symbol, _, __ = sym_gen(default_bucket_key)\n        self.module = mx.mod.Module(symbol=symbol,\n                                    data_names=data_names,\n                                    label_names=label_names,\n                                    logger=logger,\n                                    context=self.context)\n\n        self.module.bind(data_shapes=provide_data,\n                         label_shapes=provide_label,\n                         for_training=False,\n                         force_rebind=False,\n                         grad_req='null')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrunning the forward pass and returns the outputs.", "response": "def run(self, batch: mx.io.DataBatch) -> List[mx.nd.NDArray]:\n        \"\"\"\n        Runs the forward pass and returns the outputs.\n\n        :param batch: The batch to run.\n        :return: The grouped symbol (probs and target dists) and lists containing the data names and label names.\n        \"\"\"\n        self.module.forward(batch, is_train=False)\n        return self.module.get_outputs()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a mask for variable sequence lengths.", "response": "def get_valid_length_mask_for(data: mx.sym.Symbol,\n                              lengths: mx.sym.Symbol,\n                              num_heads: Optional[int] = None,\n                              fold_heads: bool = True,\n                              name: str = '') -> mx.sym.Symbol:\n    \"\"\"\n    Returns bias/mask for variable sequence lengths.\n    :param data: Input data to mask. Shape: (batch, seq_len, _).\n    :param lengths: Sequence lengths. Shape: (batch,).\n    :param num_heads: Number of attention heads.\n    :param fold_heads: Whether to fold heads dimension into batch dimension.\n    :param name: Name of symbol.\n    :return: Bias symbol. Shape: (batch, seq_len)\n    \"\"\"\n    if mx.__version__.startswith(\"1.3\"):\n        # TODO(fhieber): remove old branch eventually\n        # mxnet 1.3.1's broadcast_like operator does not support individual axes yet. This branch uses another way\n        # of creating the required zeros array.\n        # (batch, seq_len)\n        zeros = mx.sym.sum(mx.sym.zeros_like(data), axis=2, keepdims=False)\n    else:\n        # (batch, 1)\n        zeros = mx.sym.reshape(mx.sym.zeros_like(lengths), shape=(-1, 1))\n        # (batch, seq_len)\n        zeros = mx.sym.broadcast_like(zeros, data, lhs_axes=(1,), rhs_axes=(1,))\n    # (batch_size, max_length)\n    x = mx.sym.SequenceMask(data=zeros,\n                            use_sequence_length=True,\n                            sequence_length=lengths,\n                            axis=1,\n                            value=C.LARGE_NEGATIVE_VALUE)\n\n    if num_heads is not None:\n        # (batch_size, heads, max_length) if fold_heads == False else (batch_size * heads, max_length)\n        x = layers.broadcast_to_heads(x, num_heads, ndim=2, fold_heads=fold_heads)\n    return mx.sym.BlockGrad(x, name='%sbias' % name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a Bias symbol that can be used to automatically generate positions.", "response": "def get_autoregressive_bias(max_length: int, dtype: str = C.DTYPE_FP32) -> mx.sym.Symbol:\n    \"\"\"\n    Returns bias/mask to ensure position i can only attend to positions <i.\n\n    :param max_length: Sequence length.\n    :param dtype: dtype of bias\n    :return: Bias symbol of shape (1, max_length, max_length).\n    \"\"\"\n    length_array = mx.sym.arange(max_length, dtype=dtype)\n    # matrix with lower triangle and main diagonal set to 0, upper triangle set to 1\n    bias = mx.sym.broadcast_greater(mx.sym.reshape(length_array, shape=(1, -1)),\n                                    mx.sym.reshape(length_array, shape=(-1, 1)))\n    bias = bias * -C.LARGE_VALUES[dtype]\n    bias = mx.sym.reshape(bias, shape=(1, max_length, max_length))\n    return mx.sym.BlockGrad(bias)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nbuild a vocabulary from a list of paths to a file in sentence - per - line format.", "response": "def build_from_paths(paths: List[str], num_words: Optional[int] = None, min_count: int = 1,\n                     pad_to_multiple_of: Optional[int] = None) -> Vocab:\n    \"\"\"\n    Creates vocabulary from paths to a file in sentence-per-line format. A sentence is just a whitespace delimited\n    list of tokens. Note that special symbols like the beginning of sentence (BOS) symbol will be added to the\n    vocabulary.\n\n    :param paths: List of paths to files with one sentence per line.\n    :param num_words: Optional maximum number of words in the vocabulary.\n    :param min_count: Minimum occurrences of words to be included in the vocabulary.\n    :param pad_to_multiple_of: If not None, pads the vocabulary to a size that is the next multiple of this int.\n    :return: Word-to-id mapping.\n    \"\"\"\n    with ExitStack() as stack:\n        logger.info(\"Building vocabulary from dataset(s): %s\", paths)\n        files = (stack.enter_context(utils.smart_open(path)) for path in paths)\n        return build_vocab(chain(*files), num_words, min_count, pad_to_multiple_of)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbuilding a vocabulary mapping from words to ids.", "response": "def build_vocab(data: Iterable[str], num_words: Optional[int] = None, min_count: int = 1,\n                pad_to_multiple_of: Optional[int] = None) -> Vocab:\n    \"\"\"\n    Creates a vocabulary mapping from words to ids. Increasing integer ids are assigned by word frequency,\n    using lexical sorting as a tie breaker. The only exception to this are special symbols such as the padding symbol\n    (PAD).\n\n    :param data: Sequence of sentences containing whitespace delimited tokens.\n    :param num_words: Optional maximum number of words in the vocabulary.\n    :param min_count: Minimum occurrences of words to be included in the vocabulary.\n    :param pad_to_multiple_of: If not None, pads the vocabulary to a size that is the next multiple of this int.\n    :return: Word-to-id mapping.\n    \"\"\"\n    vocab_symbols_set = set(C.VOCAB_SYMBOLS)\n    raw_vocab = Counter(token for line in data for token in utils.get_tokens(line)\n                        if token not in vocab_symbols_set)\n    # For words with the same count, they will be ordered reverse alphabetically.\n    # Not an issue since we only care for consistency\n    pruned_vocab = [w for c, w in sorted(((c, w) for w, c in raw_vocab.items() if c >= min_count), reverse=True)]\n\n    if num_words is not None:\n        vocab = list(islice(pruned_vocab, num_words))\n        num_words_log = str(num_words)\n    else:\n        vocab = pruned_vocab\n        num_words_log = \"None\"\n\n    if pad_to_multiple_of is not None:\n        current_vocab_size = len(vocab) + len(C.VOCAB_SYMBOLS)\n        rest = current_vocab_size % pad_to_multiple_of\n        padded_vocab_size = current_vocab_size if rest == 0 else current_vocab_size + pad_to_multiple_of - rest\n        logger.info(\"Padding vocabulary to a multiple of %d: %d -> %d\",\n                    pad_to_multiple_of, current_vocab_size, padded_vocab_size)\n        pad_entries = [C.PAD_FORMAT % idx for idx in range(current_vocab_size, padded_vocab_size)]\n        pad_to_multiple_log = str(pad_to_multiple_of)\n    else:\n        pad_entries = []\n        pad_to_multiple_log = \"None\"\n\n    word_to_id = {word: idx for idx, word in enumerate(chain(C.VOCAB_SYMBOLS, vocab, pad_entries))}\n    logger.info(\"Vocabulary: types: %d/%d/%d/%d (initial/min_pruned/max_pruned/+special) \" +\n                \"[min_frequency=%d, max_num_types=%s, pad_to_multiple_of=%s]\",\n                len(raw_vocab), len(pruned_vocab), len(vocab),\n                len(word_to_id), min_count, num_words_log, pad_to_multiple_log)\n\n    # Important: pad symbol becomes index 0\n    assert word_to_id[C.PAD_SYMBOL] == C.PAD_ID\n    return word_to_id"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsaves vocabulary in human - readable json.", "response": "def vocab_to_json(vocab: Vocab, path: str):\n    \"\"\"\n    Saves vocabulary in human-readable json.\n\n    :param vocab: Vocabulary mapping.\n    :param path: Output file path.\n    \"\"\"\n    with open(path, \"w\", encoding=C.VOCAB_ENCODING) as out:\n        json.dump(vocab, out, indent=4, ensure_ascii=False)\n        logger.info('Vocabulary saved to \"%s\"', path)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_valid_vocab(vocab: Vocab) -> bool:\n    for symbol in [C.PAD_SYMBOL, C.UNK_SYMBOL, C.BOS_SYMBOL, C.EOS_SYMBOL]:\n        if symbol not in vocab:\n            logger.warning(\"%s missing from vocabulary.\", symbol)\n            return False\n    if vocab[C.PAD_SYMBOL] != 0:\n        logger.warning(\"PAD_ID does not have word id 0 in vocabulary.\")\n        return False\n    word_ids = []\n    for word, word_id in vocab.items():\n        word_ids.append(word_id)\n    word_ids_set = set(word_ids)\n    if len(word_ids_set) != len(word_ids):\n        logger.warning(\"Duplicate word_ids in vocabulary.\")\n        return False\n\n    expected_word_ids = set(range(0, len(vocab)))\n    if expected_word_ids != word_ids_set:\n        logger.warning(\"Not all word_ids from 0 to len(vocabulary) present in vocabulary.\")\n        return False\n\n    return True", "response": "Checks if a vocabulary is valid."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads a vocabulary from json file.", "response": "def vocab_from_json(path: str, encoding: str = C.VOCAB_ENCODING) -> Vocab:\n    \"\"\"\n    Saves vocabulary in json format.\n\n    :param path: Path to json file containing the vocabulary.\n    :param encoding: Vocabulary encoding.\n    :return: The loaded vocabulary.\n    \"\"\"\n    with open(path, encoding=encoding) as inp:\n        vocab = json.load(inp)\n        utils.check_condition(is_valid_vocab(vocab), \"Vocabulary %s not valid.\" % path)\n        logger.info('Vocabulary (%d words) loaded from \"%s\"', len(vocab), path)\n        return vocab"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsave source vocabularies to folder.", "response": "def save_source_vocabs(source_vocabs: List[Vocab], folder: str):\n    \"\"\"\n    Saves source vocabularies (primary surface form vocabulary) and optional factor vocabularies to folder.\n\n    :param source_vocabs: List of source vocabularies.\n    :param folder: Destination folder.\n    \"\"\"\n    for i, vocab in enumerate(source_vocabs):\n        vocab_to_json(vocab, os.path.join(folder, C.VOCAB_SRC_NAME % i))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef save_target_vocab(target_vocab: Vocab, folder: str):\n    vocab_to_json(target_vocab, os.path.join(folder, C.VOCAB_TRG_NAME % 0))", "response": "Saves target vocabulary to folder."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load_source_vocabs(folder: str) -> List[Vocab]:\n    return [vocab_from_json(os.path.join(folder, fname)) for fname in\n            sorted([f for f in os.listdir(folder) if f.startswith(C.VOCAB_SRC_PREFIX)])]", "response": "Loads source vocabularies from folder."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_target_vocab(folder: str) -> Vocab:\n    return vocab_from_json(os.path.join(folder, C.VOCAB_TRG_NAME % 0))", "response": "Loads target vocabulary from folder."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads or create a vocabulary from a JSON file.", "response": "def load_or_create_vocab(data: str, vocab_path: Optional[str], num_words: int, word_min_count: int,\n                         pad_to_multiple_of: Optional[int] = None) -> Vocab:\n    \"\"\"\n    If the vocabulary path is defined, the vocabulary is loaded from the path.\n    Otherwise, it is built from the data file. No writing to disk occurs.\n    \"\"\"\n    if vocab_path is None:\n        return build_from_paths(paths=[data], num_words=num_words, min_count=word_min_count,\n                                pad_to_multiple_of=pad_to_multiple_of)\n    else:\n        return vocab_from_json(vocab_path)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_or_create_vocabs(source_paths: List[str],\n                          target_path: str,\n                          source_vocab_paths: List[Optional[str]],\n                          target_vocab_path: Optional[str],\n                          shared_vocab: bool,\n                          num_words_source: Optional[int], word_min_count_source: int,\n                          num_words_target: Optional[int], word_min_count_target: int,\n                          pad_to_multiple_of: Optional[int] = None) -> Tuple[List[Vocab], Vocab]:\n    \"\"\"\n    Returns vocabularies for source files (including factors) and target.\n    If the respective vocabulary paths are not None, the vocabulary is read from the path and returned.\n    Otherwise, it is built from the support and saved to the path.\n\n    :param source_paths: The path to the source text (and optional token-parallel factor files).\n    :param target_path: The target text.\n    :param source_vocab_paths: The source vocabulary path (and optional factor vocabulary paths).\n    :param target_vocab_path: The target vocabulary path.\n    :param shared_vocab: Whether the source and target vocabularies are shared.\n    :param num_words_source: Number of words in the source vocabulary.\n    :param word_min_count_source: Minimum frequency of words in the source vocabulary.\n    :param num_words_target: Number of words in the target vocabulary.\n    :param word_min_count_target: Minimum frequency of words in the target vocabulary.\n    :param pad_to_multiple_of: If not None, pads the vocabularies to a size that is the next multiple of this int.\n    :return: List of source vocabularies (for source and factors), and target vocabulary.\n    \"\"\"\n    source_path, *source_factor_paths = source_paths\n    source_vocab_path, *source_factor_vocab_paths = source_vocab_paths\n\n    logger.info(\"=============================\")\n    logger.info(\"Loading/creating vocabularies\")\n    logger.info(\"=============================\")\n    logger.info(\"(1) Surface form vocabularies (source & target)\")\n\n    if shared_vocab:\n        if source_vocab_path and target_vocab_path:\n            vocab_source = vocab_from_json(source_vocab_path)\n            vocab_target = vocab_from_json(target_vocab_path)\n            utils.check_condition(are_identical(vocab_source, vocab_target),\n                                  \"Shared vocabulary requires identical source and target vocabularies. \"\n                                  \"The vocabularies in %s and %s are not identical.\" % (source_vocab_path,\n                                                                                        target_vocab_path))\n\n        elif source_vocab_path is None and target_vocab_path is None:\n            utils.check_condition(num_words_source == num_words_target,\n                                  \"A shared vocabulary requires the number of source and target words to be the same.\")\n            utils.check_condition(word_min_count_source == word_min_count_target,\n                                  \"A shared vocabulary requires the minimum word count for source and target \"\n                                  \"to be the same.\")\n            vocab_source = vocab_target = build_from_paths(paths=[source_path, target_path],\n                                                           num_words=num_words_source,\n                                                           min_count=word_min_count_source,\n                                                           pad_to_multiple_of=pad_to_multiple_of)\n\n        else:\n            vocab_path = source_vocab_path if source_vocab_path is not None else target_vocab_path\n            logger.info(\"Using %s as a shared source/target vocabulary.\" % vocab_path)\n            vocab_source = vocab_target = vocab_from_json(vocab_path)\n\n    else:\n        vocab_source = load_or_create_vocab(source_path, source_vocab_path, num_words_source, word_min_count_source,\n                                            pad_to_multiple_of=pad_to_multiple_of)\n        vocab_target = load_or_create_vocab(target_path, target_vocab_path, num_words_target, word_min_count_target,\n                                            pad_to_multiple_of=pad_to_multiple_of)\n\n    vocab_source_factors = []  # type: List[Vocab]\n    if source_factor_paths:\n        logger.info(\"(2) Additional source factor vocabularies\")\n        # source factor vocabs are always created\n        for factor_path, factor_vocab_path in zip(source_factor_paths, source_factor_vocab_paths):\n            vocab_source_factors.append(load_or_create_vocab(factor_path, factor_vocab_path,\n                                                             num_words_source, word_min_count_source))\n\n    return [vocab_source] + vocab_source_factors, vocab_target", "response": "Loads source and target vocabularies from source and target files."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef reverse_vocab(vocab: Vocab) -> InverseVocab:\n    return {v: k for k, v in vocab.items()}", "response": "Returns value - to - key mapping from key - to - value - mapping."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_ordered_tokens_from_vocab(vocab: Vocab) -> List[str]:\n    return [token for token, token_id in sorted(vocab.items(), key=lambda i: i[1])]", "response": "Returns the list of tokens in a vocabulary ordered by increasing vocabulary id."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef make_inputs(input_file: Optional[str],\n                translator: inference.Translator,\n                input_is_json: bool,\n                input_factors: Optional[List[str]] = None) -> Generator[inference.TranslatorInput, None, None]:\n    \"\"\"\n    Generates TranslatorInput instances from input. If input is None, reads from stdin. If num_input_factors > 1,\n    the function will look for factors attached to each token, separated by '|'.\n    If source is not None, reads from the source file. If num_source_factors > 1, num_source_factors source factor\n    filenames are required.\n\n    :param input_file: The source file (possibly None).\n    :param translator: Translator that will translate each line of input.\n    :param input_is_json: Whether the input is in json format.\n    :param input_factors: Source factor files.\n    :return: TranslatorInput objects.\n    \"\"\"\n    if input_file is None:\n        check_condition(input_factors is None, \"Translating from STDIN, not expecting any factor files.\")\n        for sentence_id, line in enumerate(sys.stdin, 1):\n            if input_is_json:\n                yield inference.make_input_from_json_string(sentence_id=sentence_id,\n                                                            json_string=line,\n                                                            translator=translator)\n            else:\n                yield inference.make_input_from_factored_string(sentence_id=sentence_id,\n                                                                factored_string=line,\n                                                                translator=translator)\n    else:\n        input_factors = [] if input_factors is None else input_factors\n        inputs = [input_file] + input_factors\n        if not input_is_json:\n            check_condition(translator.num_source_factors == len(inputs),\n                            \"Model(s) require %d factors, but %d given (through --input and --input-factors).\" % (\n                                translator.num_source_factors, len(inputs)))\n        with ExitStack() as exit_stack:\n            streams = [exit_stack.enter_context(data_io.smart_open(i)) for i in inputs]\n            for sentence_id, inputs in enumerate(zip(*streams), 1):\n                if input_is_json:\n                    yield inference.make_input_from_json_string(sentence_id=sentence_id,\n                                                                json_string=inputs[0],\n                                                                translator=translator)\n                else:\n                    yield inference.make_input_from_multiple_strings(sentence_id=sentence_id, strings=list(inputs))", "response": "Generates TranslatorInput instances from input."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read_and_translate(translator: inference.Translator,\n                       output_handler: OutputHandler,\n                       chunk_size: Optional[int],\n                       input_file: Optional[str] = None,\n                       input_factors: Optional[List[str]] = None,\n                       input_is_json: bool = False) -> None:\n    \"\"\"\n    Reads from either a file or stdin and translates each line, calling the output_handler with the result.\n\n    :param output_handler: Handler that will write output to a stream.\n    :param translator: Translator that will translate each line of input.\n    :param chunk_size: The size of the portion to read at a time from the input.\n    :param input_file: Optional path to file which will be translated line-by-line if included, if none use stdin.\n    :param input_factors: Optional list of paths to files that contain source factors.\n    :param input_is_json: Whether the input is in json format.\n    \"\"\"\n    batch_size = translator.max_batch_size\n    if chunk_size is None:\n        if translator.max_batch_size == 1:\n            # No batching, therefore there is not need to read segments in chunks.\n            chunk_size = C.CHUNK_SIZE_NO_BATCHING\n        else:\n            # Get a constant number of batches per call to Translator.translate.\n            chunk_size = C.CHUNK_SIZE_PER_BATCH_SEGMENT * translator.max_batch_size\n    else:\n        if chunk_size < translator.max_batch_size:\n            logger.warning(\"You specified a chunk size (%d) smaller than the max batch size (%d). This will lead to \"\n                           \"a reduction in translation speed. Consider choosing a larger chunk size.\" % (chunk_size,\n                                                                                                         batch_size))\n\n    logger.info(\"Translating...\")\n\n    total_time, total_lines = 0.0, 0\n    for chunk in grouper(make_inputs(input_file, translator, input_is_json, input_factors), size=chunk_size):\n        chunk_time = translate(output_handler, chunk, translator)\n        total_lines += len(chunk)\n        total_time += chunk_time\n\n    if total_lines != 0:\n        logger.info(\"Processed %d lines. Total time: %.4f, sec/sent: %.4f, sent/sec: %.4f\",\n                    total_lines, total_time, total_time / total_lines, total_lines / total_time)\n    else:\n        logger.info(\"Processed 0 lines.\")", "response": "Reads from either a file or stdin and translates each line of input into a single file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef translate(output_handler: OutputHandler,\n              trans_inputs: List[inference.TranslatorInput],\n              translator: inference.Translator) -> float:\n    \"\"\"\n    Translates each line from source_data, calling output handler after translating a batch.\n\n    :param output_handler: A handler that will be called once with the output of each translation.\n    :param trans_inputs: A enumerable list of translator inputs.\n    :param translator: The translator that will be used for each line of input.\n    :return: Total time taken.\n    \"\"\"\n    tic = time.time()\n    trans_outputs = translator.translate(trans_inputs)\n    total_time = time.time() - tic\n    batch_time = total_time / len(trans_inputs)\n    for trans_input, trans_output in zip(trans_inputs, trans_outputs):\n        output_handler.handle(trans_input, trans_output, batch_time)\n    return total_time", "response": "Translate each line from source_data calling output handler after translating each batch."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef step(self, data):\n\n        # As we only run convolution over a single window that is exactly the size of the convolutional filter\n        # we can use FullyConnected instead of Convolution for efficiency reasons. Additionally we do not need to\n        # perform any masking.\n\n        num_hidden = self._pre_activation_num_hidden()\n\n        # (batch_size, num_hidden, kernel_width)\n        data = mx.sym.swapaxes(data, dim1=1, dim2=2)\n        # (batch_size, num_hidden * kernel_width)\n        data = mx.sym.reshape(data, shape=(0, -3))\n        # (preact_num_hidden, num_hidden * kernel_width)\n        weight = mx.sym.reshape(self.conv_weight, shape=(0, -3))\n        data_conv = mx.sym.FullyConnected(data=data,\n                                          weight=weight,\n                                          bias=self.conv_bias,\n                                          num_hidden=num_hidden)\n        # (batch_size, num_hidden, 1)\n        data_conv = mx.sym.expand_dims(data_conv, axis=2)\n        return self._post_convolution(data_conv)", "response": "Run convolution over a single position."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef benchmark(cores, args):\n    model = args.module\n    fileInput = args.input_file\n    fileOutput = args.output_file\n    batchsize = args.batch_size\n   \n    thread = []\n    for i in range(cores): \n        command = \"taskset -c %d-%d python3 -m sockeye.translate -m %s -i %s -o %s --batch-size %d --output-type benchmark --use-cpu > /dev/null 2>&1 \" % (i, i, model, fileInput, fileOutput, batchsize)\n        t = threading.Thread(target = task, args=(command,))\n        thread.append(t)\n        t.start()\n    \n    for t in thread:\n        t.join()", "response": "This function is used for processing per core translation. Each core translates the whole input file. Each core will launch a thread to translate the whole input file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef split_file(splitNum, fileInput, lines):\n    quot = lines // splitNum\n    rema = lines % splitNum\n    files = []\n    current_line = 0\n    for i in range(splitNum):\n        if i < rema:\n            read_line = quot + 1\n        else:\n            read_line = quot\n        temp = tempfile.NamedTemporaryFile()\n        os.system(\"head -n%d %s| tail -n%d > %s\" % (current_line + read_line, fileInput, read_line, temp.name))\n        current_line += read_line\n        files.append(temp)\n\n    return files", "response": "split_file is used to split fileInput into splitNum small pieces file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef translate(cores, files, args):\n    model = args.module\n    batchsize = args.batch_size\n\n    # split inputfile to a series of small files which number is equal cores\n    file = []\n    thread = []\n    for i in range(cores):\n        files[i].seek(0)\n        temp = tempfile.NamedTemporaryFile()\n        command = \"taskset -c %d-%d python3 -m sockeye.translate -m %s -i %s -o %s --batch-size %d --output-type benchmark --use-cpu > /dev/null 2>&1 \" % (i, i, model, files[i].name, temp.name, batchsize)\n        file.append(temp)\n\n        t = threading.Thread(target = task, args=(command,))\n        thread.append(t) \n        t.start()\n    #wait for all translation done\n    for t in thread:\n        t.join()\n    \n    return file", "response": "This function is used for processing per core translation."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nindents util function compute new indent_string", "response": "def _indent(indent=0, quote='', indent_char=' '):\n    \"\"\"Indent util function, compute new indent_string\"\"\"\n    if indent > 0:\n        indent_string = ''.join((\n            str(quote),\n            (indent_char * (indent - len(quote)))\n        ))\n    else:\n        indent_string = ''.join((\n            ('\\x08' * (-1 * (indent - len(quote)))),\n            str(quote))\n        )\n\n    if len(indent_string):\n        INDENT_STRINGS.append(indent_string)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprint given string to stdout.", "response": "def puts(s='', newline=True, stream=STDOUT):\n    \"\"\"Prints given string to stdout.\"\"\"\n    max_width_ctx = _get_max_width_context()\n    if max_width_ctx:\n        cols, separator = max_width_ctx[-1]\n        s = max_width(s, cols, separator)\n\n    if newline:\n        s = tsplit(s, NEWLINES)\n        s = map(str, s)\n        indent = ''.join(INDENT_STRINGS)\n\n        s = (str('\\n' + indent)).join(s)\n\n    _str = ''.join((\n        ''.join(INDENT_STRINGS),\n        str(s),\n        '\\n' if newline else ''\n    ))\n    stream(_str)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprints given string to stderr.", "response": "def puts_err(s='', newline=True, stream=STDERR):\n    \"\"\"Prints given string to stderr.\"\"\"\n    puts(s, newline, stream)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate current AppDir at AppDir. path.", "response": "def _create(self):\n        \"\"\"Creates current AppDir at AppDir.path.\"\"\"\n\n        self._raise_if_none()\n        if not self._exists:\n            mkdir_p(self.path)\n            self._exists = True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a file object from given filename.", "response": "def open(self, filename, mode='r'):\n        \"\"\"Returns file object from given filename.\"\"\"\n\n        self._raise_if_none()\n        fn = path_join(self.path, filename)\n\n        return open(fn, mode)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nappend given content to given filename.", "response": "def append(self, filename, content, binary=False):\n        \"\"\"Appends given content to given filename.\"\"\"\n\n        self._raise_if_none()\n        fn = path_join(self.path, filename)\n\n        if binary:\n            flags = 'ab'\n        else:\n            flags = 'a'\n\n        with open(fn, 'a') as f:\n            f.write(content)\n            return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delete(self, filename=''):\n        self._raise_if_none()\n        fn = path_join(self.path, filename)\n\n        try:\n            if isfile(fn):\n                remove(fn)\n            else:\n                removedirs(fn)\n        except OSError as why:\n            if why.errno == errno.ENOENT:\n                pass\n            else:\n                raise why", "response": "Deletes given file or directory."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read(self, filename, binary=False):\n\n        self._raise_if_none()\n        fn = path_join(self.path, filename)\n\n        if binary:\n            flags = 'br'\n        else:\n            flags = 'r'\n\n        try:\n            with open(fn, flags) as f:\n                return f.read()\n        except IOError:\n            return None", "response": "Reads contents of given file with AppDir.\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning AppDir instance for given subdirectory name.", "response": "def sub(self, path):\n        \"\"\"Returns AppDir instance for given subdirectory name.\"\"\"\n\n        if is_collection(path):\n            path = path_join(path)\n\n        return AppDir(path_join(self.path, path))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef first_with(self, x):\n\n        def _find(x):\n            try:\n                for arg in self.all:\n                    if x in arg:\n                        return self.all.index(arg)\n            except ValueError:\n                return None\n\n        if is_collection(x):\n            for item in x:\n                found = _find(item)\n                if found:\n                    return found\n            return None\n        else:\n            return _find(x)", "response": "Returns first found index containing value or list of values"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns all arguments beginning with given string or list thereof", "response": "def start_with(self, x):\n           \"\"\"Returns all arguments beginning with given string (or list thereof)\"\"\"\n\n           _args = []\n\n           for arg in self.all:\n               if is_collection(x):\n                   for _x in x:\n                       if arg.startswith(x):\n                           _args.append(arg)\n                           break\n               else:\n                   if arg.startswith(x):\n                       _args.append(arg)\n\n           return Args(_args, no_argv=True)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntest if given list of string is at given index.", "response": "def contains_at(self, x, index):\n        \"\"\"Tests if given [list of] string is at given index.\"\"\"\n\n        try:\n            if is_collection(x):\n                for _x in x:\n                    if (_x in self.all[index]) or (_x == self.all[index]):\n                        return True\n                    else:\n                        return False\n            else:\n                return (x in self.all[index])\n\n        except IndexError:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning value of argument after given found argument or list thereof.", "response": "def value_after(self, x):\n        \"\"\"Returns value of argument after given found argument (or list thereof).\"\"\"\n\n        try:\n            try:\n                i = self.all.index(x)\n            except ValueError:\n                return None\n\n            return self.all[i + 1]\n\n        except IndexError:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nextracting all flag groups from argument list.", "response": "def grouped(self):\n        \"\"\"Extracts --flag groups from argument list.\n           Returns {format: Args, ...}\n        \"\"\"\n\n        collection = OrderedDict(_=Args(no_argv=True))\n\n        _current_group = None\n\n        for arg in self.all:\n            if arg.startswith('-'):\n                _current_group = arg\n                collection.setdefault(arg, Args(no_argv=True))\n            else:\n                if _current_group:\n                    collection[_current_group]._args.append(arg)\n                else:\n                    collection['_']._args.append(arg)\n\n        return collection"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns all arguments containing given string or list thereof", "response": "def all_with(self, x):\n        \"\"\"Returns all arguments containing given string (or list thereof)\"\"\"\n\n        _args = []\n\n        for arg in self.all:\n            if is_collection(x):\n                for _x in x:\n                    if _x in arg:\n                        _args.append(arg)\n                        break\n            else:\n                if x in arg:\n                    _args.append(arg)\n\n        return Args(_args, no_argv=True)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef not_files(self):\n\n        _args = []\n\n        for arg in self.all:\n            if not len(expand_path(arg)):\n                if not os.path.exists(arg):\n                    _args.append(arg)\n\n        return Args(_args, no_argv=True)", "response": "Returns a list of all arguments that aren t files or globs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef expand_path(path):\n\n    paths = []\n    path = os.path.expanduser(path)\n    path = os.path.expandvars(path)\n\n    if os.path.isdir(path):\n\n        for (dir, dirs, files) in os.walk(path):\n            for file in files:\n                paths.append(os.path.join(dir, file))\n    else:\n        paths.extend(glob(path))\n\n    return paths", "response": "Expands directories and globs in given path."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nbehaving str. split but supports tuples of delimiters.", "response": "def tsplit(string, delimiters):\n    \"\"\"Behaves str.split but supports tuples of delimiters.\"\"\"\n    delimiters = tuple(delimiters)\n    if len(delimiters) < 1:\n        return [string,]\n    final_delimiter = delimiters[0]\n    for i in delimiters[1:]:\n        string = string.replace(i, final_delimiter)\n    return string.split(final_delimiter)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsplitting string into n sized chunks.", "response": "def schunk(string, size):\n    \"\"\"Splits string into n sized chunks.\"\"\"\n    return [string[i:i+size] for i in range(0, len(string), size)]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef mill(it, label='', hide=None, expected_size=None, every=1):\n\n    def _mill_char(_i):\n        if _i >= count:\n            return ' '\n        else:\n            return MILL_CHARS[(_i // every) % len(MILL_CHARS)]\n\n    def _show(_i):\n        if not hide:\n            if ((_i % every) == 0 or         # True every \"every\" updates\n                (_i == count)):            # And when we're done\n\n                STREAM.write(MILL_TEMPLATE % (\n                    label, _mill_char(_i), _i, count))\n                STREAM.flush()\n\n    count = len(it) if expected_size is None else expected_size\n\n    if count:\n        _show(0)\n\n    for i, item in enumerate(it):\n        yield item\n        _show(i + 1)\n\n    if not hide:\n        STREAM.write('\\n')\n        STREAM.flush()", "response": "Progress iterator. Prints a mill while iterating over the items."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns given string with right padding.", "response": "def min_width(string, cols, padding=' '):\n    \"\"\"Returns given string with right padding.\"\"\"\n\n    is_color = isinstance(string, ColoredString)\n\n    stack = tsplit(str(string), NEWLINES)\n\n    for i, substring in enumerate(stack):\n        _sub = clean(substring).ljust((cols + 0), padding)\n        if is_color:\n            _sub = (_sub.replace(clean(substring), substring))\n        stack[i] = _sub\n        \n    return '\\n'.join(stack)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a context manager that can be used to display text or context manager for a given set of arguments.", "response": "def max_width(*args, **kwargs):\n    \"\"\"Returns formatted text or context manager for textui:puts.\n\n        >>> from clint.textui import puts, max_width\n        >>> max_width('123 5678', 8)\n        '123 5678'\n        >>> max_width('123 5678', 7)\n        '123 \\n5678'\n        >>> with max_width(7):\n        ...     puts('123 5678')\n        '123 \\n5678'\n    \"\"\"\n    args = list(args)\n\n    if not args:\n        args.append(kwargs.get('string'))\n        args.append(kwargs.get('cols'))\n        args.append(kwargs.get('separator'))\n    elif len(args) == 1:\n        args.append(kwargs.get('cols'))\n        args.append(kwargs.get('separator'))\n    elif len(args) == 2:\n        args.append(kwargs.get('separator'))\n\n    string, cols, separator = args\n    if separator is None:\n        separator = '\\n'  # default value\n    if cols is None:\n        # cols should be specified vitally\n        # because string can be specified at textui:puts function\n        string, cols = cols, string\n\n    if string is None:\n        MAX_WIDTHS.append((cols, separator))\n        return _max_width_context()\n    else:\n        return _max_width_formatter(string, cols, separator)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a freshly formatted version of the string with max width.", "response": "def _max_width_formatter(string, cols, separator='\\n'):\n    \"\"\"Returns a freshly formatted\n    :param string: string to be formatted\n    :type string: basestring or clint.textui.colored.ColoredString\n    :param cols: max width the text to be formatted\n    :type cols: int\n    :param separator: separator to break rows\n    :type separator: basestring\n    \"\"\"\n\n    is_color = isinstance(string, ColoredString)\n\n    if is_color:\n        string_copy = string._new('')\n        string = string.s\n\n    stack = tsplit(string, NEWLINES)\n\n    for i, substring in enumerate(stack):\n        stack[i] = substring.split()\n\n    _stack = []\n    \n    for row in stack:\n        _row = ['',]\n        _row_i = 0\n\n        for word in row:\n            if (len(_row[_row_i]) + len(word)) <= cols:\n                _row[_row_i] += word\n                _row[_row_i] += ' '\n                \n            elif len(word) > cols:\n\n                # ensure empty row\n                if len(_row[_row_i]):\n                    _row[_row_i] = _row[_row_i].rstrip()\n                    _row.append('')\n                    _row_i += 1\n\n                chunks = schunk(word, cols)\n                for i, chunk in enumerate(chunks):\n                    if not (i + 1) == len(chunks):\n                        _row[_row_i] += chunk\n                        _row[_row_i] = _row[_row_i].rstrip()\n                        _row.append('')\n                        _row_i += 1\n                    else:\n                        _row[_row_i] += chunk\n                        _row[_row_i] += ' '\n            else:\n                _row[_row_i] = _row[_row_i].rstrip()\n                _row.append('')\n                _row_i += 1\n                _row[_row_i] += word\n                _row[_row_i] += ' '\n        else:\n            _row[_row_i] = _row[_row_i].rstrip()\n\n        _row = map(str, _row)\n        _stack.append(separator.join(_row))\n\n    _s = '\\n'.join(_stack)\n    if is_color:\n        _s = string_copy._new(_s)\n    return _s"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef join(l, conj=CONJUNCTION, im_a_moron=MORON_MODE, separator=COMMA):\n\n    collector = []\n    left = len(l)\n    separator = separator + SPACE\n    conj = conj + SPACE\n\n    for _l in l[:]:\n\n        left += -1\n\n        collector.append(_l)\n        if left == 1:\n            if len(l) == 2 or im_a_moron:\n                collector.append(SPACE)\n            else:\n                collector.append(separator)\n\n            collector.append(conj)\n\n        elif left is not 0:\n            collector.append(separator)\n\n    return unicode(str().join(collector))", "response": "Joins lists of words. Oxford comma and all."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef user_data_dir(appname, appauthor=None, version=None, roaming=False):\n    if sys.platform.startswith(\"win\"):\n        if appauthor is None:\n            raise AppDirsError(\"must specify 'appauthor' on Windows\")\n        const = roaming and \"CSIDL_APPDATA\" or \"CSIDL_LOCAL_APPDATA\"\n        path = os.path.join(_get_win_folder(const), appauthor, appname)\n    elif sys.platform == 'darwin':\n        path = os.path.join(\n            os.path.expanduser('~/Library/Application Support/'),\n            appname)\n    else:\n        path = os.path.join(\n            os.getenv('XDG_CONFIG_HOME', os.path.expanduser(\"~/.config\")),\n            appname.lower())\n    if version:\n        path = os.path.join(path, version)\n    return path", "response": "r Returns full path to the user - specific data directory for this application."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn full path to the user - shared data dir for this application.", "response": "def site_data_dir(appname, appauthor=None, version=None):\n    \"\"\"Return full path to the user-shared data dir for this application.\n\n        \"appname\" is the name of application.\n        \"appauthor\" (only required and used on Windows) is the name of the\n            appauthor or distributing body for this application. Typically\n            it is the owning company name.\n        \"version\" is an optional version path element to append to the\n            path. You might want to use this if you want multiple versions\n            of your app to be able to run independently. If used, this\n            would typically be \"<major>.<minor>\".\n\n    Typical user data directories are:\n        Mac OS X:   /Library/Application Support/<AppName>\n        Unix:       /etc/xdg/<appname>\n        Win XP:     C:\\Documents and Settings\\All Users\\Application Data\\<AppAuthor>\\<AppName>\n        Vista:      (Fail! \"C:\\ProgramData\" is a hidden *system* directory on Vista.)\n        Win 7:      C:\\ProgramData\\<AppAuthor>\\<AppName>   # Hidden, but writeable on Win 7.\n\n    For Unix, this is using the $XDG_CONFIG_DIRS[0] default.\n\n    WARNING: Do not use this on Windows. See the Vista-Fail note above for why.\n    \"\"\"\n    if sys.platform.startswith(\"win\"):\n        if appauthor is None:\n            raise AppDirsError(\"must specify 'appauthor' on Windows\")\n        path = os.path.join(_get_win_folder(\"CSIDL_COMMON_APPDATA\"),\n                            appauthor, appname)\n    elif sys.platform == 'darwin':\n        path = os.path.join(\n            os.path.expanduser('/Library/Application Support'),\n            appname)\n    else:\n        # XDG default for $XDG_CONFIG_DIRS[0]. Perhaps should actually\n        # *use* that envvar, if defined.\n        path = \"/etc/xdg/\"+appname.lower()\n    if version:\n        path = os.path.join(path, version)\n    return path"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef user_cache_dir(appname, appauthor=None, version=None, opinion=True):\n    if sys.platform.startswith(\"win\"):\n        if appauthor is None:\n            raise AppDirsError(\"must specify 'appauthor' on Windows\")\n        path = os.path.join(_get_win_folder(\"CSIDL_LOCAL_APPDATA\"),\n                            appauthor, appname)\n        if opinion:\n            path = os.path.join(path, \"Cache\")\n    elif sys.platform == 'darwin':\n        path = os.path.join(\n            os.path.expanduser('~/Library/Caches'),\n            appname)\n    else:\n        path = os.path.join(\n            os.getenv('XDG_CACHE_HOME', os.path.expanduser('~/.cache')),\n            appname.lower())\n    if version:\n        path = os.path.join(path, version)\n    return path", "response": "Return full path to the user - specific cache dir for this application."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef user_log_dir(appname, appauthor=None, version=None, opinion=True):\n    if sys.platform == \"darwin\":\n        path = os.path.join(\n            os.path.expanduser('~/Library/Logs'),\n            appname)\n    elif sys.platform == \"win32\":\n        path = user_data_dir(appname, appauthor, version); version=False\n        if opinion:\n            path = os.path.join(path, \"Logs\")\n    else:\n        path = user_cache_dir(appname, appauthor, version); version=False\n        if opinion:\n            path = os.path.join(path, \"log\")\n    if version:\n        path = os.path.join(path, version)\n    return path", "response": "r Returns full path to the user - specific log dir for this application."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_win_folder_from_registry(csidl_name):\n    import _winreg\n\n    shell_folder_name = {\n        \"CSIDL_APPDATA\": \"AppData\",\n        \"CSIDL_COMMON_APPDATA\": \"Common AppData\",\n        \"CSIDL_LOCAL_APPDATA\": \"Local AppData\",\n    }[csidl_name]\n\n    key = _winreg.OpenKey(_winreg.HKEY_CURRENT_USER,\n        r\"Software\\Microsoft\\Windows\\CurrentVersion\\Explorer\\Shell Folders\")\n    dir, type = _winreg.QueryValueEx(key, shell_folder_name)\n    return dir", "response": "Get the win folder name from the registry."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the etags from the header and perform a validation against the required preconditions.", "response": "def get_etags_and_matchers(self, request):\n        \"\"\"Get the etags from the header and perform a validation against the required preconditions.\"\"\"\n        # evaluate the preconditions, raises 428 if condition is not met\n        self.evaluate_preconditions(request)\n        # alright, headers are present, extract the values and match the conditions\n        return super(APIETAGProcessor, self).get_etags_and_matchers(request)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef evaluate_preconditions(self, request):\n        if request.method.upper() in self.precondition_map.keys():\n            required_headers = self.precondition_map.get(request.method.upper(), [])\n            # check the required headers\n            for header in required_headers:\n                if not request.META.get(prepare_header_name(header)):\n                    # raise an error for each header that does not match\n                    logger.warning('Precondition required: %s', request.path,\n                                   extra={\n                                       'status_code': status.HTTP_428_PRECONDITION_REQUIRED,\n                                       'request': request\n                                   }\n                                   )\n                    # raise an RFC 6585 compliant exception\n                    raise PreconditionRequiredException(detail='Precondition required. This \"%s\" request '\n                                                               'is required to be conditional. '\n                                                               'Try again using \"%s\".' % (request.method, header)\n                                                        )\n        return True", "response": "Evaluate whether the precondition for the request is met."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _slugify(text, delim=u'-'):\n    result = []\n    for word in _punct_re.split(text.lower()):\n        word = word.encode('utf-8')\n        if word:\n            result.append(word)\n    slugified = delim.join([i.decode('utf-8') for i in result])\n    return re.sub('[^a-zA-Z0-9\\\\s\\\\-]{1}', replace_char, slugified).lower()", "response": "Generates an ASCII - only slug."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _regex_from_encoded_pattern(s):\n    if s.startswith('/') and s.rfind('/') != 0:\n        # Parse it: /PATTERN/FLAGS\n        idx = s.rfind('/')\n        pattern, flags_str = s[1:idx], s[idx+1:]\n        flag_from_char = {\n            \"i\": re.IGNORECASE,\n            \"l\": re.LOCALE,\n            \"s\": re.DOTALL,\n            \"m\": re.MULTILINE,\n            \"u\": re.UNICODE,\n        }\n        flags = 0\n        for char in flags_str:\n            try:\n                flags |= flag_from_char[char]\n            except KeyError:\n                raise ValueError(\"unsupported regex flag: '%s' in '%s' \"\n                                 \"(must be one of '%s')\"\n                                 % (char, s, ''.join(list(flag_from_char.keys()))))\n        return re.compile(s[1:idx], flags)\n    else: # not an encoded regex\n        return re.compile(re.escape(s))", "response": "Convert a encoded regex into a regex object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _dedent(text, tabsize=8, skip_first_line=False):\n    lines = text.splitlines(1)\n    _dedentlines(lines, tabsize=tabsize, skip_first_line=skip_first_line)\n    return ''.join(lines)", "response": "dedent text into a single word or list of words"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nescape the given string for use in an HTML or XML tag attribute.", "response": "def _xml_escape_attr(attr, skip_single_quote=True):\n    \"\"\"Escape the given string for use in an HTML/XML tag attribute.\n\n    By default this doesn't bother with escaping `'` to `&#39;`, presuming that\n    the tag attribute is surrounded by double quotes.\n    \"\"\"\n    escaped = (attr\n        .replace('&', '&amp;')\n        .replace('\"', '&quot;')\n        .replace('<', '&lt;')\n        .replace('>', '&gt;'))\n    if not skip_single_quote:\n        escaped = escaped.replace(\"'\", \"&#39;\")\n    return escaped"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef convert(self, text):\n        # Main function. The order in which other subs are called here is\n        # essential. Link and image substitutions need to happen before\n        # _EscapeSpecialChars(), so that any *'s or _'s in the <a>\n        # and <img> tags get encoded.\n\n        # Clear the global hashes. If we don't clear these, you get conflicts\n        # from other articles when generating a page which contains more than\n        # one article (e.g. an index page that shows the N most recent\n        # articles):\n        self.reset()\n\n        if not isinstance(text, unicode):\n            #TODO: perhaps shouldn't presume UTF-8 for string input?\n            text = unicode(text, 'utf-8')\n\n        if self.use_file_vars:\n            # Look for emacs-style file variable hints.\n            emacs_vars = self._get_emacs_vars(text)\n            if \"markdown-extras\" in emacs_vars:\n                splitter = re.compile(\"[ ,]+\")\n                for e in splitter.split(emacs_vars[\"markdown-extras\"]):\n                    if '=' in e:\n                        ename, earg = e.split('=', 1)\n                        try:\n                            earg = int(earg)\n                        except ValueError:\n                            pass\n                    else:\n                        ename, earg = e, None\n                    self.extras[ename] = earg\n\n        # Standardize line endings:\n        text = re.sub(\"\\r\\n|\\r\", \"\\n\", text)\n\n        # Make sure $text ends with a couple of newlines:\n        text += \"\\n\\n\"\n\n        # Convert all tabs to spaces.\n        text = self._detab(text)\n\n        # Strip any lines consisting only of spaces and tabs.\n        # This makes subsequent regexen easier to write, because we can\n        # match consecutive blank lines with /\\n+/ instead of something\n        # contorted like /[ \\t]*\\n+/ .\n        text = self._ws_only_line_re.sub(\"\", text)\n\n        # strip metadata from head and extract\n        if \"metadata\" in self.extras:\n            text = self._extract_metadata(text)\n\n        text = self.preprocess(text)\n\n        if self.safe_mode:\n            text = self._hash_html_spans(text)\n\n        # Turn block-level HTML blocks into hash entries\n        text = self._hash_html_blocks(text, raw=True)\n\n        # Strip link definitions, store in hashes.\n        if \"footnotes\" in self.extras:\n            # Must do footnotes first because an unlucky footnote defn\n            # looks like a link defn:\n            #   [^4]: this \"looks like a link defn\"\n            text = self._strip_footnote_definitions(text)\n        text = self._strip_link_definitions(text)\n\n        text = self._run_block_gamut(text)\n\n        if \"footnotes\" in self.extras:\n            text = self._add_footnotes(text)\n\n        text = self.postprocess(text)\n\n        text = self._unescape_special_chars(text)\n\n        if self.safe_mode:\n            text = self._unhash_html_spans(text)\n\n        if \"nofollow\" in self.extras:\n            text = self._a_nofollow.sub(r'<\\1 rel=\"nofollow\"\\2', text)\n\n        text += \"\\n\"\n\n        rv = UnicodeWithAttrs(text)\n        if \"toc\" in self.extras:\n            rv._toc = self._toc\n        if \"metadata\" in self.extras:\n            rv.metadata = self.metadata\n        return rv", "response": "Convert the given text into a new entry in the internal dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse the local variables in the given text.", "response": "def _get_emacs_vars(self, text):\n        \"\"\"Return a dictionary of emacs-style local variables.\n\n        Parsing is done loosely according to this spec (and according to\n        some in-practice deviations from this):\n        http://www.gnu.org/software/emacs/manual/html_node/emacs/Specifying-File-Variables.html#Specifying-File-Variables\n        \"\"\"\n        emacs_vars = {}\n        SIZE = pow(2, 13) # 8kB\n\n        # Search near the start for a '-*-'-style one-liner of variables.\n        head = text[:SIZE]\n        if \"-*-\" in head:\n            match = self._emacs_oneliner_vars_pat.search(head)\n            if match:\n                emacs_vars_str = match.group(1)\n                assert '\\n' not in emacs_vars_str\n                emacs_var_strs = [s.strip() for s in emacs_vars_str.split(';')\n                                  if s.strip()]\n                if len(emacs_var_strs) == 1 and ':' not in emacs_var_strs[0]:\n                    # While not in the spec, this form is allowed by emacs:\n                    #   -*- Tcl -*-\n                    # where the implied \"variable\" is \"mode\". This form\n                    # is only allowed if there are no other variables.\n                    emacs_vars[\"mode\"] = emacs_var_strs[0].strip()\n                else:\n                    for emacs_var_str in emacs_var_strs:\n                        try:\n                            variable, value = emacs_var_str.strip().split(':', 1)\n                        except ValueError:\n                            log.debug(\"emacs variables error: malformed -*- \"\n                                      \"line: %r\", emacs_var_str)\n                            continue\n                        # Lowercase the variable name because Emacs allows \"Mode\"\n                        # or \"mode\" or \"MoDe\", etc.\n                        emacs_vars[variable.lower()] = value.strip()\n\n        tail = text[-SIZE:]\n        if \"Local Variables\" in tail:\n            match = self._emacs_local_vars_pat.search(tail)\n            if match:\n                prefix = match.group(\"prefix\")\n                suffix = match.group(\"suffix\")\n                lines = match.group(\"content\").splitlines(0)\n                #print \"prefix=%r, suffix=%r, content=%r, lines: %s\"\\\n                #      % (prefix, suffix, match.group(\"content\"), lines)\n\n                # Validate the Local Variables block: proper prefix and suffix\n                # usage.\n                for i, line in enumerate(lines):\n                    if not line.startswith(prefix):\n                        log.debug(\"emacs variables error: line '%s' \"\n                                  \"does not use proper prefix '%s'\"\n                                  % (line, prefix))\n                        return {}\n                    # Don't validate suffix on last line. Emacs doesn't care,\n                    # neither should we.\n                    if i != len(lines)-1 and not line.endswith(suffix):\n                        log.debug(\"emacs variables error: line '%s' \"\n                                  \"does not use proper suffix '%s'\"\n                                  % (line, suffix))\n                        return {}\n\n                # Parse out one emacs var per line.\n                continued_for = None\n                for line in lines[:-1]: # no var on the last line (\"PREFIX End:\")\n                    if prefix: line = line[len(prefix):] # strip prefix\n                    if suffix: line = line[:-len(suffix)] # strip suffix\n                    line = line.strip()\n                    if continued_for:\n                        variable = continued_for\n                        if line.endswith('\\\\'):\n                            line = line[:-1].rstrip()\n                        else:\n                            continued_for = None\n                        emacs_vars[variable] += ' ' + line\n                    else:\n                        try:\n                            variable, value = line.split(':', 1)\n                        except ValueError:\n                            log.debug(\"local variables error: missing colon \"\n                                      \"in local variables entry: '%s'\" % line)\n                            continue\n                        # Do NOT lowercase the variable name, because Emacs only\n                        # allows \"mode\" (and not \"Mode\", \"MoDe\", etc.) in this block.\n                        value = value.strip()\n                        if value.endswith('\\\\'):\n                            value = value[:-1].rstrip()\n                            continued_for = variable\n                        else:\n                            continued_for = None\n                        emacs_vars[variable] = value\n\n        # Unquote values.\n        for var, val in list(emacs_vars.items()):\n            if len(val) > 1 and (val.startswith('\"') and val.endswith('\"')\n               or val.startswith('\"') and val.endswith('\"')):\n                emacs_vars[var] = val[1:-1]\n\n        return emacs_vars"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove leading tabs from a file.", "response": "def _detab(self, text):\n        r\"\"\"Remove (leading?) tabs from a file.\n\n            >>> m = Markdown()\n            >>> m._detab(\"\\tfoo\")\n            '    foo'\n            >>> m._detab(\"  \\tfoo\")\n            '    foo'\n            >>> m._detab(\"\\t  foo\")\n            '      foo'\n            >>> m._detab(\"  foo\")\n            '  foo'\n            >>> m._detab(\"  foo\\n\\tbar\\tblam\")\n            '  foo\\n    bar blam'\n        \"\"\"\n        if '\\t' not in text:\n            return text\n        return self._detab_re.subn(self._detab_sub, text)[0]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _hash_html_blocks(self, text, raw=False):\n        if '<' not in text:\n            return text\n\n        # Pass `raw` value into our calls to self._hash_html_block_sub.\n        hash_html_block_sub = _curry(self._hash_html_block_sub, raw=raw)\n\n        # First, look for nested blocks, e.g.:\n        #   <div>\n        #       <div>\n        #       tags for inner block must be indented.\n        #       </div>\n        #   </div>\n        #\n        # The outermost tags must start at the left margin for this to match, and\n        # the inner nested divs must be indented.\n        # We need to do this before the next, more liberal match, because the next\n        # match will start at the first `<div>` and stop at the first `</div>`.\n        text = self._strict_tag_block_re.sub(hash_html_block_sub, text)\n\n        # Now match more liberally, simply from `\\n<tag>` to `</tag>\\n`\n        text = self._liberal_tag_block_re.sub(hash_html_block_sub, text)\n\n        # Special case just for <hr />. It was easier to make a special\n        # case than to make the other regex more complicated.\n        if \"<hr\" in text:\n            _hr_tag_re = _hr_tag_re_from_tab_width(self.tab_width)\n            text = _hr_tag_re.sub(hash_html_block_sub, text)\n\n        # Special case for standalone HTML comments:\n        if \"<!--\" in text:\n            start = 0\n            while True:\n                # Delimiters for next comment block.\n                try:\n                    start_idx = text.index(\"<!--\", start)\n                except ValueError:\n                    break\n                try:\n                    end_idx = text.index(\"-->\", start_idx) + 3\n                except ValueError:\n                    break\n\n                # Start position for next comment block search.\n                start = end_idx\n\n                # Validate whitespace before comment.\n                if start_idx:\n                    # - Up to `tab_width - 1` spaces before start_idx.\n                    for i in range(self.tab_width - 1):\n                        if text[start_idx - 1] != ' ':\n                            break\n                        start_idx -= 1\n                        if start_idx == 0:\n                            break\n                    # - Must be preceded by 2 newlines or hit the start of\n                    #   the document.\n                    if start_idx == 0:\n                        pass\n                    elif start_idx == 1 and text[0] == '\\n':\n                        start_idx = 0  # to match minute detail of Markdown.pl regex\n                    elif text[start_idx-2:start_idx] == '\\n\\n':\n                        pass\n                    else:\n                        break\n\n                # Validate whitespace after comment.\n                # - Any number of spaces and tabs.\n                while end_idx < len(text):\n                    if text[end_idx] not in ' \\t':\n                        break\n                    end_idx += 1\n                # - Must be following by 2 newlines or hit end of text.\n                if text[end_idx:end_idx+2] not in ('', '\\n', '\\n\\n'):\n                    continue\n\n                # Escape and hash (must match `_hash_html_block_sub`).\n                html = text[start_idx:end_idx]\n                if raw and self.safe_mode:\n                    html = self._sanitize_html(html)\n                key = _hash_text(html)\n                self.html_blocks[key] = html\n                text = text[:start_idx] + \"\\n\\n\" + key + \"\\n\\n\" + text[end_idx:]\n\n        if \"xml\" in self.extras:\n            # Treat XML processing instructions and namespaced one-liner\n            # tags as if they were block HTML tags. E.g., if standalone\n            # (i.e. are their own paragraph), the following do not get\n            # wrapped in a <p> tag:\n            #    <?foo bar?>\n            #\n            #    <xi:include xmlns:xi=\"http://www.w3.org/2001/XInclude\" href=\"chapter_1.md\"/>\n            _xml_oneliner_re = _xml_oneliner_re_from_tab_width(self.tab_width)\n            text = _xml_oneliner_re.sub(hash_html_block_sub, text)\n\n        return text", "response": "Hash the HTML blocks in text."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nstrip footnote definitions from the text.", "response": "def _strip_footnote_definitions(self, text):\n        \"\"\"A footnote definition looks like this:\n\n            [^note-id]: Text of the note.\n\n                May include one or more indented paragraphs.\n\n        Where,\n        - The 'note-id' can be pretty much anything, though typically it\n          is the number of the footnote.\n        - The first paragraph may start on the next line, like so:\n\n            [^note-id]:\n                Text of the note.\n        \"\"\"\n        less_than_tab = self.tab_width - 1\n        footnote_def_re = re.compile(r'''\n            ^[ ]{0,%d}\\[\\^(.+)\\]:   # id = \\1\n            [ \\t]*\n            (                       # footnote text = \\2\n              # First line need not start with the spaces.\n              (?:\\s*.*\\n+)\n              (?:\n                (?:[ ]{%d} | \\t)  # Subsequent lines must be indented.\n                .*\\n+\n              )*\n            )\n            # Lookahead for non-space at line-start, or end of doc.\n            (?:(?=^[ ]{0,%d}\\S)|\\Z)\n            ''' % (less_than_tab, self.tab_width, self.tab_width),\n            re.X | re.M)\n        return footnote_def_re.sub(self._extract_footnote_def_sub, text)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _prepare_pyshell_blocks(self, text):\n        if \">>>\" not in text:\n            return text\n\n        less_than_tab = self.tab_width - 1\n        _pyshell_block_re = re.compile(r\"\"\"\n            ^([ ]{0,%d})>>>[ ].*\\n   # first line\n            ^(\\1.*\\S+.*\\n)*         # any number of subsequent lines\n            ^\\n                     # ends with a blank line\n            \"\"\" % less_than_tab, re.M | re.X)\n\n        return _pyshell_block_re.sub(self._pyshell_block_sub, text)", "response": "Prepare Python shell code blocks for the log entry."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nturn Markdown link shortcuts into XHTML <a> and <img> tags. This is a combination of Markdown.pl's _DoAnchors() and _DoImages(). They are done together because that simplified the approach. It was necessary to use a different approach than Markdown.pl because of the lack of atomic matching support in Python's regex engine used in $g_nested_brackets.", "response": "def _do_links(self, text):\n        \"\"\"Turn Markdown link shortcuts into XHTML <a> and <img> tags.\n\n        This is a combination of Markdown.pl's _DoAnchors() and\n        _DoImages(). They are done together because that simplified the\n        approach. It was necessary to use a different approach than\n        Markdown.pl because of the lack of atomic matching support in\n        Python's regex engine used in $g_nested_brackets.\n        \"\"\"\n        MAX_LINK_TEXT_SENTINEL = 3000  # markdown2 issue 24\n\n        # `anchor_allowed_pos` is used to support img links inside\n        # anchors, but not anchors inside anchors. An anchor's start\n        # pos must be `>= anchor_allowed_pos`.\n        anchor_allowed_pos = 0\n\n        curr_pos = 0\n        while True: # Handle the next link.\n            # The next '[' is the start of:\n            # - an inline anchor:   [text](url \"title\")\n            # - a reference anchor: [text][id]\n            # - an inline img:      ![text](url \"title\")\n            # - a reference img:    ![text][id]\n            # - a footnote ref:     [^id]\n            #   (Only if 'footnotes' extra enabled)\n            # - a footnote defn:    [^id]: ...\n            #   (Only if 'footnotes' extra enabled) These have already\n            #   been stripped in _strip_footnote_definitions() so no\n            #   need to watch for them.\n            # - a link definition:  [id]: url \"title\"\n            #   These have already been stripped in\n            #   _strip_link_definitions() so no need to watch for them.\n            # - not markup:         [...anything else...\n            try:\n                start_idx = text.index('[', curr_pos)\n            except ValueError:\n                break\n            text_length = len(text)\n\n            # Find the matching closing ']'.\n            # Markdown.pl allows *matching* brackets in link text so we\n            # will here too. Markdown.pl *doesn't* currently allow\n            # matching brackets in img alt text -- we'll differ in that\n            # regard.\n            bracket_depth = 0\n            for p in range(start_idx+1, min(start_idx+MAX_LINK_TEXT_SENTINEL,\n                                            text_length)):\n                ch = text[p]\n                if ch == ']':\n                    bracket_depth -= 1\n                    if bracket_depth < 0:\n                        break\n                elif ch == '[':\n                    bracket_depth += 1\n            else:\n                # Closing bracket not found within sentinel length.\n                # This isn't markup.\n                curr_pos = start_idx + 1\n                continue\n            link_text = text[start_idx+1:p]\n\n            # Possibly a footnote ref?\n            if \"footnotes\" in self.extras and link_text.startswith(\"^\"):\n                normed_id = re.sub(r'\\W', '-', link_text[1:])\n                if normed_id in self.footnotes:\n                    self.footnote_ids.append(normed_id)\n                    result = '<sup class=\"footnote-ref\" id=\"fnref-%s\">' \\\n                             '<a href=\"#fn-%s\">%s</a></sup>' \\\n                             % (normed_id, normed_id, len(self.footnote_ids))\n                    text = text[:start_idx] + result + text[p+1:]\n                else:\n                    # This id isn't defined, leave the markup alone.\n                    curr_pos = p+1\n                continue\n\n            # Now determine what this is by the remainder.\n            p += 1\n            if p == text_length:\n                return text\n\n            # Inline anchor or img?\n            if text[p] == '(': # attempt at perf improvement\n                match = self._tail_of_inline_link_re.match(text, p)\n                if match:\n                    # Handle an inline anchor or img.\n                    is_img = start_idx > 0 and text[start_idx-1] == \"!\"\n                    if is_img:\n                        start_idx -= 1\n\n                    url, title = match.group(\"url\"), match.group(\"title\")\n                    if url and url[0] == '<':\n                        url = url[1:-1]  # '<url>' -> 'url'\n                    # We've got to encode these to avoid conflicting\n                    # with italics/bold.\n                    url = url.replace('*', self._escape_table['*']) \\\n                             .replace('_', self._escape_table['_'])\n                    if title:\n                        title_str = ' title=\"%s\"' % (\n                            _xml_escape_attr(title)\n                                .replace('*', self._escape_table['*'])\n                                .replace('_', self._escape_table['_']))\n                    else:\n                        title_str = ''\n                    if is_img:\n                        result = '<img src=\"%s\" alt=\"%s\"%s%s' \\\n                            % (url.replace('\"', '&quot;'),\n                               _xml_escape_attr(link_text),\n                               title_str, self.empty_element_suffix)\n                        if \"smarty-pants\" in self.extras:\n                            result = result.replace('\"', self._escape_table['\"'])\n                        curr_pos = start_idx + len(result)\n                        text = text[:start_idx] + result + text[match.end():]\n                    elif start_idx >= anchor_allowed_pos:\n                        result_head = '<a href=\"%s\"%s>' % (url, title_str)\n                        result = '%s%s</a>' % (result_head, link_text)\n                        if \"smarty-pants\" in self.extras:\n                            result = result.replace('\"', self._escape_table['\"'])\n                        # <img> allowed from curr_pos on, <a> from\n                        # anchor_allowed_pos on.\n                        curr_pos = start_idx + len(result_head)\n                        anchor_allowed_pos = start_idx + len(result)\n                        text = text[:start_idx] + result + text[match.end():]\n                    else:\n                        # Anchor not allowed here.\n                        curr_pos = start_idx + 1\n                    continue\n\n            # Reference anchor or img?\n            else:\n                match = self._tail_of_reference_link_re.match(text, p)\n                if match:\n                    # Handle a reference-style anchor or img.\n                    is_img = start_idx > 0 and text[start_idx-1] == \"!\"\n                    if is_img:\n                        start_idx -= 1\n                    link_id = match.group(\"id\").lower()\n                    if not link_id:\n                        link_id = link_text.lower()  # for links like [this][]\n                    if link_id in self.urls:\n                        url = self.urls[link_id]\n                        # We've got to encode these to avoid conflicting\n                        # with italics/bold.\n                        url = url.replace('*', self._escape_table['*']) \\\n                                 .replace('_', self._escape_table['_'])\n                        title = self.titles.get(link_id)\n                        if title:\n                            before = title\n                            title = _xml_escape_attr(title) \\\n                                .replace('*', self._escape_table['*']) \\\n                                .replace('_', self._escape_table['_'])\n                            title_str = ' title=\"%s\"' % title\n                        else:\n                            title_str = ''\n                        if is_img:\n                            result = '<img src=\"%s\" alt=\"%s\"%s%s' \\\n                                % (url.replace('\"', '&quot;'),\n                                   link_text.replace('\"', '&quot;'),\n                                   title_str, self.empty_element_suffix)\n                            if \"smarty-pants\" in self.extras:\n                                result = result.replace('\"', self._escape_table['\"'])\n                            curr_pos = start_idx + len(result)\n                            text = text[:start_idx] + result + text[match.end():]\n                        elif start_idx >= anchor_allowed_pos:\n                            result = '<a href=\"%s\"%s>%s</a>' \\\n                                % (url, title_str, link_text)\n                            result_head = '<a href=\"%s\"%s>' % (url, title_str)\n                            result = '%s%s</a>' % (result_head, link_text)\n                            if \"smarty-pants\" in self.extras:\n                                result = result.replace('\"', self._escape_table['\"'])\n                            # <img> allowed from curr_pos on, <a> from\n                            # anchor_allowed_pos on.\n                            curr_pos = start_idx + len(result_head)\n                            anchor_allowed_pos = start_idx + len(result)\n                            text = text[:start_idx] + result + text[match.end():]\n                        else:\n                            # Anchor not allowed here.\n                            curr_pos = start_idx + 1\n                    else:\n                        # This id isn't defined, leave the markup alone.\n                        curr_pos = match.end()\n                    continue\n\n            # Otherwise, it isn't markup.\n            curr_pos = start_idx + 1\n\n        return text"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef header_id_from_text(self, text, prefix, n):\n        header_id = _slugify(text)\n        if prefix and isinstance(prefix, base_string_type):\n            header_id = prefix + '-' + header_id\n        if header_id in self._count_from_header_id:\n            self._count_from_header_id[header_id] += 1\n            header_id += '-%s' % self._count_from_header_id[header_id]\n        else:\n            self._count_from_header_id[header_id] = 1\n        return header_id", "response": "Generate a header id attribute value from the given text."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the appropriate html class string for the given tag.", "response": "def _html_class_str_from_tag(self, tag):\n        \"\"\"Get the appropriate ' class=\"...\"' string (note the leading\n        space), if any, for the given tag.\n        \"\"\"\n        if \"html-classes\" not in self.extras:\n            return \"\"\n        try:\n            html_classes_from_tag = self.extras[\"html-classes\"]\n        except TypeError:\n            return \"\"\n        else:\n            if tag in html_classes_from_tag:\n                return ' class=\"%s\"' % html_classes_from_tag[tag]\n        return \"\""}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprocess Markdown code blocks.", "response": "def _do_code_blocks(self, text):\n        \"\"\"Process Markdown `<pre><code>` blocks.\"\"\"\n        code_block_re = re.compile(r'''\n            (?:\\n\\n|\\A\\n?)\n            (               # $1 = the code block -- one or more lines, starting with a space/tab\n              (?:\n                (?:[ ]{%d} | \\t)  # Lines must start with a tab or a tab-width of spaces\n                .*\\n+\n              )+\n            )\n            ((?=^[ ]{0,%d}\\S)|\\Z)   # Lookahead for non-space at line-start, or end of doc\n            ''' % (self.tab_width, self.tab_width),\n            re.M | re.X)\n        return code_block_re.sub(self._code_block_sub, text)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nencode certain characters inside Markdown code runs.", "response": "def _encode_code(self, text):\n        \"\"\"Encode/escape certain characters inside Markdown code runs.\n        The point is that in code, these characters are literals,\n        and lose their special Markdown meanings.\n        \"\"\"\n        replacements = [\n            # Encode all ampersands; HTML entities are not\n            # entities within a Markdown code span.\n            ('&', '&amp;'),\n            # Do the angle bracket song and dance:\n            ('<', '&lt;'),\n            ('>', '&gt;'),\n        ]\n        for before, after in replacements:\n            text = text.replace(before, after)\n        hashed = _hash_text(text)\n        self._escape_table[text] = hashed\n        return hashed"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _do_smart_punctuation(self, text):\n        if \"'\" in text: # guard for perf\n            text = self._do_smart_contractions(text)\n            text = self._opening_single_quote_re.sub(\"&#8216;\", text)\n            text = self._closing_single_quote_re.sub(\"&#8217;\", text)\n\n        if '\"' in text: # guard for perf\n            text = self._opening_double_quote_re.sub(\"&#8220;\", text)\n            text = self._closing_double_quote_re.sub(\"&#8221;\", text)\n\n        text = text.replace(\"---\", \"&#8212;\")\n        text = text.replace(\"--\", \"&#8211;\")\n        text = text.replace(\"...\", \"&#8230;\")\n        text = text.replace(\" . . . \", \"&#8230;\")\n        text = text.replace(\". . .\", \"&#8230;\")\n        return text", "response": "Fancifies single quotes double quotes and apostrophes."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngive a string of text and a list of link patterns return the text with the link patterns.", "response": "def _do_link_patterns(self, text):\n        \"\"\"Caveat emptor: there isn't much guarding against link\n        patterns being formed inside other standard Markdown links, e.g.\n        inside a [link def][like this].\n\n        Dev Notes: *Could* consider prefixing regexes with a negative\n        lookbehind assertion to attempt to guard against this.\n        \"\"\"\n        link_from_hash = {}\n        for regex, repl in self.link_patterns:\n            replacements = []\n            for match in regex.finditer(text):\n                if hasattr(repl, \"__call__\"):\n                    href = repl(match)\n                else:\n                    href = match.expand(repl)\n                replacements.append((match.span(), href))\n            for (start, end), href in reversed(replacements):\n                escaped_href = (\n                    href.replace('\"', '&quot;')  # b/c of attr quote\n                        # To avoid markdown <em> and <strong>:\n                        .replace('*', self._escape_table['*'])\n                        .replace('_', self._escape_table['_']))\n                link = '<a href=\"%s\">%s</a>' % (escaped_href, text[start:end])\n                hash = _hash_text(link)\n                link_from_hash[hash] = link\n                text = text[:start] + hash + text[end:]\n        for hash, link in list(link_from_hash.items()):\n            text = text.replace(hash, link)\n        return text"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef toc_html(self):\n        if self._toc is None:\n            return None\n\n        def indent():\n            return '  ' * (len(h_stack) - 1)\n        lines = []\n        h_stack = [0]   # stack of header-level numbers\n        for level, id, name in self._toc:\n            if level > h_stack[-1]:\n                lines.append(\"%s<ul>\" % indent())\n                h_stack.append(level)\n            elif level == h_stack[-1]:\n                lines[-1] += \"</li>\"\n            else:\n                while level < h_stack[-1]:\n                    h_stack.pop()\n                    if not lines[-1].endswith(\"</li>\"):\n                        lines[-1] += \"</li>\"\n                    lines.append(\"%s</ul></li>\" % indent())\n            lines.append('%s<li><a href=\"#%s\">%s</a>' % (\n                indent(), id, name))\n        while len(h_stack) > 1:\n            h_stack.pop()\n            if not lines[-1].endswith(\"</li>\"):\n                lines[-1] += \"</li>\"\n            lines.append(\"%s</ul>\" % indent())\n        return '\\n'.join(lines) + '\\n'", "response": "Return the HTML for the current TOC."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nqueues the function given with the first argument with the parameters given with the rest of the argument list.", "response": "def handle(self, *args, **options):\n        \"\"\"\n        Queues the function given with the first argument with the\n        parameters given with the rest of the argument list.\n        \"\"\"\n        verbosity = int(options.get('verbosity', 1))\n        timeout = options.get('timeout')\n        queue = get_queue(options.get('queue'))\n        job = queue.enqueue_call(args[0], args=args[1:], timeout=timeout)\n        if verbosity:\n            print('Job %s created' % job.id)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_worker_class(worker_class=None):\n    RQ = getattr(settings, 'RQ', {})\n\n    if worker_class is None:\n        worker_class = Worker\n        if 'WORKER_CLASS' in RQ:\n            worker_class = RQ.get('WORKER_CLASS')\n\n    if isinstance(worker_class, six.string_types):\n        worker_class = import_attribute(worker_class)\n    return worker_class", "response": "Get worker class from RQ settings if worker_class is None return Worker."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_worker(*queue_names, **kwargs):\n    job_class = get_job_class(kwargs.pop('job_class', None))\n    queue_class = kwargs.pop('queue_class', None)\n    queues = get_queues(*queue_names, **{'job_class': job_class,\n                                         'queue_class': queue_class})\n    # normalize queue_class to what get_queues returns\n    queue_class = queues[0].__class__\n    worker_class = get_worker_class(kwargs.pop('worker_class', None))\n    return worker_class(queues,\n                        connection=queues[0].connection,\n                        exception_handlers=get_exception_handlers() or None,\n                        job_class=job_class,\n                        queue_class=queue_class,\n                        **kwargs)", "response": "Returns a RQ worker for all queues or specified ones."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_job_class(job_class=None):\n    RQ = getattr(settings, 'RQ', {})\n\n    if job_class is None:\n        job_class = RQ.get('JOB_CLASS', Job)\n\n    if isinstance(job_class, six.string_types):\n        job_class = import_attribute(job_class)\n    return job_class", "response": "Get the job class from RQ settings if specified otherwise return Job."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncommit all jobs in the queue.", "response": "def commit(*args, **kwargs):\n    \"\"\"\n    Processes all jobs in the delayed queue.\n    \"\"\"\n    delayed_queue = get_queue()\n    try:\n        while delayed_queue:\n            queue, args, kwargs = delayed_queue.pop(0)\n            queue.original_enqueue_call(*args, **kwargs)\n    finally:\n        clear()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets queue class from config or from RQ settings.", "response": "def get_queue_class(config=None, queue_class=None):\n    \"\"\"\n    Return queue class from config or from RQ settings, otherwise return DjangoRQ.\n    If ``queue_class`` is provided, it takes priority.\n\n    The full priority list for queue class sources:\n    1. ``queue_class`` argument\n    2. ``QUEUE_CLASS`` in ``config`` argument\n    3. ``QUEUE_CLASS`` in base settings (``RQ``)\n    \"\"\"\n    RQ = getattr(settings, 'RQ', {})\n    if queue_class is None:\n        queue_class = RQ.get('QUEUE_CLASS', DjangoRQ)\n        if config:\n            queue_class = config.get('QUEUE_CLASS', queue_class)\n\n    if isinstance(queue_class, six.string_types):\n        queue_class = import_attribute(queue_class)\n    return queue_class"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a redis connection from a connection config", "response": "def get_redis_connection(config, use_strict_redis=False):\n    \"\"\"\n    Returns a redis connection from a connection config\n    \"\"\"\n    redis_cls = redis.StrictRedis if use_strict_redis else redis.Redis\n\n    if 'URL' in config:\n        return redis_cls.from_url(config['URL'], db=config.get('DB'))\n\n    if 'USE_REDIS_CACHE' in config.keys():\n\n        try:\n            # Assume that we're using django-redis\n            from django_redis import get_redis_connection as get_redis\n            return get_redis(config['USE_REDIS_CACHE'])\n        except ImportError:\n            pass\n\n        from django.core.cache import caches\n        cache = caches[config['USE_REDIS_CACHE']]\n        # We're using django-redis-cache\n        try:\n            return cache._client\n        except AttributeError:\n            # For django-redis-cache > 0.13.1\n            return cache.get_master_client()\n\n    if 'UNIX_SOCKET_PATH' in config:\n        return redis_cls(unix_socket_path=config['UNIX_SOCKET_PATH'], db=config['DB'])\n\n    if 'SENTINELS' in config:\n        sentinel_kwargs = {\n            'db': config.get('DB'),\n            'password': config.get('PASSWORD'),\n            'socket_timeout': config.get('SOCKET_TIMEOUT'),\n        }\n        sentinel_kwargs.update(config.get('CONNECTION_KWARGS', {}))\n        sentinel = Sentinel(config['SENTINELS'], **sentinel_kwargs)\n        return sentinel.master_for(\n            service_name=config['MASTER_NAME'], redis_class=redis_cls,\n        )\n\n    return redis_cls(host=config['HOST'], port=config['PORT'], db=config['DB'], password=config.get('PASSWORD'), ssl=config.get('SSL', False))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_connection(name='default', use_strict_redis=False):\n    from .settings import QUEUES\n    return get_redis_connection(QUEUES[name], use_strict_redis)", "response": "Returns a Redis connection to use based on parameters in settings. RQ_QUEUES\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns an rq Queue object from the given name", "response": "def get_queue(name='default', default_timeout=None, is_async=None,\n              autocommit=None, connection=None, queue_class=None, job_class=None, **kwargs):\n    \"\"\"\n    Returns an rq Queue using parameters defined in ``RQ_QUEUES``\n    \"\"\"\n    from .settings import QUEUES\n\n    if kwargs.get('async') is not None:\n        is_async = kwargs['async']\n        warnings.warn('The `async` keyword is deprecated. Use `is_async` instead', DeprecationWarning)\n\n    # If is_async is provided, use it, otherwise, get it from the configuration\n    if is_async is None:\n        is_async = QUEUES[name].get('ASYNC', True)\n    # same for job_class\n    job_class = get_job_class(job_class)\n\n    if default_timeout is None:\n        default_timeout = QUEUES[name].get('DEFAULT_TIMEOUT')\n    if connection is None:\n        connection = get_connection(name)\n    queue_class = get_queue_class(QUEUES[name], queue_class)\n    return queue_class(name, default_timeout=default_timeout,\n                       connection=connection, is_async=is_async,\n                       job_class=job_class, autocommit=autocommit, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns an rq Queue object by index", "response": "def get_queue_by_index(index):\n    \"\"\"\n    Returns an rq Queue using parameters defined in ``QUEUES_LIST``\n    \"\"\"\n    from .settings import QUEUES_LIST\n    config = QUEUES_LIST[int(index)]\n    return get_queue_class(config)(\n        config['name'],\n        connection=get_redis_connection(config['connection_config']),\n        is_async=config.get('ASYNC', True))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef filter_connection_params(queue_params):\n    CONNECTION_PARAMS = ('URL', 'DB', 'USE_REDIS_CACHE',\n                         'UNIX_SOCKET_PATH', 'HOST', 'PORT', 'PASSWORD',\n                         'SENTINELS', 'MASTER_NAME', 'SOCKET_TIMEOUT',\n                         'SSL', 'CONNECTION_KWARGS',)\n\n    #return {p:v for p,v in queue_params.items() if p in CONNECTION_PARAMS}\n    # Dict comprehension compatible with python 2.6\n    return dict((p,v) for (p,v) in queue_params.items() if p in CONNECTION_PARAMS)", "response": "Filters the queue params to keep only the connection related params."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_queues(*queue_names, **kwargs):\n    from .settings import QUEUES\n\n    if len(queue_names) <= 1:\n        # Return \"default\" queue if no queue name is specified\n        # or one queue with specified name\n        return [get_queue(*queue_names, **kwargs)]\n\n    # will return more than one queue\n    # import job class only once for all queues\n    kwargs['job_class'] = get_job_class(kwargs.pop('job_class', None))\n\n    queue_params = QUEUES[queue_names[0]]\n    connection_params = filter_connection_params(queue_params)\n    queues = [get_queue(queue_names[0], **kwargs)]\n\n    # do consistency checks while building return list\n    for name in queue_names[1:]:\n        queue = get_queue(name, **kwargs)\n        if type(queue) is not type(queues[0]):\n            raise ValueError(\n                'Queues must have the same class.'\n                '\"{0}\" and \"{1}\" have '\n                'different classes'.format(name, queue_names[0]))\n        if connection_params != filter_connection_params(QUEUES[name]):\n            raise ValueError(\n                'Queues must have the same redis connection.'\n                '\"{0}\" and \"{1}\" have '\n                'different connections'.format(name, queue_names[0]))\n        queues.append(queue)\n\n    return queues", "response": "Returns a list of queues from the specified queue names."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_unique_connection_configs(config=None):\n    if config is None:\n        from .settings import QUEUES\n        config = QUEUES\n\n    connection_configs = []\n    for key, value in config.items():\n        value = filter_connection_params(value)\n        if value not in connection_configs:\n            connection_configs.append(value)\n    return connection_configs", "response": "Returns a list of unique Redis connections from config"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef job(func_or_queue, connection=None, *args, **kwargs):\n    if callable(func_or_queue):\n        func = func_or_queue\n        queue = 'default'\n    else:\n        func = None\n        queue = func_or_queue\n\n    if isinstance(queue, six.string_types):\n        try:\n            queue = get_queue(queue)\n            if connection is None:\n                connection = queue.connection\n        except KeyError:\n            pass\n\n    RQ = getattr(settings, 'RQ', {})\n    default_result_ttl = RQ.get('DEFAULT_RESULT_TTL')\n    if default_result_ttl:\n        kwargs.setdefault('result_ttl', default_result_ttl)\n\n    decorator = _rq_job(queue, connection=connection, *args, **kwargs)\n    if func:\n        return decorator(func)\n    return decorator", "response": "Decorator for creating a new job in a queue."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting naive datetime to localtime based on settings", "response": "def to_localtime(time):\n    '''Converts naive datetime to localtime based on settings'''\n\n    utc_time = time.replace(tzinfo=timezone.utc)\n    to_zone = timezone.get_default_timezone()\n    return utc_time.astimezone(to_zone)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsend command byte to display.", "response": "def command(self, c):\n        \"\"\"Send command byte to display.\"\"\"\n        if self._spi is not None:\n            # SPI write.\n            self._gpio.set_low(self._dc)\n            self._spi.write([c])\n        else:\n            # I2C write.\n            control = 0x00   # Co = 0, DC = 0\n            self._i2c.write8(control, c)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef data(self, c):\n        if self._spi is not None:\n            # SPI write.\n            self._gpio.set_high(self._dc)\n            self._spi.write([c])\n        else:\n            # I2C write.\n            control = 0x40   # Co = 0, DC = 0\n            self._i2c.write8(control, c)", "response": "Send byte of data to display."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwriting display buffer to physical display.", "response": "def display(self):\n        \"\"\"Write display buffer to physical display.\"\"\"\n        self.command(SSD1306_COLUMNADDR)\n        self.command(0)              # Column start address. (0 = reset)\n        self.command(self.width-1)   # Column end address.\n        self.command(SSD1306_PAGEADDR)\n        self.command(0)              # Page start address. (0 = reset)\n        self.command(self._pages-1)  # Page end address.\n        # Write buffer data.\n        if self._spi is not None:\n            # Set DC high for data.\n            self._gpio.set_high(self._dc)\n            # Write buffer.\n            self._spi.write(self._buffer)\n        else:\n            for i in range(0, len(self._buffer), 16):\n                control = 0x40   # Co = 0, DC = 0\n                self._i2c.writeList(control, self._buffer[i:i+16])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef image(self, image):\n        if image.mode != '1':\n            raise ValueError('Image must be in mode 1.')\n        imwidth, imheight = image.size\n        if imwidth != self.width or imheight != self.height:\n            raise ValueError('Image must be same dimensions as display ({0}x{1}).' \\\n                .format(self.width, self.height))\n        # Grab all the pixels from the image, faster than getpixel.\n        pix = image.load()\n        # Iterate through the memory pages\n        index = 0\n        for page in range(self._pages):\n            # Iterate through all x axis columns.\n            for x in range(self.width):\n                # Set the bits for the column of pixels at the current position.\n                bits = 0\n                # Don't use range here as it's a bit slow\n                for bit in [0, 1, 2, 3, 4, 5, 6, 7]:\n                    bits = bits << 1\n                    bits |= 0 if pix[(x, page*8+7-bit)] == 0 else 1\n                # Update buffer byte and increment to next byte.\n                self._buffer[index] = bits\n                index += 1", "response": "Set buffer to value of Python Imaging Library image."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the contrast of the display.", "response": "def set_contrast(self, contrast):\n        \"\"\"Sets the contrast of the display.  Contrast should be a value between\n        0 and 255.\"\"\"\n        if contrast < 0 or contrast > 255:\n            raise ValueError('Contrast must be a value from 0 to 255 (inclusive).')\n        self.command(SSD1306_SETCONTRAST)\n        self.command(contrast)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadjusts contrast to dim if dim is True otherwise sets the contrast to normal brightness if dim is False.", "response": "def dim(self, dim):\n        \"\"\"Adjusts contrast to dim the display if dim is True, otherwise sets the\n        contrast to normal brightness if dim is False.\n        \"\"\"\n        # Assume dim display.\n        contrast = 0\n        # Adjust contrast based on VCC if not dimming.\n        if not dim:\n            if self._vccstate == SSD1306_EXTERNALVCC:\n                contrast = 0x9F\n            else:\n                contrast = 0xCF"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the SELECT part of the SQL statement.", "response": "def _select_sql(self, **kwargs):\n        \"\"\"\n        Overridden function to generate the SELECT part of the SQL statement,\n        with the addition of the a modifier if present.\n        \"\"\"\n        return 'SELECT {distinct}{modifier}{select}'.format(\n              distinct='DISTINCT ' if self._distinct else '',\n              modifier='{} '.format(' '.join(self._modifiers)) if self._modifiers else '',\n              select=','.join(term.get_sql(with_alias=True, subquery=True, **kwargs)\n                              for term in self._selects),\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _orderby_sql(self, quote_char=None, **kwargs):\n        clauses = []\n        selected_aliases = {s.alias for s in self.base_query._selects}\n        for field, directionality in self._orderbys:\n            term = \"{quote}{alias}{quote}\".format(alias=field.alias, quote=quote_char or '') \\\n                if field.alias and field.alias in selected_aliases \\\n                else field.get_sql(quote_char=quote_char, **kwargs)\n\n            clauses.append('{term} {orient}'.format(term=term, orient=directionality.value)\n                           if directionality is not None else term)\n\n        return ' ORDER BY {orderby}'.format(orderby=','.join(clauses))", "response": "Generates the SQL for the ORDER BY part of the query."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding a table to the query.", "response": "def from_(self, selectable):\n        \"\"\"\n        Adds a table to the query.  This function can only be called once and will raise an AttributeError if called a\n        second time.\n\n        :param selectable:\n            Type: ``Table``, ``Query``, or ``str``\n\n            When a ``str`` is passed, a table with the name matching the ``str`` value is used.\n\n        :returns\n            A copy of the query with the table added.\n        \"\"\"\n\n        self._from.append(Table(selectable) if isinstance(selectable, str) else selectable)\n\n        if isinstance(selectable, (QueryBuilder, _UnionQuery)) and selectable.alias is None:\n            if isinstance(selectable, QueryBuilder):\n                sub_query_count = selectable._subquery_count\n            else:\n                sub_query_count = 0\n\n            sub_query_count = max(self._subquery_count, sub_query_count)\n            selectable.alias = 'sq%d' % sub_query_count\n            self._subquery_count = sub_query_count + 1"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _validate_table(self, term):\n        base_tables = self._from + [self._update_table]\n\n        for field in term.fields():\n            table_in_base_tables = field.table in base_tables\n            table_in_joins = field.table in [join.item for join in self._joins]\n            if field.table is not None \\\n                  and not table_in_base_tables \\\n                  and not table_in_joins \\\n                  and field.table != self._update_table:\n                return False\n        return True", "response": "Returns False if the term references a table not already part of the\n        FROM clause or JOINS and True otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the SQL for the Columns clause for INSERT queries", "response": "def _columns_sql(self, with_namespace=False, **kwargs):\n        \"\"\"\n        SQL for Columns clause for INSERT queries\n        :param with_namespace:\n            Remove from kwargs, never format the column terms with namespaces since only one table can be inserted into\n        \"\"\"\n        return ' ({columns})'.format(\n              columns=','.join(term.get_sql(with_namespace=False, **kwargs)\n                               for term in self._columns)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _group_sql(self, quote_char=None, groupby_alias=True, **kwargs):\n        clauses = []\n        selected_aliases = {s.alias for s in self._selects}\n        for field in self._groupbys:\n            if groupby_alias and field.alias and field.alias in selected_aliases:\n                clauses.append(\"{quote}{alias}{quote}\".format(\n                      alias=field.alias,\n                      quote=quote_char or '',\n                ))\n            else:\n                clauses.append(field.get_sql(quote_char=quote_char, **kwargs))\n\n        sql = ' GROUP BY {groupby}'.format(groupby=','.join(clauses))\n        if self._with_totals:\n            return sql + ' WITH TOTALS'\n        return sql", "response": "Produces the GROUP BY part of the query."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nresolves the is_aggregate flag for an expression that contains multiple terms.", "response": "def resolve_is_aggregate(values):\n    \"\"\"\n    Resolves the is_aggregate flag for an expression that contains multiple terms.  This works like a voter system,\n    each term votes True or False or abstains with None.\n\n    :param values: A list of booleans (or None) for each term in the expression\n    :return: If all values are True or None, True is returned.  If all values are None, None is returned. Otherwise,\n        False is returned.\n    \"\"\"\n    result = [x\n              for x in values\n              if x is not None]\n    if result:\n        return all(result)\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef wrap_constant(self, val):\n        from .queries import QueryBuilder\n\n        if isinstance(val, (Term, QueryBuilder, Interval)):\n            return val\n        if val is None:\n            return NullValue()\n        if isinstance(val, list):\n            return Array(*val)\n        if isinstance(val, tuple):\n            return Tuple(*val)\n\n        _ValueWrapper = getattr(self, '_wrapper_cls', ValueWrapper)\n        return _ValueWrapper(val)", "response": "Used for wrapping raw input values such as string number or decimal values."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef for_(self, table):\n        self.args = [param.for_(table) for param in self.args]", "response": "Replaces the tables of this term with the table value provided. Useful when reusing fields across queries across queries."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets head state. :return:", "response": "def _get_head_state(self):\n        \"\"\"Get head state.\n\n        :return:\n        \"\"\"\n        if not self.head_state:\n            root = self._get_head_block().state_root\n            self.head_state = State(self.db, root)\n        return self.head_state"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget account by address.", "response": "def _get_account(self, address):\n        \"\"\"Get account by address.\n\n        :param address:\n        :return:\n        \"\"\"\n        state = self._get_head_state()\n        account_address = binascii.a2b_hex(utils.remove_0x_head(address))\n        return state.get_and_cache_account(account_address)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_block_hash(self, number):\n        num = _format_block_number(number)\n        hash_key = header_prefix + num + num_suffix\n        return self.db.get(hash_key)", "response": "Get the block hash by block number."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets head block header.", "response": "def _get_head_block(self):\n        \"\"\"Get head block header.\n\n        :return:\n        \"\"\"\n        if not self.head_block_header:\n            block_hash = self.db.get(head_header_key)\n            num = self._get_block_number(block_hash)\n            self.head_block_header = self._get_block_header(block_hash, num)\n            # find header with valid state\n            while (\n                not self.db.get(self.head_block_header.state_root)\n                and self.head_block_header.prevhash is not None\n            ):\n                block_hash = self.head_block_header.prevhash\n                num = self._get_block_number(block_hash)\n                self.head_block_header = self._get_block_header(block_hash, num)\n\n        return self.head_block_header"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_block_number(self, block_hash):\n        number_key = block_hash_prefix + block_hash\n        return self.db.get(number_key)", "response": "Get the block number by its hash."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the block header by block header hash & number.", "response": "def _get_block_header(self, block_hash, num):\n        \"\"\"Get block header by block header hash & number.\n\n        :param block_hash:\n        :param num:\n        :return:\n        \"\"\"\n        header_key = header_prefix + num + block_hash\n\n        block_header_data = self.db.get(header_key)\n        header = rlp.decode(block_header_data, sedes=BlockHeader)\n        return header"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the mapped address by its hash.", "response": "def _get_address_by_hash(self, block_hash):\n        \"\"\"Get mapped address by its hash.\n\n        :param block_hash:\n        :return:\n        \"\"\"\n        address_key = address_prefix + block_hash\n        return self.db.get(address_key)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_block_receipts(self, block_hash, num):\n        number = _format_block_number(num)\n        receipts_key = block_receipts_prefix + number + block_hash\n        receipts_data = self.db.get(receipts_key)\n        receipts = rlp.decode(receipts_data, sedes=CountableList(ReceiptForStorage))\n        return receipts", "response": "Get the receipts for a given block header hash & number."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nstoring the account address in the cache.", "response": "def _store_account_address(self, address):\n        \"\"\"Get block transaction receipts by block header hash & number.\n\n        :param address:\n        \"\"\"\n        address_key = address_prefix + utils.sha3(address)\n        self.wb.put(address_key, address)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\niterating through all contracts.", "response": "def get_contracts(self):\n        \"\"\"Iterate through all contracts.\"\"\"\n        for account in self.reader._get_head_state().get_all_accounts():\n            if account.code is not None:\n                code = _encode_hex(account.code)\n                contract = EVMContract(code, enable_online_lookup=False)\n\n                yield contract, account.address, account.balance"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef search(self, expression, callback_func):\n        cnt = 0\n        indexer = AccountIndexer(self)\n\n        for contract, address_hash, balance in self.get_contracts():\n\n            if contract.matches_expression(expression):\n\n                try:\n                    address = _encode_hex(indexer.get_contract_by_hash(address_hash))\n                except AddressNotFoundError:\n                    \"\"\"The hash->address mapping does not exist in our index.\n\n                    If the index is up-to-date, this likely means that\n                    the contract was created by an internal transaction.\n                    Skip this contract as right now we don't have a good\n                    solution for this.\n                    \"\"\"\n\n                    continue\n\n                callback_func(contract, address, balance)\n\n            cnt += 1\n\n            if not cnt % 1000:\n                log.info(\"Searched %d contracts\" % cnt)", "response": "Search through all contracts in the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef contract_hash_to_address(self, contract_hash):\n\n        address_hash = binascii.a2b_hex(utils.remove_0x_head(contract_hash))\n        indexer = AccountIndexer(self)\n\n        return _encode_hex(indexer.get_contract_by_hash(address_hash))", "response": "Try to find corresponding account address."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting block header by block number.", "response": "def eth_getBlockHeaderByNumber(self, number):\n        \"\"\"Get block header by block number.\n\n        :param number:\n        :return:\n        \"\"\"\n        block_hash = self.reader._get_block_hash(number)\n        block_number = _format_block_number(number)\n        return self.reader._get_block_header(block_hash, block_number)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef eth_getBlockByNumber(self, number):\n        block_hash = self.reader._get_block_hash(number)\n        block_number = _format_block_number(number)\n        body_key = body_prefix + block_number + block_hash\n        block_data = self.db.get(body_key)\n        body = rlp.decode(block_data, sedes=Block)\n        return body", "response": "Get the block body by block number."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef eth_getCode(self, address):\n        account = self.reader._get_account(address)\n        return _encode_hex(account.code)", "response": "Get account code.\n\n        :param address:\n        :return:"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef eth_getBalance(self, address):\n        account = self.reader._get_account(address)\n        return account.balance", "response": "Get account balance.\n\n        :param address:\n        :return:"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting account storage data at position.", "response": "def eth_getStorageAt(self, address, position):\n        \"\"\"Get account storage data at position.\n\n        :param address:\n        :param position:\n        :return:\n        \"\"\"\n        account = self.reader._get_account(address)\n        return _encode_hex(\n            utils.zpad(utils.encode_int(account.get_storage_data(position)), 32)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntake integer address 1 2 3 4.", "response": "def native_contracts(address: int, data: BaseCalldata) -> List[int]:\n    \"\"\"Takes integer address 1, 2, 3, 4.\n\n    :param address:\n    :param data:\n    :return:\n    \"\"\"\n    functions = (ecrecover, sha256, ripemd160, identity)\n\n    if isinstance(data, ConcreteCalldata):\n        concrete_data = data.concrete(None)\n    else:\n        raise NativeContractException()\n\n    return functions[address - 1](concrete_data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _analyze_state(state: GlobalState):\n    instruction = state.get_current_instruction()\n\n    annotations = cast(\n        List[MultipleSendsAnnotation],\n        list(state.get_annotations(MultipleSendsAnnotation)),\n    )\n    if len(annotations) == 0:\n        log.debug(\"Creating annotation for state\")\n        state.annotate(MultipleSendsAnnotation())\n        annotations = cast(\n            List[MultipleSendsAnnotation],\n            list(state.get_annotations(MultipleSendsAnnotation)),\n        )\n\n    calls = annotations[0].calls\n\n    if instruction[\"opcode\"] in [\"CALL\", \"DELEGATECALL\", \"STATICCALL\", \"CALLCODE\"]:\n        call = get_call_from_state(state)\n        if call:\n            calls += [call]\n\n    else:  # RETURN or STOP\n        if len(calls) > 1:\n\n            description_tail = (\n                \"Consecutive calls are executed at the following bytecode offsets:\\n\"\n            )\n\n            for call in calls:\n                description_tail += \"Offset: {}\\n\".format(\n                    call.state.get_current_instruction()[\"address\"]\n                )\n\n            description_tail += (\n                \"Try to isolate each external call into its own transaction,\"\n                \" as external calls can fail accidentally or deliberately.\\n\"\n            )\n\n            issue = Issue(\n                contract=state.environment.active_account.contract_name,\n                function_name=state.environment.active_function_name,\n                address=instruction[\"address\"],\n                swc_id=MULTIPLE_SENDS,\n                bytecode=state.environment.code.bytecode,\n                title=\"Multiple Calls in a Single Transaction\",\n                severity=\"Medium\",\n                description_head=\"Multiple sends are executed in one transaction.\",\n                description_tail=description_tail,\n                gas_used=(state.mstate.min_gas_used, state.mstate.max_gas_used),\n            )\n\n            return [issue]\n\n    return []", "response": "Analyze the state and returns the issues for that corresponding state"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_call_parameters(\n    global_state: GlobalState, dynamic_loader: DynLoader, with_value=False\n):\n    \"\"\"Gets call parameters from global state Pops the values from the stack\n    and determines output parameters.\n\n    :param global_state: state to look in\n    :param dynamic_loader: dynamic loader to use\n    :param with_value: whether to pop the value argument from the stack\n    :return: callee_account, call_data, value, call_data_type, gas\n    \"\"\"\n    gas, to = global_state.mstate.pop(2)\n    value = global_state.mstate.pop() if with_value else 0\n    memory_input_offset, memory_input_size, memory_out_offset, memory_out_size = global_state.mstate.pop(\n        4\n    )\n\n    callee_address = get_callee_address(global_state, dynamic_loader, to)\n\n    callee_account = None\n    call_data = get_call_data(global_state, memory_input_offset, memory_input_size)\n    if int(callee_address, 16) >= 5 or int(callee_address, 16) == 0:\n        callee_account = get_callee_account(\n            global_state, callee_address, dynamic_loader\n        )\n\n    return (\n        callee_address,\n        callee_account,\n        call_data,\n        value,\n        gas,\n        memory_out_offset,\n        memory_out_size,\n    )", "response": "Gets the call parameters from the global state and determines the output parameters."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the address of the callee.", "response": "def get_callee_address(\n    global_state: GlobalState,\n    dynamic_loader: DynLoader,\n    symbolic_to_address: Expression,\n):\n    \"\"\"Gets the address of the callee.\n\n    :param global_state: state to look in\n    :param dynamic_loader:  dynamic loader to use\n    :param symbolic_to_address: The (symbolic) callee address\n    :return: Address of the callee\n    \"\"\"\n    environment = global_state.environment\n\n    try:\n        callee_address = hex(util.get_concrete_int(symbolic_to_address))\n    except TypeError:\n        log.debug(\"Symbolic call encountered\")\n\n        match = re.search(r\"storage_(\\d+)\", str(simplify(symbolic_to_address)))\n        log.debug(\"CALL to: \" + str(simplify(symbolic_to_address)))\n\n        if match is None or dynamic_loader is None:\n            raise ValueError()\n\n        index = int(match.group(1))\n        log.debug(\"Dynamic contract address at storage index {}\".format(index))\n\n        # attempt to read the contract address from instance storage\n        try:\n            callee_address = dynamic_loader.read_storage(\n                environment.active_account.address, index\n            )\n        # TODO: verify whether this happens or not\n        except:\n            log.debug(\"Error accessing contract storage.\")\n            raise ValueError\n\n        # testrpc simply returns the address, geth response is more elaborate.\n        if not re.match(r\"^0x[0-9a-f]{40}$\", callee_address):\n            callee_address = \"0x\" + callee_address[26:]\n\n    return callee_address"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the callee account from the global_state.", "response": "def get_callee_account(\n    global_state: GlobalState, callee_address: str, dynamic_loader: DynLoader\n):\n    \"\"\"Gets the callees account from the global_state.\n\n    :param global_state: state to look in\n    :param callee_address: address of the callee\n    :param dynamic_loader: dynamic loader to use\n    :return: Account belonging to callee\n    \"\"\"\n    environment = global_state.environment\n    accounts = global_state.accounts\n\n    try:\n        return global_state.accounts[callee_address]\n    except KeyError:\n        # We have a valid call address, but contract is not in the modules list\n        log.debug(\"Module with address \" + callee_address + \" not loaded.\")\n\n    if dynamic_loader is None:\n        raise ValueError()\n\n    log.debug(\"Attempting to load dependency\")\n\n    try:\n        code = dynamic_loader.dynld(callee_address)\n    except ValueError as error:\n        log.debug(\"Unable to execute dynamic loader because: {}\".format(str(error)))\n        raise error\n    if code is None:\n        log.debug(\"No code returned, not a contract account?\")\n        raise ValueError()\n    log.debug(\"Dependency loaded: \" + callee_address)\n\n    callee_account = Account(\n        callee_address, code, callee_address, dynamic_loader=dynamic_loader\n    )\n    accounts[callee_address] = callee_account\n\n    return callee_account"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the call_data array from the global_state.", "response": "def get_call_data(\n    global_state: GlobalState,\n    memory_start: Union[int, BitVec],\n    memory_size: Union[int, BitVec],\n):\n    \"\"\"Gets call_data from the global_state.\n\n    :param global_state: state to look in\n    :param memory_start: Start index\n    :param memory_size: Size\n    :return: Tuple containing: call_data array from memory or empty array if symbolic, type found\n    \"\"\"\n    state = global_state.mstate\n    transaction_id = \"{}_internalcall\".format(global_state.current_transaction.id)\n\n    memory_start = cast(\n        BitVec,\n        (\n            symbol_factory.BitVecVal(memory_start, 256)\n            if isinstance(memory_start, int)\n            else memory_start\n        ),\n    )\n    memory_size = cast(\n        BitVec,\n        (\n            symbol_factory.BitVecVal(memory_size, 256)\n            if isinstance(memory_size, int)\n            else memory_size\n        ),\n    )\n\n    uses_entire_calldata = simplify(\n        memory_size - global_state.environment.calldata.calldatasize == 0\n    )\n\n    if uses_entire_calldata is True:\n        return global_state.environment.calldata\n\n    try:\n        calldata_from_mem = state.memory[\n            util.get_concrete_int(memory_start) : util.get_concrete_int(\n                memory_start + memory_size\n            )\n        ]\n        return ConcreteCalldata(transaction_id, calldata_from_mem)\n    except TypeError:\n        log.debug(\"Unsupported symbolic calldata offset\")\n        return SymbolicCalldata(transaction_id)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninitializing the instruction coverage plugin for each instruction in the current module.", "response": "def initialize(self, symbolic_vm: LaserEVM):\n        \"\"\"Initializes the instruction coverage plugin\n\n        Introduces hooks for each instruction\n        :param symbolic_vm:\n        :return:\n        \"\"\"\n        self.coverage = {}\n        self.initial_coverage = 0\n        self.tx_id = 0\n\n        @symbolic_vm.laser_hook(\"stop_sym_exec\")\n        def stop_sym_exec_hook():\n            # Print results\n            for code, code_cov in self.coverage.items():\n                cov_percentage = sum(code_cov[1]) / float(code_cov[0]) * 100\n\n                log.info(\n                    \"Achieved {:.2f}% coverage for code: {}\".format(\n                        cov_percentage, code\n                    )\n                )\n\n        @symbolic_vm.laser_hook(\"execute_state\")\n        def execute_state_hook(global_state: GlobalState):\n            # Record coverage\n            code = global_state.environment.code.bytecode\n\n            if code not in self.coverage.keys():\n                number_of_instructions = len(\n                    global_state.environment.code.instruction_list\n                )\n                self.coverage[code] = (\n                    number_of_instructions,\n                    [False] * number_of_instructions,\n                )\n\n            self.coverage[code][1][global_state.mstate.pc] = True\n\n        @symbolic_vm.laser_hook(\"start_sym_trans\")\n        def execute_start_sym_trans_hook():\n            self.initial_coverage = self._get_covered_instructions()\n\n        @symbolic_vm.laser_hook(\"stop_sym_trans\")\n        def execute_stop_sym_trans_hook():\n            end_coverage = self._get_covered_instructions()\n            log.info(\n                \"Number of new instructions covered in tx %d: %d\"\n                % (self.tx_id, end_coverage - self.initial_coverage)\n            )\n            self.tx_id += 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_covered_instructions(self) -> int:\n        total_covered_instructions = 0\n        for _, cv in self.coverage.items():\n            total_covered_instructions += sum(cv[1])\n        return total_covered_instructions", "response": "Gets the total number of covered instructions for all accounts in\n        the svm."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nexecutes a message call transaction from all open states.", "response": "def execute_message_call(laser_evm, callee_address: str) -> None:\n    \"\"\"Executes a message call transaction from all open states.\n\n    :param laser_evm:\n    :param callee_address:\n    \"\"\"\n    # TODO: Resolve circular import between .transaction and ..svm to import LaserEVM here\n    open_states = laser_evm.open_states[:]\n    del laser_evm.open_states[:]\n\n    for open_world_state in open_states:\n        if open_world_state[callee_address].deleted:\n            log.debug(\"Can not execute dead contract, skipping.\")\n            continue\n\n        next_transaction_id = get_next_transaction_id()\n        transaction = MessageCallTransaction(\n            world_state=open_world_state,\n            identifier=next_transaction_id,\n            gas_price=symbol_factory.BitVecSym(\n                \"gas_price{}\".format(next_transaction_id), 256\n            ),\n            gas_limit=8000000,  # block gas limit\n            origin=symbol_factory.BitVecSym(\n                \"origin{}\".format(next_transaction_id), 256\n            ),\n            caller=symbol_factory.BitVecVal(ATTACKER_ADDRESS, 256),\n            callee_account=open_world_state[callee_address],\n            call_data=SymbolicCalldata(next_transaction_id),\n            call_value=symbol_factory.BitVecSym(\n                \"call_value{}\".format(next_transaction_id), 256\n            ),\n        )\n        _setup_global_state_for_execution(laser_evm, transaction)\n\n    laser_evm.exec()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef execute_contract_creation(\n    laser_evm, contract_initialization_code, contract_name=None\n) -> Account:\n    \"\"\"Executes a contract creation transaction from all open states.\n\n    :param laser_evm:\n    :param contract_initialization_code:\n    :param contract_name:\n    :return:\n    \"\"\"\n    # TODO: Resolve circular import between .transaction and ..svm to import LaserEVM here\n    open_states = laser_evm.open_states[:]\n    del laser_evm.open_states[:]\n\n    new_account = laser_evm.world_state.create_account(\n        0, concrete_storage=True, dynamic_loader=None, creator=CREATOR_ADDRESS\n    )\n    if contract_name:\n        new_account.contract_name = contract_name\n\n    for open_world_state in open_states:\n        next_transaction_id = get_next_transaction_id()\n        transaction = ContractCreationTransaction(\n            world_state=open_world_state,\n            identifier=next_transaction_id,\n            gas_price=symbol_factory.BitVecSym(\n                \"gas_price{}\".format(next_transaction_id), 256\n            ),\n            gas_limit=8000000,  # block gas limit\n            origin=symbol_factory.BitVecSym(\n                \"origin{}\".format(next_transaction_id), 256\n            ),\n            code=Disassembly(contract_initialization_code),\n            caller=symbol_factory.BitVecVal(CREATOR_ADDRESS, 256),\n            callee_account=new_account,\n            call_data=[],\n            call_value=symbol_factory.BitVecSym(\n                \"call_value{}\".format(next_transaction_id), 256\n            ),\n        )\n        _setup_global_state_for_execution(laser_evm, transaction)\n    laser_evm.exec(True)\n\n    return new_account", "response": "Executes a contract creation transaction from all open states."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset up global state and cfg for a transactions execution.", "response": "def _setup_global_state_for_execution(laser_evm, transaction) -> None:\n    \"\"\"Sets up global state and cfg for a transactions execution.\n\n    :param laser_evm:\n    :param transaction:\n    \"\"\"\n    # TODO: Resolve circular import between .transaction and ..svm to import LaserEVM here\n    global_state = transaction.initial_global_state()\n    global_state.transaction_stack.append((transaction, None))\n\n    new_node = Node(\n        global_state.environment.active_account.contract_name,\n        function_name=global_state.environment.active_function_name,\n    )\n    if laser_evm.requires_statespace:\n        laser_evm.nodes[new_node.uid] = new_node\n\n    if transaction.world_state.node:\n        if laser_evm.requires_statespace:\n            laser_evm.edges.append(\n                Edge(\n                    transaction.world_state.node.uid,\n                    new_node.uid,\n                    edge_type=JumpType.Transaction,\n                    condition=None,\n                )\n            )\n\n        global_state.mstate.constraints += transaction.world_state.node.constraints\n        new_node.constraints = global_state.mstate.constraints.as_list\n\n    global_state.world_state.transaction_sequence.append(transaction)\n    global_state.node = new_node\n    new_node.states.append(global_state)\n    laser_evm.work_list.append(global_state)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _init_config(self):\n\n        system = platform.system().lower()\n        leveldb_fallback_dir = os.path.expanduser(\"~\")\n        if system.startswith(\"darwin\"):\n            leveldb_fallback_dir = os.path.join(\n                leveldb_fallback_dir, \"Library\", \"Ethereum\"\n            )\n        elif system.startswith(\"windows\"):\n            leveldb_fallback_dir = os.path.join(\n                leveldb_fallback_dir, \"AppData\", \"Roaming\", \"Ethereum\"\n            )\n        else:\n            leveldb_fallback_dir = os.path.join(leveldb_fallback_dir, \".ethereum\")\n        leveldb_fallback_dir = os.path.join(leveldb_fallback_dir, \"geth\", \"chaindata\")\n\n        if not os.path.exists(self.config_path):\n            log.info(\"No config file found. Creating default: \" + self.config_path)\n            open(self.config_path, \"a\").close()\n\n        config = ConfigParser(allow_no_value=True)\n        config.optionxform = str\n        config.read(self.config_path, \"utf-8\")\n        if \"defaults\" not in config.sections():\n            self._add_default_options(config)\n\n        if not config.has_option(\"defaults\", \"leveldb_dir\"):\n            self._add_leveldb_option(config, leveldb_fallback_dir)\n\n        if not config.has_option(\"defaults\", \"dynamic_loading\"):\n            self._add_dynamic_loading_option(config)\n\n        with codecs.open(self.config_path, \"w\", \"utf-8\") as fp:\n            config.write(fp)\n\n        leveldb_dir = config.get(\n            \"defaults\", \"leveldb_dir\", fallback=leveldb_fallback_dir\n        )\n        return os.path.expanduser(leveldb_dir)", "response": "Create the config file and add default options."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _init_solc_binary(version):\n\n        if not version:\n            return os.environ.get(\"SOLC\") or \"solc\"\n\n        # tried converting input to semver, seemed not necessary so just slicing for now\n        main_version = solc.main.get_solc_version_string()\n        main_version_number = re.match(r\"\\d+.\\d+.\\d+\", main_version)\n        if main_version is None:\n            raise CriticalError(\n                \"Could not extract solc version from string {}\".format(main_version)\n            )\n        if version == main_version_number:\n            log.info(\"Given version matches installed version\")\n            solc_binary = os.environ.get(\"SOLC\") or \"solc\"\n        else:\n            solc_binary = util.solc_exists(version)\n            if solc_binary:\n                log.info(\"Given version is already installed\")\n            else:\n                try:\n                    solc.install_solc(\"v\" + version)\n                    solc_binary = util.solc_exists(version)\n                    if not solc_binary:\n                        raise SolcError()\n                except SolcError:\n                    raise CriticalError(\n                        \"There was an error when trying to install the specified solc version\"\n                    )\n\n            log.info(\"Setting the compiler to %s\", solc_binary)\n\n        return solc_binary", "response": "Figure out solc binary and version."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_api_from_config_path(self):\n        config = ConfigParser(allow_no_value=False)\n        config.optionxform = str\n        config.read(self.config_path, \"utf-8\")\n        if config.has_option(\"defaults\", \"dynamic_loading\"):\n            dynamic_loading = config.get(\"defaults\", \"dynamic_loading\")\n        else:\n            dynamic_loading = \"infura\"\n        if dynamic_loading == \"infura\":\n            self.set_api_rpc_infura()\n        elif dynamic_loading == \"localhost\":\n            self.set_api_rpc_localhost()\n        else:\n            self.set_api_rpc(dynamic_loading)", "response": "Set the RPC mode based on a given config file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef And(*args: Union[Bool, bool]) -> Bool:\n    union = []\n    args_list = [arg if isinstance(arg, Bool) else Bool(arg) for arg in args]\n    for arg in args_list:\n        union.append(arg.annotations)\n    return Bool(z3.And([a.raw for a in args_list]), union)", "response": "Create an And expression."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate an or expression.", "response": "def Or(a: Bool, b: Bool) -> Bool:\n    \"\"\"Create an or expression.\n\n    :param a:\n    :param b:\n    :return:\n    \"\"\"\n    union = a.annotations + b.annotations\n    return Bool(z3.Or(a.raw, b.raw), annotations=union)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef Not(a: Bool) -> Bool:\n    return Bool(z3.Not(a.raw), a.annotations)", "response": "Create a Not expression."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef value(self) -> Union[bool, None]:\n        self.simplify()\n        if self.is_true:\n            return True\n        elif self.is_false:\n            return False\n        else:\n            return None", "response": "Returns the concrete value of this bool if concrete otherwise None."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _normalize_byte_sig(byte_sig: str) -> str:\n        if not byte_sig.startswith(\"0x\"):\n            byte_sig = \"0x\" + byte_sig\n        if not len(byte_sig) == 10:\n            raise ValueError(\n                \"Invalid byte signature %s, must have 10 characters\", byte_sig\n            )\n        return byte_sig", "response": "Normalizes the byte signature string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add(self, byte_sig: str, text_sig: str) -> None:\n        byte_sig = self._normalize_byte_sig(byte_sig)\n        with SQLiteDB(self.path) as cur:\n            # ignore new row if it's already in the DB (and would cause a unique constraint error)\n            cur.execute(\n                \"INSERT OR IGNORE INTO signatures (byte_sig, text_sig) VALUES (?,?)\",\n                (byte_sig, text_sig),\n            )", "response": "Adds a new byte - text signature pair to the database."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget a function text signature for a byte signature", "response": "def get(self, byte_sig: str, online_timeout: int = 2) -> List[str]:\n        \"\"\"Get a function text signature for a byte signature 1) try local\n        cache 2) try online lookup (if enabled; if not flagged as unavailable)\n\n        :param byte_sig: function signature hash as hexstr\n        :param online_timeout: online lookup timeout\n        :return: list of matching function text signatures\n        \"\"\"\n\n        byte_sig = self._normalize_byte_sig(byte_sig)\n\n        # check if we have any Solidity signatures to look up\n        text_sigs = self.solidity_sigs.get(byte_sig)\n        if text_sigs is not None:\n            return text_sigs\n\n        # try lookup in the local DB\n        with SQLiteDB(self.path) as cur:\n            cur.execute(\"SELECT text_sig FROM signatures WHERE byte_sig=?\", (byte_sig,))\n            text_sigs = cur.fetchall()\n\n        if text_sigs:\n            return [t[0] for t in text_sigs]\n\n        # abort if we're not allowed to check 4byte or we already missed\n        # the signature, or we're on a timeout\n        if (\n            not self.enable_online_lookup\n            or byte_sig in self.online_lookup_miss\n            or time.time() < self.online_lookup_timeout\n        ):\n            return []\n\n        try:\n            text_sigs = self.lookup_online(byte_sig=byte_sig, timeout=online_timeout)\n            if not text_sigs:\n                self.online_lookup_miss.add(byte_sig)\n                return []\n            else:\n                for resolved in text_sigs:\n                    self.add(byte_sig, resolved)\n                return text_sigs\n        except FourByteDirectoryOnlineLookupError as fbdole:\n            # wait at least 2 mins to try again\n            self.online_lookup_timeout = int(time.time()) + 2 * 60\n            log.warning(\"Online lookup failed, not retrying for 2min: %s\", fbdole)\n            return []"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef import_solidity_file(\n        self, file_path: str, solc_binary: str = \"solc\", solc_args: str = None\n    ):\n        \"\"\"Import Function Signatures from solidity source files.\n\n        :param solc_binary:\n        :param solc_args:\n        :param file_path: solidity source code file path\n        :return:\n        \"\"\"\n        cmd = [solc_binary, \"--hashes\", file_path]\n        if solc_args:\n            cmd.extend(solc_args.split())\n\n        try:\n            p = Popen(cmd, stdout=PIPE, stderr=PIPE)\n            stdout, stderr = p.communicate()\n            ret = p.returncode\n\n            if ret != 0:\n                raise CompilerError(\n                    \"Solc has experienced a fatal error (code {}).\\n\\n{}\".format(\n                        ret, stderr.decode(\"utf-8\")\n                    )\n                )\n        except FileNotFoundError:\n            raise CompilerError(\n                (\n                    \"Compiler not found. Make sure that solc is installed and in PATH, \"\n                    \"or the SOLC environment variable is set.\"\n                )\n            )\n\n        stdout = stdout.decode(\"unicode_escape\").split(\"\\n\")\n        for line in stdout:\n            # the ':' need not be checked but just to be sure\n            if all(map(lambda x: x in line, [\"(\", \")\", \":\"])):\n                solc_bytes = \"0x\" + line.split(\":\")[0]\n                solc_text = line.split(\":\")[1].strip()\n                self.solidity_sigs[solc_bytes].append(solc_text)\n        log.debug(\n            \"Signatures: found %d signatures after parsing\" % len(self.solidity_sigs)\n        )\n\n        # update DB with what we've found\n        for byte_sig, text_sigs in self.solidity_sigs.items():\n            for text_sig in text_sigs:\n                self.add(byte_sig, text_sig)", "response": "Import Function Signatures from solidity source files."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef execute(self, state: GlobalState):\n        address = _get_address_from_state(state)\n        has_overflow = self._overflow_cache.get(address, False)\n        has_underflow = self._underflow_cache.get(address, False)\n        if has_overflow or has_underflow:\n            return\n        opcode = state.get_current_instruction()[\"opcode\"]\n        funcs = {\n            \"ADD\": [self._handle_add],\n            \"SUB\": [self._handle_sub],\n            \"MUL\": [self._handle_mul],\n            \"SSTORE\": [self._handle_sstore],\n            \"JUMPI\": [self._handle_jumpi],\n            \"RETURN\": [self._handle_return, self._handle_transaction_end],\n            \"STOP\": [self._handle_transaction_end],\n            \"EXP\": [self._handle_exp],\n        }\n        for func in funcs[opcode]:\n            func(state)", "response": "Executes analysis module for integer underflow and integer overflow."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _handle_return(state: GlobalState) -> None:\n        stack = state.mstate.stack\n        try:\n            offset, length = get_concrete_int(stack[-1]), get_concrete_int(stack[-2])\n        except TypeError:\n            return\n        for element in state.mstate.memory[offset : offset + length]:\n            if not isinstance(element, Expression):\n                continue\n            for annotation in element.annotations:\n                if isinstance(annotation, OverUnderflowAnnotation):\n                    state.annotate(\n                        OverUnderflowStateAnnotation(\n                            annotation.overflowing_state,\n                            annotation.operator,\n                            annotation.constraint,\n                        )\n                    )", "response": "Adds all the annotations that are added to the state which correspond to the locations in the memory returned by RETURN opcode."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the source data from the contracts list", "response": "def get_source_from_contracts_list(self, contracts):\n        \"\"\"\n        get the source data from the contracts list\n        :param contracts: the list of contracts\n        :return:\n        \"\"\"\n        if contracts is None or len(contracts) == 0:\n            return\n        if isinstance(contracts[0], SolidityContract):\n            self.source_type = \"solidity-file\"\n            self.source_format = \"text\"\n            for contract in contracts:\n                self.source_list += [file.filename for file in contract.solidity_files]\n                self._source_hash.append(contract.bytecode_hash)\n                self._source_hash.append(contract.creation_bytecode_hash)\n        elif isinstance(contracts[0], EVMContract):\n            self.source_format = \"evm-byzantium-bytecode\"\n            self.source_type = (\n                \"ethereum-address\"\n                if len(contracts[0].name) == 42 and contracts[0].name[0:2] == \"0x\"\n                else \"raw-bytecode\"\n            )\n            for contract in contracts:\n                if contract.creation_code:\n                    self.source_list.append(contract.creation_bytecode_hash)\n                if contract.code:\n                    self.source_list.append(contract.bytecode_hash)\n            self._source_hash = self.source_list\n\n        else:\n            assert False"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfinds the index of the contract in the list that corresponds to the given bytecode hash.", "response": "def get_source_index(self, bytecode_hash: str) -> int:\n        \"\"\"\n        Find the contract index in the list\n        :param bytecode_hash: The contract hash\n        :return: The index of the contract in the _source_hash list\n        \"\"\"\n        # TODO: Add this part to exception logs\n        try:\n            return self._source_hash.index(bytecode_hash)\n        except ValueError:\n            self._source_hash.append(bytecode_hash)\n            return len(self._source_hash) - 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_transaction_sequence(global_state, constraints):\n\n    transaction_sequence = global_state.world_state.transaction_sequence\n\n    # gaslimit & gasprice don't exist yet\n    tx_template = {\n        \"calldata\": None,\n        \"call_value\": None,\n        \"caller\": \"0xCA11EDEADBEEF37E636E6CA11EDEADBEEFCA11ED\",\n    }\n\n    concrete_transactions = {}\n    creation_tx_ids = []\n    tx_constraints = constraints.copy()\n    minimize = []\n\n    transactions = []\n    for transaction in transaction_sequence:\n        tx_id = str(transaction.id)\n        if not isinstance(transaction, ContractCreationTransaction):\n            transactions.append(transaction)\n            # Constrain calldatasize\n            max_calldatasize = symbol_factory.BitVecVal(5000, 256)\n            tx_constraints.append(\n                UGE(max_calldatasize, transaction.call_data.calldatasize)\n            )\n\n            minimize.append(transaction.call_data.calldatasize)\n            minimize.append(transaction.call_value)\n\n            concrete_transactions[tx_id] = tx_template.copy()\n\n        else:\n            creation_tx_ids.append(tx_id)\n\n    model = get_model(tx_constraints, minimize=minimize)\n\n    for transaction in transactions:\n        tx_id = str(transaction.id)\n\n        concrete_transactions[tx_id][\"calldata\"] = \"0x\" + \"\".join(\n            [\n                hex(b)[2:] if len(hex(b)) % 2 == 0 else \"0\" + hex(b)[2:]\n                for b in transaction.call_data.concrete(model)\n            ]\n        )\n\n        concrete_transactions[tx_id][\"call_value\"] = (\n            \"0x%x\"\n            % model.eval(transaction.call_value.raw, model_completion=True).as_long()\n        )\n        concrete_transactions[tx_id][\"caller\"] = \"0x\" + (\n            \"%x\" % model.eval(transaction.caller.raw, model_completion=True).as_long()\n        ).zfill(40)\n\n    return concrete_transactions", "response": "Generate concrete transaction sequence."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef exit_with_error(format_, message):\n    if format_ == \"text\" or format_ == \"markdown\":\n        log.error(message)\n    elif format_ == \"json\":\n        result = {\"success\": False, \"error\": str(message), \"issues\": []}\n        print(json.dumps(result))\n    else:\n        result = [\n            {\n                \"issues\": [],\n                \"sourceType\": \"\",\n                \"sourceFormat\": \"\",\n                \"sourceList\": [],\n                \"meta\": {\n                    \"logs\": [{\"level\": \"error\", \"hidden\": \"true\", \"msg\": message}]\n                },\n            }\n        ]\n        print(json.dumps(result))\n    sys.exit()", "response": "exit with error message"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates the argument parser by setting all possible arguments.", "response": "def create_parser(parser: argparse.ArgumentParser) -> None:\n    \"\"\"\n    Creates the parser by setting all the possible arguments\n    :param parser: The parser\n    \"\"\"\n    parser.add_argument(\"solidity_file\", nargs=\"*\")\n\n    commands = parser.add_argument_group(\"commands\")\n    commands.add_argument(\"-g\", \"--graph\", help=\"generate a control flow graph\")\n    commands.add_argument(\n        \"-V\",\n        \"--version\",\n        action=\"store_true\",\n        help=\"print the Mythril version number and exit\",\n    )\n    commands.add_argument(\n        \"-x\",\n        \"--fire-lasers\",\n        action=\"store_true\",\n        help=\"detect vulnerabilities, use with -c, -a or solidity file(s)\",\n    )\n    commands.add_argument(\n        \"--truffle\",\n        action=\"store_true\",\n        help=\"analyze a truffle project (run from project dir)\",\n    )\n    commands.add_argument(\n        \"-d\", \"--disassemble\", action=\"store_true\", help=\"print disassembly\"\n    )\n    commands.add_argument(\n        \"-j\",\n        \"--statespace-json\",\n        help=\"dumps the statespace json\",\n        metavar=\"OUTPUT_FILE\",\n    )\n\n    inputs = parser.add_argument_group(\"input arguments\")\n    inputs.add_argument(\n        \"-c\",\n        \"--code\",\n        help='hex-encoded bytecode string (\"6060604052...\")',\n        metavar=\"BYTECODE\",\n    )\n    inputs.add_argument(\n        \"-f\",\n        \"--codefile\",\n        help=\"file containing hex-encoded bytecode string\",\n        metavar=\"BYTECODEFILE\",\n        type=argparse.FileType(\"r\"),\n    )\n    inputs.add_argument(\n        \"-a\",\n        \"--address\",\n        help=\"pull contract from the blockchain\",\n        metavar=\"CONTRACT_ADDRESS\",\n    )\n    inputs.add_argument(\n        \"-l\",\n        \"--dynld\",\n        action=\"store_true\",\n        help=\"auto-load dependencies from the blockchain\",\n    )\n    inputs.add_argument(\n        \"--no-onchain-storage-access\",\n        action=\"store_true\",\n        help=\"turns off getting the data from onchain contracts\",\n    )\n    inputs.add_argument(\n        \"--bin-runtime\",\n        action=\"store_true\",\n        help=\"Only when -c or -f is used. Consider the input bytecode as binary runtime code, default being the contract creation bytecode.\",\n    )\n\n    outputs = parser.add_argument_group(\"output formats\")\n    outputs.add_argument(\n        \"-o\",\n        \"--outform\",\n        choices=[\"text\", \"markdown\", \"json\", \"jsonv2\"],\n        default=\"text\",\n        help=\"report output format\",\n        metavar=\"<text/markdown/json/jsonv2>\",\n    )\n    outputs.add_argument(\n        \"--verbose-report\",\n        action=\"store_true\",\n        help=\"Include debugging information in report\",\n    )\n\n    database = parser.add_argument_group(\"local contracts database\")\n    database.add_argument(\n        \"-s\", \"--search\", help=\"search the contract database\", metavar=\"EXPRESSION\"\n    )\n    database.add_argument(\n        \"--leveldb-dir\",\n        help=\"specify leveldb directory for search or direct access operations\",\n        metavar=\"LEVELDB_PATH\",\n    )\n\n    utilities = parser.add_argument_group(\"utilities\")\n    utilities.add_argument(\n        \"--hash\", help=\"calculate function signature hash\", metavar=\"SIGNATURE\"\n    )\n    utilities.add_argument(\n        \"--storage\",\n        help=\"read state variables from storage index, use with -a\",\n        metavar=\"INDEX,NUM_SLOTS,[array] / mapping,INDEX,[KEY1, KEY2...]\",\n    )\n    utilities.add_argument(\n        \"--solv\",\n        help=\"specify solidity compiler version. If not present, will try to install it (Experimental)\",\n        metavar=\"SOLV\",\n    )\n    utilities.add_argument(\n        \"--contract-hash-to-address\",\n        help=\"returns corresponding address for a contract address hash\",\n        metavar=\"SHA3_TO_LOOK_FOR\",\n    )\n\n    options = parser.add_argument_group(\"options\")\n    options.add_argument(\n        \"-m\",\n        \"--modules\",\n        help=\"Comma-separated list of security analysis modules\",\n        metavar=\"MODULES\",\n    )\n    options.add_argument(\n        \"--max-depth\",\n        type=int,\n        default=50,\n        help=\"Maximum recursion depth for symbolic execution\",\n    )\n\n    options.add_argument(\n        \"--strategy\",\n        choices=[\"dfs\", \"bfs\", \"naive-random\", \"weighted-random\"],\n        default=\"bfs\",\n        help=\"Symbolic execution strategy\",\n    )\n    options.add_argument(\n        \"-t\",\n        \"--transaction-count\",\n        type=int,\n        default=2,\n        help=\"Maximum number of transactions issued by laser\",\n    )\n    options.add_argument(\n        \"--execution-timeout\",\n        type=int,\n        default=600,\n        help=\"The amount of seconds to spend on symbolic execution\",\n    )\n    options.add_argument(\n        \"--create-timeout\",\n        type=int,\n        default=10,\n        help=\"The amount of seconds to spend on \" \"the initial contract creation\",\n    )\n    options.add_argument(\"--solc-args\", help=\"Extra arguments for solc\")\n    options.add_argument(\n        \"--phrack\", action=\"store_true\", help=\"Phrack-style call graph\"\n    )\n    options.add_argument(\n        \"--enable-physics\", action=\"store_true\", help=\"enable graph physics simulation\"\n    )\n    options.add_argument(\n        \"-v\", type=int, help=\"log level (0-5)\", metavar=\"LOG_LEVEL\", default=2\n    )\n    options.add_argument(\n        \"-q\",\n        \"--query-signature\",\n        action=\"store_true\",\n        help=\"Lookup function signatures through www.4byte.directory\",\n    )\n    options.add_argument(\n        \"--enable-iprof\", action=\"store_true\", help=\"enable the instruction profiler\"\n    )\n\n    rpc = parser.add_argument_group(\"RPC options\")\n\n    rpc.add_argument(\n        \"--rpc\",\n        help=\"custom RPC settings\",\n        metavar=\"HOST:PORT / ganache / infura-[network_name]\",\n        default=\"infura-mainnet\",\n    )\n    rpc.add_argument(\n        \"--rpctls\", type=bool, default=False, help=\"RPC connection over TLS\"\n    )\n    parser.add_argument(\"--epic\", action=\"store_true\", help=argparse.SUPPRESS)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_args(parser: argparse.ArgumentParser, args: argparse.Namespace) -> None:\n\n    if args.epic:\n        path = os.path.dirname(os.path.realpath(__file__))\n        sys.argv.remove(\"--epic\")\n        os.system(\" \".join(sys.argv) + \" | python3 \" + path + \"/epic.py\")\n        sys.exit()\n\n    if args.version:\n        if args.outform == \"json\":\n            print(json.dumps({\"version_str\": VERSION}))\n        else:\n            print(\"Mythril version {}\".format(VERSION))\n        sys.exit()\n\n    # Parse cmdline args\n    validate_args(parser, args)\n    try:\n        quick_commands(args)\n        config = set_config(args)\n        leveldb_search(config, args)\n        disassembler = MythrilDisassembler(\n            eth=config.eth,\n            solc_version=args.solv,\n            solc_args=args.solc_args,\n            enable_online_lookup=args.query_signature,\n        )\n        if args.truffle:\n            try:\n                disassembler.analyze_truffle_project(args)\n            except FileNotFoundError:\n                print(\n                    \"Build directory not found. Make sure that you start the analysis from the project root, and that 'truffle compile' has executed successfully.\"\n                )\n            sys.exit()\n\n        address = get_code(disassembler, args)\n        execute_command(\n            disassembler=disassembler, address=address, parser=parser, args=args\n        )\n    except CriticalError as ce:\n        exit_with_error(args.outform, str(ce))\n    except Exception:\n        exit_with_error(args.outform, traceback.format_exc())", "response": "Parses the arguments and executes the appropriate command."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting mapped contract_address by its hash", "response": "def get_contract_by_hash(self, contract_hash):\n        \"\"\"get mapped contract_address by its hash, if not found try\n        indexing.\"\"\"\n        contract_address = self.db.reader._get_address_by_hash(contract_hash)\n        if contract_address is not None:\n            return contract_address\n\n        else:\n            raise AddressNotFoundError"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _process(self, startblock):\n        log.debug(\"Processing blocks %d to %d\" % (startblock, startblock + BATCH_SIZE))\n\n        addresses = []\n\n        for blockNum in range(startblock, startblock + BATCH_SIZE):\n            block_hash = self.db.reader._get_block_hash(blockNum)\n            if block_hash is not None:\n                receipts = self.db.reader._get_block_receipts(block_hash, blockNum)\n\n                for receipt in receipts:\n                    if receipt.contractAddress is not None and not all(\n                        b == 0 for b in receipt.contractAddress\n                    ):\n                        addresses.append(receipt.contractAddress)\n            else:\n                if len(addresses) == 0:\n                    raise Exception()\n\n        return addresses", "response": "Processes the block and returns a list of all the addresses that are in the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef instruction_list_to_easm(instruction_list: list) -> str:\n    result = \"\"\n\n    for instruction in instruction_list:\n        result += \"{} {}\".format(instruction[\"address\"], instruction[\"opcode\"])\n        if \"argument\" in instruction:\n            result += \" \" + instruction[\"argument\"]\n        result += \"\\n\"\n\n    return result", "response": "Convert a list of instructions into an easm op code string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_opcode_from_name(operation_name: str) -> int:\n    for op_code, value in opcodes.items():\n        if operation_name == value[0]:\n            return op_code\n    raise RuntimeError(\"Unknown opcode\")", "response": "Get an opcode based on its name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning all indices in instruction_list that point to instruction sequences following a pattern.", "response": "def find_op_code_sequence(pattern: list, instruction_list: list) -> Generator:\n    \"\"\"Returns all indices in instruction_list that point to instruction\n    sequences following a pattern.\n\n    :param pattern: The pattern to look for, e.g. [[\"PUSH1\", \"PUSH2\"], [\"EQ\"]] where [\"PUSH1\", \"EQ\"] satisfies pattern\n    :param instruction_list: List of instructions to look in\n    :return: Indices to the instruction sequences\n    \"\"\"\n    for i in range(0, len(instruction_list) - len(pattern) + 1):\n        if is_sequence_match(pattern, instruction_list, i):\n            yield i"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_sequence_match(pattern: list, instruction_list: list, index: int) -> bool:\n    for index, pattern_slot in enumerate(pattern, start=index):\n        try:\n            if not instruction_list[index][\"opcode\"] in pattern_slot:\n                return False\n        except IndexError:\n            return False\n    return True", "response": "Checks if the instructions starting at index follow a pattern."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndisassembles an evm bytecode and returns a list of instructions.", "response": "def disassemble(bytecode: bytes) -> list:\n    \"\"\"Disassembles evm bytecode and returns a list of instructions.\n\n    :param bytecode:\n    :return:\n    \"\"\"\n    instruction_list = []\n    address = 0\n    length = len(bytecode)\n    if \"bzzr\" in str(bytecode[-43:]):\n        # ignore swarm hash\n        length -= 43\n\n    while address < length:\n        try:\n            op_code = opcodes[bytecode[address]]\n        except KeyError:\n            instruction_list.append(EvmInstruction(address, \"INVALID\"))\n            address += 1\n            continue\n\n        op_code_name = op_code[0]\n        current_instruction = EvmInstruction(address, op_code_name)\n\n        match = re.search(regex_PUSH, op_code_name)\n        if match:\n            argument_bytes = bytecode[address + 1 : address + 1 + int(match.group(1))]\n            current_instruction.argument = \"0x\" + argument_bytes.hex()\n            address += int(match.group(1))\n\n        instruction_list.append(current_instruction)\n        address += 1\n\n    # We use a to_dict() here for compatibility reasons\n    return [element.to_dict() for element in instruction_list]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_code_hash(code: str) -> str:\n    code = code[2:] if code[:2] == \"0x\" else code\n    try:\n        keccak = sha3.keccak_256()\n        keccak.update(bytes.fromhex(code))\n        return \"0x\" + keccak.hexdigest()\n    except ValueError:\n        log.debug(\"Unable to change the bytecode to bytes. Bytecode: {}\".format(code))\n        return \"\"", "response": "Returns the hash of the given bytecode"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload the plugin from the symbolic virtual machine.", "response": "def load(self, laser_plugin: LaserPlugin) -> None:\n        \"\"\" Loads the plugin\n\n        :param laser_plugin: plugin that will be loaded in the symbolic virtual machine\n        \"\"\"\n        log.info(\"Loading plugin: {}\".format(str(laser_plugin)))\n        laser_plugin.initialize(self.symbolic_vm)\n        self.laser_plugins.append(laser_plugin)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmeasuring statistics for annotated smt query check function", "response": "def stat_smt_query(func: Callable):\n    \"\"\"Measures statistics for annotated smt query check function\"\"\"\n    stat_store = SolverStatistics()\n\n    def function_wrapper(*args, **kwargs):\n        if not stat_store.enabled:\n            return func(*args, **kwargs)\n\n        stat_store.query_count += 1\n        begin = time()\n\n        result = func(*args, **kwargs)\n\n        end = time()\n        stat_store.solver_time += end - begin\n\n        return result\n\n    return function_wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a new non - contract account.", "response": "def create_account(\n        self,\n        balance=0,\n        address=None,\n        concrete_storage=False,\n        dynamic_loader=None,\n        creator=None,\n    ) -> Account:\n        \"\"\"Create non-contract account.\n\n        :param address: The account's address\n        :param balance: Initial balance for the account\n        :param concrete_storage: Interpret account storage as concrete\n        :param dynamic_loader: used for dynamically loading storage from the block chain\n        :return: The new account\n        \"\"\"\n        address = address if address else self._generate_new_address(creator)\n        new_account = Account(\n            address,\n            balance=balance,\n            dynamic_loader=dynamic_loader,\n            concrete_storage=concrete_storage,\n        )\n        self._put_account(new_account)\n        return new_account"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a new contract account based on the contract code and storage provided", "response": "def create_initialized_contract_account(self, contract_code, storage) -> None:\n        \"\"\"Creates a new contract account, based on the contract code and\n        storage provided The contract code only includes the runtime contract\n        bytecode.\n\n        :param contract_code: Runtime bytecode for the contract\n        :param storage: Initial storage for the contract\n        :return: The new account\n        \"\"\"\n        # TODO: Add type hints\n        new_account = Account(\n            self._generate_new_address(), code=contract_code, balance=0\n        )\n        new_account.storage = storage\n        self._put_account(new_account)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_annotations(self, annotation_type: type) -> Iterator[StateAnnotation]:\n        return filter(lambda x: isinstance(x, annotation_type), self.annotations)", "response": "Returns an iterator over the annotations of the specified type."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate a new address for the global state.", "response": "def _generate_new_address(self, creator=None) -> str:\n        \"\"\"Generates a new address for the global state.\n\n        :return:\n        \"\"\"\n        if creator:\n            # TODO: Use nounce\n            return \"0x\" + str(mk_contract_address(creator, 0).hex())\n        while True:\n            address = \"0x\" + \"\".join([str(hex(randint(0, 16)))[-1] for _ in range(40)])\n            if address not in self.accounts.keys():\n                return address"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef build_benchmark_plugin(name: str) -> LaserPlugin:\n        from mythril.laser.ethereum.plugins.implementations.benchmark import (\n            BenchmarkPlugin,\n        )\n\n        return BenchmarkPlugin(name)", "response": "Builds an instance of the benchmark plugin with the given name."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef build_mutation_pruner_plugin() -> LaserPlugin:\n        from mythril.laser.ethereum.plugins.implementations.mutation_pruner import (\n            MutationPruner,\n        )\n\n        return MutationPruner()", "response": "Creates an instance of the mutation pruner plugin"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate an instance of the instruction coverage plugin", "response": "def build_instruction_coverage_plugin() -> LaserPlugin:\n        \"\"\" Creates an instance of the instruction coverage plugin\"\"\"\n        from mythril.laser.ethereum.plugins.implementations.coverage import (\n            InstructionCoveragePlugin,\n        )\n\n        return InstructionCoveragePlugin()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the serializable state space of the contract.", "response": "def dump_statespace(self, contract: EVMContract = None) -> str:\n        \"\"\"\n        Returns serializable statespace of the contract\n        :param contract: The Contract on which the analysis should be done\n        :return: The serialized state space\n        \"\"\"\n        sym = SymExecWrapper(\n            contract or self.contracts[0],\n            self.address,\n            self.strategy,\n            dynloader=DynLoader(\n                self.eth,\n                storage_loading=self.onchain_storage_access,\n                contract_loading=self.dynld,\n            ),\n            max_depth=self.max_depth,\n            execution_timeout=self.execution_timeout,\n            create_timeout=self.create_timeout,\n            enable_iprof=self.enable_iprof,\n        )\n\n        return get_serializable_statespace(sym)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfire all the issues and vulnerabilities for the given modules.", "response": "def fire_lasers(\n        self,\n        modules: Optional[List[str]] = None,\n        verbose_report: bool = False,\n        transaction_count: Optional[int] = None,\n    ) -> Report:\n        \"\"\"\n        :param modules: The analysis modules which should be executed\n        :param verbose_report: Gives out the transaction sequence of the vulnerability\n        :param transaction_count: The amount of transactions to be executed\n        :return: The Report class which contains the all the issues/vulnerabilities\n        \"\"\"\n        all_issues = []  # type: List[Issue]\n        SolverStatistics().enabled = True\n        exceptions = []\n        for contract in self.contracts:\n            StartTime()  # Reinitialize start time for new contracts\n            try:\n                sym = SymExecWrapper(\n                    contract,\n                    self.address,\n                    self.strategy,\n                    dynloader=DynLoader(\n                        self.eth,\n                        storage_loading=self.onchain_storage_access,\n                        contract_loading=self.dynld,\n                    ),\n                    max_depth=self.max_depth,\n                    execution_timeout=self.execution_timeout,\n                    create_timeout=self.create_timeout,\n                    transaction_count=transaction_count,\n                    modules=modules,\n                    compulsory_statespace=False,\n                    enable_iprof=self.enable_iprof,\n                )\n                issues = fire_lasers(sym, modules)\n            except KeyboardInterrupt:\n                log.critical(\"Keyboard Interrupt\")\n                issues = retrieve_callback_issues(modules)\n            except Exception:\n                log.critical(\n                    \"Exception occurred, aborting analysis. Please report this issue to the Mythril GitHub page.\\n\"\n                    + traceback.format_exc()\n                )\n                issues = retrieve_callback_issues(modules)\n                exceptions.append(traceback.format_exc())\n            for issue in issues:\n                issue.add_code_info(contract)\n\n            all_issues += issues\n            log.info(\"Solver statistics: \\n{}\".format(str(SolverStatistics())))\n\n        source_data = Source()\n        source_data.get_source_from_contracts_list(self.contracts)\n        # Finally, output the results\n        report = Report(verbose_report, contracts=self.contracts, exceptions=exceptions)\n        for issue in all_issues:\n            report.append_issue(issue)\n\n        return report"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nanalyzing the state and returns the issues for that current state.", "response": "def _analyze_states(state: GlobalState) -> List[Issue]:\n    \"\"\"\n    :param state: the current state\n    :return: returns the issues for that corresponding state\n    \"\"\"\n    call = get_call_from_state(state)\n    if call is None:\n        return []\n    issues = []  # type: List[Issue]\n\n    if call.type is not \"DELEGATECALL\":\n        return []\n    if state.environment.active_function_name is not \"fallback\":\n        return []\n\n    state = call.state\n    address = state.get_current_instruction()[\"address\"]\n    meminstart = get_variable(state.mstate.stack[-3])\n\n    if meminstart.type == VarType.CONCRETE:\n        issues += _concrete_call(call, state, address, meminstart)\n\n    return issues"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _concrete_call(\n    call: Call, state: GlobalState, address: int, meminstart: Variable\n) -> List[Issue]:\n    \"\"\"\n    :param call: The current call's information\n    :param state: The current state\n    :param address: The PC address\n    :param meminstart: memory starting position\n    :return: issues\n    \"\"\"\n    if not re.search(r\"calldata.*\\[0\", str(state.mstate.memory[meminstart.val])):\n        return []\n\n    issue = Issue(\n        contract=state.environment.active_account.contract_name,\n        function_name=state.environment.active_function_name,\n        address=address,\n        swc_id=DELEGATECALL_TO_UNTRUSTED_CONTRACT,\n        bytecode=state.environment.code.bytecode,\n        title=\"Delegatecall Proxy\",\n        severity=\"Low\",\n        description_head=\"The contract implements a delegatecall proxy.\",\n        description_tail=\"The smart contract forwards the received calldata via delegatecall. Note that callers \"\n        \"can execute arbitrary functions in the callee contract and that the callee contract \"\n        \"can access the storage of the calling contract. \"\n        \"Make sure that the callee contract is audited properly.\",\n        gas_used=(state.mstate.min_gas_used, state.mstate.max_gas_used),\n    )\n\n    target = hex(call.to.val) if call.to.type == VarType.CONCRETE else str(call.to)\n    issue.description += \"DELEGATECALL target: {}\".format(target)\n\n    return [issue]", "response": "Returns a list of issues that can be used to concrete the call."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_word_at(self, index: int) -> Union[int, BitVec]:\n        try:\n            return symbol_factory.BitVecVal(\n                util.concrete_int_from_bytes(\n                    bytes([util.get_concrete_int(b) for b in self[index : index + 32]]),\n                    0,\n                ),\n                256,\n            )\n        except TypeError:\n            result = simplify(\n                Concat(\n                    [\n                        b if isinstance(b, BitVec) else symbol_factory.BitVecVal(b, 8)\n                        for b in cast(\n                            List[Union[int, BitVec]], self[index : index + 32]\n                        )\n                    ]\n                )\n            )\n            assert result.size() == 256\n            return result", "response": "Access a 32 byte word from a specified memory index."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrite a 32 byte word to memory at the specified index.", "response": "def write_word_at(self, index: int, value: Union[int, BitVec, bool, Bool]) -> None:\n        \"\"\"Writes a 32 byte word to memory at the specified index`\n\n        :param index: index to write to\n        :param value: the value to write to memory\n        \"\"\"\n        try:\n            # Attempt to concretize value\n            if isinstance(value, bool):\n                _bytes = (\n                    int(1).to_bytes(32, byteorder=\"big\")\n                    if value\n                    else int(0).to_bytes(32, byteorder=\"big\")\n                )\n            else:\n                _bytes = util.concrete_int_to_bytes(value)\n            assert len(_bytes) == 32\n            self[index : index + 32] = list(bytearray(_bytes))\n        except (Z3Exception, AttributeError):  # BitVector or BoolRef\n            value = cast(Union[BitVec, Bool], value)\n            if isinstance(value, Bool):\n                value_to_write = If(\n                    value,\n                    symbol_factory.BitVecVal(1, 256),\n                    symbol_factory.BitVecVal(0, 256),\n                )\n            else:\n                value_to_write = value\n            assert value_to_write.size() == 256\n\n            for i in range(0, value_to_write.size(), 8):\n                self[index + 31 - (i // 8)] = Extract(i + 7, i, value_to_write)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef initialize(self, symbolic_vm: LaserEVM):\n        self._reset()\n\n        @symbolic_vm.laser_hook(\"execute_state\")\n        def execute_state_hook(_):\n            current_time = time() - self.begin\n            self.nr_of_executed_insns += 1\n\n            for key, value in symbolic_vm.coverage.items():\n                try:\n                    self.coverage[key][current_time] = sum(value[1]) * 100 / value[0]\n                except KeyError:\n                    self.coverage[key] = {}\n                    self.coverage[key][current_time] = sum(value[1]) * 100 / value[0]\n\n        @symbolic_vm.laser_hook(\"start_sym_exec\")\n        def start_sym_exec_hook():\n            self.begin = time()\n\n        @symbolic_vm.laser_hook(\"stop_sym_exec\")\n        def stop_sym_exec_hook():\n            self.end = time()\n\n            self._write_to_graph()\n            self._store_report()", "response": "Initializes the BenchmarkPlugin\n        Introduces hooks in symbolic_vm to track the desired values"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nresets this plugin s internal state.", "response": "def _reset(self):\n        \"\"\"Reset this plugin\"\"\"\n        self.nr_of_executed_insns = 0\n        self.begin = None\n        self.end = None\n        self.coverage = {}"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _write_to_graph(self):\n        traces = []\n        for byte_code, trace_data in self.coverage.items():\n            traces += [list(trace_data.keys()), list(trace_data.values()), \"r--\"]\n\n        plt.plot(*traces)\n        plt.axis([0, self.end - self.begin, 0, 100])\n        plt.xlabel(\"Duration (seconds)\")\n        plt.ylabel(\"Coverage (percentage)\")\n\n        plt.savefig(\"{}.png\".format(self.name))", "response": "Write the coverage results to a graph"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the variables that make up the current expression", "response": "def _get_expr_variables(expression: z3.ExprRef) -> List[z3.ExprRef]:\n    \"\"\"\n    Gets the variables that make up the current expression\n    :param expression:\n    :return:\n    \"\"\"\n    result = []\n    if not expression.children() and not isinstance(expression, z3.BitVecNumRef):\n        result.append(expression)\n    for child in expression.children():\n        c_children = _get_expr_variables(child)\n        result.extend(c_children)\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_condition(self, condition: z3.BoolRef) -> None:\n        variables = set(_get_expr_variables(condition))\n        relevant_buckets = set()\n        for variable in variables:\n            try:\n                bucket = self.variable_map[str(variable)]\n                relevant_buckets.add(bucket)\n            except KeyError:\n                continue\n\n        new_bucket = DependenceBucket(variables, [condition])\n        self.buckets.append(new_bucket)\n\n        if relevant_buckets:\n            # Merge buckets, and rewrite variable map accordingly\n            relevant_buckets.add(new_bucket)\n            new_bucket = self._merge_buckets(relevant_buckets)\n\n        for variable in new_bucket.variables:\n            self.variable_map[str(variable)] = new_bucket", "response": "Adds a condition to the dependence map."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmerge the buckets in the list into a new bucket.", "response": "def _merge_buckets(self, bucket_list: Set[DependenceBucket]) -> DependenceBucket:\n        \"\"\" Merges the buckets in bucket list \"\"\"\n        variables = []  # type: List[str]\n        conditions = []  # type: List[z3.BoolRef]\n        for bucket in bucket_list:\n            self.buckets.remove(bucket)\n            variables += bucket.variables\n            conditions += bucket.conditions\n\n        new_bucket = DependenceBucket(variables, conditions)\n        self.buckets.append(new_bucket)\n\n        return new_bucket"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add(self, *constraints: Tuple[Bool]) -> None:\n        raw_constraints = [\n            c.raw for c in cast(Tuple[Bool], constraints)\n        ]  # type: List[z3.BoolRef]\n        self.constraints.extend(raw_constraints)", "response": "Adds the constraints to this solver."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check(self) -> z3.CheckSatResult:\n        dependence_map = DependenceMap()\n        for constraint in self.constraints:\n            dependence_map.add_condition(constraint)\n\n        self.models = []\n        for bucket in dependence_map.buckets:\n            self.raw.reset()\n            self.raw.append(*bucket.conditions)\n            check_result = self.raw.check()\n            if check_result == z3.sat:\n                self.models.append(self.raw.model())\n            else:\n                return check_result\n\n        return z3.sat", "response": "Checks the smt for the current state of the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef annotate(self, annotation: Any) -> None:\n        if isinstance(annotation, list):\n            self._annotations += annotation\n        else:\n            self._annotations.append(annotation)", "response": "Annotates this expression with the given annotation."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nexecute a message call transaction from all open states.", "response": "def execute_message_call(\n    laser_evm,\n    callee_address,\n    caller_address,\n    origin_address,\n    code,\n    data,\n    gas_limit,\n    gas_price,\n    value,\n    track_gas=False,\n) -> Union[None, List[GlobalState]]:\n    \"\"\"Execute a message call transaction from all open states.\n\n    :param laser_evm:\n    :param callee_address:\n    :param caller_address:\n    :param origin_address:\n    :param code:\n    :param data:\n    :param gas_limit:\n    :param gas_price:\n    :param value:\n    :param track_gas:\n    :return:\n    \"\"\"\n    # TODO: Resolve circular import between .transaction and ..svm to import LaserEVM here\n    open_states = laser_evm.open_states[:]\n    del laser_evm.open_states[:]\n\n    for open_world_state in open_states:\n        next_transaction_id = get_next_transaction_id()\n        transaction = MessageCallTransaction(\n            world_state=open_world_state,\n            identifier=next_transaction_id,\n            gas_price=gas_price,\n            gas_limit=gas_limit,\n            origin=origin_address,\n            code=Disassembly(code),\n            caller=caller_address,\n            callee_account=open_world_state[callee_address],\n            call_data=ConcreteCalldata(next_transaction_id, data),\n            call_value=value,\n        )\n\n        _setup_global_state_for_execution(laser_evm, transaction)\n\n    return laser_evm.exec(track_gas=track_gas)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef detect_mode(term_hint=\"xterm-256color\"):\n    if \"ANSICON\" in os.environ:\n        return 16\n    elif os.environ.get(\"ConEmuANSI\", \"OFF\") == \"ON\":\n        return 256\n    else:\n        term = os.environ.get(\"TERM\", term_hint)\n        if term.endswith(\"-256color\") or term in (\"xterm\", \"screen\"):\n            return 256\n        elif term.endswith(\"-color\") or term in (\"rxvt\",):\n            return 16\n        else:\n            return 256", "response": "Detects the color mode of the current process."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the Disassembly object for the given dependency address.", "response": "def dynld(self, dependency_address):\n        \"\"\"\n        :param dependency_address:\n        :return:\n        \"\"\"\n        if not self.contract_loading:\n            raise ValueError(\"Cannot load contract when contract_loading flag is false\")\n\n        log.debug(\"Dynld at contract \" + dependency_address)\n\n        # Ensure that dependency_address is the correct length, with 0s prepended as needed.\n        dependency_address = (\n            \"0x\" + \"0\" * (42 - len(dependency_address)) + dependency_address[2:]\n        )\n\n        m = re.match(r\"^(0x[0-9a-fA-F]{40})$\", dependency_address)\n\n        if m:\n            dependency_address = m.group(1)\n\n        else:\n            return None\n\n        log.debug(\"Dependency address: \" + dependency_address)\n\n        code = self.eth.eth_getCode(dependency_address)\n\n        if code == \"0x\":\n            return None\n        else:\n            return Disassembly(code)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef blank_account(cls, db, addr, initial_nonce=0):\n        db.put(BLANK_HASH, b\"\")\n        o = cls(initial_nonce, 0, trie.BLANK_ROOT, BLANK_HASH, db, addr)\n        o.existent_at_start = False\n        return o", "response": "creates a blank account."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting and caches an account for an addres. Creates blank if not found.", "response": "def get_and_cache_account(self, addr):\n        \"\"\"Gets and caches an account for an addres, creates blank if not\n        found.\n\n        :param addr:\n        :return:\n        \"\"\"\n\n        if addr in self.cache:\n            return self.cache[addr]\n        rlpdata = self.secure_trie.get(addr)\n        if (\n            rlpdata == trie.BLANK_NODE and len(addr) == 32\n        ):  # support for hashed addresses\n            rlpdata = self.trie.get(addr)\n\n        if rlpdata != trie.BLANK_NODE:\n            o = rlp.decode(rlpdata, Account, db=self.db, addr=addr)\n        else:\n            o = Account.blank_account(self.db, addr, 0)\n        self.cache[addr] = o\n        o._mutable = True\n        o._cached_rlp = None\n        return o"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\niterating through trie to and yields non - blank leafs as accounts.", "response": "def get_all_accounts(self):\n        \"\"\"iterates through trie to and yields non-blank leafs as accounts.\"\"\"\n        for address_hash, rlpdata in self.secure_trie.trie.iter_branch():\n            if rlpdata != trie.BLANK_NODE:\n                yield rlp.decode(rlpdata, Account, db=self.db, addr=address_hash)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_word_at(self, offset: int) -> Expression:\n        parts = self[offset : offset + 32]\n        return simplify(Concat(parts))", "response": "Gets the word at the given offset."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef initialize(self, symbolic_vm: LaserEVM):\n\n        @symbolic_vm.pre_hook(\"SSTORE\")\n        def mutator_hook(global_state: GlobalState):\n            global_state.annotate(MutationAnnotation())\n\n        @symbolic_vm.laser_hook(\"add_world_state\")\n        def world_state_filter_hook(global_state: GlobalState):\n            if And(\n                *global_state.mstate.constraints[:]\n                + [\n                    global_state.environment.callvalue\n                    > symbol_factory.BitVecVal(0, 256)\n                ]\n            ).is_false:\n                return\n            if isinstance(\n                global_state.current_transaction, ContractCreationTransaction\n            ):\n                return\n            if len(list(global_state.get_annotations(MutationAnnotation))) == 0:\n                raise PluginSkipWorldState", "response": "Initializes the mutation pruner\n        Introduces hooks for SSTORE operations\n       "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_from_bytecode(\n        self, code: str, bin_runtime: bool = False, address: Optional[str] = None\n    ) -> Tuple[str, EVMContract]:\n        \"\"\"\n        Returns the address and the contract class for the given bytecode\n        :param code: Bytecode\n        :param bin_runtime: Whether the code is runtime code or creation code\n        :param address: address of contract\n        :return: tuple(address, Contract class)\n        \"\"\"\n        if address is None:\n            address = util.get_indexed_address(0)\n        if bin_runtime:\n            self.contracts.append(\n                EVMContract(\n                    code=code,\n                    name=\"MAIN\",\n                    enable_online_lookup=self.enable_online_lookup,\n                )\n            )\n        else:\n            self.contracts.append(\n                EVMContract(\n                    creation_code=code,\n                    name=\"MAIN\",\n                    enable_online_lookup=self.enable_online_lookup,\n                )\n            )\n        return address, self.contracts[-1]", "response": "Loads the contract class for the given bytecode."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the state variable from the storage.", "response": "def get_state_variable_from_storage(\n        self, address: str, params: Optional[List[str]] = None\n    ) -> str:\n        \"\"\"\n        Get variables from the storage\n        :param address: The contract address\n        :param params: The list of parameters\n                        param types: [position, length] or [\"mapping\", position, key1, key2, ...  ]\n                        or [position, length, array]\n        :return: The corresponding storage slot and its value\n        \"\"\"\n        params = params or []\n        (position, length, mappings) = (0, 1, [])\n        try:\n            if params[0] == \"mapping\":\n                if len(params) < 3:\n                    raise CriticalError(\"Invalid number of parameters.\")\n                position = int(params[1])\n                position_formatted = utils.zpad(utils.int_to_big_endian(position), 32)\n                for i in range(2, len(params)):\n                    key = bytes(params[i], \"utf8\")\n                    key_formatted = utils.rzpad(key, 32)\n                    mappings.append(\n                        int.from_bytes(\n                            utils.sha3(key_formatted + position_formatted),\n                            byteorder=\"big\",\n                        )\n                    )\n\n                length = len(mappings)\n                if length == 1:\n                    position = mappings[0]\n\n            else:\n                if len(params) >= 4:\n                    raise CriticalError(\"Invalid number of parameters.\")\n\n                if len(params) >= 1:\n                    position = int(params[0])\n                if len(params) >= 2:\n                    length = int(params[1])\n                if len(params) == 3 and params[2] == \"array\":\n                    position_formatted = utils.zpad(\n                        utils.int_to_big_endian(position), 32\n                    )\n                    position = int.from_bytes(\n                        utils.sha3(position_formatted), byteorder=\"big\"\n                    )\n\n        except ValueError:\n            raise CriticalError(\n                \"Invalid storage index. Please provide a numeric value.\"\n            )\n\n        outtxt = []\n\n        try:\n            if length == 1:\n                outtxt.append(\n                    \"{}: {}\".format(\n                        position, self.eth.eth_getStorageAt(address, position)\n                    )\n                )\n            else:\n                if len(mappings) > 0:\n                    for i in range(0, len(mappings)):\n                        position = mappings[i]\n                        outtxt.append(\n                            \"{}: {}\".format(\n                                hex(position),\n                                self.eth.eth_getStorageAt(address, position),\n                            )\n                        )\n                else:\n                    for i in range(position, position + length):\n                        outtxt.append(\n                            \"{}: {}\".format(\n                                hex(i), self.eth.eth_getStorageAt(address, i)\n                            )\n                        )\n        except FileNotFoundError as e:\n            raise CriticalError(\"IPC error: \" + str(e))\n        except ConnectionError:\n            raise CriticalError(\n                \"Could not connect to RPC server. \"\n                \"Make sure that your node is running and that RPC parameters are set correctly.\"\n            )\n        return \"\\n\".join(outtxt)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_strategic_global_state(self) -> GlobalState:\n        for global_state in self.work_list:\n            if not self._is_covered(global_state):\n                self.work_list.remove(global_state)\n                return global_state\n        return self.super_strategy.get_strategic_global_state()", "response": "Returns the first uncovered global state in the work list if it exists otherwise returns the super_strategy s get_strategic_global_state method."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _is_covered(self, global_state: GlobalState) -> bool:\n        bytecode = global_state.environment.code.bytecode\n        index = global_state.mstate.pc\n        return self.instruction_coverage_plugin.is_instruction_covered(bytecode, index)", "response": "Checks if the instruction for the given global state is already covered"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _init_mythril_dir() -> str:\n\n        try:\n            mythril_dir = os.environ[\"MYTHRIL_DIR\"]\n        except KeyError:\n            mythril_dir = os.path.join(os.path.expanduser(\"~\"), \".mythril\")\n\n        if not os.path.exists(mythril_dir):\n            # Initialize data directory\n            log.info(\"Creating mythril data directory\")\n            os.mkdir(mythril_dir)\n\n        db_path = str(Path(mythril_dir) / \"signatures.db\")\n        if not os.path.exists(db_path):\n            # if the default mythril dir doesn't contain a signature DB\n            # initialize it with the default one from the project root\n            asset_dir = Path(__file__).parent.parent / \"support\" / \"assets\"\n            copyfile(str(asset_dir / \"signatures.db\"), db_path)\n\n        return mythril_dir", "response": "Initializes the mythril dir and config. ini file and the mythril data directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _init_config(self):\n\n        leveldb_default_path = self._get_default_leveldb_path()\n\n        if not os.path.exists(self.config_path):\n            log.info(\"No config file found. Creating default: \" + self.config_path)\n            open(self.config_path, \"a\").close()\n\n        config = ConfigParser(allow_no_value=True)\n\n        config.optionxform = str\n        config.read(self.config_path, \"utf-8\")\n        if \"defaults\" not in config.sections():\n            self._add_default_options(config)\n\n        if not config.has_option(\"defaults\", \"leveldb_dir\"):\n            self._add_leveldb_option(config, leveldb_default_path)\n\n        if not config.has_option(\"defaults\", \"dynamic_loading\"):\n            self._add_dynamic_loading_option(config)\n\n        with codecs.open(self.config_path, \"w\", \"utf-8\") as fp:\n            config.write(fp)\n\n        leveldb_dir = config.get(\n            \"defaults\", \"leveldb_dir\", fallback=leveldb_default_path\n        )\n        self.leveldb_dir = os.path.expanduser(leveldb_dir)", "response": "Create the config file and add default options and set self. leveldb_dir path"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the default LevelDB path for the current system.", "response": "def _get_default_leveldb_path() -> str:\n        \"\"\"\n        Returns the LevelDB path\n        :return: The LevelDB path\n        \"\"\"\n        system = platform.system().lower()\n        leveldb_fallback_dir = os.path.expanduser(\"~\")\n        if system.startswith(\"darwin\"):\n            leveldb_fallback_dir = os.path.join(\n                leveldb_fallback_dir, \"Library\", \"Ethereum\"\n            )\n        elif system.startswith(\"windows\"):\n            leveldb_fallback_dir = os.path.join(\n                leveldb_fallback_dir, \"AppData\", \"Roaming\", \"Ethereum\"\n            )\n        else:\n            leveldb_fallback_dir = os.path.join(leveldb_fallback_dir, \".ethereum\")\n        return os.path.join(leveldb_fallback_dir, \"geth\", \"chaindata\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a default leveldb option to the config file.", "response": "def _add_leveldb_option(config: ConfigParser, leveldb_fallback_dir: str) -> None:\n        \"\"\"\n        Sets a default leveldb path in .mythril/config.ini file\n        :param config: The config file object\n        :param leveldb_fallback_dir: The leveldb dir to use by default for searches\n        :return: None\n        \"\"\"\n        config.set(\"defaults\", \"#Default chaindata locations:\", \"\")\n        config.set(\"defaults\", \"#\u2013 Mac: ~/Library/Ethereum/geth/chaindata\", \"\")\n        config.set(\"defaults\", \"#\u2013 Linux: ~/.ethereum/geth/chaindata\", \"\")\n        config.set(\n            \"defaults\",\n            \"#\u2013 Windows: %USERPROFILE%\\\\AppData\\\\Roaming\\\\Ethereum\\\\geth\\\\chaindata\",\n            \"\",\n        )\n        config.set(\"defaults\", \"leveldb_dir\", leveldb_fallback_dir)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding the dynamic loading option in. mythril config. ini file", "response": "def _add_dynamic_loading_option(config: ConfigParser) -> None:\n        \"\"\"\n        Sets the dynamic loading config option in .mythril/config.ini file\n        :param config: The config file object\n        :return: None\n        \"\"\"\n        config.set(\n            \"defaults\", \"#\u2013 To connect to Infura use dynamic_loading: infura\", \"\"\n        )\n        config.set(\n            \"defaults\",\n            \"#\u2013 To connect to Rpc use \"\n            \"dynamic_loading: HOST:PORT / ganache / infura-[network_name]\",\n            \"\",\n        )\n        config.set(\n            \"defaults\", \"#\u2013 To connect to local host use dynamic_loading: localhost\", \"\"\n        )\n        config.set(\"defaults\", \"dynamic_loading\", \"infura\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the RPC mode to either of ganache infura - mainnet or infura - kovan infura - ropsten.", "response": "def set_api_rpc(self, rpc: str = None, rpctls: bool = False) -> None:\n        \"\"\"\n        Sets the RPC mode to either of ganache or infura\n        :param rpc: either of the strings - ganache, infura-mainnet, infura-rinkeby, infura-kovan, infura-ropsten\n        \"\"\"\n        if rpc == \"ganache\":\n            rpcconfig = (\"localhost\", 8545, False)\n        else:\n            m = re.match(r\"infura-(.*)\", rpc)\n            if m and m.group(1) in [\"mainnet\", \"rinkeby\", \"kovan\", \"ropsten\"]:\n                rpcconfig = (m.group(1) + \".infura.io\", 443, True)\n            else:\n                try:\n                    host, port = rpc.split(\":\")\n                    rpcconfig = (host, int(port), rpctls)\n                except ValueError:\n                    raise CriticalError(\n                        \"Invalid RPC argument, use 'ganache', 'infura-[network]' or 'HOST:PORT'\"\n                    )\n\n        if rpcconfig:\n            log.info(\"Using RPC settings: %s\" % str(rpcconfig))\n            self.eth = EthJsonRpc(rpcconfig[0], int(rpcconfig[1]), rpcconfig[2])\n        else:\n            raise CriticalError(\"Invalid RPC settings, check help for details.\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the RPC mode based on a given config file.", "response": "def set_api_from_config_path(self) -> None:\n        \"\"\"Set the RPC mode based on a given config file.\"\"\"\n        config = ConfigParser(allow_no_value=False)\n        # TODO: Remove this after this issue https://github.com/python/mypy/issues/2427 is closed\n        config.optionxform = str  # type:ignore\n        config.read(self.config_path, \"utf-8\")\n        if config.has_option(\"defaults\", \"dynamic_loading\"):\n            dynamic_loading = config.get(\"defaults\", \"dynamic_loading\")\n        else:\n            dynamic_loading = \"infura\"\n        self._set_rpc(dynamic_loading)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the rpc based on the type of connection.", "response": "def _set_rpc(self, rpc_type: str) -> None:\n        \"\"\"\n        Sets rpc based on the type\n        :param rpc_type: The type of connection: like infura, ganache, localhost\n        :return:\n        \"\"\"\n        if rpc_type == \"infura\":\n            self.set_api_rpc_infura()\n        elif rpc_type == \"localhost\":\n            self.set_api_rpc_localhost()\n        else:\n            self.set_api_rpc(rpc_type)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef BitVecFuncVal(\n        value: int,\n        func_name: str,\n        size: int,\n        annotations: Annotations = None,\n        input_: Union[int, \"BitVec\"] = None,\n    ) -> BitVecFunc:\n        \"\"\"Creates a new bit vector function with a symbolic value.\n\n        :param value: The concrete value to set the bit vector to\n        :param func_name: The name of the bit vector function\n        :param size: The size of the bit vector\n        :param annotations: The annotations to initialize the bit vector with\n        :param input_: The input to the bit vector function\n        :return: The freshly created bit vector function\n        \"\"\"\n        raise NotImplementedError()", "response": "Creates a new bit vector function with a symbolic value."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a new bit vector function with a symbolic value.", "response": "def BitVecFuncSym(\n        name: str,\n        func_name: str,\n        size: int,\n        annotations: Annotations = None,\n        input_: Union[int, \"BitVec\"] = None,\n    ) -> BitVecFunc:\n        \"\"\"Creates a new bit vector function with a symbolic value.\n\n        :param name: The name of the symbolic bit vector\n        :param func_name: The name of the bit vector function\n        :param size: The size of the bit vector\n        :param annotations: The annotations to initialize the bit vector with\n        :param input_: The input to the bit vector function\n        :return: The freshly created bit vector function\n        \"\"\"\n        raise NotImplementedError()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a freshly created Bool with concrete value value.", "response": "def Bool(value: \"__builtins__.bool\", annotations: Annotations = None) -> bool.Bool:\n        \"\"\"\n        Creates a Bool with concrete value\n        :param value: The boolean value\n        :param annotations: The annotations to initialize the bool with\n        :return: The freshly created Bool()\n        \"\"\"\n        raw = z3.BoolVal(value)\n        return Bool(raw, annotations)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a new bit vector with a concrete value.", "response": "def BitVecVal(value: int, size: int, annotations: Annotations = None) -> BitVec:\n        \"\"\"Creates a new bit vector with a concrete value.\"\"\"\n        raw = z3.BitVecVal(value, size)\n        return BitVec(raw, annotations)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef BitVecSym(name: str, size: int, annotations: Annotations = None) -> BitVec:\n        raw = z3.BitVec(name, size)\n        return BitVec(raw, annotations)", "response": "Creates a new bit vector with a symbolic value."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef BitVecFuncVal(\n        value: int,\n        func_name: str,\n        size: int,\n        annotations: Annotations = None,\n        input_: Union[int, \"BitVec\"] = None,\n    ) -> BitVecFunc:\n        \"\"\"Creates a new bit vector function with a concrete value.\"\"\"\n        raw = z3.BitVecVal(value, size)\n        return BitVecFunc(raw, func_name, input_, annotations)", "response": "Creates a new bit vector function with a concrete value."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a new bit vector function with a symbolic value.", "response": "def BitVecFuncSym(\n        name: str,\n        func_name: str,\n        size: int,\n        annotations: Annotations = None,\n        input_: Union[int, \"BitVec\"] = None,\n    ) -> BitVecFunc:\n        \"\"\"Creates a new bit vector function with a symbolic value.\"\"\"\n        raw = z3.BitVec(name, size)\n        return BitVecFunc(raw, func_name, input_, annotations)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a new bit vector with a concrete value.", "response": "def BitVecVal(\n        value: int, size: int, annotations: Annotations = None\n    ) -> z3.BitVecRef:\n        \"\"\"Creates a new bit vector with a concrete value.\"\"\"\n        return z3.BitVecVal(value, size)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef BitVecSym(\n        name: str, size: int, annotations: Annotations = None\n    ) -> z3.BitVecRef:\n        \"\"\"Creates a new bit vector with a symbolic value.\"\"\"\n        return z3.BitVec(name, size)", "response": "Creates a new bit vector with a symbolic value."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nappends the given element to the end of the list", "response": "def append(self, element: Union[int, Expression]) -> None:\n        \"\"\"\n        :param element: element to be appended to the list\n        :function: appends the element to list if the size is less than STACK_LIMIT, else throws an error\n        \"\"\"\n        if super(MachineStack, self).__len__() >= self.STACK_LIMIT:\n            raise StackOverflowException(\n                \"Reached the EVM stack limit of {}, you can't append more \"\n                \"elements\".format(self.STACK_LIMIT)\n            )\n        super(MachineStack, self).append(element)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\npopping from the stack", "response": "def pop(self, index=-1) -> Union[int, Expression]:\n        \"\"\"\n        :param index:index to be popped, same as the list() class.\n        :returns popped value\n        :function: same as list() class but throws StackUnderflowException for popping from an empty list\n        \"\"\"\n\n        try:\n            return super(MachineStack, self).pop(index)\n        except IndexError:\n            raise StackUnderflowException(\"Trying to pop from an empty stack\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef mem_extend(self, start: int, size: int) -> None:\n        m_extend = self.calculate_extension_size(start, size)\n        if m_extend:\n            extend_gas = self.calculate_memory_gas(start, size)\n            self.min_gas_used += extend_gas\n            self.max_gas_used += extend_gas\n            self.check_gas()\n            self.memory.extend(m_extend)", "response": "Extends the memory of this machine state."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwrites data to memory starting at offset.", "response": "def memory_write(self, offset: int, data: List[Union[int, BitVec]]) -> None:\n        \"\"\"Writes data to memory starting at offset.\n\n        :param offset:\n        :param data:\n        \"\"\"\n        self.mem_extend(offset, len(data))\n        self.memory[offset : offset + len(data)] = data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\npop amount elements from the stack.", "response": "def pop(self, amount=1) -> Union[BitVec, List[BitVec]]:\n        \"\"\"Pops amount elements from the stack.\n\n        :param amount:\n        :return:\n        \"\"\"\n        if amount > len(self.stack):\n            raise StackUnderflowException\n        values = self.stack[-amount:][::-1]\n        del self.stack[-amount:]\n\n        return values[0] if amount == 1 else values"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn True if the current set of constraints is possible.", "response": "def is_possible(self) -> bool:\n        \"\"\"\n        :return: True/False based on the existence of solution of constraints\n        \"\"\"\n        if self._is_possible is not None:\n            return self._is_possible\n        solver = Solver()\n        solver.set_timeout(self._default_timeout)\n        for constraint in self[:]:\n            constraint = (\n                symbol_factory.Bool(constraint)\n                if isinstance(constraint, bool)\n                else constraint\n            )\n            solver.add(constraint)\n        self._is_possible = solver.check() != unsat\n        return self._is_possible"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsearch the corresponding code part in the database.", "response": "def search_db(self, search):\n        \"\"\"\n        Searches the corresponding code\n        :param search: The code part to be searched\n        \"\"\"\n\n        def search_callback(_, address, balance):\n            \"\"\"\n\n            :param _:\n            :param address: The address of the contract with the code in search\n            :param balance: The balance of the corresponding contract\n            \"\"\"\n            print(\"Address: \" + address + \", balance: \" + str(balance))\n\n        try:\n            self.leveldb.search(search, search_callback)\n\n        except SyntaxError:\n            raise CriticalError(\"Syntax error in search expression.\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the address of the corresponding hash by searching the leveldb for the corresponding hash.", "response": "def contract_hash_to_address(self, contract_hash):\n        \"\"\"\n        Returns address of the corresponding hash by searching the leveldb\n        :param contract_hash: Hash to be searched\n        \"\"\"\n        if not re.match(r\"0x[a-fA-F0-9]{64}\", contract_hash):\n            raise CriticalError(\"Invalid address hash. Expected format is '0x...'.\")\n\n        print(self.leveldb.contract_hash_to_address(contract_hash))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates an if - then - else expression.", "response": "def If(a: Union[Bool, bool], b: Union[BitVec, int], c: Union[BitVec, int]) -> BitVec:\n    \"\"\"Create an if-then-else expression.\n\n    :param a:\n    :param b:\n    :param c:\n    :return:\n    \"\"\"\n    # TODO: Handle BitVecFunc\n\n    if not isinstance(a, Bool):\n        a = Bool(z3.BoolVal(a))\n    if not isinstance(b, BitVec):\n        b = BitVec(z3.BitVecVal(b, 256))\n    if not isinstance(c, BitVec):\n        c = BitVec(z3.BitVecVal(c, 256))\n    union = a.annotations + b.annotations + c.annotations\n    return BitVec(z3.If(a.raw, b.raw, c.raw), union)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef UGT(a: BitVec, b: BitVec) -> Bool:\n    return _comparison_helper(a, b, z3.UGT, default_value=False, inputs_equal=False)", "response": "Create an unsigned greater than expression."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates an unsigned greater or equals expression.", "response": "def UGE(a: BitVec, b: BitVec) -> Bool:\n    \"\"\"Create an unsigned greater or equals expression.\n\n    :param a:\n    :param b:\n    :return:\n    \"\"\"\n    return Or(UGT(a, b), a == b)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ULE(a: BitVec, b: BitVec) -> Bool:\n    return Or(ULT(a, b), a == b)", "response": "Create an unsigned less than expression."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef Concat(*args: Union[BitVec, List[BitVec]]) -> BitVec:\n    # The following statement is used if a list is provided as an argument to concat\n    if len(args) == 1 and isinstance(args[0], list):\n        bvs = args[0]  # type: List[BitVec]\n    else:\n        bvs = cast(List[BitVec], args)\n\n    nraw = z3.Concat([a.raw for a in bvs])\n    annotations = []  # type: Annotations\n    bitvecfunc = False\n    for bv in bvs:\n        annotations += bv.annotations\n        if isinstance(bv, BitVecFunc):\n            bitvecfunc = True\n\n    if bitvecfunc:\n        # Is there a better value to set func_name and input to in this case?\n        return BitVecFunc(\n            raw=nraw, func_name=None, input_=None, annotations=annotations\n        )\n\n    return BitVec(nraw, annotations)", "response": "Create a concatenation expression."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates an extract expression.", "response": "def Extract(high: int, low: int, bv: BitVec) -> BitVec:\n    \"\"\"Create an extract expression.\n\n    :param high:\n    :param low:\n    :param bv:\n    :return:\n    \"\"\"\n    raw = z3.Extract(high, low, bv.raw)\n    if isinstance(bv, BitVecFunc):\n        # Is there a better value to set func_name and input to in this case?\n        return BitVecFunc(\n            raw=raw, func_name=None, input_=None, annotations=bv.annotations\n        )\n\n    return BitVec(raw, annotations=bv.annotations)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef URem(a: BitVec, b: BitVec) -> BitVec:\n    return _arithmetic_helper(a, b, z3.URem)", "response": "Create an unsigned remainder expression."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a signed remainder expression.", "response": "def SRem(a: BitVec, b: BitVec) -> BitVec:\n    \"\"\"Create a signed remainder expression.\n\n    :param a:\n    :param b:\n    :return:\n    \"\"\"\n    return _arithmetic_helper(a, b, z3.SRem)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef UDiv(a: BitVec, b: BitVec) -> BitVec:\n    return _arithmetic_helper(a, b, z3.UDiv)", "response": "Create an unsigned division expression."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate sum expression. :return:", "response": "def Sum(*args: BitVec) -> BitVec:\n    \"\"\"Create sum expression.\n\n    :return:\n    \"\"\"\n    raw = z3.Sum([a.raw for a in args])\n    annotations = []  # type: Annotations\n    bitvecfuncs = []\n\n    for bv in args:\n        annotations += bv.annotations\n        if isinstance(bv, BitVecFunc):\n            bitvecfuncs.append(bv)\n\n    if len(bitvecfuncs) >= 2:\n        return BitVecFunc(raw=raw, func_name=None, input_=None, annotations=annotations)\n    elif len(bitvecfuncs) == 1:\n        return BitVecFunc(\n            raw=raw,\n            func_name=bitvecfuncs[0].func_name,\n            input_=bitvecfuncs[0].input_,\n            annotations=annotations,\n        )\n\n    return BitVec(raw, annotations)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef BVAddNoOverflow(a: Union[BitVec, int], b: Union[BitVec, int], signed: bool) -> Bool:\n    if not isinstance(a, BitVec):\n        a = BitVec(z3.BitVecVal(a, 256))\n    if not isinstance(b, BitVec):\n        b = BitVec(z3.BitVecVal(b, 256))\n    return Bool(z3.BVAddNoOverflow(a.raw, b.raw, signed))", "response": "Creates predicate that verifies that the addition doesn t overflow."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef symbolic(self) -> bool:\n        self.simplify()\n        return not isinstance(self.raw, z3.BitVecNumRef)", "response": "Returns whether this symbol doesn t have a concrete value."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the value of this symbol if concrete otherwise None.", "response": "def value(self) -> Optional[int]:\n        \"\"\"Returns the value of this symbol if concrete, otherwise None.\n\n        :return:\n        \"\"\"\n        if self.symbolic:\n            return None\n        assert isinstance(self.raw, z3.BitVecNumRef)\n        return self.raw.as_long()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nhandling the shift operator.", "response": "def _handle_shift(self, other: Union[int, \"BitVec\"], operator: Callable) -> \"BitVec\":\n        \"\"\"\n        Handles shift\n        :param other: The other BitVector\n        :param operator: The shift operator\n        :return: the resulting output\n        \"\"\"\n        if isinstance(other, BitVecFunc):\n            return operator(other, self)\n        if not isinstance(other, BitVec):\n            return BitVec(\n                operator(self.raw, other), annotations=self.annotations\n            )\n        union = self.annotations + other.annotations\n        return BitVec(operator(self.raw, other.raw), annotations=union)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninitialize the execution environment and return a GlobalState object.", "response": "def initial_global_state(self) -> GlobalState:\n        \"\"\"Initialize the execution environment.\"\"\"\n        environment = Environment(\n            self.callee_account,\n            self.caller,\n            self.call_data,\n            self.gas_price,\n            self.call_value,\n            self.origin,\n            code=self.code or self.callee_account.code,\n        )\n        return super().initial_global_state_from_environment(\n            environment, active_function=\"fallback\"\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfind the function hash entry point and function name for a given call table entry", "response": "def get_function_info(\n    index: int, instruction_list: list, signature_database: SignatureDB\n) -> Tuple[str, int, str]:\n    \"\"\"Finds the function information for a call table entry Solidity uses the\n    first 4 bytes of the calldata to indicate which function the message call\n    should execute The generated code that directs execution to the correct\n    function looks like this:\n\n    - PUSH function_hash\n    - EQ\n    - PUSH entry_point\n    - JUMPI\n\n    This function takes an index that points to the first instruction, and from that finds out the function hash,\n    function entry and the function name.\n\n    :param index: Start of the entry pattern\n    :param instruction_list: Instruction list for the contract that is being analyzed\n    :param signature_database: Database used to map function hashes to their respective function names\n    :return: function hash, function entry point, function name\n    \"\"\"\n\n    # Append with missing 0s at the beginning\n    function_hash = \"0x\" + instruction_list[index][\"argument\"][2:].rjust(8, \"0\")\n    function_names = signature_database.get(function_hash)\n    if len(function_names) > 1:\n        # In this case there was an ambiguous result\n        function_name = \"[{}] (ambiguous)\".format(\", \".join(function_names))\n    elif len(function_names) == 1:\n        function_name = function_names[0]\n    else:\n        function_name = \"_function_\" + function_hash\n\n    try:\n        offset = instruction_list[index + 2][\"argument\"]\n        entry_point = int(offset, 16)\n    except (KeyError, IndexError):\n        return function_hash, None, None\n\n    return function_hash, entry_point, function_name"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting internal compiler error message.", "response": "def _set_internal_compiler_error(self):\n        \"\"\"\n        Adds the false positive to description and changes severity to low\n        \"\"\"\n        self.severity = \"Low\"\n        self.description_tail += (\n            \" This issue is reported for internal compiler generated code.\"\n        )\n        self.description = \"%s\\n%s\" % (self.description_head, self.description_tail)\n        self.code = \"\""}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef as_swc_standard_format(self):\n        _issues = []\n        source_list = []\n\n        for key, issue in self.issues.items():\n\n            idx = self.source.get_source_index(issue.bytecode_hash)\n            try:\n                title = SWC_TO_TITLE[issue.swc_id]\n            except KeyError:\n                title = \"Unspecified Security Issue\"\n\n            _issues.append(\n                {\n                    \"swcID\": \"SWC-\" + issue.swc_id,\n                    \"swcTitle\": title,\n                    \"description\": {\n                        \"head\": issue.description_head,\n                        \"tail\": issue.description_tail,\n                    },\n                    \"severity\": issue.severity,\n                    \"locations\": [{\"sourceMap\": \"%d:1:%d\" % (issue.address, idx)}],\n                    \"extra\": {\"discoveryTime\": int(issue.discovery_time * 10 ** 9)},\n                }\n            )\n        meta_data = self._get_exception_data()\n        result = [\n            {\n                \"issues\": _issues,\n                \"sourceType\": self.source.source_type,\n                \"sourceFormat\": self.source.source_format,\n                \"sourceList\": self.source.source_list,\n                \"meta\": meta_data,\n            }\n        ]\n\n        return json.dumps(result, sort_keys=True)", "response": "Returns a string that can be used to generate SWC standard data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef evaluate(self, global_state: GlobalState, post=False) -> List[GlobalState]:\n        # Generalize some ops\n        log.debug(\"Evaluating {}\".format(self.op_code))\n        op = self.op_code.lower()\n        if self.op_code.startswith(\"PUSH\"):\n            op = \"push\"\n        elif self.op_code.startswith(\"DUP\"):\n            op = \"dup\"\n        elif self.op_code.startswith(\"SWAP\"):\n            op = \"swap\"\n        elif self.op_code.startswith(\"LOG\"):\n            op = \"log\"\n\n        instruction_mutator = (\n            getattr(self, op + \"_\", None)\n            if not post\n            else getattr(self, op + \"_\" + \"post\", None)\n        )\n\n        if instruction_mutator is None:\n            raise NotImplementedError\n\n        if self.iprof is None:\n            result = instruction_mutator(global_state)\n        else:\n            start_time = datetime.now()\n            result = instruction_mutator(global_state)\n            end_time = datetime.now()\n            self.iprof.record(op, start_time, end_time)\n\n        return result", "response": "Performs the mutation for this instruction."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding the constraints to this solver.", "response": "def add(self, *constraints: List[Bool]) -> None:\n        \"\"\"Adds the constraints to this solver.\n\n        :param constraints:\n        :return:\n        \"\"\"\n        z3_constraints = [\n            c.raw for c in cast(List[Bool], constraints)\n        ]  # type: Sequence[z3.BoolRef]\n        self.raw.add(z3_constraints)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn z3 smt check result.", "response": "def check(self) -> z3.CheckSatResult:\n        \"\"\"Returns z3 smt check result.\n        Also suppresses the stdout when running z3 library's check() to avoid unnecessary output\n        :return: The evaluated result which is either of sat, unsat or unknown\n        \"\"\"\n        old_stdout = sys.stdout\n        sys.stdout = open(os.devnull, \"w\")\n        evaluate = self.raw.check()\n        sys.stdout = old_stdout\n        return evaluate"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef maximize(self, element: Expression[z3.ExprRef]) -> None:\n        self.raw.maximize(element.raw)", "response": "In solving this solver will try to maximize the passed expression."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef decls(self) -> List[z3.ExprRef]:\n        result = []  # type: List[z3.ExprRef]\n        for internal_model in self.raw:\n            result.extend(internal_model.decls())\n        return result", "response": "Get the declarations for this model"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef eval(\n        self, expression: z3.ExprRef, model_completion: bool = False\n    ) -> Union[None, z3.ExprRef]:\n        \"\"\" Evaluate the expression using this model\n\n        :param expression: The expression to evaluate\n        :param model_completion: Use the default value if the model has no interpretation of the given expression\n        :return: The evaluated expression\n        \"\"\"\n        for internal_model in self.raw:\n            is_last_model = self.raw.index(internal_model) == len(self.raw) - 1\n            is_relevant_model = expression.decl() in list(internal_model.decls())\n            if is_relevant_model or is_last_model:\n                return internal_model.eval(expression, model_completion)\n        return None", "response": "Evaluate the expression using this model."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _add_world_state(self, global_state: GlobalState):\n        for hook in self._add_world_state_hooks:\n            try:\n                hook(global_state)\n            except PluginSkipWorldState:\n                return\n\n        self.open_states.append(global_state.world_state)", "response": "Stores the world_state of the passed global state in the open states"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef register_laser_hooks(self, hook_type: str, hook: Callable):\n        if hook_type == \"add_world_state\":\n            self._add_world_state_hooks.append(hook)\n        elif hook_type == \"execute_state\":\n            self._execute_state_hooks.append(hook)\n        elif hook_type == \"start_sym_exec\":\n            self._start_sym_exec_hooks.append(hook)\n        elif hook_type == \"stop_sym_exec\":\n            self._stop_sym_exec_hooks.append(hook)\n        elif hook_type == \"start_sym_trans\":\n            self._start_sym_trans_hooks.append(hook)\n        elif hook_type == \"stop_sym_trans\":\n            self._stop_sym_trans_hooks.append(hook)\n        else:\n            raise ValueError(\n                \"Invalid hook type %s. Must be one of {add_world_state}\", hook_type\n            )", "response": "registers the hook with this Laser VM"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nregisters the annotated function with register_laser_hooks :param hook_type: :return: hook decorator", "response": "def laser_hook(self, hook_type: str) -> Callable:\n        \"\"\"Registers the annotated function with register_laser_hooks\n\n        :param hook_type:\n        :return: hook decorator\n        \"\"\"\n\n        def hook_decorator(func: Callable):\n            \"\"\" Hook decorator generated by laser_hook\n\n            :param func: Decorated function\n            \"\"\"\n            self.register_laser_hooks(hook_type, func)\n            return func\n\n        return hook_decorator"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the balance of an address in a block.", "response": "def eth_getBalance(self, address=None, block=BLOCK_TAG_LATEST):\n        \"\"\"TODO: documentation\n\n        https://github.com/ethereum/wiki/wiki/JSON-RPC#eth_getbalance\n\n        TESTED\n        \"\"\"\n        address = address or self.eth_coinbase()\n        block = validate_block(block)\n        return hex_to_dec(self._call(\"eth_getBalance\", [address, block]))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef eth_getStorageAt(self, address=None, position=0, block=BLOCK_TAG_LATEST):\n        block = validate_block(block)\n        return self._call(\"eth_getStorageAt\", [address, hex(position), block])", "response": "Get the storage at the specified position."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget code from an Ethernet address.", "response": "def eth_getCode(self, address, default_block=BLOCK_TAG_LATEST):\n        \"\"\"TODO: documentation\n\n        https://github.com/ethereum/wiki/wiki/JSON-RPC#eth_getcode\n\n        NEEDS TESTING\n        \"\"\"\n        if isinstance(default_block, str):\n            if default_block not in BLOCK_TAGS:\n                raise ValueError\n        return self._call(\"eth_getCode\", [address, default_block])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting block by number.", "response": "def eth_getBlockByNumber(self, block=BLOCK_TAG_LATEST, tx_objects=True):\n        \"\"\"TODO: documentation\n\n        https://github.com/ethereum/wiki/wiki/JSON-RPC#eth_getblockbynumber\n\n        TESTED\n        \"\"\"\n        block = validate_block(block)\n        return self._call(\"eth_getBlockByNumber\", [block, tx_objects])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_current_instruction(self) -> Dict:\n\n        instructions = self.environment.code.instruction_list\n        return instructions[self.mstate.pc]", "response": "Gets the current instruction for this GlobalState."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning an OAuth Request object for the current request.", "response": "def get_oauth_request(self):\n        \"\"\"Return an OAuth Request object for the current request.\"\"\"\n\n        try:\n            method = os.environ['REQUEST_METHOD']\n        except:\n            method = 'GET'\n\n        postdata = None\n        if method in ('POST', 'PUT'):\n            postdata = self.request.body\n\n        return oauth.Request.from_request(method, self.request.uri,\n            headers=self.request.headers, query_string=postdata)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_client(self, request=None):\n\n        if not isinstance(request, oauth.Request):\n            request = self.get_oauth_request()\n        client_key = request.get_parameter('oauth_consumer_key')\n        if not client_key:\n            raise Exception('Missing \"oauth_consumer_key\" parameter in ' \\\n                'OAuth \"Authorization\" header')\n\n        client = models.Client.get_by_key_name(client_key)\n        if not client:\n            raise Exception('Client \"%s\" not found.' % client_key)\n\n        return client", "response": "Return the client from the OAuth parameters."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a Client object if this is a valid OAuth request.", "response": "def is_valid(self):\n        \"\"\"Returns a Client object if this is a valid OAuth request.\"\"\"\n\n        try:\n            request = self.get_oauth_request()\n            client = self.get_client(request)\n            params = self._server.verify_request(request, client, None)\n        except Exception as e:\n            raise e\n\n        return client"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef build_xoauth_string(url, consumer, token=None):\n    request = Request.from_consumer_and_token(consumer, token,\n        \"GET\", url)\n\n    signing_method = SignatureMethod_HMAC_SHA1()\n    request.sign_request(signing_method, consumer, token)\n\n    params = []\n    for k, v in sorted(request.items()):\n        if v is not None:\n            params.append('%s=\"%s\"' % (k, escape(v)))\n\n    return \"%s %s %s\" % (\"GET\", url, ','.join(params))", "response": "Build an XOAUTH string for use in SMTP / IMPA authentication."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting an iterable of strings to unicode.", "response": "def to_unicode_optional_iterator(x):\n    \"\"\"\n    Raise TypeError if x is a str containing non-utf8 bytes or if x is\n    an iterable which contains such a str.\n    \"\"\"\n    if isinstance(x, STRING_TYPES):\n        return to_unicode(x)\n\n    try:\n        l = list(x)\n    except TypeError as e:\n        assert 'is not iterable' in str(e)\n        return x\n    else:\n        return [ to_unicode(e) for e in l ]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_utf8_optional_iterator(x):\n    if isinstance(x, STRING_TYPES):\n        return to_utf8(x)\n\n    try:\n        l = list(x)\n    except TypeError as e:\n        assert 'is not iterable' in str(e)\n        return x\n    else:\n        return [ to_utf8_if_string(e) for e in l ]", "response": "Convert an iterable of strings to utf8."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nescape a URL including any slash.", "response": "def escape(s):\n    \"\"\"Escape a URL including any /.\"\"\"\n    if not isinstance(s, bytes):\n        s = s.encode('utf-8')\n    return quote(s, safe='~')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning this token as a plain string suitable for storage.", "response": "def to_string(self):\n        \"\"\"Returns this token as a plain string, suitable for storage.\n \n        The resulting string includes the token's secret, so you should never\n        send or store this string where a third party can read it.\n        \"\"\"\n        items = [\n            ('oauth_token', self.key),\n            ('oauth_token_secret', self.secret),\n        ]\n\n        if self.callback_confirmed is not None:\n            items.append(('oauth_callback_confirmed', self.callback_confirmed))\n        return urlencode(items)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_string(s):\n\n        if not len(s):\n            raise ValueError(\"Invalid parameter string.\")\n\n        params = parse_qs(u(s), keep_blank_values=False)\n        if not len(params):\n            raise ValueError(\"Invalid parameter string.\")\n\n        try:\n            key = params['oauth_token'][0]\n        except Exception:\n            raise ValueError(\"'oauth_token' not found in OAuth request.\")\n\n        try:\n            secret = params['oauth_token_secret'][0]\n        except Exception:\n            raise ValueError(\"'oauth_token_secret' not found in \" \n                \"OAuth request.\")\n\n        token = Token(key, secret)\n        try:\n            token.callback_confirmed = params['oauth_callback_confirmed'][0]\n        except KeyError:\n            pass  # 1.0, no callback confirmed.\n        return token", "response": "Deserializes a token from a string like one returned by\n        to_string ()."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_nonoauth_parameters(self):\n        return dict([(k, v) for k, v in self.items() \n                    if not k.startswith('oauth_')])", "response": "Get any non - OAuth parameters."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nserialize as a header for an HTTPAuth request.", "response": "def to_header(self, realm=''):\n        \"\"\"Serialize as a header for an HTTPAuth request.\"\"\"\n        oauth_params = ((k, v) for k, v in self.items() \n                            if k.startswith('oauth_'))\n        stringy_params = ((k, escape(v)) for k, v in oauth_params)\n        header_params = ('%s=\"%s\"' % (k, v) for k, v in stringy_params)\n        params_header = ', '.join(header_params)\n \n        auth_header = 'OAuth realm=\"%s\"' % realm\n        if params_header:\n            auth_header = \"%s, %s\" % (auth_header, params_header)\n \n        return {'Authorization': auth_header}"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nserializes as a URL for a GET request.", "response": "def to_url(self):\n        \"\"\"Serialize as a URL for a GET request.\"\"\"\n        base_url = urlparse(self.url)\n\n        if PY3:\n            query = parse_qs(base_url.query)\n            for k, v in self.items():\n                query.setdefault(k, []).append(to_utf8_optional_iterator(v))\n            scheme = base_url.scheme\n            netloc = base_url.netloc\n            path = base_url.path\n            params = base_url.params\n            fragment = base_url.fragment\n        else:\n            query = parse_qs(to_utf8(base_url.query))\n            for k, v in self.items():\n                query.setdefault(to_utf8(k), []).append(to_utf8_optional_iterator(v))\n            scheme = to_utf8(base_url.scheme)\n            netloc = to_utf8(base_url.netloc)\n            path = to_utf8(base_url.path)\n            params = to_utf8(base_url.params)\n            fragment = to_utf8(base_url.fragment)\n\n        url = (scheme, netloc, path, params, urlencode(query, True), fragment)\n        return urlunparse(url)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a string that contains the parameters that must be signed.", "response": "def get_normalized_parameters(self):\n        \"\"\"Return a string that contains the parameters that must be signed.\"\"\"\n        items = []\n        for key, value in self.items():\n            if key == 'oauth_signature':\n                continue\n            # 1.0a/9.1.1 states that kvp must be sorted by key, then by value,\n            # so we unpack sequence values into multiple items for sorting.\n            if isinstance(value, STRING_TYPES):\n                items.append((to_utf8_if_string(key), to_utf8(value)))\n            else:\n                try:\n                    value = list(value)\n                except TypeError as e:\n                    assert 'is not iterable' in str(e)\n                    items.append((to_utf8_if_string(key), to_utf8_if_string(value)))\n                else:\n                    items.extend((to_utf8_if_string(key), to_utf8_if_string(item)) for item in value)\n\n        # Include any query string parameters from the provided URL\n        query = urlparse(self.url)[4]\n\n        url_items = self._split_url_string(query).items()\n        url_items = [(to_utf8(k), to_utf8_optional_iterator(v)) for k, v in url_items if k != 'oauth_signature' ]\n        items.extend(url_items)\n\n        items.sort()\n        encoded_str = urlencode(items, True)\n        # Encode signature parameters per Oauth Core 1.0 protocol\n        # spec draft 7, section 3.6\n        # (http://tools.ietf.org/html/draft-hammer-oauth-07#section-3.6)\n        # Spaces must be encoded with \"%20\" instead of \"+\"\n        return encoded_str.replace('+', '%20').replace('%7E', '~')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the signature parameter to the result of sign.", "response": "def sign_request(self, signature_method, consumer, token):\n        \"\"\"Set the signature parameter to the result of sign.\"\"\"\n\n        if not self.is_form_encoded:\n            # according to\n            # http://oauth.googlecode.com/svn/spec/ext/body_hash/1.0/oauth-bodyhash.html\n            # section 4.1.1 \"OAuth Consumers MUST NOT include an\n            # oauth_body_hash parameter on requests with form-encoded\n            # request bodies.\"\n            if not self.body:\n               self.body = ''\n            self['oauth_body_hash'] = base64.b64encode(sha1(to_utf8(self.body)).digest())\n\n        if 'oauth_consumer_key' not in self:\n            self['oauth_consumer_key'] = consumer.key\n\n        if token and 'oauth_token' not in self:\n            self['oauth_token'] = token.key\n\n        self['oauth_signature_method'] = signature_method.name\n        self['oauth_signature'] = signature_method.sign(self, consumer, token)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_request(cls, http_method, http_url, headers=None, parameters=None,\n            query_string=None):\n        \"\"\"Combines multiple parameter sources.\"\"\"\n        if parameters is None:\n            parameters = {}\n \n        # Headers\n        if headers:\n            auth_header = None\n            for k, v in headers.items():\n                if k.lower() == 'authorization' or \\\n                    k.upper() == 'HTTP_AUTHORIZATION':\n                    auth_header = v\n\n            # Check that the authorization header is OAuth.\n            if auth_header and auth_header[:6] == 'OAuth ':\n                auth_header = auth_header[6:]\n                try:\n                    # Get the parameters from the header.\n                    header_params = cls._split_header(auth_header)\n                    parameters.update(header_params)\n                except:\n                    raise Error('Unable to parse OAuth parameters from '\n                        'Authorization header.')\n \n        # GET or POST query string.\n        if query_string:\n            query_params = cls._split_url_string(query_string)\n\n            parameters.update(query_params)\n \n        # URL parameters.\n        param_str = urlparse(http_url)[4] # query\n        url_params = cls._split_url_string(param_str)\n        parameters.update(url_params)\n \n        if parameters:\n            return cls(http_method, http_url, parameters)\n \n        return None", "response": "Combines multiple parameter sources."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _split_header(header):\n        params = {}\n        parts = header.split(',')\n        for param in parts:\n            # Ignore realm parameter.\n            if param.find('realm') > -1:\n                continue\n            # Remove whitespace.\n            param = param.strip()\n            # Split key-value.\n            param_parts = param.split('=', 1)\n            # Remove quotes and unescape the value.\n            params[param_parts[0]] = unquote(param_parts[1].strip('\\\"'))\n        return params", "response": "Turn Authorization header into parameters."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _split_url_string(param_str):\n        if not PY3:\n            # If passed unicode with quoted UTF8, Python2's parse_qs leaves\n            # mojibake'd uniocde after unquoting, so encode first.\n            param_str = b(param_str, 'utf-8')\n        parameters = parse_qs(param_str, keep_blank_values=True)\n        for k, v in parameters.items():\n            if len(v) == 1:\n                parameters[k] = unquote(v[0])\n            else:\n                parameters[k] = sorted([unquote(s) for s in v])\n        return parameters", "response": "Turn a URL string into parameters."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef verify_request(self, request, consumer, token):\n\n        self._check_version(request)\n        self._check_signature(request, consumer, token)\n        parameters = request.get_nonoauth_parameters()\n        return parameters", "response": "Verifies an api call and checks all the parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _check_version(self, request):\n        version = self._get_version(request)\n        if version and version != self.version:\n            raise Error('OAuth version %s not supported.' % str(version))", "response": "Verify the correct version of the request for this server."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_signature_method(self, request):\n        signature_method = request.get('oauth_signature_method')\n        if signature_method is None:\n            signature_method = SIGNATURE_METHOD\n\n        try:\n            # Get the signature method object.\n            return self.signature_methods[signature_method]\n        except KeyError:\n            signature_method_names = ', '.join(self.signature_methods.keys())\n            raise Error('Signature method %s not supported try one of the '\n                        'following: %s'\n                            % (signature_method, signature_method_names))", "response": "Figure out the signature method."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nverify that timestamp is recentish.", "response": "def _check_timestamp(self, timestamp):\n        \"\"\"Verify that timestamp is recentish.\"\"\"\n        timestamp = int(timestamp)\n        now = int(time.time())\n        lapsed = now - timestamp\n        if lapsed > self.timestamp_threshold:\n            raise Error('Expired timestamp: given %d and now %s has a '\n                'greater difference than threshold %d' % (timestamp, now, \n                    self.timestamp_threshold))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check(self, request, consumer, token, signature):\n        built = self.sign(request, consumer, token)\n        return built == signature", "response": "Checks whether the given signature is the correct signature for the given consumer and token."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sign(self, request, consumer, token):\n        key, raw = self.signing_base(request, consumer, token)\n\n        hashed = hmac.new(key, raw, sha1)\n\n        # Calculate the digest base 64.\n        return binascii.b2a_base64(hashed.digest())[:-1]", "response": "Builds the base signature string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconcatenating the consumer key and secret with the token s secret.", "response": "def signing_base(self, request, consumer, token):\n        \"\"\"Concatenates the consumer key and secret with the token's\n        secret.\"\"\"\n        sig = '%s&' % escape(consumer.secret)\n        if token:\n            sig = sig + escape(token.secret)\n        return sig, sig"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the value for the option or default if not defined.", "response": "def get(self, option, default=undefined, cast=undefined):\n        \"\"\"\n        Return the value for option or default if defined.\n        \"\"\"\n\n        # We can't avoid __contains__ because value may be empty.\n        if option in os.environ:\n            value = os.environ[option]\n        elif option in self.repository:\n            value = self.repository[option]\n        else:\n            if isinstance(default, Undefined):\n                raise UndefinedValueError('{} not found. Declare it as envvar or define a default value.'.format(option))\n\n            value = default\n\n        if isinstance(cast, Undefined):\n            cast = self._cast_do_nothing\n        elif cast is bool:\n            cast = self._cast_boolean\n\n        return cast(value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a timezone instance from a string Timezone int float or _datetime. tzinfoInfo.", "response": "def _safe_timezone(obj):\n    # type: (Union[str, int, float, _datetime.tzinfo]) -> _Timezone\n    \"\"\"\n    Creates a timezone instance\n    from a string, Timezone, TimezoneInfo or integer offset.\n    \"\"\"\n    if isinstance(obj, _Timezone):\n        return obj\n\n    if obj is None or obj == \"local\":\n        return local_timezone()\n\n    if isinstance(obj, (int, float)):\n        obj = int(obj * 60 * 60)\n    elif isinstance(obj, _datetime.tzinfo):\n        # pytz\n        if hasattr(obj, \"localize\"):\n            obj = obj.zone\n        else:\n            offset = obj.utcoffset(None)\n\n            if offset is None:\n                offset = _datetime.timedelta(0)\n\n            obj = int(offset.total_seconds())\n\n    return timezone(obj)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a new DateTime instance from a specific date and time.", "response": "def datetime(\n    year,  # type: int\n    month,  # type: int\n    day,  # type: int\n    hour=0,  # type: int\n    minute=0,  # type: int\n    second=0,  # type: int\n    microsecond=0,  # type: int\n    tz=UTC,  # type: Union[str, _Timezone]\n    dst_rule=POST_TRANSITION,  # type: str\n):  # type: (...) -> DateTime\n    \"\"\"\n    Creates a new DateTime instance from a specific date and time.\n    \"\"\"\n    if tz is not None:\n        tz = _safe_timezone(tz)\n\n    if not _HAS_FOLD:\n        dt = naive(year, month, day, hour, minute, second, microsecond)\n    else:\n        dt = _datetime.datetime(year, month, day, hour, minute, second, microsecond)\n    if tz is not None:\n        dt = tz.convert(dt, dst_rule=dst_rule)\n\n    return DateTime(\n        dt.year,\n        dt.month,\n        dt.day,\n        dt.hour,\n        dt.minute,\n        dt.second,\n        dt.microsecond,\n        tzinfo=dt.tzinfo,\n        fold=dt.fold,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a DateTime in the local timezone.", "response": "def local(\n    year, month, day, hour=0, minute=0, second=0, microsecond=0\n):  # type: (int, int, int, int, int, int, int) -> DateTime\n    \"\"\"\n    Return a DateTime in the local timezone.\n    \"\"\"\n    return datetime(\n        year, month, day, hour, minute, second, microsecond, tz=local_timezone()\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef naive(\n    year, month, day, hour=0, minute=0, second=0, microsecond=0\n):  # type: (int, int, int, int, int, int, int) -> DateTime\n    \"\"\"\n    Return a naive DateTime.\n    \"\"\"\n    return DateTime(year, month, day, hour, minute, second, microsecond)", "response": "Return a naive DateTime.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a new Time instance.", "response": "def time(hour, minute=0, second=0, microsecond=0):  # type: (int, int, int, int) -> Time\n    \"\"\"\n    Create a new Time instance.\n    \"\"\"\n    return Time(hour, minute, second, microsecond)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a DateTime instance from a datetime one.", "response": "def instance(\n    dt, tz=UTC  # type: _datetime.datetime  # type: Union[str, _Timezone, None]\n):  # type: (...) -> DateTime\n    \"\"\"\n    Create a DateTime instance from a datetime one.\n    \"\"\"\n    if not isinstance(dt, _datetime.datetime):\n        raise ValueError(\"instance() only accepts datetime objects.\")\n\n    if isinstance(dt, DateTime):\n        return dt\n\n    tz = dt.tzinfo or tz\n\n    # Checking for pytz/tzinfo\n    if isinstance(tz, _datetime.tzinfo) and not isinstance(tz, _Timezone):\n        # pytz\n        if hasattr(tz, \"localize\") and tz.zone:\n            tz = tz.zone\n        else:\n            # We have no sure way to figure out\n            # the timezone name, we fallback\n            # on a fixed offset\n            tz = tz.utcoffset(dt).total_seconds() / 3600\n\n    return datetime(\n        dt.year, dt.month, dt.day, dt.hour, dt.minute, dt.second, dt.microsecond, tz=tz\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef now(tz=None):  # type: (Union[str, _Timezone, None]) -> DateTime\n    if has_test_now():\n        test_instance = get_test_now()\n        _tz = _safe_timezone(tz)\n\n        if tz is not None and _tz != test_instance.timezone:\n            test_instance = test_instance.in_tz(_tz)\n\n        return test_instance\n\n    if tz is None or tz == \"local\":\n        dt = _datetime.datetime.now(local_timezone())\n    elif tz is UTC or tz == \"UTC\":\n        dt = _datetime.datetime.now(UTC)\n    else:\n        dt = _datetime.datetime.now(UTC)\n        tz = _safe_timezone(tz)\n        dt = tz.convert(dt)\n\n    return instance(dt, tz)", "response": "Get a DateTime instance for the current date and time."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a DateTime instance from a specific format.", "response": "def from_format(\n    string,  # type: str\n    fmt,  # type: str\n    tz=UTC,  # type: Union[str, _Timezone]\n    locale=None,  # type: Union[str, None]\n):  # type: (...) -> DateTime\n    \"\"\"\n    Creates a DateTime instance from a specific format.\n    \"\"\"\n    parts = _formatter.parse(string, fmt, now(), locale=locale)\n    if parts[\"tz\"] is None:\n        parts[\"tz\"] = tz\n\n    return datetime(**parts)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a DateTime instance from a timestamp.", "response": "def from_timestamp(\n    timestamp, tz=UTC  # type: Union[int, float]  # type: Union[str, _Timezone]\n):  # type: (...) -> DateTime\n    \"\"\"\n    Create a DateTime instance from a timestamp.\n    \"\"\"\n    dt = _datetime.datetime.utcfromtimestamp(timestamp)\n\n    dt = datetime(\n        dt.year, dt.month, dt.day, dt.hour, dt.minute, dt.second, dt.microsecond\n    )\n\n    if tz is not UTC or tz != \"UTC\":\n        dt = dt.in_timezone(tz)\n\n    return dt"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef duration(\n    days=0,  # type: float\n    seconds=0,  # type: float\n    microseconds=0,  # type: float\n    milliseconds=0,  # type: float\n    minutes=0,  # type: float\n    hours=0,  # type: float\n    weeks=0,  # type: float\n    years=0,  # type: float\n    months=0,  # type: float\n):  # type: (...) -> Duration\n    \"\"\"\n    Create a Duration instance.\n    \"\"\"\n    return Duration(\n        days=days,\n        seconds=seconds,\n        microseconds=microseconds,\n        milliseconds=milliseconds,\n        minutes=minutes,\n        hours=hours,\n        weeks=weeks,\n        years=years,\n        months=months,\n    )", "response": "Create a Duration instance."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef period(\n    start, end, absolute=False  # type: DateTime  # type: DateTime  # type: bool\n):  # type: (...) -> Period\n    \"\"\"\n    Create a Period instance.\n    \"\"\"\n    return Period(start, end, absolute=absolute)", "response": "Create a Period instance."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_duration(\n    dt,  # type:  Union[datetime, date]\n    years=0,  # type: int\n    months=0,  # type: int\n    weeks=0,  # type: int\n    days=0,  # type: int\n    hours=0,  # type: int\n    minutes=0,  # type: int\n    seconds=0,  # type: int\n    microseconds=0,\n):  # type: (...) -> Union[datetime, date]\n    \"\"\"\n    Adds a duration to a date/datetime instance.\n    \"\"\"\n    days += weeks * 7\n\n    if (\n        isinstance(dt, date)\n        and not isinstance(dt, datetime)\n        and any([hours, minutes, seconds, microseconds])\n    ):\n        raise RuntimeError(\"Time elements cannot be added to a date instance.\")\n\n    # Normalizing\n    if abs(microseconds) > 999999:\n        s = _sign(microseconds)\n        div, mod = divmod(microseconds * s, 1000000)\n        microseconds = mod * s\n        seconds += div * s\n\n    if abs(seconds) > 59:\n        s = _sign(seconds)\n        div, mod = divmod(seconds * s, 60)\n        seconds = mod * s\n        minutes += div * s\n\n    if abs(minutes) > 59:\n        s = _sign(minutes)\n        div, mod = divmod(minutes * s, 60)\n        minutes = mod * s\n        hours += div * s\n\n    if abs(hours) > 23:\n        s = _sign(hours)\n        div, mod = divmod(hours * s, 24)\n        hours = mod * s\n        days += div * s\n\n    if abs(months) > 11:\n        s = _sign(months)\n        div, mod = divmod(months * s, 12)\n        months = mod * s\n        years += div * s\n\n    year = dt.year + years\n    month = dt.month\n\n    if months:\n        month += months\n        if month > 12:\n            year += 1\n            month -= 12\n        elif month < 1:\n            year -= 1\n            month += 12\n\n    day = min(DAYS_PER_MONTHS[int(is_leap(year))][month], dt.day)\n\n    dt = dt.replace(year=year, month=month, day=day)\n\n    return dt + timedelta(\n        days=days,\n        hours=hours,\n        minutes=minutes,\n        seconds=seconds,\n        microseconds=microseconds,\n    )", "response": "Adds a duration to a date or datetime instance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndivide a by b and round result to the nearest integer", "response": "def _divide_and_round(a, b):\n    \"\"\"divide a by b and round result to the nearest integer\n\n    When the ratio is exactly half-way between two integers,\n    the even integer is returned.\n    \"\"\"\n    # Based on the reference implementation for divmod_near\n    # in Objects/longobject.c.\n    q, r = divmod(a, b)\n    # round up if either r / b > 0.5, or r / b == 0.5 and q is odd.\n    # The expression r / b > 0.5 is equivalent to 2 * r > b if b is\n    # positive, 2 * r < b if b negative.\n    r *= 2\n    greater_than_half = r > b if b > 0 else r < b\n    if greater_than_half or r == b and q % 2 == 1:\n        q += 1\n\n    return q"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the current interval in words in the current locale.", "response": "def in_words(self, locale=None, separator=\" \"):\n        \"\"\"\n        Get the current interval in words in the current locale.\n\n        Ex: 6 jours 23 heures 58 minutes\n\n        :param locale: The locale to use. Defaults to current locale.\n        :type locale: str\n\n        :param separator: The separator to use between each unit\n        :type separator: str\n\n        :rtype: str\n        \"\"\"\n        periods = [\n            (\"year\", self.years),\n            (\"month\", self.months),\n            (\"week\", self.weeks),\n            (\"day\", self.remaining_days),\n            (\"hour\", self.hours),\n            (\"minute\", self.minutes),\n            (\"second\", self.remaining_seconds),\n        ]\n\n        if locale is None:\n            locale = pendulum.get_locale()\n\n        locale = pendulum.locale(locale)\n        parts = []\n        for period in periods:\n            unit, count = period\n            if abs(count) > 0:\n                translation = locale.translation(\n                    \"units.{}.{}\".format(unit, locale.plural(abs(count)))\n                )\n                parts.append(translation.format(count))\n\n        if not parts and abs(self.microseconds) > 0:\n            translation = locale.translation(\"units.second.{}\".format(locale.plural(1)))\n            us = abs(self.microseconds) / 1e6\n            parts.append(translation.format(\"{:.2f}\".format(us)))\n\n        return decode(separator.join(parts))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef format(self, diff, is_now=True, absolute=False, locale=None):\n        if locale is None:\n            locale = self._locale\n        else:\n            locale = Locale.load(locale)\n\n        count = diff.remaining_seconds\n\n        if diff.years > 0:\n            unit = \"year\"\n            count = diff.years\n\n            if diff.months > 6:\n                count += 1\n        elif diff.months == 11 and (diff.weeks * 7 + diff.remaining_days) > 15:\n            unit = \"year\"\n            count = 1\n        elif diff.months > 0:\n            unit = \"month\"\n            count = diff.months\n\n            if (diff.weeks * 7 + diff.remaining_days) >= 27:\n                count += 1\n        elif diff.weeks > 0:\n            unit = \"week\"\n            count = diff.weeks\n\n            if diff.remaining_days > 3:\n                count += 1\n        elif diff.remaining_days > 0:\n            unit = \"day\"\n            count = diff.remaining_days\n\n            if diff.hours >= 22:\n                count += 1\n        elif diff.hours > 0:\n            unit = \"hour\"\n            count = diff.hours\n        elif diff.minutes > 0:\n            unit = \"minute\"\n            count = diff.minutes\n        elif 10 < diff.remaining_seconds <= 59:\n            unit = \"second\"\n            count = diff.remaining_seconds\n        else:\n            # We check if the \"a few seconds\" unit exists\n            time = locale.get(\"custom.units.few_second\")\n            if time is not None:\n                if absolute:\n                    return time\n\n                key = \"custom\"\n                is_future = diff.invert\n                if is_now:\n                    if is_future:\n                        key += \".from_now\"\n                    else:\n                        key += \".ago\"\n                else:\n                    if is_future:\n                        key += \".after\"\n                    else:\n                        key += \".before\"\n\n                return locale.get(key).format(time)\n            else:\n                unit = \"second\"\n                count = diff.remaining_seconds\n\n        if count == 0:\n            count = 1\n\n        if absolute:\n            key = \"translations.units.{}\".format(unit)\n        else:\n            is_future = diff.invert\n\n            if is_now:\n                # Relative to now, so we can use\n                # the CLDR data\n                key = \"translations.relative.{}\".format(unit)\n\n                if is_future:\n                    key += \".future\"\n                else:\n                    key += \".past\"\n            else:\n                # Absolute comparison\n                # So we have to use the custom locale data\n\n                # Checking for special pluralization rules\n                key = \"custom.units_relative\"\n                if is_future:\n                    key += \".{}.future\".format(unit)\n                else:\n                    key += \".{}.past\".format(unit)\n\n                trans = locale.get(key)\n                if not trans:\n                    # No special rule\n                    time = locale.get(\n                        \"translations.units.{}.{}\".format(unit, locale.plural(count))\n                    ).format(count)\n                else:\n                    time = trans[locale.plural(count)].format(count)\n\n                key = \"custom\"\n                if is_future:\n                    key += \".after\"\n                else:\n                    key += \".before\"\n\n                return locale.get(key).format(decode(time))\n\n        key += \".{}\".format(locale.plural(count))\n\n        return decode(locale.get(key).format(count))", "response": "Formats a difference.\n\n        :param diff: The difference to format\n        :type diff: pendulum.period.Period\n\n        :param is_now: Whether the difference includes now\n        :type is_now: bool\n\n        :param absolute: Whether it's an absolute difference or not\n        :type absolute: bool\n\n        :param locale: The locale to use\n        :type locale: str or None\n\n        :rtype: str"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef closest(self, dt1, dt2):\n        dt1 = self.__class__(dt1.hour, dt1.minute, dt1.second, dt1.microsecond)\n        dt2 = self.__class__(dt2.hour, dt2.minute, dt2.second, dt2.microsecond)\n\n        if self.diff(dt1).in_seconds() < self.diff(dt2).in_seconds():\n            return dt1\n\n        return dt2", "response": "Get the closest time from the instance."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add(self, hours=0, minutes=0, seconds=0, microseconds=0):\n        from .datetime import DateTime\n\n        return (\n            DateTime.EPOCH.at(self.hour, self.minute, self.second, self.microsecond)\n            .add(\n                hours=hours, minutes=minutes, seconds=seconds, microseconds=microseconds\n            )\n            .time()\n        )", "response": "Add duration to the instance."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_timedelta(self, delta):\n        if delta.days:\n            raise TypeError(\"Cannot add timedelta with days to Time.\")\n\n        return self.add(seconds=delta.seconds, microseconds=delta.microseconds)", "response": "Adds a timedelta to the instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsubtracts the timedelta from the instance.", "response": "def subtract_timedelta(self, delta):\n        \"\"\"\n        Remove timedelta duration from the instance.\n\n        :param delta: The timedelta instance\n        :type delta: datetime.timedelta\n\n        :rtype: Time\n        \"\"\"\n        if delta.days:\n            raise TypeError(\"Cannot subtract timedelta with days to Time.\")\n\n        return self.subtract(seconds=delta.seconds, microseconds=delta.microseconds)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the difference between two Time objects as an Duration.", "response": "def diff(self, dt=None, abs=True):\n        \"\"\"\n        Returns the difference between two Time objects as an Duration.\n\n        :type dt: Time or None\n\n        :param abs: Whether to return an absolute interval or not\n        :type abs: bool\n\n        :rtype: Duration\n        \"\"\"\n        if dt is None:\n            dt = pendulum.now().time()\n        else:\n            dt = self.__class__(dt.hour, dt.minute, dt.second, dt.microsecond)\n\n        us1 = (\n            self.hour * SECS_PER_HOUR + self.minute * SECS_PER_MIN + self.second\n        ) * USECS_PER_SEC\n\n        us2 = (\n            dt.hour * SECS_PER_HOUR + dt.minute * SECS_PER_MIN + dt.second\n        ) * USECS_PER_SEC\n\n        klass = Duration\n        if abs:\n            klass = AbsoluteDuration\n\n        return klass(microseconds=us2 - us1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses ISO 8601 compliant string into a datetime object.", "response": "def parse_iso8601(text):\n    \"\"\"\n    ISO 8601 compliant parser.\n\n    :param text: The string to parse\n    :type text: str\n\n    :rtype: datetime.datetime or datetime.time or datetime.date\n    \"\"\"\n    parsed = _parse_iso8601_duration(text)\n    if parsed is not None:\n        return parsed\n\n    m = ISO8601_DT.match(text)\n    if not m:\n        raise ParserError(\"Invalid ISO 8601 string\")\n\n    ambiguous_date = False\n    is_date = False\n    is_time = False\n    year = 0\n    month = 1\n    day = 1\n    minute = 0\n    second = 0\n    microsecond = 0\n    tzinfo = None\n\n    if m:\n        if m.group(\"date\"):\n            # A date has been specified\n            is_date = True\n\n            if m.group(\"isocalendar\"):\n                # We have a ISO 8601 string defined\n                # by week number\n                if (\n                    m.group(\"weeksep\")\n                    and not m.group(\"weekdaysep\")\n                    and m.group(\"isoweekday\")\n                ):\n                    raise ParserError(\"Invalid date string: {}\".format(text))\n\n                if not m.group(\"weeksep\") and m.group(\"weekdaysep\"):\n                    raise ParserError(\"Invalid date string: {}\".format(text))\n\n                try:\n                    date = _get_iso_8601_week(\n                        m.group(\"isoyear\"), m.group(\"isoweek\"), m.group(\"isoweekday\")\n                    )\n                except ParserError:\n                    raise\n                except ValueError:\n                    raise ParserError(\"Invalid date string: {}\".format(text))\n\n                year = date[\"year\"]\n                month = date[\"month\"]\n                day = date[\"day\"]\n            else:\n                # We have a classic date representation\n                year = int(m.group(\"year\"))\n\n                if not m.group(\"monthday\"):\n                    # No month and day\n                    month = 1\n                    day = 1\n                else:\n                    if m.group(\"month\") and m.group(\"day\"):\n                        # Month and day\n                        if not m.group(\"daysep\") and len(m.group(\"day\")) == 1:\n                            # Ordinal day\n                            ordinal = int(m.group(\"month\") + m.group(\"day\"))\n                            leap = is_leap(year)\n                            months_offsets = MONTHS_OFFSETS[leap]\n\n                            if ordinal > months_offsets[13]:\n                                raise ParserError(\"Ordinal day is out of range\")\n\n                            for i in range(1, 14):\n                                if ordinal <= months_offsets[i]:\n                                    day = ordinal - months_offsets[i - 1]\n                                    month = i - 1\n\n                                    break\n                        else:\n                            month = int(m.group(\"month\"))\n                            day = int(m.group(\"day\"))\n                    else:\n                        # Only month\n                        if not m.group(\"monthsep\"):\n                            # The date looks like 201207\n                            # which is invalid for a date\n                            # But it might be a time in the form hhmmss\n                            ambiguous_date = True\n\n                        month = int(m.group(\"month\"))\n                        day = 1\n\n        if not m.group(\"time\"):\n            # No time has been specified\n            if ambiguous_date:\n                # We can \"safely\" assume that the ambiguous date\n                # was actually a time in the form hhmmss\n                hhmmss = \"{}{:0>2}\".format(str(year), str(month))\n\n                return datetime.time(int(hhmmss[:2]), int(hhmmss[2:4]), int(hhmmss[4:]))\n\n            return datetime.date(year, month, day)\n\n        if ambiguous_date:\n            raise ParserError(\"Invalid date string: {}\".format(text))\n\n        if is_date and not m.group(\"timesep\"):\n            raise ParserError(\"Invalid date string: {}\".format(text))\n\n        if not is_date:\n            is_time = True\n\n        # Grabbing hh:mm:ss\n        hour = int(m.group(\"hour\"))\n        minsep = m.group(\"minsep\")\n\n        if m.group(\"minute\"):\n            minute = int(m.group(\"minute\"))\n        elif minsep:\n            raise ParserError(\"Invalid ISO 8601 time part\")\n\n        secsep = m.group(\"secsep\")\n        if secsep and not minsep and m.group(\"minute\"):\n            # minute/second separator but no hour/minute separator\n            raise ParserError(\"Invalid ISO 8601 time part\")\n\n        if m.group(\"second\"):\n            if not secsep and minsep:\n                # No minute/second separator but hour/minute separator\n                raise ParserError(\"Invalid ISO 8601 time part\")\n\n            second = int(m.group(\"second\"))\n        elif secsep:\n            raise ParserError(\"Invalid ISO 8601 time part\")\n\n        # Grabbing subseconds, if any\n        if m.group(\"subsecondsection\"):\n            # Limiting to 6 chars\n            subsecond = m.group(\"subsecond\")[:6]\n\n            microsecond = int(\"{:0<6}\".format(subsecond))\n\n        # Grabbing timezone, if any\n        tz = m.group(\"tz\")\n        if tz:\n            if tz == \"Z\":\n                offset = 0\n            else:\n                negative = True if tz.startswith(\"-\") else False\n                tz = tz[1:]\n                if \":\" not in tz:\n                    if len(tz) == 2:\n                        tz = \"{}00\".format(tz)\n\n                    off_hour = tz[0:2]\n                    off_minute = tz[2:4]\n                else:\n                    off_hour, off_minute = tz.split(\":\")\n\n                offset = ((int(off_hour) * 60) + int(off_minute)) * 60\n\n                if negative:\n                    offset = -1 * offset\n\n            tzinfo = FixedTimezone(offset)\n\n        if is_time:\n            return datetime.time(hour, minute, second, microsecond)\n\n        return datetime.datetime(\n            year, month, day, hour, minute, second, microsecond, tzinfo=tzinfo\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreads the zoneinfo structure for a given timezone name.", "response": "def read_for(self, timezone):  # type: (str) -> Timezone\n        \"\"\"\n        Read the zoneinfo structure for a given timezone name.\n\n        :param timezone: The timezone.\n        \"\"\"\n        try:\n            file_path = pytzdata.tz_path(timezone)\n        except TimezoneNotFound:\n            raise InvalidTimezone(timezone)\n\n        return self.read(file_path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreads a zoneinfo file from the given path.", "response": "def read(self, file_path):  # type: (str) -> Timezone\n        \"\"\"\n        Read a zoneinfo structure from the given path.\n\n        :param file_path: The path of a zoneinfo file.\n        \"\"\"\n        if not os.path.exists(file_path):\n            raise InvalidZoneinfoFile(\"The tzinfo file does not exist\")\n\n        with open(file_path, \"rb\") as fd:\n            return self._parse(fd)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _check_read(self, fd, nbytes):  # type: (...) -> bytes\n        result = fd.read(nbytes)\n\n        if (not result and nbytes > 0) or len(result) != nbytes:\n            raise InvalidZoneinfoFile(\n                \"Expected {} bytes reading {}, \"\n                \"but got {}\".format(nbytes, fd.name, len(result) if result else 0)\n            )\n\n        if PY2:\n            return bytearray(result)\n\n        return result", "response": "Reads the given number of bytes from the given file\n        and checks that the correct number of bytes could be read."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses a zoneinfo file and return a Timezone object.", "response": "def _parse(self, fd):  # type: (...) -> Timezone\n        \"\"\"\n        Parse a zoneinfo file.\n        \"\"\"\n        hdr = self._parse_header(fd)\n\n        if hdr.version in (2, 3):\n            # We're skipping the entire v1 file since\n            # at least the same data will be found in TZFile 2.\n            fd.seek(\n                hdr.transitions * 5\n                + hdr.types * 6\n                + hdr.abbr_size\n                + hdr.leaps * 4\n                + hdr.stdwalls\n                + hdr.utclocals,\n                1,\n            )\n\n            # Parse the second header\n            hdr = self._parse_header(fd)\n\n            if hdr.version != 2 and hdr.version != 3:\n                raise InvalidZoneinfoFile(\n                    \"Header versions mismatch for file {}\".format(fd.name)\n                )\n\n            # Parse the v2 data\n            trans = self._parse_trans_64(fd, hdr.transitions)\n            type_idx = self._parse_type_idx(fd, hdr.transitions)\n            types = self._parse_types(fd, hdr.types)\n            abbrs = self._parse_abbrs(fd, hdr.abbr_size, types)\n\n            fd.seek(hdr.leaps * 8 + hdr.stdwalls + hdr.utclocals, 1)\n\n            trule = self._parse_posix_tz(fd)\n        else:\n            # TZFile v1\n            trans = self._parse_trans_32(fd, hdr.transitions)\n            type_idx = self._parse_type_idx(fd, hdr.transitions)\n            types = self._parse_types(fd, hdr.types)\n            abbrs = self._parse_abbrs(fd, hdr.abbr_size, types)\n            trule = None\n\n        types = [\n            TransitionType(off, is_dst, abbrs[abbr]) for off, is_dst, abbr in types\n        ]\n\n        transitions = []\n        previous = None\n        for trans, idx in zip(trans, type_idx):\n            transition = Transition(trans, types[idx], previous)\n            transitions.append(transition)\n\n            previous = transition\n\n        if not transitions:\n            transitions.append(Transition(0, types[0], None))\n\n        return Timezone(transitions, posix_rule=trule, extended=self._extend)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a Timezone instance given its name.", "response": "def timezone(name, extended=True):  # type: (Union[str, int]) -> _Timezone\n    \"\"\"\n    Return a Timezone instance given its name.\n    \"\"\"\n    if isinstance(name, int):\n        return fixed_timezone(name)\n\n    if name.lower() == \"utc\":\n        return UTC\n\n    if name in _tz_cache:\n        return _tz_cache[name]\n\n    tz = _Timezone(name, extended=extended)\n    _tz_cache[name] = tz\n\n    return tz"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a fixed Timezone instance given its offset in seconds.", "response": "def fixed_timezone(offset):  # type: (int) -> _FixedTimezone\n    \"\"\"\n    Return a Timezone instance given its offset in seconds.\n    \"\"\"\n    if offset in _tz_cache:\n        return _tz_cache[offset]\n\n    tz = _FixedTimezone(offset)\n    _tz_cache[offset] = tz\n\n    return tz"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef convert(\n        self, dt, dst_rule=None  # type: datetime  # type: Union[str, None]\n    ):  # type: (...) -> datetime\n        \"\"\"\n        Converts a datetime in the current timezone.\n\n        If the datetime is naive, it will be normalized.\n\n        >>> from datetime import datetime\n        >>> from pendulum import timezone\n        >>> paris = timezone('Europe/Paris')\n        >>> dt = datetime(2013, 3, 31, 2, 30, fold=1)\n        >>> in_paris = paris.convert(dt)\n        >>> in_paris.isoformat()\n        '2013-03-31T03:30:00+02:00'\n\n        If the datetime is aware, it will be properly converted.\n\n        >>> new_york = timezone('America/New_York')\n        >>> in_new_york = new_york.convert(in_paris)\n        >>> in_new_york.isoformat()\n        '2013-03-30T21:30:00-04:00'\n        \"\"\"\n        if dt.tzinfo is None:\n            return self._normalize(dt, dst_rule=dst_rule)\n\n        return self._convert(dt)", "response": "Converts a datetime into a new ISO - 8601 time."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef datetime(\n        self, year, month, day, hour=0, minute=0, second=0, microsecond=0\n    ):  # type: (int, int, int, int, int, int, int) -> datetime\n        \"\"\"\n        Return a normalized datetime for the current timezone.\n        \"\"\"\n        if _HAS_FOLD:\n            return self.convert(\n                datetime(year, month, day, hour, minute, second, microsecond, fold=1)\n            )\n\n        return self.convert(\n            datetime(year, month, day, hour, minute, second, microsecond),\n            dst_rule=POST_TRANSITION,\n        )", "response": "Returns a normalized datetime for the current timezone."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef naive(self):  # type: () -> DateTime\n        return self.__class__(\n            self.year,\n            self.month,\n            self.day,\n            self.hour,\n            self.minute,\n            self.second,\n            self.microsecond,\n        )", "response": "Return the DateTime without timezone information."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef on(self, year, month, day):\n        return self.set(year=int(year), month=int(month), day=int(day))", "response": "Returns a new instance with the current date set to a different date."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef at(self, hour, minute=0, second=0, microsecond=0):\n        return self.set(\n            hour=hour, minute=minute, second=second, microsecond=microsecond\n        )", "response": "Returns a new instance with the current time to a different time."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef in_timezone(self, tz):  # type: (Union[str, Timezone]) -> DateTime\n        tz = pendulum._safe_timezone(tz)\n\n        return tz.convert(self, dst_rule=pendulum.POST_TRANSITION)", "response": "Returns a new instance in a given timezone."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the ISO 8601 string representation of the instance as a string.", "response": "def to_iso8601_string(self):\n        \"\"\"\n        Format the instance as ISO 8601.\n\n        :rtype: str\n        \"\"\"\n        string = self._to_string(\"iso8601\")\n\n        if self.tz and self.tz.name == \"UTC\":\n            string = string.replace(\"+00:00\", \"Z\")\n\n        return string"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _to_string(self, fmt, locale=None):\n        if fmt not in self._FORMATS:\n            raise ValueError(\"Format [{}] is not supported\".format(fmt))\n\n        fmt = self._FORMATS[fmt]\n        if callable(fmt):\n            return fmt(self)\n\n        return self.format(fmt, locale=locale)", "response": "Return the string representation of the current instance in the given format."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the closest date from the instance.", "response": "def closest(self, dt1, dt2, *dts):\n        from functools import reduce\n\n        \"\"\"\n        Get the farthest date from the instance.\n\n        :type dt1: datetime.datetime\n        :type dt2: datetime.datetime\n        :type dts: list[datetime.datetime,]\n\n        :rtype: DateTime\n        \"\"\"\n        dt1 = pendulum.instance(dt1)\n        dt2 = pendulum.instance(dt2)\n        dts = [dt1, dt2] + [pendulum.instance(x) for x in dts]\n        dts = [(abs(self - dt), dt) for dt in dts]\n\n        return min(dts)[1]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the farthest date from the instance.", "response": "def farthest(self, dt1, dt2, *dts):\n        from functools import reduce\n\n        \"\"\"\n        Get the farthest date from the instance.\n\n        :type dt1: datetime.datetime\n        :type dt2: datetime.datetime\n        :type dts: list[datetime.datetime,]\n\n        :rtype: DateTime\n        \"\"\"\n        dt1 = pendulum.instance(dt1)\n        dt2 = pendulum.instance(dt2)\n\n        dts = [dt1, dt2] + [pendulum.instance(x) for x in dts]\n        dts = [(abs(self - dt), dt) for dt in dts]\n\n        return max(dts)[1]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_long_year(self):\n        return (\n            pendulum.datetime(self.year, 12, 28, 0, 0, 0, tz=self.tz).isocalendar()[1]\n            == 53\n        )", "response": "Determines if the instance is a long year."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_same_day(self, dt):\n        dt = pendulum.instance(dt)\n\n        return self.to_date_string() == dt.to_date_string()", "response": "Checks if the passed in date is the same day as the instance current day."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_birthday(self, dt=None):\n        if dt is None:\n            dt = self.now(self.tz)\n\n        instance = pendulum.instance(dt)\n\n        return (self.month, self.day) == (instance.month, instance.day)", "response": "Checks if the current date is the birthday."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add(\n        self,\n        years=0,\n        months=0,\n        weeks=0,\n        days=0,\n        hours=0,\n        minutes=0,\n        seconds=0,\n        microseconds=0,\n    ):  # type: (int, int, int, int, int, int, int) -> DateTime\n        \"\"\"\n        Add a duration to the instance.\n\n        If we're adding units of variable length (i.e., years, months),\n        move forward from curren time,\n        otherwise move forward from utc, for accuracy\n        when moving across DST boundaries.\n        \"\"\"\n        units_of_variable_length = any([years, months, weeks, days])\n\n        current_dt = datetime.datetime(\n            self.year,\n            self.month,\n            self.day,\n            self.hour,\n            self.minute,\n            self.second,\n            self.microsecond,\n        )\n        if not units_of_variable_length:\n            offset = self.utcoffset()\n            if offset:\n                current_dt = current_dt - offset\n\n        dt = add_duration(\n            current_dt,\n            years=years,\n            months=months,\n            weeks=weeks,\n            days=days,\n            hours=hours,\n            minutes=minutes,\n            seconds=seconds,\n            microseconds=microseconds,\n        )\n\n        if units_of_variable_length or self.tzinfo is None:\n            return pendulum.datetime(\n                dt.year,\n                dt.month,\n                dt.day,\n                dt.hour,\n                dt.minute,\n                dt.second,\n                dt.microsecond,\n                tz=self.tz,\n            )\n\n        dt = self.__class__(\n            dt.year,\n            dt.month,\n            dt.day,\n            dt.hour,\n            dt.minute,\n            dt.second,\n            dt.microsecond,\n            tzinfo=UTC,\n        )\n\n        dt = self.tz.convert(dt)\n\n        return self.__class__(\n            dt.year,\n            dt.month,\n            dt.day,\n            dt.hour,\n            dt.minute,\n            dt.second,\n            dt.microsecond,\n            tzinfo=self.tz,\n            fold=dt.fold,\n        )", "response": "Adds a duration to the current instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef subtract(\n        self,\n        years=0,\n        months=0,\n        weeks=0,\n        days=0,\n        hours=0,\n        minutes=0,\n        seconds=0,\n        microseconds=0,\n    ):\n        \"\"\"\n        Remove duration from the instance.\n\n        :param years: The number of years\n        :type years: int\n\n        :param months: The number of months\n        :type months: int\n\n        :param weeks: The number of weeks\n        :type weeks: int\n\n        :param days: The number of days\n        :type days: int\n\n        :param hours: The number of hours\n        :type hours: int\n\n        :param minutes: The number of minutes\n        :type minutes: int\n\n        :param seconds: The number of seconds\n        :type seconds: int\n\n        :param microseconds: The number of microseconds\n        :type microseconds: int\n\n        :rtype: DateTime\n        \"\"\"\n        return self.add(\n            years=-years,\n            months=-months,\n            weeks=-weeks,\n            days=-days,\n            hours=-hours,\n            minutes=-minutes,\n            seconds=-seconds,\n            microseconds=-microseconds,\n        )", "response": "Subtracts the duration from the instance."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd a timedelta to the instance.", "response": "def _add_timedelta(self, delta):\n        \"\"\"\n        Add timedelta duration to the instance.\n\n        :param delta: The timedelta instance\n        :type delta: pendulum.Duration or datetime.timedelta\n\n        :rtype: DateTime\n        \"\"\"\n        if isinstance(delta, pendulum.Period):\n            return self.add(\n                years=delta.years,\n                months=delta.months,\n                weeks=delta.weeks,\n                days=delta.remaining_days,\n                hours=delta.hours,\n                minutes=delta.minutes,\n                seconds=delta.remaining_seconds,\n                microseconds=delta.microseconds,\n            )\n        elif isinstance(delta, pendulum.Duration):\n            return self.add(\n                years=delta.years, months=delta.months, seconds=delta.total_seconds()\n            )\n\n        return self.add(seconds=delta.total_seconds())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _subtract_timedelta(self, delta):\n        if isinstance(delta, pendulum.Duration):\n            return self.subtract(\n                years=delta.years,\n                months=delta.months,\n                weeks=delta.weeks,\n                days=delta.remaining_days,\n                hours=delta.hours,\n                minutes=delta.minutes,\n                seconds=delta.remaining_seconds,\n                microseconds=delta.microseconds,\n            )\n\n        return self.subtract(\n            days=delta.days, seconds=delta.seconds, microseconds=delta.microseconds\n        )", "response": "Removes the timedelta duration from the instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the difference between two DateTime objects represented as a Period.", "response": "def diff(self, dt=None, abs=True):\n        \"\"\"\n        Returns the difference between two DateTime objects represented as a Duration.\n\n        :type dt: DateTime or None\n\n        :param abs: Whether to return an absolute interval or not\n        :type abs: bool\n\n        :rtype: Period\n        \"\"\"\n        if dt is None:\n            dt = self.now(self.tz)\n\n        return Period(self, dt, absolute=abs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef start_of(self, unit):\n        if unit not in self._MODIFIERS_VALID_UNITS:\n            raise ValueError('Invalid unit \"{}\" for start_of()'.format(unit))\n\n        return getattr(self, \"_start_of_{}\".format(unit))()", "response": "Returns a copy of the time reset\n        with the time reset to 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef end_of(self, unit):\n        if unit not in self._MODIFIERS_VALID_UNITS:\n            raise ValueError('Invalid unit \"%s\" for end_of()' % unit)\n\n        return getattr(self, \"_end_of_%s\" % unit)()", "response": "Returns a copy of the time reset\n        with the time reset to the end of the given unit"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nresets the date to the last day of the month and the time to 999999.", "response": "def _end_of_month(self):\n        \"\"\"\n        Reset the date to the last day of the month\n        and the time to 23:59:59.999999.\n\n        :rtype: DateTime\n        \"\"\"\n        return self.set(self.year, self.month, self.days_in_month, 23, 59, 59, 999999)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreset the date to the last day of the century and the time to 23 : 59. 999.", "response": "def _end_of_century(self):\n        \"\"\"\n        Reset the date to the last day of the century\n        and the time to 23:59:59.999999.\n\n        :rtype: DateTime\n        \"\"\"\n        year = self.year - 1 - (self.year - 1) % YEARS_PER_CENTURY + YEARS_PER_CENTURY\n\n        return self.set(year, 12, 31, 23, 59, 59, 999999)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _start_of_week(self):\n        dt = self\n\n        if self.day_of_week != pendulum._WEEK_STARTS_AT:\n            dt = self.previous(pendulum._WEEK_STARTS_AT)\n\n        return dt.start_of(\"day\")", "response": "Return the start of the week in the current date."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _end_of_week(self):\n        dt = self\n\n        if self.day_of_week != pendulum._WEEK_ENDS_AT:\n            dt = self.next(pendulum._WEEK_ENDS_AT)\n\n        return dt.end_of(\"day\")", "response": "Returns the end of the week in the current time."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef next(self, day_of_week=None, keep_time=False):\n        if day_of_week is None:\n            day_of_week = self.day_of_week\n\n        if day_of_week < SUNDAY or day_of_week > SATURDAY:\n            raise ValueError(\"Invalid day of week\")\n\n        if keep_time:\n            dt = self\n        else:\n            dt = self.start_of(\"day\")\n\n        dt = dt.add(days=1)\n        while dt.day_of_week != day_of_week:\n            dt = dt.add(days=1)\n\n        return dt", "response": "Modify to the next occurrence of a given day of the week. If no day_of_week is provided modify to the next occurrence of the current day of the week. Use the consts\nTaxField. MONDAY to indicate the desired day_of_week ex. DateTime. MONDAY."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef previous(self, day_of_week=None, keep_time=False):\n        if day_of_week is None:\n            day_of_week = self.day_of_week\n\n        if day_of_week < SUNDAY or day_of_week > SATURDAY:\n            raise ValueError(\"Invalid day of week\")\n\n        if keep_time:\n            dt = self\n        else:\n            dt = self.start_of(\"day\")\n\n        dt = dt.subtract(days=1)\n        while dt.day_of_week != day_of_week:\n            dt = dt.subtract(days=1)\n\n        return dt", "response": "Modify to the previous occurrence of a given day of the week. If no day_of_week is provided modify to the previous occurrence of the current day of the week. Use the consts\nTaxField. MONDAY to indicate the desired day_of_week ex. DateTime. MONDAY."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef first_of(self, unit, day_of_week=None):\n        if unit not in [\"month\", \"quarter\", \"year\"]:\n            raise ValueError('Invalid unit \"{}\" for first_of()'.format(unit))\n\n        return getattr(self, \"_first_of_{}\".format(unit))(day_of_week)", "response": "Returns an instance set to the first occurrence of a given day of the week in the current unit. If no day_of_week is provided modify to the first day of the week in the current unit."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns an instance set to the last occurrence of a given day of the week in the current unit. If no day_of_week is provided modify to the last day of the unit. Use the consts DateTime. MONDAY to indicate the desired day_of_week ex. DateTime. MONDAY.", "response": "def last_of(self, unit, day_of_week=None):\n        \"\"\"\n        Returns an instance set to the last occurrence\n        of a given day of the week in the current unit.\n        If no day_of_week is provided, modify to the last day of the unit.\n        Use the supplied consts to indicate the desired day_of_week, ex. DateTime.MONDAY.\n\n        Supported units are month, quarter and year.\n\n        :param unit: The unit to use\n        :type unit: str\n\n        :type day_of_week: int or None\n\n        :rtype: DateTime\n        \"\"\"\n        if unit not in [\"month\", \"quarter\", \"year\"]:\n            raise ValueError('Invalid unit \"{}\" for first_of()'.format(unit))\n\n        return getattr(self, \"_last_of_{}\".format(unit))(day_of_week)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a new instance set to the given occurrence of a given day of the week in the given unit. If the occurrence is outside the scope of the current unit or an exception is raised.", "response": "def nth_of(self, unit, nth, day_of_week):\n        \"\"\"\n        Returns a new instance set to the given occurrence\n        of a given day of the week in the current unit.\n        If the calculated occurrence is outside the scope of the current unit,\n        then raise an error. Use the supplied consts\n        to indicate the desired day_of_week, ex. DateTime.MONDAY.\n\n        Supported units are month, quarter and year.\n\n        :param unit: The unit to use\n        :type unit: str\n\n        :type nth: int\n\n        :type day_of_week: int or None\n\n        :rtype: DateTime\n        \"\"\"\n        if unit not in [\"month\", \"quarter\", \"year\"]:\n            raise ValueError('Invalid unit \"{}\" for first_of()'.format(unit))\n\n        dt = getattr(self, \"_nth_of_{}\".format(unit))(nth, day_of_week)\n        if dt is False:\n            raise PendulumException(\n                \"Unable to find occurence {} of {} in {}\".format(\n                    nth, self._days[day_of_week], unit\n                )\n            )\n\n        return dt"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmodify to the first occurrence of a given day of the week in the current month. If no day_of_week is provided modify to the first occurrence of the current month. Use the consts to indicate the desired day_of_week ex. DateTime. MONDAY.", "response": "def _first_of_month(self, day_of_week):\n        \"\"\"\n        Modify to the first occurrence of a given day of the week\n        in the current month. If no day_of_week is provided,\n        modify to the first day of the month. Use the supplied consts\n        to indicate the desired day_of_week, ex. DateTime.MONDAY.\n\n        :type day_of_week: int\n\n        :rtype: DateTime\n        \"\"\"\n        dt = self.start_of(\"day\")\n\n        if day_of_week is None:\n            return dt.set(day=1)\n\n        month = calendar.monthcalendar(dt.year, dt.month)\n\n        calendar_day = (day_of_week - 1) % 7\n\n        if month[0][calendar_day] > 0:\n            day_of_month = month[0][calendar_day]\n        else:\n            day_of_month = month[1][calendar_day]\n\n        return dt.set(day=day_of_month)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmodify to the first occurrence of a given day of the week in the current quarter. If no day_of_week is provided modify to the first day of the quarter. Use the consts to indicate the desired day_of_week ex. DateTime. MONDAY.", "response": "def _first_of_quarter(self, day_of_week=None):\n        \"\"\"\n        Modify to the first occurrence of a given day of the week\n        in the current quarter. If no day_of_week is provided,\n        modify to the first day of the quarter. Use the supplied consts\n        to indicate the desired day_of_week, ex. DateTime.MONDAY.\n\n        :type day_of_week: int or None\n\n        :rtype: DateTime\n        \"\"\"\n        return self.on(self.year, self.quarter * 3 - 2, 1).first_of(\n            \"month\", day_of_week\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _last_of_quarter(self, day_of_week=None):\n        return self.on(self.year, self.quarter * 3, 1).last_of(\"month\", day_of_week)", "response": "Modify to the last occurrence of a given day of the week in the current quarter. If no day_of_week is provided modify to the last day of the quarter. Use the consts\n       . MONDAY to indicate the desired day_of_week ex. DateTime. MONDAY."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _nth_of_year(self, nth, day_of_week):\n        if nth == 1:\n            return self.first_of(\"year\", day_of_week)\n\n        dt = self.first_of(\"year\")\n        year = dt.year\n        for i in range(nth - (1 if dt.day_of_week == day_of_week else 0)):\n            dt = dt.next(day_of_week)\n\n        if year != dt.year:\n            return False\n\n        return self.on(self.year, dt.month, dt.day).start_of(\"day\")", "response": "Modify to the given occurrence of a given day of the week in the current year. If the occurrence is outside the scope of the current year then return False and no modifications are made."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmodifies the current instance to the average of the current instance and the current instance.", "response": "def average(self, dt=None):\n        \"\"\"\n        Modify the current instance to the average\n        of a given instance (default now) and the current instance.\n\n        :type dt: DateTime or datetime\n\n        :rtype: DateTime\n        \"\"\"\n        if dt is None:\n            dt = self.now(self.tz)\n\n        diff = self.diff(dt, False)\n        return self.add(\n            microseconds=(diff.in_seconds() * 1000000 + diff.microseconds) // 2\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef day_of_year(self):\n        k = 1 if self.is_leap_year() else 2\n\n        return (275 * self.month) // 9 - k * ((self.month + 9) // 12) + self.day - 30", "response": "Returns the day of the year."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the closest date from the instance.", "response": "def closest(self, dt1, dt2):\n        \"\"\"\n        Get the closest date from the instance.\n\n        :type dt1: Date or date\n        :type dt2: Date or date\n\n        :rtype: Date\n        \"\"\"\n        dt1 = self.__class__(dt1.year, dt1.month, dt1.day)\n        dt2 = self.__class__(dt2.year, dt2.month, dt2.day)\n\n        if self.diff(dt1).in_seconds() < self.diff(dt2).in_seconds():\n            return dt1\n\n        return dt2"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks if the current date is the birthday.", "response": "def is_birthday(self, dt=None):\n        \"\"\"\n        Check if its the birthday.\n\n        Compares the date/month values of the two dates.\n\n        :rtype: bool\n        \"\"\"\n        if dt is None:\n            dt = Date.today()\n\n        instance = dt1 = self.__class__(dt.year, dt.month, dt.day)\n\n        return (self.month, self.day) == (instance.month, instance.day)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a duration to the instance.", "response": "def add(self, years=0, months=0, weeks=0, days=0):\n        \"\"\"\n        Add duration to the instance.\n\n        :param years: The number of years\n        :type years: int\n\n        :param months: The number of months\n        :type months: int\n\n        :param weeks: The number of weeks\n        :type weeks: int\n\n        :param days: The number of days\n        :type days: int\n\n        :rtype: Date\n        \"\"\"\n        dt = add_duration(\n            date(self.year, self.month, self.day),\n            years=years,\n            months=months,\n            weeks=weeks,\n            days=days,\n        )\n\n        return self.__class__(dt.year, dt.month, dt.day)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsubtracts the duration from the instance.", "response": "def subtract(self, years=0, months=0, weeks=0, days=0):\n        \"\"\"\n        Remove duration from the instance.\n\n        :param years: The number of years\n        :type years: int\n\n        :param months: The number of months\n        :type months: int\n\n        :param weeks: The number of weeks\n        :type weeks: int\n\n        :param days: The number of days\n        :type days: int\n\n        :rtype: Date\n        \"\"\"\n        return self.add(years=-years, months=-months, weeks=-weeks, days=-days)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a timedelta to the instance.", "response": "def _add_timedelta(self, delta):\n        \"\"\"\n        Add timedelta duration to the instance.\n\n        :param delta: The timedelta instance\n        :type delta: pendulum.Duration or datetime.timedelta\n\n        :rtype: Date\n        \"\"\"\n        if isinstance(delta, pendulum.Duration):\n            return self.add(\n                years=delta.years,\n                months=delta.months,\n                weeks=delta.weeks,\n                days=delta.remaining_days,\n            )\n\n        return self.add(days=delta.days)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsubtract the timedelta from the instance.", "response": "def _subtract_timedelta(self, delta):\n        \"\"\"\n        Remove timedelta duration from the instance.\n\n        :param delta: The timedelta instance\n        :type delta: pendulum.Duration or datetime.timedelta\n\n        :rtype: Date\n        \"\"\"\n        if isinstance(delta, pendulum.Duration):\n            return self.subtract(\n                years=delta.years,\n                months=delta.months,\n                weeks=delta.weeks,\n                days=delta.remaining_days,\n            )\n\n        return self.subtract(days=delta.days)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the difference between two Date objects as a Period.", "response": "def diff(self, dt=None, abs=True):\n        \"\"\"\n        Returns the difference between two Date objects as a Period.\n\n        :type dt: Date or None\n\n        :param abs: Whether to return an absolute interval or not\n        :type abs: bool\n\n        :rtype: Period\n        \"\"\"\n        if dt is None:\n            dt = self.today()\n\n        return Period(self, Date(dt.year, dt.month, dt.day), absolute=abs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the difference in a human readable format for the current locale.", "response": "def diff_for_humans(self, other=None, absolute=False, locale=None):\n        \"\"\"\n        Get the difference in a human readable format in the current locale.\n\n        When comparing a value in the past to default now:\n        1 day ago\n        5 months ago\n\n        When comparing a value in the future to default now:\n        1 day from now\n        5 months from now\n\n        When comparing a value in the past to another value:\n        1 day before\n        5 months before\n\n        When comparing a value in the future to another value:\n        1 day after\n        5 months after\n\n        :type other: Date\n\n        :param absolute: removes time difference modifiers ago, after, etc\n        :type absolute: bool\n\n        :param locale: The locale to use for localization\n        :type locale: str\n\n        :rtype: str\n        \"\"\"\n        is_now = other is None\n\n        if is_now:\n            other = self.today()\n\n        diff = self.diff(other)\n\n        return pendulum.format_diff(diff, is_now, absolute, locale)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nresetting the date to the first day of the decade.", "response": "def _start_of_decade(self):\n        \"\"\"\n        Reset the date to the first day of the decade.\n\n        :rtype: Date\n        \"\"\"\n        year = self.year - self.year % YEARS_PER_DECADE\n\n        return self.set(year, 1, 1)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nresetting the date to the last day of the decade.", "response": "def _end_of_decade(self):\n        \"\"\"\n        Reset the date to the last day of the decade.\n\n        :rtype: Date\n        \"\"\"\n        year = self.year - self.year % YEARS_PER_DECADE + YEARS_PER_DECADE - 1\n\n        return self.set(year, 12, 31)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nresets the date to the first day of the century.", "response": "def _start_of_century(self):\n        \"\"\"\n        Reset the date to the first day of the century.\n\n        :rtype: Date\n        \"\"\"\n        year = self.year - 1 - (self.year - 1) % YEARS_PER_CENTURY + 1\n\n        return self.set(year, 1, 1)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef next(self, day_of_week=None):\n        if day_of_week is None:\n            day_of_week = self.day_of_week\n\n        if day_of_week < SUNDAY or day_of_week > SATURDAY:\n            raise ValueError(\"Invalid day of week\")\n\n        dt = self.add(days=1)\n        while dt.day_of_week != day_of_week:\n            dt = dt.add(days=1)\n\n        return dt", "response": "Modify to the next occurrence of a given day of the week. If no day_of_week is provided modify to the next occurrence of the current day of the week. Use the consts\n       . MONDAY to indicate the desired day_of_week ex. pendulum. MONDAY."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmodifies to the previous occurrence of a given day of the week. If no day_of_week is provided modify to the previous occurrence of the current day of the week. Use the consts . MONDAY to indicate the desired day_of_week ex. pendulum. MONDAY.", "response": "def previous(self, day_of_week=None):\n        \"\"\"\n        Modify to the previous occurrence of a given day of the week.\n        If no day_of_week is provided, modify to the previous occurrence\n        of the current day of the week.  Use the supplied consts\n        to indicate the desired day_of_week, ex. pendulum.MONDAY.\n\n        :param day_of_week: The previous day of week to reset to.\n        :type day_of_week: int or None\n\n        :rtype: Date\n        \"\"\"\n        if day_of_week is None:\n            day_of_week = self.day_of_week\n\n        if day_of_week < SUNDAY or day_of_week > SATURDAY:\n            raise ValueError(\"Invalid day of week\")\n\n        dt = self.subtract(days=1)\n        while dt.day_of_week != day_of_week:\n            dt = dt.subtract(days=1)\n\n        return dt"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmodify to the given occurrence of a given day of the week in the current month. If the occurrence is outside the scope of the current month then return False and no modifications are made. If the occurrence is outside the scope of the current month then return False and no modifications are made.", "response": "def _nth_of_month(self, nth, day_of_week):\n        \"\"\"\n        Modify to the given occurrence of a given day of the week\n        in the current month. If the calculated occurrence is outside,\n        the scope of the current month, then return False and no\n        modifications are made. Use the supplied consts\n        to indicate the desired day_of_week, ex. pendulum.MONDAY.\n\n        :type nth: int\n\n        :type day_of_week: int or None\n\n        :rtype: Date\n        \"\"\"\n        if nth == 1:\n            return self.first_of(\"month\", day_of_week)\n\n        dt = self.first_of(\"month\")\n        check = dt.format(\"YYYY-MM\")\n        for i in range(nth - (1 if dt.day_of_week == day_of_week else 0)):\n            dt = dt.next(day_of_week)\n\n        if dt.format(\"YYYY-MM\") == check:\n            return self.set(day=dt.day)\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmodifies to the first occurrence of a given day of the week in the current quarter. If no day_of_week is provided modify to the first day of the quarter. Use the consts to indicate the desired day_of_week ex. pendulum. MONDAY.", "response": "def _first_of_quarter(self, day_of_week=None):\n        \"\"\"\n        Modify to the first occurrence of a given day of the week\n        in the current quarter. If no day_of_week is provided,\n        modify to the first day of the quarter. Use the supplied consts\n        to indicate the desired day_of_week, ex. pendulum.MONDAY.\n\n        :type day_of_week: int or None\n\n        :rtype: Date\n        \"\"\"\n        return self.set(self.year, self.quarter * 3 - 2, 1).first_of(\n            \"month\", day_of_week\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmodifying to the last occurrence of a given day of the week in the current quarter. If no day_of_week is provided modify to the last day of the quarter. Use the consts to indicate the desired day_of_week ex. pendulum. MONDAY.", "response": "def _last_of_quarter(self, day_of_week=None):\n        \"\"\"\n        Modify to the last occurrence of a given day of the week\n        in the current quarter. If no day_of_week is provided,\n        modify to the last day of the quarter. Use the supplied consts\n        to indicate the desired day_of_week, ex. pendulum.MONDAY.\n\n        :type day_of_week: int or None\n\n        :rtype: Date\n        \"\"\"\n        return self.set(self.year, self.quarter * 3, 1).last_of(\"month\", day_of_week)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmodify to the given occurrence of a given day of the week in the current quarter. If the occurrence is outside the current quarter then return False and no modifications are made. If the occurrence is outside the current quarter then return False and no modifications are made.", "response": "def _nth_of_quarter(self, nth, day_of_week):\n        \"\"\"\n        Modify to the given occurrence of a given day of the week\n        in the current quarter. If the calculated occurrence is outside,\n        the scope of the current quarter, then return False and no\n        modifications are made. Use the supplied consts\n        to indicate the desired day_of_week, ex. pendulum.MONDAY.\n\n        :type nth: int\n\n        :type day_of_week: int or None\n\n        :rtype: Date\n        \"\"\"\n        if nth == 1:\n            return self.first_of(\"quarter\", day_of_week)\n\n        dt = self.replace(self.year, self.quarter * 3, 1)\n        last_month = dt.month\n        year = dt.year\n        dt = dt.first_of(\"quarter\")\n        for i in range(nth - (1 if dt.day_of_week == day_of_week else 0)):\n            dt = dt.next(day_of_week)\n\n        if last_month < dt.month or year != dt.year:\n            return False\n\n        return self.set(self.year, dt.month, dt.day)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef average(self, dt=None):\n        if dt is None:\n            dt = Date.today()\n\n        return self.add(days=int(self.diff(dt, False).in_days() / 2))", "response": "Modify the current instance to the average of the current instance and the current instance."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef format(self, dt, fmt, locale=None):\n        if not locale:\n            locale = pendulum.get_locale()\n\n        locale = Locale.load(locale)\n\n        result = self._FORMAT_RE.sub(\n            lambda m: m.group(1)\n            if m.group(1)\n            else m.group(2)\n            if m.group(2)\n            else self._format_token(dt, m.group(3), locale),\n            fmt,\n        )\n\n        return decode(result)", "response": "Formats a DateTime instance with a given format and locale."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _format_token(self, dt, token, locale):\n        if token in self._DATE_FORMATS:\n            fmt = locale.get(\"custom.date_formats.{}\".format(token))\n            if fmt is None:\n                fmt = self._DEFAULT_DATE_FORMATS[token]\n\n            return self.format(dt, fmt, locale)\n\n        if token in self._LOCALIZABLE_TOKENS:\n            return self._format_localizable_token(dt, token, locale)\n\n        if token in self._TOKENS_RULES:\n            return self._TOKENS_RULES[token](dt)\n\n        # Timezone\n        if token in [\"ZZ\", \"Z\"]:\n            if dt.tzinfo is None:\n                return \"\"\n\n            separator = \":\" if token == \"Z\" else \"\"\n            offset = dt.utcoffset() or datetime.timedelta()\n            minutes = offset.total_seconds() / 60\n\n            if minutes >= 0:\n                sign = \"+\"\n            else:\n                sign = \"-\"\n\n            hour, minute = divmod(abs(int(minutes)), 60)\n\n            return \"{}{:02d}{}{:02d}\".format(sign, hour, separator, minute)", "response": "Formats a DateTime instance with a given token and locale."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nformats a DateTime instance with a given localizable token and locale.", "response": "def _format_localizable_token(self, dt, token, locale):\n        \"\"\"\n        Formats a DateTime instance\n        with a given localizable token and locale.\n\n        :param dt: The instance to format\n        :type dt: pendulum.DateTime\n\n        :param token: The token to use\n        :type token: str\n\n        :param locale: The locale to use\n        :type locale: Locale\n\n        :rtype: str\n        \"\"\"\n        if token == \"MMM\":\n            return locale.get(\"translations.months.abbreviated\")[dt.month]\n        elif token == \"MMMM\":\n            return locale.get(\"translations.months.wide\")[dt.month]\n        elif token == \"dd\":\n            return locale.get(\"translations.days.short\")[dt.day_of_week]\n        elif token == \"ddd\":\n            return locale.get(\"translations.days.abbreviated\")[dt.day_of_week]\n        elif token == \"dddd\":\n            return locale.get(\"translations.days.wide\")[dt.day_of_week]\n        elif token == \"Do\":\n            return locale.ordinalize(dt.day)\n        elif token == \"do\":\n            return locale.ordinalize(dt.day_of_week)\n        elif token == \"Mo\":\n            return locale.ordinalize(dt.month)\n        elif token == \"Qo\":\n            return locale.ordinalize(dt.quarter)\n        elif token == \"wo\":\n            return locale.ordinalize(dt.week_of_year)\n        elif token == \"DDDo\":\n            return locale.ordinalize(dt.day_of_year)\n        elif token == \"A\":\n            key = \"translations.day_periods\"\n            if dt.hour >= 12:\n                key += \".pm\"\n            else:\n                key += \".am\"\n\n            return locale.get(key)\n        else:\n            return token"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse(\n        self,\n        time,  # type: str\n        fmt,  # type: str\n        now,  # type: pendulum.DateTime\n        locale=None,  # type:  typing.Union[str, None]\n    ):  # type: (...) -> dict\n        \"\"\"\n        Parses a time string matching a given format as a tuple.\n\n        :param time: The timestring\n        :param fmt: The format\n        :param now: The datetime to use as \"now\"\n        :param locale: The locale to use\n\n        :return: The parsed elements\n        \"\"\"\n        escaped_fmt = re.escape(fmt)\n\n        tokens = self._FROM_FORMAT_RE.findall(escaped_fmt)\n        if not tokens:\n            return time\n\n        if not locale:\n            locale = pendulum.get_locale()\n\n        locale = Locale.load(locale)\n\n        parsed = {\n            \"year\": None,\n            \"month\": None,\n            \"day\": None,\n            \"hour\": None,\n            \"minute\": None,\n            \"second\": None,\n            \"microsecond\": None,\n            \"tz\": None,\n            \"quarter\": None,\n            \"day_of_week\": None,\n            \"day_of_year\": None,\n            \"meridiem\": None,\n            \"timestamp\": None,\n        }\n\n        pattern = self._FROM_FORMAT_RE.sub(\n            lambda m: self._replace_tokens(m.group(0), locale), escaped_fmt\n        )\n\n        if not re.match(pattern, time):\n            raise ValueError(\"String does not match format {}\".format(fmt))\n\n        re.sub(pattern, lambda m: self._get_parsed_values(m, parsed, locale, now), time)\n\n        return self._check_parsed(parsed, now)", "response": "Parses a time string matching a given format as a tuple."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _check_parsed(self, parsed, now):  # type: (dict, pendulum.DateTime) -> dict\n        validated = {\n            \"year\": parsed[\"year\"],\n            \"month\": parsed[\"month\"],\n            \"day\": parsed[\"day\"],\n            \"hour\": parsed[\"hour\"],\n            \"minute\": parsed[\"minute\"],\n            \"second\": parsed[\"second\"],\n            \"microsecond\": parsed[\"microsecond\"],\n            \"tz\": None,\n        }\n\n        # If timestamp has been specified\n        # we use it and don't go any further\n        if parsed[\"timestamp\"] is not None:\n            str_us = str(parsed[\"timestamp\"])\n            if \".\" in str_us:\n                microseconds = int(\"{}\".format(str_us.split(\".\")[1].ljust(6, \"0\")))\n            else:\n                microseconds = 0\n\n            time = local_time(parsed[\"timestamp\"], 0, microseconds)\n            validated[\"year\"] = time[0]\n            validated[\"month\"] = time[1]\n            validated[\"day\"] = time[2]\n            validated[\"hour\"] = time[3]\n            validated[\"minute\"] = time[4]\n            validated[\"second\"] = time[5]\n            validated[\"microsecond\"] = time[6]\n\n            return validated\n\n        if parsed[\"quarter\"] is not None:\n            if validated[\"year\"] is not None:\n                dt = pendulum.datetime(validated[\"year\"], 1, 1)\n            else:\n                dt = now\n\n            dt = dt.start_of(\"year\")\n\n            while dt.quarter != parsed[\"quarter\"]:\n                dt = dt.add(months=3)\n\n            validated[\"year\"] = dt.year\n            validated[\"month\"] = dt.month\n            validated[\"day\"] = dt.day\n\n        if validated[\"year\"] is None:\n            validated[\"year\"] = now.year\n\n        if parsed[\"day_of_year\"] is not None:\n            dt = pendulum.parse(\n                \"{}-{:>03d}\".format(validated[\"year\"], parsed[\"day_of_year\"])\n            )\n\n            validated[\"month\"] = dt.month\n            validated[\"day\"] = dt.day\n\n        if parsed[\"day_of_week\"] is not None:\n            dt = pendulum.datetime(\n                validated[\"year\"],\n                validated[\"month\"] or now.month,\n                validated[\"day\"] or now.day,\n            )\n            dt = dt.start_of(\"week\").subtract(days=1)\n            dt = dt.next(parsed[\"day_of_week\"])\n            validated[\"year\"] = dt.year\n            validated[\"month\"] = dt.month\n            validated[\"day\"] = dt.day\n\n        # Meridiem\n        if parsed[\"meridiem\"] is not None:\n            # If the time is greater than 13:00:00\n            # This is not valid\n            if validated[\"hour\"] is None:\n                raise ValueError(\"Invalid Date\")\n\n            t = (\n                validated[\"hour\"],\n                validated[\"minute\"],\n                validated[\"second\"],\n                validated[\"microsecond\"],\n            )\n            if t >= (13, 0, 0, 0):\n                raise ValueError(\"Invalid date\")\n\n            pm = parsed[\"meridiem\"] == \"pm\"\n            validated[\"hour\"] %= 12\n            if pm:\n                validated[\"hour\"] += 12\n\n        if validated[\"month\"] is None:\n            if parsed[\"year\"] is not None:\n                validated[\"month\"] = parsed[\"month\"] or 1\n            else:\n                validated[\"month\"] = parsed[\"month\"] or now.month\n\n        if validated[\"day\"] is None:\n            if parsed[\"year\"] is not None or parsed[\"month\"] is not None:\n                validated[\"day\"] = parsed[\"day\"] or 1\n            else:\n                validated[\"day\"] = parsed[\"day\"] or now.day\n\n        for part in [\"hour\", \"minute\", \"second\", \"microsecond\"]:\n            if validated[part] is None:\n                validated[part] = 0\n\n        validated[\"tz\"] = parsed[\"tz\"]\n\n        return validated", "response": "Checks validity of parsed elements."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef local_time(unix_time, utc_offset, microseconds):\n    year = EPOCH_YEAR\n    seconds = int(math.floor(unix_time))\n\n    # Shift to a base year that is 400-year aligned.\n    if seconds >= 0:\n        seconds -= 10957 * SECS_PER_DAY\n        year += 30  # == 2000\n    else:\n        seconds += (146097 - 10957) * SECS_PER_DAY\n        year -= 370  # == 1600\n\n    seconds += utc_offset\n\n    # Handle years in chunks of 400/100/4/1\n    year += 400 * (seconds // SECS_PER_400_YEARS)\n    seconds %= SECS_PER_400_YEARS\n    if seconds < 0:\n        seconds += SECS_PER_400_YEARS\n        year -= 400\n\n    leap_year = 1  # 4-century aligned\n\n    sec_per_100years = SECS_PER_100_YEARS[leap_year]\n    while seconds >= sec_per_100years:\n        seconds -= sec_per_100years\n        year += 100\n        leap_year = 0  # 1-century, non 4-century aligned\n        sec_per_100years = SECS_PER_100_YEARS[leap_year]\n\n    sec_per_4years = SECS_PER_4_YEARS[leap_year]\n    while seconds >= sec_per_4years:\n        seconds -= sec_per_4years\n        year += 4\n        leap_year = 1  # 4-year, non century aligned\n        sec_per_4years = SECS_PER_4_YEARS[leap_year]\n\n    sec_per_year = SECS_PER_YEAR[leap_year]\n    while seconds >= sec_per_year:\n        seconds -= sec_per_year\n        year += 1\n        leap_year = 0  # non 4-year aligned\n        sec_per_year = SECS_PER_YEAR[leap_year]\n\n    # Handle months and days\n    month = TM_DECEMBER + 1\n    day = seconds // SECS_PER_DAY + 1\n    seconds %= SECS_PER_DAY\n    while month != TM_JANUARY + 1:\n        month_offset = MONTHS_OFFSETS[leap_year][month]\n        if day > month_offset:\n            day -= month_offset\n            break\n\n        month -= 1\n\n    # Handle hours, minutes, seconds and microseconds\n    hour = seconds // SECS_PER_HOUR\n    seconds %= SECS_PER_HOUR\n    minute = seconds // SECS_PER_MIN\n    second = seconds % SECS_PER_MIN\n\n    return (year, month, day, hour, minute, second, microseconds)", "response": "Returns a broken down timebase for a particular transition type."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef precise_diff(d1, d2):\n    sign = 1\n\n    if d1 == d2:\n        return PreciseDiff(0, 0, 0, 0, 0, 0, 0, 0)\n\n    tzinfo1 = d1.tzinfo if isinstance(d1, datetime.datetime) else None\n    tzinfo2 = d2.tzinfo if isinstance(d2, datetime.datetime) else None\n\n    if (\n        tzinfo1 is None\n        and tzinfo2 is not None\n        or tzinfo2 is None\n        and tzinfo1 is not None\n    ):\n        raise ValueError(\n            \"Comparison between naive and aware datetimes is not supported\"\n        )\n\n    if d1 > d2:\n        d1, d2 = d2, d1\n        sign = -1\n\n    d_diff = 0\n    hour_diff = 0\n    min_diff = 0\n    sec_diff = 0\n    mic_diff = 0\n    total_days = _day_number(d2.year, d2.month, d2.day) - _day_number(\n        d1.year, d1.month, d1.day\n    )\n    in_same_tz = False\n    tz1 = None\n    tz2 = None\n\n    # Trying to figure out the timezone names\n    # If we can't find them, we assume different timezones\n    if tzinfo1 and tzinfo2:\n        if hasattr(tzinfo1, \"name\"):\n            # Pendulum timezone\n            tz1 = tzinfo1.name\n        elif hasattr(tzinfo1, \"zone\"):\n            # pytz timezone\n            tz1 = tzinfo1.zone\n\n        if hasattr(tzinfo2, \"name\"):\n            tz2 = tzinfo2.name\n        elif hasattr(tzinfo2, \"zone\"):\n            tz2 = tzinfo2.zone\n\n        in_same_tz = tz1 == tz2 and tz1 is not None\n\n    if isinstance(d2, datetime.datetime):\n        if isinstance(d1, datetime.datetime):\n            # If we are not in the same timezone\n            # we need to adjust\n            #\n            # We also need to adjust if we do not\n            # have variable-length units\n            if not in_same_tz or total_days == 0:\n                offset1 = d1.utcoffset()\n                offset2 = d2.utcoffset()\n\n                if offset1:\n                    d1 = d1 - offset1\n\n                if offset2:\n                    d2 = d2 - offset2\n\n            hour_diff = d2.hour - d1.hour\n            min_diff = d2.minute - d1.minute\n            sec_diff = d2.second - d1.second\n            mic_diff = d2.microsecond - d1.microsecond\n        else:\n            hour_diff = d2.hour\n            min_diff = d2.minute\n            sec_diff = d2.second\n            mic_diff = d2.microsecond\n\n        if mic_diff < 0:\n            mic_diff += 1000000\n            sec_diff -= 1\n\n        if sec_diff < 0:\n            sec_diff += 60\n            min_diff -= 1\n\n        if min_diff < 0:\n            min_diff += 60\n            hour_diff -= 1\n\n        if hour_diff < 0:\n            hour_diff += 24\n            d_diff -= 1\n\n    y_diff = d2.year - d1.year\n    m_diff = d2.month - d1.month\n    d_diff += d2.day - d1.day\n\n    if d_diff < 0:\n        year = d2.year\n        month = d2.month\n\n        if month == 1:\n            month = 12\n            year -= 1\n        else:\n            month -= 1\n\n        leap = int(is_leap(year))\n\n        days_in_last_month = DAYS_PER_MONTHS[leap][month]\n        days_in_month = DAYS_PER_MONTHS[int(is_leap(d2.year))][d2.month]\n\n        if d_diff < days_in_month - days_in_last_month:\n            # We don't have a full month, we calculate days\n            if days_in_last_month < d1.day:\n                d_diff += d1.day\n            else:\n                d_diff += days_in_last_month\n        elif d_diff == days_in_month - days_in_last_month:\n            # We have exactly a full month\n            # We remove the days difference\n            # and add one to the months difference\n            d_diff = 0\n            m_diff += 1\n        else:\n            # We have a full month\n            d_diff += days_in_last_month\n\n        m_diff -= 1\n\n    if m_diff < 0:\n        m_diff += 12\n        y_diff -= 1\n\n    return PreciseDiff(\n        sign * y_diff,\n        sign * m_diff,\n        sign * d_diff,\n        sign * hour_diff,\n        sign * min_diff,\n        sign * sec_diff,\n        sign * mic_diff,\n        sign * total_days,\n    )", "response": "Calculate a precise difference between two datetimes."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse(text, **options):\n    _options = copy.copy(DEFAULT_OPTIONS)\n    _options.update(options)\n\n    return _normalize(_parse(text, **_options), **_options)", "response": "Parses a string with the given options."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nnormalizing the parsed element.", "response": "def _normalize(parsed, **options):\n    \"\"\"\n    Normalizes the parsed element.\n\n    :param parsed: The parsed elements.\n    :type parsed: Parsed\n\n    :rtype: Parsed\n    \"\"\"\n    if options.get(\"exact\"):\n        return parsed\n\n    if isinstance(parsed, time):\n        now = options[\"now\"] or datetime.now()\n\n        return datetime(\n            now.year,\n            now.month,\n            now.day,\n            parsed.hour,\n            parsed.minute,\n            parsed.second,\n            parsed.microsecond,\n        )\n    elif isinstance(parsed, date) and not isinstance(parsed, datetime):\n        return datetime(parsed.year, parsed.month, parsed.day)\n\n    return parsed"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _parse_common(text, **options):\n    m = COMMON.match(text)\n    has_date = False\n    year = 0\n    month = 1\n    day = 1\n\n    if not m:\n        raise ParserError(\"Invalid datetime string\")\n\n    if m.group(\"date\"):\n        # A date has been specified\n        has_date = True\n\n        year = int(m.group(\"year\"))\n\n        if not m.group(\"monthday\"):\n            # No month and day\n            month = 1\n            day = 1\n        else:\n            if options[\"day_first\"]:\n                month = int(m.group(\"day\"))\n                day = int(m.group(\"month\"))\n            else:\n                month = int(m.group(\"month\"))\n                day = int(m.group(\"day\"))\n\n    if not m.group(\"time\"):\n        return date(year, month, day)\n\n    # Grabbing hh:mm:ss\n    hour = int(m.group(\"hour\"))\n\n    minute = int(m.group(\"minute\"))\n\n    if m.group(\"second\"):\n        second = int(m.group(\"second\"))\n    else:\n        second = 0\n\n    # Grabbing subseconds, if any\n    microsecond = 0\n    if m.group(\"subsecondsection\"):\n        # Limiting to 6 chars\n        subsecond = m.group(\"subsecond\")[:6]\n\n        microsecond = int(\"{:0<6}\".format(subsecond))\n\n    if has_date:\n        return datetime(year, month, day, hour, minute, second, microsecond)\n\n    return time(hour, minute, second, microsecond)", "response": "Parses the string text as a common datetime format."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse a string with the given options.", "response": "def _parse(text, **options):\n    \"\"\"\n    Parses a string with the given options.\n\n    :param text: The string to parse.\n    :type text: str\n\n    :rtype: mixed\n    \"\"\"\n    # Handling special cases\n    if text == \"now\":\n        return pendulum.now()\n\n    parsed = base_parse(text, **options)\n\n    if isinstance(parsed, datetime.datetime):\n        return pendulum.datetime(\n            parsed.year,\n            parsed.month,\n            parsed.day,\n            parsed.hour,\n            parsed.minute,\n            parsed.second,\n            parsed.microsecond,\n            tz=parsed.tzinfo or options.get(\"tz\", UTC),\n        )\n\n    if isinstance(parsed, datetime.date):\n        return pendulum.date(parsed.year, parsed.month, parsed.day)\n\n    if isinstance(parsed, datetime.time):\n        return pendulum.time(\n            parsed.hour, parsed.minute, parsed.second, parsed.microsecond\n        )\n\n    if isinstance(parsed, _Interval):\n        if parsed.duration is not None:\n            duration = parsed.duration\n\n            if parsed.start is not None:\n                dt = pendulum.instance(parsed.start, tz=options.get(\"tz\", UTC))\n\n                return pendulum.period(\n                    dt,\n                    dt.add(\n                        years=duration.years,\n                        months=duration.months,\n                        weeks=duration.weeks,\n                        days=duration.remaining_days,\n                        hours=duration.hours,\n                        minutes=duration.minutes,\n                        seconds=duration.remaining_seconds,\n                        microseconds=duration.microseconds,\n                    ),\n                )\n\n            dt = pendulum.instance(parsed.end, tz=options.get(\"tz\", UTC))\n\n            return pendulum.period(\n                dt.subtract(\n                    years=duration.years,\n                    months=duration.months,\n                    weeks=duration.weeks,\n                    days=duration.remaining_days,\n                    hours=duration.hours,\n                    minutes=duration.minutes,\n                    seconds=duration.remaining_seconds,\n                    microseconds=duration.microseconds,\n                ),\n                dt,\n            )\n\n        return pendulum.period(\n            pendulum.instance(parsed.start, tz=options.get(\"tz\", UTC)),\n            pendulum.instance(parsed.end, tz=options.get(\"tz\", UTC)),\n        )\n\n    if CDuration and isinstance(parsed, CDuration):\n        return pendulum.duration(\n            years=parsed.years,\n            months=parsed.months,\n            weeks=parsed.weeks,\n            days=parsed.days,\n            hours=parsed.hours,\n            minutes=parsed.minutes,\n            seconds=parsed.seconds,\n            microseconds=parsed.microseconds,\n        )\n\n    return parsed"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef format(self, fmt, locale=None):\n        return self._formatter.format(self, fmt, locale)", "response": "Formats the instance using the given format."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the txo as binary.", "response": "def as_bin(self, as_spendable=False):\n        \"\"\"Return the txo as binary.\"\"\"\n        f = io.BytesIO()\n        self.stream(f, as_spendable=as_spendable)\n        return f.getvalue()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing the Block Header from the file - like object f.", "response": "def parse_as_header(class_, f):\n        \"\"\"\n        Parse the Block header from the file-like object\n        \"\"\"\n        version, previous_block_hash, merkle_root, height = parse_struct(\"L##L\", f)\n        # https://github.com/BTCGPU/BTCGPU/wiki/Technical-Spec\n        f.read(28)  # reserved area\n        (timestamp, difficulty, nonce, solution) = parse_struct(\"LL#S\", f)\n        return class_(version, previous_block_hash, merkle_root, timestamp,\n                      difficulty, nonce, height, solution)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstreaming the block header in the standard way to the file - like object f.", "response": "def stream_header_legacy(self, f):\n        \"\"\"Stream the block header in the standard way to the file-like object f.\"\"\"\n        stream_struct(\"L##LL\", f, self.version, self.previous_block_hash,\n                      self.merkle_root, self.timestamp, self.difficulty)\n        f.write(self.nonce[:4])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef stream_header(self, f):\n        stream_struct(\"L##L\", f, self.version, self.previous_block_hash,\n                      self.merkle_root, self.height)\n        f.write(b'\\0' * 28)\n        stream_struct(\"LL#S\", f, self.timestamp, self.difficulty, self.nonce, self.solution)", "response": "Stream the block header in the standard way to the file - like object f."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _delete_signature(self, script, sig_blob):\n        subscript = self.ScriptTools.compile_push_data_list([sig_blob])\n        new_script = bytearray()\n        pc = 0\n        for opcode, data, pc, new_pc in self.ScriptTools.get_opcodes(script):\n            section = script[pc:new_pc]\n            if section != subscript:\n                new_script.extend(section)\n        return bytes(new_script)", "response": "Removes the given signature from the given script."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef delete_subscript(class_, script, subscript):\n        new_script = bytearray()\n        pc = 0\n        for opcode, data, pc, new_pc in class_.ScriptTools.get_opcodes(script):\n            section = script[pc:new_pc]\n            if section != subscript:\n                new_script.extend(section)\n        return bytes(new_script)", "response": "Returns a new script with the given subscript removed."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _signature_hash(self, tx_out_script, unsigned_txs_out_idx, hash_type):\n\n        # In case concatenating two scripts ends up with two codeseparators,\n        # or an extra one at the end, this prevents all those possible incompatibilities.\n        tx_out_script = self.delete_subscript(tx_out_script, self.ScriptTools.compile(\"OP_CODESEPARATOR\"))\n\n        # blank out other inputs' signatures\n        txs_in = [self._tx_in_for_idx(i, tx_in, tx_out_script, unsigned_txs_out_idx)\n                  for i, tx_in in enumerate(self.tx.txs_in)]\n        txs_out = self.tx.txs_out\n\n        # Blank out some of the outputs\n        if (hash_type & 0x1f) == SIGHASH_NONE:\n            # Wildcard payee\n            txs_out = []\n\n            # Let the others update at will\n            for i in range(len(txs_in)):\n                if i != unsigned_txs_out_idx:\n                    txs_in[i].sequence = 0\n\n        elif (hash_type & 0x1f) == SIGHASH_SINGLE:\n            # This preserves the ability to validate existing legacy\n            # transactions which followed a buggy path in Satoshi's\n            # original code.\n            if unsigned_txs_out_idx >= len(txs_out):\n                # This should probably be moved to a constant, but the\n                # likelihood of ever getting here is already really small\n                # and getting smaller\n                return (1 << 248)\n\n            # Only lock in the txout payee at same index as txin; delete\n            # any outputs after this one and set all outputs before this\n            # one to \"null\" (where \"null\" means an empty script and a\n            # value of -1)\n            txs_out = [self.tx.TxOut(0xffffffffffffffff, b'')] * unsigned_txs_out_idx\n            txs_out.append(self.tx.txs_out[unsigned_txs_out_idx])\n\n            # Let the others update at will\n            for i in range(len(txs_in)):\n                if i != unsigned_txs_out_idx:\n                    txs_in[i].sequence = 0\n\n        # Blank out other inputs completely, not recommended for open transactions\n        if hash_type & SIGHASH_ANYONECANPAY:\n            txs_in = [txs_in[unsigned_txs_out_idx]]\n\n        tmp_tx = self.tx.__class__(self.tx.version, txs_in, txs_out, self.tx.lock_time)\n        return from_bytes_32(tmp_tx.hash(hash_type=hash_type))", "response": "Return the canonical hash for a transaction."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a TxContext object for the given index.", "response": "def tx_context_for_idx(self, tx_in_idx):\n        \"\"\"\n        solution_script: alleged solution to the puzzle_script\n        puzzle_script: the script protecting the coins\n        \"\"\"\n        tx_in = self.tx.txs_in[tx_in_idx]\n\n        tx_context = TxContext()\n        tx_context.lock_time = self.tx.lock_time\n        tx_context.version = self.tx.version\n        tx_context.puzzle_script = b'' if self.tx.missing_unspent(tx_in_idx) else self.tx.unspents[tx_in_idx].script\n        tx_context.solution_script = tx_in.script\n        tx_context.witness_solution_stack = tx_in.witness\n        tx_context.sequence = tx_in.sequence\n        tx_context.tx_in_idx = tx_in_idx\n        return tx_context"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_solution(self, tx_context, flags=None, traceback_f=None):\n\n        for t in self.puzzle_and_solution_iterator(tx_context, flags=flags, traceback_f=traceback_f):\n            puzzle_script, solution_stack, flags, sighash_f = t\n\n            vm = self.VM(puzzle_script, tx_context, sighash_f, flags=flags, initial_stack=solution_stack[:])\n\n            vm.is_solution_script = False\n            vm.traceback_f = traceback_f\n\n            stack = vm.eval_script()\n            if len(stack) == 0 or not vm.bool_from_script_bytes(stack[-1]):\n                raise self.ScriptError(\"eval false\", errno.EVAL_FALSE)\n\n        if flags & VERIFY_CLEANSTACK and len(stack) != 1:\n            raise self.ScriptError(\"stack not clean after evaluation\", errno.CLEANSTACK)", "response": "Checks if the solution of a VM is valid."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef recommended_fee_for_tx(tx):\n    s = io.BytesIO()\n    tx.stream(s)\n    tx_byte_count = len(s.getvalue())\n    tx_fee = TX_FEE_PER_THOUSAND_BYTES * ((999+tx_byte_count)//1000)\n    return tx_fee", "response": "Return the recommended fee for a given transaction."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef from_bin(class_, blob):\n        f = io.BytesIO(blob)\n        tx = class_.parse(f)\n        try:\n            tx.parse_unspents(f)\n        except Exception:\n            # parsing unspents failed\n            tx.unspents = []\n        return tx", "response": "Return the Tx for the given binary blob."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef as_bin(self, *args, **kwargs):\n        f = io.BytesIO()\n        self.stream(f, *args, **kwargs)\n        return f.getvalue()", "response": "Returns a binary blob containing the streamed transaction.\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the unspent inputs for a transaction.", "response": "def set_unspents(self, unspents):\n        \"\"\"\n        Set the unspent inputs for a transaction.\n\n        :param unspents: a list of :class:`TxOut` (or the subclass :class:`Spendable`) objects\n            corresponding to the :class:`TxIn` objects for this transaction (same number of\n            items in each list)\n        \"\"\"\n        if len(unspents) != len(self.txs_in):\n            raise ValueError(\"wrong number of unspents\")\n        self.unspents = unspents"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sign(self, *args, **kwargs):\n        self.Solver(self).sign(*args, **kwargs)\n        return self", "response": "Sign all transaction inputs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a count of how many : class : TxIn objects are not correctly solved.", "response": "def bad_solution_count(self, *args, **kwargs):\n        \"Return a count of how many :class:`TxIn` objects are not correctly solved.\"\n        return sum(0 if self.is_solution_ok(idx, *args, **kwargs) else 1 for idx in range(len(self.txs_in)))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a public pair to the internal sec binary format used by OpenSSL.", "response": "def public_pair_to_sec(public_pair, compressed=True):\n    \"\"\"Convert a public pair (a pair of bignums corresponding to a public key) to the\n    gross internal sec binary format used by OpenSSL.\"\"\"\n    x_str = to_bytes_32(public_pair[0])\n    if compressed:\n        return int2byte((2 + (public_pair[1] & 1))) + x_str\n    y_str = to_bytes_32(public_pair[1])\n    return b'\\4' + x_str + y_str"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert a public key in sec binary format to a public pair.", "response": "def sec_to_public_pair(sec, generator=None, strict=True):\n    \"\"\"Convert a public key in sec binary format to a public pair.\"\"\"\n    byte_count = (generator.p().bit_length() + 7) >> 3 if generator else (len(sec) - 1)\n    x = from_bytes_32(sec[1:1 + byte_count])\n    sec0 = sec[:1]\n    if len(sec) == 1 + byte_count * 2:\n        isok = sec0 == b'\\4'\n        if not strict:\n            isok = isok or (sec0 in [b'\\6', b'\\7'])\n        if isok:\n            y = from_bytes_32(sec[1+byte_count:1+2*byte_count])\n            return (x, y)\n    elif len(sec) == 1 + byte_count:\n        if not strict or (sec0 in (b'\\2', b'\\3')):\n            is_y_odd = (sec0 != b'\\2')\n            return generator.points_for_x(x)[is_y_odd]\n    raise EncodingError(\"bad sec encoding for public key\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef tx_for_tx_hash(self, tx_hash):\n        \"Get a Tx by its hash.\"\n        URL = self.api_domain + (\"/rawtx/%s?format=hex\" % b2h_rev(tx_hash))\n        tx = Tx.from_hex(urlopen(URL).read().decode(\"utf8\"))\n        return tx", "response": "Get a Tx by its hash."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef payments_for_address(self, address):\n        \"return an array of (TX ids, net_payment)\"\n        URL = self.api_domain + (\"/address/%s?format=json\" % address)\n        d = urlopen(URL).read()\n        json_response = json.loads(d.decode(\"utf8\"))\n        response = []\n        for tx in json_response.get(\"txs\", []):\n            total_out = 0\n            for tx_out in tx.get(\"out\", []):\n                if tx_out.get(\"addr\") == address:\n                    total_out += tx_out.get(\"value\", 0)\n            if total_out > 0:\n                response.append((tx.get(\"hash\"), total_out))\n        return response", "response": "return an array of ( TX ids net_payment )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of Spendable objects for the given bitcoin address.", "response": "def spendables_for_address(self, address):\n        \"\"\"\n        Return a list of Spendable objects for the\n        given bitcoin address.\n        \"\"\"\n        URL = self.api_domain + \"/unspent?active=%s\" % address\n        r = json.loads(urlopen(URL).read().decode(\"utf8\"))\n        spendables = []\n        for u in r[\"unspent_outputs\"]:\n            coin_value = u[\"value\"]\n            script = h2b(u[\"script\"])\n            previous_hash = h2b(u[\"tx_hash\"])\n            previous_index = u[\"tx_output_n\"]\n            spendables.append(Tx.Spendable(coin_value, script, previous_hash, previous_index))\n        return spendables"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef subkey_secret_exponent_chain_code_pair(\n        generator, secret_exponent, chain_code_bytes, i, is_hardened, public_pair=None):\n    \"\"\"\n    Yield info for a child node for this node.\n\n    generator:\n        the ecdsa generator\n    secret_exponent:\n        base secret exponent\n    chain_code:\n        base chain code\n    i:\n        the index for this node.\n    is_hardened:\n        use \"hardened key derivation\". The public version of this node cannot calculate this child.\n    public_pair:\n        the public_pair for the given secret exponent. If you leave it None, it's calculated for you\n        (but then it's slower)\n\n    Returns a pair (new_secret_exponent, new_chain_code)\n    \"\"\"\n    ORDER = generator.order()\n    i_as_bytes = struct.pack(\">L\", i)\n\n    if is_hardened:\n        data = b'\\0' + to_bytes_32(secret_exponent) + i_as_bytes\n    else:\n        if public_pair is None:\n            public_pair = secret_exponent * generator\n        sec = public_pair_to_sec(public_pair, compressed=True)\n        data = sec + i_as_bytes\n\n    I64 = hmac.HMAC(key=chain_code_bytes, msg=data, digestmod=hashlib.sha512).digest()\n    I_left_as_exponent = from_bytes_32(I64[:32]) % ORDER\n    new_secret_exponent = (I_left_as_exponent + secret_exponent) % ORDER\n    if new_secret_exponent == 0:\n        logger.critical(_SUBKEY_VALIDATION_LOG_ERR_FMT)\n        raise DerivationError('k_{} == 0'.format(i))\n\n    new_chain_code = I64[32:]\n    return new_secret_exponent, new_chain_code", "response": "Generate a pair of secret exponent and chain code for a given key."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef subkey_public_pair_chain_code_pair(generator, public_pair, chain_code_bytes, i):\n    INFINITY = generator.infinity()\n    ORDER = generator.order()\n    i_as_bytes = struct.pack(\">l\", i)\n    sec = public_pair_to_sec(public_pair, compressed=True)\n    data = sec + i_as_bytes\n\n    I64 = hmac.HMAC(key=chain_code_bytes, msg=data, digestmod=hashlib.sha512).digest()\n    I_left_as_exponent = from_bytes_32(I64[:32]) % ORDER\n    the_point = I_left_as_exponent * generator + generator.Point(*public_pair)\n    if the_point == INFINITY:\n        logger.critical(_SUBKEY_VALIDATION_LOG_ERR_FMT)\n        raise DerivationError('K_{} == {}'.format(i, the_point))\n\n    new_chain_code = I64[32:]\n    return the_point, new_chain_code", "response": "Generate a new public pair and chain code pair for this node."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of Spendable objects for the given bitcoin address.", "response": "def spendables_for_address(self, address):\n        \"\"\"\n        Return a list of Spendable objects for the\n        given bitcoin address.\n        \"\"\"\n        spendables = []\n        r = json.loads(urlopen(self.base_url('get_tx_unspent', address)).read().decode(\"utf8\"))\n\n        for u in r['data']['txs']:\n            coin_value = int(float(u['value']) * 100000000)\n            script = h2b(u[\"script_hex\"])\n            previous_hash = h2b_rev(u[\"txid\"])\n            previous_index = u[\"output_no\"]\n            spendables.append(Tx.Spendable(coin_value, script, previous_hash, previous_index))\n\n        return spendables"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a Tx by its hash.", "response": "def tx_for_tx_hash(self, tx_hash):\n        \"Get a Tx by its hash.\"\n        url = self.base_url(\"get_tx\", b2h_rev(tx_hash))\n        r = json.loads(urlopen(url).read().decode(\"utf8\"))\n        tx = Tx.parse(io.BytesIO(h2b(r.get(\"data\").get(\"tx_hex\"))))\n        return tx"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting a byte string to an integer.", "response": "def _from_bytes(bytes, byteorder=\"big\", signed=False):\n    \"\"\"This is the same functionality as ``int.from_bytes`` in python 3\"\"\"\n    return int.from_bytes(bytes, byteorder=byteorder, signed=signed)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_signed_message(class_, msg_in):\n\n        msg, hdr = class_.parse_sections(msg_in)\n\n        # after message, expect something like an email/http headers, so split into lines\n        hdr = list(filter(None, [i.strip() for i in hdr.split('\\n')]))\n\n        if '-----END' not in hdr[-1]:\n            raise EncodingError(\"expecting END on last line\")\n\n        sig = hdr[-2]\n        addr = None\n        for line in hdr:\n            line = line.strip()\n            if not line:\n                continue\n\n            if line.startswith('-----END'):\n                break\n\n            if ':' in line:\n                label, value = [i.strip() for i in line.split(':', 1)]\n\n                if label.lower() == 'address':\n                    addr = line.split(':')[1].strip()\n                    break\n\n                continue\n\n            addr = line\n            break\n\n        if not addr or addr == sig:\n            raise EncodingError(\"Could not find address\")\n\n        return msg, addr, sig", "response": "Takes an armoured message and split into the message body signing address\n        and base64 signature."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a signature encoded in Base64 of msg_hash.", "response": "def signature_for_message_hash(self, secret_exponent, msg_hash, is_compressed):\n        \"\"\"\n        Return a signature, encoded in Base64, of msg_hash.\n        \"\"\"\n        r, s, recid = self._generator.sign_with_recid(secret_exponent, msg_hash)\n\n        # See http://bitcoin.stackexchange.com/questions/14263 and key.cpp\n        # for discussion of the proprietary format used for the signature\n\n        first = 27 + recid + (4 if is_compressed else 0)\n        sig = b2a_base64(int2byte(first) + to_bytes_32(r) + to_bytes_32(s)).strip()\n        sig = sig.decode(\"utf8\")\n        return sig"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sign_message(self, key, message, verbose=False):\n        secret_exponent = key.secret_exponent()\n        if not secret_exponent:\n            raise ValueError(\"Private key is required to sign a message\")\n\n        addr = key.address()\n\n        msg_hash = self.hash_for_signing(message)\n        is_compressed = key.is_compressed()\n\n        sig = self.signature_for_message_hash(secret_exponent, msg_hash, is_compressed)\n\n        if not verbose or message is None:\n            return sig\n\n        return self.signature_template.format(\n            msg=message, sig=sig, addr=addr,\n            net_name=self._network_name.upper())", "response": "Signs a message using the private key."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngiving a message hash and a signature return the pair that was signed by the message hash.", "response": "def pair_for_message_hash(self, signature, msg_hash):\n        \"\"\"\n        Take a signature, encoded in Base64, and return the pair it was signed by.\n        May raise EncodingError (from _decode_signature)\n        \"\"\"\n\n        # Decode base64 and a bitmask in first byte.\n        is_compressed, recid, r, s = self._decode_signature(signature)\n\n        # Calculate the specific public key used to sign this message.\n        y_parity = recid & 1\n        q = self._generator.possible_public_pairs_for_signature(msg_hash, (r, s), y_parity=y_parity)[0]\n        if recid > 1:\n            order = self._generator.order()\n            q = self._generator.Point(q[0] + order, q[1])\n        return q, is_compressed"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nverify a message against a specific base58 - encoded public key or private key.", "response": "def verify_message(self, key_or_address, signature, message=None, msg_hash=None):\n        \"\"\"\n        Take a signature, encoded in Base64, and verify it against a\n        key object (which implies the public key),\n        or a specific base58-encoded pubkey hash.\n        \"\"\"\n        if isinstance(key_or_address, str):\n            # they gave us a private key or a public key already loaded.\n            key = self._network.parse.address(key_or_address)\n        else:\n            key = key_or_address\n\n        try:\n            msg_hash = self.hash_for_signing(message) if message is not None else msg_hash\n            pair, is_compressed = self.pair_for_message_hash(signature, msg_hash)\n        except EncodingError:\n            return False\n        return self.pair_matches_key(pair, key, is_compressed)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _decode_signature(self, signature):\n\n        sig = a2b_base64(signature)\n        if len(sig) != 65:\n            raise EncodingError(\"Wrong length, expected 65\")\n\n        # split into the parts.\n        first = byte2int(sig)\n        r = from_bytes_32(sig[1:33])\n        s = from_bytes_32(sig[33:33+32])\n\n        # first byte encodes a bits we need to know about the point used in signature\n        if not (27 <= first < 35):\n            raise EncodingError(\"First byte out of range\")\n\n        # NOTE: The first byte encodes the \"recovery id\", or \"recid\" which is a 3-bit values\n        # which selects compressed/not-compressed and one of 4 possible public pairs.\n        #\n        first -= 27\n        is_compressed = bool(first & 0x4)\n\n        return is_compressed, (first & 0x3), r, s", "response": "Decode the internal fields of the base64 - encoded signature."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a hash of msg according to odd bitcoin method double SHA256 over a bitcoin encoded stream of two strings a fixed magic prefix and the actual message.", "response": "def hash_for_signing(self, msg):\n        \"\"\"\n        Return a hash of msg, according to odd bitcoin method: double SHA256 over a bitcoin\n        encoded stream of two strings: a fixed magic prefix and the actual message.\n        \"\"\"\n        magic = self.msg_magic_for_netcode()\n\n        fd = io.BytesIO()\n        stream_satoshi_string(fd, magic.encode('utf8'))\n        stream_satoshi_string(fd, msg.encode('utf8'))\n\n        # return as a number, since it's an input to signing algos like that anyway\n        return from_bytes_32(double_sha256(fd.getvalue()))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntake a list of hashes and return the root merkle hash.", "response": "def merkle(hashes, hash_f=double_sha256):\n    \"\"\"Take a list of hashes, and return the root merkle hash.\"\"\"\n    while len(hashes) > 1:\n        hashes = merkle_pair(hashes, hash_f)\n    return hashes[0]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntake a list of hashes and return the parent row in the tree of merkle hashes.", "response": "def merkle_pair(hashes, hash_f):\n    \"\"\"Take a list of hashes, and return the parent row in the tree of merkle hashes.\"\"\"\n    if len(hashes) % 2 == 1:\n        hashes = list(hashes)\n        hashes.append(hashes[-1])\n    items = []\n    for i in range(0, len(hashes), 2):\n        items.append(hash_f(hashes[i] + hashes[i+1]))\n    return items"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating a deterministic k value for a given generator_order and a value.", "response": "def deterministic_generate_k(generator_order, secret_exponent, val, hash_f=hashlib.sha256):\n    \"\"\"\n    :param generator_order: result from :method:`pycoin.ecdsa.Generator.Generator.order`,\n        necessary to ensure the k value is within bound\n    :param secret_exponent: an integer secret_exponent to generate the k value for\n    :param val: the value to be signed, also used as an entropy source for the k value\n    :returns: an integer k such that ``1 <= k < generator_order``, complying with\n        <https://tools.ietf.org/html/rfc6979>\n    \"\"\"\n    n = generator_order\n    bln = bit_length(n)\n    order_size = (bln + 7) // 8\n    hash_size = hash_f().digest_size\n    v = b'\\x01' * hash_size\n    k = b'\\x00' * hash_size\n    priv = intstream.to_bytes(secret_exponent, length=order_size)\n    shift = 8 * hash_size - bln\n    if shift > 0:\n        val >>= shift\n    if val > n:\n        val -= n\n    h1 = intstream.to_bytes(val, length=order_size)\n    k = hmac.new(k, v + b'\\x00' + priv + h1, hash_f).digest()\n    v = hmac.new(k, v, hash_f).digest()\n    k = hmac.new(k, v + b'\\x01' + priv + h1, hash_f).digest()\n    v = hmac.new(k, v, hash_f).digest()\n\n    while 1:\n        t = bytearray()\n\n        while len(t) < order_size:\n            v = hmac.new(k, v, hash_f).digest()\n            t.extend(v)\n\n        k1 = intstream.from_bytes(bytes(t))\n\n        k1 >>= (len(t)*8 - bln)\n        if k1 >= 1 and k1 < n:\n            return k1\n\n        k = hmac.new(k, v + b'\\x00', hash_f).digest()\n        v = hmac.new(k, v, hash_f).digest()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates a Wallet from a master password.", "response": "def from_master_secret(class_, master_secret):\n        \"\"\"Generate a Wallet from a master password.\"\"\"\n        I64 = hmac.HMAC(key=b\"Bitcoin seed\", msg=master_secret, digestmod=hashlib.sha512).digest()\n        return class_(chain_code=I64[32:], secret_exponent=from_bytes_32(I64[:32]))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nserialize this node into a 74 - byte binary blob.", "response": "def serialize(self, as_private=None):\n        \"\"\"\n        Yield a 74-byte binary blob corresponding to this node.\n        You must add a 4-byte prefix before converting to base58.\n        \"\"\"\n        if as_private is None:\n            as_private = self.secret_exponent() is not None\n        if self.secret_exponent() is None and as_private:\n            raise PublicPrivateMismatchError(\"public key has no private parts\")\n\n        ba = bytearray()\n        ba.extend([self._depth])\n        ba.extend(self._parent_fingerprint + struct.pack(\">L\", self._child_index) + self._chain_code)\n        if as_private:\n            ba += b'\\0' + self._secret_exponent_bytes\n        else:\n            ba += self.sec(is_compressed=True)\n        return bytes(ba)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nyields a 111 - byte string corresponding to this node.", "response": "def hwif(self, as_private=False):\n        \"\"\"Yield a 111-byte string corresponding to this node.\"\"\"\n        return self._network.bip32_as_string(self.serialize(as_private=as_private), as_private=as_private)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nyielding the corresponding public node for this node.", "response": "def public_copy(self):\n        \"\"\"Yield the corresponding public node for this node.\"\"\"\n        d = dict(chain_code=self._chain_code, depth=self._depth,\n                 parent_fingerprint=self._parent_fingerprint,\n                 child_index=self._child_index, public_pair=self.public_pair())\n        return self.__class__(**d)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef subkey(self, i=0, is_hardened=False, as_private=None):\n        if as_private is None:\n            as_private = self.secret_exponent() is not None\n        is_hardened = not not is_hardened\n        as_private = not not as_private\n        lookup = (i, is_hardened, as_private)\n        if lookup not in self._subkey_cache:\n            self._subkey_cache[lookup] = self._subkey(i, is_hardened, as_private)\n        return self._subkey_cache[lookup]", "response": "Yield a child node for this node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the key object for the given path.", "response": "def subkey_for_path(self, path):\n        \"\"\"\n        path: a path of subkeys denoted by numbers and slashes. Use H or p\n            for private key derivation. End with .pub to force the key\n            public.\n\n        Examples: 1H/5/2/1 would call subkey(i=1, is_hardened=True)\n            .subkey(i=5).subkey(i=2).subkey(i=1) and then yield the\n            private key 0/0/458.pub would call subkey(i=0).subkey(i=0)\n            .subkey(i=458) and then yield the public key\n\n        You should choose one of the H or p convention for private key\n        derivation and stick with it.\n        \"\"\"\n        force_public = (path[-4:] == '.pub')\n        if force_public:\n            path = path[:-4]\n        key = self\n        if path:\n            invocations = path.split(\"/\")\n            for v in invocations:\n                is_hardened = v[-1] in (\"'pH\")\n                if is_hardened:\n                    v = v[:-1]\n                v = int(v)\n                key = key.subkey(i=v, is_hardened=is_hardened, as_private=key.secret_exponent() is not None)\n        if force_public and key.secret_exponent() is not None:\n            key = key.public_copy()\n        return key"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef crack_secret_exponent_from_k(generator, signed_value, sig, k):\n    r, s = sig\n    return ((s * k - signed_value) * generator.inverse(r)) % generator.order()", "response": "Given a signed value and a known k return the secret exponent."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef crack_k_from_sigs(generator, sig1, val1, sig2, val2):\n\n    # s1 = v1 / k1 + (se * r1) / k1\n    # s2 = v2 / k2 + (se * r2) / k2\n    # and k = k1 = k2\n    # so\n    # k * s1 = v1 + (se * r1)\n    # k * s2 = v2 + (se * r2)\n    # so\n    # k * s1 * r2 = r2 * v1 + (se * r1 * r2)\n    # k * s2 * r1 = r1 * v2 + (se * r2 * r1)\n    # so\n    # k (s1 * r2 - s2 * r1) = r2 * v1 - r1 * v2\n    # so\n    # k = (r2 * v1 - r1 * v2) / (s1 * r2 - s2 * r1)\n\n    r1, s1 = sig1\n    r2, s2 = sig2\n    if r1 != r2:\n        raise ValueError(\"r values of signature do not match\")\n    k = (r2 * val1 - r1 * val2) * generator.inverse(r2 * s1 - r1 * s2)\n    return k % generator.order()", "response": "Given two signatures with the same secret exponent and K value return that K value."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nyields a list of subpaths for a given path range.", "response": "def subpaths_for_path_range(path_range, hardening_chars=\"'pH\"):\n    \"\"\"\n    Return an iterator of paths\n\n        # examples:\n        #   0/1H/0-4 => ['0/1H/0', '0/1H/1', '0/1H/2', '0/1H/3', '0/1H/4']\n        #   0/2,5,9-11 => ['0/2', '0/5', '0/9', '0/10', '0/11']\n        #   3H/2/5/15-20p => ['3H/2/5/15p', '3H/2/5/16p', '3H/2/5/17p', '3H/2/5/18p',\n        #          '3H/2/5/19p', '3H/2/5/20p']\n        #   5-6/7-8p,15/1-2 => ['5/7H/1', '5/7H/2', '5/8H/1', '5/8H/2',\n        #         '5/15/1', '5/15/2', '6/7H/1', '6/7H/2', '6/8H/1', '6/8H/2', '6/15/1', '6/15/2']\n    \"\"\"\n    if path_range == '':\n        yield ''\n        return\n\n    def range_iterator(the_range):\n        for r in the_range.split(\",\"):\n            is_hardened = r[-1] in hardening_chars\n            hardened_char = hardening_chars[-1] if is_hardened else ''\n            if is_hardened:\n                r = r[:-1]\n            if '-' in r:\n                low, high = [int(x) for x in r.split(\"-\", 1)]\n                for t in range(low, high+1):\n                    yield \"%d%s\" % (t, hardened_char)\n            else:\n                yield \"%s%s\" % (r, hardened_char)\n\n    components = path_range.split(\"/\")\n    iterators = [range_iterator(c) for c in components]\n    for v in itertools.product(*iterators):\n        yield '/'.join(v)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing a bip32 private key from a seed.", "response": "def bip32_seed(self, s):\n        \"\"\"\n        Parse a bip32 private key from a seed.\n        Return a :class:`BIP32 <pycoin.key.BIP32Node.BIP32Node>` or None.\n        \"\"\"\n        pair = parse_colon_prefix(s)\n        if pair is None or pair[0] not in \"HP\":\n            return None\n        if pair[0] == \"H\":\n            try:\n                master_secret = h2b(pair[1])\n            except ValueError:\n                return None\n        else:\n            master_secret = pair[1].encode(\"utf8\")\n        return self._network.keys.bip32_seed(master_secret)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse a bip32 private key from a text string.", "response": "def bip32_prv(self, s):\n        \"\"\"\n        Parse a bip32 private key from a text string (\"xprv\" type).\n        Return a :class:`BIP32 <pycoin.key.BIP32Node.BIP32Node>` or None.\n        \"\"\"\n        data = self.parse_b58_hashed(s)\n        if data is None or not data.startswith(self._bip32_prv_prefix):\n            return None\n        return self._network.keys.bip32_deserialize(data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef bip32_pub(self, s):\n        data = self.parse_b58_hashed(s)\n        if data is None or not data.startswith(self._bip32_pub_prefix):\n            return None\n        return self._network.keys.bip32_deserialize(data)", "response": "Parse a bip32 public key from a text string."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef bip32(self, s):\n        s = parseable_str(s)\n        return self.bip32_prv(s) or self.bip32_pub(s)", "response": "Parse a bip32 public key from a text string either a seed a prv or a pub."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse an electrum key from a text string in seed form.", "response": "def electrum_seed(self, s):\n        \"\"\"\n        Parse an electrum key from a text string in seed form (\"E:xxx\" where xxx\n        is a 32-character hex string).\n        Return a :class:`ElectrumWallet <pycoin.key.electrum.ElectrumWallet>` or None.\n        \"\"\"\n        blob = self._electrum_to_blob(s)\n        if blob and len(blob) == 16:\n            blob = b2h(blob)\n            return self._network.keys.electrum_seed(seed=blob)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef electrum_prv(self, s):\n        blob = self._electrum_to_blob(s)\n        if blob and len(blob) == 32:\n            mpk = from_bytes_32(blob)\n            return self._network.keys.electrum_private(master_private_key=mpk)", "response": "Parse an electrum private key from a text string in seed form."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse an electrum public key from a text string in seed form.", "response": "def electrum_pub(self, s):\n        \"\"\"\n        Parse an electrum public key from a text string in seed form (\"E:xxx\" where xxx\n        is a 128-character hex string).\n        Return a :class:`ElectrumWallet <pycoin.key.electrum.ElectrumWallet>` or None.\n        \"\"\"\n        blob = self._electrum_to_blob(s)\n        if blob and len(blob) == 64:\n            return self._network.keys.electrum_public(master_public_key=blob)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses a pay - to - public - key - hash address. Return a Contract object.", "response": "def p2pkh(self, s):\n        \"\"\"\n        Parse a pay-to-public-key-hash address.\n        Return a :class:`Contract` or None.\n        \"\"\"\n        data = self.parse_b58_hashed(s)\n        if data is None or not data.startswith(self._address_prefix):\n            return None\n        size = len(self._address_prefix)\n        script = self._network.contract.for_p2pkh(data[size:])\n        script_info = self._network.contract.info_for_script(script)\n        return Contract(script_info, self._network)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse a pay - to - script - hash address. Return a Contract object.", "response": "def p2sh(self, s):\n        \"\"\"\n        Parse a pay-to-script-hash address.\n        Return a :class:`Contract` or None.\n        \"\"\"\n        data = self.parse_b58_hashed(s)\n        if (None in (data, self._pay_to_script_prefix) or\n                not data.startswith(self._pay_to_script_prefix)):\n            return None\n        size = len(self._pay_to_script_prefix)\n        script = self._network.contract.for_p2sh(data[size:])\n        script_info = self._network.contract.info_for_script(script)\n        return Contract(script_info, self._network)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses a script by compiling it. Return a Contract or None.", "response": "def script(self, s):\n        \"\"\"\n        Parse a script by compiling it.\n        Return a :class:`Contract` or None.\n        \"\"\"\n        try:\n            script = self._network.script.compile(s)\n            script_info = self._network.contract.info_for_script(script)\n            return Contract(script_info, self._network)\n        except Exception:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses a WIF. Return a : class : Key <pycoin. key. Key > or None", "response": "def wif(self, s):\n        \"\"\"\n        Parse a WIF.\n        Return a :class:`Key <pycoin.key.Key>` or None.\n        \"\"\"\n        data = self.parse_b58_hashed(s)\n        if data is None or not data.startswith(self._wif_prefix):\n            return None\n        data = data[len(self._wif_prefix):]\n        is_compressed = (len(data) > 32)\n        if is_compressed:\n            data = data[:-1]\n        se = from_bytes_32(data)\n        return self._network.keys.private(se, is_compressed=is_compressed)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef secret_exponent(self, s):\n        v = self.as_number(s)\n        if v:\n            try:\n                return self._network.keys.private(v)\n            except ValueError:\n                pass", "response": "Parse an integer secret exponent."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef public_pair(self, s):\n        point = None\n        Key = self._network.keys.private\n        # BRAIN DAMAGE\n        generator = Key(1)._generator\n        for c in \",/\":\n            if c in s:\n                s0, s1 = s.split(c, 1)\n                v0 = self.as_number(s0)\n                if v0:\n                    if s1 in (\"even\", \"odd\"):\n                        is_y_odd = (s1 == \"odd\")\n                        point = generator.points_for_x(v0)[is_y_odd]\n                    v1 = self.as_number(s1)\n                    if v1:\n                        if generator.contains_point(v0, v1):\n                            point = generator.Point(v0, v1)\n        if point:\n            return self._network.keys.public(point)", "response": "Parse a public pair X Y or X Y where X is a coordinate and Y is a coordinate and X is a coordinate and Y is a coordinate and Y is a coordinate."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses a public pair as a text SEC. Return a : class : Key <pycoin. key. Key > or None", "response": "def sec(self, s):\n        \"\"\"\n        Parse a public pair as a text SEC.\n        Return a :class:`Key <pycoin.key.Key>` or None.\n        \"\"\"\n        pair = parse_colon_prefix(s)\n        if pair is not None and pair[0] == self._wif_prefix:\n            s = pair[1]\n        try:\n            sec = h2b(s)\n            return self._network.keys.public(sec)\n        except Exception:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef address(self, s):\n        s = parseable_str(s)\n        return self.p2pkh(s) or self.p2sh(s) or self.p2pkh_segwit(s) or self.p2sh_segwit(s)", "response": "Parse an address any of p2pkh p2sh p2sh_segwit or p2sh_segwit."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse text as either an address or a script to be compiled.", "response": "def payable(self, s):\n        \"\"\"\n        Parse text as either an address or a script to be compiled.\n        Return a :class:`Contract <Contract>`, or None.\n        \"\"\"\n        s = parseable_str(s)\n        return self.address(s) or self.script(s)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse text as some kind of hierarchical key.", "response": "def hierarchical_key(self, s):\n        \"\"\"\n        Parse text as some kind of hierarchical key.\n        Return a subclass of :class:`Key <pycoin.key.Key>`, or None.\n        \"\"\"\n        s = parseable_str(s)\n        for f in [self.bip32_seed, self.bip32_prv, self.bip32_pub,\n                  self.electrum_seed, self.electrum_prv, self.electrum_pub]:\n            v = f(s)\n            if v:\n                return v"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses text as some kind of private key. It returns a subclass of : class : Key or None.", "response": "def private_key(self, s):\n        \"\"\"\n        Parse text as some kind of private key.\n        Return a subclass of :class:`Key <pycoin.key.Key>`, or None.\n        \"\"\"\n        s = parseable_str(s)\n        for f in [self.wif, self.secret_exponent]:\n            v = f(s)\n            if v:\n                return v"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing text either a private key or a private hierarchical key. Return a subclass of : class : Key or None.", "response": "def secret(self, s):\n        \"\"\"\n        Parse text either a private key or a private hierarchical key.\n        Return a subclass of :class:`Key <pycoin.key.Key>`, or None.\n        \"\"\"\n        s = parseable_str(s)\n        for f in [self.private_key, self.hierarchical_key]:\n            v = f(s)\n            if v:\n                return v"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef public_key(self, s):\n        s = parseable_str(s)\n        for f in [self.public_pair, self.sec]:\n            v = f(s)\n            if v:\n                return v", "response": "Parse text as either a public pair or an sec.\n        Return a subclass of : class : Key or None."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef spendables_for_address(address, netcode, format=None):\n    if format:\n        method = \"as_%s\" % format\n    for m in service_provider_methods(\"spendables_for_address\", get_default_providers_for_netcode(netcode)):\n        try:\n            spendables = m(address)\n            if format:\n                spendables = [getattr(s, method)() for s in spendables]\n            return spendables\n        except Exception:\n            pass\n    return []", "response": "Return a list of Spendable objects for the given bitcoin address."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of possible public key pairs for a given value and signature.", "response": "def possible_public_pairs_for_signature(self, value, signature, y_parity=None):\n        \"\"\"\n        :param: value: an integer value\n        :param: signature: an ``(r, s)`` pair of integers representing an ecdsa signature of ``value``\n        :param: y_parity: (optional) for a given value and signature, there are either two points\n            that sign it, or none if the signature is invalid. One of the points has an even y\n            value, the other an odd. If this parameter is set, only points whose y value matches\n            this value will be returned in the list.\n\n        :return: a list of :class:`Point <pycoin.ecdsa.Point.Point>` objects p where each p is\n            a possible public key for which ``signature`` correctly signs the given ``value``.\n            If something goes wrong, this list will be empty.\n        \"\"\"\n        r, s = signature\n\n        try:\n            points = self.points_for_x(r)\n        except ValueError:\n            return []\n\n        if y_parity is not None:\n            if y_parity & 1:\n                points = points[1:]\n            else:\n                points = points[:1]\n\n        inv_r = self.inverse(r)\n        s_over_r = s * inv_r\n        minus_E_over_r = -(inv_r * value) * self\n        try:\n            return [s_over_r * p + minus_E_over_r for p in points]\n        except ValueError:\n            return []"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmultiplying the internal table by an integer value.", "response": "def raw_mul(self, e):\n        \"\"\"\n        :param: e: an integer value\n        :returns: e * self\n\n        This method uses a precomputed table as an optimization.\n        \"\"\"\n        e %= self._order\n        P = self._infinity\n        for bit in range(256):\n            # add the power of the generator every time to make it more time-deterministic\n            a = [P, P + self._powers[bit]]\n            # choose the correct result\n            P = a[e & 1]\n            e >>= 1\n        return P"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nverify that the value is a valid signature for the curve.", "response": "def verify(self, public_pair, val, sig):\n        \"\"\"\n        :param: public_pair: a :class:`Point <pycoin.ecdsa.Point.Point>` on the curve\n        :param: val: an integer value\n        :param: sig: a pair of integers ``(r, s)`` representing an ecdsa signature\n\n        :returns: True if and only if the signature ``sig`` is a valid signature\n            of ``val`` using ``public_pair`` public key.\n        \"\"\"\n        order = self._order\n        r, s = sig\n        if r < 1 or r >= order or s < 1 or s >= order:\n            return False\n        s_inverse = self.inverse(s)\n        u1 = val * s_inverse\n        u2 = r * s_inverse\n        point = u1 * self + u2 * self.Point(*public_pair)\n        v = point[0] % order\n        return v == r"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsign a value with a specific recovery id", "response": "def sign_with_recid(self, secret_exponent, val, gen_k=None):\n        \"\"\"\n        :param: secret_exponent: a :class:`Point <pycoin.ecdsa.Point.Point>` on the curve\n        :param: val: an integer value\n        :param: gen_k: a function generating __k values__\n\n        :returns: a tuple of integers ``(r, s, recid)`` where ``(r, s)`` represents an ecdsa\n            signature of ``val`` with public key ``self * secret_exponent``; and ``recid``\n            is the **recovery id**, a number from 0-3 used to eliminate ambiguity about\n            which public key signed the value.\n\n        If gen_k is set, it will be called with (n, secret_exponent, val), and an unguessable\n        K value should be returned. Otherwise, the default K value, generated according\n        to rfc6979 will be used.\n        \"\"\"\n        if gen_k is None:\n            gen_k = deterministic_generate_k\n        n = self._order\n        k = gen_k(n, secret_exponent, val)\n        while True:\n            p1 = k * self\n            r = p1[0]\n            s = (self.inverse(k) * (val + (secret_exponent * r) % n)) % n\n            if r != 0 and s != 0:\n                recid = p1[1] & 1\n                if p1[1] > self._p:\n                    recid += 2\n                return r, s, recid\n            k += 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sign(self, secret_exponent, val, gen_k=None):\n        return self.sign_with_recid(secret_exponent, val, gen_k)[0:2]", "response": "signs a value with a secret exponent"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmultiply a point by an integer.", "response": "def multiply(self, p, e):\n        \"\"\"Multiply a point by an integer.\"\"\"\n        e %= self.order()\n        if p == self._infinity or e == 0:\n            return self._infinity\n        pubkey = create_string_buffer(64)\n        public_pair_bytes = b'\\4' + to_bytes_32(p[0]) + to_bytes_32(p[1])\n        r = libsecp256k1.secp256k1_ec_pubkey_parse(\n            libsecp256k1.ctx, pubkey, public_pair_bytes, len(public_pair_bytes))\n        if not r:\n            return False\n        r = libsecp256k1.secp256k1_ec_pubkey_tweak_mul(libsecp256k1.ctx, pubkey, to_bytes_32(e))\n        if not r:\n            return self._infinity\n\n        pubkey_serialized = create_string_buffer(65)\n        pubkey_size = c_size_t(65)\n        libsecp256k1.secp256k1_ec_pubkey_serialize(\n            libsecp256k1.ctx, pubkey_serialized, byref(pubkey_size), pubkey, SECP256K1_EC_UNCOMPRESSED)\n        x = from_bytes_32(pubkey_serialized[1:33])\n        y = from_bytes_32(pubkey_serialized[33:])\n        return self.Point(x, y)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef make_const_handler(data):\n    data = bytes_as_hex(data)\n\n    def constant_data_opcode_handler(script, pc, verify_minimal_data=False):\n        return pc+1, data\n    return constant_data_opcode_handler", "response": "Create a handler for a data opcode that returns a constant."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_sized_handler(size, const_values, non_minimal_data_handler):\n    const_values = list(const_values)\n\n    def constant_size_opcode_handler(script, pc, verify_minimal_data=False):\n        pc += 1\n        data = bytes_as_hex(script[pc:pc+size])\n        if len(data) < size:\n            return pc+1, None\n        if verify_minimal_data and data in const_values:\n            non_minimal_data_handler(\"not minimal push of %s\" % repr(data))\n        return pc+size, data\n    return constant_size_opcode_handler", "response": "Create a handler for a data opcode that returns a fixed size."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make_variable_handler(dec_f, sized_values, min_size, non_minimal_data_handler):\n    sized_values = list(sized_values)\n\n    def f(script, pc, verify_minimal_data=False):\n        size, pc = dec_f(script, pc)\n        data = bytes_as_hex(script[pc:pc+size])\n        if len(data) < size:\n            return pc+1, None\n        if verify_minimal_data:\n            if size in sized_values or size <= min_size:\n                non_minimal_data_handler(\"not minimal push of data with size %d\" % size)\n        return pc+size, data\n    return f", "response": "Create a handler for a data opcode that returns literal data of a variable size."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngiving a script and PC return the opcode data pc and whether it represents data.", "response": "def get_opcode(self, script, pc, verify_minimal_data=False):\n        \"\"\"\n        Step through the script, returning a tuple with the next opcode, the next\n        piece of data (if the opcode represents data), the new PC, and a boolean indicated\n        valid parsing.\n        \"\"\"\n        opcode = indexbytes(script, pc)\n        decoder = self.decoder.get(opcode)\n        # lambda s, p, verify_minimal_data: (p+1, None))\n        if decoder:\n            pc, data = decoder(script, pc, verify_minimal_data=verify_minimal_data)\n            is_ok = (data is not None)\n        else:\n            pc += 1\n            data = None\n            is_ok = True\n        return opcode, data, pc, is_ok"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ascend_bip32(bip32_pub_node, secret_exponent, child):\n    i_as_bytes = struct.pack(\">l\", child)\n    sec = public_pair_to_sec(bip32_pub_node.public_pair(), compressed=True)\n    data = sec + i_as_bytes\n    I64 = hmac.HMAC(key=bip32_pub_node._chain_code, msg=data, digestmod=hashlib.sha512).digest()\n    I_left_as_exponent = from_bytes_32(I64[:32])\n    return (secret_exponent - I_left_as_exponent) % bip32_pub_node._generator.order()", "response": "Given a BIP32Node with public derivation child return the secret exponent for the child."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef from_sec(class_, sec):\n        public_pair = sec_to_public_pair(sec, class_._generator)\n        return class_(public_pair=public_pair, is_compressed=is_sec_compressed(sec))", "response": "Create a key from an sec bytestream."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the WIF representation of this key.", "response": "def wif(self, is_compressed=None):\n        \"\"\"\n        Return the WIF representation of this key, if available.\n        \"\"\"\n        secret_exponent = self.secret_exponent()\n        if secret_exponent is None:\n            return None\n        if is_compressed is None:\n            is_compressed = self.is_compressed()\n        blob = to_bytes_32(secret_exponent)\n        if is_compressed:\n            blob += b'\\01'\n        return self._network.wif_for_blob(blob)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sec(self, is_compressed=None):\n        if is_compressed is None:\n            is_compressed = self.is_compressed()\n        public_pair = self.public_pair()\n        if public_pair is None:\n            return None\n        return public_pair_to_sec(public_pair, compressed=is_compressed)", "response": "Return the SEC representation of this key if available."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the SEC representation of this key as hex text.", "response": "def sec_as_hex(self, is_compressed=None):\n        \"\"\"\n        Return the SEC representation of this key as hex text.\n        \"\"\"\n        sec = self.sec(is_compressed=is_compressed)\n        return self._network.sec_text_for_blob(sec)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the hash160 representation of this key if available.", "response": "def hash160(self, is_compressed=None):\n        \"\"\"\n        Return the hash160 representation of this key, if available.\n        \"\"\"\n        if is_compressed is None:\n            is_compressed = self.is_compressed()\n        if is_compressed:\n            if self._hash160_compressed is None:\n                self._hash160_compressed = hash160(self.sec(is_compressed=is_compressed))\n            return self._hash160_compressed\n\n        if self._hash160_uncompressed is None:\n            self._hash160_uncompressed = hash160(self.sec(is_compressed=is_compressed))\n        return self._hash160_uncompressed"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef address(self, is_compressed=None):\n        return self._network.address.for_p2pkh(self.hash160(is_compressed=is_compressed))", "response": "Returns the public address representation of this key if available."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a textual representation of this key.", "response": "def as_text(self):\n        \"\"\"\n        Return a textual representation of this key.\n        \"\"\"\n        if self.secret_exponent():\n            return self.wif()\n        sec_hex = self.sec_as_hex()\n        if sec_hex:\n            return sec_hex\n        return self.address()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sign(self, h):\n        if not self.is_private():\n            raise RuntimeError(\"Key must be private to be able to sign\")\n        val = from_bytes_32(h)\n        r, s = self._generator.sign(self.secret_exponent(), val)\n        return sigencode_der(r, s)", "response": "Sign a hash h."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nverifying a signature for a hash h using this key.", "response": "def verify(self, h, sig):\n        \"\"\"\n        Return whether a signature is valid for hash h using this key.\n        \"\"\"\n        val = from_bytes_32(h)\n        pubkey = self.public_pair()\n        return self._generator.verify(pubkey, val, sigdecode_der(sig))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_tx(network, spendables, payables, fee=\"standard\", lock_time=0, version=1):\n\n    Tx = network.tx\n\n    def _fix_spendable(s):\n        if isinstance(s, Tx.Spendable):\n            return s\n        if not hasattr(s, \"keys\"):\n            return Tx.Spendable.from_text(s)\n        return Tx.Spendable.from_dict(s)\n\n    spendables = [_fix_spendable(s) for s in spendables]\n    txs_in = [spendable.tx_in() for spendable in spendables]\n\n    txs_out = []\n    for payable in payables:\n        if len(payable) == 2:\n            address, coin_value = payable\n        else:\n            address = payable\n            coin_value = 0\n        script = network.contract.for_address(address)\n        txs_out.append(Tx.TxOut(coin_value, script))\n\n    tx = Tx(version=version, txs_in=txs_in, txs_out=txs_out, lock_time=lock_time)\n    tx.set_unspents(spendables)\n\n    distribute_from_split_pool(tx, fee)\n    return tx", "response": "This function creates a unsigned transaction from the given list of spendables and payables."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef distribute_from_split_pool(tx, fee):\n\n    # calculate fees\n    if fee == 'standard':\n        # TODO: improve this\n        # 1: the tx is not fully built out, so it will actually be larger than implied at this point\n        # 2: recommended_fee_for_tx gives estimates that are too high\n        fee = tx_fee.recommended_fee_for_tx(tx)\n\n    zero_txs_out = [tx_out for tx_out in tx.txs_out if tx_out.coin_value == 0]\n    zero_count = len(zero_txs_out)\n    if zero_count > 0:\n        total_coin_value = sum(spendable.coin_value for spendable in tx.unspents)\n        coins_allocated = sum(tx_out.coin_value for tx_out in tx.txs_out) + fee\n        remaining_coins = total_coin_value - coins_allocated\n        if remaining_coins < 0:\n            raise ValueError(\"insufficient inputs for outputs\")\n        if remaining_coins < zero_count:\n            raise ValueError(\"not enough to pay nonzero amounts to at least one of the unspecified outputs\")\n        for value, tx_out in zip(split_with_remainder(remaining_coins, zero_count), zero_txs_out):\n            tx_out.coin_value = value\n    return zero_count", "response": "Distribute amount of TxOut items into a split pool."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sign_tx(network, tx, wifs=[], **kwargs):\n    keychain = network.keychain()\n    keychain.add_secrets((network.parse.wif(_) for _ in wifs))\n    solver = tx.Solver(tx)\n    solver.sign(keychain, **kwargs)", "response": ":param tx: a transaction\n    :param wifs: the list of WIFs required to sign this transaction.\n    :return: :class:`Tx <Tx>` object, modified in place\n\n    This is a convenience function used to sign a transaction.\n    The transaction must have \"unspents\" set by, for example, calling tx.unspents_from_db.\n\n    Returns the signed Tx transaction, or raises an exception.\n\n    Usage::\n\n        >> sign_tx(network, tx, wifs=[\"KwDiBf89QgGbjEhKnhXJuH7LrciVrZi3qYjgd9M7rFU73sVHnoWn\"])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef contains_point(self, x, y):\n        if x is None and y is None:\n            return True\n        return (y * y - (x * x * x + self._a * x + self._b)) % self._p == 0", "response": "Returns True if the point x y is on the curve False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add(self, p0, p1):\n        p = self._p\n        infinity = self._infinity\n\n        if p0 == infinity:\n            return p1\n        if p1 == infinity:\n            return p0\n\n        x0, y0 = p0\n        x1, y1 = p1\n        if (x0 - x1) % p == 0:\n            if (y0 + y1) % p == 0:\n                return infinity\n            else:\n                slope = ((3 * x0 * x0 + self._a) * self.inverse_mod(2 * y0, p)) % p\n        else:\n            slope = ((y1 - y0) * self.inverse_mod(x1 - x0, p)) % p\n\n        x3 = (slope * slope - x0 - x1) % p\n        y3 = (slope * (x0 - x3) - y0) % p\n\n        return self.Point(x3, y3)", "response": "returns the sum of the two points\n           "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef multiply(self, p, e):\n\n        if self._order:\n            e %= self._order\n        if p == self._infinity or e == 0:\n            return self._infinity\n\n        e3 = 3 * e\n        i = _leftmost_bit(e3) >> 1\n        result = p\n        while i > 1:\n            result += result\n            if (e3 & i):\n                v = [result, result+p]\n            else:\n                v = [result-p, result]\n            result = v[0 if (e & i) else 1]\n            i >>= 1\n\n        return result", "response": "multiplies a point by an integer."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndo an OP_EQUAL operation on the sequence of bytes.", "response": "def do_OP_EQUAL(vm):\n    \"\"\"\n    >>> s = [b'string1', b'string1']\n    >>> do_OP_EQUAL(s)\n    >>> print(s == [b'\\1'])\n    True\n    >>> s = [b'string1', b'string2']\n    >>> do_OP_EQUAL(s)\n    >>> print(s == [b''])\n    True\n    \"\"\"\n    v1, v2 = [vm.pop() for i in range(2)]\n    vm.append(vm.bool_to_script_bytes(v1 == v2))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef do_OP_WITHIN(vm):\n    v3, v2, v1 = [vm.pop_int() for i in range(3)]\n    ok = (v2 <= v1 < v3)\n    vm.append(vm.bool_to_script_bytes(ok))", "response": "This function is used to check if a sequence of bytes is in the sequence of bytes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn True if the constraint matches the given constraint.", "response": "def constraint_matches(self, c, m):\n        \"\"\"\n        Return dict noting the substitution values (or False for no match)\n        \"\"\"\n        if isinstance(m, tuple):\n            d = {}\n            if isinstance(c, Operator) and c._op_name == m[0]:\n                for c1, m1 in zip(c._args, m[1:]):\n                    r = self.constraint_matches(c1, m1)\n                    if r is False:\n                        return r\n                    d.update(r)\n                return d\n            return False\n        return m.match(c)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse(class_, f, include_transactions=True, include_offsets=None, check_merkle_hash=True):\n        block = class_.parse_as_header(f)\n        if include_transactions:\n            count = parse_struct(\"I\", f)[0]\n            txs = block._parse_transactions(f, count, include_offsets=include_offsets)\n            block.set_txs(txs, check_merkle_hash=check_merkle_hash)\n        return block", "response": "Parse the Block from the file - like object f."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_as_header(class_, f):\n        (version, previous_block_hash, merkle_root, timestamp,\n            difficulty, nonce) = parse_struct(\"L##LLL\", f)\n        return class_(version, previous_block_hash, merkle_root, timestamp, difficulty, nonce)", "response": "Parse the Block Header from the file - like object f."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncalculate the hash for the block header.", "response": "def hash(self):\n        \"\"\"Calculate the hash for the block header. Note that this has the bytes\n        in the opposite order from how the header is usually displayed (so the\n        long string of 00 bytes is at the end, not the beginning).\"\"\"\n        if not hasattr(self, \"__hash\"):\n            self.__hash = self._calculate_hash()\n        return self.__hash"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef stream_header(self, f):\n        stream_struct(\"L##LLL\", f, self.version, self.previous_block_hash,\n                      self.merkle_root, self.timestamp, self.difficulty, self.nonce)", "response": "Stream the block header in the standard way to the file - like object f."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef as_bin(self):\n        f = io.BytesIO()\n        self.stream(f)\n        return f.getvalue()", "response": "Return the block or header as binary."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nraises a BadMerkleRootError if the Merkle hash of the transactions does not match the merkle hash included in the block.", "response": "def check_merkle_hash(self):\n        \"\"\"Raise a BadMerkleRootError if the Merkle hash of the\n        transactions does not match the Merkle hash included in the block.\"\"\"\n        calculated_hash = merkle([tx.hash() for tx in self.txs], double_sha256)\n        if calculated_hash != self.merkle_root:\n            raise BadMerkleRootError(\n                \"calculated %s but block contains %s\" % (b2h(calculated_hash), b2h(self.merkle_root)))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef output(self):\n        hash160 = self._script_info.get(\"hash160\", None)\n        if hash160:\n            yield (\"hash160\", b2h(hash160), None)\n\n        address = self.address()\n        yield (\"address\", address, \"%s address\" % self._network.network_name)\n        yield (\"%s_address\" % self._network.symbol, address, \"legacy\")", "response": "Yield the output of the script."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef initial_key_to_master_key(initial_key):\n    b = initial_key.encode(\"utf8\")\n    orig_input = b\n    for i in range(100000):\n        b = hashlib.sha256(b + orig_input).digest()\n    return from_bytes_32(b)", "response": "This function takes an initial key and returns a master key"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef subkey(self, path):\n        t = path.split(\"/\")\n        if len(t) == 2:\n            n, for_change = t\n        else:\n            n, = t\n            for_change = 0\n        b = (str(n) + ':' + str(for_change) + ':').encode(\"utf8\") + self.master_public_key()\n        offset = from_bytes_32(double_sha256(b))\n        if self.secret_exponent():\n            return self.__class__(\n                master_private_key=((self.master_private_key() + offset) % self._generator.order())\n            )\n        p1 = offset * self._generator\n        x, y = self.public_pair()\n        p2 = self._generator.Point(x, y)\n        p = p1 + p2\n        return self.__class__(public_pair=p)", "response": "Return a new object with the master private key and the subkey of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef subkeys(self, path):\n        for _ in subpaths_for_path_range(path, hardening_chars=\"'pH\"):\n            yield self.subkey(_)", "response": "A generalized form that can return multiple subkeys."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the public key as sec or None in case of failure.", "response": "def public_key_sec(self):\n        \"\"\"Return the public key as sec, or None in case of failure.\"\"\"\n        if self.is_coinbase():\n            return None\n        opcodes = ScriptTools.opcode_list(self.script)\n        if len(opcodes) == 2 and opcodes[0].startswith(\"[30\"):\n            # the second opcode is probably the public key as sec\n            sec = h2b(opcodes[1][1:-1])\n            return sec\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if the point is actually on the curve", "response": "def check_on_curve(self):\n        \"\"\"raise :class:`NoSuchPointError` if the point is not actually on the curve.\"\"\"\n        if not self._curve.contains_point(*self):\n            raise NoSuchPointError('({},{}) is not on the curve {}'.format(self[0], self[1], self._curve))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates the special first in block transaction that includes the mining fees.", "response": "def coinbase_tx(cls, public_key_sec, coin_value, coinbase_bytes=b'', version=1, lock_time=0):\n        \"\"\"Create the special \"first in block\" transaction that includes the mining fees.\"\"\"\n        tx_in = cls.TxIn.coinbase_tx_in(script=coinbase_bytes)\n        COINBASE_SCRIPT_OUT = \"%s OP_CHECKSIG\"\n        script_text = COINBASE_SCRIPT_OUT % b2h(public_key_sec)\n        script_bin = BitcoinScriptTools.compile(script_text)\n        tx_out = cls.TxOut(coin_value, script_bin)\n        return cls(version, [tx_in], [tx_out], lock_time)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse(class_, f, allow_segwit=None):\n        if allow_segwit is None:\n            allow_segwit = class_.ALLOW_SEGWIT\n        txs_in = []\n        txs_out = []\n        version, = parse_struct(\"L\", f)\n        v1 = ord(f.read(1))\n        is_segwit = allow_segwit and (v1 == 0)\n        v2 = None\n        if is_segwit:\n            flag = f.read(1)\n            if flag == b'\\0':\n                raise ValueError(\"bad flag in segwit\")\n            if flag == b'\\1':\n                v1 = None\n            else:\n                is_segwit = False\n                v2 = ord(flag)\n        count = parse_satoshi_int(f, v=v1)\n        txs_in = []\n        for i in range(count):\n            txs_in.append(class_.TxIn.parse(f))\n        count = parse_satoshi_int(f, v=v2)\n        txs_out = []\n        for i in range(count):\n            txs_out.append(class_.TxOut.parse(f))\n\n        if is_segwit:\n            for tx_in in txs_in:\n                stack = []\n                count = parse_satoshi_int(f)\n                for i in range(count):\n                    stack.append(parse_satoshi_string(f))\n                tx_in.witness = stack\n        lock_time, = parse_struct(\"L\", f)\n        return class_(version, txs_in, txs_out, lock_time)", "response": "Parse a Bitcoin transaction Tx."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nstreaming a Bitcoin transaction to the file - like object f.", "response": "def stream(self, f, blank_solutions=False, include_unspents=False, include_witness_data=True):\n        \"\"\"Stream a Bitcoin transaction Tx to the file-like object f.\n\n        :param f: writable file-like object to stream binary data of transaction\n        :param blank_solutions: (optional) clear out the solutions scripts, effectively \"unsigning\" the\n            transaction before writing it. Defaults to False\n        :param include_unspents: (optional) stread out the Spendable objects after streaming the transaction.\n            This is a pycoin-specific extension. Defaults to False.\n        :param include_witness_data: (optional) stream segwit transactions including the witness data if the\n            transaction has any witness data. Defaults to True.\n        \"\"\"\n        include_witnesses = include_witness_data and self.has_witness_data()\n        stream_struct(\"L\", f, self.version)\n        if include_witnesses:\n            f.write(b'\\0\\1')\n        stream_struct(\"I\", f, len(self.txs_in))\n        for t in self.txs_in:\n            t.stream(f, blank_solutions=blank_solutions)\n        stream_struct(\"I\", f, len(self.txs_out))\n        for t in self.txs_out:\n            t.stream(f)\n        if include_witnesses:\n            for tx_in in self.txs_in:\n                witness = tx_in.witness\n                stream_struct(\"I\", f, len(witness))\n                for w in witness:\n                    stream_satoshi_string(f, w)\n        stream_struct(\"L\", f, self.lock_time)\n        if include_unspents and not self.missing_unspents():\n            self.stream_unspents(f)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_witness(self, tx_idx_in, witness):\n        self.txs_in[tx_idx_in].witness = tuple(witness)", "response": "Set the witness data for a given TxIn."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check(self):\n        self._check_tx_inout_count()\n        self._check_txs_out()\n        self._check_txs_in()\n        # Size limits\n        self._check_size_limit()", "response": "Basic checks that don t depend on any context."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef validate_unspents(self, tx_db):\n        tx_hashes = set((tx_in.previous_hash for tx_in in self.txs_in))\n\n        # build a local copy of the DB\n        tx_lookup = {}\n        for h in tx_hashes:\n            if h == ZERO32:\n                continue\n            the_tx = tx_db.get(h)\n            if the_tx is None:\n                raise KeyError(\"hash id %s not in tx_db\" % b2h_rev(h))\n            if the_tx.hash() != h:\n                raise KeyError(\"attempt to load Tx %s yielded a Tx with id %s\" % (h2b_rev(h), the_tx.id()))\n            tx_lookup[h] = the_tx\n\n        for idx, tx_in in enumerate(self.txs_in):\n            if tx_in.previous_hash == ZERO32:\n                continue\n            txs_out = tx_lookup[tx_in.previous_hash].txs_out\n            if tx_in.previous_index > len(txs_out):\n                raise BadSpendableError(\"tx_out index %d is too big for Tx %s\" %\n                                        (tx_in.previous_index, b2h_rev(tx_in.previous_hash)))\n            tx_out1 = txs_out[tx_in.previous_index]\n            tx_out2 = self.unspents[idx]\n            if tx_out1.coin_value != tx_out2.coin_value:\n                raise BadSpendableError(\n                    \"unspents[%d] coin value mismatch (%d vs %d)\" % (\n                        idx, tx_out1.coin_value, tx_out2.coin_value))\n            if tx_out1.script != tx_out2.script:\n                raise BadSpendableError(\"unspents[%d] script mismatch!\" % idx)\n\n        return self.fee()", "response": "Validate that all unspent transactions are set by the user."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef spendables_for_address(self, address):\n        URL = \"%s/addr/%s/utxo\" % (self.base_url, address)\n        r = json.loads(urlopen(URL).read().decode(\"utf8\"))\n        spendables = []\n        for u in r:\n            coin_value = btc_to_satoshi(str(u.get(\"amount\")))\n            script = h2b(u.get(\"scriptPubKey\"))\n            previous_hash = h2b_rev(u.get(\"txid\"))\n            previous_index = u.get(\"vout\")\n            spendables.append(Tx.Spendable(coin_value, script, previous_hash, previous_index))\n        return spendables", "response": "Return a list of Spendable objects for the bitcoin address."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef post_unpack_merkleblock(d, f):\n    level_widths = []\n    count = d[\"total_transactions\"]\n    while count > 1:\n        level_widths.append(count)\n        count += 1\n        count //= 2\n    level_widths.append(1)\n    level_widths.reverse()\n\n    tx_acc = []\n    flags = d[\"flags\"]\n    hashes = list(reversed(d[\"hashes\"]))\n    left_hash, flag_index = _recurse(level_widths, 0, 0, hashes, flags, 0, tx_acc)\n\n    if len(hashes) > 0:\n        raise ValueError(\"extra hashes: %s\" % hashes)\n\n    idx, r = divmod(flag_index-1, 8)\n    if idx != len(flags) - 1:\n        raise ValueError(\"not enough flags consumed\")\n\n    if flags[idx] > (1 << (r+1))-1:\n        raise ValueError(\"unconsumed 1 flag bits set\")\n\n    if left_hash != d[\"header\"].merkle_root:\n        raise ValueError(\n            \"merkle root %s does not match calculated hash %s\" % (\n                b2h_rev(d[\"header\"].merkle_root), b2h_rev(left_hash)))\n\n    d[\"tx_hashes\"] = tx_acc\n    return d", "response": "A post - processing post - unpack to merkleblock messages."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _make_parser(streamer, the_struct):\n    \"Return a function that parses the given structure into a dict\"\n    struct_items = [s.split(\":\") for s in the_struct.split()]\n    names = [s[0] for s in struct_items]\n    types = ''.join(s[1] for s in struct_items)\n\n    def f(message_stream):\n        return streamer.parse_as_dict(names, types, message_stream)\n    return f", "response": "Return a function that parses the given structure into a dict"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make_post_unpack_alert(streamer):\n    the_struct = (\"version:L relayUntil:Q expiration:Q id:L cancel:L setCancel:[L] minVer:L \"\n                  \"maxVer:L setSubVer:[S] priority:L comment:S statusBar:S reserved:S\")\n\n    alert_submessage_parser = _make_parser(streamer, the_struct)\n\n    def post_unpack_alert(d, f):\n        d1 = alert_submessage_parser(io.BytesIO(d[\"payload\"]))\n        d[\"alert_info\"] = d1\n        return d\n    return post_unpack_alert", "response": "Create a post - processor to unpack an alert message."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the standard parsing functions for a given Block and Tx class.", "response": "def standard_parsing_functions(Block, Tx):\n    \"\"\"\n    Return the standard parsing functions for a given Block and Tx class.\n    The return value is expected to be used with the standard_streamer function.\n    \"\"\"\n    def stream_block(f, block):\n        assert isinstance(block, Block)\n        block.stream(f)\n\n    def stream_blockheader(f, blockheader):\n        assert isinstance(blockheader, Block)\n        blockheader.stream_header(f)\n\n    def stream_tx(f, tx):\n        assert isinstance(tx, Tx)\n        tx.stream(f)\n\n    def parse_int_6(f):\n        b = f.read(6) + b'\\0\\0'\n        return struct.unpack(b, \"<L\")[0]\n\n    def stream_int_6(f, v):\n        f.write(struct.pack(v, \"<L\")[:6])\n\n    more_parsing = [\n        (\"A\", (PeerAddress.parse, lambda f, peer_addr: peer_addr.stream(f))),\n        (\"v\", (InvItem.parse, lambda f, inv_item: inv_item.stream(f))),\n        (\"T\", (Tx.parse, stream_tx)),\n        (\"B\", (Block.parse, stream_block)),\n        (\"z\", (Block.parse_as_header, stream_blockheader)),\n        (\"1\", (lambda f: struct.unpack(\"B\", f.read(1))[0], lambda f, v: f.write(struct.pack(\"B\", v)))),\n        (\"6\", (parse_int_6, stream_int_6)),\n        (\"O\", (lambda f: True if f.read(1) else False,\n               lambda f, v: f.write(b'' if v is None else struct.pack(\"B\", v)))),\n    ]\n    all_items = list(STREAMER_FUNCTIONS.items())\n    all_items.extend(more_parsing)\n    return all_items"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a streamer that parses and packs using the bitcoin protocol .", "response": "def standard_streamer(parsing_functions, parse_satoshi_int=parse_satoshi_int):\n    \"\"\"\n    Create a satoshi_streamer, which parses and packs using the bitcoin protocol\n    (mostly the custom way arrays and integers are parsed and packed).\n    \"\"\"\n    streamer = Streamer()\n    streamer.register_array_count_parse(parse_satoshi_int)\n    streamer.register_functions(parsing_functions)\n    return streamer"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef make_parser_and_packer(streamer, message_dict, message_post_unpacks):\n    message_parsers = dict((k, _make_parser(streamer, v)) for k, v in message_dict.items())\n\n    def parse_from_data(message_name, data):\n        message_stream = io.BytesIO(data)\n        parser = message_parsers.get(message_name)\n        if parser is None:\n            raise KeyError(\"unknown message: %s\" % message_name)\n        d = parser(message_stream)\n        post_unpack = message_post_unpacks.get(message_name)\n        if post_unpack:\n            d = post_unpack(d, message_stream)\n        return d\n\n    def pack_from_data(message_name, **kwargs):\n        the_struct = message_dict[message_name]\n        if not the_struct:\n            return b''\n        f = io.BytesIO()\n        the_fields = the_struct.split(\" \")\n        pairs = [t.split(\":\") for t in the_fields]\n        for name, type in pairs:\n            if type[0] == '[':\n                streamer.stream_struct(\"I\", f, len(kwargs[name]))\n                for v in kwargs[name]:\n                    if not isinstance(v, (tuple, list)):\n                        v = [v]\n                    streamer.stream_struct(type[1:-1], f, *v)\n            else:\n                streamer.stream_struct(type, f, kwargs[name])\n        return f.getvalue()\n\n    return parse_from_data, pack_from_data", "response": "Create a parser and packer for a peer s network messages."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef compile(self, s):\n        f = io.BytesIO()\n        for t in s.split():\n            t_up = t.upper()\n            if t_up in self.opcode_to_int:\n                f.write(int2byte(self.opcode_to_int[t]))\n            elif (\"OP_%s\" % t_up) in self.opcode_to_int:\n                f.write(int2byte(self.opcode_to_int[\"OP_%s\" % t]))\n            elif t_up.startswith(\"0X\"):\n                d = binascii.unhexlify(t[2:])\n                f.write(d)\n            else:\n                v = self.compile_expression(t)\n                self.write_push_data([v], f)\n        return f.getvalue()", "response": "Compiles the given script. Returns a bytes object with the compiled script."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_opcodes(self, script, verify_minimal_data=False, pc=0):\n        while pc < len(script):\n            opcode, data, new_pc, is_ok = self.scriptStreamer.get_opcode(\n                script, pc, verify_minimal_data=verify_minimal_data)\n            yield opcode, data, pc, new_pc\n            pc = new_pc", "response": "Iterator. Return opcode data pc new_pc at each step."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef opcode_list(self, script):\n        opcodes = []\n        new_pc = 0\n        try:\n            for opcode, data, pc, new_pc in self.get_opcodes(script):\n                opcodes.append(self.disassemble_for_opcode_data(opcode, data))\n        except ScriptError:\n            opcodes.append(binascii.hexlify(script[new_pc:]).decode(\"utf8\"))\n\n        return opcodes", "response": "Disassemble the given script. Returns a list of opcodes."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _signature_hash(self, tx_out_script, unsigned_txs_out_idx, hash_type):\n\n        if hash_type & SIGHASH_FORKID != SIGHASH_FORKID:\n            raise self.ScriptError()\n\n        return self._signature_for_hash_type_segwit(tx_out_script, unsigned_txs_out_idx, hash_type)", "response": "Return the canonical hash for a transaction."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns list of pre_annotations pc opcode instruction post_annotations", "response": "def annotate_scripts(self, tx, tx_in_idx):\n        \"return list of pre_annotations, pc, opcode, instruction, post_annotations\"\n        # input_annotations_f, output_annotations_f = annotation_f_for_scripts(tx, tx_in_idx)\n\n        data_annotations = collections.defaultdict(list)\n\n        def traceback_f(opcode, data, pc, vmc):\n            if opcode in (self.OP_CHECKSIG, self.OP_CHECKSIGVERIFY):\n                self.annotate_checksig(vmc, data_annotations)\n            if opcode in (self.OP_CHECKMULTISIG, self.OP_CHECKMULTISIGVERIFY):\n                self.annotate_checkmultisig(vmc, data_annotations)\n            return\n\n        try:\n            tx.check_solution(tx_in_idx, traceback_f=traceback_f)\n        except ScriptError:\n            pass\n\n        r = []\n\n        def traceback_f(opcode, data, pc, vmc):\n            a0 = []\n            if vmc.pc == 0:\n                if vmc.is_solution_script:\n                    a0.append(\"--- SIGNATURE SCRIPT START\")\n                else:\n                    a0.append(\"--- PUBLIC KEY SCRIPT START\")\n            r.append((a0, vmc.pc, opcode, self.instruction_for_opcode(opcode, data), data_annotations[data]))\n\n        try:\n            tx.check_solution(tx_in_idx, traceback_f=traceback_f)\n        except ScriptError:\n            pass\n\n        # the script may have ended early, so let's just double-check\n        try:\n            for idx, (opcode, data, pc, new_pc) in enumerate(itertools.chain(\n                self._script_tools.get_opcodes(tx.unspents[tx_in_idx].script),\n                    self._script_tools.get_opcodes(tx.txs_in[tx_in_idx].script))):\n                if idx >= len(r):\n                    r.append(([], pc, opcode, self.instruction_for_opcode(opcode, data), []))\n        except IndexError:\n            pass\n\n        return r"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a Tx by its hash.", "response": "def tx_for_tx_hash(self, tx_hash):\n        \"\"\"\n        Get a Tx by its hash.\n        \"\"\"\n        url = \"%s/rawtx/%s\" % (self.url, b2h_rev(tx_hash))\n        d = urlopen(url).read()\n        j = json.loads(d.decode(\"utf8\"))\n        tx = Tx.from_hex(j.get(\"rawtx\", \"\"))\n        if tx.hash() == tx_hash:\n            return tx"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef solve(self, hash160_lookup, tx_in_idx, hash_type=None, **kwargs):\n        if hash_type is None:\n            hash_type = SIGHASH_ALL\n        kwargs[\"hash160_lookup\"] = hash160_lookup\n        if \"signature_placeholder\" not in kwargs:\n            kwargs[\"signature_placeholder\"] = generate_default_placeholder_signature(kwargs.get(\"generator\"))\n        if self.tx.txs_in[tx_in_idx].witness:\n            kwargs[\"existing_script\"] = self.tx.txs_in[tx_in_idx].witness\n        else:\n            kwargs[\"existing_script\"] = [\n                data for opcode, data, pc, new_pc in self.ScriptTools.get_opcodes(\n                    self.tx.txs_in[tx_in_idx].script) if data is not None]\n        kwargs[\"signature_type\"] = hash_type\n        kwargs[\"generator_for_signature_type_f\"] = self.solution_checker.VM.generator_for_signature_type\n        constraints = self.determine_constraints(tx_in_idx, p2sh_lookup=kwargs.get(\"p2sh_lookup\"))\n        solution_list, witness_list = self.solve_for_constraints(constraints, **kwargs)\n        solution_script = self.ScriptTools.compile_push_data_list(solution_list)\n        if witness_list:\n            return solution_script, witness_list\n        return solution_script", "response": "Solve a standard transaction."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sign(self, hash160_lookup, tx_in_idx_set=None, hash_type=None, **kwargs):\n        checker = self.SolutionChecker(self.tx)\n        if tx_in_idx_set is None:\n            tx_in_idx_set = range(len(self.tx.txs_in))\n        self.tx.check_unspents()\n        for tx_in_idx in sorted(tx_in_idx_set):\n            tx_context = checker.tx_context_for_idx(tx_in_idx)\n            try:\n                checker.check_solution(tx_context, flags=None)\n                continue\n            except ScriptError:\n                pass\n            try:\n                r = self.solve(hash160_lookup, tx_in_idx, hash_type=hash_type, **kwargs)\n                if isinstance(r, bytes):\n                    self.tx.txs_in[tx_in_idx].script = r\n                else:\n                    self.tx.txs_in[tx_in_idx].script = r[0]\n                    self.tx.set_witness(tx_in_idx, r[1])\n            except (SolvingError, ValueError):\n                pass\n        return self", "response": "Sign a standard transaction."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\niterates over the sec blobs for a given script solution yield its sec blobs", "response": "def extract_secs(self, tx, tx_in_idx):\n        \"\"\"\n        For a given script solution, iterate yield its sec blobs\n        \"\"\"\n        sc = tx.SolutionChecker(tx)\n        tx_context = sc.tx_context_for_idx(tx_in_idx)\n        # set solution_stack in case there are no results from puzzle_and_solution_iterator\n        solution_stack = []\n        for puzzle_script, solution_stack, flags, sighash_f in sc.puzzle_and_solution_iterator(tx_context):\n            for opcode, data, pc, new_pc in self._script_tools.get_opcodes(puzzle_script):\n                if data and is_sec(data):\n                    yield data\n            for data in solution_stack:\n                if is_sec(data):\n                    yield data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of public pairs encoded as sec values.", "response": "def public_pairs_for_script(self, tx, tx_in_idx, generator):\n        \"\"\"\n        For a given script, iterate over and pull out public pairs encoded as sec values.\n        \"\"\"\n        public_pairs = []\n        for sec in self.extract_secs(tx, tx_in_idx):\n            try:\n                public_pairs.append(sec_to_public_pair(sec, generator))\n            except EncodingError:\n                pass\n        return public_pairs"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngive a transaction and an input index returns a list of public_pairs and sig_type pairs that are used in signing.", "response": "def who_signed_tx(self, tx, tx_in_idx):\n        \"\"\"\n        Given a transaction (tx) an input index (tx_in_idx), attempt to figure\n        out which addresses where used in signing (so far). This method\n        depends on tx.unspents being properly configured. This should work on\n        partially-signed MULTISIG transactions (it will return as many\n        addresses as there are good signatures).\n        Returns a list of (public_pairs, sig_type) pairs.\n        \"\"\"\n        public_pair_sig_type_list = self.public_pairs_signed(tx, tx_in_idx)\n        sig_type_list = [pp[-1] for pp in public_pair_sig_type_list]\n        hash160_list = [public_pair_to_hash160_sec(pp[0]) for pp in public_pair_sig_type_list]\n        address_list = [self._address.for_p2pkh(h160) for h160 in hash160_list]\n        return list(zip(address_list, sig_type_list))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef iterate_symbols():\n    for prefix in search_prefixes():\n        package = importlib.import_module(prefix)\n        for importer, modname, ispkg in pkgutil.walk_packages(path=package.__path__, onerror=lambda x: None):\n            network = network_for_netcode(modname)\n            if network:\n                yield network.symbol.upper()", "response": "Iterate over all registered netcodes."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of Spendable objects for the given bitcoin address.", "response": "def spendables_for_address(self, address):\n        \"\"\"\n        Return a list of Spendable objects for the\n        given bitcoin address.\n        \"\"\"\n        spendables = []\n        url_append = \"?unspentOnly=true&token=%s&includeScript=true\" % self.api_key\n        url = self.base_url(\"addrs/%s%s\" % (address, url_append))\n        result = json.loads(urlopen(url).read().decode(\"utf8\"))\n        for txn in result.get(\"txrefs\", []):\n            coin_value = txn.get(\"value\")\n            script = h2b(txn.get(\"script\"))\n            previous_hash = h2b_rev(txn.get(\"tx_hash\"))\n            previous_index = txn.get(\"tx_output_n\")\n            spendables.append(Tx.Spendable(coin_value, script, previous_hash, previous_index))\n        return spendables"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef tx_for_tx_hash(self, tx_hash):\n        try:\n            url_append = \"?token=%s&includeHex=true\" % self.api_key\n            url = self.base_url(\"txs/%s%s\" % (b2h_rev(tx_hash), url_append))\n            result = json.loads(urlopen(url).read().decode(\"utf8\"))\n            tx = Tx.parse(io.BytesIO(h2b(result.get(\"hex\"))))\n            return tx\n        except Exception:\n            raise Exception", "response": "returns the pycoin. tx object for the given tx_hash"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the balance object for the given address", "response": "def get_balance(self, address):\n        \"\"\"\n        returns the balance object from blockcypher for address\n        \"\"\"\n        url_append = \"/balance?token=%s\" % self.api_key\n        url = self.base_url(\"addrs/%s\" % (address + url_append))\n        result = json.loads(urlopen(url).read().decode(\"utf8\"))\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nbroadcast a transaction to the network", "response": "def broadcast_tx(self, tx):\n        \"\"\"\n        broadcast a transaction to the network\n        \"\"\"\n        url = self.base_url(\"txs/push\")\n        data = {\"tx\": tx.as_hex()}\n        result = json.loads(urlopen(url, data=json.dumps(data)).read().decode(\"utf8\"))\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates a shared public key for the given private key and the given public pair.", "response": "def generate_shared_public_key(my_private_key, their_public_pair, generator):\n    \"\"\"\n    Two parties each generate a private key and share their public key with the\n    other party over an insecure channel. The shared public key can be generated by\n    either side, but not by eavesdroppers. You can then use the entropy from the\n    shared public key to created a common symmetric key for encryption. (This\n    is beyond of the scope of pycoin.)\n\n    See also <https://en.wikipedia.org/wiki/Key_exchange>\n\n    :param my_private_key: an integer private key\n    :param their_public_pair: a pair ``(x, y)`` representing a public key for the ``generator``\n    :param generator: a :class:`Generator <pycoin.ecdsa.Generator.Generator>`\n    :returns: a :class:`Point <pycoin.ecdsa.Point.Point>`, which can be used as a shared\n        public key.\n    \"\"\"\n    p = generator.Point(*their_public_pair)\n    return my_private_key * p"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts binary to base58 using BASE58_ALPHABET. Like Bitcoin addresses.", "response": "def b2a_base58(s):\n    \"\"\"Convert binary to base58 using BASE58_ALPHABET. Like Bitcoin addresses.\"\"\"\n    v, prefix = to_long(256, lambda x: x, iterbytes(s))\n    s = from_long(v, prefix, BASE58_BASE, lambda v: BASE58_ALPHABET[v])\n    return s.decode(\"utf8\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert base58 to binary using BASE58_ALPHABET.", "response": "def a2b_base58(s):\n    \"\"\"Convert base58 to binary using BASE58_ALPHABET.\"\"\"\n    v, prefix = to_long(BASE58_BASE, lambda c: BASE58_LOOKUP[c], s.encode(\"utf8\"))\n    return from_long(v, prefix, 256, lambda x: x)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a base58 encoded string to a base58 - encoded binary string.", "response": "def a2b_hashed_base58(s):\n    \"\"\"\n    If the passed string is hashed_base58, return the binary data.\n    Otherwise raises an EncodingError.\n    \"\"\"\n    data = a2b_base58(s)\n    data, the_hash = data[:-4], data[-4:]\n    if double_sha256(data)[:4] == the_hash:\n        return data\n    raise EncodingError(\"hashed base58 has bad checksum %s\" % s)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd a key to the bloom filter. Returns True if the key already exists in this bloom filter False otherwise.", "response": "def add(self, key, skip_check=False):\n        \"\"\" Adds a key to this bloom filter. If the key already exists in this\n        filter it will return True. Otherwise False.\n\n        >>> b = BloomFilter(capacity=100)\n        >>> b.add(\"hello\")\n        False\n        >>> b.add(\"hello\")\n        True\n        >>> b.count\n        1\n\n        \"\"\"\n        bitarray = self.bitarray\n        bits_per_slice = self.bits_per_slice\n        hashes = self.make_hashes(key)\n        found_all_bits = True\n        if self.count > self.capacity:\n            raise IndexError(\"BloomFilter is at capacity\")\n        offset = 0\n        for k in hashes:\n            if not skip_check and found_all_bits and not bitarray[offset + k]:\n                found_all_bits = False\n            self.bitarray[offset + k] = True\n            offset += bits_per_slice\n\n        if skip_check:\n            self.count += 1\n            return False\n        elif not found_all_bits:\n            self.count += 1\n            return False\n        else:\n            return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef copy(self):\n        new_filter = BloomFilter(self.capacity, self.error_rate)\n        new_filter.bitarray = self.bitarray.copy()\n        return new_filter", "response": "Return a copy of this bloom filter."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates the intersection of the two underlying bitarrays and returns a new bloom filter object.", "response": "def intersection(self, other):\n        \"\"\" Calculates the intersection of the two underlying bitarrays and returns\n        a new bloom filter object.\"\"\"\n        if self.capacity != other.capacity or \\\n            self.error_rate != other.error_rate:\n            raise ValueError(\"Intersecting filters requires both filters to \\\nhave equal capacity and error rate\")\n        new_bloom = self.copy()\n        new_bloom.bitarray = new_bloom.bitarray & other.bitarray\n        return new_bloom"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrite the bloom filter to file object f. Underlying bits are written as machine values.", "response": "def tofile(self, f):\n        \"\"\"Write the bloom filter to file object `f'. Underlying bits\n        are written as machine values. This is much more space\n        efficient than pickling the object.\"\"\"\n        f.write(pack(self.FILE_FMT, self.error_rate, self.num_slices,\n                     self.bits_per_slice, self.capacity, self.count))\n        (f.write(self.bitarray.tobytes()) if is_string_io(f)\n         else self.bitarray.tofile(f))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fromfile(cls, f, n=-1):\n        headerlen = calcsize(cls.FILE_FMT)\n\n        if 0 < n < headerlen:\n            raise ValueError('n too small!')\n\n        filter = cls(1)  # Bogus instantiation, we will `_setup'.\n        filter._setup(*unpack(cls.FILE_FMT, f.read(headerlen)))\n        filter.bitarray = bitarray.bitarray(endian='little')\n        if n > 0:\n            (filter.bitarray.frombytes(f.read(n-headerlen)) if is_string_io(f)\n             else filter.bitarray.fromfile(f, n - headerlen))\n        else:\n            (filter.bitarray.frombytes(f.read()) if is_string_io(f)\n             else filter.bitarray.fromfile(f))\n        if filter.num_bits != filter.bitarray.length() and \\\n               (filter.num_bits + (8 - filter.num_bits % 8)\n                != filter.bitarray.length()):\n            raise ValueError('Bit length mismatch!')\n\n        return filter", "response": "Read a bloom filter from file - object f serialized with\n       . tofile. If n > 0 read only so many bytes."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a key to this bloom filter. Returns True if the key already exists in this bloom filter otherwise False.", "response": "def add(self, key):\n        \"\"\"Adds a key to this bloom filter.\n        If the key already exists in this filter it will return True.\n        Otherwise False.\n\n        >>> b = ScalableBloomFilter(initial_capacity=100, error_rate=0.001, \\\n                                    mode=ScalableBloomFilter.SMALL_SET_GROWTH)\n        >>> b.add(\"hello\")\n        False\n        >>> b.add(\"hello\")\n        True\n\n        \"\"\"\n        if key in self:\n            return True\n        if not self.filters:\n            filter = BloomFilter(\n                capacity=self.initial_capacity,\n                error_rate=self.error_rate * (1.0 - self.ratio))\n            self.filters.append(filter)\n        else:\n            filter = self.filters[-1]\n            if filter.count >= filter.capacity:\n                filter = BloomFilter(\n                    capacity=filter.capacity * self.scale,\n                    error_rate=filter.error_rate * self.ratio)\n                self.filters.append(filter)\n        filter.add(key, skip_check=True)\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef tofile(self, f):\n        f.write(pack(self.FILE_FMT, self.scale, self.ratio,\n                     self.initial_capacity, self.error_rate))\n\n        # Write #-of-filters\n        f.write(pack(b'<l', len(self.filters)))\n\n        if len(self.filters) > 0:\n            # Then each filter directly, with a header describing\n            # their lengths.\n            headerpos = f.tell()\n            headerfmt = b'<' + b'Q'*(len(self.filters))\n            f.write(b'.' * calcsize(headerfmt))\n            filter_sizes = []\n            for filter in self.filters:\n                begin = f.tell()\n                filter.tofile(f)\n                filter_sizes.append(f.tell() - begin)\n\n            f.seek(headerpos)\n            f.write(pack(headerfmt, *filter_sizes))", "response": "Serialize this ScalableBloomFilter into the file - object\n        f."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fromfile(cls, f):\n        filter = cls()\n        filter._setup(*unpack(cls.FILE_FMT, f.read(calcsize(cls.FILE_FMT))))\n        nfilters, = unpack(b'<l', f.read(calcsize(b'<l')))\n        if nfilters > 0:\n            header_fmt = b'<' + b'Q'*nfilters\n            bytes = f.read(calcsize(header_fmt))\n            filter_lengths = unpack(header_fmt, bytes)\n            for fl in filter_lengths:\n                filter.filters.append(BloomFilter.fromfile(f, fl))\n        else:\n            filter.filters = []\n\n        return filter", "response": "Deserialize the ScalableBloomFilter in file object f."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef safe_get_user_model():\n    user_app, user_model = settings.AUTH_USER_MODEL.split('.')\n    return apps.get_registered_model(user_app, user_model)", "response": "Safe loading of the User model customized or not."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the title with word count and number of comments.", "response": "def get_title(self, entry):\n        \"\"\"\n        Return the title with word count and number of comments.\n        \"\"\"\n        title = _('%(title)s (%(word_count)i words)') % \\\n            {'title': entry.title, 'word_count': entry.word_count}\n        reaction_count = int(entry.comment_count +\n                             entry.pingback_count +\n                             entry.trackback_count)\n        if reaction_count:\n            return ungettext_lazy(\n                '%(title)s (%(reactions)i reaction)',\n                '%(title)s (%(reactions)i reactions)', reaction_count) % \\\n                {'title': title,\n                 'reactions': reaction_count}\n        return title"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the authors in HTML.", "response": "def get_authors(self, entry):\n        \"\"\"\n        Return the authors in HTML.\n        \"\"\"\n        try:\n            return format_html_join(\n                ', ', '<a href=\"{}\" target=\"blank\">{}</a>',\n                [(author.get_absolute_url(),\n                  getattr(author, author.USERNAME_FIELD))\n                 for author in entry.authors.all()])\n        except NoReverseMatch:\n            return ', '.join(\n                [conditional_escape(getattr(author, author.USERNAME_FIELD))\n                 for author in entry.authors.all()])"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the categories linked in HTML.", "response": "def get_categories(self, entry):\n        \"\"\"\n        Return the categories linked in HTML.\n        \"\"\"\n        try:\n            return format_html_join(\n                ', ', '<a href=\"{}\" target=\"blank\">{}</a>',\n                [(category.get_absolute_url(), category.title)\n                 for category in entry.categories.all()])\n        except NoReverseMatch:\n            return ', '.join([conditional_escape(category.title)\n                              for category in entry.categories.all()])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the tags linked in HTML.", "response": "def get_tags(self, entry):\n        \"\"\"\n        Return the tags linked in HTML.\n        \"\"\"\n        try:\n            return format_html_join(\n                ', ', '<a href=\"{}\" target=\"blank\">{}</a>',\n                [(reverse('zinnia:tag_detail', args=[tag]), tag)\n                 for tag in entry.tags_list])\n        except NoReverseMatch:\n            return conditional_escape(entry.tags)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_sites(self, entry):\n        try:\n            index_url = reverse('zinnia:entry_archive_index')\n        except NoReverseMatch:\n            index_url = ''\n        return format_html_join(\n            ', ', '<a href=\"{}://{}{}\" target=\"blank\">{}</a>',\n            [(settings.PROTOCOL, site.domain, index_url,\n              conditional_escape(site.name)) for site in entry.sites.all()])", "response": "Return the sites linked in HTML."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the short url in HTML.", "response": "def get_short_url(self, entry):\n        \"\"\"\n        Return the short url in HTML.\n        \"\"\"\n        try:\n            short_url = entry.short_url\n        except NoReverseMatch:\n            short_url = entry.get_absolute_url()\n        return format_html('<a href=\"{url}\" target=\"blank\">{url}</a>',\n                           url=short_url)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmakes special filtering by user s permissions.", "response": "def get_queryset(self, request):\n        \"\"\"\n        Make special filtering by user's permissions.\n        \"\"\"\n        if not request.user.has_perm('zinnia.can_view_all'):\n            queryset = self.model.objects.filter(authors__pk=request.user.pk)\n        else:\n            queryset = super(EntryAdmin, self).get_queryset(request)\n        return queryset.prefetch_related('categories', 'authors', 'sites')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nproviding initial datas when creating an entry.", "response": "def get_changeform_initial_data(self, request):\n        \"\"\"\n        Provide initial datas when creating an entry.\n        \"\"\"\n        get_data = super(EntryAdmin, self).get_changeform_initial_data(request)\n        return get_data or {\n            'sites': [Site.objects.get_current().pk],\n            'authors': [request.user.pk]\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef formfield_for_manytomany(self, db_field, request, **kwargs):\n        if db_field.name == 'authors':\n            kwargs['queryset'] = Author.objects.filter(\n                Q(is_staff=True) | Q(entries__isnull=False)\n                ).distinct()\n\n        return super(EntryAdmin, self).formfield_for_manytomany(\n            db_field, request, **kwargs)", "response": "Filter the disposable authors."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn readonly fields by user s permissions.", "response": "def get_readonly_fields(self, request, obj=None):\n        \"\"\"\n        Return readonly fields by user's permissions.\n        \"\"\"\n        readonly_fields = list(super(EntryAdmin, self).get_readonly_fields(\n            request, obj))\n\n        if not request.user.has_perm('zinnia.can_change_status'):\n            readonly_fields.append('status')\n\n        if not request.user.has_perm('zinnia.can_change_author'):\n            readonly_fields.append('authors')\n\n        return readonly_fields"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_actions(self, request):\n        actions = super(EntryAdmin, self).get_actions(request)\n        if not actions:\n            return actions\n        if (not request.user.has_perm('zinnia.can_change_author') or\n                not request.user.has_perm('zinnia.can_view_all')):\n            del actions['make_mine']\n        if not request.user.has_perm('zinnia.can_change_status'):\n            del actions['make_hidden']\n            del actions['make_published']\n        if not settings.PING_DIRECTORIES:\n            del actions['ping_directories']\n\n        return actions", "response": "Define actions by user s permissions."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the entries to the current user.", "response": "def make_mine(self, request, queryset):\n        \"\"\"\n        Set the entries to the current user.\n        \"\"\"\n        author = Author.objects.get(pk=request.user.pk)\n        for entry in queryset:\n            if author not in entry.authors.all():\n                entry.authors.add(author)\n        self.message_user(\n            request, _('The selected entries now belong to you.'))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make_published(self, request, queryset):\n        queryset.update(status=PUBLISHED)\n        EntryPublishedVectorBuilder().cache_flush()\n        self.ping_directories(request, queryset, messages=False)\n        self.message_user(\n            request, _('The selected entries are now marked as published.'))", "response": "Set entries selected as published."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets entries selected as hidden.", "response": "def make_hidden(self, request, queryset):\n        \"\"\"\n        Set entries selected as hidden.\n        \"\"\"\n        queryset.update(status=HIDDEN)\n        EntryPublishedVectorBuilder().cache_flush()\n        self.message_user(\n            request, _('The selected entries are now marked as hidden.'))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nclose the comments for selected entries.", "response": "def close_comments(self, request, queryset):\n        \"\"\"\n        Close the comments for selected entries.\n        \"\"\"\n        queryset.update(comment_enabled=False)\n        self.message_user(\n            request, _('Comments are now closed for selected entries.'))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef close_pingbacks(self, request, queryset):\n        queryset.update(pingback_enabled=False)\n        self.message_user(\n            request, _('Pingbacks are now closed for selected entries.'))", "response": "Close the pingbacks for selected entries."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef close_trackbacks(self, request, queryset):\n        queryset.update(trackback_enabled=False)\n        self.message_user(\n            request, _('Trackbacks are now closed for selected entries.'))", "response": "Close the trackbacks for selected entries."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef put_on_top(self, request, queryset):\n        queryset.update(publication_date=timezone.now())\n        self.ping_directories(request, queryset, messages=False)\n        self.message_user(request, _(\n            'The selected entries are now set at the current date.'))", "response": "Put the selected entries on top at the current date."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmark selected entries as featured post.", "response": "def mark_featured(self, request, queryset):\n        \"\"\"\n        Mark selected as featured post.\n        \"\"\"\n        queryset.update(featured=True)\n        self.message_user(\n            request, _('Selected entries are now marked as featured.'))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef unmark_featured(self, request, queryset):\n        queryset.update(featured=False)\n        self.message_user(\n            request, _('Selected entries are no longer marked as featured.'))", "response": "Un - Mark selected featured posts."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nping web directories for selected entries.", "response": "def ping_directories(self, request, queryset, messages=True):\n        \"\"\"\n        Ping web directories for selected entries.\n        \"\"\"\n        for directory in settings.PING_DIRECTORIES:\n            pinger = DirectoryPinger(directory, queryset)\n            pinger.join()\n            if messages:\n                success = 0\n                for result in pinger.results:\n                    if not result.get('flerror', True):\n                        success += 1\n                    else:\n                        self.message_user(request,\n                                          '%s : %s' % (directory,\n                                                       result['message']))\n                if success:\n                    self.message_user(\n                        request,\n                        _('%(directory)s directory succesfully '\n                          'pinged %(success)d entries.') %\n                        {'directory': directory, 'success': success})"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef choice(self, obj):\n        tree_id = getattr(obj, self.queryset.model._mptt_meta.tree_id_attr, 0)\n        left = getattr(obj, self.queryset.model._mptt_meta.left_attr, 0)\n        return super(MPTTModelChoiceIterator,\n                     self).choice(obj) + ((tree_id, left),)", "response": "Overloads the choice method to add the position\n        of the object in the tree for future sorting."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates labels which represent the tree level of each node in the tree.", "response": "def label_from_instance(self, obj):\n        \"\"\"\n        Create labels which represent the tree level of each node\n        when generating option labels.\n        \"\"\"\n        label = smart_text(obj)\n        prefix = self.level_indicator * getattr(obj, obj._mptt_meta.level_attr)\n        if prefix:\n            return '%s %s' % (prefix, label)\n        return label"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\npopulate the context of the template with technical informations for building urls.", "response": "def get_context_data(self, **kwargs):\n        \"\"\"\n        Populate the context of the template\n        with technical informations for building urls.\n        \"\"\"\n        context = super(CapabilityView, self).get_context_data(**kwargs)\n        context.update({'protocol': PROTOCOL,\n                        'copyright': COPYRIGHT,\n                        'feeds_format': FEEDS_FORMAT,\n                        'site': Site.objects.get_current()})\n        return context"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_user_flagger():\n    user_klass = get_user_model()\n    try:\n        user = user_klass.objects.get(pk=COMMENT_FLAG_USER_ID)\n    except user_klass.DoesNotExist:\n        try:\n            user = user_klass.objects.get(\n                **{user_klass.USERNAME_FIELD: FLAGGER_USERNAME})\n        except user_klass.DoesNotExist:\n            user = user_klass.objects.create_user(FLAGGER_USERNAME)\n    return user", "response": "Returns an User instance used by the system\n    when flagging a comment as trackback or pingback."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the model type for templates.", "response": "def get_model_type(self):\n        \"\"\"\n        Return the model type for templates.\n        \"\"\"\n        if self.model_type is None:\n            raise ImproperlyConfigured(\n                \"%s requires either a definition of \"\n                \"'model_type' or an implementation of 'get_model_type()'\" %\n                self.__class__.__name__)\n        return self.model_type"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the model name for templates.", "response": "def get_model_name(self):\n        \"\"\"\n        Return the model name for templates.\n        \"\"\"\n        if self.model_name is None:\n            raise ImproperlyConfigured(\n                \"%s requires either a definition of \"\n                \"'model_name' or an implementation of 'get_model_name()'\" %\n                self.__class__.__name__)\n        return self.model_name"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_template_names(self):\n        model_type = self.get_model_type()\n        model_name = self.get_model_name()\n\n        templates = [\n            'zinnia/%s/%s/entry_list.html' % (model_type, model_name),\n            'zinnia/%s/%s_entry_list.html' % (model_type, model_name),\n            'zinnia/%s/entry_list.html' % model_type,\n            'zinnia/entry_list.html']\n\n        if self.template_name is not None:\n            templates.insert(0, self.template_name)\n\n        return templates", "response": "Return a list of template names to be used for the view."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of template names to be used for the view.", "response": "def get_template_names(self):\n        \"\"\"\n        Return a list of template names to be used for the view.\n        \"\"\"\n        year = self.get_archive_part_value('year')\n        week = self.get_archive_part_value('week')\n        month = self.get_archive_part_value('month')\n        day = self.get_archive_part_value('day')\n\n        templates = []\n        path = 'zinnia/archives'\n        template_names = self.get_default_base_template_names()\n        for template_name in template_names:\n            templates.extend([template_name,\n                              'zinnia/%s' % template_name,\n                              '%s/%s' % (path, template_name)])\n        if year:\n            for template_name in template_names:\n                templates.append(\n                    '%s/%s/%s' % (path, year, template_name))\n        if week:\n            for template_name in template_names:\n                templates.extend([\n                    '%s/week/%s/%s' % (path, week, template_name),\n                    '%s/%s/week/%s/%s' % (path, year, week, template_name)])\n        if month:\n            for template_name in template_names:\n                templates.extend([\n                    '%s/month/%s/%s' % (path, month, template_name),\n                    '%s/%s/month/%s/%s' % (path, year, month, template_name)])\n        if day:\n            for template_name in template_names:\n                templates.extend([\n                    '%s/day/%s/%s' % (path, day, template_name),\n                    '%s/%s/day/%s/%s' % (path, year, day, template_name),\n                    '%s/month/%s/day/%s/%s' % (path, month, day,\n                                               template_name),\n                    '%s/%s/%s/%s/%s' % (path, year, month, day,\n                                        template_name)])\n\n        if self.template_name is not None:\n            templates.append(self.template_name)\n\n        templates.reverse()\n        return templates"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the value of the given archive part for today", "response": "def get_archive_part_value(self, part):\n        \"\"\"Return archive part for today\"\"\"\n        parts_dict = {'year': '%Y',\n                      'month': self.month_format,\n                      'week': self.week_format,\n                      'day': '%d'}\n        if self.today is None:\n            today = timezone.now()\n            if timezone.is_aware(today):\n                today = timezone.localtime(today)\n            self.today = today\n        return self.today.strftime(parts_dict[part])"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the default base template names for the Entry. template value.", "response": "def get_default_base_template_names(self):\n        \"\"\"\n        Return the Entry.template value.\n        \"\"\"\n        return [self.object.detail_template,\n                '%s.html' % self.object.slug,\n                '%s_%s' % (self.object.slug, self.object.detail_template)]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a queryset of published discussions.", "response": "def discussions(self):\n        \"\"\"\n        Returns a queryset of the published discussions.\n        \"\"\"\n        return comments.get_model().objects.for_model(\n            self).filter(is_public=True, is_removed=False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef comments(self):\n        return self.discussions.filter(Q(flags=None) | Q(\n            flags__flag=CommentFlag.MODERATOR_APPROVAL))", "response": "Returns a queryset of published comments."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef discussion_is_still_open(self, discussion_type, auto_close_after):\n        discussion_enabled = getattr(self, discussion_type)\n        if (discussion_enabled and isinstance(auto_close_after, int) and\n                auto_close_after >= 0):\n            return (timezone.now() - (\n                self.start_publication or self.publication_date)).days < \\\n                auto_close_after\n        return discussion_enabled", "response": "Checks if a type of discussion is still open."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\noverride the save method to create an excerpt from the content field if it is not already set.", "response": "def save(self, *args, **kwargs):\n        \"\"\"\n        Overrides the save method to create an excerpt\n        from the content field if void.\n        \"\"\"\n        if not self.excerpt and self.status == PUBLISHED:\n            self.excerpt = Truncator(strip_tags(\n                getattr(self, 'content', ''))).words(50)\n        super(ExcerptEntry, self).save(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef image_upload_to(self, filename):\n        now = timezone.now()\n        filename, extension = os.path.splitext(filename)\n\n        return os.path.join(\n            UPLOAD_TO,\n            now.strftime('%Y'),\n            now.strftime('%m'),\n            now.strftime('%d'),\n            '%s%s' % (slugify(filename), extension))", "response": "Compute the upload path for the image field."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve a Category instance by a path.", "response": "def get_category_or_404(path):\n    \"\"\"\n    Retrieve a Category instance by a path.\n    \"\"\"\n    path_bits = [p for p in path.split('/') if p]\n    return get_object_or_404(Category, slug=path_bits[-1])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_queryset(self):\n        self.category = get_category_or_404(self.kwargs['path'])\n        return self.category.entries_published()", "response": "Retrieve the category by his path and\n        build a queryset of her published entries."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_context_data(self, **kwargs):\n        context = super(BaseCategoryDetail, self).get_context_data(**kwargs)\n        context['category'] = self.category\n        return context", "response": "Add the current category in context."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the first author of an entry.", "response": "def item_author_name(self, item):\n        \"\"\"\n        Return the first author of an entry.\n        \"\"\"\n        if item.authors.count():\n            self.item_author = item.authors.all()[0]\n            return self.item_author.__str__()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the author s URL.", "response": "def item_author_link(self, item):\n        \"\"\"\n        Return the author's URL.\n        Should not be called if self.item_author_name has returned None.\n        \"\"\"\n        try:\n            author_url = self.item_author.get_absolute_url()\n            return self.site_url + author_url\n        except NoReverseMatch:\n            return self.site_url"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn an image for enclosure.", "response": "def item_enclosure_url(self, item):\n        \"\"\"\n        Return an image for enclosure.\n        \"\"\"\n        try:\n            url = item.image.url\n        except (AttributeError, ValueError):\n            img = BeautifulSoup(item.html_content, 'html.parser').find('img')\n            url = img.get('src') if img else None\n        self.cached_enclosure_url = url\n        if url:\n            url = urljoin(self.site_url, url)\n            if self.feed_format == 'rss':\n                url = url.replace('https://', 'http://')\n        return url"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the length of the enclosure if it s present on the FS otherwise returns an hardcoded value.", "response": "def item_enclosure_length(self, item):\n        \"\"\"\n        Try to obtain the size of the enclosure if it's present on the FS,\n        otherwise returns an hardcoded value.\n        Note: this method is only called if item_enclosure_url\n        has returned something.\n        \"\"\"\n        try:\n            return str(item.image.size)\n        except (AttributeError, ValueError, os.error):\n            pass\n        return '100000'"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef item_enclosure_mime_type(self, item):\n        mime_type, encoding = guess_type(self.cached_enclosure_url)\n        if mime_type:\n            return mime_type\n        return 'image/jpeg'", "response": "Guess the mime type of the enclosure."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef items(self, obj):\n        return TaggedItem.objects.get_by_model(\n            Entry.published.all(), obj)[:self.limit]", "response": "Get the published items of the tag."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the object that matches the GET parameter pattern.", "response": "def get_object(self, request):\n        \"\"\"\n        The GET parameter 'pattern' is the object.\n        \"\"\"\n        pattern = request.GET.get('pattern', '')\n        if len(pattern) < 3:\n            raise ObjectDoesNotExist\n        return pattern"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the items of the entries in the database.", "response": "def items(self):\n        \"\"\"\n        Items are the discussions on the entries.\n        \"\"\"\n        content_type = ContentType.objects.get_for_model(Entry)\n        return comments.get_model().objects.filter(\n            content_type=content_type, is_public=True).order_by(\n            '-submit_date')[:self.limit]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_object(self, request, year, month, day, slug):\n        return get_object_or_404(Entry, slug=slug,\n                                 publication_date__year=year,\n                                 publication_date__month=month,\n                                 publication_date__day=day)", "response": "Retrieve the object for the given entry."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef markdown(value, extensions=MARKDOWN_EXTENSIONS):\n    try:\n        import markdown\n    except ImportError:\n        warnings.warn(\"The Python markdown library isn't installed.\",\n                      RuntimeWarning)\n        return value\n\n    return markdown.markdown(force_text(value), extensions=extensions)", "response": "Markdown processing with optionally using various extensions\n    that python - markdown supports."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef restructuredtext(value, settings=RESTRUCTUREDTEXT_SETTINGS):\n    try:\n        from docutils.core import publish_parts\n    except ImportError:\n        warnings.warn(\"The Python docutils library isn't installed.\",\n                      RuntimeWarning)\n        return value\n\n    parts = publish_parts(source=force_bytes(value),\n                          writer_name='html4css1',\n                          settings_overrides=settings)\n    return force_text(parts['fragment'])", "response": "Converts a string to a RestructuredText object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef html_format(value):\n    if not value:\n        return ''\n    elif MARKUP_LANGUAGE == 'markdown':\n        return markdown(value)\n    elif MARKUP_LANGUAGE == 'textile':\n        return textile(value)\n    elif MARKUP_LANGUAGE == 'restructuredtext':\n        return restructuredtext(value)\n    elif '</p>' not in value:\n        return linebreaks(value)\n    return value", "response": "Returns the value formatted in HTML"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if category parent is self - parented.", "response": "def clean_parent(self):\n        \"\"\"\n        Check if category parent is not selfish.\n        \"\"\"\n        data = self.cleaned_data['parent']\n        if data == self.instance:\n            raise forms.ValidationError(\n                _('A category cannot be parent of itself.'),\n                code='self_parenting')\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pearson_score(list1, list2):\n    size = len(list1)\n    sum1 = sum(list1)\n    sum2 = sum(list2)\n    sum_sq1 = sum([pow(l, 2) for l in list1])\n    sum_sq2 = sum([pow(l, 2) for l in list2])\n\n    prod_sum = sum([list1[i] * list2[i] for i in range(size)])\n\n    num = prod_sum - (sum1 * sum2 / float(size))\n    den = sqrt((sum_sq1 - pow(sum1, 2.0) / size) *\n               (sum_sq2 - pow(sum2, 2.0) / size))\n\n    return num / den", "response": "Compute the Pearson score between two lists of vectors."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of the most related objects to instance.", "response": "def get_related(self, instance, number):\n        \"\"\"\n        Return a list of the most related objects to instance.\n        \"\"\"\n        related_pks = self.compute_related(instance.pk)[:number]\n        related_pks = [pk for pk, score in related_pks]\n        related_objects = sorted(\n            self.queryset.model.objects.filter(pk__in=related_pks),\n            key=lambda x: related_pks.index(x.pk))\n        return related_objects"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef compute_related(self, object_id, score=pearson_score):\n        dataset = self.dataset\n        object_vector = dataset.get(object_id)\n        if not object_vector:\n            return []\n\n        object_related = {}\n        for o_id, o_vector in dataset.items():\n            if o_id != object_id:\n                try:\n                    object_related[o_id] = score(object_vector, o_vector)\n                except ZeroDivisionError:\n                    pass\n        related = sorted(object_related.items(),\n                         key=lambda k_v: (k_v[1], k_v[0]), reverse=True)\n        return related", "response": "Compute the most related pks to an object s pk."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef raw_dataset(self):\n        dataset = {}\n        queryset = self.queryset.values_list(*(['pk'] + self.fields))\n        if self.limit:\n            queryset = queryset[:self.limit]\n        for item in queryset:\n            item = list(item)\n            item_pk = item.pop(0)\n            datas = ' '.join(map(six.text_type, item))\n            dataset[item_pk] = self.raw_clean(datas)\n        return dataset", "response": "Generate a raw dataset based on the queryset\n            and the specified fields."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\napplies a cleaning on raw datas.", "response": "def raw_clean(self, datas):\n        \"\"\"\n        Apply a cleaning on raw datas.\n        \"\"\"\n        datas = strip_tags(datas)             # Remove HTML\n        datas = STOP_WORDS.rebase(datas, '')  # Remove STOP WORDS\n        datas = PUNCTUATION.sub('', datas)    # Remove punctuation\n        datas = datas.lower()\n        return [d for d in datas.split() if len(d) > 1]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate the columns and the whole dataset.", "response": "def columns_dataset(self):\n        \"\"\"\n        Generate the columns and the whole dataset.\n        \"\"\"\n        data = {}\n        words_total = {}\n\n        for instance, words in self.raw_dataset.items():\n            words_item_total = {}\n            for word in words:\n                words_total.setdefault(word, 0)\n                words_item_total.setdefault(word, 0)\n                words_total[word] += 1\n                words_item_total[word] += 1\n            data[instance] = words_item_total\n\n        columns = sorted(words_total.keys(),\n                         key=lambda w: words_total[w],\n                         reverse=True)[:250]\n        columns = sorted(columns)\n        dataset = {}\n        for instance in data.keys():\n            dataset[instance] = [data[instance].get(word, 0)\n                                 for word in columns]\n        return columns, dataset"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the cache in the cache backend.", "response": "def set_cache(self, value):\n        \"\"\"\n        Assign the cache in cache.\n        \"\"\"\n        value.update(self.cache)\n        return self.cache_backend.set(self.cache_key, value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_related(self, instance, number):\n        cache = self.cache\n        cache_key = '%s:%s' % (instance.pk, number)\n        if cache_key not in cache:\n            related_objects = super(CachedModelVectorBuilder,\n                                    self).get_related(instance, number)\n            cache[cache_key] = related_objects\n            self.cache = cache\n        return cache[cache_key]", "response": "Implement high level cache system for get_related."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the columns_dataset for this model vector.", "response": "def columns_dataset(self):\n        \"\"\"\n        Implement high level cache system for columns and dataset.\n        \"\"\"\n        cache = self.cache\n        cache_key = 'columns_dataset'\n        if cache_key not in cache:\n            columns_dataset = super(CachedModelVectorBuilder, self\n                                    ).columns_dataset\n            cache[cache_key] = columns_dataset\n            self.cache = cache\n        return cache[cache_key]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the cache key for the current site.", "response": "def cache_key(self):\n        \"\"\"\n        Key for the cache handling current site.\n        \"\"\"\n        return '%s:%s' % (super(EntryPublishedVectorBuilder, self).cache_key,\n                          Site.objects.get_current().pk)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the object that is visible for the current user.", "response": "def get_object(self, queryset=None):\n        \"\"\"\n        If the status of the entry is not PUBLISHED,\n        a preview is requested, so we check if the user\n        has the 'zinnia.can_view_all' permission or if\n        it's an author of the entry.\n        \"\"\"\n        obj = super(EntryPreviewMixin, self).get_object(queryset)\n        if obj.is_visible:\n            return obj\n        if (self.request.user.has_perm('zinnia.can_view_all') or\n                self.request.user.pk in [\n                author.pk for author in obj.authors.all()]):\n            return obj\n        raise Http404(_('No entry found matching the query'))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the absolute url of the entry that has been published in base36.", "response": "def get_redirect_url(self, **kwargs):\n        \"\"\"\n        Get entry corresponding to 'pk' encoded in base36\n        in the 'token' variable and return the get_absolute_url\n        of the entry.\n        \"\"\"\n        entry = get_object_or_404(Entry.published, pk=int(kwargs['token'], 36))\n        return entry.get_absolute_url()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_q(token):\n    meta = getattr(token, 'meta', None)\n    query = getattr(token, 'query', '')\n    wildcards = None\n\n    if isinstance(query, six.string_types):  # Unicode -> Quoted string\n        search = query\n    else:  # List -> No quoted string (possible wildcards)\n        if len(query) == 1:\n            search = query[0]\n        elif len(query) == 3:\n            wildcards = 'BOTH'\n            search = query[1]\n        elif len(query) == 2:\n            if query[0] == '*':\n                wildcards = 'START'\n                search = query[1]\n            else:\n                wildcards = 'END'\n                search = query[0]\n\n    # Ignore short term and stop words\n    if (len(search) < 3 and not search.isdigit()) or search in STOP_WORDS:\n        return Q()\n\n    if not meta:\n        q = Q()\n        for field in SEARCH_FIELDS:\n            q |= Q(**{'%s__icontains' % field: search})\n        return q\n\n    if meta == 'category':\n        if wildcards == 'BOTH':\n            return (Q(categories__title__icontains=search) |\n                    Q(categories__slug__icontains=search))\n        elif wildcards == 'START':\n            return (Q(categories__title__iendswith=search) |\n                    Q(categories__slug__iendswith=search))\n        elif wildcards == 'END':\n            return (Q(categories__title__istartswith=search) |\n                    Q(categories__slug__istartswith=search))\n        else:\n            return (Q(categories__title__iexact=search) |\n                    Q(categories__slug__iexact=search))\n    elif meta == 'author':\n        if wildcards == 'BOTH':\n            return Q(**{'authors__%s__icontains' % Author.USERNAME_FIELD:\n                        search})\n        elif wildcards == 'START':\n            return Q(**{'authors__%s__iendswith' % Author.USERNAME_FIELD:\n                        search})\n        elif wildcards == 'END':\n            return Q(**{'authors__%s__istartswith' % Author.USERNAME_FIELD:\n                        search})\n        else:\n            return Q(**{'authors__%s__iexact' % Author.USERNAME_FIELD:\n                        search})\n    elif meta == 'tag':  # TODO: tags ignore wildcards\n        return Q(tags__icontains=search)", "response": "Create the Q object for the given token."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef union_q(token):\n    query = Q()\n    operation = 'and'\n    negation = False\n\n    for t in token:\n        if type(t) is ParseResults:  # See tokens recursively\n            query &= union_q(t)\n        else:\n            if t in ('or', 'and'):  # Set the new op and go to next token\n                operation = t\n            elif t == '-':  # Next tokens needs to be negated\n                negation = True\n            else:  # Append to query the token\n                if negation:\n                    t = ~t\n                if operation == 'or':\n                    query |= t\n                else:\n                    query &= t\n    return query", "response": "Returns a Q object that contains all the Q objects in the given token."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef advanced_search(pattern):\n    query_parsed = QUERY.parseString(pattern)\n    return Entry.published.filter(query_parsed[0]).distinct()", "response": "Return a queryset of entries that match the given pattern."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_tree_path(self, category):\n        try:\n            return format_html(\n                '<a href=\"{}\" target=\"blank\">/{}/</a>',\n                category.get_absolute_url(), category.tree_path)\n        except NoReverseMatch:\n            return '/%s/' % category.tree_path", "response": "Return the category s tree path in HTML."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrite out a message to stdout.", "response": "def write_out(self, message, verbosity_level=1):\n        \"\"\"\n        Convenient method for outputing.\n        \"\"\"\n        if self.verbosity and self.verbosity >= verbosity_level:\n            sys.stdout.write(smart_str(message))\n            sys.stdout.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef lookups(self, request, model_admin):\n        active_objects = self.model.published.all().annotate(\n            count_entries_published=Count('entries')).order_by(\n            '-count_entries_published', '-pk')\n        for active_object in active_objects:\n            yield (\n                str(active_object.pk), ungettext_lazy(\n                    '%(item)s (%(count)i entry)',\n                    '%(item)s (%(count)i entries)',\n                    active_object.count_entries_published) % {\n                    'item': smart_text(active_object),\n                    'count': active_object.count_entries_published})", "response": "Return published objects with the number of entries."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef queryset(self, request, queryset):\n        if self.value():\n            params = {self.lookup_key: self.value()}\n            return queryset.filter(**params)", "response": "Return the object s entries if a value is set."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a Crumb for a month.", "response": "def month_crumb(date):\n    \"\"\"\n    Crumb for a month.\n    \"\"\"\n    year = date.strftime('%Y')\n    month = date.strftime('%m')\n    month_text = DateFormat(date).format('F').capitalize()\n    return Crumb(month_text, reverse('zinnia:entry_archive_month',\n                                     args=[year, month]))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef day_crumb(date):\n    year = date.strftime('%Y')\n    month = date.strftime('%m')\n    day = date.strftime('%d')\n    return Crumb(day, reverse('zinnia:entry_archive_day',\n                              args=[year, month, day]))", "response": "Returns a Crumb for a day."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef entry_breadcrumbs(entry):\n    date = entry.publication_date\n    if is_aware(date):\n        date = localtime(date)\n    return [year_crumb(date), month_crumb(date),\n            day_crumb(date), Crumb(entry.title)]", "response": "Returns a list of breadcrumbs for an Entry."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef retrieve_breadcrumbs(path, model_instance, root_name=''):\n    breadcrumbs = []\n    zinnia_root_path = reverse('zinnia:entry_archive_index')\n\n    if root_name:\n        breadcrumbs.append(Crumb(root_name, zinnia_root_path))\n\n    if model_instance is not None:\n        key = model_instance.__class__.__name__\n        if key in MODEL_BREADCRUMBS:\n            breadcrumbs.extend(MODEL_BREADCRUMBS[key](model_instance))\n            return breadcrumbs\n\n    date_match = ARCHIVE_WEEK_REGEXP.match(path)\n    if date_match:\n        year, week = date_match.groups()\n        year_date = datetime(int(year), 1, 1)\n        date_breadcrumbs = [year_crumb(year_date),\n                            Crumb(_('Week %s') % week)]\n        breadcrumbs.extend(date_breadcrumbs)\n        return breadcrumbs\n\n    date_match = ARCHIVE_REGEXP.match(path)\n    if date_match:\n        date_dict = date_match.groupdict()\n        path_date = datetime(\n            int(date_dict['year']),\n            date_dict.get('month') is not None and\n            int(date_dict.get('month')) or 1,\n            date_dict.get('day') is not None and\n            int(date_dict.get('day')) or 1)\n\n        date_breadcrumbs = [year_crumb(path_date)]\n        if date_dict['month']:\n            date_breadcrumbs.append(month_crumb(path_date))\n        if date_dict['day']:\n            date_breadcrumbs.append(day_crumb(path_date))\n        breadcrumbs.extend(date_breadcrumbs)\n        breadcrumbs[-1].url = None\n        return breadcrumbs\n\n    url_components = [comp for comp in\n                      path.replace(zinnia_root_path, '', 1).split('/')\n                      if comp]\n    if len(url_components):\n        breadcrumbs.append(Crumb(_(url_components[-1].capitalize())))\n\n    return breadcrumbs", "response": "Builds a semi - hardcoded breadcrumbs based on the path and the model instance."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef authenticate(username, password, permission=None):\n    try:\n        author = Author.objects.get(\n            **{'%s__exact' % Author.USERNAME_FIELD: username})\n    except Author.DoesNotExist:\n        raise Fault(LOGIN_ERROR, _('Username is incorrect.'))\n    if not author.check_password(password):\n        raise Fault(LOGIN_ERROR, _('Password is invalid.'))\n    if not author.is_staff or not author.is_active:\n        raise Fault(PERMISSION_DENIED, _('User account unavailable.'))\n    if permission:\n        if not author.has_perm(permission):\n            raise Fault(PERMISSION_DENIED, _('User cannot %s.') % permission)\n    return author", "response": "Authenticate staff_user with permission."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef user_structure(user, site):\n    full_name = user.get_full_name().split()\n    first_name = full_name[0]\n    try:\n        last_name = full_name[1]\n    except IndexError:\n        last_name = ''\n    return {'userid': user.pk,\n            'email': user.email,\n            'nickname': user.get_username(),\n            'lastname': last_name,\n            'firstname': first_name,\n            'url': '%s://%s%s' % (\n                PROTOCOL, site.domain,\n                user.get_absolute_url())}", "response": "Returns a user structure for the user."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets blog structure for users", "response": "def get_users_blogs(apikey, username, password):\n    \"\"\"\n    blogger.getUsersBlogs(api_key, username, password)\n    => blog structure[]\n    \"\"\"\n    authenticate(username, password)\n    site = Site.objects.get_current()\n    return [blog_structure(site)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_user_info(apikey, username, password):\n    user = authenticate(username, password)\n    site = Site.objects.get_current()\n    return user_structure(user, site)", "response": "blogger.getUserInfo(api_key, username, password)\n    => user structure"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_authors(apikey, username, password):\n    authenticate(username, password)\n    return [author_structure(author)\n            for author in Author.objects.filter(is_staff=True)]", "response": "get authors from wp. getAuthors"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delete_post(apikey, post_id, username, password, publish):\n    user = authenticate(username, password, 'zinnia.delete_entry')\n    entry = Entry.objects.get(id=post_id, authors=user)\n    entry.delete()\n    return True", "response": "blogger.deletePost(api_key, post_id, username, password, 'publish')\n    => boolean"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget a single post by ID.", "response": "def get_post(post_id, username, password):\n    \"\"\"\n    metaWeblog.getPost(post_id, username, password)\n    => post structure\n    \"\"\"\n    user = authenticate(username, password)\n    site = Site.objects.get_current()\n    return post_structure(Entry.objects.get(id=post_id, authors=user), site)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets recent posts for a given blog.", "response": "def get_recent_posts(blog_id, username, password, number):\n    \"\"\"\n    metaWeblog.getRecentPosts(blog_id, username, password, number)\n    => post structure[]\n    \"\"\"\n    user = authenticate(username, password)\n    site = Site.objects.get_current()\n    return [post_structure(entry, site)\n            for entry in Entry.objects.filter(authors=user)[:number]]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets tags for blog", "response": "def get_tags(blog_id, username, password):\n    \"\"\"\n    wp.getTags(blog_id, username, password)\n    => tag structure[]\n    \"\"\"\n    authenticate(username, password)\n    site = Site.objects.get_current()\n    return [tag_structure(tag, site)\n            for tag in Tag.objects.usage_for_queryset(\n                Entry.published.all(), counts=True)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a list of categories from the given blog.", "response": "def get_categories(blog_id, username, password):\n    \"\"\"\n    metaWeblog.getCategories(blog_id, username, password)\n    => category structure[]\n    \"\"\"\n    authenticate(username, password)\n    site = Site.objects.get_current()\n    return [category_structure(category, site)\n            for category in Category.objects.all()]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a new category.", "response": "def new_category(blog_id, username, password, category_struct):\n    \"\"\"\n    wp.newCategory(blog_id, username, password, category)\n    => category_id\n    \"\"\"\n    authenticate(username, password, 'zinnia.add_category')\n    category_dict = {'title': category_struct['name'],\n                     'description': category_struct['description'],\n                     'slug': category_struct['slug']}\n    if int(category_struct['parent_id']):\n        category_dict['parent'] = Category.objects.get(\n            pk=category_struct['parent_id'])\n    category = Category.objects.create(**category_dict)\n\n    return category.pk"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new post in the order they were added.", "response": "def new_post(blog_id, username, password, post, publish):\n    \"\"\"\n    metaWeblog.newPost(blog_id, username, password, post, publish)\n    => post_id\n    \"\"\"\n    user = authenticate(username, password, 'zinnia.add_entry')\n    if post.get('dateCreated'):\n        creation_date = datetime.strptime(\n            post['dateCreated'].value[:18], '%Y-%m-%dT%H:%M:%S')\n        if settings.USE_TZ:\n            creation_date = timezone.make_aware(\n                creation_date, timezone.utc)\n    else:\n        creation_date = timezone.now()\n\n    entry_dict = {'title': post['title'],\n                  'content': post['description'],\n                  'excerpt': post.get('mt_excerpt', ''),\n                  'publication_date': creation_date,\n                  'creation_date': creation_date,\n                  'last_update': creation_date,\n                  'comment_enabled': post.get('mt_allow_comments', 1) == 1,\n                  'pingback_enabled': post.get('mt_allow_pings', 1) == 1,\n                  'trackback_enabled': post.get('mt_allow_pings', 1) == 1,\n                  'featured': post.get('sticky', 0) == 1,\n                  'tags': 'mt_keywords' in post and post['mt_keywords'] or '',\n                  'slug': 'wp_slug' in post and post['wp_slug'] or slugify(\n                      post['title']),\n                  'password': post.get('wp_password', '')}\n    if user.has_perm('zinnia.can_change_status'):\n        entry_dict['status'] = publish and PUBLISHED or DRAFT\n\n    entry = Entry.objects.create(**entry_dict)\n\n    author = user\n    if 'wp_author_id' in post and user.has_perm('zinnia.can_change_author'):\n        if int(post['wp_author_id']) != user.pk:\n            author = Author.objects.get(pk=post['wp_author_id'])\n    entry.authors.add(author)\n\n    entry.sites.add(Site.objects.get_current())\n    if 'categories' in post:\n        entry.categories.add(*[\n            Category.objects.get_or_create(\n                title=cat, slug=slugify(cat))[0]\n            for cat in post['categories']])\n\n    return entry.pk"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef edit_post(post_id, username, password, post, publish):\n    user = authenticate(username, password, 'zinnia.change_entry')\n    entry = Entry.objects.get(id=post_id, authors=user)\n    if post.get('dateCreated'):\n        creation_date = datetime.strptime(\n            post['dateCreated'].value[:18], '%Y-%m-%dT%H:%M:%S')\n        if settings.USE_TZ:\n            creation_date = timezone.make_aware(\n                creation_date, timezone.utc)\n    else:\n        creation_date = entry.creation_date\n\n    entry.title = post['title']\n    entry.content = post['description']\n    entry.excerpt = post.get('mt_excerpt', '')\n    entry.publication_date = creation_date\n    entry.creation_date = creation_date\n    entry.last_update = timezone.now()\n    entry.comment_enabled = post.get('mt_allow_comments', 1) == 1\n    entry.pingback_enabled = post.get('mt_allow_pings', 1) == 1\n    entry.trackback_enabled = post.get('mt_allow_pings', 1) == 1\n    entry.featured = post.get('sticky', 0) == 1\n    entry.tags = 'mt_keywords' in post and post['mt_keywords'] or ''\n    entry.slug = 'wp_slug' in post and post['wp_slug'] or slugify(\n        post['title'])\n    if user.has_perm('zinnia.can_change_status'):\n        entry.status = publish and PUBLISHED or DRAFT\n    entry.password = post.get('wp_password', '')\n    entry.save()\n\n    if 'wp_author_id' in post and user.has_perm('zinnia.can_change_author'):\n        if int(post['wp_author_id']) != user.pk:\n            author = Author.objects.get(pk=post['wp_author_id'])\n            entry.authors.clear()\n            entry.authors.add(author)\n\n    if 'categories' in post:\n        entry.categories.clear()\n        entry.categories.add(*[\n            Category.objects.get_or_create(\n                title=cat, slug=slugify(cat))[0]\n            for cat in post['categories']])\n    return True", "response": "Edit a single entry in the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef new_media_object(blog_id, username, password, media):\n    authenticate(username, password)\n    path = default_storage.save(Entry().image_upload_to(media['name']),\n                                ContentFile(media['bits'].data))\n    return {'url': default_storage.url(path)}", "response": "create a new media object"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_redirect_url(self, **kwargs):\n        entry = Entry.published.all().order_by('?')[0]\n        return entry.get_absolute_url()", "response": "Get the absolute url of the entry that is related to pk and\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef optgroups(self, name, value, attrs=None):\n        groups = []\n        has_selected = False\n        if attrs is None:\n            attrs = {}\n\n        for index, (option_value, option_label, sort_fields) in enumerate(\n                chain(self.choices)):\n\n            # Set tree attributes\n            attrs['data-tree-id'] = sort_fields[0]\n            attrs['data-left-value'] = sort_fields[1]\n\n            subgroup = []\n            subindex = None\n            choices = [(option_value, option_label)]\n            groups.append((None, subgroup, index))\n\n            for subvalue, sublabel in choices:\n                selected = (\n                    force_text(subvalue) in value and\n                    (has_selected is False or self.allow_multiple_selected)\n                )\n                if selected is True and has_selected is False:\n                    has_selected = True\n                subgroup.append(self.create_option(\n                    name, subvalue, sublabel, selected, index,\n                    subindex=subindex, attrs=attrs,\n                ))\n\n        return groups", "response": "Return a list of optgroups for this widget."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef render(self, name, value, attrs=None, renderer=None):\n        output = [super(TagAutoComplete, self).render(name, value, attrs)]\n        output.append('<script type=\"text/javascript\">')\n        output.append('(function($) {')\n        output.append('  $(document).ready(function() {')\n        output.append('    $(\"#id_%s\").select2({' % name)\n        output.append('       width: \"element\",')\n        output.append('       maximumInputLength: 50,')\n        output.append('       tokenSeparators: [\",\", \" \"],')\n        output.append('       tags: %s' % json.dumps(self.get_tags()))\n        output.append('     });')\n        output.append('    });')\n        output.append('}(django.jQuery));')\n        output.append('</script>')\n        return mark_safe('\\n'.join(output))", "response": "Render the default widget and initialize select2."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the base queryset that will prefetch related items for this object.", "response": "def get_queryset(self):\n        \"\"\"\n        Check if relation_names is correctly set and\n        do a prefetch related on the queryset with it.\n        \"\"\"\n        if self.relation_names is None:\n            raise ImproperlyConfigured(\n                \"'%s' must define 'relation_names'\" %\n                self.__class__.__name__)\n        if not isinstance(self.relation_names, (tuple, list)):\n            raise ImproperlyConfigured(\n                \"%s's relation_names property must be a tuple or list.\" %\n                self.__class__.__name__)\n        return super(PrefetchRelatedMixin, self\n                     ).get_queryset().prefetch_related(*self.relation_names)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_context_first_matching_object(context, context_lookups):\n    for key in context_lookups:\n        context_object = context.get(key)\n        if context_object:\n            return key, context_object\n    return None, None", "response": "Return the first object found in the context with the matching key."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the paginated current position within a loop and the non - paginated position within a loop.", "response": "def get_context_loop_positions(context):\n    \"\"\"\n    Return the paginated current position within a loop,\n    and the non-paginated position.\n    \"\"\"\n    try:\n        loop_counter = context['forloop']['counter']\n    except KeyError:\n        return 0, 0\n    try:\n        page = context['page_obj']\n    except KeyError:\n        return loop_counter, loop_counter\n    total_loop_counter = ((page.number - 1) * page.paginator.per_page +\n                          loop_counter)\n    return total_loop_counter, loop_counter"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_queryset(self):\n        if self.queryset is None:\n            raise ImproperlyConfigured(\n                \"'%s' must define 'queryset'\" % self.__class__.__name__)\n        return self.queryset()", "response": "Returns the base queryset for this object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef base36(value):\n    result = ''\n    while value:\n        value, i = divmod(value, 36)\n        result = BASE36_ALPHABET[i] + result\n    return result", "response": "Encode int to base 36."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndefault URL shortener backend for Zinnia.", "response": "def backend(entry):\n    \"\"\"\n    Default URL shortener backend for Zinnia.\n    \"\"\"\n    return '%s://%s%s' % (\n        PROTOCOL, Site.objects.get_current().domain,\n        reverse('zinnia:entry_shortlink', args=[base36(entry.pk)]))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_context_data(self, **kwargs):\n        context = super(BaseEntryChannel, self).get_context_data(**kwargs)\n        context.update({'query': self.query})\n        return context", "response": "Add query in context.\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the published tags.", "response": "def tags_published():\n    \"\"\"\n    Return the published tags.\n    \"\"\"\n    from tagging.models import Tag\n    from zinnia.models.entry import Entry\n    tags_entry_published = Tag.objects.usage_for_queryset(\n        Entry.published.all())\n    # Need to do that until the issue #44 of django-tagging is fixed\n    return Tag.objects.filter(name__in=[t.name for t in tags_entry_published])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef entries_published(queryset):\n    now = timezone.now()\n    return queryset.filter(\n        models.Q(start_publication__lte=now) |\n        models.Q(start_publication=None),\n        models.Q(end_publication__gt=now) |\n        models.Q(end_publication=None),\n        status=PUBLISHED, sites=Site.objects.get_current())", "response": "Return only the entries published."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn entries published on current site.", "response": "def on_site(self):\n        \"\"\"\n        Return entries published on current site.\n        \"\"\"\n        return super(EntryPublishedManager, self).get_queryset().filter(\n            sites=Site.objects.get_current())"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef search(self, pattern):\n        try:\n            return self.advanced_search(pattern)\n        except Exception:\n            return self.basic_search(pattern)", "response": "Top level search method on entries."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef basic_search(self, pattern):\n        lookup = None\n        for pattern in pattern.split():\n            query_part = models.Q()\n            for field in SEARCH_FIELDS:\n                query_part |= models.Q(**{'%s__icontains' % field: pattern})\n            if lookup is None:\n                lookup = query_part\n            else:\n                lookup |= query_part\n\n        return self.get_queryset().filter(lookup)", "response": "Basic search on entries."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_queryset(self):\n        now = timezone.now()\n        return super(\n            EntryRelatedPublishedManager, self).get_queryset().filter(\n            models.Q(entries__start_publication__lte=now) |\n            models.Q(entries__start_publication=None),\n            models.Q(entries__end_publication__gt=now) |\n            models.Q(entries__end_publication=None),\n            entries__status=PUBLISHED,\n            entries__sites=Site.objects.get_current()\n            ).distinct()", "response": "Return a queryset containing published entries."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload a class by a string path like module. models. MyModel.", "response": "def load_model_class(model_path):\n    \"\"\"\n    Load by import a class by a string path like:\n    'module.models.MyModel'.\n    This mechanism allows extension and customization of\n    the Entry model class.\n    \"\"\"\n    dot = model_path.rindex('.')\n    module_name = model_path[:dot]\n    class_name = model_path[dot + 1:]\n    try:\n        _class = getattr(import_module(module_name), class_name)\n        return _class\n    except (ImportError, AttributeError):\n        raise ImproperlyConfigured('%s cannot be imported' % model_path)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget a queryset for standardized access to them later Set the max entries attribute to the maximum of entries that are used by the items later .", "response": "def items(self):\n        \"\"\"\n        Get a queryset, cache infos for standardized access to them later\n        then compute the maximum of entries to define the priority\n        of each items.\n        \"\"\"\n        queryset = self.get_queryset()\n        self.cache_infos(queryset)\n        self.set_max_entries()\n        return queryset"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbuilds a queryset of items with published entries and annotated with the number of entries and the latest modification date.", "response": "def get_queryset(self):\n        \"\"\"\n        Build a queryset of items with published entries and annotated\n        with the number of entries and the latest modification date.\n        \"\"\"\n        return self.model.published.annotate(\n            count_entries_published=Count('entries')).annotate(\n            last_update=Max('entries__last_update')).order_by(\n            '-count_entries_published', '-last_update', '-pk')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncache infos like the number of entries published and the last modification date for standardized access later.", "response": "def cache_infos(self, queryset):\n        \"\"\"\n        Cache infos like the number of entries published and\n        the last modification date for standardized access later.\n        \"\"\"\n        self.cache = {}\n        for item in queryset:\n            self.cache[item.pk] = (item.count_entries_published,\n                                   item.last_update)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_max_entries(self):\n        if self.cache:\n            self.max_entries = float(max([i[0] for i in self.cache.values()]))", "response": "Define the maximum number of entries for computing the priority\n            of each item later."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef priority(self, item):\n        return '%.1f' % max(self.cache[item.pk][0] / self.max_entries, 0.1)", "response": "Returns the priority of the item in the cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the published Tags with option counts.", "response": "def get_queryset(self):\n        \"\"\"\n        Return the published Tags with option counts.\n        \"\"\"\n        self.entries_qs = Entry.published.all()\n        return Tag.objects.usage_for_queryset(\n            self.entries_qs, counts=True)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cache_infos(self, queryset):\n        self.cache = {}\n        for item in queryset:\n            # If the sitemap is too slow, don't hesitate to do this :\n            #   self.cache[item.pk] = (item.count, None)\n            self.cache[item.pk] = (\n                item.count, TaggedItem.objects.get_by_model(\n                    self.entries_qs, item)[0].last_update)", "response": "Cache the number of entries published and the last_update date under each tag."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nping entries to a directory in a thread.", "response": "def run(self):\n        \"\"\"\n        Ping entries to a directory in a thread.\n        \"\"\"\n        logger = getLogger('zinnia.ping.directory')\n        socket.setdefaulttimeout(self.timeout)\n        for entry in self.entries:\n            reply = self.ping_entry(entry)\n            self.results.append(reply)\n            logger.info('%s : %s', self.server_name, reply['message'])\n        socket.setdefaulttimeout(None)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nping an entry to a directory.", "response": "def ping_entry(self, entry):\n        \"\"\"\n        Ping an entry to a directory.\n        \"\"\"\n        entry_url = '%s%s' % (self.ressources.site_url,\n                              entry.get_absolute_url())\n        categories = '|'.join([c.title for c in entry.categories.all()])\n\n        try:\n            reply = self.server.weblogUpdates.extendedPing(\n                self.ressources.current_site.name,\n                self.ressources.blog_url, entry_url,\n                self.ressources.blog_feed, categories)\n        except Exception:\n            try:\n                reply = self.server.weblogUpdates.ping(\n                    self.ressources.current_site.name,\n                    self.ressources.blog_url, entry_url,\n                    categories)\n            except Exception:\n                reply = {'message': '%s is an invalid directory.' %\n                         self.server_name,\n                         'flerror': True}\n        return reply"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nping external URLs in a Thread.", "response": "def run(self):\n        \"\"\"\n        Ping external URLs in a Thread.\n        \"\"\"\n        logger = getLogger('zinnia.ping.external_urls')\n        socket.setdefaulttimeout(self.timeout)\n\n        external_urls = self.find_external_urls(self.entry)\n        external_urls_pingable = self.find_pingback_urls(external_urls)\n\n        for url, server_name in external_urls_pingable.items():\n            reply = self.pingback_url(server_name, url)\n            self.results.append(reply)\n            logger.info('%s : %s', url, reply)\n\n        socket.setdefaulttimeout(None)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_external_url(self, url, site_url):\n        url_splitted = urlsplit(url)\n        if not url_splitted.netloc:\n            return False\n        return url_splitted.netloc != urlsplit(site_url).netloc", "response": "Check if the URL is an external URL."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind external URLs in an entry.", "response": "def find_external_urls(self, entry):\n        \"\"\"\n        Find external URLs in an entry.\n        \"\"\"\n        soup = BeautifulSoup(entry.html_content, 'html.parser')\n        external_urls = [a['href'] for a in soup.find_all('a')\n                         if self.is_external_url(\n                             a['href'], self.ressources.site_url)]\n        return external_urls"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntries to find LINK markups to pingback URL.", "response": "def find_pingback_href(self, content):\n        \"\"\"\n        Try to find LINK markups to pingback URL.\n        \"\"\"\n        soup = BeautifulSoup(content, 'html.parser')\n        for link in soup.find_all('link'):\n            dict_attr = dict(link.attrs)\n            if 'rel' in dict_attr and 'href' in dict_attr:\n                for rel_type in dict_attr['rel']:\n                    if rel_type.lower() == PINGBACK:\n                        return dict_attr.get('href')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef find_pingback_urls(self, urls):\n        pingback_urls = {}\n\n        for url in urls:\n            try:\n                page = urlopen(url)\n                headers = page.info()\n\n                server_url = headers.get('X-Pingback')\n\n                if not server_url:\n                    content_type = headers.get('Content-Type', '').split(\n                        ';')[0].strip().lower()\n                    if content_type in ['text/html', 'application/xhtml+xml']:\n                        server_url = self.find_pingback_href(\n                            page.read(5 * 1024))\n\n                if server_url:\n                    server_url_splitted = urlsplit(server_url)\n                    if not server_url_splitted.netloc:\n                        url_splitted = urlsplit(url)\n                        server_url = '%s://%s%s' % (url_splitted.scheme,\n                                                    url_splitted.netloc,\n                                                    server_url)\n                    pingback_urls[url] = server_url\n            except IOError:\n                pass\n        return pingback_urls", "response": "Find the pingback URL for each URL in urls."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef pingback_url(self, server_name, target_url):\n        try:\n            server = ServerProxy(server_name)\n            reply = server.pingback.ping(self.entry_url, target_url)\n        except (Error, socket.error):\n            reply = '%s cannot be pinged.' % target_url\n        return reply", "response": "Pingback the target URL."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_categories(context, template='zinnia/tags/categories.html'):\n    return {'template': template,\n            'categories': Category.published.all().annotate(\n                count_entries_published=Count('entries')),\n            'context_category': context.get('category')}", "response": "Return the published categories."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the categories as a tree.", "response": "def get_categories_tree(context, template='zinnia/tags/categories_tree.html'):\n    \"\"\"\n    Return the categories as a tree.\n    \"\"\"\n    return {'template': template,\n            'categories': Category.objects.all().annotate(\n                count_entries=Count('entries')),\n            'context_category': context.get('category')}"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the published authors.", "response": "def get_authors(context, template='zinnia/tags/authors.html'):\n    \"\"\"\n    Return the published authors.\n    \"\"\"\n    return {'template': template,\n            'authors': Author.published.all().annotate(\n                count_entries_published=Count('entries')),\n            'context_author': context.get('author')}"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_featured_entries(number=5,\n                         template='zinnia/tags/entries_featured.html'):\n    \"\"\"\n    Return the featured entries.\n    \"\"\"\n    return {'template': template,\n            'entries': Entry.published.filter(featured=True)[:number]}", "response": "Return the number of featured entries."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the last 5 draft entries.", "response": "def get_draft_entries(number=5,\n                      template='zinnia/tags/entries_draft.html'):\n    \"\"\"\n    Return the last draft entries.\n    \"\"\"\n    return {'template': template,\n            'entries': Entry.objects.filter(status=DRAFT)[:number]}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning an HTML calendar of entries.", "response": "def get_calendar_entries(context, year=None, month=None,\n                         template='zinnia/tags/entries_calendar.html'):\n    \"\"\"\n    Return an HTML calendar of entries.\n    \"\"\"\n    if not (year and month):\n        day_week_month = (context.get('day') or\n                          context.get('week') or\n                          context.get('month'))\n        publication_date = getattr(context.get('object'),\n                                   'publication_date', None)\n        if day_week_month:\n            current_month = day_week_month\n        elif publication_date:\n            if settings.USE_TZ:\n                publication_date = timezone.localtime(publication_date)\n            current_month = publication_date.date()\n        else:\n            today = timezone.now()\n            if settings.USE_TZ:\n                today = timezone.localtime(today)\n            current_month = today.date()\n        current_month = current_month.replace(day=1)\n    else:\n        current_month = date(year, month, 1)\n\n    dates = list(map(\n        lambda x: settings.USE_TZ and timezone.localtime(x).date() or x.date(),\n        Entry.published.datetimes('publication_date', 'month')))\n\n    if current_month not in dates:\n        dates.append(current_month)\n        dates.sort()\n    index = dates.index(current_month)\n\n    previous_month = index > 0 and dates[index - 1] or None\n    next_month = index != len(dates) - 1 and dates[index + 1] or None\n    calendar = Calendar()\n\n    return {'template': template,\n            'next_month': next_month,\n            'previous_month': previous_month,\n            'calendar': calendar.formatmonth(\n                current_month.year,\n                current_month.month,\n                previous_month=previous_month,\n                next_month=next_month)}"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_recent_comments(number=5, template='zinnia/tags/comments_recent.html'):\n    # Using map(smart_text... fix bug related to issue #8554\n    entry_published_pks = map(smart_text,\n                              Entry.published.values_list('id', flat=True))\n    content_type = ContentType.objects.get_for_model(Entry)\n\n    comments = get_comment_model().objects.filter(\n        Q(flags=None) | Q(flags__flag=CommentFlag.MODERATOR_APPROVAL),\n        content_type=content_type, object_pk__in=entry_published_pks,\n        is_public=True).order_by('-pk')[:number]\n\n    comments = comments.prefetch_related('content_object')\n\n    return {'template': template,\n            'comments': comments}", "response": "Return the most recent comments."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_recent_linkbacks(number=5,\n                         template='zinnia/tags/linkbacks_recent.html'):\n    \"\"\"\n    Return the most recent linkbacks.\n    \"\"\"\n    entry_published_pks = map(smart_text,\n                              Entry.published.values_list('id', flat=True))\n    content_type = ContentType.objects.get_for_model(Entry)\n\n    linkbacks = get_comment_model().objects.filter(\n        content_type=content_type,\n        object_pk__in=entry_published_pks,\n        flags__flag__in=[PINGBACK, TRACKBACK],\n        is_public=True).order_by('-pk')[:number]\n\n    linkbacks = linkbacks.prefetch_related('content_object')\n\n    return {'template': template,\n            'linkbacks': linkbacks}", "response": "Return the most recent linkbacks."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef zinnia_pagination(context, page, begin_pages=1, end_pages=1,\n                      before_pages=2, after_pages=2,\n                      template='zinnia/tags/pagination.html'):\n    \"\"\"\n    Return a Digg-like pagination,\n    by splitting long list of page into 3 blocks of pages.\n    \"\"\"\n    get_string = ''\n    for key, value in context['request'].GET.items():\n        if key != 'page':\n            get_string += '&%s=%s' % (key, value)\n\n    page_range = list(page.paginator.page_range)\n    begin = page_range[:begin_pages]\n    end = page_range[-end_pages:]\n    middle = page_range[max(page.number - before_pages - 1, 0):\n                        page.number + after_pages]\n\n    if set(begin) & set(middle):  # [1, 2, 3], [2, 3, 4], [...]\n        begin = sorted(set(begin + middle))  # [1, 2, 3, 4]\n        middle = []\n    elif begin[-1] + 1 == middle[0]:  # [1, 2, 3], [4, 5, 6], [...]\n        begin += middle  # [1, 2, 3, 4, 5, 6]\n        middle = []\n    elif middle[-1] + 1 == end[0]:  # [...], [15, 16, 17], [18, 19, 20]\n        end = middle + end  # [15, 16, 17, 18, 19, 20]\n        middle = []\n    elif set(middle) & set(end):  # [...], [17, 18, 19], [18, 19, 20]\n        end = sorted(set(middle + end))  # [17, 18, 19, 20]\n        middle = []\n\n    if set(begin) & set(end):  # [1, 2, 3], [...], [2, 3, 4]\n        begin = sorted(set(begin + end))  # [1, 2, 3, 4]\n        middle, end = [], []\n    elif begin[-1] + 1 == end[0]:  # [1, 2, 3], [...], [4, 5, 6]\n        begin += end  # [1, 2, 3, 4, 5, 6]\n        middle, end = [], []\n\n    return {'template': template,\n            'page': page,\n            'begin': begin,\n            'middle': middle,\n            'end': end,\n            'GET_string': get_string}", "response": "Returns a Digg - like pagination for the given page."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a breadcrumb for the application.", "response": "def zinnia_breadcrumbs(context, root_name='',\n                       template='zinnia/tags/breadcrumbs.html',):\n    \"\"\"\n    Return a breadcrumb for the application.\n    \"\"\"\n    path = context['request'].path\n    context_object = get_context_first_object(\n        context, ['object', 'category', 'tag', 'author'])\n    context_page = context.get('page_obj')\n    breadcrumbs = retrieve_breadcrumbs(\n        path, context_object, context_page, root_name)\n\n    return {'template': template,\n            'breadcrumbs': breadcrumbs}"}
