{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read_frames(self, frame_size, hop_size, offset=0,\n                    duration=None, buffer_size=5760000):\n        \"\"\"\n        Generator that reads and returns the samples of the track in frames.\n\n        Args:\n            frame_size (int): The number of samples per frame.\n            hop_size (int): The number of samples between two frames.\n            offset (float): The time in seconds, from where to start\n                            reading the samples (rel. to the track start).\n            duration (float): The length of the samples to read in seconds.\n\n        Returns:\n            Generator: A generator yielding a tuple for every frame.\n            The first item is the frame,\n            the second the sampling-rate and\n            the third a boolean indicating if it is the last frame.\n        \"\"\"\n        if duration is not None:\n            end = offset + duration\n        else:\n            end = float('inf')\n\n        return audio.read_frames(\n            self.path,\n            frame_size,\n            hop_size,\n            start=offset,\n            end=end,\n            buffer_size=buffer_size)", "response": "Reads the audio file and returns the samples of the track in frames."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add(self, label):\n        label.label_list = self\n        self.label_tree.addi(label.start, label.end, label)", "response": "Adds a label to the end of the list."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef merge_overlaps(self, threshold=0.0):\n\n        updated_labels = []\n        all_intervals = self.label_tree.copy()\n\n        # recursivly find a group of overlapping labels with the same value\n        def recursive_overlaps(interval):\n            range_start = interval.begin - threshold\n            range_end = interval.end + threshold\n\n            direct_overlaps = all_intervals.overlap(range_start, range_end)\n            all_overlaps = [interval]\n            all_intervals.discard(interval)\n\n            for overlap in direct_overlaps:\n                if overlap.data.value == interval.data.value:\n                    all_overlaps.extend(recursive_overlaps(overlap))\n\n            return all_overlaps\n\n        # For every remaining interval\n        # - Find overlapping intervals recursively\n        # - Remove them\n        # - Create a concatenated new label\n        while not all_intervals.is_empty():\n            next_interval = list(all_intervals)[0]\n            overlapping = recursive_overlaps(next_interval)\n\n            ov_start = float('inf')\n            ov_end = 0.0\n            ov_value = next_interval.data.value\n\n            for overlap in overlapping:\n                ov_start = min(ov_start, overlap.begin)\n                ov_end = max(ov_end, overlap.end)\n                all_intervals.discard(overlap)\n\n            updated_labels.append(Label(\n                ov_value,\n                ov_start,\n                ov_end\n            ))\n\n        # Replace the old labels with the updated ones\n        self.label_tree.clear()\n        self.update(updated_labels)", "response": "Merge overlapping labels with the same value."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns for each distinct label value the total duration of all occurrences.", "response": "def label_total_duration(self):\n        \"\"\"\n        Return for each distinct label value the total duration of all occurrences.\n\n        Returns:\n            dict: A dictionary containing for every label-value (key)\n                  the total duration in seconds (value).\n\n        Example:\n            >>> ll = LabelList(labels=[\n            >>>     Label('a', 3, 5),\n            >>>     Label('b', 5, 8),\n            >>>     Label('a', 8, 10),\n            >>>     Label('b', 10, 14),\n            >>>     Label('a', 15, 18.5)\n            >>> ])\n            >>> ll.label_total_duration()\n            {'a': 7.5 'b': 7.0}\n        \"\"\"\n\n        durations = collections.defaultdict(float)\n\n        for label in self:\n            durations[label.value] += label.duration\n\n        return durations"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of all occuring label values.", "response": "def label_values(self):\n        \"\"\"\n        Return a list of all occuring label values.\n\n        Returns:\n            list: Lexicographically sorted list (str) of label values.\n\n        Example:\n            >>> ll = LabelList(labels=[\n            >>>     Label('a', 3.2, 4.5),\n            >>>     Label('b', 5.1, 8.9),\n            >>>     Label('c', 7.2, 10.5),\n            >>>     Label('d', 10.5, 14),\n            >>>     Label('d', 15, 18)\n            >>> ])\n            >>> ll.label_values()\n            ['a', 'b', 'c', 'd']\n        \"\"\"\n\n        all_labels = set([l.value for l in self])\n        return sorted(all_labels)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn for each label the number of occurrences within the list.", "response": "def label_count(self):\n        \"\"\"\n        Return for each label the number of occurrences within the list.\n\n        Returns:\n            dict: A dictionary containing for every label-value (key)\n            the number of occurrences (value).\n\n        Example:\n            >>> ll = LabelList(labels=[\n            >>>     Label('a', 3.2, 4.5),\n            >>>     Label('b', 5.1, 8.9),\n            >>>     Label('a', 7.2, 10.5),\n            >>>     Label('b', 10.5, 14),\n            >>>     Label('a', 15, 18)\n            >>> ])\n            >>> ll.label_count()\n            {'a': 3 'b': 2}\n        \"\"\"\n\n        occurrences = collections.defaultdict(int)\n\n        for label in self:\n            occurrences[label.value] += 1\n\n        return occurrences"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef all_tokens(self, delimiter=' '):\n        tokens = set()\n\n        for label in self:\n            tokens = tokens.union(set(label.tokenized(delimiter=delimiter)))\n\n        return tokens", "response": "Returns a list of all tokens occurring in the label - list."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef join(self, delimiter=' ', overlap_threshold=0.1):\n\n        sorted_by_start = sorted(self.labels)\n        concat_values = []\n        last_label_end = None\n\n        for label in sorted_by_start:\n            if last_label_end is None or (last_label_end - label.start < overlap_threshold and last_label_end > 0):\n                concat_values.append(label.value)\n                last_label_end = label.end\n            else:\n                raise ValueError('Labels overlap, not able to define the correct order')\n\n        return delimiter.join(concat_values)", "response": "Returns a string with all labels concatenated together."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of tokens based on all labels.", "response": "def tokenized(self, delimiter=' ', overlap_threshold=0.1):\n        \"\"\"\n        Return a ordered list of tokens based on all labels.\n        Joins all token from all labels (``label.tokenized()```).\n        If the overlapping between two labels is greater than ``overlap_threshold``,\n        an Exception is thrown.\n\n        Args:\n            delimiter (str): The delimiter used to split labels into tokens. (default: space)\n            overlap_threshold (float): Maximum overlap between two consecutive labels.\n\n        Returns:\n            str: A list containing tokens of all labels ordered according to the label order.\n\n        Example:\n            >>> ll = LabelList(idx='some', labels=[\n            >>>     Label('a d q', start=0, end=4),\n            >>>     Label('b', start=3.95, end=6.0),\n            >>>     Label('c a', start=7.0, end=10.2),\n            >>>     Label('f g', start=10.3, end=14.0)\n            >>> ])\n            >>> ll.tokenized(delimiter=' ', overlap_threshold=0.1)\n            ['a', 'd', 'q', 'b', 'c', 'a', 'f', 'g']\n        \"\"\"\n\n        sorted_by_start = sorted(self.labels)\n        tokens = []\n        last_label_end = None\n\n        for label in sorted_by_start:\n            if last_label_end is None or (last_label_end - label.start < overlap_threshold and last_label_end > 0):\n                tokens.extend(label.tokenized(delimiter=delimiter))\n                last_label_end = label.end\n            else:\n                raise ValueError('Labels overlap, not able to define the correct order')\n\n        return tokens"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a separate Label - List for every distinct label - value.", "response": "def separated(self):\n        \"\"\"\n        Create a separate Label-List for every distinct label-value.\n\n        Returns:\n            dict: A dictionary with distinct label-values as keys.\n                  Every value is a LabelList containing only labels with the same value.\n\n        Example:\n            >>> ll = LabelList(idx='some', labels=[\n            >>>     Label('a', start=0, end=4),\n            >>>     Label('b', start=3.95, end=6.0),\n            >>>     Label('a', start=7.0, end=10.2),\n            >>>     Label('b', start=10.3, end=14.0)\n            >>> ])\n            >>> s = ll.separate()\n            >>> s['a'].labels\n            [Label('a', start=0, end=4), Label('a', start=7.0, end=10.2)]\n            >>> s['b'].labels\n            [Label('b', start=3.95, end=6.0), Label('b', start=10.3, end=14.0)]\n        \"\"\"\n        separated_lls = collections.defaultdict(LabelList)\n\n        for label in self.labels:\n            separated_lls[label.value].add(label)\n\n        for ll in separated_lls.values():\n            ll.idx = self.idx\n\n        return separated_lls"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef labels_in_range(self, start, end, fully_included=False):\n\n        if fully_included:\n            intervals = self.label_tree.envelop(start, end)\n        else:\n            intervals = self.label_tree.overlap(start, end)\n\n        return [iv.data for iv in intervals]", "response": "Return a list of labels that are within the given range."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate all ranges of the label - list.", "response": "def ranges(self, yield_ranges_without_labels=False, include_labels=None):\n        \"\"\"\n        Generate all ranges of the label-list. A range is defined\n        as a part of the label-list for which the same labels are defined.\n\n        Args:\n            yield_ranges_without_labels(bool): If True also yields ranges for\n                                                which no labels are defined.\n            include_labels(list): If not empty, only the label values in\n                                   the list will be considered.\n\n        Returns:\n            generator: A generator which yields one range\n            (tuple start/end/list-of-labels) at a time.\n\n        Example:\n            >>> ll = LabelList(labels=[\n            >>>     Label('a', 3.2, 4.5),\n            >>>     Label('b', 5.1, 8.9),\n            >>>     Label('c', 7.2, 10.5),\n            >>>     Label('d', 10.5, 14)\n            >>>])\n            >>> ranges = ll.ranges()\n            >>> next(ranges)\n            (3.2, 4.5, [ < audiomate.annotations.Label at 0x1090527c8 > ])\n            >>> next(ranges)\n            (4.5, 5.1, [])\n            >>> next(ranges)\n            (5.1, 7.2, [ < audiomate.annotations.label.Label at 0x1090484c8 > ])\n        \"\"\"\n\n        tree_copy = self.label_tree.copy()\n\n        # Remove labels not included\n        if include_labels is not None:\n            for iv in list(tree_copy):\n                if iv.data.value not in include_labels:\n                    tree_copy.remove(iv)\n\n        def reduce(x, y):\n            x.append(y)\n            return x\n\n        # Split labels when overlapping and merge equal ranges to a list of labels\n        tree_copy.split_overlaps()\n        tree_copy.merge_equals(data_reducer=reduce, data_initializer=[])\n\n        intervals = sorted(tree_copy)\n        last_end = intervals[0].begin\n\n        # yield range by range\n        for i in range(len(intervals)):\n            iv = intervals[i]\n\n            # yield an empty range if necessary\n            if yield_ranges_without_labels and iv.begin > last_end:\n                yield (last_end, iv.begin, [])\n\n            yield (iv.begin, iv.end, iv.data)\n\n            last_end = iv.end"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsplitting the label - list into two parts and return them as new label - lists.", "response": "def split(self, cutting_points, shift_times=False, overlap=0.0):\n        \"\"\"\n        Split the label-list into x parts and return them as new label-lists.\n        x is defined by the number of cutting-points(``x == len(cutting_points) + 1``)\n\n        The result is a list of label-lists corresponding to each part.\n        Label-list 0 contains labels between ``0`` and ``cutting_points[0]``.\n        Label-list 1 contains labels between ``cutting_points[0]`` and ``cutting_points[1]``.\n        And so on.\n\n        Args:\n            cutting_points(list): List of floats defining the points in seconds,\n                                  where the label-list is splitted.\n            shift_times(bool): If True, start and end-time are shifted in splitted label-lists.\n                               So the start is relative to the cutting point and\n                               not to the beginning of the original label-list.\n            overlap(float): Amount of overlap in seconds. This amount is subtracted\n                            from a start-cutting-point, and added to a end-cutting-point.\n\n        Returns:\n            list: A list of of: class: `audiomate.annotations.LabelList`.\n\n        Example:\n\n            >>> ll = LabelList(labels=[\n            >>>     Label('a', 0, 5),\n            >>>     Label('b', 5, 10),\n            >>>     Label('c', 11, 15),\n            >>>])\n            >>>\n            >>> res = ll.split([4.1, 8.9, 12.0])\n            >>> len(res)\n            4\n            >>> res[0].labels\n            [Label('a', 0.0, 4.1)]\n            >>> res[1].labels\n            [\n                Label('a', 4.1, 5.0),\n                Label('b', 5.0, 8.9)\n            ]\n            >>> res[2].labels\n            [\n                Label('b', 8.9, 10.0),\n                Label('c', 11.0, 12.0)\n            ]\n            >>> res[3].labels\n            [Label('c', 12.0, 15.0)]\n\n        If ``shift_times = True``, the times are adjusted to be relative\n        to the cutting-points for every label-list but the first.\n\n            >>> ll = LabelList(labels=[\n            >>>     Label('a', 0, 5),\n            >>>     Label('b', 5, 10),\n            >>>])\n            >>>\n            >>> res = ll.split([4.6])\n            >>> len(res)\n            4\n            >>> res[0].labels\n            [Label('a', 0.0, 4.6)]\n            >>> res[1].labels\n            [\n                Label('a', 0.0, 0.4),\n                Label('b', 0.4, 5.4)\n            ]\n        \"\"\"\n\n        if len(cutting_points) == 0:\n            raise ValueError('At least one cutting-point is needed!')\n\n        # we have to loop in sorted order\n        cutting_points = sorted(cutting_points)\n\n        splits = []\n        iv_start = 0.0\n\n        for i in range(len(cutting_points) + 1):\n            if i < len(cutting_points):\n                iv_end = cutting_points[i]\n            else:\n                iv_end = float('inf')\n\n            # get all intervals intersecting range\n            intervals = self.label_tree.overlap(\n                iv_start - overlap,\n                iv_end + overlap\n            )\n\n            cp_splits = LabelList(idx=self.idx)\n\n            # Extract labels from intervals with updated times\n            for iv in intervals:\n                label = copy.deepcopy(iv.data)\n                label.start = max(0, iv_start - overlap, label.start)\n                label.end = min(iv_end + overlap, label.end)\n\n                if shift_times:\n                    orig_start = max(0, iv_start - overlap)\n                    label.start -= orig_start\n                    label.end -= orig_start\n\n                cp_splits.add(label)\n\n            splits.append(cp_splits)\n            iv_start = iv_end\n\n        return splits"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a label - list with a single label containing the given value.", "response": "def create_single(cls, value, idx='default'):\n        \"\"\" Create a label-list with a single label containing the given value. \"\"\"\n\n        return LabelList(idx=idx, labels=[\n            Label(value=value)\n        ])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef with_label_values(cls, values, idx='default'):\n\n        ll = LabelList(idx=idx)\n\n        for label_value in values:\n            ll.add(Label(label_value))\n\n        return ll", "response": "Create a new label - list containing labels with the given values."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of all subfolder - paths in the given path.", "response": "def get_folders(path):\n        \"\"\"\n        Return a list of all subfolder-paths in the given path.\n        \"\"\"\n        folder_paths = []\n\n        for item in os.listdir(path):\n            folder_path = os.path.join(path, item)\n\n            if os.path.isdir(folder_path):\n                folder_paths.append(folder_path)\n\n        return folder_paths"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_tag(corpus, path):\n        tag_idx = os.path.basename(path)\n        data_path = os.path.join(path, 'by_book')\n        tag_utt_ids = []\n\n        for gender_path in MailabsReader.get_folders(data_path):\n            # IN MIX FOLDERS THERE ARE NO SPEAKERS\n            # HANDLE EVERY UTT AS DIFFERENT ISSUER\n            if os.path.basename(gender_path) == 'mix':\n                utt_ids = MailabsReader.load_books_of_speaker(corpus,\n                                                              gender_path,\n                                                              None)\n\n                tag_utt_ids.extend(utt_ids)\n\n            else:\n                for speaker_path in MailabsReader.get_folders(gender_path):\n                    speaker = MailabsReader.load_speaker(corpus, speaker_path)\n                    utt_ids = MailabsReader.load_books_of_speaker(corpus,\n                                                                  speaker_path,\n                                                                  speaker)\n\n                    tag_utt_ids.extend(utt_ids)\n\n        filter = subset.MatchingUtteranceIdxFilter(\n            utterance_idxs=set(tag_utt_ids)\n        )\n        subview = subset.Subview(corpus, filter_criteria=[filter])\n        corpus.import_subview(tag_idx, subview)", "response": "Load all speakers and utterances from a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_speaker(corpus, path):\n        base_path, speaker_name = os.path.split(path)\n        base_path, gender_desc = os.path.split(base_path)\n        base_path, __ = os.path.split(base_path)\n        base_path, tag = os.path.split(base_path)\n\n        gender = issuers.Gender.UNKNOWN\n\n        if gender_desc == 'male':\n            gender = issuers.Gender.MALE\n        elif gender_desc == 'female':\n            gender = issuers.Gender.FEMALE\n\n        speaker = issuers.Speaker(speaker_name, gender=gender)\n        corpus.import_issuers(speaker)\n\n        return speaker", "response": "Load a speaker instance from the given path."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads all utterances for the given speaker at the given path.", "response": "def load_books_of_speaker(corpus, path, speaker):\n        \"\"\"\n        Load all utterances for the speaker at the given path.\n        \"\"\"\n        utt_ids = []\n\n        for book_path in MailabsReader.get_folders(path):\n            meta_path = os.path.join(book_path, 'metadata.csv')\n            wavs_path = os.path.join(book_path, 'wavs')\n\n            meta = textfile.read_separated_lines(meta_path,\n                                                 separator='|',\n                                                 max_columns=3)\n\n            for entry in meta:\n                file_basename = entry[0]\n                transcription_raw = entry[1]\n                transcription_clean = entry[2]\n\n                if speaker is None:\n                    idx = file_basename\n                    utt_speaker = issuers.Speaker(idx)\n                    speaker_idx = idx\n                    corpus.import_issuers(utt_speaker)\n                else:\n                    idx = '{}-{}'.format(speaker.idx, file_basename)\n                    speaker_idx = speaker.idx\n\n                wav_name = '{}.wav'.format(file_basename)\n                wav_path = os.path.join(wavs_path, wav_name)\n\n                if os.path.isfile(wav_path):\n                    corpus.new_file(wav_path, idx)\n\n                    ll_raw = annotations.LabelList.create_single(\n                        transcription_raw,\n                        idx=audiomate.corpus.LL_WORD_TRANSCRIPT_RAW\n                    )\n\n                    ll_clean = annotations.LabelList.create_single(\n                        transcription_clean,\n                        idx=audiomate.corpus.LL_WORD_TRANSCRIPT\n                    )\n\n                    utterance = corpus.new_utterance(idx, idx, speaker_idx)\n                    utterance.set_label_list(ll_raw)\n                    utterance.set_label_list(ll_clean)\n\n                    utt_ids.append(utterance.idx)\n\n        return utt_ids"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nopen the container file.", "response": "def open(self, mode=None):\n        \"\"\"\n        Open the container file.\n\n        Args:\n            mode (str): Either 'r' for read-only, 'w' for truncate and write or\n                        'a' for append. (default: 'a').\n                        If ``None``, uses ``self.mode``.\n        \"\"\"\n\n        if mode is None:\n            mode = self.mode\n        elif mode not in ['r', 'w', 'a']:\n            raise ValueError('Invalid mode! Modes: [\\'a\\', \\'r\\', \\'w\\']')\n\n        if self._file is None:\n            self._file = h5py.File(self.path, mode=mode)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef open_if_needed(self, mode=None):\n        was_open = self.is_open()\n\n        if not was_open:\n            self.open(mode=mode)\n\n        try:\n            yield self\n        finally:\n            if not was_open:\n                self.close()", "response": "Context manager for the use with with."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread and return the data stored for the given key.", "response": "def get(self, key, mem_map=True):\n        \"\"\"\n        Read and return the data stored for the given key.\n\n        Args:\n            key (str): The key to read the data from.\n            mem_map (bool): If ``True`` returns the data as\n                            memory-mapped array, otherwise a copy is returned.\n\n        Note:\n            The container has to be opened in advance.\n\n        Returns:\n            numpy.ndarray: The stored data.\n        \"\"\"\n        self.raise_error_if_not_open()\n\n        if key in self._file:\n            data = self._file[key]\n\n            if not mem_map:\n                data = data[()]\n\n            return data\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set(self, key, data):\n        self.raise_error_if_not_open()\n\n        if key in self._file:\n            del self._file[key]\n\n        self._file.create_dataset(key, data=data)", "response": "Set the given data to the container with the given key."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef append(self, key, data):\n        existing = self.get(key, mem_map=True)\n\n        if existing is not None:\n            num_existing = existing.shape[0]\n\n            if existing.shape[1:] != data.shape[1:]:\n                error_msg = (\n                    'The data to append needs to'\n                    'have the same dimensions ({}).'\n                )\n                raise ValueError(error_msg.format(existing.shape[1:]))\n\n            existing.resize(num_existing + data.shape[0], 0)\n            existing[num_existing:] = data\n        else:\n            max_shape = list(data.shape)\n            max_shape[0] = None\n\n            self._file.create_dataset(key, data=data,\n                                      chunks=True, maxshape=max_shape)", "response": "Append the given data to the HDF5 - Dataset for the given key."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remove(self, key):\n        self.raise_error_if_not_open()\n\n        if key in self._file:\n            del self._file[key]", "response": "Removes the data stored for the given key."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the absolute end of the utterance relative to the signal.", "response": "def end_abs(self):\n        \"\"\"\n        Return the absolute end of the utterance relative to the signal.\n        \"\"\"\n        if self.end == float('inf'):\n            return self.track.duration\n        else:\n            return self.end"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the number of samples for the given n - items in the current node.", "response": "def num_samples(self, sr=None):\n        \"\"\"\n        Return the number of samples.\n\n        Args:\n            sr (int): Calculate the number of samples with the given\n                      sampling-rate. If None use the native sampling-rate.\n\n        Returns:\n            int: Number of samples\n        \"\"\"\n        native_sr = self.sampling_rate\n        num_samples = units.seconds_to_sample(self.duration, native_sr)\n\n        if sr is not None:\n            ratio = float(sr) / native_sr\n            num_samples = int(np.ceil(num_samples * ratio))\n\n        return num_samples"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read_samples(self, sr=None, offset=0, duration=None):\n\n        read_duration = self.duration\n\n        if offset > 0 and read_duration is not None:\n            read_duration -= offset\n\n        if duration is not None:\n            if read_duration is None:\n                read_duration = duration\n            else:\n                read_duration = min(duration, read_duration)\n\n        return self.track.read_samples(\n            sr=sr,\n            offset=self.start + offset,\n            duration=read_duration\n        )", "response": "Reads the samples of the utterance."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_label_list(self, label_lists):\n\n        if isinstance(label_lists, annotations.LabelList):\n            label_lists = [label_lists]\n\n        for label_list in label_lists:\n            if label_list.idx is None:\n                label_list.idx = 'default'\n\n            label_list.utterance = self\n            self.label_lists[label_list.idx] = label_list", "response": "Set the label - list for this utterance."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a set of all label - values occurring in this utterance.", "response": "def all_label_values(self, label_list_ids=None):\n        \"\"\"\n        Return a set of all label-values occurring in this utterance.\n\n        Args:\n            label_list_ids (list): If not None, only label-values from\n                                   label-lists with an id contained in this list\n                                   are considered.\n\n        Returns:\n             :class:`set`: A set of distinct label-values.\n        \"\"\"\n        values = set()\n\n        for label_list in self.label_lists.values():\n            if label_list_ids is None or label_list.idx in label_list_ids:\n                values = values.union(label_list.label_values())\n\n        return values"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef label_count(self, label_list_ids=None):\n        count = collections.defaultdict(int)\n\n        for label_list in self.label_lists.values():\n            if label_list_ids is None or label_list.idx in label_list_ids:\n                for label_value, label_count in label_list.label_count().items():\n                    count[label_value] += label_count\n\n        return count", "response": "Returns a dictionary containing the number of times every label - value in this utterance is occurring."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of all tokens occurring in one of the labels in the label - lists with .", "response": "def all_tokens(self, delimiter=' ', label_list_ids=None):\n        \"\"\"\n        Return a list of all tokens occurring in\n        one of the labels in the label-lists.\n\n        Args:\n            delimiter (str): The delimiter used to split labels into tokens\n                             (see :meth:`audiomate.annotations.Label.tokenized`).\n            label_list_ids (list): If not None, only labels from label-lists with\n                                   an idx contained in this list are considered.\n\n        Returns:\n             :class:`set`: A set of distinct tokens.\n        \"\"\"\n        tokens = set()\n\n        for label_list in self.label_lists.values():\n            if label_list_ids is None or label_list.idx in label_list_ids:\n                tokens = tokens.union(label_list.all_tokens(delimiter=delimiter))\n\n        return tokens"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a dictionary containing the number of seconds every label - value occursring in this utterance.", "response": "def label_total_duration(self, label_list_ids=None):\n        \"\"\"\n        Return a dictionary containing the number of seconds,\n        every label-value is occurring in this utterance.\n\n        Args:\n            label_list_ids (list): If not None, only labels from label-lists\n                                   with an id contained in this\n                                   list are considered.\n\n        Returns:\n            dict: A dictionary containing the number of seconds\n                  with the label-value as key.\n        \"\"\"\n        duration = collections.defaultdict(float)\n\n        for label_list in self.label_lists.values():\n            if label_list_ids is None or label_list.idx in label_list_ids:\n                for label_value, label_duration in label_list.label_total_duration().items():\n                    duration[label_value] += label_duration\n\n        return duration"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef split(self, cutting_points, track_relative=False, overlap=0.0):\n\n        if not track_relative:\n            cutting_points = [c + self.start for c in cutting_points]\n\n        if len(cutting_points) == 0:\n            raise ValueError('At least 1 cutting point is needed!')\n\n        splitted_label_lists = collections.defaultdict(list)\n\n        for idx, label_list in self.label_lists.items():\n            label_cutting_points = [x - self.start for x in cutting_points]\n            parts = label_list.split(\n                label_cutting_points,\n                shift_times=True,\n                overlap=overlap\n            )\n            splitted_label_lists[idx] = parts\n\n        # Only consider cutting-points within utterance.\n        filtered_cutting_points = []\n\n        for cutting_point in cutting_points:\n            if cutting_point > self.start and cutting_point < self.end:\n                filtered_cutting_points.append(cutting_point)\n\n        sub_utterances = []\n\n        for index in range(len(filtered_cutting_points) + 1):\n            if index == 0:\n                sub_start = self.start\n            else:\n                sub_start = max(self.start, cutting_points[index - 1] - overlap)\n\n            if index >= len(filtered_cutting_points):\n                sub_end = self.end\n            else:\n                sub_end = min(self.end, filtered_cutting_points[index] + overlap)\n\n            new_idx = '{}_{}'.format(self.idx, index)\n            new_utt = Utterance(\n                new_idx,\n                track=self.track,\n                issuer=self.issuer,\n                start=sub_start,\n                end=sub_end\n            )\n\n            for parts in splitted_label_lists.values():\n                new_utt.set_label_list(parts[index])\n\n            sub_utterances.append(new_utt)\n\n        return sub_utterances", "response": "Split the utterance into x parts and return them as new utterances."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef stats(self):\n        self.raise_error_if_not_open()\n\n        per_key_stats = self.stats_per_key()\n\n        return stats.DataStats.concatenate(per_key_stats.values())", "response": "Return statistics calculated overall features in the feature container."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef stats_per_key(self):\n        self.raise_error_if_not_open()\n\n        all_stats = {}\n\n        for key, data in self._file.items():\n            data = data[()]\n            all_stats[key] = stats.DataStats(float(np.mean(data)),\n                                             float(np.var(data)),\n                                             np.min(data),\n                                             np.max(data),\n                                             data.size)\n\n        return all_stats", "response": "Return statistics calculated for each key in the feature container."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef all_label_values(self, label_list_ids=None):\n        values = set()\n\n        for utterance in self.utterances.values():\n            values = values.union(utterance.all_label_values(label_list_ids=label_list_ids))\n\n        return values", "response": "Return a set of all label - values occurring in this corpus."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a dictionary containing the number of times every label - value in this corpus is occurring.", "response": "def label_count(self, label_list_ids=None):\n        \"\"\"\n        Return a dictionary containing the number of times, every label-value in this corpus is occurring.\n\n        Args:\n            label_list_ids (list): If not None, only labels from label-lists with an id contained in this list\n                                   are considered.\n\n        Returns:\n            dict: A dictionary containing the number of occurrences with the label-value as key.\n        \"\"\"\n        count = collections.defaultdict(int)\n\n        for utterance in self.utterances.values():\n            for label_value, utt_count in utterance.label_count(label_list_ids=label_list_ids).items():\n                count[label_value] += utt_count\n\n        return count"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a dictionary containing the total duration of every label - value in this corpus is occurring.", "response": "def label_durations(self, label_list_ids=None):\n        \"\"\"\n        Return a dictionary containing the total duration, every label-value in this corpus is occurring.\n\n        Args:\n            label_list_ids (list): If not None, only labels from label-lists with an id contained in this list\n                                   are considered.\n\n        Returns:\n            dict: A dictionary containing the total duration with the label-value as key.\n        \"\"\"\n        duration = collections.defaultdict(int)\n\n        for utterance in self.utterances.values():\n            for label_value, utt_count in utterance.label_total_duration(label_list_ids=label_list_ids).items():\n                duration[label_value] += utt_count\n\n        return duration"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef all_tokens(self, delimiter=' ', label_list_ids=None):\n        tokens = set()\n\n        for utterance in self.utterances.values():\n            tokens = tokens.union(utterance.all_tokens(delimiter=delimiter, label_list_ids=label_list_ids))\n\n        return tokens", "response": "Return a list of all tokens occurring in one of the labels in the corpus."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the total amount of audio summed over all utterances in the corpus in seconds.", "response": "def total_duration(self):\n        \"\"\"\n        Return the total amount of audio summed over all utterances in the corpus in seconds.\n        \"\"\"\n        duration = 0\n\n        for utterance in self.utterances.values():\n            duration += utterance.duration\n\n        return duration"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef stats(self):\n\n        per_utt_stats = self.stats_per_utterance()\n        return stats.DataStats.concatenate(per_utt_stats.values())", "response": "Returns statistics calculated overall samples of all utterances in the corpus."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns statistics calculated for all samples of each utterance in the corpus.", "response": "def stats_per_utterance(self):\n        \"\"\"\n        Return statistics calculated for all samples of each utterance in the corpus.\n\n        Returns:\n            dict: A dictionary containing a DataStats object for each utt.\n        \"\"\"\n\n        all_stats = {}\n\n        for utterance in self.utterances.values():\n            data = utterance.read_samples()\n            all_stats[utterance.idx] = stats.DataStats(float(np.mean(data)),\n                                                       float(np.var(data)),\n                                                       np.min(data),\n                                                       np.max(data),\n                                                       data.size)\n\n        return all_stats"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a new corpus where all the utterances are of given maximal duration.", "response": "def split_utterances_to_max_time(self, max_time=60.0, overlap=0.0):\n        \"\"\"\n        Create a new corpus, where all the utterances are of given maximal duration.\n        Utterance longer than ``max_time`` are split up into multiple utterances.\n\n        .. warning::\n            Subviews and FeatureContainers are not added to the newly create corpus.\n\n        Arguments:\n            max_time (float): Maximal duration for target utterances in seconds.\n            overlap (float): Amount of overlap in seconds.\n                             The overlap is measured from the center of the splitting.\n                             (The actual overlap of two segments is 2 * overlap)\n\n        Returns:\n            Corpus: A new corpus instance.\n        \"\"\"\n\n        from audiomate.corpus import Corpus\n\n        result = Corpus()\n\n        # Copy Tracks\n        tracks = copy.deepcopy(list(self.tracks.values()))\n        result.import_tracks(tracks)\n\n        # Copy Issuers\n        issuers = copy.deepcopy(list(self.issuers.values()))\n        result.import_issuers(issuers)\n\n        for utterance in self.utterances.values():\n            orig_dur = utterance.duration\n\n            if orig_dur > max_time:\n                # Compute times where the utterance is split\n                num_sub_utts = math.ceil(orig_dur / max_time)\n                sub_utt_dur = orig_dur / num_sub_utts\n                cutting_points = []\n\n                for i in range(1, num_sub_utts):\n                    cutting_points.append(i * sub_utt_dur)\n\n                sub_utts = utterance.split(cutting_points, overlap=overlap)\n\n                # Set track/issuer from new corpus\n                for sub_utt in sub_utts:\n                    sub_utt.track = result.tracks[utterance.track.idx]\n\n                    if utterance.issuer is not None:\n                        sub_utt.issuer = result.issuers[utterance.issuer.idx]\n\n                result.import_utterances(sub_utts)\n\n            # If utterance <= max_time, just copy\n            else:\n                new_utt = copy.deepcopy(utterance)\n                new_utt.track = result.tracks[new_utt.track.idx]\n\n                if new_utt.issuer is not None:\n                    new_utt.issuer = result.issuers[new_utt.issuer.idx]\n\n                result.import_utterances(new_utt)\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef longest_utterances_per_container(self):\n        lengths = []\n\n        for cnt in self.containers:\n            longest_in_container = 0\n            for utt_idx in self.utt_ids:\n                utt_length = cnt._file[utt_idx].shape[0]\n                longest_in_container = max(utt_length, longest_in_container)\n\n            lengths.append(longest_in_container)\n\n        return lengths", "response": "Return a tuple containing the length of the longest utterance of ever container."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a partitioning iterator over the data in the UTT table.", "response": "def partitioned_iterator(self, partition_size, shuffle=True, seed=None):\n        \"\"\"\n        Return a partitioning :class:`audiomate.feeding.MultiFrameIterator` for the dataset.\n\n        Args:\n            partition_size (str): Size of the partitions in bytes. The units ``k`` (kibibytes), ``m``\n                                  (mebibytes) and ``g`` (gibibytes) are supported, i.e. a ``partition_size``\n                                  of ``1g`` equates :math:`2^{30}` bytes.\n            shuffle (bool): Indicates whether the data should be returned in\n                            random order (``True``) or not (``False``).\n            seed (int): Seed to be used for the random number generator.\n\n        Returns:\n            MultiFrameIterator: A partition iterator over the dataset.\n        \"\"\"\n        return iterator.MultiFrameIterator(self.utt_ids, self.containers, partition_size, self.frames_per_chunk,\n                                           return_length=self.return_length, pad=self.pad, shuffle=shuffle, seed=seed)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_utt_regions(self):\n\n        regions = []\n        current_offset = 0\n\n        for utt_idx in sorted(self.utt_ids):\n            offset = current_offset\n\n            num_frames = []\n            refs = []\n\n            for cnt in self.containers:\n                num_frames.append(cnt.get(utt_idx).shape[0])\n                refs.append(cnt.get(utt_idx, mem_map=True))\n\n            if len(set(num_frames)) != 1:\n                raise ValueError('Utterance {} has not the same number of frames in all containers!'.format(utt_idx))\n\n            num_chunks = math.ceil(num_frames[0] / float(self.frames_per_chunk))\n\n            region = (offset, num_chunks, refs)\n            regions.append(region)\n\n            # Sets the offset for the next utterances\n            current_offset += num_chunks\n\n        return regions", "response": "Returns the regions of all utterances assuming all utterances are concatenated."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef partitioned_iterator(self, partition_size, shuffle=True, seed=None):\n        return iterator.FrameIterator(self.utt_ids, self.containers, partition_size, shuffle=shuffle, seed=seed)", "response": "Returns an iterator over the data in the specified size."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the samples for the given key and the sampling - rate.", "response": "def get(self, key, mem_map=True):\n        \"\"\"\n        Return the samples for the given key and the sampling-rate.\n\n        Args:\n            key (str): The key to read the data from.\n            mem_map (bool): If ``True`` returns the data as\n                            memory-mapped array, otherwise a copy is returned.\n\n        Note:\n            The container has to be opened in advance.\n\n        Returns:\n            tuple: A tuple containing the samples as numpy array\n                   with ``np.float32`` [-1.0,1.0] and the sampling-rate.\n        \"\"\"\n        self.raise_error_if_not_open()\n\n        if key in self._file:\n            data = self._file[key]\n            sampling_rate = data.attrs[SAMPLING_RATE_ATTR]\n\n            if not mem_map:\n                data = data[()]\n\n            data = np.float32(data) / MAX_INT16_VALUE\n\n            return data, sampling_rate"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the samples and sampling - rate for the given key.", "response": "def set(self, key, samples, sampling_rate):\n        \"\"\"\n        Set the samples and sampling-rate for the given key.\n        Existing data will be overwritten.\n        The samples have to have ``np.float32`` datatype and values in\n        the range of -1.0 and 1.0.\n\n        Args:\n            key (str): A key to store the data for.\n            samples (numpy.ndarray): 1-D array of audio samples (np.float32).\n            sampling_rate (int): The sampling-rate of the audio samples.\n\n        Note:\n            The container has to be opened in advance.\n        \"\"\"\n        if not np.issubdtype(samples.dtype, np.floating):\n            raise ValueError('Samples are required as np.float32!')\n\n        if len(samples.shape) > 1:\n            raise ValueError('Only single channel supported!')\n\n        self.raise_error_if_not_open()\n\n        if key in self._file:\n            del self._file[key]\n\n        samples = (samples * MAX_INT16_VALUE).astype(np.int16)\n\n        dset = self._file.create_dataset(key, data=samples)\n        dset.attrs[SAMPLING_RATE_ATTR] = sampling_rate"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nappending the given samples to the HDF5 - Dataset for the given key.", "response": "def append(self, key, samples, sampling_rate):\n        \"\"\"\n        Append the given samples to the data that already exists\n        in the container for the given key.\n\n        Args:\n            key (str): A key to store the data for.\n            samples (numpy.ndarray): 1-D array of audio samples (int-16).\n            sampling_rate (int): The sampling-rate of the audio samples.\n\n        Note:\n            The container has to be opened in advance.\n            For appending to existing data the HDF5-Dataset has to be chunked,\n            so it is not allowed to first add data via ``set``.\n        \"\"\"\n        if not np.issubdtype(samples.dtype, np.floating):\n            raise ValueError('Samples are required as np.float32!')\n\n        if len(samples.shape) > 1:\n            raise ValueError('Only single channel supported!')\n\n        existing = self.get(key, mem_map=True)\n        samples = (samples * MAX_INT16_VALUE).astype(np.int16)\n\n        if existing is not None:\n            existing_samples, existing_sr = existing\n\n            if existing_sr != sampling_rate:\n                raise ValueError('Different sampling-rate than existing data!')\n\n            num_existing = existing_samples.shape[0]\n            self._file[key].resize(num_existing + samples.shape[0], 0)\n            self._file[key][num_existing:] = samples\n        else:\n            dset = self._file.create_dataset(key, data=samples,\n                                             chunks=True, maxshape=(None,))\n\n            dset.attrs[SAMPLING_RATE_ATTR] = sampling_rate"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef available_files(url):\n        req = requests.get(url)\n\n        if req.status_code != 200:\n            raise base.FailedDownloadException('Failed to download data (status {}) from {}!'.format(req.status_code,\n                                                                                                     url))\n\n        page_content = req.text\n        link_pattern = re.compile(r'<a href=\"(.*?)\">(.*?)</a>')\n        available_files = []\n\n        for match in link_pattern.findall(page_content):\n            if match[0].endswith('.tgz'):\n                available_files.append(os.path.join(url, match[0]))\n\n        return available_files", "response": "Extract and return urls for all available. tgz files."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndownloading all files and store them to the given path.", "response": "def download_files(file_urls, target_path):\n        \"\"\" Download all files and store to the given path. \"\"\"\n        os.makedirs(target_path, exist_ok=True)\n        downloaded_files = []\n\n        for file_url in file_urls:\n            req = requests.get(file_url)\n\n            if req.status_code != 200:\n                raise base.FailedDownloadException('Failed to download file {} (status {})!'.format(req.status_code,\n                                                                                                    file_url))\n\n            file_name = os.path.basename(file_url)\n            target_file_path = os.path.join(target_path, file_name)\n\n            with open(target_file_path, 'wb') as f:\n                f.write(req.content)\n\n            downloaded_files.append(target_file_path)\n\n        return downloaded_files"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nunpacks all files to the given path.", "response": "def extract_files(file_paths, target_path):\n        \"\"\" Unpack all files to the given path. \"\"\"\n        os.makedirs(target_path, exist_ok=True)\n        extracted = []\n\n        for file_path in file_paths:\n            with tarfile.open(file_path, 'r') as archive:\n                archive.extractall(target_path)\n\n            file_name = os.path.splitext(os.path.basename(file_path))[0]\n            extracted.append(os.path.join(target_path, file_name))\n\n        return extracted"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef data_folders(path):\n        for item in os.listdir(path):\n            dir_path = os.path.join(path, item)\n            wav_folder = os.path.join(dir_path, 'wav')\n\n            if os.path.isdir(dir_path) and os.path.isdir(wav_folder):\n                yield dir_path", "response": "Generator which yields a list of valid data directories."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_speaker_info(readme_path):\n        idx = None\n        gender = issuers.Gender.UNKNOWN\n        age_group = issuers.AgeGroup.UNKNOWN\n        native_lang = None\n\n        with open(readme_path, 'r', errors='ignore') as f:\n            for raw_line in f:\n                line = raw_line.strip()\n\n                if line is not None and line is not '':\n                    line = line.rstrip(';.')\n                    parts = line.split(':', maxsplit=1)\n\n                    if len(parts) > 1:\n                        key = parts[0].strip().lower()\n                        value = parts[1].strip()\n\n                        if key == 'user name':\n                            idx = value\n\n                        value = value.lower()\n\n                        if key == 'gender':\n                            if value in ['m\u00e4nnlich', 'male', 'mnnlich']:\n                                gender = issuers.Gender.MALE\n                            elif value in ['weiblich', 'female', '[female]']:\n                                gender = issuers.Gender.FEMALE\n\n                        if key == 'age range':\n                            if value in ['erwachsener', 'adult', '[adult]', '[erwachsener]']:\n                                age_group = issuers.AgeGroup.ADULT\n                            elif value in ['senior', '[senior']:\n                                age_group = issuers.AgeGroup.SENIOR\n                            elif value in ['youth', 'jugendlicher', '[youth]', '[jugendlicher]']:\n                                age_group = issuers.AgeGroup.YOUTH\n                            elif value in ['kind', 'child']:\n                                age_group = issuers.AgeGroup.CHILD\n\n                        if key == 'language':\n                            if value in ['de', 'ger', 'deu', '[de]']:\n                                native_lang = 'deu'\n                            elif value in ['en', 'eng', '[en]']:\n                                native_lang = 'eng'\n\n        return issuers.Speaker(idx, gender=gender, age_group=age_group, native_language=native_lang)", "response": "Parse the speaker info from a file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_prompts(etc_folder):\n        prompts_path = os.path.join(etc_folder, 'PROMPTS')\n        prompts_orig_path = os.path.join(etc_folder, 'prompts-original')\n\n        prompts = textfile.read_key_value_lines(prompts_path, separator=' ')\n        prompts_orig = textfile.read_key_value_lines(prompts_orig_path, separator=' ')\n\n        prompts_key_fixed = {}\n\n        for k, v in prompts.items():\n            parts = k.split('/')\n            key = k\n\n            if len(parts) > 1:\n                key = parts[-1]\n\n            prompts_key_fixed[key] = v\n\n        prompts = prompts_key_fixed\n\n        return prompts, prompts_orig", "response": "Read prompts and prompts - orignal and return as dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef index_name_if_in_list(name, name_list, suffix='', prefix=''):\n    new_name = '{}'.format(name)\n    index = 1\n\n    while new_name in name_list:\n        new_name = '{}_{}{}{}'.format(name, prefix, index, suffix)\n        index += 1\n\n    return new_name", "response": "Find a unique name by adding an index to the name so it is unique within the given list."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating a random string of lowercase letters with the given length.", "response": "def generate_name(length=15, not_in=None):\n    \"\"\"\n    Generates a random string of lowercase letters with the given length.\n\n    Parameters:\n        length (int): Length of the string to output.\n        not_in (list): Only return a string not in the given iterator.\n\n    Returns:\n        str: A new name thats not in the given list.\n    \"\"\"\n    value = ''.join(random.choice(string.ascii_lowercase) for i in range(length))\n\n    while (not_in is not None) and (value in not_in):\n        value = ''.join(random.choice(string.ascii_lowercase) for i in range(length))\n\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_downloader_of_type(type_name):\n    downloaders = available_downloaders()\n\n    if type_name not in downloaders.keys():\n        raise UnknownDownloaderException('Unknown downloader: %s' % (type_name,))\n\n    return downloaders[type_name]()", "response": "Create an instance of a downloader of the given type."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_reader_of_type(type_name):\n    readers = available_readers()\n\n    if type_name not in readers.keys():\n        raise UnknownReaderException('Unknown reader: %s' % (type_name,))\n\n    return readers[type_name]()", "response": "Create an instance of a reader of the given type."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates an instance of the given type.", "response": "def create_writer_of_type(type_name):\n    \"\"\"\n        Create an instance of the writer with the given name.\n\n        Args:\n            type_name: The name of a writer.\n\n        Returns:\n            An instance of the writer with the given type.\n    \"\"\"\n    writers = available_writers()\n\n    if type_name not in writers.keys():\n        raise UnknownWriterException('Unknown writer: %s' % (type_name,))\n\n    return writers[type_name]()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a string representing the subview with all of its filter criteria.", "response": "def serialize(self):\n        \"\"\"\n        Return a string representing the subview with all of its filter criteria.\n\n        Returns:\n            str: String with subview definition.\n        \"\"\"\n        lines = []\n\n        for criterion in self.filter_criteria:\n            lines.append(criterion.name())\n            lines.append(criterion.serialize())\n\n        return '\\n'.join(lines)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse(cls, representation, corpus=None):\n\n        criteria_definitions = representation.split('\\n')\n        criteria = []\n\n        for i in range(0, len(criteria_definitions), 2):\n            filter_name = criteria_definitions[i]\n            filter_repr = criteria_definitions[i + 1]\n\n            if filter_name not in available_filter_criteria():\n                raise UnknownFilterCriteriaException('Unknown filter-criterion {}'.format(filter_name))\n\n            criterion = available_filter_criteria()[filter_name].parse(filter_repr)\n            criteria.append(criterion)\n\n        return cls(corpus, criteria)", "response": "Creates a subview from a string representation."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the short - time - fourier - transfrom from a signal.", "response": "def stft_from_frames(frames, window='hann', dtype=np.complex64):\n    \"\"\"\n    Variation of the librosa.core.stft function,\n    that computes the short-time-fourier-transfrom from frames instead from the signal.\n\n    See http://librosa.github.io/librosa/_modules/librosa/core/spectrum.html#stft\n    \"\"\"\n\n    win_length = frames.shape[0]\n    n_fft = win_length\n\n    fft_window = filters.get_window(window, win_length, fftbins=True)\n\n    # Reshape so that the window can be broadcast\n    fft_window = fft_window.reshape((-1, 1))\n\n    # Pre-allocate the STFT matrix\n    stft_matrix = np.empty((int(1 + n_fft // 2), frames.shape[1]),\n                           dtype=dtype,\n                           order='F')\n\n    # how many columns can we fit within MAX_MEM_BLOCK?\n    n_columns = int(util.MAX_MEM_BLOCK / (stft_matrix.shape[0] *\n                                          stft_matrix.itemsize))\n\n    for bl_s in range(0, stft_matrix.shape[1], n_columns):\n        bl_t = min(bl_s + n_columns, stft_matrix.shape[1])\n\n        # RFFT and Conjugate here to match phase from DPWE code\n        stft_matrix[:, bl_s:bl_t] = fft.fft(fft_window *\n                                            frames[:, bl_s:bl_t],\n                                            axis=0)[:stft_matrix.shape[0]].conj()\n\n    return stft_matrix"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nperforms validation on the given corpus.", "response": "def validate(self, corpus):\n        \"\"\"\n        Perform validation on the given corpus.\n\n        Args:\n            corpus (Corpus): The corpus to test/validate.\n        \"\"\"\n\n        passed = True\n        results = {}\n\n        for validator in self.validators:\n            sub_result = validator.validate(corpus)\n            results[validator.name()] = sub_result\n\n            if not sub_result.passed:\n                passed = False\n\n        return CombinedValidationResult(passed, results)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nperforms the validation on the given corpus.", "response": "def validate(self, corpus):\n        \"\"\"\n        Perform the validation on the given corpus.\n\n        Args:\n            corpus (Corpus): The corpus to test/validate.\n\n        Returns:\n            InvalidUtterancesResult: Validation result.\n        \"\"\"\n        invalid_utterances = {}\n\n        for utterance in corpus.utterances.values():\n            duration = utterance.duration\n            ll = utterance.label_lists[self.label_list_idx]\n\n            # We count the characters of all labels\n            transcription = ' '.join([l.value for l in ll])\n            num_chars = len(transcription.replace(' ', ''))\n\n            char_per_sec = num_chars / duration\n\n            if char_per_sec > self.max_characters_per_second:\n                invalid_utterances[utterance.idx] = char_per_sec\n\n        passed = len(invalid_utterances) <= 0\n        info = {\n            'Threshold max. characters per second': str(self.max_characters_per_second),\n            'Label-List ID': self.label_list_idx\n        }\n\n        return base.InvalidUtterancesResult(passed, invalid_utterances, name=self.name(), info=info)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nperforming the validation on the given corpus.", "response": "def validate(self, corpus):\n        \"\"\"\n        Perform the validation on the given corpus.\n\n        Args:\n            corpus (Corpus): The corpus to test/validate.\n\n        Returns:\n            InvalidUtterancesResult: Validation result.\n        \"\"\"\n        invalid_utterances = {}\n\n        for utterance in corpus.utterances.values():\n            if self.label_list_idx in utterance.label_lists.keys():\n                ll = utterance.label_lists[self.label_list_idx]\n\n                if len(ll) < self.min_number_of_labels:\n                    invalid_utterances[utterance.idx] = 'Only {} labels'.format(len(ll))\n            else:\n                invalid_utterances[utterance.idx] = 'No label-list {}'.format(self.label_list_idx)\n\n        passed = len(invalid_utterances) <= 0\n        info = {\n            'Min. number of labels': str(self.min_number_of_labels),\n            'Label-List ID': self.label_list_idx\n        }\n\n        return base.InvalidUtterancesResult(passed, invalid_utterances, name=self.name(), info=info)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_report(self):\n\n        lines = [super(LabelCoverageValidationResult, self).get_report()]\n\n        if len(self.uncovered_segments) > 0:\n            lines.append('\\nUncovered segments:')\n\n            for utt_idx, utt_segments in self.uncovered_segments.items():\n                if len(utt_segments) > 0:\n                    lines.append('\\n{}'.format(utt_idx))\n                    sorted_items = sorted(utt_segments, key=lambda x: x[0])\n                    lines.extend(['    * {:10.2f}  -  {:10.2f}'.format(x[0], x[1]) for x in sorted_items])\n\n        return '\\n'.join(lines)", "response": "Return a string containing a report of the result."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef validate_utterance(self, utterance):\n        uncovered_segments = []\n\n        if self.label_list_idx in utterance.label_lists.keys():\n            start = 0\n            end = utterance.duration\n            ll = utterance.label_lists[self.label_list_idx]\n            ranges = list(ll.ranges(yield_ranges_without_labels=True))\n\n            # Check coverage at start\n            if ranges[0][0] - start > self.threshold:\n                uncovered_segments.append((start, ranges[0][0]))\n\n            # Check for empty ranges\n            for range in ranges:\n                if len(range[2]) == 0 and range[1] - range[0] > self.threshold:\n                    uncovered_segments.append((range[0], range[1]))\n\n            # Check coverage at end\n            if ranges[-1][1] > 0 and end - ranges[-1][1] > self.threshold:\n                uncovered_segments.append((ranges[-1][1], end))\n\n        else:\n            uncovered_segments.append((utterance.start, utterance.end))\n\n        return uncovered_segments", "response": "Validate the given utterance and return a list of uncovered segments."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef validate(self, corpus):\n\n        overflow_segments = {}\n\n        for utterance in corpus.utterances.values():\n            utt_segments = self.validate_utterance(utterance)\n\n            if len(utt_segments) > 0:\n                overflow_segments[utterance.idx] = utt_segments\n\n        passed = len(overflow_segments) <= 0\n        info = {\n            'Label-List ID': self.label_list_idx,\n            'Threshold': str(self.threshold)\n        }\n\n        return LabelOverflowValidationResult(passed, overflow_segments, self.name(), info)", "response": "Perform the validation on the given corpus."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nvalidate the given utterance and return a list of segments that are outside of the utterance.", "response": "def validate_utterance(self, utterance):\n        \"\"\"\n        Validate the given utterance and return a list of segments (start, end, label-value),\n        that are outside of the utterance.\n        \"\"\"\n        overflow_segments = []\n\n        if self.label_list_idx in utterance.label_lists.keys():\n            ll = utterance.label_lists[self.label_list_idx]\n            start = 0\n            end = utterance.duration\n\n            for label in ll:\n                if start - label.start > self.threshold:\n                    label_end = label.end if label.end != float('inf') else end\n                    overflow_end = min(start, label_end)\n                    overflow_segments.append((label.start, overflow_end, label.value))\n\n                if label.end != float('inf') and label.end - end > self.threshold:\n                    overflow_start = max(end, label.start)\n                    overflow_segments.append((overflow_start, label.end, label.value))\n\n        return overflow_segments"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef feature_scp_generator(path):\n\n        scp_entries = textfile.read_key_value_lines(path, separator=' ')\n\n        for utterance_id, rx_specifier in scp_entries.items():\n            yield utterance_id, KaldiWriter.read_float_matrix(rx_specifier)", "response": "Return a generator over all feature matrices defined in a scp."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef read_float_matrix(rx_specifier):\n\n        path, offset = rx_specifier.strip().split(':', maxsplit=1)\n        offset = int(offset)\n        sample_format = 4\n\n        with open(path, 'rb') as f:\n            # move to offset\n            f.seek(offset)\n\n            # assert binary ark\n            binary = f.read(2)\n            assert (binary == b'\\x00B')\n\n            # assert type float 32\n            format = f.read(3)\n            assert (format == b'FM ')\n\n            # get number of mfcc features\n            f.read(1)\n            num_frames = struct.unpack('<i', f.read(4))[0]\n\n            # get size of mfcc features\n            f.read(1)\n            feature_size = struct.unpack('<i', f.read(4))[0]\n\n            # read feature data\n            data = f.read(num_frames * feature_size * sample_format)\n\n            feature_vector = np.frombuffer(data, dtype='float32')\n            feature_matrix = np.reshape(feature_vector, (num_frames, feature_size))\n\n            return feature_matrix", "response": "Return the float matrix for the given rx specifier."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write_float_matrices(scp_path, ark_path, matrices):\n\n        scp_entries = []\n\n        with open(ark_path, 'wb') as f:\n            for utterance_id in sorted(list(matrices.keys())):\n                matrix = matrices[utterance_id]\n\n                assert (matrix.dtype == np.float32)\n\n                f.write(('{} '.format(utterance_id)).encode('utf-8'))\n\n                offset = f.tell()\n\n                f.write(b'\\x00B')\n                f.write(b'FM ')\n                f.write(b'\\x04')\n                f.write(struct.pack('<i', np.size(matrix, 0)))\n                f.write(b'\\x04')\n                f.write(struct.pack('<i', np.size(matrix, 1)))\n\n                flattened = matrix.reshape(np.size(matrix, 0) * np.size(matrix, 1))\n                flattened.tofile(f, sep='')\n\n                scp_entries.append('{} {}:{}'.format(utterance_id, ark_path, offset))\n\n        with open(scp_path, 'w') as f:\n            f.write('\\n'.join(scp_entries))", "response": "Write the given dict matrices to the given scp and ark files."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef encode_corpus(self, corpus, output_path):\n\n        out_container = containers.Container(output_path)\n        out_container.open()\n\n        for utterance in corpus.utterances.values():\n            data = self.encode_utterance(utterance, corpus=corpus)\n            out_container.set(utterance.idx, data)\n\n        out_container.close()\n        return out_container", "response": "Encodes all utterances of the given corpus and stores them in a container."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef reload(self):\n\n        # Create the order in which utterances will be loaded\n        utt_ids = sorted(self.utt_ids)\n\n        if self.shuffle:\n            self.rand.shuffle(utt_ids)\n\n        partitions = []\n\n        current_partition = PartitionInfo()\n\n        for utt_id in utt_ids:\n            utt_size = self.utt_sizes[utt_id]\n            utt_lengths = self.utt_lengths[utt_id]\n\n            # We add utterance to the partition as long the partition-size is not exceeded\n            # Otherwise we start with new partition.\n            if current_partition.size + utt_size > self.partition_size:\n                partitions.append(current_partition)\n                current_partition = PartitionInfo()\n\n            current_partition.utt_ids.append(utt_id)\n            current_partition.utt_lengths.append(utt_lengths)\n            current_partition.size += utt_size\n\n        if current_partition.size > 0:\n            partitions.append(current_partition)\n\n        self.partitions = partitions\n        return self.partitions", "response": "Reloads the list of partitions from the current utterances."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads and return the partition with the given index.", "response": "def load_partition_data(self, index):\n        \"\"\"\n        Load and return the partition with the given index.\n\n        Args:\n            index (int): The index of partition, that refers to the index in ``self.partitions``.\n\n        Returns:\n            PartitionData: A PartitionData object containing the data for the partition with the given index.\n        \"\"\"\n\n        info = self.partitions[index]\n        data = PartitionData(info)\n\n        for utt_id in info.utt_ids:\n            utt_data = [c._file[utt_id][:] for c in self.containers]\n            data.utt_data.append(utt_data)\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _raise_error_if_container_is_missing_an_utterance(self):\n        expected_keys = frozenset(self.utt_ids)\n\n        for cnt in self.containers:\n            keys = set(cnt.keys())\n\n            if not keys.issuperset(expected_keys):\n                raise ValueError('Container is missing data for some utterances!')", "response": "Check if there is a dataset for every utterance in every container and if not raise an error."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _scan(self):\n        utt_sizes = {}\n\n        for dset_name in self.utt_ids:\n            per_container = []\n\n            for cnt in self.containers:\n                dset = cnt._file[dset_name]\n                dtype_size = dset.dtype.itemsize\n\n                record_size = dtype_size * dset.size\n                per_container.append(record_size)\n\n            utt_size = sum(per_container)\n\n            if utt_size > self.partition_size:\n                raise ValueError('Records in \"{0}\" are larger than the partition size'.format(dset_name))\n\n            utt_sizes[dset_name] = utt_size\n\n        return utt_sizes", "response": "Scan the utterance files for all utterance IDs and return a dictionary of the size of each utterance."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_all_lengths(self):\n        utt_lengths = {}\n\n        for utt_idx in self.utt_ids:\n            per_container = [c._file[utt_idx].shape[0] for c in self.containers]\n            utt_lengths[utt_idx] = tuple(per_container)\n\n        return utt_lengths", "response": "For every utterance get the length of the data in every container. Return a list of tuples."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nupdates the buffer at the given index.", "response": "def update(self, data, offset, is_last, buffer_index=0):\n        \"\"\"\n        Update the buffer at the given index.\n\n        Args:\n            data (np.ndarray): The frames.\n            offset (int): The index of the first frame in `data` within the sequence.\n            is_last (bool): Whether this is the last block of frames in the sequence.\n            buffer_index (int): The index of the buffer to update (< self.num_buffers).\n        \"\"\"\n        if buffer_index >= self.num_buffers:\n            raise ValueError('Expected buffer index < {} but got index {}.'.format(self.num_buffers, buffer_index))\n\n        if self.buffers[buffer_index] is not None and self.buffers[buffer_index].shape[0] > 0:\n            expected_next_frame = self.current_frame + self.buffers[buffer_index].shape[0]\n            if expected_next_frame != offset:\n                raise ValueError(\n                    'There are missing frames. Last frame in buffer is {}. The passed frames start at {}.'.format(\n                        expected_next_frame, offset))\n\n            self.buffers[buffer_index] = np.vstack([self.buffers[buffer_index], data])\n        else:\n            self.buffers[buffer_index] = data\n\n        self.buffers_full[buffer_index] = is_last"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get(self):\n        chunk_size = self._smallest_buffer()\n        all_full = self._all_full()\n\n        if all_full:\n            right_context = 0\n            num_frames = chunk_size - self.current_left_context\n        else:\n            right_context = self.right_context\n            num_frames = self.min_frames\n\n        chunk_size_needed = num_frames + self.current_left_context + right_context\n\n        if chunk_size >= chunk_size_needed:\n            data = []\n            keep_frames = self.left_context + self.right_context\n            keep_from = max(0, chunk_size - keep_frames)\n\n            for index in range(self.num_buffers):\n                data.append(self.buffers[index][:chunk_size])\n                self.buffers[index] = self.buffers[index][keep_from:]\n\n            if self.num_buffers == 1:\n                data = data[0]\n\n            chunk = Chunk(data,\n                          self.current_frame,\n                          all_full,\n                          self.current_left_context,\n                          right_context)\n\n            self.current_left_context = min(self.left_context, chunk_size)\n            self.current_frame = max(self.current_frame + chunk_size - keep_frames, 0)\n\n            return chunk", "response": "Get a new chunk if enough frames are available."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _smallest_buffer(self):\n\n        smallest = np.inf\n\n        for buffer in self.buffers:\n            if buffer is None:\n                return 0\n            elif buffer.shape[0] < smallest:\n                smallest = buffer.shape[0]\n\n        return smallest", "response": "Get the size of the smallest buffer."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nprocessing the data and return the data.", "response": "def process_frames(self, data, sampling_rate, offset=0, last=False, utterance=None, corpus=None):\n        \"\"\"\n        Execute the processing of this step and all dependent parent steps.\n        \"\"\"\n\n        if offset == 0:\n            self.steps_sorted = list(nx.algorithms.dag.topological_sort(self.graph))\n            self._create_buffers()\n            self._define_output_buffers()\n\n        # Update buffers with input data\n        self._update_buffers(None, data, offset, last)\n\n        # Go through the ordered (by dependencies) steps\n        for step in self.steps_sorted:\n\n            chunk = self.buffers[step].get()\n\n            if chunk is not None:\n                res = step.compute(chunk, sampling_rate, utterance=utterance, corpus=corpus)\n\n                # If step is self, we know its the last step so return the data\n                if step == self:\n                    return res\n\n                # Otherwise update buffers of child steps\n                else:\n                    self._update_buffers(step, res, chunk.offset + chunk.left_context, chunk.is_last)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating the buffers of all steps that need data from from_step.", "response": "def _update_buffers(self, from_step, data, offset, is_last):\n        \"\"\"\n        Update the buffers of all steps that need data from ``from_step``.\n        If ``from_step`` is None it means the data is the input data.\n        \"\"\"\n\n        for to_step, buffer in self.target_buffers[from_step]:\n            parent_index = 0\n\n            # if there multiple inputs we have to get the correct index, to keep the ordering\n            if isinstance(to_step, Reduction):\n                parent_index = to_step.parents.index(from_step)\n\n            buffer.update(data, offset, is_last, buffer_index=parent_index)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _define_output_buffers(self):\n\n        # First define buffers that need input data\n        self.target_buffers = {\n            None: [(step, self.buffers[step]) for step in self._get_input_steps()]\n        }\n\n        # Go through all steps and append the buffers of their child nodes\n        for step in self.steps_sorted:\n            if step != self:\n                child_steps = [edge[1] for edge in self.graph.out_edges(step)]\n                self.target_buffers[step] = [(child_step, self.buffers[child_step]) for child_step in child_steps]", "response": "Define the buffers that need output data for each node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsearch and return all steps that have no parents. These are the steps that are not in the input data.", "response": "def _get_input_steps(self):\n        \"\"\"\n        Search and return all steps that have no parents. These are the steps that are get the input data.\n        \"\"\"\n        input_steps = []\n\n        for step in self.steps_sorted:\n            parent_steps = self._parent_steps(step)\n\n            if len(parent_steps) == 0:\n                input_steps.append(step)\n\n        return input_steps"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _create_buffers(self):\n\n        self.buffers = {}\n\n        for step in self.graph.nodes():\n            num_buffers = 1\n\n            if isinstance(step, Reduction):\n                num_buffers = len(step.parents)\n\n            self.buffers[step] = Buffer(step.min_frames, step.left_context, step.right_context, num_buffers)\n\n        return self.buffers", "response": "Create a buffer for every step in the pipeline."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsave the data set at the given path.", "response": "def save(self, writer=None):\n        \"\"\"\n        If self.path is defined, it tries to save the corpus at the given path.\n        \"\"\"\n\n        if self.path is None:\n            raise ValueError('No path given to save the data set.')\n\n        self.save_at(self.path, writer)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsaving this corpus at the given path.", "response": "def save_at(self, path, writer=None):\n        \"\"\"\n        Save this corpus at the given path. If the path differs from the current path set, the path\n        gets updated.\n\n        Parameters:\n            path (str): Path to save the data set to.\n            writer (str, CorpusWriter): The writer or the name of the reader to use.\n        \"\"\"\n\n        if writer is None:\n            from . import io\n            writer = io.DefaultWriter()\n        elif type(writer) == str:\n            # If a loader is given as string, try to create such a loader.\n            from . import io\n            writer = io.create_writer_of_type(writer)\n\n        writer.save(self, path)\n\n        self.path = path"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload the corpus from the given path using the given reader.", "response": "def load(cls, path, reader=None):\n        \"\"\"\n        Loads the corpus from the given path, using the given reader. If no reader is given the\n        :py:class:`audiomate.corpus.io.DefaultReader` is used.\n\n        Args:\n            path (str): Path to load the corpus from.\n            reader (str, CorpusReader): The reader or the name of the reader to use.\n\n        Returns:\n            Corpus: The loaded corpus.\n        \"\"\"\n\n        if reader is None:\n            from . import io\n            reader = io.DefaultReader()\n\n        elif type(reader) == str:\n            from . import io\n            reader = io.create_reader_of_type(reader)\n\n        return reader.load(path)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a new audio file to the corpus with the given data.", "response": "def new_file(self, path, track_idx, copy_file=False):\n        \"\"\"\n        Adds a new audio file to the corpus with the given data.\n\n        Parameters:\n            path (str): Path of the file to add.\n            track_idx (str): The id to associate the file-track with.\n            copy_file (bool): If True the file is copied to the data set folder, otherwise the given\n                              path is used directly.\n\n        Returns:\n            FileTrack: The newly added file.\n        \"\"\"\n\n        new_file_idx = track_idx\n        new_file_path = os.path.abspath(path)\n\n        # Add index to idx if already existing\n        if new_file_idx in self._tracks.keys():\n            new_file_idx = naming.index_name_if_in_list(new_file_idx, self._tracks.keys())\n\n        # Copy file to default file dir\n        if copy_file:\n            if not os.path.isdir(self.path):\n                raise ValueError('To copy file the dataset needs to have a path.')\n\n            __, ext = os.path.splitext(path)\n\n            new_file_folder = os.path.join(self.path, DEFAULT_FILE_SUBDIR)\n            new_file_path = os.path.join(new_file_folder, '{}{}'.format(new_file_idx, ext))\n            os.makedirs(new_file_folder, exist_ok=True)\n            shutil.copy(path, new_file_path)\n\n        # Create file obj\n        new_file = tracks.FileTrack(new_file_idx, new_file_path)\n        self._tracks[new_file_idx] = new_file\n\n        return new_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds the given tracks to the corpus.", "response": "def import_tracks(self, import_tracks):\n        \"\"\"\n        Add the given tracks/track to the corpus.\n        If any of the given track-ids already exists, a suffix is appended so it is unique.\n\n        Args:\n            import_tracks (list): Either a list of or a single :py:class:`audiomate.tracks.Track`.\n\n        Returns:\n            dict: A dictionary containing track-idx mappings (old-track-idx/track-instance).\n                  If a track is imported, whose idx already exists this mapping can be used to check\n                  the new id.\n        \"\"\"\n\n        if isinstance(import_tracks, tracks.Track):\n            import_tracks = [import_tracks]\n\n        idx_mapping = {}\n\n        for track in import_tracks:\n            idx_mapping[track.idx] = track\n\n            # Add index to idx if already existing\n            if track.idx in self._tracks.keys():\n                track.idx = naming.index_name_if_in_list(track.idx, self._tracks.keys())\n\n            self._tracks[track.idx] = track\n\n        return idx_mapping"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef new_utterance(self, utterance_idx, track_idx, issuer_idx=None, start=0, end=float('inf')):\n\n        new_utt_idx = utterance_idx\n\n        # Check if there is a track with the given idx\n        if track_idx not in self._tracks.keys():\n            raise ValueError('Track with id {} does not exist!'.format(track_idx))\n\n        # Check if issuer exists\n        issuer = None\n\n        if issuer_idx is not None:\n            if issuer_idx not in self._issuers.keys():\n                raise ValueError('Issuer with id {} does not exist!'.format(issuer_idx))\n            else:\n                issuer = self._issuers[issuer_idx]\n\n        # Add index to idx if already existing\n        if new_utt_idx in self._utterances.keys():\n            new_utt_idx = naming.index_name_if_in_list(new_utt_idx, self._utterances.keys())\n\n        new_utt = tracks.Utterance(new_utt_idx,\n                                   self.tracks[track_idx],\n                                   issuer=issuer,\n                                   start=start,\n                                   end=end)\n\n        self._utterances[new_utt_idx] = new_utt\n\n        return new_utt", "response": "Adds a new utterance to the corpus with the given data."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds the given utterances to the corpus.", "response": "def import_utterances(self, utterances):\n        \"\"\"\n        Add the given utterances/utterance to the corpus.\n        If any of the given utterance-ids already exists, a suffix is appended so it is unique.\n\n        Args:\n            utterances (list): Either a list of or a single :py:class:`audiomate.tracks.Utterance`.\n\n        Returns:\n            dict: A dictionary containing idx mappings (old-utterance-idx/utterance-instance).\n                  If a utterance is imported, whose id already exists this mapping can be used to\n                  check the new id.\n        \"\"\"\n\n        if isinstance(utterances, tracks.Utterance):\n            utterances = [utterances]\n\n        idx_mapping = {}\n\n        for utterance in utterances:\n            idx_mapping[utterance.idx] = utterance\n\n            # Check if there is a track with the given idx\n            if utterance.track not in self._tracks.values():\n                raise ValueError('Track with id {} is not in the corpus.'.format(utterance.track.idx, utterance.idx))\n\n            # Check if there is a issuer with the given idx\n            if utterance.issuer is not None and utterance.issuer not in self._issuers.values():\n                raise ValueError('No issuer in corpus with id {} to add utterance {}.'.format(\n                    utterance.issuer.idx, utterance.idx))\n\n            # Add index to idx if already existing\n            if utterance.idx in self._utterances.keys():\n                utterance.idx = naming.index_name_if_in_list(utterance.idx, self._utterances.keys())\n\n            self._utterances[utterance.idx] = utterance\n\n        return idx_mapping"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef new_issuer(self, issuer_idx, info=None):\n\n        new_issuer_idx = issuer_idx\n\n        # Add index to idx if already existing\n        if new_issuer_idx in self._issuers.keys():\n            new_issuer_idx = naming.index_name_if_in_list(new_issuer_idx, self._issuers.keys())\n\n        new_issuer = issuers.Issuer(new_issuer_idx, info=info)\n        self._issuers[new_issuer_idx] = new_issuer\n\n        return new_issuer", "response": "Adds a new issuer to the dataset with the given data."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds the given issuers to the corpus.", "response": "def import_issuers(self, new_issuers):\n        \"\"\"\n        Add the given issuers/issuer to the corpus.\n        If any of the given issuer-ids already exists, a suffix is appended so it is unique.\n\n        Args:\n            issuers (list): Either a list of or a single :py:class:`audiomate.issuers.Issuer`.\n\n        Returns:\n            dict: A dictionary containing idx mappings (old-issuer-idx/issuer-instance).\n                  If a issuer is imported, whose id already exists this mapping can be used to check\n                  the new id.\n        \"\"\"\n\n        if isinstance(new_issuers, issuers.Issuer):\n            new_issuers = [new_issuers]\n\n        idx_mapping = {}\n\n        for issuer in new_issuers:\n            idx_mapping[issuer.idx] = issuer\n\n            # Add index to idx if already existing\n            if issuer.idx in self._issuers.keys():\n                issuer.idx = naming.index_name_if_in_list(issuer.idx, self._issuers.keys())\n\n            self._issuers[issuer.idx] = issuer\n\n        return idx_mapping"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef new_feature_container(self, idx, path=None):\n\n        new_feature_idx = idx\n        new_feature_path = path\n\n        # Add index to idx if already existing\n        if new_feature_idx in self._feature_containers.keys():\n            new_feature_idx = naming.index_name_if_in_list(new_feature_idx,\n                                                           self._feature_containers.keys())\n\n        # Set default path if none given\n        if new_feature_path is None:\n            if not os.path.isdir(self.path):\n                raise ValueError('To copy file the dataset needs to have a path.')\n\n            new_feature_path = os.path.join(self.path, DEFAULT_FEAT_SUBDIR, new_feature_idx)\n        else:\n            new_feature_path = os.path.abspath(new_feature_path)\n\n        feat_container = containers.FeatureContainer(new_feature_path)\n        self._feature_containers[new_feature_idx] = feat_container\n\n        return feat_container", "response": "Adds a new feature container with the given data."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef import_subview(self, idx, subview):\n\n        subview.corpus = self\n        self._subviews[idx] = subview", "response": "Adds the given subview to the corpus."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef merge_corpus(self, corpus):\n\n        # Create a copy, so objects aren't changed in the original merging corpus\n        merging_corpus = Corpus.from_corpus(corpus)\n\n        self.import_tracks(corpus.tracks.values())\n        self.import_issuers(corpus.issuers.values())\n        utterance_idx_mapping = self.import_utterances(corpus.utterances.values())\n\n        for subview_idx, subview in merging_corpus.subviews.items():\n            for filter in subview.filter_criteria:\n                if isinstance(filter, subset.MatchingUtteranceIdxFilter):\n                    new_filtered_utt_ids = set()\n                    for utt_idx in filter.utterance_idxs:\n                        new_filtered_utt_ids.add(utterance_idx_mapping[utt_idx].idx)\n                    filter.utterance_idxs = new_filtered_utt_ids\n\n            new_idx = naming.index_name_if_in_list(subview_idx, self.subviews.keys())\n            self.import_subview(new_idx, subview)\n\n        for feat_container_idx, feat_container in merging_corpus.feature_containers.items():\n            self.new_feature_container(feat_container_idx, feat_container.path)", "response": "Merge the given corpus into this corpus."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrelocating audio files to a single container.", "response": "def relocate_audio_to_single_container(self, target_path):\n        \"\"\"\n        Copies every track to a single container.\n        Afterwards all tracks in the container are linked against\n        this single container.\n        \"\"\"\n\n        cont = containers.AudioContainer(target_path)\n        cont.open()\n\n        new_tracks = {}\n\n        # First create a new container track for all existing tracks\n        for track in self.tracks.values():\n            sr = track.sampling_rate\n            samples = track.read_samples()\n\n            cont.set(track.idx, samples, sr)\n            new_track = tracks.ContainerTrack(track.idx, cont)\n\n            new_tracks[track.idx] = new_track\n\n        # Update track list of corpus\n        self._tracks = new_tracks\n\n        # Update utterances to point to new tracks\n        for utterance in self.utterances.values():\n            new_track = self.tracks[utterance.track.idx]\n            utterance.track = new_track\n\n        cont.close()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrelocating audio files to a new folder.", "response": "def relocate_audio_to_wav_files(self, target_path):\n        \"\"\"\n        Copies every track to its own wav file in the given folder.\n        Every track will be stored at ``target_path/track_id.wav``.\n        \"\"\"\n\n        if not os.path.isdir(target_path):\n            os.makedirs(target_path)\n\n        new_tracks = {}\n\n        # First create a new container track for all existing tracks\n        for track in self.tracks.values():\n            track_path = os.path.join(target_path, '{}.wav'.format(track.idx))\n            sr = track.sampling_rate\n            samples = track.read_samples()\n\n            audio.write_wav(track_path, samples, sr=sr)\n            new_track = tracks.FileTrack(track.idx, track_path)\n\n            new_tracks[track.idx] = new_track\n\n        # Update track list of corpus\n        self._tracks = new_tracks\n\n        # Update utterances to point to new tracks\n        for utterance in self.utterances.values():\n            new_track = self.tracks[utterance.track.idx]\n            utterance.track = new_track"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_corpus(cls, corpus):\n\n        ds = Corpus()\n\n        # Tracks\n        tracks = copy.deepcopy(list(corpus.tracks.values()))\n        track_mapping = ds.import_tracks(tracks)\n\n        # Issuers\n        issuers = copy.deepcopy(list(corpus.issuers.values()))\n        issuer_mapping = ds.import_issuers(issuers)\n\n        # Utterances, with replacing changed track- and issuer-ids\n        utterances = copy.deepcopy(list(corpus.utterances.values()))\n        for utterance in utterances:\n            utterance.track = track_mapping[utterance.track.idx]\n\n            if utterance.issuer is not None:\n                utterance.issuer = issuer_mapping[utterance.issuer.idx]\n\n        ds.import_utterances(utterances)\n\n        # Subviews\n        subviews = copy.deepcopy(corpus.subviews)\n        for subview_idx, subview in subviews.items():\n            ds.import_subview(subview_idx, subview)\n\n        # Feat-Containers\n        for feat_container_idx, feature_container in corpus.feature_containers.items():\n            ds.new_feature_container(feat_container_idx, feature_container.path)\n\n        return ds", "response": "Create a new modifiable corpus from a given corpus."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a new corpus with the data from all given corpora into one.", "response": "def merge_corpora(cls, corpora):\n        \"\"\"\n        Merge a list of corpora into one.\n\n        Args:\n            corpora (Iterable): An iterable of :py:class:`audiomate.corpus.CorpusView`.\n\n        Returns:\n            Corpus: A corpus with the data from all given corpora merged into one.\n        \"\"\"\n\n        ds = Corpus()\n\n        for merging_corpus in corpora:\n            ds.merge_corpus(merging_corpus)\n\n        return ds"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list with ids of all available subsets based on existing csv - files.", "response": "def get_subset_ids(path):\n        \"\"\" Return a list with ids of all available subsets (based on existing csv-files). \"\"\"\n        all = []\n\n        for path in glob.glob(os.path.join(path, '*.tsv')):\n            file_name = os.path.split(path)[1]\n            basename = os.path.splitext(file_name)[0]\n            all.append(basename)\n\n        return all"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_subset(corpus, path, subset_idx):\n        csv_file = os.path.join(path, '{}.tsv'.format(subset_idx))\n        subset_utt_ids = []\n\n        entries = textfile.read_separated_lines_generator(\n            csv_file,\n            separator='\\t',\n            max_columns=8,\n            ignore_lines_starting_with=['client_id'],\n            keep_empty=True\n        )\n\n        for entry in entries:\n\n            file_idx = CommonVoiceReader.create_assets_if_needed(\n                corpus,\n                path,\n                entry\n            )\n            subset_utt_ids.append(file_idx)\n\n        filter = subset.MatchingUtteranceIdxFilter(utterance_idxs=set(subset_utt_ids))\n        subview = subset.Subview(corpus, filter_criteria=[filter])\n        corpus.import_subview(subset_idx, subview)", "response": "Load subset into corpus."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating File Utterance and Issuer if they not already exist and return utt - idx.", "response": "def create_assets_if_needed(corpus, path, entry):\n        \"\"\" Create File/Utterance/Issuer, if they not already exist and return utt-idx. \"\"\"\n        file_idx = entry[1]\n\n        if file_idx not in corpus.utterances.keys():\n            speaker_idx = entry[0]\n            transcription = entry[2]\n\n            age = CommonVoiceReader.map_age(entry[5])\n            gender = CommonVoiceReader.map_gender(entry[6])\n\n            file_path = os.path.join(path, 'clips', '{}.wav'.format(file_idx))\n\n            corpus.new_file(file_path, file_idx)\n\n            if speaker_idx in corpus.issuers.keys():\n                issuer = corpus.issuers[speaker_idx]\n            else:\n                issuer = issuers.Speaker(speaker_idx, gender=gender, age_group=age)\n                corpus.import_issuers(issuer)\n\n            utterance = corpus.new_utterance(file_idx, file_idx, issuer.idx)\n            utterance.set_label_list(\n                annotations.LabelList.create_single(\n                    transcription,\n                    idx=audiomate.corpus.LL_WORD_TRANSCRIPT\n                )\n            )\n\n        return file_idx"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef map_age(age):\n\n        if age in [None, '']:\n            return issuers.AgeGroup.UNKNOWN\n        elif age == 'teens':\n            return issuers.AgeGroup.YOUTH\n        elif age in ['sixties', 'seventies', 'eighties', 'nineties']:\n            return issuers.AgeGroup.SENIOR\n        else:\n            return issuers.AgeGroup.ADULT", "response": "Map age to correct age - group."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef map_gender(gender):\n\n        if gender == 'male':\n            return issuers.Gender.MALE\n        elif gender == 'female':\n            return issuers.Gender.FEMALE\n        else:\n            return issuers.Gender.UNKNOWN", "response": "Map gender to correct value."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses a string that represents an amount of storage and returns the number of bytes it represents.", "response": "def parse_storage_size(storage_size):\n    \"\"\"\n    Parses an expression that represents an amount of storage/memory and returns the number of bytes it represents.\n\n    Args:\n        storage_size(str): Size in bytes. The units ``k`` (kibibytes), ``m`` (mebibytes) and ``g``\n                           (gibibytes) are supported, i.e. a ``partition_size`` of ``1g`` equates :math:`2^{30}` bytes.\n\n    Returns:\n        int: Number of bytes.\n    \"\"\"\n    pattern = re.compile(r'^([0-9]+(\\.[0-9]+)?)([gmk])?$', re.I)\n\n    units = {\n        'k': 1024,\n        'm': 1024 * 1024,\n        'g': 1024 * 1024 * 1024\n    }\n\n    match = pattern.fullmatch(str(storage_size))\n\n    if match is None:\n        raise ValueError('Invalid partition size: {0}'.format(storage_size))\n\n    groups = match.groups()\n\n    # no units\n    if groups[2] is None:\n        # silently dropping the float, because byte is the smallest unit)\n        return int(float(groups[0]))\n\n    return int(float(groups[0]) * units[groups[2].lower()])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the number of frames that will be used for a signal with the length of num_samples.", "response": "def num_frames(self, num_samples):\n        \"\"\"\n        Return the number of frames that will be used for a signal with the length of ``num_samples``.\n        \"\"\"\n        return math.ceil(float(max(num_samples - self.frame_size, 0)) / float(self.hop_size)) + 1"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sample_to_frame_range(self, sample_index):\n        start = max(0, int((sample_index - self.frame_size) / self.hop_size) + 1)\n        end = int(sample_index / self.hop_size) + 1\n        return start, end", "response": "Return a tuple containing the indices of the first and last frames containing the sample with the given index."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef frame_to_sample(self, frame_index):\n        start = frame_index * self.hop_size\n        end = start + self.frame_size\n        return start, end", "response": "Return a tuple containing the indices of the first sample and the end of the frame whose index is frame_index."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef frame_to_seconds(self, frame_index, sr):\n        start_sample, end_sample = self.frame_to_sample(frame_index)\n        return sample_to_seconds(start_sample, sampling_rate=sr), sample_to_seconds(end_sample, sampling_rate=sr)", "response": "Return a tuple containing the start and end of the frame in seconds."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the frames containing samples from the given time range in seconds.", "response": "def time_range_to_frame_range(self, start, end, sr):\n        \"\"\"\n        Calculate the frames containing samples from the given time range in seconds.\n\n        Args:\n            start (float): Start time in seconds.\n            end (float): End time in seconds.\n            sr (int): The sampling rate to use for time-to-sample conversion.\n\n        Returns:\n            tuple: A tuple containing the start and end (exclusive) frame indices.\n        \"\"\"\n\n        start_sample = seconds_to_sample(start, sr)\n        end_sample = seconds_to_sample(end, sr)\n\n        return self.sample_to_frame_range(start_sample)[0], self.sample_to_frame_range(end_sample - 1)[1]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef process_corpus(self, corpus, output_path, frame_size=400, hop_size=160, sr=None):\n\n        def processing_func(utterance, feat_container, frame_size, hop_size, sr, corpus):\n            data = self.process_utterance(utterance, frame_size=frame_size, hop_size=hop_size, sr=sr, corpus=corpus)\n            feat_container.set(utterance.idx, data)\n\n        return self._process_corpus(corpus, output_path, processing_func,\n                                    frame_size=frame_size, hop_size=hop_size, sr=sr)", "response": "Processes all utterances of the given corpus and saves the processed features in a feature - container."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprocesses all utterances in a corpus and saves the processed features in a feature - container.", "response": "def process_corpus_online(self, corpus, output_path, frame_size=400, hop_size=160,\n                              chunk_size=1, buffer_size=5760000):\n        \"\"\"\n        Process all utterances of the given corpus and save the processed features in a feature-container.\n        The utterances are processed in **online** mode, so chunk by chunk.\n\n        Args:\n            corpus (Corpus): The corpus to process the utterances from.\n            output_path (str): A path to save the feature-container to.\n            frame_size (int): The number of samples per frame.\n            hop_size (int): The number of samples between two frames.\n            chunk_size (int): Number of frames to process per chunk.\n            buffer_size (int): Number of samples to load into memory at once.\n                             The exact number of loaded samples depends on the block-size of the audioread library.\n                             So it can be of block-size higher, where the block-size is typically 1024 or 4096.\n\n        Returns:\n            FeatureContainer: The feature-container containing the processed features.\n        \"\"\"\n\n        def processing_func(utterance, feat_container, frame_size, hop_size, corpus, sr):\n            for chunk in self.process_utterance_online(utterance,\n                                                       frame_size=frame_size,\n                                                       hop_size=hop_size,\n                                                       corpus=corpus,\n                                                       chunk_size=chunk_size,\n                                                       buffer_size=buffer_size):\n                feat_container.append(utterance.idx, chunk)\n\n        return self._process_corpus(corpus, output_path, processing_func,\n                                    frame_size=frame_size, hop_size=hop_size, sr=None)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprocessing all features of the given corpus and saves the processed features in a feature - container.", "response": "def process_features(self, corpus, input_features, output_path):\n        \"\"\"\n        Process all features of the given corpus and save the processed features in a feature-container.\n        The features are processed in **offline** mode, all features of an utterance at once.\n\n        Args:\n            corpus (Corpus): The corpus to process the utterances from.\n            input_features (FeatureContainer): The feature-container to process the frames from.\n            output_path (str): A path to save the feature-container to.\n\n        Returns:\n            FeatureContainer: The feature-container containing the processed features.\n        \"\"\"\n        feat_container = containers.FeatureContainer(output_path)\n        feat_container.open()\n\n        input_features.open()\n\n        for utterance in corpus.utterances.values():\n            sampling_rate = input_features.sampling_rate\n            frames = input_features.get(utterance.idx, mem_map=False)\n            processed = self.process_frames(frames, sampling_rate, offset=0, last=True,\n                                            utterance=utterance, corpus=corpus)\n            feat_container.set(utterance.idx, processed)\n\n        tf_frame_size, tf_hop_size = self.frame_transform(input_features.frame_size, input_features.hop_size)\n        feat_container.frame_size = tf_frame_size\n        feat_container.hop_size = tf_hop_size\n        feat_container.sampling_rate = input_features.sampling_rate\n\n        feat_container.close()\n\n        return feat_container"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprocess all features of the given corpus and saves them in a feature - container.", "response": "def process_features_online(self, corpus, input_features, output_path, chunk_size=1):\n        \"\"\"\n        Process all features of the given corpus and save the processed features in a feature-container.\n        The features are processed in **online** mode, chunk by chunk.\n\n        Args:\n            corpus (Corpus): The corpus to process the utterances from.\n            input_features (FeatureContainer): The feature-container to process the frames from.\n            output_path (str): A path to save the feature-container to.\n            chunk_size (int): Number of frames to process per chunk.\n\n        Returns:\n            FeatureContainer: The feature-container containing the processed features.\n        \"\"\"\n        feat_container = containers.FeatureContainer(output_path)\n        feat_container.open()\n\n        input_features.open()\n\n        for utterance in corpus.utterances.values():\n            sampling_rate = input_features.sampling_rate\n            frames = input_features.get(utterance.idx, mem_map=True)\n\n            current_frame = 0\n\n            while current_frame < frames.shape[0]:\n                last = current_frame + chunk_size > frames.shape[0]\n                to_frame = current_frame + chunk_size\n\n                chunk = frames[current_frame:to_frame]\n\n                processed = self.process_frames(chunk, sampling_rate, current_frame,\n                                                last=last, utterance=utterance, corpus=corpus)\n\n                if processed is not None:\n                    feat_container.append(utterance.idx, processed)\n\n                current_frame += chunk_size\n\n        tf_frame_size, tf_hop_size = self.frame_transform(input_features.frame_size, input_features.hop_size)\n        feat_container.frame_size = tf_frame_size\n        feat_container.hop_size = tf_hop_size\n        feat_container.sampling_rate = input_features.sampling_rate\n\n        feat_container.close()\n\n        return feat_container"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef process_utterance(self, utterance, frame_size=400, hop_size=160, sr=None, corpus=None):\n        return self.process_track(utterance.track, frame_size=frame_size, hop_size=hop_size, sr=sr,\n                                  start=utterance.start, end=utterance.end, utterance=utterance, corpus=corpus)", "response": "Process the utterance in offline mode."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprocessing the utterance in online mode.", "response": "def process_utterance_online(self, utterance, frame_size=400, hop_size=160, chunk_size=1,\n                                 buffer_size=5760000, corpus=None):\n        \"\"\"\n        Process the utterance in **online** mode, chunk by chunk.\n        The processed chunks are yielded one after another.\n\n        Args:\n            utterance (Utterance): The utterance to process.\n            frame_size (int): The number of samples per frame.\n            hop_size (int): The number of samples between two frames.\n            chunk_size (int): Number of frames to process per chunk.\n            buffer_size (int): Number of samples to load into memory at once.\n                             The exact number of loaded samples depends on the block-size of the audioread library.\n                             So it can be of block-size higher, where the block-size is typically 1024 or 4096.\n            corpus (Corpus): The corpus this utterance is part of, if available.\n\n        Returns:\n            Generator: A generator that yield processed chunks.\n        \"\"\"\n        return self.process_track_online(utterance.track,\n                                         frame_size=frame_size,\n                                         hop_size=hop_size,\n                                         start=utterance.start,\n                                         end=utterance.end,\n                                         utterance=utterance,\n                                         corpus=corpus,\n                                         chunk_size=chunk_size,\n                                         buffer_size=buffer_size)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprocesses a single track in offline mode.", "response": "def process_track(self, track, frame_size=400, hop_size=160, sr=None,\n                      start=0, end=float('inf'), utterance=None, corpus=None):\n        \"\"\"\n        Process the track in **offline** mode, in one go.\n\n        Args:\n            track (Track): The track to process.\n            frame_size (int): The number of samples per frame.\n            hop_size (int): The number of samples between two frames.\n            sr (int): Use the given sampling rate. If ``None``,\n                      uses the native sampling rate from the underlying data.\n            start (float): The point within the track in seconds,\n                           to start processing from.\n            end (float): The point within the track in seconds,\n                         to end processing.\n            utterance (Utterance): The utterance that is associated with\n                                   this track, if available.\n            corpus (Corpus): The corpus this track is part of, if available.\n\n        Returns:\n            np.ndarray: The processed features.\n        \"\"\"\n        frame_settings = units.FrameSettings(frame_size, hop_size)\n\n        if end != float('inf'):\n            samples = track.read_samples(sr=sr, offset=start, duration=end-start)\n        else:\n            samples = track.read_samples(sr=sr, offset=start)\n\n        if sr is None:\n            sr = track.sampling_rate\n\n        if samples.size <= 0:\n            raise ValueError('Track {} has no samples'.format(track.idx))\n\n        # Pad with zeros to match frames\n        num_frames = frame_settings.num_frames(samples.size)\n        num_pad_samples = (num_frames - 1) * hop_size + frame_size\n\n        if num_pad_samples > samples.size:\n            samples = np.pad(samples, (0, num_pad_samples - samples.size), mode='constant', constant_values=0)\n\n        # Get sampling-rate if not given\n        sampling_rate = sr or utterance.sampling_rate\n\n        frames = librosa.util.frame(samples, frame_length=frame_size, hop_length=hop_size).T\n        return self.process_frames(frames, sampling_rate, 0, last=True, utterance=utterance, corpus=corpus)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef process_track_online(self, track, frame_size=400, hop_size=160,\n                             start=0, end=float('inf'), utterance=None, corpus=None,\n                             chunk_size=1, buffer_size=5760000):\n        \"\"\"\n        Process the track in **online** mode, chunk by chunk.\n        The processed chunks are yielded one after another.\n\n        Args:\n            track (Track): The track to process.\n            frame_size (int): The number of samples per frame.\n            hop_size (int): The number of samples between two frames.\n            start (float): The point within the track in seconds to start processing from.\n            end (float): The point within the trac in seconds to end processing.\n            utterance (Utterance): The utterance that is associated with this track, if available.\n            corpus (Corpus): The corpus this track is part of, if available.\n            chunk_size (int): Number of frames to process per chunk.\n            buffer_size (int): Number of samples to load into memory at once.\n                               The exact number of loaded samples depends\n                               on the type of track.\n                               It can be of block-size higher,\n                               where the block-size is typically 1024 or 4096.\n\n        Returns:\n            Generator: A generator that yield processed chunks.\n        \"\"\"\n\n        current_frame = 0\n        frames = []\n\n        duration = None\n        sr = track.sampling_rate\n\n        if end != float('inf'):\n            duration = end - start\n\n        # Process chunks that are within end bounds\n        for frame, is_last in track.read_frames(frame_size,\n                                                hop_size,\n                                                offset=start,\n                                                duration=duration,\n                                                buffer_size=buffer_size):\n\n            frames.append(frame)\n\n            if len(frames) == chunk_size:\n                processed = self.process_frames(np.array(frames), sr, current_frame,\n                                                last=is_last, utterance=utterance, corpus=corpus)\n                if processed is not None:\n                    yield processed\n                current_frame += chunk_size\n                frames = frames[chunk_size:]\n\n        # Process overlapping chunks with zero frames at the end\n        if len(frames) > 0:\n            processed = self.process_frames(np.array(frames), sr, current_frame,\n                                            last=True, utterance=utterance, corpus=corpus)\n            yield processed", "response": "This method processes a track in online mode."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _process_corpus(self, corpus, output_path, processing_func, frame_size=400, hop_size=160, sr=None):\n        feat_container = containers.FeatureContainer(output_path)\n        feat_container.open()\n\n        sampling_rate = -1\n\n        for utterance in corpus.utterances.values():\n            utt_sampling_rate = utterance.sampling_rate\n\n            if sr is None:\n                if sampling_rate > 0 and sampling_rate != utt_sampling_rate:\n                    raise ValueError(\n                        'File {} has a different sampling-rate than the previous ones!'.format(utterance.track.idx))\n\n                sampling_rate = utt_sampling_rate\n\n            processing_func(utterance, feat_container, frame_size, hop_size, sr, corpus)\n\n        tf_frame_size, tf_hop_size = self.frame_transform(frame_size, hop_size)\n        feat_container.frame_size = tf_frame_size\n        feat_container.hop_size = tf_hop_size\n        feat_container.sampling_rate = sr or sampling_rate\n\n        feat_container.close()\n\n        return feat_container", "response": "Utility function for processing a corpus with a separate processing function."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmerges the read blocks and resample if necessary.", "response": "def process_buffer(buffer, n_channels):\n    \"\"\"\n    Merge the read blocks and resample if necessary.\n\n    Args:\n        buffer (list): A list of blocks of samples.\n        n_channels (int): The number of channels of the input data.\n\n    Returns:\n        np.array: The samples\n    \"\"\"\n    samples = np.concatenate(buffer)\n\n    if n_channels > 1:\n        samples = samples.reshape((-1, n_channels)).T\n        samples = librosa.to_mono(samples)\n\n    return samples"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread an audio file block after a block.", "response": "def read_blocks(file_path, start=0.0, end=float('inf'), buffer_size=5760000):\n    \"\"\"\n    Read an audio file block after block. The blocks are yielded one by one.\n\n    Args:\n        file_path (str): Path to the file to read.\n        start (float): Start in seconds to read from.\n        end (float): End in seconds to read to.\n                     ``inf`` means to the end of the file.\n        buffer_size (int): Number of samples to load into memory at once and\n                           return as a single block. The exact number of loaded\n                           samples depends on the block-size of the\n                           audioread library. So it can be of x higher,\n                           where the x is typically 1024 or 4096.\n\n    Returns:\n        Generator: A generator yielding the samples for every block.\n    \"\"\"\n    buffer = []\n    n_buffer = 0\n    n_samples = 0\n\n    with audioread.audio_open(file_path) as input_file:\n        n_channels = input_file.channels\n        sr_native = input_file.samplerate\n\n        start_sample = int(np.round(sr_native * start)) * n_channels\n        end_sample = end\n\n        if end_sample != np.inf:\n            end_sample = int(np.round(sr_native * end)) * n_channels\n\n        for block in input_file:\n            block = librosa.util.buf_to_float(block)\n            n_prev = n_samples\n            n_samples += len(block)\n\n            if n_samples < start_sample:\n                continue\n\n            if n_prev > end_sample:\n                break\n\n            if n_samples > end_sample:\n                block = block[:end_sample - n_prev]\n\n            if n_prev <= start_sample <= n_samples:\n                block = block[start_sample - n_prev:]\n\n            n_buffer += len(block)\n            buffer.append(block)\n\n            if n_buffer >= buffer_size:\n                yield process_buffer(buffer, n_channels)\n\n                buffer = []\n                n_buffer = 0\n\n        if len(buffer) > 0:\n            yield process_buffer(buffer, n_channels)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread an audio file frame by frame.", "response": "def read_frames(file_path, frame_size, hop_size, start=0.0,\n                end=float('inf'), buffer_size=5760000):\n    \"\"\"\n    Read an audio file frame by frame. The frames are yielded one after another.\n\n    Args:\n        file_path (str): Path to the file to read.\n        frame_size (int): The number of samples per frame.\n        hop_size (int): The number of samples between two frames.\n        start (float): Start in seconds to read from.\n        end (float): End in seconds to read to.\n                     ``inf`` means to the end of the file.\n        buffer_size (int): Number of samples to load into memory at once\n                           and return as a single block.\n                           The exact number of loaded samples depends on the\n                           block-size of the audioread library. So it can be\n                           of x higher, where the x is typically 1024 or 4096.\n\n    Returns:\n        Generator: A generator yielding a tuple for every frame.\n        The first item is the frame and\n        the second a boolean indicating if it is the last frame.\n    \"\"\"\n    rest_samples = np.array([], dtype=np.float32)\n\n    for block in read_blocks(file_path, start=start, end=end, buffer_size=buffer_size):\n\n        # Prepend rest samples from previous block\n        block = np.concatenate([rest_samples, block])\n\n        current_sample = 0\n\n        # Get frames that are fully contained in the block\n        while current_sample + frame_size < block.size:\n            frame = block[current_sample:current_sample + frame_size]\n            yield frame, False\n            current_sample += hop_size\n\n        # Store rest samples for next block\n        rest_samples = block[current_sample:]\n\n    if rest_samples.size > 0:\n        rest_samples = np.pad(\n            rest_samples,\n            (0, frame_size - rest_samples.size),\n            mode='constant',\n            constant_values=0\n        )\n        yield rest_samples, True"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef write_wav(path, samples, sr=16000):\n    max_value = np.abs(np.iinfo(np.int16).min)\n    data = (samples * max_value).astype(np.int16)\n    scipy.io.wavfile.write(path, sr, data)", "response": "Writes to given samples to a wav file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef values(self):\n        return np.array([self.mean, self.var, self.min, self.max, self.num])", "response": "Return all values as numpy - array"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the stats as a dictionary.", "response": "def to_dict(self):\n        \"\"\"\n        Return the stats as a dictionary.\n        \"\"\"\n        return {\n            'mean': self.mean,\n            'var': self.var,\n            'min': self.min,\n            'max': self.max,\n            'num': self.num\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef concatenate(cls, list_of_stats):\n\n        all_stats = np.stack([stats.values for stats in list_of_stats])\n        all_counts = all_stats[:, 4]\n        all_counts_relative = all_counts / np.sum(all_counts)\n\n        min_value = float(np.min(all_stats[:, 2]))\n        max_value = float(np.max(all_stats[:, 3]))\n        mean_value = float(np.sum(all_counts_relative * all_stats[:, 0]))\n        var_value = float(np.sum(all_counts_relative * (all_stats[:, 1] + np.power(all_stats[:, 0] - mean_value, 2))))\n        num_value = int(np.sum(all_counts))\n\n        return cls(mean_value, var_value, min_value, max_value, num_value)", "response": "Takes a list of stats from different sets of data points and merge the stats for getting stats overall data points."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating a corpus with mock data.", "response": "def generate_corpus(n_issuers,\n                    n_tracks_per_issuer,\n                    n_utts_per_track,\n                    n_ll_per_utt,\n                    n_label_per_ll,\n                    rand=None):\n    \"\"\"\n    Generate a corpus with mock data.\n    \"\"\"\n    corpus = audiomate.Corpus()\n\n    for issuer in generate_issuers(n_issuers, rand):\n        corpus.import_issuers(issuer)\n\n        n_tracks = rand.randint(*n_tracks_per_issuer)\n        tracks = generate_tracks(n_tracks, rand)\n        corpus.import_tracks(tracks)\n\n        n_utts = rand.randint(*n_utts_per_track)\n        for track in tracks:\n            utts = generate_utterances(\n                track,\n                issuer,\n                n_utts,\n                n_ll_per_utt,\n                n_label_per_ll,\n                rand\n            )\n\n            corpus.import_utterances(utts)\n\n    return corpus"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _load_folder(folder_entry, corpus):\n        for wav_path in glob.glob(os.path.join(folder_entry.path, '*.wav')):\n            wav_name = os.path.basename(wav_path)\n            basename, __ = os.path.splitext(wav_name)\n\n            command = folder_entry.name\n            file_idx = '{}_{}'.format(basename, command)\n            issuer_idx = str(basename).split('_', maxsplit=1)[0]\n\n            corpus.new_file(wav_path, file_idx)\n\n            if issuer_idx not in corpus.issuers.keys():\n                corpus.import_issuers(issuers.Speaker(\n                    issuer_idx\n                ))\n\n            utt = corpus.new_utterance(file_idx, file_idx, issuer_idx)\n\n            labels = annotations.LabelList.create_single(command, idx=audiomate.corpus.LL_WORD_TRANSCRIPT)\n            utt.set_label_list(labels)", "response": "Load the given folder into the corpus."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload the subviews based on testing_list. txt and validation_list. txt.", "response": "def _create_subviews(path, corpus):\n        \"\"\" Load the subviews based on testing_list.txt and validation_list.txt \"\"\"\n        test_list_path = os.path.join(path, 'testing_list.txt')\n        dev_list_path = os.path.join(path, 'validation_list.txt')\n\n        test_list = textfile.read_separated_lines(test_list_path, separator='/', max_columns=2)\n        dev_list = textfile.read_separated_lines(dev_list_path, separator='/', max_columns=2)\n\n        test_set = set(['{}_{}'.format(os.path.splitext(x[1])[0], x[0]) for x in test_list])\n        dev_set = set(['{}_{}'.format(os.path.splitext(x[1])[0], x[0]) for x in dev_list])\n        inv_train_set = test_set.union(dev_set)\n\n        train_filter = subview.MatchingUtteranceIdxFilter(utterance_idxs=inv_train_set, inverse=True)\n        train_view = subview.Subview(corpus, filter_criteria=train_filter)\n        corpus.import_subview('train', train_view)\n\n        dev_filter = subview.MatchingUtteranceIdxFilter(utterance_idxs=dev_set, inverse=False)\n        dev_view = subview.Subview(corpus, filter_criteria=dev_filter)\n        corpus.import_subview('dev', dev_view)\n\n        test_filter = subview.MatchingUtteranceIdxFilter(utterance_idxs=test_set, inverse=False)\n        test_view = subview.Subview(corpus, filter_criteria=test_filter)\n        corpus.import_subview('test', test_view)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrite the given label_list to an audacity label file.", "response": "def write_label_list(path, label_list):\n    \"\"\"\n    Writes the given `label_list` to an audacity label file.\n\n    Args:\n        path (str): Path to write the file to.\n        label_list (audiomate.annotations.LabelList): Label list\n    \"\"\"\n    entries = []\n    for label in label_list:\n        entries.append([label.start, label.end, label.value])\n\n    textfile.write_separated_lines(path, entries, separator='\\t')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading the labels from an audacity label file.", "response": "def read_label_file(path):\n    \"\"\"\n    Read the labels from an audacity label file.\n\n    Args:\n        path (str): Path to the label file.\n\n    Returns:\n        list: List of labels (start [sec], end [sec], label)\n\n    Example::\n\n        >>> read_label_file('/path/to/label/file.txt')\n        [\n            [0.0, 0.2, 'sie'],\n            [0.2, 2.2, 'hallo']\n        ]\n    \"\"\"\n    labels = []\n\n    for record in textfile.read_separated_lines_generator(path, separator='\\t', max_columns=3):\n        value = ''\n\n        if len(record) > 2:\n            value = str(record[2])\n\n        labels.append([float(_clean_time(record[0])), float(_clean_time(record[1])), value])\n\n    return labels"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading labels from an Audacity label file and returns them wrapped in a : py : class : annotations. LabelList.", "response": "def read_label_list(path):\n    \"\"\"\n    Reads labels from an Audacity label file\n    and returns them wrapped in a :py:class:`audiomate.annotations.LabelList`.\n\n    Args:\n        path (str): Path to the Audacity label file\n\n    Returns:\n        audiomate.annotations.LabelList: Label list containing the labels\n    \"\"\"\n    ll = annotations.LabelList()\n\n    for record in read_label_file(path):\n        ll.add(annotations.Label(record[2], start=record[0], end=record[1]))\n\n    return ll"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef extract_zip(zip_path, target_folder):\n    with zipfile.ZipFile(zip_path) as archive:\n        archive.extractall(target_folder)", "response": "Extracts the content of the zip - file at zip_path into target_folder."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef extract_tar(tar_path, target_folder):\n    with tarfile.open(tar_path, 'r') as archive:\n        archive.extractall(target_folder)", "response": "Extract the content of the tar - file at tar_path into target_folder."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef move_all_files_from_subfolders_to_top(folder_path, delete_subfolders=False, copy=False):\n    for item in os.listdir(folder_path):\n        sub_path = os.path.join(folder_path, item)\n\n        if os.path.isdir(sub_path):\n\n            for sub_item in os.listdir(sub_path):\n                src = os.path.join(sub_path, sub_item)\n                target = os.path.join(folder_path, sub_item)\n\n                if copy:\n                    if os.path.isfile(src):\n                        shutil.copy(src, target)\n                    else:\n                        shutil.copytree(src, target)\n                else:\n                    shutil.move(src, target)\n\n            if delete_subfolders:\n                shutil.rmtree(sub_path)", "response": "Move all files from all subfolders of folder_path to top."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_utt_regions(self):\n\n        regions = []\n        current_offset = 0\n\n        for utt_idx, utt_data in zip(self.data.info.utt_ids, self.data.utt_data):\n            offset = current_offset\n\n            num_frames = []\n            refs = []\n\n            for part in utt_data:\n                num_frames.append(part.shape[0])\n                refs.append(part)\n\n            if len(set(num_frames)) != 1:\n                raise ValueError('Utterance {} has not the same number of frames in all containers!'.format(utt_idx))\n\n            num_chunks = math.ceil(num_frames[0] / float(self.frames_per_chunk))\n\n            region = (offset, num_chunks, refs)\n            regions.append(region)\n\n            # Sets the offset for the next utterances\n            current_offset += num_chunks\n\n        return regions", "response": "Returns the regions of all utterances assuming all utterances are concatenated."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read_separated_lines(path, separator=' ', max_columns=-1, keep_empty=False):\n\n    gen = read_separated_lines_generator(path, separator, max_columns, keep_empty=keep_empty)\n    return list(gen)", "response": "Reads a text file where each line represents a record with some separated columns."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read_separated_lines_with_first_key(path: str, separator: str = ' ', max_columns: int = -1,\n                                        keep_empty: bool = False):\n    \"\"\"\n    Reads the separated lines of a file and return a dictionary with the first column as keys, value\n    is a list with the rest of the columns.\n\n    Parameters:\n        path (str): Path to the file to read.\n        separator (str): Separator that is used to split the columns.\n        max_columns (str): Number of max columns (if the separator occurs within the last column).\n        keep_empty (bool): If True empty columns are returned as well.\n\n    Returns:\n        dict: Dictionary with list of column values and first column value as key.\n    \"\"\"\n    gen = read_separated_lines_generator(path, separator, max_columns, keep_empty=keep_empty)\n\n    dic = {}\n\n    for record in gen:\n        if len(record) > 0:\n            dic[record[0]] = record[1:len(record)]\n\n    return dic", "response": "Reads the separated lines of a file and returns a dictionary with the first column as keys value\n            is a list with the rest of the columns as values."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads lines of a text file with two columns as key and second as value dictionary.", "response": "def read_key_value_lines(path, separator=' ', default_value=''):\n    \"\"\"\n    Reads lines of a text file with two columns as key/value dictionary.\n\n    Parameters:\n        path (str): Path to the file.\n        separator (str): Separator that is used to split key and value.\n        default_value (str): If no value is given this value is used.\n\n    Returns:\n        dict: A dictionary with first column as key and second as value.\n    \"\"\"\n    gen = read_separated_lines_generator(path, separator, 2)\n\n    dic = {}\n\n    for record in gen:\n        if len(record) > 1:\n            dic[record[0]] = record[1]\n        elif len(record) > 0:\n            dic[record[0]] = default_value\n\n    return dic"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write_separated_lines(path, values, separator=' ', sort_by_column=0):\n    f = open(path, 'w', encoding='utf-8')\n\n    if type(values) is dict:\n        if sort_by_column in [0, 1]:\n            items = sorted(values.items(), key=lambda t: t[sort_by_column])\n        else:\n            items = values.items()\n\n        for key, value in items:\n            if type(value) in [list, set]:\n                value = separator.join([str(x) for x in value])\n\n            f.write('{}{}{}\\n'.format(key, separator, value))\n    elif type(values) is list or type(values) is set:\n        if 0 <= sort_by_column < len(values):\n            items = sorted(values)\n        else:\n            items = values\n\n        for record in items:\n            str_values = [str(value) for value in record]\n\n            f.write('{}\\n'.format(separator.join(str_values)))\n\n    f.close()", "response": "Writes list or dict to file line by line."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read_separated_lines_generator(path, separator=' ', max_columns=-1,\n                                   ignore_lines_starting_with=[], keep_empty=False):\n    \"\"\"\n    Creates a generator through all lines of a file and returns the splitted line.\n\n    Parameters:\n        path: Path to the file.\n        separator: Separator that is used to split the columns.\n        max_columns: Number of max columns (if the separator occurs within the last column).\n        ignore_lines_starting_with: Lines starting with a string in this list will be ignored.\n        keep_empty (bool): If True empty columns are returned as well.\n    \"\"\"\n    if not os.path.isfile(path):\n        print('File doesnt exist or is no file: {}'.format(path))\n        return\n\n    f = open(path, 'r', errors='ignore', encoding='utf-8')\n\n    if max_columns > -1:\n        max_splits = max_columns - 1\n    else:\n        max_splits = -1\n\n    for line in f:\n        if keep_empty:\n            stripped_line = line\n        else:\n            stripped_line = line.strip()\n\n        should_ignore = text.starts_with_prefix_in_list(stripped_line, ignore_lines_starting_with)\n\n        if not should_ignore and stripped_line != '':\n            record = stripped_line.split(sep=separator, maxsplit=max_splits)\n            record = [field.strip() for field in record]\n            yield record\n\n    f.close()", "response": "Reads a file and returns a generator that returns all lines in that file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfinding the msvcrt command and return the output of wine - python command.", "response": "def find_msvcrt():\n\t\"\"\"\n\tLikely useless and will return None, see https://bugs.python.org/issue23606\n\tOffered for full compatibility, though.\n\t\"\"\"\n\n\t# Compile Python command for wine-python\n\tcommand = '\"from ctypes.util import find_msvcrt; print(find_msvcrt())\"'\n\n\t# Start wine-python\n\twinepython_p = subprocess.Popen(\n\t\t'wine-python -c' + command,\n\t\tstdout = subprocess.PIPE,\n\t\tstderr = subprocess.PIPE,\n\t\tshell = True\n\t\t)\n\n\t# Get stdout and stderr\n\twinepython_out, winepython_err = winepython_p.communicate()\n\n\t# Change encoding\n\twinepython_out = winepython_out.decode(encoding = 'UTF-8').strip()\n\n\t# Handle None values\n\tif winepython_out in ['', 'None']:\n\t\twinepython_out = None\n\n\treturn winepython_out"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef unix_to_wine(self, in_path):\n\n\t\tif len(in_path) > MAX_PATH:\n\t\t\traise # TODO\n\n\t\tin_path_astr_p = ctypes.pointer(self.__str_to_winastr__(in_path))\n\t\tout_path_ustr_p = ctypes.pointer(UNICODE_STRING())\n\n\t\tntstatus = self.__unix_to_wine__(\n\t\t\tin_path_astr_p, out_path_ustr_p\n\t\t\t)\n\n\t\tif ntstatus != 0:\n\t\t\traise # TODO\n\n\t\tout_path = self.__winustr_to_str__(out_path_ustr_p.contents)\n\n\t\t# https://github.com/wine-mirror/wine/blob/07cf14dc928a1a00baecbbc7ca5a6f3fe680238c/dlls/kernel32/path.c#L1955\n\t\tif out_path[5] == ':':\n\t\t\treturn out_path[4:]\n\t\telse:\n\t\t\tout_path[1] = '\\\\'\n\t\t\treturn out_path", "response": "Convert a Unix path to Wine path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef wine_to_unix(self, in_path):\n\n\t\tif len(in_path) > MAX_PATH:\n\t\t\traise # TODO\n\n\t\tin_path_buffer_p = ctypes.pointer(ctypes.create_unicode_buffer(in_path + '\\0'))\n\t\tin_path_ustr_p = ctypes.pointer(UNICODE_STRING())\n\n\t\tself.__dos_to_nt__(in_path_buffer_p, in_path_ustr_p, None, None)\n\n\t\tout_path_astr_p = ctypes.pointer(ANSI_STRING())\n\n\t\tntstatus = self.__wine_to_unix__(\n\t\t\tin_path_ustr_p, out_path_astr_p,\n\t\t\tself.FILE_OPEN_IF, int(False)\n\t\t\t)\n\n\t\tif ntstatus != 0:\n\t\t\traise # TODO\n\n\t\treturn self.__winastr_to_str__(out_path_astr_p.contents)", "response": "Convert a Wine path to a Unix path."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef list(self, page: int = None, pageSize: int = None, search: str = None, workspaceId: str = None) -> dict:\n        return self.__client.request('get', '/forms', params={\n            'page': page,\n            'page_size': pageSize,\n            'search': search,\n            'workspace_id': workspaceId\n        })", "response": "Returns a list of JSON descriptions for all Typeform accounts."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nupdates an existing form.", "response": "def update(self, uid: str, patch=False, data: any = {}) -> typing.Union[str, dict]:\n        \"\"\"\n        Updates an existing form.\n        Defaults to `put`.\n        `put` will return the modified form as a `dict` object.\n        `patch` will return a `str` based on success of change, `OK` on success, otherwise an error message.\n        \"\"\"\n        methodType = 'put' if patch is False else 'patch'\n        return self.__client.request(methodType, '/forms/%s' % uid, data=data)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating the customizable messages for a form.", "response": "def update(self, uid: str, data={}) -> str:\n        \"\"\"\n        Specifies new values for the customizable messages in a form (specified by form_id).\n        You can format messages with bold (*bold*) and italic (_italic_) text. HTML tags are forbidden.\n        Return a `str` based on success of change, `OK` on success, otherwise an error message.\n        \"\"\"\n        return self.__client.request('put', '/forms/%s/messages' % uid, data=data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads configuration from application configuration.", "response": "def configure(self, app):\n        '''\n        Load configuration from application configuration.\n\n        For each storage, the configuration is loaded with the following pattern::\n\n            FS_{BACKEND_NAME}_{KEY} then\n            {STORAGE_NAME}_FS_{KEY}\n\n        If no configuration is set for a given key, global config is taken as default.\n        '''\n        config = Config()\n\n        prefix = PREFIX.format(self.name.upper())\n        backend_key = '{0}BACKEND'.format(prefix)\n        self.backend_name = app.config.get(backend_key, app.config['FS_BACKEND'])\n        self.backend_prefix = BACKEND_PREFIX.format(self.backend_name.upper())\n        backend_excluded_keys = [''.join((self.backend_prefix, k)) for k in BACKEND_EXCLUDED_CONFIG]\n\n        # Set default values\n        for key, value in DEFAULT_CONFIG.items():\n            config.setdefault(key, value)\n\n        # Set backend level values\n        for key, value in app.config.items():\n            if key.startswith(self.backend_prefix) and key not in backend_excluded_keys:\n                config[key.replace(self.backend_prefix, '').lower()] = value\n\n        # Set storage level values\n        for key, value in app.config.items():\n            if key.startswith(prefix):\n                config[key.replace(prefix, '').lower()] = value\n\n        if self.backend_name not in BACKENDS:\n            raise ValueError('Unknown backend \"{0}\"'.format(self.backend_name))\n        backend_class = BACKENDS[self.backend_name].load()\n        backend_class.backend_name = self.backend_name\n        self.backend = backend_class(self.name, config)\n        self.config = config"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef path(self, filename):\n        '''\n        This returns the absolute path of a file uploaded to this set. It\n        doesn't actually check whether said file exists.\n\n        :param filename: The filename to return the path for.\n        :param folder: The subfolder within the upload set previously used\n                       to save to.\n\n        :raises OperationNotSupported: when the backenddoesn't support direct file access\n        '''\n        if not self.backend.root:\n            raise OperationNotSupported(\n                'Direct file access is not supported by ' +\n                self.backend.__class__.__name__\n            )\n        return os.path.join(self.backend.root, filename)", "response": "This returns the absolute path of a file uploaded to this set."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading a file content.", "response": "def read(self, filename):\n        '''\n        Read a file content.\n\n        :param string filename: The storage root-relative filename\n        :raises FileNotFound: If the file does not exists\n        '''\n        if not self.backend.exists(filename):\n            raise FileNotFound(filename)\n        return self.backend.read(filename)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nopen the file and return a file - like object.", "response": "def open(self, filename, mode='r', **kwargs):\n        '''\n        Open the file and return a file-like object.\n\n        :param str filename: The storage root-relative filename\n        :param str mode: The open mode (``(r|w)b?``)\n        :raises FileNotFound: If trying to read a file that does not exists\n        '''\n        if 'r' in mode and not self.backend.exists(filename):\n            raise FileNotFound(filename)\n        return self.backend.open(filename, mode, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write(self, filename, content, overwrite=False):\n        '''\n        Write content to a file.\n\n        :param str filename: The storage root-relative filename\n        :param content: The content to write in the file\n        :param bool overwrite: Whether to wllow overwrite or not\n        :raises FileExists: If the file exists and `overwrite` is `False`\n        '''\n        if not self.overwrite and not overwrite and self.backend.exists(filename):\n            raise FileExists()\n        return self.backend.write(filename, content)", "response": "Write content to a file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsaving a file or a FileStorage into this storage.", "response": "def save(self, file_or_wfs, filename=None, prefix=None, overwrite=None):\n        '''\n        Saves a `file` or a :class:`~werkzeug.FileStorage` into this storage.\n\n        If the upload is not allowed, an :exc:`UploadNotAllowed` error will be raised.\n        Otherwise, the file will be saved and its name (including the folder)\n        will be returned.\n\n        :param file_or_wfs: a file or :class:`werkzeug.FileStorage` file to save.\n        :param string filename: The expected filename in the storage.\n            Optionnal with a :class:`~werkzeug.FileStorage` but allow to override clietn value\n        :param string prefix: a path or a callable returning a path to be prepended to the filename.\n        :param bool overwrite: if specified, override the storage default value.\n\n        :raise UnauthorizedFileType: If the file type is not allowed\n        '''\n        if not filename and isinstance(file_or_wfs, FileStorage):\n            filename = lower_extension(secure_filename(file_or_wfs.filename))\n\n        if not filename:\n            raise ValueError('filename is required')\n\n        if not self.file_allowed(file_or_wfs, filename):\n            raise UnauthorizedFileType()\n\n        if prefix:\n            filename = '/'.join((prefix() if callable(prefix) else prefix, filename))\n\n        if self.upload_to:\n            upload_to = self.upload_to() if callable(self.upload_to) else self.upload_to\n            filename = '/'.join((upload_to, filename))\n\n        overwrite = self.overwrite if overwrite is None else overwrite\n        if not overwrite and self.exists(filename):\n            raise FileExists(filename)\n\n        self.backend.save(file_or_wfs, filename)\n\n        return filename"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets some metadata for a given file.", "response": "def metadata(self, filename):\n        '''\n        Get some metadata for a given file.\n\n        Can vary from a backend to another but some are always present:\n        - `filename`: the base filename (without the path/prefix)\n        - `url`: the file public URL\n        - `checksum`: a checksum expressed in the form `algo:hash`\n        - 'mime': the mime type\n        - `modified`: the last modification date\n        '''\n        metadata = self.backend.metadata(filename)\n        metadata['filename'] = os.path.basename(filename)\n        metadata['url'] = self.url(filename, external=True)\n        return metadata"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nserve a file given its filename", "response": "def serve(self, filename):\n        '''Serve a file given its filename'''\n        if not self.exists(filename):\n            abort(404)\n        return self.backend.serve(filename)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef make_thumbnail(file, size, bbox=None):\n    '''\n    Generate a thumbnail for a given image file.\n\n    :param file file: The source image file to thumbnail\n    :param int size: The thumbnail size in pixels (Thumbnails are squares)\n    :param tuple bbox: An optionnal Bounding box definition for the thumbnail\n    '''\n    image = Image.open(file)\n    if bbox:\n        thumbnail = crop_thumbnail(image, size, bbox)\n    else:\n        thumbnail = center_thumbnail(image, size)\n    return _img_to_file(thumbnail)", "response": "Generate a thumbnail for a given image file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse pip requirements file and transform it to setuptools requirements.", "response": "def pip(filename):\n    \"\"\"Parse pip reqs file and transform it to setuptools requirements.\"\"\"\n    requirements = []\n    for line in open(join(ROOT, 'requirements', filename)):\n        line = line.strip()\n        if not line or '://' in line:\n            continue\n        match = RE_REQUIREMENT.match(line)\n        if match:\n            requirements.extend(pip(match.group('filename')))\n        else:\n            requirements.append(line)\n    return requirements"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmoving a file given its filename to another path in the storage backend.", "response": "def move(self, filename, target):\n        '''\n        Move a file given its filename to another path in the storage\n\n        Default implementation perform a copy then a delete.\n        Backends should overwrite it if there is a better way.\n        '''\n        self.copy(filename, target)\n        self.delete(filename)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsaving a file - like object or a wfs. FileStorage with the specified filename.", "response": "def save(self, file_or_wfs, filename, overwrite=False):\n        '''\n        Save a file-like object or a `werkzeug.FileStorage` with the specified filename.\n\n        :param storage: The file or the storage to be saved.\n        :param filename: The destination in the storage.\n        :param overwrite: if `False`, raise an exception if file exists in storage\n\n        :raises FileExists: when file exists and overwrite is `False`\n        '''\n        self.write(filename, file_or_wfs.read())\n        return filename"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfetch all available metadata for a given file", "response": "def metadata(self, filename):\n        '''\n        Fetch all available metadata for a given file\n        '''\n        meta = self.get_metadata(filename)\n        # Fix backend mime misdetection\n        meta['mime'] = meta.get('mime') or files.mime(filename, self.DEFAULT_MIME)\n        return meta"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nperforms content encoding for binary write", "response": "def as_binary(self, content, encoding='utf8'):\n        '''Perform content encoding for binary write'''\n        if hasattr(content, 'read'):\n            return content.read()\n        elif isinstance(content, six.text_type):\n            return content.encode(encoding)\n        else:\n            return content"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfetches all availabe metadata for a file", "response": "def get_metadata(self, filename):\n        '''Fetch all availabe metadata'''\n        obj = self.bucket.Object(filename)\n        checksum = 'md5:{0}'.format(obj.e_tag[1:-1])\n        mime = obj.content_type.split(';', 1)[0] if obj.content_type else None\n        return {\n            'checksum': checksum,\n            'size': obj.content_length,\n            'mime': mime,\n            'modified': obj.last_modified,\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsave a Werkzeug FileStorage object", "response": "def save(self, wfs, filename=None):\n        '''Save a Werkzeug FileStorage object'''\n        if self.basename and not filename:\n            ext = extension(filename or wfs.filename)\n            filename = '.'.join([self.basename(self._instance), ext])\n        prefix = self.upload_to(self._instance) if callable(self.upload_to) else self.upload_to\n        self.filename = self.fs.save(wfs, filename, prefix=prefix)\n        return self.filename"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsaving a Werkzeug FileStorage object.", "response": "def save(self, file_or_wfs, filename=None, bbox=None, overwrite=None):\n        '''Save a Werkzeug FileStorage object'''\n        self._mark_as_changed()\n        override = filename is not None\n        filename = filename or getattr(file_or_wfs, 'filename')\n\n        if self.basename and not override:\n            basename = self.basename(self._instance)\n        elif filename:\n            basename = splitext(filename)[0]\n        else:\n            raise ValueError('Filename is required')\n\n        ext = extension(filename)\n        prefix = self.upload_to(self._instance) if callable(self.upload_to) else self.upload_to\n        kwargs = {'prefix': prefix, 'overwrite': overwrite}\n\n        if self.optimize is not None:\n            should_optimize = self.optimize\n        else:\n            should_optimize = current_app.config['FS_IMAGES_OPTIMIZE']\n\n        def name(size=None):\n            if size:\n                return '.'.join(['-'.join([basename, str(size)]), ext])\n            else:\n                return '.'.join([basename, ext])\n\n        if self.max_size:\n            resized = resize(file_or_wfs, self.max_size)\n            file_or_wfs.seek(0)\n            if resized:\n                self.original = self.fs.save(file_or_wfs, name('original'), **kwargs)\n                self.filename = self.fs.save(resized, name(), **kwargs)\n            else:\n                self.filename = self.fs.save(file_or_wfs, name(), **kwargs)\n        elif should_optimize:\n            optimized = optimize(file_or_wfs)\n            file_or_wfs.seek(0)\n            self.original = self.fs.save(file_or_wfs, name('original'), **kwargs)\n            self.filename = self.fs.save(optimized, name(), **kwargs)\n        else:\n            self.filename = self.fs.save(file_or_wfs, name(), **kwargs)\n\n        if self.thumbnail_sizes:\n            self.bbox = bbox\n            for size in self.thumbnail_sizes:\n                file_or_wfs.seek(0)\n                thumbnail = make_thumbnail(file_or_wfs, size, self.bbox)\n                self.thumbnails[str(size)] = self.fs.save(FileStorage(thumbnail),\n                                                          name(size),\n                                                          **kwargs)\n        return self.filename"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef thumbnail(self, size):\n        '''Get the thumbnail filename for a given size'''\n        if size in self.thumbnail_sizes:\n            return self.thumbnails.get(str(size))\n        else:\n            raise ValueError('Unregistered thumbnail size {0}'.format(size))", "response": "Get the thumbnail filename for a given size"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef full(self, external=False):\n        '''Get the full image URL in respect with ``max_size``'''\n        return self.fs.url(self.filename, external=external) if self.filename else None", "response": "Get the full image URL in respect with max_size"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the best URL for downscaling.", "response": "def best_url(self, size=None, external=False):\n        '''\n        Provide the best thumbnail for downscaling.\n\n        If there is no match, provide the bigger if exists or the original\n        '''\n        if not self.thumbnail_sizes:\n            return self.url\n        elif not size:\n            self.thumbnail_sizes.sort()\n            best_size = self.thumbnail_sizes[-1]\n        else:\n            self.thumbnail_sizes.sort()\n            index = bisect.bisect_left(self.thumbnail_sizes, size)\n            if index >= len(self.thumbnail_sizes):\n                best_size = self.thumbnail_sizes[-1]\n            else:\n                best_size = self.thumbnail_sizes[index]\n        filename = self.thumbnail(best_size)\n        return self.fs.url(filename, external=external) if filename else None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nserves files for storages with direct file access", "response": "def get_file(fs, filename):\n    '''Serve files for storages with direct file access'''\n    storage = by_name(fs)\n    if storage is None:\n        abort(404)\n    return storage.serve(filename)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef init_app(app, *storages):\n    '''\n    Initialize Storages configuration\n    Register blueprint if necessary.\n\n    :param app: The `~flask.Flask` instance to get the configuration from.\n    :param storages: A  `Storage` instance list to register and configure.\n    '''\n\n    # Set default configuration\n    app.config.setdefault('FS_SERVE', app.config.get('DEBUG', False))\n    app.config.setdefault('FS_ROOT', join(app.instance_path, 'fs'))\n    app.config.setdefault('FS_PREFIX', None)\n    app.config.setdefault('FS_URL', None)\n    app.config.setdefault('FS_BACKEND', DEFAULT_BACKEND)\n    app.config.setdefault('FS_IMAGES_OPTIMIZE', False)\n\n    state = app.extensions['fs'] = app.extensions.get('fs', {})\n    for storage in storages:\n        storage.configure(app)\n        state[storage.name] = storage\n\n    from .views import bp\n    app.register_blueprint(bp, url_prefix=app.config['FS_PREFIX'])", "response": "Initialize Storages configuration and register blueprint if necessary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef lower_extension(filename):\n    '''\n    This is a helper used by :meth:`Storage.save` to provide lowercase extensions for\n    all processed files, to compare with configured extensions in the same\n    case.\n\n    :param str filename: The filename to ensure has a lowercase extension.\n    '''\n    if '.' in filename:\n        main, ext = os.path.splitext(filename)\n        return main + ext.lower()\n    # For consistency with os.path.splitext,\n    # do not treat a filename without an extension as an extension.\n    # That is, do not return filename.lower().\n    return filename", "response": "Lowercase the extension of a file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_metadata(self, filename):\n        '''Fetch all available metadata'''\n        dest = self.path(filename)\n        with open(dest, 'rb', buffering=0) as f:\n            checksum = 'sha1:{0}'.format(sha1(f))\n        return {\n            'checksum': checksum,\n            'size': os.path.getsize(dest),\n            'mime': files.mime(filename),\n            'modified': datetime.fromtimestamp(os.path.getmtime(dest)),\n        }", "response": "Fetch all available metadata for a file"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef clean(ctx, docs=False, bytecode=False, extra=''):\n    '''Cleanup all build artifacts'''\n    patterns = ['build', 'dist', 'cover', 'docs/_build', '**/*.pyc', '*.egg-info', '.tox']\n    for pattern in patterns:\n        print('Removing {0}'.format(pattern))\n        with ctx.cd(ROOT):\n            ctx.run('rm -rf {0}'.format(pattern))", "response": "Clean all build artifacts"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstops the middlewares (docker)", "response": "def stop(ctx, rm=False):\n    '''Stop the middlewares (docker)'''\n    with ctx.cd(ROOT):\n        compose(ctx, 'stop')\n        if rm:\n            compose(ctx, 'rm --force')"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrun tests suite with coverage", "response": "def cover(ctx, html=False):\n    '''Run tests suite with coverage'''\n    params = '--cov-report term --cov-report html' if html else ''\n    with ctx.cd(ROOT):\n        ctx.run('pytest --cov flask_fs {0}'.format(params), pty=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef on_touch_down(self, touch):\n        if not self.collide_point(*touch.pos):\n            return False\n        self.pos_start = self.pos\n        self.pos_down = (\n            self.x - touch.x,\n            self.y - touch.y\n        )\n        touch.grab(self)\n        self._touch = touch\n        return True", "response": "If hit record my starting position that I may return to it."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef on_touch_move(self, touch):\n        if touch is not self._touch:\n            return False\n        self.pos = (\n            touch.x + self.x_down,\n            touch.y + self.y_down\n        )\n        return True", "response": "Follow the touch and set the position of the touch."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn to pos_start but first save my current pos into pos_up", "response": "def on_touch_up(self, touch):\n        \"\"\"Return to ``pos_start``, but first, save my current ``pos`` into\n        ``pos_up``, so that the layout knows where to put the real\n        :class:`board.Spot` or :class:`board.Pawn` instance.\n\n        \"\"\"\n        if touch is not self._touch:\n            return False\n        self.pos_up = self.pos\n        self.pos = self.pos_start\n        self._touch = None\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef clear(self):\n        for k in list(self.keys()):\n            if k not in self.extrakeys:\n                del self[k]", "response": "Unset all data from the cache."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nassuming I am in a place and has a Portal return the number of turns the travel will take.", "response": "def go_to_place(self, place, weight=''):\n        \"\"\"Assuming I'm in a :class:`Place` that has a :class:`Portal` direct\n        to the given :class:`Place`, schedule myself to travel to the\n        given :class:`Place`, taking an amount of time indicated by\n        the ``weight`` stat on the :class:`Portal`, if given; else 1\n        turn.\n\n        Return the number of turns the travel will take.\n\n        \"\"\"\n        if hasattr(place, 'name'):\n            placen = place.name\n        else:\n            placen = place\n        curloc = self[\"location\"]\n        orm = self.character.engine\n        turns = self.engine._portal_objs[\n            (self.character.name, curloc, place)].get(weight, 1)\n        with self.engine.plan():\n            orm.turn += turns\n            self['location'] = placen\n        return turns"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfollow the path and returns the total number of turns the travel will take.", "response": "def follow_path(self, path, weight=None):\n        \"\"\"Go to several :class:`Place`s in succession, deciding how long to\n        spend in each by consulting the ``weight`` stat of the\n        :class:`Portal` connecting the one :class:`Place` to the next.\n\n        Return the total number of turns the travel will take. Raise\n        :class:`TravelException` if I can't follow the whole path,\n        either because some of its nodes don't exist, or because I'm\n        scheduled to be somewhere else.\n\n        \"\"\"\n        if len(path) < 2:\n            raise ValueError(\"Paths need at least 2 nodes\")\n        eng = self.character.engine\n        turn_now, tick_now = eng.time\n        with eng.plan():\n            prevplace = path.pop(0)\n            if prevplace != self['location']:\n                raise ValueError(\"Path does not start at my present location\")\n            subpath = [prevplace]\n            for place in path:\n                if (\n                        prevplace not in self.character.portal or\n                        place not in self.character.portal[prevplace]\n                ):\n                    raise TravelException(\n                        \"Couldn't follow portal from {} to {}\".format(\n                            prevplace,\n                            place\n                        ),\n                        path=subpath,\n                        traveller=self\n                    )\n                subpath.append(place)\n                prevplace = place\n            turns_total = 0\n            prevsubplace = subpath.pop(0)\n            subsubpath = [prevsubplace]\n            for subplace in subpath:\n                portal = self.character.portal[prevsubplace][subplace]\n                turn_inc = portal.get(weight, 1)\n                eng.turn += turn_inc\n                self.location = subplace\n                turns_total += turn_inc\n                subsubpath.append(subplace)\n                prevsubplace = subplace\n            self.location = subplace\n            eng.time = turn_now, tick_now\n        return turns_total"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngive a destination location and a weight try to follow it.", "response": "def travel_to(self, dest, weight=None, graph=None):\n        \"\"\"Find the shortest path to the given :class:`Place` from where I am\n        now, and follow it.\n\n        If supplied, the ``weight`` stat of the :class:`Portal`s along\n        the path will be used in pathfinding, and for deciding how\n        long to stay in each Place along the way.\n\n        The ``graph`` argument may be any NetworkX-style graph. It\n        will be used for pathfinding if supplied, otherwise I'll use\n        my :class:`Character`. In either case, however, I will attempt\n        to actually follow the path using my :class:`Character`, which\n        might not be possible if the supplied ``graph`` and my\n        :class:`Character` are too different. If it's not possible,\n        I'll raise a :class:`TravelException`, whose ``subpath``\n        attribute holds the part of the path that I *can* follow. To\n        make me follow it, pass it to my ``follow_path`` method.\n\n        Return value is the number of turns the travel will take.\n\n        \"\"\"\n        destn = dest.name if hasattr(dest, 'name') else dest\n        if destn == self.location.name:\n            raise ValueError(\"I'm already at {}\".format(destn))\n        graph = self.character if graph is None else graph\n        path = nx.shortest_path(graph, self[\"location\"], destn, weight)\n        return self.follow_path(path, weight)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef loading(self):\n        if getattr(self, '_initialized', False):\n            raise ValueError(\"Already loading\")\n        self._initialized = False\n        yield\n        self._initialized = True", "response": "Context manager for when you need to instantiate entities upon unpacking."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dice(self, n, d):\n        for i in range(0, n):\n            yield self.roll_die(d)", "response": "Roll n dice with d faces yield the results."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nrolls n dice with d sides sum them and return whether they are < target.", "response": "def dice_check(self, n, d, target, comparator='<='):\n        \"\"\"Roll ``n`` dice with ``d`` sides, sum them, and return whether they\n        are <= ``target``.\n\n        If ``comparator`` is provided, use it instead of <=. You may\n        use a string like '<' or '>='.\n\n        \"\"\"\n        from operator import gt, lt, ge, le, eq, ne\n\n        comps = {\n            '>': gt,\n            '<': lt,\n            '>=': ge,\n            '<=': le,\n            '=': eq,\n            '==': eq,\n            '!=': ne\n        }\n        try:\n            comparator = comps.get(comparator, comparator)\n        except TypeError:\n            pass\n        return comparator(sum(self.dice(n, d)), target)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngive a percentage of probability decide whether it actually happens right now and return True or False as appropriate.", "response": "def percent_chance(self, pct):\n        \"\"\"Given a ``pct``% chance of something happening right now, decide at\n        random whether it actually happens, and return ``True`` or\n        ``False`` as appropriate.\n\n        Values not between 0 and 100 are treated as though they\n        were 0 or 100, whichever is nearer.\n\n        \"\"\"\n        if pct <= 0:\n            return False\n        if pct >= 100:\n            return True\n        return pct / 100 < self.random()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_delta(self, branch, turn_from, tick_from, turn_to, tick_to):\n        from allegedb.window import update_window, update_backward_window\n        if turn_from == turn_to:\n            return self.get_turn_delta(branch, turn_to, tick_to, start_tick=tick_from)\n        delta = super().get_delta(branch, turn_from, tick_from, turn_to, tick_to)\n        if turn_from < turn_to:\n            updater = partial(update_window, turn_from, tick_from, turn_to, tick_to)\n            univbranches = self._universal_cache.settings\n            avbranches = self._avatarness_cache.settings\n            thbranches = self._things_cache.settings\n            rbbranches = self._rulebooks_cache.settings\n            trigbranches = self._triggers_cache.settings\n            preqbranches = self._prereqs_cache.settings\n            actbranches = self._actions_cache.settings\n            charrbbranches = self._characters_rulebooks_cache.settings\n            avrbbranches = self._avatars_rulebooks_cache.settings\n            charthrbbranches = self._characters_things_rulebooks_cache.settings\n            charplrbbranches = self._characters_places_rulebooks_cache.settings\n            charporbbranches = self._characters_portals_rulebooks_cache.settings\n            noderbbranches = self._nodes_rulebooks_cache.settings\n            edgerbbranches = self._portals_rulebooks_cache.settings\n        else:\n            updater = partial(update_backward_window, turn_from, tick_from, turn_to, tick_to)\n            univbranches = self._universal_cache.presettings\n            avbranches = self._avatarness_cache.presettings\n            thbranches = self._things_cache.presettings\n            rbbranches = self._rulebooks_cache.presettings\n            trigbranches = self._triggers_cache.presettings\n            preqbranches = self._prereqs_cache.presettings\n            actbranches = self._actions_cache.presettings\n            charrbbranches = self._characters_rulebooks_cache.presettings\n            avrbbranches = self._avatars_rulebooks_cache.presettings\n            charthrbbranches = self._characters_things_rulebooks_cache.presettings\n            charplrbbranches = self._characters_places_rulebooks_cache.presettings\n            charporbbranches = self._characters_portals_rulebooks_cache.presettings\n            noderbbranches = self._nodes_rulebooks_cache.presettings\n            edgerbbranches = self._portals_rulebooks_cache.presettings\n\n        def upduniv(_, key, val):\n            delta.setdefault('universal', {})[key] = val\n        if branch in univbranches:\n            updater(upduniv, univbranches[branch])\n\n        def updav(char, graph, node, av):\n            delta.setdefault(char, {}).setdefault('avatars', {}).setdefault(graph, {})[node] = bool(av)\n        if branch in avbranches:\n            updater(updav, avbranches[branch])\n\n        def updthing(char, thing, loc):\n            if (\n                char in delta and 'nodes' in delta[char]\n                and thing in delta[char]['nodes'] and not\n                delta[char]['nodes'][thing]\n            ):\n                return\n            thingd = delta.setdefault(char, {}).setdefault('node_val', {}).setdefault(thing, {})\n            thingd['location'] = loc\n        if branch in thbranches:\n            updater(updthing, thbranches[branch])\n        # TODO handle arrival_time and next_arrival_time stats of things\n\n        def updrb(whatev, rulebook, rules):\n            delta.setdefault('rulebooks', {})[rulebook] = rules\n\n        if branch in rbbranches:\n            updater(updrb, rbbranches[branch])\n\n        def updru(key, _, rule, funs):\n            delta.setdefault('rules', {}).setdefault(rule, {})[key] = funs\n\n        if branch in trigbranches:\n            updater(partial(updru, 'triggers'), trigbranches[branch])\n\n        if branch in preqbranches:\n            updater(partial(updru, 'prereqs'), preqbranches[branch])\n\n        if branch in actbranches:\n            updater(partial(updru, 'actions'), actbranches[branch])\n\n        def updcrb(key, _, character, rulebook):\n            delta.setdefault(character, {})[key] = rulebook\n\n        if branch in charrbbranches:\n            updater(partial(updcrb, 'character_rulebook'), charrbbranches[branch])\n\n        if branch in avrbbranches:\n            updater(partial(updcrb, 'avatar_rulebook'), avrbbranches[branch])\n\n        if branch in charthrbbranches:\n            updater(partial(updcrb, 'character_thing_rulebook'), charthrbbranches[branch])\n\n        if branch in charplrbbranches:\n            updater(partial(updcrb, 'character_place_rulebook'), charplrbbranches[branch])\n\n        if branch in charporbbranches:\n            updater(partial(updcrb, 'character_portal_rulebook'), charporbbranches[branch])\n\n        def updnoderb(character, node, rulebook):\n            if (\n                character in delta and 'nodes' in delta[character]\n                and node in delta[character]['nodes'] and not delta[character]['nodes'][node]\n            ):\n                return\n            delta.setdefault(character, {}).setdefault('node_val', {}).setdefault(node, {})['rulebook'] = rulebook\n\n        if branch in noderbbranches:\n            updater(updnoderb, noderbbranches[branch])\n\n        def updedgerb(character, orig, dest, rulebook):\n            if (\n                character in delta and 'edges' in delta[character]\n                and orig in delta[character]['edges'] and dest in delta[character]['edges'][orig]\n                and not delta[character]['edges'][orig][dest]\n            ):\n                return\n            delta.setdefault(character, {}).setdefault('edge_val', {}).setdefault(\n                orig, {}).setdefault(dest, {})['rulebook'] = rulebook\n\n        if branch in edgerbbranches:\n            updater(updedgerb, edgerbbranches[branch])\n\n        return delta", "response": "Get a dictionary describing changes to the world."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_turn_delta(self, branch=None, turn=None, tick=None, start_tick=0):\n        branch = branch or self.branch\n        turn = turn or self.turn\n        tick = tick or self.tick\n        delta = super().get_turn_delta(branch, turn, start_tick, tick)\n        if branch in self._avatarness_cache.settings and turn in self._avatarness_cache.settings[branch]:\n            for chara, graph, node, is_av in self._avatarness_cache.settings[branch][turn][start_tick:tick]:\n                delta.setdefault(chara, {}).setdefault('avatars', {}).setdefault(graph, {})[node] = is_av\n        if branch in self._things_cache.settings and turn in self._things_cache.settings[branch]:\n            for chara, thing, location in self._things_cache.settings[branch][turn][start_tick:tick]:\n                thingd = delta.setdefault(chara, {}).setdefault('node_val', {}).setdefault(thing, {})\n                thingd['location'] = location\n        delta['rulebooks'] = rbdif = {}\n        if branch in self._rulebooks_cache.settings and turn in self._rulebooks_cache.settings[branch]:\n            for _, rulebook, rules in self._rulebooks_cache.settings[branch][turn][start_tick:tick]:\n                rbdif[rulebook] = rules\n        delta['rules'] = rdif = {}\n        if branch in self._triggers_cache.settings and turn in self._triggers_cache.settings[branch]:\n            for _, rule, funs in self._triggers_cache.settings[branch][turn][start_tick:tick]:\n                rdif.setdefault(rule, {})['triggers'] = funs\n        if branch in self._prereqs_cache.settings and turn in self._prereqs_cache.settings[branch]:\n            for _, rule, funs in self._prereqs_cache.settings[branch][turn][start_tick:tick]:\n                rdif.setdefault(rule, {})['prereqs'] = funs\n        if branch in self._actions_cache.settings and turn in self._triggers_cache.settings[branch]:\n            for _, rule, funs in self._triggers_cache.settings[branch][turn][start_tick:tick]:\n                rdif.setdefault(rule, {})['actions'] = funs\n\n        if branch in self._characters_rulebooks_cache.settings and turn in self._characters_rulebooks_cache.settings[branch]:\n            for _, character, rulebook in self._characters_rulebooks_cache.settings[branch][turn][start_tick:tick]:\n                delta.setdefault(character, {})['character_rulebook'] = rulebook\n        if branch in self._avatars_rulebooks_cache.settings and turn in self._avatars_rulebooks_cache.settings[branch]:\n            for _, character, rulebook in self._avatars_rulebooks_cache.settings[branch][turn][start_tick:tick]:\n                delta.setdefault(character, {})['avatar_rulebook'] = rulebook\n        if branch in self._characters_things_rulebooks_cache.settings and turn in self._characters_things_rulebooks_cache.settings[branch]:\n            for _, character, rulebook in self._characters_things_rulebooks_cache.settings[branch][turn][start_tick:tick]:\n                delta.setdefault(character, {})['character_thing_rulebook'] = rulebook\n        if branch in self._characters_places_rulebooks_cache.settings and turn in self._characters_places_rulebooks_cache.settings[branch]:\n            for _, character, rulebook in self._characters_places_rulebooks_cache.settings[branch][turn][start_tick:tick]:\n                delta.setdefault(character, {})['character_place_rulebook'] = rulebook\n        if branch in self._characters_portals_rulebooks_cache.settings and turn in self._characters_portals_rulebooks_cache.settings[branch]:\n            for _, character, rulebook in self._characters_portals_rulebooks_cache.settings[branch][turn][start_tick:tick]:\n                delta.setdefault(character, {})['character_portal_rulebook'] = rulebook\n\n        if branch in self._nodes_rulebooks_cache.settings and turn in self._nodes_rulebooks_cache.settings[branch]:\n            for character, node, rulebook in self._nodes_rulebooks_cache.settings[branch][turn][start_tick:tick]:\n                delta.setdefault(character, {}).setdefault('node_val', {}).setdefault(node, {})['rulebook'] = rulebook\n        if branch in self._portals_rulebooks_cache.settings and turn in self._portals_rulebooks_cache.settings[branch]:\n            for character, orig, dest, rulebook in self._portals_rulebooks_cache.settings[branch][turn][start_tick:tick]:\n                delta.setdefault(character, {}).setdefault('edge_val', {})\\\n                    .setdefault(orig, {}).setdefault(dest, {})['rulebook'] = rulebook\n        return delta", "response": "Returns a dictionary describing changes to the world within a given branch and turn."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nuses this to remember a change in avatarness.", "response": "def _remember_avatarness(\n            self, character, graph, node,\n            is_avatar=True, branch=None, turn=None,\n            tick=None\n    ):\n        \"\"\"Use this to record a change in avatarness.\n\n        Should be called whenever a node that wasn't an avatar of a\n        character now is, and whenever a node that was an avatar of a\n        character now isn't.\n\n        ``character`` is the one using the node as an avatar,\n        ``graph`` is the character the node is in.\n\n        \"\"\"\n        branch = branch or self.branch\n        turn = turn or self.turn\n        tick = tick or self.tick\n        self._avatarness_cache.store(\n            character,\n            graph,\n            node,\n            branch,\n            turn,\n            tick,\n            is_avatar\n        )\n        self.query.avatar_set(\n            character,\n            graph,\n            node,\n            branch,\n            turn,\n            tick,\n            is_avatar\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _init_caches(self):\n        from .xcollections import (\n            StringStore,\n            FunctionStore,\n            CharacterMapping,\n            UniversalMapping\n        )\n        from .cache import (\n            Cache,\n            NodeContentsCache,\n            InitializedCache,\n            EntitylessCache,\n            InitializedEntitylessCache,\n            AvatarnessCache,\n            AvatarRulesHandledCache,\n            CharacterThingRulesHandledCache,\n            CharacterPlaceRulesHandledCache,\n            CharacterPortalRulesHandledCache,\n            NodeRulesHandledCache,\n            PortalRulesHandledCache,\n            CharacterRulesHandledCache,\n            ThingsCache\n        )\n        from .rule import AllRuleBooks, AllRules\n\n        super()._init_caches()\n        self._things_cache = ThingsCache(self)\n        self._things_cache.setdb = self.query.set_thing_loc\n        self._node_contents_cache = NodeContentsCache(self)\n        self.character = self.graph = CharacterMapping(self)\n        self._universal_cache = EntitylessCache(self)\n        self._universal_cache.setdb = self.query.universal_set\n        self._rulebooks_cache = InitializedEntitylessCache(self)\n        self._rulebooks_cache.setdb = self.query.rulebook_set\n        self._characters_rulebooks_cache = InitializedEntitylessCache(self)\n        self._avatars_rulebooks_cache = InitializedEntitylessCache(self)\n        self._characters_things_rulebooks_cache = InitializedEntitylessCache(self)\n        self._characters_places_rulebooks_cache = InitializedEntitylessCache(self)\n        self._characters_portals_rulebooks_cache = InitializedEntitylessCache(self)\n        self._nodes_rulebooks_cache = InitializedCache(self)\n        self._portals_rulebooks_cache = InitializedCache(self)\n        self._triggers_cache = InitializedEntitylessCache(self)\n        self._prereqs_cache = InitializedEntitylessCache(self)\n        self._actions_cache = InitializedEntitylessCache(self)\n        self._node_rules_handled_cache = NodeRulesHandledCache(self)\n        self._portal_rules_handled_cache = PortalRulesHandledCache(self)\n        self._character_rules_handled_cache = CharacterRulesHandledCache(self)\n        self._avatar_rules_handled_cache = AvatarRulesHandledCache(self)\n        self._character_thing_rules_handled_cache \\\n            = CharacterThingRulesHandledCache(self)\n        self._character_place_rules_handled_cache \\\n            = CharacterPlaceRulesHandledCache(self)\n        self._character_portal_rules_handled_cache \\\n            = CharacterPortalRulesHandledCache(self)\n        self._avatarness_cache = AvatarnessCache(self)\n        self._turns_completed = defaultdict(lambda: max((0, self.turn - 1)))\n        \"\"\"The last turn when the rules engine ran in each branch\"\"\"\n        self.eternal = self.query.globl\n        self.universal = UniversalMapping(self)\n        if hasattr(self, '_action_file'):\n            self.action = FunctionStore(self._action_file)\n        if hasattr(self, '_prereq_file'):\n            self.prereq = FunctionStore(self._prereq_file)\n        if hasattr(self, '_trigger_file'):\n            self.trigger = FunctionStore(self._trigger_file)\n        if hasattr(self, '_function_file'):\n            self.function = FunctionStore(self._function_file)\n        if hasattr(self, '_method_file'):\n            self.method = FunctionStore(self._method_file)\n        self.rule = AllRules(self)\n        self.rulebook = AllRuleBooks(self)\n        if hasattr(self, '_string_file'):\n            self.string = StringStore(\n                self.query,\n                self._string_file,\n                self.eternal.setdefault('language', 'eng')\n            )", "response": "Initializes the internal caches for the object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncommit changes and close the database.", "response": "def close(self):\n        \"\"\"Commit changes and close the database.\"\"\"\n        import sys, os\n        for store in self.stores:\n            if hasattr(store, 'save'):\n                store.save(reimport=False)\n            path, filename = os.path.split(store._filename)\n            modname = filename[:-3]\n            if modname in sys.modules:\n                del sys.modules[modname]\n        super().close()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfollow the next rule if available.", "response": "def advance(self):\n        \"\"\"Follow the next rule if available.\n\n        If we've run out of rules, reset the rules iterator.\n\n        \"\"\"\n        try:\n            return next(self._rules_iter)\n        except InnerStopIteration:\n            self._rules_iter = self._follow_rules()\n            return StopIteration()\n        except StopIteration:\n            self._rules_iter = self._follow_rules()\n            return final_rule"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates and return a new character.", "response": "def new_character(self, name, data=None, **kwargs):\n        \"\"\"Create and return a new :class:`Character`.\"\"\"\n        self.add_character(name, data, **kwargs)\n        return self.character[name]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_character(self, name, data=None, **kwargs):\n        self._init_graph(name, 'DiGraph')\n        self._graph_objs[name] = self.char_cls(self, name, data, **kwargs)", "response": "Add a new character to the networkx graph by looking up name in my character property."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nremove the Character from the database entirely.", "response": "def del_character(self, name):\n        \"\"\"Remove the Character from the database entirely.\n\n        This also deletes all its history. You'd better be sure.\n\n        \"\"\"\n        self.query.del_character(name)\n        self.del_graph(name)\n        del self.character[name]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef alias(self, v, stat='dummy'):\n        from .util import EntityStatAccessor\n        r = DummyEntity(self)\n        r[stat] = v\n        return EntityStatAccessor(r, stat, engine=self)", "response": "Return a representation of a value suitable for use in historical queries."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchanging the interval at which self. play is called to match myIsoVersion. play_speed.", "response": "def on_play_speed(self, *args):\n        \"\"\"Change the interval at which ``self.play`` is called to match my\n        current ``play_speed``.\n\n        \"\"\"\n        Clock.unschedule(self.play)\n        Clock.schedule_interval(self.play, 1.0 / self.play_speed)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef remake_display(self, *args):\n        Builder.load_string(self.kv)\n        if hasattr(self, '_kv_layout'):\n            self.remove_widget(self._kv_layout)\n            del self._kv_layout\n        self._kv_layout = KvLayout()\n        self.add_widget(self._kv_layout)", "response": "Remake any affected widgets after a change in my kv."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef on_dummies(self, *args):\n        def renum_dummy(dummy, *args):\n            dummy.num = dummynum(self.app.character, dummy.prefix) + 1\n\n        for dummy in self.dummies:\n            if dummy is None or hasattr(dummy, '_numbered'):\n                continue\n            if dummy == self.dummything:\n                self.app.pawncfg.bind(imgpaths=self._propagate_thing_paths)\n            if dummy == self.dummyplace:\n                self.app.spotcfg.bind(imgpaths=self._propagate_place_paths)\n            dummy.num = dummynum(self.app.character, dummy.prefix) + 1\n            Logger.debug(\"MainScreen: dummy #{}\".format(dummy.num))\n            dummy.bind(prefix=partial(renum_dummy, dummy))\n            dummy._numbered = True", "response": "Give the dummies numbers such that when appended to their names they give a unique name for the resulting new\n       ."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadvance time by one turn.", "response": "def next_turn(self, *args):\n        \"\"\"Advance time by one turn, if it's not blocked.\n\n        Block time by setting ``engine.universal['block'] = True``\"\"\"\n        if self.tmp_block:\n            return\n        eng = self.app.engine\n        dial = self.dialoglayout\n        if eng.universal.get('block'):\n            Logger.info(\"MainScreen: next_turn blocked, delete universal['block'] to unblock\")\n            return\n        if dial.idx < len(dial.todo):\n            Logger.info(\"MainScreen: not advancing time while there's a dialog\")\n            return\n        self.tmp_block = True\n        self.app.unbind(\n            branch=self.app._push_time,\n            turn=self.app._push_time,\n            tick=self.app._push_time\n        )\n        eng.next_turn(cb=self._update_from_next_turn)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef setgraphval(delta, graph, key, val):\n    delta.setdefault(graph, {})[key] = val", "response": "Change a delta to say that a graph stat was set to a certain value"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef setnode(delta, graph, node, exists):\n    delta.setdefault(graph, {}).setdefault('nodes', {})[node] = bool(exists)", "response": "Change a delta to say that a node was created or deleted"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchanging a delta to say that a node stat was set to a certain value", "response": "def setnodeval(delta, graph, node, key, value):\n    \"\"\"Change a delta to say that a node stat was set to a certain value\"\"\"\n    if (\n        graph in delta and 'nodes' in delta[graph] and\n        node in delta[graph]['nodes'] and not delta[graph]['nodes'][node]\n    ):\n        return\n    delta.setdefault(graph, {}).setdefault('node_val', {}).setdefault(node, {})[key] = value"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchanging a delta to say that an edge was created or deleted", "response": "def setedge(delta, is_multigraph, graph, orig, dest, idx, exists):\n    \"\"\"Change a delta to say that an edge was created or deleted\"\"\"\n    if is_multigraph(graph):\n        delta.setdefault(graph, {}).setdefault('edges', {})\\\n            .setdefault(orig, {}).setdefault(dest, {})[idx] = bool(exists)\n    else:\n        delta.setdefault(graph, {}).setdefault('edges', {})\\\n            .setdefault(orig, {})[dest] = bool(exists)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nchange a delta to say that an edge stat was set to a certain value", "response": "def setedgeval(delta, is_multigraph, graph, orig, dest, idx, key, value):\n    \"\"\"Change a delta to say that an edge stat was set to a certain value\"\"\"\n    if is_multigraph(graph):\n        if (\n            graph in delta and 'edges' in delta[graph] and\n            orig in delta[graph]['edges'] and dest in delta[graph]['edges'][orig]\n            and idx in delta[graph]['edges'][orig][dest]\n            and not delta[graph]['edges'][orig][dest][idx]\n        ):\n            return\n        delta.setdefault(graph, {}).setdefault('edge_val', {})\\\n            .setdefault(orig, {}).setdefault(dest, {})\\\n            .setdefault(idx, {})[key] = value\n    else:\n        if (\n                                    graph in delta and 'edges' in delta[graph] and\n                                orig in delta[graph]['edges'] and dest in delta[graph]['edges'][orig]\n                and not delta[graph]['edges'][orig][dest]\n        ):\n            return\n        delta.setdefault(graph, {}).setdefault('edge_val', {})\\\n            .setdefault(orig, {}).setdefault(dest, {})[key] = value"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting a dictionary describing changes to all graphs.", "response": "def get_delta(self, branch, turn_from, tick_from, turn_to, tick_to):\n        \"\"\"Get a dictionary describing changes to all graphs.\n\n        The keys are graph names. Their values are dictionaries of the graphs'\n        attributes' new values, with ``None`` for deleted keys. Also in those graph\n        dictionaries are special keys 'node_val' and 'edge_val' describing changes\n        to node and edge attributes, and 'nodes' and 'edges' full of booleans\n        indicating whether a node or edge exists.\n\n        \"\"\"\n        from functools import partial\n        if turn_from == turn_to:\n            return self.get_turn_delta(branch, turn_from, tick_from, tick_to)\n        delta = {}\n        graph_objs = self._graph_objs\n        if turn_to < turn_from:\n            updater = partial(update_backward_window, turn_from, tick_from, turn_to, tick_to)\n            gvbranches = self._graph_val_cache.presettings\n            nbranches = self._nodes_cache.presettings\n            nvbranches = self._node_val_cache.presettings\n            ebranches = self._edges_cache.presettings\n            evbranches = self._edge_val_cache.presettings\n        else:\n            updater = partial(update_window, turn_from, tick_from, turn_to, tick_to)\n            gvbranches = self._graph_val_cache.settings\n            nbranches = self._nodes_cache.settings\n            nvbranches = self._node_val_cache.settings\n            ebranches = self._edges_cache.settings\n            evbranches = self._edge_val_cache.settings\n\n        if branch in gvbranches:\n            updater(partial(setgraphval, delta), gvbranches[branch])\n\n        if branch in nbranches:\n            updater(partial(setnode, delta), nbranches[branch])\n\n        if branch in nvbranches:\n            updater(partial(setnodeval, delta), nvbranches[branch])\n\n        if branch in ebranches:\n            updater(partial(setedge, delta, lambda g: graph_objs[g].is_multigraph()), ebranches[branch])\n\n        if branch in evbranches:\n            updater(partial(setedgeval, delta, lambda g: graph_objs[g].is_multigraph()), evbranches[branch])\n\n        return delta"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a dictionary describing changes made on a given turn.", "response": "def get_turn_delta(self, branch=None, turn=None, tick_from=0, tick_to=None):\n        \"\"\"Get a dictionary describing changes made on a given turn.\n\n        If ``tick_to`` is not supplied, report all changes after ``tick_from``\n        (default 0).\n\n        The keys are graph names. Their values are dictionaries of the graphs'\n        attributes' new values, with ``None`` for deleted keys. Also in those graph\n        dictionaries are special keys 'node_val' and 'edge_val' describing changes\n        to node and edge attributes, and 'nodes' and 'edges' full of booleans\n        indicating whether a node or edge exists.\n\n        :arg branch: A branch of history; defaults to the current branch\n        :arg turn: The turn in the branch; defaults to the current turn\n        :arg tick_from: Starting tick; defaults to 0\n\n        \"\"\"\n        branch = branch or self.branch\n        turn = turn or self.turn\n        tick_to = tick_to or self.tick\n        delta = {}\n        if tick_from < tick_to:\n            gvbranches = self._graph_val_cache.settings\n            nbranches = self._nodes_cache.settings\n            nvbranches = self._node_val_cache.settings\n            ebranches = self._edges_cache.settings\n            evbranches = self._edge_val_cache.settings\n        else:\n            gvbranches = self._graph_val_cache.presettings\n            nbranches = self._nodes_cache.presettings\n            nvbranches = self._node_val_cache.presettings\n            ebranches = self._edges_cache.presettings\n            evbranches = self._edge_val_cache.presettings\n\n        if branch in gvbranches and turn in gvbranches[branch]:\n            for graph, key, value in gvbranches[branch][turn][tick_from:tick_to]:\n                if graph in delta:\n                    delta[graph][key] = value\n                else:\n                    delta[graph] = {key: value}\n\n        if branch in nbranches and turn in nbranches[branch]:\n            for graph, node, exists in nbranches[branch][turn][tick_from:tick_to]:\n                delta.setdefault(graph, {}).setdefault('nodes', {})[node] = bool(exists)\n\n        if branch in nvbranches and turn in nvbranches[branch]:\n            for graph, node, key, value in nvbranches[branch][turn][tick_from:tick_to]:\n                if (\n                    graph in delta and 'nodes' in delta[graph] and\n                    node in delta[graph]['nodes'] and not delta[graph]['nodes'][node]\n                ):\n                    continue\n                nodevd = delta.setdefault(graph, {}).setdefault('node_val', {})\n                if node in nodevd:\n                    nodevd[node][key] = value\n                else:\n                    nodevd[node] = {key: value}\n\n        graph_objs = self._graph_objs\n        if branch in ebranches and turn in ebranches[branch]:\n            for graph, orig, dest, idx, exists in ebranches[branch][turn][tick_from:tick_to]:\n                if graph_objs[graph].is_multigraph():\n                    if (\n                        graph in delta and 'edges' in delta[graph] and\n                        orig in delta[graph]['edges'] and dest in delta[graph]['edges'][orig]\n                        and idx in delta[graph]['edges'][orig][dest]\n                        and not delta[graph]['edges'][orig][dest][idx]\n                    ):\n                        continue\n                    delta.setdefault(graph, {}).setdefault('edges', {})\\\n                        .setdefault(orig, {}).setdefault(dest, {})[idx] = bool(exists)\n                else:\n                    if (\n                        graph in delta and 'edges' in delta[graph] and\n                        orig in delta[graph]['edges'] and dest in delta[graph]['edges'][orig]\n                        and not delta[graph]['edges'][orig][dest]\n                    ):\n                        continue\n                    delta.setdefault(graph, {}).setdefault('edges', {})\\\n                        .setdefault(orig, {})[dest] = bool(exists)\n\n        if branch in evbranches and turn in evbranches[branch]:\n            for graph, orig, dest, idx, key, value in evbranches[branch][turn][tick_from:tick_to]:\n                edgevd = delta.setdefault(graph, {}).setdefault('edge_val', {})\\\n                    .setdefault(orig, {}).setdefault(dest, {})\n                if graph_objs[graph].is_multigraph():\n                    if idx in edgevd:\n                        edgevd[idx][key] = value\n                    else:\n                        edgevd[idx] = {key: value}\n                else:\n                    edgevd[key] = value\n\n        return delta"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _init_caches(self):\n        from collections import defaultdict\n        from .cache import Cache, NodesCache, EdgesCache\n        self._where_cached = defaultdict(list)\n        self._global_cache = self.query._global_cache = {}\n        self._node_objs = node_objs = WeakValueDictionary()\n        self._get_node_stuff = (node_objs, self._node_exists, self._make_node)\n        self._edge_objs = edge_objs = WeakValueDictionary()\n        self._get_edge_stuff = (edge_objs, self._edge_exists, self._make_edge)\n        for k, v in self.query.global_items():\n            if k == 'branch':\n                self._obranch = v\n            elif k == 'turn':\n                self._oturn = int(v)\n            elif k == 'tick':\n                self._otick = int(v)\n            else:\n                self._global_cache[k] = v\n        self._childbranch = defaultdict(set)\n        \"\"\"Immediate children of a branch\"\"\"\n        self._branches = {}\n        \"\"\"Start time, end time, and parent of each branch\"\"\"\n        self._branch_parents = defaultdict(set)\n        \"\"\"Parents of a branch at any remove\"\"\"\n        self._turn_end = defaultdict(lambda: 0)\n        \"\"\"Tick on which a (branch, turn) ends\"\"\"\n        self._turn_end_plan = defaultdict(lambda: 0)\n        \"\"\"Tick on which a (branch, turn) ends, even if it hasn't been simulated\"\"\"\n        self._graph_objs = {}\n        self._plans = {}\n        self._branches_plans = defaultdict(set)\n        self._plan_ticks = defaultdict(lambda: defaultdict(list))\n        self._time_plan = {}\n        self._plans_uncommitted = []\n        self._plan_ticks_uncommitted = []\n        self._graph_val_cache = Cache(self)\n        self._graph_val_cache.setdb = self.query.graph_val_set\n        self._graph_val_cache.deldb = self.query.graph_val_del_time\n        self._nodes_cache = NodesCache(self)\n        self._nodes_cache.setdb = self.query.exist_node\n        self._nodes_cache.deldb = self.query.nodes_del_time\n        self._edges_cache = EdgesCache(self)\n        self._edges_cache.setdb = self.query.exist_edge\n        self._edges_cache.deldb = self.query.edges_del_time\n        self._node_val_cache = Cache(self)\n        self._node_val_cache.setdb = self.query.node_val_set\n        self._node_val_cache.deldb = self.query.node_val_del_time\n        self._edge_val_cache = Cache(self)\n        self._edge_val_cache.setdb = self.query.edge_val_set\n        self._edge_val_cache.deldb = self.query.edge_val_del_time", "response": "Initializes the internal caches for the current instance of this class."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_parent_of(self, parent, child):\n        if parent == 'trunk':\n            return True\n        if child == 'trunk':\n            return False\n        if child not in self._branches:\n            raise ValueError(\n                \"The branch {} seems not to have ever been created\".format(\n                    child\n                )\n            )\n        if self._branches[child][0] == parent:\n            return True\n        return self.is_parent_of(parent, self._branches[child][0])", "response": "Return whether the child is a branch descended from parent at\n        any remove."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncopy all active plans to the current branch", "response": "def _copy_plans(self, branch_from, turn_from, tick_from):\n        \"\"\"Collect all plans that are active at the given time and copy them to the current branch\"\"\"\n        plan_ticks = self._plan_ticks\n        plan_ticks_uncommitted = self._plan_ticks_uncommitted\n        time_plan = self._time_plan\n        plans = self._plans\n        branch = self.branch\n        where_cached = self._where_cached\n        last_plan = self._last_plan\n        turn_end_plan = self._turn_end_plan\n        for plan_id in self._branches_plans[branch_from]:\n            _, start_turn, start_tick = plans[plan_id]\n            if start_turn > turn_from or (start_turn == turn_from and start_tick > tick_from):\n                continue\n            incremented = False\n            for turn, ticks in list(plan_ticks[plan_id].items()):\n                if turn < turn_from:\n                    continue\n                for tick in ticks:\n                    if turn == turn_from and tick < tick_from:\n                        continue\n                    if not incremented:\n                        self._last_plan = last_plan = last_plan + 1\n                        incremented = True\n                        plans[last_plan] = branch, turn, tick\n                    for cache in where_cached[branch_from, turn, tick]:\n                        data = cache.settings[branch_from][turn][tick]\n                        value = data[-1]\n                        key = data[:-1]\n                        args = key + (branch, turn, tick, value)\n                        if hasattr(cache, 'setdb'):\n                            cache.setdb(*args)\n                        cache.store(*args, planning=True)\n                        plan_ticks[last_plan][turn].append(tick)\n                        plan_ticks_uncommitted.append((last_plan, turn, tick))\n                        time_plan[branch, turn, tick] = last_plan\n                        turn_end_plan[branch, turn] = tick"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef delete_plan(self, plan):\n        branch, turn, tick = self._btt()\n        to_delete = []\n        plan_ticks = self._plan_ticks[plan]\n        for trn, tcks in plan_ticks.items():  # might improve performance to use a WindowDict for plan_ticks\n            if turn == trn:\n                for tck in tcks:\n                    if tck >= tick:\n                        to_delete.append((trn, tck))\n            elif trn > turn:\n                to_delete.extend((trn, tck) for tck in tcks)\n        # Delete stuff that happened at contradicted times, and then delete the times from the plan\n        where_cached = self._where_cached\n        time_plan = self._time_plan\n        for trn, tck in to_delete:\n            for cache in where_cached[branch, trn, tck]:\n                cache.remove(branch, trn, tck)\n                if hasattr(cache, 'deldb'):\n                    cache.deldb(branch, trn, tck)\n            del where_cached[branch, trn, tck]\n            plan_ticks[trn].remove(tck)\n            if not plan_ticks[trn]:\n                del plan_ticks[trn]\n            del time_plan[branch, trn, tck]", "response": "Delete the portion of a plan that has yet to occur."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _nbtt(self):\n        from .cache import HistoryError\n        branch, turn, tick = self._btt()\n        tick += 1\n        if (branch, turn) in self._turn_end_plan:\n            if tick > self._turn_end_plan[branch, turn]:\n                self._turn_end_plan[branch, turn] = tick\n            else:\n                tick = self._turn_end_plan[branch, turn] + 1\n        self._turn_end_plan[branch, turn] = tick\n        if self._turn_end[branch, turn] > tick:\n            raise HistoryError(\n                \"You're not at the end of turn {}. Go to tick {} to change things\".format(\n                    turn, self._turn_end[branch, turn]\n                )\n            )\n        parent, turn_start, tick_start, turn_end, tick_end = self._branches[branch]\n        if turn < turn_end or (\n            turn == turn_end and tick < tick_end\n        ):\n            raise HistoryError(\n                \"You're in the past. Go to turn {}, tick {} to change things\".format(turn_end, tick_end)\n            )\n        if self._planning:\n            if (turn, tick) in self._plan_ticks[self._last_plan]:\n                raise HistoryError(\n                    \"Trying to make a plan at {}, but that time already happened\".format((branch, turn, tick))\n                )\n            self._plan_ticks[self._last_plan][turn].append(tick)\n            self._plan_ticks_uncommitted.append((self._last_plan, turn, tick))\n            self._time_plan[branch, turn, tick] = self._last_plan\n        self._otick = tick\n        return branch, turn, tick", "response": "Increment the tick and return branch turn tick."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef commit(self):\n        self.query.globl['branch'] = self._obranch\n        self.query.globl['turn'] = self._oturn\n        self.query.globl['tick'] = self._otick\n        set_branch = self.query.set_branch\n        for branch, (parent, turn_start, tick_start, turn_end, tick_end) in self._branches.items():\n            set_branch(branch, parent, turn_start, tick_start, turn_end, tick_end)\n        turn_end = self._turn_end\n        set_turn = self.query.set_turn\n        for (branch, turn), plan_end_tick in self._turn_end_plan.items():\n            set_turn(branch, turn, turn_end[branch], plan_end_tick)\n        if self._plans_uncommitted:\n            self.query.plans_insert_many(self._plans_uncommitted)\n        if self._plan_ticks_uncommitted:\n            self.query.plan_ticks_insert_many(self._plan_ticks_uncommitted)\n        self.query.commit()\n        self._plans_uncommitted = []\n        self._plan_ticks_uncommitted = []", "response": "Write the state of all graphs to the database and commit the transaction.\n\n        Also saves the current branch, turn, and tick."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef new_graph(self, name, data=None, **attr):\n        self._init_graph(name, 'Graph')\n        g = Graph(self, name, data, **attr)\n        self._graph_objs[name] = g\n        return g", "response": "Return a new Graph instance initialized with the given data if provided."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef new_digraph(self, name, data=None, **attr):\n        self._init_graph(name, 'DiGraph')\n        dg = DiGraph(self, name, data, **attr)\n        self._graph_objs[name] = dg\n        return dg", "response": "Return a new DiGraph initialized with the given data if provided."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a new instance of type MultiGraph initialized with the given data if provided.", "response": "def new_multigraph(self, name, data=None, **attr):\n        \"\"\"Return a new instance of type MultiGraph, initialized with the given\n        data if provided.\n\n        :arg name: a name for the graph\n        :arg data: dictionary or NetworkX graph object providing initial state\n\n        \"\"\"\n        self._init_graph(name, 'MultiGraph')\n        mg = MultiGraph(self, name, data, **attr)\n        self._graph_objs[name] = mg\n        return mg"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a new instance of type MultiDiGraph initialized with the given data if provided.", "response": "def new_multidigraph(self, name, data=None, **attr):\n        \"\"\"Return a new instance of type MultiDiGraph, initialized with the given\n        data if provided.\n\n        :arg name: a name for the graph\n        :arg data: dictionary or NetworkX graph object providing initial state\n\n        \"\"\"\n        self._init_graph(name, 'MultiDiGraph')\n        mdg = MultiDiGraph(self, name, data, **attr)\n        self._graph_objs[name] = mdg\n        return mdg"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a graph object for the given name.", "response": "def get_graph(self, name):\n        \"\"\"Return a graph previously created with ``new_graph``,\n        ``new_digraph``, ``new_multigraph``, or\n        ``new_multidigraph``\n\n        :arg name: name of an existing graph\n\n        \"\"\"\n        if name in self._graph_objs:\n            return self._graph_objs[name]\n        graphtypes = {\n            'Graph': Graph,\n            'DiGraph': DiGraph,\n            'MultiGraph': MultiGraph,\n            'MultiDiGraph': MultiDiGraph\n        }\n        type_s = self.query.graph_type(name)\n        if type_s not in graphtypes:\n            raise GraphNameError(\n                \"I don't know of a graph named {}\".format(name)\n            )\n        g = graphtypes[type_s](self, name)\n        self._graph_objs[name] = g\n        return g"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove all traces of a graph from the database", "response": "def del_graph(self, name):\n        \"\"\"Remove all traces of a graph's existence from the database\n\n        :arg name: name of an existing graph\n\n        \"\"\"\n        # make sure the graph exists before deleting anything\n        self.get_graph(name)\n        self.query.del_graph(name)\n        if name in self._graph_objs:\n            del self._graph_objs[name]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\niterate over all branches immediately descended from the current one.", "response": "def _branch_descendants(self, branch=None):\n        \"\"\"Iterate over all branches immediately descended from the current\n        one (or the given one, if available).\n\n        \"\"\"\n        branch = branch or self.branch\n        for (parent, (child, _, _, _, _)) in self._branches.items():\n            if parent == branch:\n                yield child"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef plot_ltt(lineages, show_plot=True, color='#000000', xmin=None, xmax=None, ymin=None, ymax=None, title=None, xlabel=None, ylabel=None):\n    '''Plot the Lineages Through Time (LTT) curve of a given tree\n\n    Args:\n        ``lineages`` (``dict``): The ``lineages`` dictionary returned by a ``Tree`` object's ``lineages_through_time()`` function call\n\n        ``show_plot`` (``bool``): ``True`` to show the plot, otherwise ``False`` to only return the dictionary. To plot multiple LTTs on the same figure, set ``show_plot`` to False for all but the last plot.\n\n        ``color`` (``str``): The color of the resulting plot\n\n        ``title`` (``str``): The title of the resulting plot\n\n        ``xmin`` (``float``): The minimum value of the horizontal axis in the resulting plot\n\n        ``xmax`` (``float``): The maximum value of the horizontal axis in the resulting plot\n\n        ``xlabel`` (``str``): The label of the horizontal axis in the resulting plot\n\n        ``ymin`` (``float``): The minimum value of the vertical axis in the resulting plot\n\n        ``ymax`` (``float``): The maximum value of the vertical axis in the resulting plot\n\n        ``ylabel`` (``str``): The label of the vertical axis in the resulting plot\n    '''\n    import matplotlib.pyplot as plt; from matplotlib.ticker import MaxNLocator\n    if 'TREESWIFT_FIGURE' not in globals():\n        global TREESWIFT_FIGURE; TREESWIFT_FIGURE = None\n    if TREESWIFT_FIGURE is None:\n        TREESWIFT_FIGURE = plt.figure()\n        TREESWIFT_FIGURE.gca().yaxis.set_major_locator(MaxNLocator(integer=True)) # integer y ticks\n        TREESWIFT_FIGURE.XMIN = float('inf'); TREESWIFT_FIGURE.XMAX = float('-inf')\n        TREESWIFT_FIGURE.YMIN = float('inf'); TREESWIFT_FIGURE.YMAX = float('-inf')\n    times = sorted(lineages.keys())\n    if times[0] < TREESWIFT_FIGURE.XMIN:\n        TREESWIFT_FIGURE.XMIN = times[0]\n    if times[-1] > TREESWIFT_FIGURE.XMAX:\n        TREESWIFT_FIGURE.XMAX = times[-1]\n    for i in range(len(times)-1):\n        if i == 0:\n            prev = 0\n        else:\n            prev = lineages[times[i-1]]\n        if lineages[times[i]] > TREESWIFT_FIGURE.YMAX:\n            TREESWIFT_FIGURE.YMAX = lineages[times[i]]\n        if lineages[times[i]] < TREESWIFT_FIGURE.YMIN:\n            TREESWIFT_FIGURE.YMIN = lineages[times[i]]\n        TREESWIFT_FIGURE.gca().plot([times[i],times[i]], [prev,lineages[times[i]]], color=color)\n        TREESWIFT_FIGURE.gca().plot([times[i],times[i+1]], [lineages[times[i]],lineages[times[i]]], color=color)\n    if len(times) > 1:\n        TREESWIFT_FIGURE.gca().plot([times[-1],times[-1]], [lineages[times[-2]],lineages[times[-1]]], color=color)\n        if lineages[times[-1]] < TREESWIFT_FIGURE.YMIN:\n            TREESWIFT_FIGURE.YMIN = lineages[times[-1]]\n    if show_plot:\n        if xmin is None:\n            xmin = TREESWIFT_FIGURE.XMIN\n        elif not isinstance(xmin,int) and not isinstance(xmin,float):\n            warn(\"xmin is invalid, so using the default\"); xmin = TREESWIFT_FIGURE.XMIN\n        if xmax is None:\n            xmax = TREESWIFT_FIGURE.XMAX\n        elif not isinstance(xmax,int) and not isinstance(xmax,float):\n            warn(\"xmax is invalid, so using the default\"); xmax = TREESWIFT_FIGURE.XMAX\n        plt.xlim(left=xmin, right=xmax)\n        if ymin is None:\n            ymin = TREESWIFT_FIGURE.YMIN\n        elif not isinstance(ymin,int) and not isinstance(ymin,float):\n            warn(\"ymin is invalid, so using the default\"); ymin = TREESWIFT_FIGURE.YMIN\n        if ymax is None:\n            ymax = ceil(TREESWIFT_FIGURE.YMAX*1.1)\n        elif not isinstance(ymax,int) and not isinstance(ymax,float):\n            warn(\"ymax is invalid, so using the default\"); ymax = ceil(TREESWIFT_FIGURE.YMAX*1.1)\n        plt.ylim(bottom=ymin, top=ymax)\n        if title is not None and not isinstance(title,str):\n            warn(\"title is invalid, so using the default\"); title = None\n        if title is None:\n            plt.title(\"Lineages Through Time\")\n        else:\n            plt.title(title)\n        if xlabel is not None and not isinstance(xlabel,str):\n            warn(\"xlabel is invalid, so using the default\"); xlabel = None\n        if xlabel is None:\n            plt.xlabel(\"Time\")\n        else:\n            plt.xlabel(xlabel)\n        if ylabel is not None and not isinstance(ylabel,str):\n            warn(\"ylabel is invalid, so using the default\"); ylabel = None\n        if ylabel is None:\n            plt.ylabel(\"Number of Lineages\")\n        else:\n            plt.ylabel(ylabel)\n        plt.show(); TREESWIFT_FIGURE = None", "response": "Plot the Lineages Through Time curve of a given tree."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a TreeSwift tree from a DendroPy tree object.", "response": "def read_tree_dendropy(tree):\n    '''Create a TreeSwift tree from a DendroPy tree\n\n    Args:\n        ``tree`` (``dendropy.datamodel.treemodel``): A Dendropy ``Tree`` object\n\n    Returns:\n        ``Tree``: A TreeSwift tree created from ``tree``\n    '''\n    out = Tree(); d2t = dict()\n    if not hasattr(tree, 'preorder_node_iter') or not hasattr(tree, 'seed_node') or not hasattr(tree, 'is_rooted'):\n        raise TypeError(\"tree must be a DendroPy Tree object\")\n    if tree.is_rooted != True:\n        out.is_rooted = False\n    for node in tree.preorder_node_iter():\n        if node == tree.seed_node:\n            curr = out.root\n        else:\n            curr = Node(); d2t[node.parent_node].add_child(curr)\n        d2t[node] = curr; curr.edge_length = node.edge_length\n        if hasattr(node, 'taxon') and node.taxon is not None:\n            curr.label = node.taxon.label\n        else:\n            curr.label = node.label\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read_tree_newick(newick):\n    '''Read a tree from a Newick string or file\n\n    Args:\n        ``newick`` (``str``): Either a Newick string or the path to a Newick file (plain-text or gzipped)\n\n    Returns:\n        ``Tree``: The tree represented by ``newick``. If the Newick file has multiple trees (one per line), a ``list`` of ``Tree`` objects will be returned\n    '''\n    if not isinstance(newick, str):\n        try:\n            newick = str(newick)\n        except:\n            raise TypeError(\"newick must be a str\")\n    if newick.lower().endswith('.gz'): # gzipped file\n        f = gopen(expanduser(newick)); ts = f.read().decode().strip(); f.close()\n    elif isfile(expanduser(newick)): # plain-text file\n        f = open(expanduser(newick)); ts = f.read().strip(); f.close()\n    else:\n        ts = newick.strip()\n    lines = ts.splitlines()\n    if len(lines) != 1:\n        return [read_tree_newick(l) for l in lines]\n    try:\n        t = Tree(); t.is_rooted = ts.startswith('[&R]')\n        if ts[0] == '[':\n            ts = ']'.join(ts.split(']')[1:]).strip(); ts = ts.replace(', ',',')\n        n = t.root; i = 0\n        while i < len(ts):\n            if ts[i] == ';':\n                if i != len(ts)-1 or n != t.root:\n                    raise RuntimeError(INVALID_NEWICK)\n            elif ts[i] == '(':\n                c = Node(); n.add_child(c); n = c\n            elif ts[i] == ')':\n                n = n.parent\n            elif ts[i] == ',':\n                n = n.parent; c = Node(); n.add_child(c); n = c\n            elif ts[i] == ':':\n                i += 1; ls = ''\n                while ts[i] != ',' and ts[i] != ')' and ts[i] != ';':\n                    ls += ts[i]; i += 1\n                n.edge_length = float(ls); i -= 1\n            else:\n                label = ''\n                while ts[i] != ':' and ts[i] != ',' and ts[i] != ';' and ts[i] != ')':\n                    label += ts[i]; i += 1\n                i -= 1; n.label = label\n            i += 1\n    except Exception as e:\n        raise RuntimeError(\"Failed to parse string as Newick: %s\"%ts)\n    return t", "response": "Read a tree from a Newick string or file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read_tree_nexml(nexml):\n    '''Read a tree from a NeXML string or file\n\n    Args:\n        ``nexml`` (``str``): Either a NeXML string or the path to a NeXML file (plain-text or gzipped)\n\n    Returns:\n        ``dict`` of ``Tree``: A dictionary of the trees represented by ``nexml``, where keys are tree names (``str``) and values are ``Tree`` objects\n    '''\n    if not isinstance(nexml, str):\n        raise TypeError(\"nexml must be a str\")\n    if nexml.lower().endswith('.gz'): # gzipped file\n        f = gopen(expanduser(nexml))\n    elif isfile(expanduser(nexml)): # plain-text file\n        f = open(expanduser(nexml))\n    else:\n        f = nexml.splitlines()\n    trees = dict(); id_to_node = dict(); tree_id = None\n    for line in f:\n        if isinstance(line,bytes):\n            l = line.decode().strip()\n        else:\n            l = line.strip()\n        l_lower = l.lower()\n        # start of tree\n        if l_lower.startswith('<tree '):\n            if tree_id is not None:\n                raise ValueError(INVALID_NEXML)\n            parts = l.split()\n            for part in parts:\n                if '=' in part:\n                    k,v = part.split('='); k = k.strip()\n                    if k.lower() == 'id':\n                        tree_id = v.split('\"')[1]; break\n            if tree_id is None:\n                raise ValueError(INVALID_NEXML)\n            trees[tree_id] = Tree(); trees[tree_id].root = None\n        # end of tree\n        elif l_lower.replace(' ','').startswith('</tree>'):\n            if tree_id is None:\n                raise ValueError(INVALID_NEXML)\n            id_to_node = dict(); tree_id = None\n        # node\n        elif l_lower.startswith('<node '):\n            if tree_id is None:\n                raise ValueError(INVALID_NEXML)\n            node_id = None; node_label = None; is_root = False\n            k = ''; v = ''; in_key = True; in_quote = False\n            for i in range(6, len(l)):\n                if l[i] == '\"' or l[i] == \"'\":\n                    in_quote = not in_quote\n                if not in_quote and in_key and l[i] == '=':\n                    in_key = False\n                elif not in_quote and not in_key and (l[i] == '\"' or l[i] == \"'\"):\n                    k = k.strip()\n                    if k.lower() == 'id':\n                        node_id = v\n                    elif k.lower() == 'label':\n                        node_label = v\n                    elif k.lower() == 'root' and v.strip().lower() == 'true':\n                        is_root = True\n                    in_key = True; k = ''; v = ''\n                elif in_key and not (l[i] == '\"' or l[i] == \"'\"):\n                    k += l[i]\n                elif not in_key and not (l[i] == '\"' or l[i] == \"'\"):\n                    v += l[i]\n            if node_id is None or node_id in id_to_node:\n                raise ValueError(INVALID_NEXML)\n            id_to_node[node_id] = Node(label=node_label)\n            if is_root:\n                if trees[tree_id].root is not None:\n                    raise ValueError(INVALID_NEXML)\n                trees[tree_id].root = id_to_node[node_id]\n        # edge\n        elif l_lower.startswith('<edge '):\n            if tree_id is None:\n                raise ValueError(INVALID_NEXML)\n            source = None; target = None; length = None\n            parts = l.split()\n            for part in parts:\n                if '=' in part:\n                    k,v = part.split('='); k = k.strip(); k_lower = k.lower()\n                    if k_lower == 'source':\n                        source = v.split('\"')[1]\n                    elif k_lower == 'target':\n                        target = v.split('\"')[1]\n                    elif k_lower == 'length':\n                        length = float(v.split('\"')[1])\n            if source is None or target is None or length is None:\n                raise ValueError(INVALID_NEXML)\n            if source not in id_to_node:\n                raise ValueError(INVALID_NEXML)\n            if target not in id_to_node:\n                raise ValueError(INVALID_NEXML)\n            id_to_node[source].add_child(id_to_node[target])\n            id_to_node[target].edge_length = length\n        elif l_lower.startswith('<rootedge '):\n            if tree_id is None:\n                raise ValueError(INVALID_NEXML)\n            root_node = None; length = None\n            parts = l.split()\n            for part in parts:\n                if '=' in part:\n                    k,v = part.split('='); k = k.strip(); k_lower = k.lower()\n                    if k_lower == 'target':\n                        root_node = id_to_node[v.split('\"')[1]]\n                    elif k_lower == 'length':\n                        length = float(v.split('\"')[1])\n            if trees[tree_id].root is None:\n                raise ValueError(INVALID_NEXML)\n            if root_node is not None and trees[tree_id].root != root_node:\n                raise ValueError(INVALID_NEXML)\n            trees[tree_id].root.edge_length = length\n    if hasattr(f,'close'):\n        f.close()\n    return trees", "response": "Read a tree from a NeXML string or file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef read_tree_nexus(nexus):\n    '''Read a tree from a Nexus string or file\n\n    Args:\n        ``nexus`` (``str``): Either a Nexus string or the path to a Nexus file (plain-text or gzipped)\n\n    Returns:\n        ``dict`` of ``Tree``: A dictionary of the trees represented by ``nexus``, where keys are tree names (``str``) and values are ``Tree`` objects\n    '''\n    if not isinstance(nexus, str):\n        raise TypeError(\"nexus must be a str\")\n    if nexus.lower().endswith('.gz'): # gzipped file\n        f = gopen(expanduser(nexus))\n    elif isfile(expanduser(nexus)): # plain-text file\n        f = open(expanduser(nexus))\n    else:\n        f = nexus.splitlines()\n    trees = dict()\n    for line in f:\n        if isinstance(line,bytes):\n            l = line.decode().strip()\n        else:\n            l = line.strip()\n        if l.lower().startswith('tree '):\n            i = l.index('='); left = l[:i].strip(); right = l[i+1:].strip()\n            name = ' '.join(left.split(' ')[1:])\n            trees[name] = read_tree_newick(right)\n    if hasattr(f,'close'):\n        f.close()\n    return trees", "response": "Read a tree from a Nexus string or file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread a tree from a string or file", "response": "def read_tree(input, schema):\n    '''Read a tree from a string or file\n\n    Args:\n        ``input`` (``str``): Either a tree string, a path to a tree file (plain-text or gzipped), or a DendroPy Tree object\n\n        ``schema`` (``str``): The schema of ``input`` (DendroPy, Newick, NeXML, or Nexus)\n\n    Returns:\n        * If the input is Newick, either a ``Tree`` object if ``input`` contains a single tree, or a ``list`` of ``Tree`` objects if ``input`` contains multiple trees (one per line)\n\n        * If the input is NeXML or Nexus, a ``dict`` of trees represented by ``input``, where keys are tree names (``str``) and values are ``Tree`` objects\n    '''\n    schema_to_function = {\n        'dendropy': read_tree_dendropy,\n        'newick': read_tree_newick,\n        'nexml': read_tree_nexml,\n        'nexus': read_tree_nexus\n    }\n    if schema.lower() not in schema_to_function:\n        raise ValueError(\"Invalid schema: %s (valid options: %s)\" % (schema, ', '.join(sorted(schema_to_function.keys()))))\n    return schema_to_function[schema.lower()](input)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef avg_branch_length(self, terminal=True, internal=True):\n        '''Compute the average length of the selected branches of this ``Tree``. Edges with length ``None`` will be treated as 0-length\n\n        Args:\n            ``terminal`` (``bool``): ``True`` to include terminal branches, otherwise ``False``\n\n            ``internal`` (``bool``): ``True`` to include internal branches, otherwise ``False``\n\n        Returns:\n            The average length of the selected branches\n        '''\n        if not isinstance(terminal, bool):\n            raise TypeError(\"terminal must be a bool\")\n        if not isinstance(internal, bool):\n            raise TypeError(\"internal must be a bool\")\n        if not internal and not terminal:\n            raise RuntimeError(\"Must select either internal or terminal branches (or both)\")\n        tot = 0.; num = 0\n        for node in self.traverse_preorder():\n            if node.edge_length is not None and (internal and not node.is_leaf()) or (terminal and node.is_leaf()):\n                tot += node.edge_length; num += 1\n        return tot/num", "response": "Compute the average length of the selected branches of this Tree. Edges with length None will be treated as 0 - length\n           "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef closest_leaf_to_root(self):\n        '''Return the leaf that is closest to the root and the corresponding distance. Edges with no length will be considered to have a length of 0\n\n        Returns:\n            ``tuple``: First value is the closest leaf to the root, and second value is the corresponding distance\n        '''\n        best = (None,float('inf')); d = dict()\n        for node in self.traverse_preorder():\n            if node.edge_length is None:\n                d[node] = 0\n            else:\n                d[node] = node.edge_length\n            if not node.is_root():\n                d[node] += d[node.parent]\n            if node.is_leaf() and d[node] < best[1]:\n                best = (node,d[node])\n        return best", "response": "Return the leaf that is closest to the root and the corresponding distance."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef collapse_short_branches(self, threshold):\n        '''Collapse internal branches (not terminal branches) with length less than or equal to ``threshold``. A branch length of ``None`` is considered 0\n\n        Args:\n            ``threshold`` (``float``): The threshold to use when collapsing branches\n        '''\n        if not isinstance(threshold,float) and not isinstance(threshold,int):\n            raise RuntimeError(\"threshold must be an integer or a float\")\n        elif threshold < 0:\n            raise RuntimeError(\"threshold cannot be negative\")\n        q = deque(); q.append(self.root)\n        while len(q) != 0:\n            next = q.popleft()\n            if next.edge_length is None or next.edge_length <= threshold:\n                if next.is_root():\n                    next.edge_length = None\n                elif not next.is_leaf():\n                    parent = next.parent; parent.remove_child(next)\n                    for c in next.children:\n                        parent.add_child(c)\n            q.extend(next.children)", "response": "Collapse internal branches with length less than or equal to threshold."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef colless(self, normalize='leaves'):\n        '''Compute the Colless balance index of this ``Tree``. If the tree has polytomies, they will be randomly resolved\n\n        Args:\n            ``normalize`` (``str``): How to normalize the Colless index (if at all)\n\n            * ``None`` to not normalize\n\n            * ``\"leaves\"`` to normalize by the number of leaves\n\n            * ``\"yule\"`` to normalize to the Yule model\n\n            * ``\"pda\"`` to normalize to the Proportional to Distinguishable Arrangements model\n\n        Returns:\n            ``float``: Colless index (either normalized or not)\n        '''\n        t_res = copy(self); t_res.resolve_polytomies(); leaves_below = dict(); n = 0; I = 0\n        for node in t_res.traverse_postorder():\n            if node.is_leaf():\n                leaves_below[node] = 1; n += 1\n            else:\n                cl,cr = node.children; nl = leaves_below[cl]; nr = leaves_below[cr]\n                leaves_below[node] = nl+nr; I += abs(nl-nr)\n        if normalize is None or normalize is False:\n            return I\n        elif not isinstance(normalize,str):\n            raise TypeError(\"normalize must be None or a string\")\n        normalize = normalize.lower()\n        if normalize == 'leaves':\n            return (2.*I)/((n-1)*(n-2))\n        elif normalize == 'yule':\n            return (I - n*log(n) - n*(EULER_GAMMA-1-log(2)))/n\n        elif normalize == 'pda':\n            return I/(n**1.5)\n        else:\n            raise RuntimeError(\"normalize must be None, 'leaves', 'yule', or 'pda'\")", "response": "Compute the Colless balance index of this Tree."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef condense(self):\n        '''If siblings have the same label, merge them. If they have edge lengths, the resulting ``Node`` will have the larger of the lengths'''\n        self.resolve_polytomies(); labels_below = dict(); longest_leaf_dist = dict()\n        for node in self.traverse_postorder():\n            if node.is_leaf():\n                labels_below[node] = [node.label]; longest_leaf_dist[node] = None\n            else:\n                labels_below[node] = set()\n                for c in node.children:\n                    labels_below[node].update(labels_below[c])\n                    d = longest_leaf_dist[c]\n                    if c.edge_length is not None:\n                        if d is None:\n                            d = 0\n                        d += c.edge_length\n                    if node not in longest_leaf_dist or longest_leaf_dist[node] is None or (d is not None and d > longest_leaf_dist[node]):\n                        longest_leaf_dist[node] = d\n        nodes = deque(); nodes.append(self.root)\n        while len(nodes) != 0:\n            node = nodes.pop()\n            if node.is_leaf():\n                continue\n            if len(labels_below[node]) == 1:\n                node.label = labels_below[node].pop(); node.children = list()\n                if longest_leaf_dist[node] is not None:\n                    if node.edge_length is None:\n                        node.edge_length = 0\n                    node.edge_length += longest_leaf_dist[node]\n            else:\n                nodes.extend(node.children)", "response": "If siblings have the same label, merge them. If they have edge lengths, the resulting ``Node`` will have the larger of the lengths"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncontract internal nodes labeled by a number ( e. g. branch support ) below threshold.", "response": "def contract_low_support(self, threshold):\n        '''Contract internal nodes labeled by a number (e.g. branch support) below ``threshold``\n\n        Args:\n            ``threshold`` (``float``): The support threshold to use when contracting nodes'''\n        if not isinstance(threshold, float) and not isinstance(threshold, int):\n            raise TypeError(\"threshold must be float or int\")\n        to_contract = list()\n        for node in self.traverse_preorder():\n            try:\n                if float(str(node)) < threshold:\n                    to_contract.append(node)\n            except:\n                pass\n        for node in to_contract:\n            node.contract()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndropping the edge of the root node if it has a root edge.", "response": "def deroot(self, label='OLDROOT'):\n        '''If the tree has a root edge, drop the edge to be a child of the root node\n\n        Args:\n            ``label`` (``str``): The desired label of the new child\n        '''\n        if self.root.edge_length is not None:\n            self.root.add_child(Node(edge_length=self.root.edge_length,label=label))\n            self.root.edge_length = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef diameter(self):\n        '''Compute the diameter (maximum leaf pairwise distance) of this ``Tree``\n\n        Returns:\n            ``float``: The diameter of this Tree\n        '''\n        d = dict(); best = float('-inf')\n        for node in self.traverse_postorder():\n            if node.is_leaf():\n                d[node] = 0\n            else:\n                dists = sorted(d[c]+c.edge_length for c in node.children)\n                d[node] = dists[-1]; max_pair = dists[-1]+dists[-2]\n                if max_pair > best:\n                    best = max_pair\n        return best", "response": "Compute the diameter of this Tree object"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef distance_between(self, u, v):\n        '''Return the distance between nodes ``u`` and ``v`` in this ``Tree``\n\n        Args:\n            ``u`` (``Node``): Node ``u``\n\n            ``v`` (``Node``): Node ``v``\n\n        Returns:\n            ``float``: The distance between nodes ``u`` and ``v``\n        '''\n        if not isinstance(u, Node):\n            raise TypeError(\"u must be a Node\")\n        if not isinstance(v, Node):\n            raise TypeError(\"v must be a Node\")\n        if u == v:\n            return 0.\n        u_dists = {u:0.}; v_dists = {v:0.}\n        c = u; p = u.parent # u traversal\n        while p is not None:\n            u_dists[p] = u_dists[c]\n            if c.edge_length is not None:\n                u_dists[p] += c.edge_length\n            c = p; p = p.parent\n        c = v; p = v.parent # v traversal\n        while p is not None:\n            v_dists[p] = v_dists[c]\n            if c.edge_length is not None:\n                v_dists[p] += c.edge_length\n            if p in u_dists:\n                return u_dists[p] + v_dists[p]\n            c = p; p = p.parent\n        raise RuntimeError(\"u and v are not in the same Tree\")", "response": "Return the distance between nodes u and v in this Tree."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef distance_matrix(self, leaf_labels=False):\n        '''Return a distance matrix (2D dictionary) of the leaves of this ``Tree``\n\n        Args:\n            ``leaf_labels`` (``bool``): ``True`` to have keys be labels of leaf ``Node`` objects, otherwise ``False`` to have keys be ``Node`` objects\n\n        Returns:\n            ``dict``: Distance matrix (2D dictionary) of the leaves of this ``Tree``, where keys are labels of leaves; ``M[u][v]`` = distance from ``u`` to ``v``\n        '''\n        M = dict(); leaf_dists = dict()\n        for node in self.traverse_postorder():\n            if node.is_leaf():\n                leaf_dists[node] = [[node,0]]\n            else:\n                for c in node.children:\n                    if c.edge_length is not None:\n                        for i in range(len(leaf_dists[c])):\n                            leaf_dists[c][i][1] += c.edge_length\n                for c1 in range(0,len(node.children)-1):\n                    leaves_c1 = leaf_dists[node.children[c1]]\n                    for c2 in range(c1+1,len(node.children)):\n                        leaves_c2 = leaf_dists[node.children[c2]]\n                        for i in range(len(leaves_c1)):\n                            for j in range(len(leaves_c2)):\n                                u,ud = leaves_c1[i]; v,vd = leaves_c2[j]; d = ud+vd\n                                if leaf_labels:\n                                    u_key = u.label; v_key = v.label\n                                else:\n                                    u_key = u; v_key = v\n                                if u_key not in M:\n                                    M[u_key] = dict()\n                                M[u_key][v_key] = d\n                                if v_key not in M:\n                                    M[v_key] = dict()\n                                M[v_key][u_key] = d\n                leaf_dists[node] = leaf_dists[node.children[0]]; del leaf_dists[node.children[0]]\n                for i in range(1,len(node.children)):\n                    leaf_dists[node] += leaf_dists[node.children[i]]; del leaf_dists[node.children[i]]\n        return M", "response": "Return a distance matrix of the leaves of this Tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef distances_from_parent(self, leaves=True, internal=True, unlabeled=False):\n        '''Generator over the node-to-parent distances of this ``Tree``; (node,distance) tuples\n\n        Args:\n            ``terminal`` (``bool``): ``True`` to include leaves, otherwise ``False``\n\n            ``internal`` (``bool``): ``True`` to include internal nodes, otherwise ``False``\n\n            ``unlabeled`` (``bool``): ``True`` to include unlabeled nodes, otherwise ``False``\n        '''\n        if not isinstance(leaves, bool):\n            raise TypeError(\"leaves must be a bool\")\n        if not isinstance(internal, bool):\n            raise TypeError(\"internal must be a bool\")\n        if not isinstance(unlabeled, bool):\n            raise TypeError(\"unlabeled must be a bool\")\n        if leaves or internal:\n            for node in self.traverse_preorder():\n                if ((leaves and node.is_leaf()) or (internal and not node.is_leaf())) and (unlabeled or node.label is not None):\n                    if node.edge_length is None:\n                        yield (node,0)\n                    else:\n                        yield (node,node.edge_length)", "response": "Generator over the node - to - parent distances of this Tree ; ( node distance ) tuples."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef edge_length_sum(self, terminal=True, internal=True):\n        '''Compute the sum of all selected edge lengths in this ``Tree``\n\n        Args:\n            ``terminal`` (``bool``): ``True`` to include terminal branches, otherwise ``False``\n\n            ``internal`` (``bool``): ``True`` to include internal branches, otherwise ``False``\n\n        Returns:\n            ``float``: Sum of all selected edge lengths in this ``Tree``\n        '''\n        if not isinstance(terminal, bool):\n            raise TypeError(\"leaves must be a bool\")\n        if not isinstance(internal, bool):\n            raise TypeError(\"internal must be a bool\")\n        return sum(node.edge_length for node in self.traverse_preorder() if node.edge_length is not None and ((terminal and node.is_leaf()) or (internal and not node.is_leaf())))", "response": "Compute the sum of all selected edge lengths in this Tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef extract_subtree(self, node):\n        '''Return a copy of the subtree rooted at ``node``\n\n        Args:\n            ``node`` (``Node``): The root of the desired subtree\n\n        Returns:\n            ``Tree``: A copy of the subtree rooted at ``node``\n        '''\n        if not isinstance(node, Node):\n            raise TypeError(\"node must be a Node\")\n        r = self.root; self.root = node; o = copy(self); self.root = r; return o", "response": "Return a copy of the subtree rooted at node"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef extract_tree_without(self, labels, suppress_unifurcations=True):\n        '''Extract a copy of this ``Tree`` without the leaves labeled by the strings in ``labels``\n\n        Args:\n            ``labels`` (``set``): Set of leaf labels to exclude\n\n            ``suppress_unifurcations`` (``bool``): ``True`` to suppress unifurcations, otherwise ``False``\n\n        Returns:\n            ``Tree``: Copy of this ``Tree``, exluding the leaves labeled by the strings in ``labels``\n        '''\n        return self.extract_tree(labels, True, suppress_unifurcations)", "response": "Returns a copy of this Tree with the leaves labeled by the strings in labels excluded."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef extract_tree_with(self, labels, suppress_unifurcations=True):\n        '''Extract a copy of this ``Tree`` with only the leaves labeled by the strings in ``labels``\n\n        Args:\n            ``leaves`` (``set``): Set of leaf labels to include.\n\n            ``suppress_unifurcations`` (``bool``): ``True`` to suppress unifurcations, otherwise ``False``\n\n        Returns:\n            Tree: Copy of this Tree, including only the leaves labeled by the strings in ``labels``\n        '''\n        return self.extract_tree(labels, False, suppress_unifurcations)", "response": "Extract a copy of this Tree with only the leaves labeled by the strings in labels."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef furthest_from_root(self):\n        '''Return the ``Node`` that is furthest from the root and the corresponding distance. Edges with no length will be considered to have a length of 0\n\n        Returns:\n            ``tuple``: First value is the furthest ``Node`` from the root, and second value is the corresponding distance\n        '''\n        best = (self.root,0); d = dict()\n        for node in self.traverse_preorder():\n            if node.edge_length is None:\n                d[node] = 0\n            else:\n                d[node] = node.edge_length\n            if not node.is_root():\n                d[node] += d[node.parent]\n            if d[node] > best[1]:\n                best = (node,d[node])\n        return best", "response": "Return the Node that is furthest from the root and the corresponding distance."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef gamma_statistic(self):\n        '''Compute the Gamma statistic of Pybus and Harvey (2000)\n\n        Returns:\n            ``float``: The Gamma statistic of Pybus and Harvey (2000)\n        '''\n        t = copy(self); t.resolve_polytomies() # need fully bifurcating tree\n        G = [g for g in t.coalescence_times(backward=False)]\n        n = len(G)+1\n        if n <= 2:\n            raise RuntimeError(\"Gamma statistic can only be computed on trees with more than 2 leaves\")\n        T = sum((j+2)*g for j,g in enumerate(G))\n        out = 0.\n        for i in range(len(G)-1):\n            for k in range(i+1):\n                out += (k+2)*G[k]\n        out /= (n-2)\n        out -= (T/2)\n        out /= T\n        out /= (1./(12*(n-2)))**0.5\n        return out", "response": "Compute the Gamma statistic of Pybus and Harvey."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns an indented Newick string just like nw_indent in Newick Utilities", "response": "def indent(self, space=4):\n        '''Return an indented Newick string, just like ``nw_indent`` in Newick Utilities\n\n        Args:\n            ``space`` (``int``): The number of spaces a tab should equal\n\n        Returns:\n            ``str``: An indented Newick string\n        '''\n        if not isinstance(space,int):\n            raise TypeError(\"space must be an int\")\n        if space < 0:\n            raise ValueError(\"space must be a non-negative integer\")\n        space = ' '*space; o = []; l = 0\n        for c in self.newick():\n            if c == '(':\n                o.append('(\\n'); l += 1; o.append(space*l)\n            elif c == ')':\n                o.append('\\n'); l -= 1; o.append(space*l); o.append(')')\n            elif c == ',':\n                o.append(',\\n'); o.append(space*l)\n            else:\n                o.append(c)\n        return ''.join(o)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef label_to_node(self, selection='leaves'):\n        '''Return a dictionary mapping labels (strings) to ``Node`` objects\n\n        * If ``selection`` is ``\"all\"``, the dictionary will contain all nodes\n\n        * If ``selection`` is ``\"leaves\"``, the dictionary will only contain leaves\n\n        * If ``selection`` is ``\"internal\"``, the dictionary will only contain internal nodes\n\n        * If ``selection`` is a ``set``, the dictionary will contain all nodes labeled by a label in ``selection``\n\n        * If multiple nodes are labeled by a given label, only the last (preorder traversal) will be obtained\n\n        Args:\n            ``selection`` (``str`` or ``set``): The selection of nodes to get\n\n            * ``\"all\"`` to select all nodes\n\n            * ``\"leaves\"`` to select leaves\n\n            * ``\"internal\"`` to select internal nodes\n\n            * A ``set`` of labels to specify nodes to select\n\n        Returns:\n            ``dict``: Dictionary mapping labels to the corresponding nodes\n        '''\n        if not isinstance(selection,set) and not isinstance(selection,list) and (not isinstance(selection,str) or not (selection != 'all' or selection != 'leaves' or selection != 'internal')):\n            raise RuntimeError('\"selection\" must be one of the strings \"all\", \"leaves\", or \"internal\", or it must be a set containing Node labels')\n        if isinstance(selection, str):\n            selection = selection[0]\n        elif isinstance(selection,list):\n            selection = set(selection)\n        label_to_node = dict()\n        for node in self.traverse_preorder():\n            if selection == 'a' or (selection == 'i' and not node.is_leaf()) or (selection == 'l' and node.is_leaf()) or str(node) in selection:\n                label_to_node[str(node)] = node\n        if not isinstance(selection,str) and len(label_to_node) != len(selection):\n            warn(\"Not all given labels exist in the tree\")\n        return label_to_node", "response": "Return a dictionary mapping labels to Node objects"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef labels(self, leaves=True, internal=True):\n        '''Generator over the (non-``None``) ``Node`` labels of this ``Tree``\n\n        Args:\n            ``leaves`` (``bool``): ``True`` to include leaves, otherwise ``False``\n\n            ``internal`` (``bool``): ``True`` to include internal nodes, otherwise ``False``\n        '''\n        if not isinstance(leaves, bool):\n            raise TypeError(\"leaves must be a bool\")\n        if not isinstance(internal, bool):\n            raise TypeError(\"internal must be a bool\")\n        for node in self.traverse_preorder():\n            if node.label is not None and ((leaves and node.is_leaf()) or (internal and not node.is_leaf())):\n                yield node.label", "response": "Generator over the ( non - None ) Node labels of this Tree."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the number of lineages through time.", "response": "def lineages_through_time(self, present_day=None, show_plot=True, color='#000000', xmin=None, xmax=None, ymin=None, ymax=None, title=None, xlabel=None, ylabel=None):\n        '''Compute the number of lineages through time. If seaborn is installed, a plot is shown as well\n\n        Args:\n            ``present_day`` (``float``): The time of the furthest node from the root. If ``None``, the top of the tree will be placed at time 0\n\n            ``show_plot`` (``bool``): ``True`` to show the plot, otherwise ``False`` to only return the dictionary. To plot multiple LTTs on the same figure, set ``show_plot`` to False for all but the last plot\n\n            ``color`` (``str``): The color of the resulting plot\n\n            ``title`` (``str``): The title of the resulting plot\n\n            ``xmin`` (``float``): The minimum value of the horizontal axis in the resulting plot\n\n            ``xmax`` (``float``): The maximum value of the horizontal axis in the resulting plot\n\n            ``xlabel`` (``str``): The label of the horizontal axis in the resulting plot\n\n            ``ymin`` (``float``): The minimum value of the vertical axis in the resulting plot\n\n            ``ymax`` (``float``): The maximum value of the vertical axis in the resulting plot\n\n            ``ylabel`` (``str``): The label of the vertical axis in the resulting plot\n\n        Returns:\n            ``dict``: A dictionary in which each ``(t,n)`` pair denotes the number of lineages ``n`` that existed at time ``t``\n        '''\n        if present_day is not None and not isinstance(present_day,int) and not isinstance(present_day,float):\n            raise TypeError(\"present_day must be a float\")\n        time = dict()\n        if self.root.edge_length is None:\n            tmproot = self.root\n        else:\n            tmproot = Node(); tmproot.add_child(self.root)\n        for node in tmproot.traverse_preorder():\n            if node.is_root():\n                time[node] = 0.\n            else:\n                time[node] = time[node.parent]\n                if node.edge_length is not None:\n                    time[node] += node.edge_length\n        nodes = sorted((time[node],node) for node in time)\n        lineages = {nodes[0][0]:0}\n        for i in range(len(nodes)):\n            if nodes[i][0] not in lineages:\n                lineages[nodes[i][0]] = lineages[nodes[i-1][0]]\n            if nodes[i][1].edge_length is not None:\n                if nodes[i][1].edge_length >= 0:\n                    lineages[nodes[i][0]] -= 1\n                else:\n                    lineages[nodes[i][0]] += 1\n            for c in nodes[i][1].children:\n                if c.edge_length >= 0:\n                    lineages[nodes[i][0]] += 1\n                else:\n                    lineages[nodes[i][0]] -= 1\n        if present_day is not None:\n            shift = present_day - max(lineages.keys())\n        else:\n            shift = max(0,-min(lineages.keys()))\n        if shift != 0:\n            tmp = dict()\n            for t in lineages:\n                tmp[t+shift] = lineages[t]\n            lineages = tmp\n        if tmproot != self.root:\n            self.root.parent = None\n        try:\n            plot_ltt(lineages, show_plot=show_plot, color=color, xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax, title=title, xlabel=xlabel, ylabel=ylabel)\n        except Exception as e:\n            warn(\"Unable to produce visualization (but dictionary will still be returned)\"); print(e)\n        return lineages"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the Node that is the MRCA of the nodes labeled by a given label.", "response": "def mrca(self, labels):\n        '''Return the Node that is the MRCA of the nodes labeled by a label in ``labels``. If multiple nodes are labeled by a given label, only the last (preorder traversal) will be obtained\n\n        Args:\n            ``labels`` (``set``): Set of leaf labels\n\n        Returns:\n            ``Node``: The MRCA of the ``Node`` objects labeled by a label in ``labels``\n        '''\n        if not isinstance(labels,set):\n            try:\n                labels = set(labels)\n            except:\n                raise TypeError(\"labels must be iterable\")\n        l2n = self.label_to_node(labels)\n        count = dict()\n        for node in l2n.values():\n            for a in node.traverse_ancestors():\n                if a not in count:\n                    count[a] = 0\n                count[a] += 1\n                if count[a] == len(l2n):\n                    return a\n        raise RuntimeError(\"There somehow does not exist an MRCA for the given labels\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a dictionary storing all pairwise MRCAs.", "response": "def mrca_matrix(self):\n        '''Return a dictionary storing all pairwise MRCAs. ``M[u][v]`` = MRCA of nodes ``u`` and ``v``. Excludes ``M[u][u]`` because MRCA of node and itself is itself\n\n        Returns:\n            ``dict``: ``M[u][v]`` = MRCA of nodes ``u`` and ``v``\n        '''\n        M = dict()\n        leaves_below = dict()\n        for node in self.traverse_postorder():\n            leaves_below[node] = list()\n            if node.is_leaf():\n                leaves_below[node].append(node); M[node] = dict()\n            else:\n                for i in range(len(node.children)-1):\n                    for l1 in leaves_below[node.children[i]]:\n                        leaves_below[node].append(l1)\n                        for j in range(i+1, len(node.children)):\n                            for l2 in leaves_below[node.children[j]]:\n                                M[l1][l2] = node; M[l2][l1] = node\n                if len(node.children) != 1:\n                    for l2 in leaves_below[node.children[-1]]:\n                        leaves_below[node].append(l2)\n        return M"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef newick(self):\n        '''Output this ``Tree`` as a Newick string\n\n        Returns:\n            ``str``: Newick string of this ``Tree``\n        '''\n        if self.root.edge_length is None:\n            suffix = ';'\n        elif isinstance(self.root.edge_length,int):\n            suffix = ':%d;' % self.root.edge_length\n        elif isinstance(self.root.edge_length,float) and self.root.edge_length.is_integer():\n            suffix = ':%d;' % int(self.root.edge_length)\n        else:\n            suffix = ':%s;' % str(self.root.edge_length)\n        if self.is_rooted:\n            return '[&R] %s%s' % (self.root.newick(),suffix)\n        else:\n            return '%s%s' % (self.root.newick(),suffix)", "response": "Output this Tree as a Newick string"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef num_lineages_at(self, distance):\n        '''Returns the number of lineages of this ``Tree`` that exist ``distance`` away from the root\n\n        Args:\n            ``distance`` (``float``): The distance away from the root\n\n        Returns:\n            ``int``: The number of lineages that exist ``distance`` away from the root\n        '''\n        if not isinstance(distance, float) and not isinstance(distance, int):\n            raise TypeError(\"distance must be an int or a float\")\n        if distance < 0:\n            raise RuntimeError(\"distance cannot be negative\")\n        d = dict(); q = deque(); q.append(self.root); count = 0\n        while len(q) != 0:\n            node = q.popleft()\n            if node.is_root():\n                d[node] = 0\n            else:\n                d[node] = d[node.parent]\n            if node.edge_length is not None:\n                d[node] += node.edge_length\n            if d[node] < distance:\n                q.extend(node.children)\n            elif node.parent is None or d[node.parent] < distance:\n                count += 1\n        return count", "response": "Returns the number of lineages that exist at a given distance away from the root node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the total number of selected nodes in this Tree.", "response": "def num_nodes(self, leaves=True, internal=True):\n        '''Compute the total number of selected nodes in this ``Tree``\n\n        Args:\n            ``leaves`` (``bool``): ``True`` to include leaves, otherwise ``False``\n\n            ``internal`` (``bool``): ``True`` to include internal nodes, otherwise ``False``\n\n        Returns:\n            ``int``: The total number of selected nodes in this ``Tree``\n        '''\n        if not isinstance(leaves, bool):\n            raise TypeError(\"leaves must be a bool\")\n        if not isinstance(internal, bool):\n            raise TypeError(\"internal must be a bool\")\n        num = 0\n        for node in self.traverse_preorder():\n            if (leaves and node.is_leaf()) or (internal and not node.is_leaf()):\n                num += 1\n        return num"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\norders the children of the nodes in this Tree based on the given mode.", "response": "def order(self, mode, ascending=True):\n        '''Order the children of the nodes in this ``Tree`` based on ``mode``\n\n        Args:\n            ``mode`` (``str``): How to order the children of the nodes of this ``Tree``\n\n            * ``\"edge_length\"`` = order by incident edge length\n\n            * ``\"edge_length_then_label\"`` = order by incident edge length, then by node label\n\n            * ``\"edge_length_then_label_then_num_descendants\"`` = order by incident edge length, then by node label, then by number of descendants\n\n            * ``\"edge_length_then_num_descendants\"`` = order by incident edge length, then by number of descendants\n\n            * ``\"edge_length_then_num_descendants_then_label\"`` = order by incident edge length, then by number of descendants, then by node label\n\n            * ``\"label\"`` = order by node label\n\n            * ``\"label_then_edge_length\"`` = order by node label, then by incident edge length\n\n            * ``\"label_then_edge_length_then_num_descendants\"`` = order by node label, then by incident edge length, then by number of descendants\n\n            * ``\"label_then_num_descendants\"`` = order by node label, then by number of descendants\n\n            * ``\"label_then_num_descendants_then_edge_length\"`` = order by node label, then by number of descendants, then by incident edge length\n\n            * ``\"num_descendants\"`` = order by number of descendants\n\n            * ``\"num_descendants_then_label\"`` = order by number of descendants, then by node label\n\n            * ``\"num_descendants_then_label_then_edge_length\"`` = order by number of descendants, then by node label, then by incident edge length\n\n            * ``\"num_descendants_then_edge_length\"`` = order by number of descendants, then by incident edge length\n\n            * ``\"num_descendants_then_edge_length_then_label\"`` = order by number of descendants, then by incident edge length, then by node label\n\n            ``ascending`` (``bool``): ``True`` to sort in ascending order of ``mode``, otherwise ``False``\n        '''\n        if not isinstance(mode, str):\n            raise TypeError(\"mode must be a str\")\n        if not isinstance(ascending, bool):\n            raise TypeError(\"ascending must be a bool\")\n        if 'num_descendants' in mode:\n            num_descendants = dict()\n            for node in self.traverse_postorder():\n                if node.is_leaf():\n                    num_descendants[node] = 0\n                else:\n                    num_descendants[node] = sum(num_descendants[c] for c in node.children) + len(node.children)\n        if mode == 'edge_length':\n            k = lambda node: (node.edge_length is not None, node.edge_length)\n        elif mode == 'edge_length_then_label':\n            k = lambda node: (node.edge_length is not None, node.edge_length, node.label is not None, node.label)\n        elif mode == 'edge_length_then_label_then_num_descendants':\n            k = lambda node: (node.edge_length is not None, node.edge_length, node.label is not None, node.label, num_descendants[node])\n        elif mode == 'edge_length_then_num_descendants':\n            k = lambda node: (node.edge_length is not None, node.edge_length, num_descendants[node])\n        elif mode == 'edge_length_then_num_descendants_then_label':\n            k = lambda node: (node.edge_length is not None, node.edge_length, num_descendants[node], node.label is not None, node.label)\n        elif mode == 'label':\n            k = lambda node: (node.label is not None, node.label)\n        elif mode == 'label_then_edge_length':\n            k = lambda node: (node.label is not None, node.label, node.edge_length is not None, node.edge_length)\n        elif mode == 'label_then_edge_length_then_num_descendants':\n            k = lambda node: (node.label is not None, node.label, node.edge_length is not None, node.edge_length, num_descendants[node])\n        elif mode == 'label_then_num_descendants':\n            k = lambda node: (node.label is not None, node.label, num_descendants[node])\n        elif mode == 'label_then_num_descendants_then_edge_length':\n            k = lambda node: (node.label is not None, node.label, num_descendants[node], node.edge_length is not None, node.edge_length)\n        elif mode == 'num_descendants':\n            k = lambda node: num_descendants[node]\n        elif mode == 'num_descendants_then_label':\n            k = lambda node: (num_descendants[node], node.label is not None, node.label)\n        elif mode == 'num_descendants_then_label_then_edge_length':\n            k = lambda node: (num_descendants[node], node.label is not None, node.label, node.edge_length is not None, node.edge_length)\n        elif mode == 'num_descendants_then_edge_length':\n            k = lambda node: (num_descendants[node], node.edge_length is not None, node.edge_length)\n        elif mode == 'num_descendants_then_edge_length_then_label':\n            k = lambda node: (num_descendants[node], node.edge_length is not None, node.edge_length, node.label is not None, node.label)\n        else:\n            raise ValueError(\"Invalid choice for mode\")\n        for node in self.traverse_preorder():\n            node.children.sort(key=k, reverse=not ascending)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nrenames nodes in this Tree.", "response": "def rename_nodes(self, renaming_map):\n        '''Rename nodes in this ``Tree``\n\n        Args:\n            ``renaming_map`` (``dict``): A dictionary mapping old labels (keys) to new labels (values)\n        '''\n        if not isinstance(renaming_map, dict):\n            raise TypeError(\"renaming_map must be a dict\")\n        for node in self.traverse_preorder():\n            if node.label in renaming_map:\n                node.label = renaming_map[node.label]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef reroot(self, node, length=None, branch_support=False):\n        '''Reroot this ``Tree`` at ``length`` up the incident edge of ``node``. If 0 or ``None``, reroot at the node (not on the incident edge)\n\n        Args:\n            ``node`` (``Node``): The ``Node`` on whose incident edge this ``Tree`` will be rerooted\n\n            ``length`` (``float``): The distance up the specified edge at which to reroot this ``Tree``. If 0 or ``None``, reroot at the node (not on the incident edge)\n\n            ``branch_support`` (``bool``): ``True`` if internal node labels represent branch support values, otherwise ``False``\n        '''\n        warn(\"TreeSwift's rerooting functionality is poorly tested and likely has bugs. It will be fixed in a future release\")\n        if not isinstance(node, Node):\n            raise TypeError(\"node must be a Node\")\n        if length is not None and not isinstance(length, float) and not isinstance(length, int):\n            raise TypeError(\"length must be a float\")\n        if not isinstance(branch_support, bool):\n            raise TypeError(\"branch_support must be a bool\")\n        if length is not None and length < 0:\n            raise ValueError(\"Specified length at which to reroot must be positive\")\n        if node.edge_length is None:\n            if length is not None and length != 0:\n                raise ValueError(\"Specified node has no edge length, so specified length must be None or 0\")\n        elif length is not None and length > node.edge_length:\n            raise ValueError(\"Specified length must be shorter than the edge at which to reroot\")\n        if length is not None and length > 0:\n            newnode = Node(edge_length=node.edge_length-length); node.edge_length -= length\n            if not node.is_root():\n                p = node.parent; p.children.remove(node); p.add_child(newnode)\n            newnode.add_child(node); node = newnode\n        if node.is_root():\n            return\n        elif self.root.edge_length is not None:\n            newnode = Node(label='ROOT'); newnode.add_child(self.root); self.root = newnode\n        ancestors = [a for a in node.traverse_ancestors(include_self=True) if not a.is_root()]\n        for i in range(len(ancestors)-1, -1, -1):\n            curr = ancestors[i]; curr.parent.edge_length = curr.edge_length; curr.edge_length = None\n            if branch_support:\n                curr.parent.label = curr.label; curr.label = None\n            curr.parent.children.remove(curr); curr.add_child(curr.parent); curr.parent = None\n        self.root = node; self.is_rooted = True", "response": "Reroot this Tree at length up the edge of node."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sackin(self, normalize='leaves'):\n        '''Compute the Sackin balance index of this ``Tree``\n\n        Args:\n            ``normalize`` (``str``): How to normalize the Sackin index (if at all)\n\n            * ``None`` to not normalize\n\n            * ``\"leaves\"`` to normalize by the number of leaves\n\n            * ``\"yule\"`` to normalize to the Yule model\n\n            * ``\"pda\"`` to normalize to the Proportional to Distinguishable Arrangements model\n\n        Returns:\n            ``float``: Sackin index (either normalized or not)\n        '''\n        num_nodes_from_root = dict(); sackin = 0; num_leaves = 0\n        for node in self.traverse_preorder():\n            num_nodes_from_root[node] = 1\n            if not node.is_root():\n                num_nodes_from_root[node] += num_nodes_from_root[node.parent]\n            if node.is_leaf():\n                num_nodes_from_root[node] -= 1; sackin += num_nodes_from_root[node]; num_leaves += 1\n        if normalize is None or normalize is False:\n            return sackin\n        elif not isinstance(normalize,str):\n            raise TypeError(\"normalize must be None or a string\")\n        normalize = normalize.lower()\n        if normalize == 'leaves':\n            return float(sackin)/num_leaves\n        elif normalize == 'yule':\n            x = sum(1./i for i in range(2, num_leaves+1))\n            return (sackin - (2*num_leaves*x)) / num_leaves\n        elif normalize == 'pda':\n            return sackin/(num_leaves**1.5)\n        else:\n            raise RuntimeError(\"normalize must be None, 'leaves', 'yule', or 'pda'\")", "response": "Compute the Sackin balance index of this Tree."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmultiplying all edges in this Tree by multiplier", "response": "def scale_edges(self, multiplier):\n        '''Multiply all edges in this ``Tree`` by ``multiplier``'''\n        if not isinstance(multiplier,int) and not isinstance(multiplier,float):\n            raise TypeError(\"multiplier must be an int or float\")\n        for node in self.traverse_preorder():\n            if node.edge_length is not None:\n                node.edge_length *= multiplier"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef suppress_unifurcations(self):\n        '''Remove all nodes with only one child and directly attach child to parent'''\n        q = deque(); q.append(self.root)\n        while len(q) != 0:\n            node = q.popleft()\n            if len(node.children) != 1:\n                q.extend(node.children); continue\n            child = node.children.pop()\n            if node.is_root():\n                self.root = child; child.parent = None\n            else:\n                parent = node.parent; parent.remove_child(node); parent.add_child(child)\n            if node.edge_length is not None:\n                if child.edge_length is None:\n                    child.edge_length = 0\n                child.edge_length += node.edge_length\n            if child.label is None and node.label is not None:\n                child.label = node.label\n            q.append(child)", "response": "Remove all nodes with only one child and directly attach child to parent"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nperforms an inorder traversal of the Node objects in this Tree.", "response": "def traverse_inorder(self, leaves=True, internal=True):\n        '''Perform an inorder traversal of the ``Node`` objects in this ``Tree``\n\n        Args:\n            ``leaves`` (``bool``): ``True`` to include leaves, otherwise ``False``\n\n            ``internal`` (``bool``): ``True`` to include internal nodes, otherwise ``False``\n        '''\n        for node in self.root.traverse_inorder(leaves=leaves, internal=internal):\n            yield node"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nperforming a levelorder traversal of the Node objects in this Tree.", "response": "def traverse_levelorder(self, leaves=True, internal=True):\n        '''Perform a levelorder traversal of the ``Node`` objects in this ``Tree``'''\n        for node in self.root.traverse_levelorder(leaves=leaves, internal=internal):\n            yield node"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef traverse_postorder(self, leaves=True, internal=True):\n        '''Perform a postorder traversal of the ``Node`` objects in this ``Tree``\n\n        Args:\n            ``leaves`` (``bool``): ``True`` to include leaves, otherwise ``False``\n\n            ``internal`` (``bool``): ``True`` to include internal nodes, otherwise ``False``\n        '''\n        for node in self.root.traverse_postorder(leaves=leaves, internal=internal):\n            yield node", "response": "Perform a postorder traversal of the Node objects in this Tree."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nperforming a preorder traversal of the Node objects in this Tree.", "response": "def traverse_preorder(self, leaves=True, internal=True):\n        '''Perform a preorder traversal of the ``Node`` objects in this ``Tree``\n\n        Args:\n            ``leaves`` (``bool``): ``True`` to include leaves, otherwise ``False``\n\n            ``internal`` (``bool``): ``True`` to include internal nodes, otherwise ``False``\n        '''\n        for node in self.root.traverse_preorder(leaves=leaves, internal=internal):\n            yield node"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef traverse_rootdistorder(self, ascending=True, leaves=True, internal=True):\n        '''Perform a traversal of the ``Node`` objects in this ``Tree`` in either ascending (``ascending=True``) or descending (``ascending=False``) order of distance from the root\n\n        Args:\n            ``ascending`` (``bool``): ``True`` to perform traversal in ascending distance from the root, otherwise ``False`` for descending\n\n            ``leaves`` (``bool``): ``True`` to include leaves, otherwise ``False``\n\n            ``internal`` (``bool``): ``True`` to include internal nodes, otherwise ``False``\n        '''\n        for node in self.root.traverse_rootdistorder(ascending=ascending, leaves=leaves, internal=internal):\n            yield node", "response": "Perform a traversal of the Node objects in this Tree in either ascending or descending order of distance from the root node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the treeness of this Tree.", "response": "def treeness(self):\n        '''Compute the `treeness` (sum of internal branch lengths / sum of all branch lengths) of this ``Tree``. Branch lengths of ``None`` are considered 0 length\n\n        Returns:\n            ``float``: `Treeness` of this ``Tree`` (sum of internal branch lengths / sum of all branch lengths)\n        '''\n        internal = 0.; all = 0.\n        for node in self.traverse_preorder():\n            if node.edge_length is not None:\n                all += node.edge_length\n                if not node.is_leaf():\n                    internal += node.edge_length\n        return internal/all"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites this Tree to a Newick file.", "response": "def write_tree_newick(self, filename, hide_rooted_prefix=False):\n        '''Write this ``Tree`` to a Newick file\n\n        Args:\n            ``filename`` (``str``): Path to desired output file (plain-text or gzipped)\n        '''\n        if not isinstance(filename, str):\n            raise TypeError(\"filename must be a str\")\n        treestr = self.newick()\n        if hide_rooted_prefix:\n            if treestr.startswith('[&R]'):\n                treestr = treestr[4:].strip()\n            else:\n                warn(\"Specified hide_rooted_prefix, but tree was not rooted\")\n        if filename.lower().endswith('.gz'): # gzipped file\n            f = gopen(expanduser(filename),'wb',9); f.write(treestr.encode()); f.close()\n        else: # plain-text file\n            f = open(expanduser(filename),'w'); f.write(treestr); f.close()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update_window(turn_from, tick_from, turn_to, tick_to, updfun, branchd):\n    if turn_from in branchd:\n        # Not including the exact tick you started from because deltas are *changes*\n        for past_state in branchd[turn_from][tick_from+1:]:\n            updfun(*past_state)\n    for midturn in range(turn_from+1, turn_to):\n        if midturn in branchd:\n            for past_state in branchd[midturn][:]:\n                updfun(*past_state)\n    if turn_to in branchd:\n        for past_state in branchd[turn_to][:tick_to+1]:\n            updfun(*past_state)", "response": "Iterate over a window of time in branchd and call updfun on the values"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\niterates backward over a window of time in branchd and call updfun on the values", "response": "def update_backward_window(turn_from, tick_from, turn_to, tick_to, updfun, branchd):\n    \"\"\"Iterate backward over a window of time in ``branchd`` and call ``updfun`` on the values\"\"\"\n    if turn_from in branchd:\n        for future_state in reversed(branchd[turn_from][:tick_from]):\n            updfun(*future_state)\n    for midturn in range(turn_from-1, turn_to, -1):\n        if midturn in branchd:\n            for future_state in reversed(branchd[midturn][:]):\n                updfun(*future_state)\n    if turn_to in branchd:\n        for future_state in reversed(branchd[turn_to][tick_to+1:]):\n            updfun(*future_state)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning whether the windowdict has history at the revision.", "response": "def within_history(rev, windowdict):\n    \"\"\"Return whether the windowdict has history at the revision.\"\"\"\n    if not windowdict:\n        return False\n    begin = windowdict._past[0][0] if windowdict._past else \\\n            windowdict._future[-1][0]\n    end = windowdict._future[0][0] if windowdict._future else \\\n          windowdict._past[-1][0]\n    return begin <= rev <= end"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef future(self, rev=None):\n        if rev is not None:\n            self.seek(rev)\n        return WindowDictFutureView(self._future)", "response": "Return a Mapping of items after the given revision."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a Mapping of items at or before the given revision.", "response": "def past(self, rev=None):\n        \"\"\"Return a Mapping of items at or before the given revision.\n\n        Default revision is the last one looked up.\n\n        \"\"\"\n        if rev is not None:\n            self.seek(rev)\n        return WindowDictPastView(self._past)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef seek(self, rev):\n        # TODO: binary search? Perhaps only when one or the other\n        # stack is very large?\n        if not self:\n            return\n        if type(rev) is not int:\n            raise TypeError(\"rev must be int\")\n        past = self._past\n        future = self._future\n        if future:\n            appender = past.append\n            popper = future.pop\n            future_start = future[-1][0]\n            while future_start <= rev:\n                appender(popper())\n                if future:\n                    future_start = future[-1][0]\n                else:\n                    break\n        if past:\n            popper = past.pop\n            appender = future.append\n            past_end = past[-1][0]\n            while past_end > rev:\n                appender(popper())\n                if past:\n                    past_end = past[-1][0]\n                else:\n                    break", "response": "Arrange the caches to help look up the given revision."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rev_before(self, rev: int) -> int:\n        self.seek(rev)\n        if self._past:\n            return self._past[-1][0]", "response": "Return the latest past rev on which the value changed."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the earliest future rev on which the value will change.", "response": "def rev_after(self, rev: int) -> int:\n        \"\"\"Return the earliest future rev on which the value will change.\"\"\"\n        self.seek(rev)\n        if self._future:\n            return self._future[-1][0]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndeleting everything after the given revision.", "response": "def truncate(self, rev: int) -> None:\n        \"\"\"Delete everything after the given revision.\"\"\"\n        self.seek(rev)\n        self._keys.difference_update(map(get0, self._future))\n        self._future = []\n        if not self._past:\n            self._beginning = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef build_config(self, config):\n        for sec in 'LiSE', 'ELiDE':\n            config.adddefaultsection(sec)\n        config.setdefaults(\n            'LiSE',\n            {\n                'world': 'sqlite:///LiSEworld.db',\n                'language': 'eng',\n                'logfile': '',\n                'loglevel': 'info'\n            }\n        )\n        config.setdefaults(\n            'ELiDE',\n            {\n                'boardchar': 'physical',\n                'debugger': 'no',\n                'inspector': 'no',\n                'user_kv': 'yes',\n                'play_speed': '1',\n                'thing_graphics': json.dumps([\n                    (\"Marsh Davies' Island\", 'marsh_davies_island_fg.atlas'),\n                    ('RLTiles: Body', 'base.atlas'),\n                    ('RLTiles: Basic clothes', 'body.atlas'),\n                    ('RLTiles: Armwear', 'arm.atlas'),\n                    ('RLTiles: Legwear', 'leg.atlas'),\n                    ('RLTiles: Right hand', 'hand1.atlas'),\n                    ('RLTiles: Left hand', 'hand2.atlas'),\n                    ('RLTiles: Boots', 'boot.atlas'),\n                    ('RLTiles: Hair', 'hair.atlas'),\n                    ('RLTiles: Beard', 'beard.atlas'),\n                    ('RLTiles: Headwear', 'head.atlas')\n                ]),\n                'place_graphics': json.dumps([\n                    (\"Marsh Davies' Island\", 'marsh_davies_island_bg.atlas'),\n                    (\"Marsh Davies' Crypt\", 'marsh_davies_crypt.atlas'),\n                    ('RLTiles: Dungeon', 'dungeon.atlas')\n                ])\n            }\n        )\n        config.write()", "response": "Build the config file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbuild the hierarchy of the hierarchy of the hierarchy.", "response": "def build(self):\n        \"\"\"Make sure I can use the database, create the tables as needed, and\n        return the root widget.\n\n        \"\"\"\n        self.icon = 'icon_24px.png'\n        config = self.config\n        Logger.debug(\n            \"ELiDEApp: starting with world {}, path {}\".format(\n                config['LiSE']['world'],\n                LiSE.__path__[-1]\n            )\n        )\n\n        if config['ELiDE']['debugger'] == 'yes':\n            import pdb\n            pdb.set_trace()\n\n\n        self.manager = ScreenManager(transition=NoTransition())\n        if config['ELiDE']['inspector'] == 'yes':\n            from kivy.core.window import Window\n            from kivy.modules import inspector\n            inspector.create_inspector(Window, self.manager)\n        \n        self._start_subprocess()\n        self._add_screens()\n        return self.manager"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef on_stop(self, *largs):\n        self.strings.save()\n        self.funcs.save()\n        self.engine.commit()\n        self.procman.shutdown()\n        self.config.write()", "response": "Sync the database wrap up the game and halt."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete_selection(self):\n        selection = self.selection\n        if selection is None:\n            return\n        if isinstance(selection, ArrowWidget):\n            self.mainscreen.boardview.board.rm_arrow(\n                selection.origin.name,\n                selection.destination.name\n            )\n            selection.portal.delete()\n        elif isinstance(selection, Spot):\n            self.mainscreen.boardview.board.rm_spot(selection.name)\n            selection.proxy.delete()\n        else:\n            assert isinstance(selection, Pawn)\n            self.mainscreen.boardview.board.rm_pawn(selection.name)\n            selection.proxy.delete()\n        self.selection = None", "response": "Delete both the selected widget and whatever it represents."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmake a board for a character name and switch to it.", "response": "def new_board(self, name):\n        \"\"\"Make a board for a character name, and switch to it.\"\"\"\n        char = self.engine.character[name]\n        board = Board(character=char)\n        self.mainscreen.boards[name] = board\n        self.character = char"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncount how many nodes there already are in the character whose name starts the same.", "response": "def dummynum(character, name):\n    \"\"\"Count how many nodes there already are in the character whose name\n    starts the same.\n\n    \"\"\"\n    num = 0\n    for nodename in character.node:\n        nodename = str(nodename)\n        if not nodename.startswith(name):\n            continue\n        try:\n            nodenum = int(nodename.lstrip(name))\n        except ValueError:\n            continue\n        num = max((nodenum, num))\n    return num"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_thin_rect_vertices(ox, oy, dx, dy, r):\n    if ox < dx:\n        leftx = ox\n        rightx = dx\n        xco = 1\n    elif ox > dx:\n        leftx = ox * -1\n        rightx = dx * -1\n        xco = -1\n    else:\n        return [\n            ox - r, oy,\n            ox + r, oy,\n            ox + r, dy,\n            ox - r, dy\n        ]\n    if oy < dy:\n        boty = oy\n        topy = dy\n        yco = 1\n    elif oy > dy:\n        boty = oy * -1\n        topy = dy * -1\n        yco = -1\n    else:\n        return [\n            ox, oy - r,\n            dx, oy - r,\n            dx, oy + r,\n            ox, oy + r\n        ]\n\n    rise = topy - boty\n    run = rightx - leftx\n    theta = atan(rise/run)\n    theta_prime = ninety - theta\n    xoff = cos(theta_prime) * r\n    yoff = sin(theta_prime) * r\n    x1 = leftx + xoff\n    y1 = boty - yoff\n    x2 = rightx + xoff\n    y2 = topy - yoff\n    x3 = rightx - xoff\n    y3 = topy + yoff\n    x4 = leftx - xoff\n    y4 = boty + yoff\n    return [\n        x1 * xco, y1 * yco,\n        x2 * xco, y2 * yco,\n        x3 * xco, y3 * yco,\n        x4 * xco, y4 * yco\n    ]", "response": "Given the starting point ending point and width return a list of vertex coordinates at the corners of the line segment segment\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nappending new entry to the given keycache.", "response": "def lru_append(kc, lru, kckey, maxsize):\n    \"\"\"Delete old data from ``kc``, then add the new ``kckey``.\n\n    :param kc: a three-layer keycache\n    :param lru: an :class:`OrderedDict` with a key for each triple that should fill out ``kc``'s three layers\n    :param kckey: a triple that indexes into ``kc``, which will be added to ``lru`` if needed\n    :param maxsize: maximum number of entries in ``lru`` and, therefore, ``kc``\n\n    \"\"\"\n    if kckey in lru:\n        return\n    while len(lru) >= maxsize:\n        (peb, turn, tick), _ = lru.popitem(False)\n        if peb not in kc:\n            continue\n        kcpeb = kc[peb]\n        if turn not in kcpeb:\n            continue\n        kcpebturn = kcpeb[turn]\n        if tick not in kcpebturn:\n            continue\n        del kcpebturn[tick]\n        if not kcpebturn:\n            del kcpeb[turn]\n        if not kcpeb:\n            del kc[peb]\n    lru[kckey] = True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load(self, data):\n        branches = defaultdict(list)\n        for row in data:\n            branches[row[-4]].append(row)\n        # Make keycaches and valcaches. Must be done chronologically\n        # to make forwarding work.\n        childbranch = self.db._childbranch\n        branch2do = deque(['trunk'])\n\n        store = self._store\n        while branch2do:\n            branch = branch2do.popleft()\n            for row in branches[branch]:\n                store(*row, planning=False, loading=True)\n            if branch in childbranch:\n                branch2do.extend(childbranch[branch])", "response": "Load a bunch of data into the database."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _valcache_lookup(self, cache, branch, turn, tick):\n        if branch in cache:\n            branc = cache[branch]\n            try:\n                if turn in branc and branc[turn].rev_gettable(tick):\n                    return branc[turn][tick]\n                elif branc.rev_gettable(turn-1):\n                    turnd = branc[turn-1]\n                    return turnd[turnd.end]\n            except HistoryError as ex:\n                # probably shouldn't ever happen, empty branches shouldn't be kept in the cache at all...\n                # but it's easy to handle\n                if ex.deleted:\n                    raise\n        for b, r, t in self.db._iter_parent_btt(branch, turn, tick):\n            if b in cache:\n                if r in cache[b] and cache[b][r].rev_gettable(t):\n                    try:\n                        return cache[b][r][t]\n                    except HistoryError as ex:\n                        if ex.deleted:\n                            raise\n                elif cache[b].rev_gettable(r-1):\n                    cbr = cache[b][r-1]\n                    try:\n                        return cbr[cbr.end]\n                    except HistoryError as ex:\n                        if ex.deleted:\n                            raise", "response": "Return the value at the given time in cache"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_keycachelike(self, keycache, keys, get_adds_dels, parentity, branch, turn, tick, *, forward):\n        keycache_key = parentity + (branch,)\n        keycache2 = keycache3 = None\n        if keycache_key in keycache:\n            keycache2 = keycache[keycache_key]\n            if turn in keycache2:\n                keycache3 = keycache2[turn]\n                if tick in keycache3:\n                    return keycache3[tick]\n        if forward:\n            # Take valid values from the past of a keycache and copy them forward, into the present.\n            # Assumes that time is only moving forward, never backward, never skipping any turns or ticks,\n            # and any changes to the world state are happening through allegedb proper, meaning they'll all get cached.\n            # In LiSE this means every change to the world state should happen inside of a call to\n            # ``Engine.next_turn`` in a rule.\n            if keycache2 and keycache2.rev_gettable(turn):\n                # there's a keycache from a prior turn in this branch. Get it\n                if turn not in keycache2:\n                    # since it's not this *exact* turn there might be changes...\n                    old_turn = keycache2.rev_before(turn)\n                    old_turn_kc = keycache2[turn]\n                    added, deleted = get_adds_dels(\n                        keys[parentity], branch, turn, tick, stoptime=(\n                            branch, old_turn, old_turn_kc.end\n                        )\n                    )\n                    ret = old_turn_kc[old_turn_kc.end].union(added).difference(deleted)\n                    # assert ret == get_adds_dels(keys[parentity], branch, turn, tick)[0]  # slow\n                    new_turn_kc = WindowDict()\n                    new_turn_kc[tick] = ret\n                    keycache2[turn] = new_turn_kc\n                    return ret\n                if not keycache3:\n                    keycache3 = keycache2[turn]\n                if tick not in keycache3:\n                    if keycache3.rev_gettable(tick):\n                        added, deleted = get_adds_dels(\n                            keys[parentity], branch, turn, tick, stoptime=(\n                                branch, turn, keycache3.rev_before(tick)\n                            )\n                        )\n                        ret = keycache3[tick].union(added).difference(deleted)\n                        # assert ret == get_adds_dels(keys[parentity], branch, turn, tick)[0]  # slow\n                        keycache3[tick] = ret\n                        return ret\n                    else:\n                        turn_before = keycache2.rev_before(turn)\n                        tick_before = keycache2[turn_before].end\n                        keys_before = keycache2[turn_before][tick_before]\n                        added, deleted = get_adds_dels(\n                            keys[parentity], branch, turn, tick, stoptime=(\n                                branch, turn_before, tick_before\n                            )\n                        )\n                        ret = keycache3[tick] = keys_before.union(added).difference(deleted)\n                        # assert ret == get_adds_dels(keys[parentity], branch, turn, tick)[0]  # slow\n                        return ret\n                # assert kcturn[tick] == get_adds_dels(keys[parentity], branch, turn, tick)[0]  # slow\n                return keycache3[tick]\n            else:\n                for (parbranch, parturn, partick) in self.db._iter_parent_btt(branch, turn, tick):\n                    par_kc_key = parentity + (parbranch,)\n                    if par_kc_key in keycache:\n                        kcpkc = keycache[par_kc_key]\n                        if parturn in kcpkc and kcpkc[parturn].rev_gettable(partick):\n                            parkeys = kcpkc[parturn][partick]\n                            break\n                        elif kcpkc.rev_gettable(parturn-1):\n                            partkeys = kcpkc[parturn-1]\n                            parkeys = partkeys[partkeys.end]\n                            break\n                else:\n                    parkeys = frozenset()\n                keycache2 = SettingsTurnDict()\n                added, deleted = get_adds_dels(\n                    keys[parentity], branch, turn, tick, stoptime=(\n                        parbranch, parturn, partick\n                    )\n                )\n                ret = parkeys.union(added).difference(deleted)\n                keycache2[turn] = {tick: ret}\n                keycache[keycache_key] = keycache2\n                # assert ret == get_adds_dels(keys[parentity], branch, turn, tick)[0]  # slow\n                return ret\n        ret = frozenset(get_adds_dels(keys[parentity], branch, turn, tick)[0])\n        if keycache2:\n            if keycache3:\n                keycache3[tick] = ret\n            else:\n                keycache2[turn] = {tick: ret}\n        else:\n            kcc = SettingsTurnDict()\n            kcc[turn] = {tick: ret}\n            keycache[keycache_key] = kcc\n        return ret", "response": "Get a keycache entry for a given branch turn tick and if it can t generate one and return it."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting a frozenset of keys that exist in the entity at the moment.", "response": "def _get_keycache(self, parentity, branch, turn, tick, *, forward):\n        \"\"\"Get a frozenset of keys that exist in the entity at the moment.\n\n        With ``forward=True``, enable an optimization that copies old key sets\n        forward and updates them.\n\n        \"\"\"\n        lru_append(self.keycache, self._kc_lru, (parentity+(branch,), turn, tick), KEYCACHE_MAXSIZE)\n        return self._get_keycachelike(\n            self.keycache, self.keys, self._get_adds_dels,\n            parentity, branch, turn, tick, forward=forward\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _update_keycache(self, *args, forward):\n        entity, key, branch, turn, tick, value = args[-6:]\n        parent = args[:-6]\n        kc = self._get_keycache(parent + (entity,), branch, turn, tick, forward=forward)\n        if value is None:\n            kc = kc.difference((key,))\n        else:\n            kc = kc.union((key,))\n        self.keycache[parent+(entity, branch)][turn][tick] = kc", "response": "Update the keycache with the new keys."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_adds_dels(self, cache, branch, turn, tick, *, stoptime=None):\n        added = set()\n        deleted = set()\n        for key, branches in cache.items():\n            for (branc, trn, tck) in self.db._iter_parent_btt(branch, turn, tick, stoptime=stoptime):\n                if branc not in branches or not branches[branc].rev_gettable(trn):\n                    continue\n                turnd = branches[branc]\n                if trn in turnd:\n                    if turnd[trn].rev_gettable(tck):\n                        if turnd[trn][tck] is None:\n                            deleted.add(key)\n                        else:\n                            added.add(key)\n                        break\n                    else:\n                        trn -= 1\n                if not turnd.rev_gettable(trn):\n                    break\n                tickd = turnd[trn]\n                if tickd[tickd.end] is None:\n                    deleted.add(key)\n                else:\n                    added.add(key)\n                break\n        return added, deleted", "response": "Return a pair of added and deleted sets describing changes since stoptime."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nstoring a value in the keycache for later. retrieve.", "response": "def store(self, *args, planning=None, forward=None, loading=False, contra=True):\n        \"\"\"Put a value in various dictionaries for later .retrieve(...).\n\n        Needs at least five arguments, of which the -1th is the value\n        to store, the -2th is the tick to store it at, the -3th\n        is the turn to store it in, the -4th is the branch the\n        revision is in, the -5th is the key the value is for,\n        and the remaining arguments identify the entity that has\n        the key, eg. a graph, node, or edge.\n\n        With ``planning=True``, you will be permitted to alter\n        \"history\" that takes place after the last non-planning\n        moment of time, without much regard to consistency.\n        Otherwise, contradictions will be handled by deleting\n        everything in the contradicted plan after the present moment,\n        unless you set ``contra=False``.\n\n        ``loading=True`` prevents me from updating the ORM's records\n        of the ends of branches and turns.\n\n        \"\"\"\n        db = self.db\n        if planning is None:\n            planning = db._planning\n        if forward is None:\n            forward = db._forward\n        self._store(*args, planning=planning, loading=loading, contra=contra)\n        if not db._no_kc:\n            self._update_keycache(*args, forward=forward)\n        self.send(self, key=args[-5], branch=args[-4], turn=args[-3], tick=args[-2], value=args[-1], action='store')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef remove(self, branch, turn, tick):\n        time_entity, parents, branches, keys, settings, presettings, remove_keycache, send = self._remove_stuff\n        parent, entity, key = time_entity[branch, turn, tick]\n        branchkey = parent + (entity, key)\n        keykey = parent + (entity,)\n        if parent in parents:\n            parentt = parents[parent]\n            if entity in parentt:\n                entty = parentt[entity]\n                if key in entty:\n                    kee = entty[key]\n                    if branch in kee:\n                        branhc = kee[branch]\n                        if turn in branhc:\n                            trn = branhc[turn]\n                            del trn[tick]\n                            if not trn:\n                                del branhc[turn]\n                            if not branhc:\n                                del kee[branch]\n                    if not kee:\n                        del entty[key]\n                if not entty:\n                    del parentt[entity]\n            if not parentt:\n                del parents[parent]\n        if branchkey in branches:\n            entty = branches[branchkey]\n            if branch in entty:\n                branhc = entty[branch]\n                if turn in branhc:\n                    trn = branhc[turn]\n                    if tick in trn:\n                        del trn[tick]\n                    if not trn:\n                        del branhc[turn]\n                if not branhc:\n                    del entty[branch]\n            if not entty:\n                del branches[branchkey]\n        if keykey in keys:\n            entty = keys[keykey]\n            if key in entty:\n                kee = entty[key]\n                if branch in kee:\n                    branhc = kee[branch]\n                    if turn in branhc:\n                        trn = entty[turn]\n                        if tick in trn:\n                            del trn[tick]\n                        if not trn:\n                            del branhc[turn]\n                    if not branhc:\n                        del kee[branch]\n                if not kee:\n                    del entty[key]\n            if not entty:\n                del keys[keykey]\n        branhc = settings[branch]\n        pbranhc = presettings[branch]\n        trn = branhc[turn]\n        ptrn = pbranhc[turn]\n        if tick in trn:\n            del trn[tick]\n        if tick in ptrn:\n            del ptrn[tick]\n        if not ptrn:\n            del pbranhc[turn]\n            del branhc[turn]\n        if not pbranhc:\n            del settings[branch]\n            del presettings[branch]\n        self.shallowest = OrderedDict()\n        remove_keycache(parent + (entity, branch), turn, tick)\n        send(self, branch=branch, turn=turn, tick=tick, action='remove')", "response": "Delete all data from a specific tick"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove the future of a given entity from the keycache", "response": "def _remove_keycache(self, entity_branch, turn, tick):\n        \"\"\"Remove the future of a given entity from a branch in the keycache\"\"\"\n        keycache = self.keycache\n        if entity_branch in keycache:\n            kc = keycache[entity_branch]\n            if turn in kc:\n                kcturn = kc[turn]\n                if tick in kcturn:\n                    del kcturn[tick]\n                kcturn.truncate(tick)\n                if not kcturn:\n                    del kc[turn]\n            kc.truncate(turn)\n            if not kc:\n                del keycache[entity_branch]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef truncate(self, branch, turn, tick):\n        parents, branches, keys, settings, presettings, keycache, send = self._truncate_stuff\n        def truncate_branhc(branhc):\n            if turn in branhc:\n                trn = branhc[turn]\n                trn.truncate(tick)\n                branhc.truncate(turn)\n                if not trn:\n                    del branhc[turn]\n            else:\n                branhc.truncate(turn)\n        for entities in parents.values():\n            for keys in entities.values():\n                for branches in keys.values():\n                    if branch not in branches:\n                        continue\n                    truncate_branhc(branches[branch])\n        for branches in branches.values():\n            if branch not in branches:\n                continue\n            truncate_branhc(branches[branch])\n        for keys in keys.values():\n            for branches in keys.values():\n                if branch not in branches:\n                    continue\n                truncate_branhc(branches[branch])\n        truncate_branhc(settings[branch])\n        truncate_branhc(presettings[branch])\n        self.shallowest = OrderedDict()\n        for entity_branch in keycache:\n            if entity_branch[-1] == branch:\n                truncate_branhc(keycache[entity_branch])\n        send(self, branch=branch, turn=turn, tick=tick, action='truncate')", "response": "Delete all data after a specific tick"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _iter_future_contradictions(entity, key, turns, branch, turn, tick, value):\n        # assumes that all future entries are in the plan\n        if not turns:\n            return\n        if turn in turns:\n            future_ticks = turns[turn].future(tick)\n            for tck, newval in future_ticks.items():\n                if newval != value:\n                    yield turn, tck\n            future_turns = turns.future(turn)\n        elif turns.rev_gettable(turn):\n            future_turns = turns.future(turn)\n        else:\n            future_turns = turns\n        if not future_turns:\n            return\n        for trn, ticks in future_turns.items():\n            for tick, newval in ticks.items():\n                if newval != value:\n                    yield trn, tick", "response": "Iterate over all future contradictions."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving a value from the store.", "response": "def retrieve(self, *args):\n        \"\"\"Get a value previously .store(...)'d.\n\n        Needs at least five arguments. The -1th is the tick\n        within the turn you want,\n        the -2th is that turn, the -3th is the branch,\n        and the -4th is the key. All other arguments identify\n        the entity that the key is in.\n\n        \"\"\"\n        ret = self._base_retrieve(args)\n        if ret is None:\n            raise HistoryError(\"Set, then deleted\", deleted=True)\n        elif ret is KeyError:\n            raise ret\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the number of keys an entity has or the number of entities.", "response": "def count_entities_or_keys(self, *args, forward=None):\n        \"\"\"Return the number of keys an entity has, if you specify an entity.\n\n        Otherwise return the number of entities.\n\n        \"\"\"\n        if forward is None:\n            forward = self.db._forward\n        entity = args[:-3]\n        branch, turn, tick = args[-3:]\n        if self.db._no_kc:\n            return len(self._get_adds_dels(self.keys[entity], branch, turn, tick)[0])\n        return len(self._get_keycache(entity, branch, turn, tick, forward=forward))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _adds_dels_sucpred(self, cache, branch, turn, tick, *, stoptime=None):\n        added = set()\n        deleted = set()\n        for node, nodes in cache.items():\n            addidx, delidx = self._get_adds_dels(nodes, branch, turn, tick, stoptime=stoptime)\n            if addidx and not delidx:\n                added.add(node)\n            elif delidx and not addidx:\n                deleted.add(node)\n        return added, deleted", "response": "Take the successors or predecessors cache and get nodes added or deleted from it\n\n        Operates like _get_adds_dels."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a set of destination nodes succeeding orig.", "response": "def _get_destcache(self, graph, orig, branch, turn, tick, *, forward):\n        \"\"\"Return a set of destination nodes succeeding ``orig``\"\"\"\n        destcache, destcache_lru, get_keycachelike, successors, adds_dels_sucpred = self._get_destcache_stuff\n        lru_append(destcache, destcache_lru, ((graph, orig, branch), turn, tick), KEYCACHE_MAXSIZE)\n        return get_keycachelike(\n            destcache, successors, adds_dels_sucpred, (graph, orig),\n            branch, turn, tick, forward=forward\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a set of origin nodes leading to dest.", "response": "def _get_origcache(self, graph, dest, branch, turn, tick, *, forward):\n        \"\"\"Return a set of origin nodes leading to ``dest``\"\"\"\n        origcache, origcache_lru, get_keycachelike, predecessors, adds_dels_sucpred = self._get_origcache_stuff\n        lru_append(origcache, origcache_lru, ((graph, dest, branch), turn, tick), KEYCACHE_MAXSIZE)\n        return get_keycachelike(\n            origcache, predecessors, adds_dels_sucpred, (graph, dest),\n            branch, turn, tick, forward=forward\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\niterates over successors of a given origin node at a given time.", "response": "def iter_successors(self, graph, orig, branch, turn, tick, *, forward=None):\n        \"\"\"Iterate over successors of a given origin node at a given time.\"\"\"\n        if self.db._no_kc:\n            yield from self._adds_dels_sucpred(self.successors[graph, orig], branch, turn, tick)[0]\n            return\n        if forward is None:\n            forward = self.db._forward\n        yield from self._get_destcache(graph, orig, branch, turn, tick, forward=forward)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef iter_predecessors(self, graph, dest, branch, turn, tick, *, forward=None):\n        if self.db._no_kc:\n            yield from self._adds_dels_sucpred(self.predecessors[graph, dest], branch, turn, tick)[0]\n            return\n        if forward is None:\n            forward = self.db._forward\n        yield from self._get_origcache(graph, dest, branch, turn, tick, forward=forward)", "response": "Iterate over predecessors to a given destination node at a given time."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef has_successor(self, graph, orig, dest, branch, turn, tick, *, forward=None):\n        if forward is None:\n            forward = self.db._forward\n        return dest in self._get_destcache(graph, orig, branch, turn, tick, forward=forward)", "response": "Return True if an edge connects the origin to the destination at the given time."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn True if an edge connects the destination to the origin at the given time.", "response": "def has_predecessor(self, graph, dest, orig, branch, turn, tick, *, forward=None):\n        \"\"\"Return whether an edge connects the destination to the origin at the given time.\n\n        Doesn't require the edge's index, which makes it slower than retrieving a\n        particular edge.\n\n        \"\"\"\n        if forward is None:\n            forward = self.db._forward\n        return orig in self._get_origcache(graph, dest, branch, turn, tick, forward=forward)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef push_pos(self, *args):\n        self.proxy['_x'] = self.x / self.board.width\n        self.proxy['_y'] = self.y / self.board.height", "response": "Set my current position expressed as proportions of the board s\n        width and height into the _x and _y keys of the base class."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert an AllegedGraph to NetworkX graph type.", "response": "def convert_to_networkx_graph(data, create_using=None, multigraph_input=False):\n    \"\"\"Convert an AllegedGraph to the corresponding NetworkX graph type.\"\"\"\n    if isinstance(data, AllegedGraph):\n        result = networkx.convert.from_dict_of_dicts(\n            data.adj,\n            create_using=create_using,\n            multigraph_input=data.is_multigraph()\n        )\n        result.graph = dict(data.graph)\n        result.node = {k: dict(v) for k, v in data.node.items()}\n        return result\n    return networkx.convert.to_networkx_graph(\n        data, create_using, multigraph_input\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef connect(self, func):\n        l = _alleged_receivers[id(self)]\n        if func not in l:\n            l.append(func)", "response": "Connect a function to the current object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndisconnecting the function from the broker.", "response": "def disconnect(self, func):\n        \"\"\"No longer call the function when something changes here.\"\"\"\n        if id(self) not in _alleged_receivers:\n            return\n        l = _alleged_receivers[id(self)]\n        try:\n            l.remove(func)\n        except ValueError:\n            return\n        if not l:\n            del _alleged_receivers[id(self)]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef send(self, sender, **kwargs):\n        if id(self) not in _alleged_receivers:\n            return\n        for func in _alleged_receivers[id(self)]:\n            func(sender, **kwargs)", "response": "Internal. Call connected functions."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update(self, other, **kwargs):\n        from itertools import chain\n        if hasattr(other, 'items'):\n            other = other.items()\n        for (k, v) in chain(other, kwargs.items()):\n            if (\n                    k not in self or\n                    self[k] != v\n            ):\n                self[k] = v", "response": "Version of update that doesn t clobber the database so much"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _del_db(self, key, branch, turn, tick):\n        self._set_db(key, branch, turn, tick, None)", "response": "Delete a key from the database."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nremoving all nodes and edges from the graph.", "response": "def clear(self):\n        \"\"\"Remove all nodes and edges from the graph.\n\n        Unlike the regular networkx implementation, this does *not*\n        remove the graph's name. But all the other graph, node, and\n        edge attributes go away.\n\n        \"\"\"\n        self.adj.clear()\n        self.node.clear()\n        self.graph.clear()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef remove_edge(self, u, v):\n        try:\n            del self.succ[u][v]\n        except KeyError:\n            raise NetworkXError(\n                \"The edge {}-{} is not in the graph.\".format(u, v)\n            )", "response": "Version of remove_edge that deletes one edge from the graph."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nversioning of remove_edges_from that deletes edges from ebunch.", "response": "def remove_edges_from(self, ebunch):\n        \"\"\"Version of remove_edges_from that's much like normal networkx but only\n        deletes once, since the database doesn't keep separate adj and\n        succ mappings\n\n        \"\"\"\n        for e in ebunch:\n            (u, v) = e[:2]\n            if u in self.succ and v in self.succ[u]:\n                del self.succ[u][v]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_edge(self, u, v, attr_dict=None, **attr):\n        if attr_dict is None:\n            attr_dict = attr\n        else:\n            try:\n                attr_dict.update(attr)\n            except AttributeError:\n                raise NetworkXError(\n                    \"The attr_dict argument must be a dictionary.\"\n                )\n        if u not in self.node:\n            self.node[u] = {}\n        if v not in self.node:\n            self.node[v] = {}\n        if u in self.adj:\n            datadict = self.adj[u].get(v, {})\n        else:\n            self.adj[u] = {v: {}}\n            datadict = self.adj[u][v]\n        datadict.update(attr_dict)\n        self.succ[u][v] = datadict\n        assert u in self.succ, \"Failed to add edge {u}->{v} ({u} not in successors)\".format(u=u, v=v)\n        assert v in self.succ[u], \"Failed to add edge {u}->{v} ({v} not in succ[{u}])\".format(u=u, v=v)", "response": "Version of add_edge that only writes to the database once"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_edges_from(self, ebunch, attr_dict=None, **attr):\n        if attr_dict is None:\n            attr_dict = attr\n        else:\n            try:\n                attr_dict.update(attr)\n            except AttributeError:\n                raise NetworkXError(\n                    \"The attr_dict argument must be a dict.\"\n                )\n        for e in ebunch:\n            ne = len(e)\n            if ne == 3:\n                u, v, dd = e\n                assert hasattr(dd, \"update\")\n            elif ne == 2:\n                u, v = e\n                dd = {}\n            else:\n                raise NetworkXError(\n                    \"Edge tupse {} must be a 2-tuple or 3-tuple.\".format(e)\n                )\n            if u not in self.node:\n                self.node[u] = {}\n            if v not in self.node:\n                self.node[v] = {}\n            datadict = self.adj.get(u, {}).get(v, {})\n            datadict.update(attr_dict)\n            datadict.update(dd)\n            self.succ[u][v] = datadict\n            assert(u in self.succ)\n            assert(v in self.succ[u])", "response": "Version of add_edges_from that only writes to the database once"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remove_edge(self, u, v, key=None):\n        try:\n            d = self.adj[u][v]\n        except KeyError:\n            raise NetworkXError(\n                \"The edge {}-{} is not in the graph.\".format(u, v)\n            )\n        if key is None:\n            d.popitem()\n        else:\n            try:\n                del d[key]\n            except KeyError:\n                raise NetworkXError(\n                    \"The edge {}-{} with key {} is not in the graph.\".format\n                    (u, v, key)\n                )\n        if len(d) == 0:\n            del self.succ[u][v]", "response": "Version of remove_edge that deletes one edge from the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_edge(self, u, v, key=None, attr_dict=None, **attr):\n        if attr_dict is None:\n            attr_dict = attr\n        else:\n            try:\n                attr_dict.update(attr)\n            except AttributeError:\n                raise NetworkXError(\n                    \"The attr_dict argument must be a dictionary.\"\n                )\n        if u not in self.node:\n            self.node[u] = {}\n        if v not in self.node:\n            self.node[v] = {}\n        if v in self.succ[u]:\n            keydict = self.adj[u][v]\n            if key is None:\n                key = len(keydict)\n                while key in keydict:\n                    key += 1\n            datadict = keydict.get(key, {})\n            datadict.update(attr_dict)\n            keydict[key] = datadict\n        else:\n            if key is None:\n                key = 0\n            datadict = {}\n            datadict.update(attr_dict)\n            keydict = {key: datadict}\n            self.succ[u][v] = keydict\n        return key", "response": "Version of add_edge that only writes to the database once."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves myself from the world model immediately.", "response": "def delete(self):\n        \"\"\"Remove myself from the world model immediately.\"\"\"\n        super().delete()\n        self.character.place.send(self.character.place, key=self.name, val=None)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef patch(self, patch):\n        self.engine.handle(\n            'update_nodes',\n            char=self.character.name,\n            patch=patch,\n            block=False\n        )\n        for node, stats in patch.items():\n            nodeproxycache = self[node]._cache\n            for k, v in stats.items():\n                if v is None:\n                    del nodeproxycache[k]\n                else:\n                    nodeproxycache[k] = v", "response": "Updates the node stats at once."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsending a command to the LiSE core.", "response": "def handle(self, cmd=None, **kwargs):\n        \"\"\"Send a command to the LiSE core.\n\n        The only positional argument should be the name of a\n        method in :class:``EngineHandle``. All keyword arguments\n        will be passed to it, with the exceptions of\n        ``cb``, ``branching``, and ``silent``.\n\n        With ``block=False``, don't wait for a result.\n        This is best for when you want to make some change to the game\n        state and already know what effect it will have.\n\n        With ``branching=True``, handle paradoxes by creating new\n        branches of history. I will switch to the new branch if needed.\n        If I have an attribute ``branching_cb``, I'll call it if and\n        only if the branch changes upon completing a command with\n        ``branching=True``.\n\n        With a function ``cb``, I will call ``cb`` when I get\n        a result. If ``block=False`` this will happen in a thread.\n        ``cb`` will be called with keyword arguments ``command``,\n        the same command you asked for; ``result``, the value returned\n        by it, possibly ``None``; and the present ``branch``,\n        ``turn``, and ``tick``, possibly different than when you called\n        ``handle``.\n\n        If any of ``branching``, ``cb``, or ``future`` are ``True``,\n        I will return a ``Future``. The ``Future``'s return value\n        is a tuple of ``(command, branch, turn, tick, result)``.\n\n        \"\"\"\n        if 'command' in kwargs:\n            cmd = kwargs['command']\n        elif cmd:\n            kwargs['command'] = cmd\n        else:\n            raise TypeError(\"No command\")\n        branching = kwargs.get('branching', False)\n        cb = kwargs.pop('cb', None)\n        future = kwargs.pop('future', False)\n        self._handle_lock.acquire()\n        if kwargs.pop('block', True):\n            assert not kwargs.get('silent')\n            self.debug('EngineProxy: sending {}'.format(kwargs))\n            self.send(self.pack(kwargs))\n            command, branch, turn, tick, result = self.recv()\n            assert cmd == command, \\\n                \"Sent command {} but received results for {}\".format(\n                    cmd, command\n                )\n            r = self.unpack(result)\n            self.debug('EngineProxy: received {}'.format((command, branch, turn, tick, r)))\n            if (branch, turn, tick) != self._btt():\n                self._branch = branch\n                self._turn = turn\n                self._tick = tick\n                self.time.send(self, branch=branch, turn=turn, tick=tick)\n            if isinstance(r, Exception):\n                self._handle_lock.release()\n                raise r\n            if cb:\n                cb(command=command, branch=branch, turn=turn, tick=tick, result=r)\n            self._handle_lock.release()\n            return r\n        else:\n            kwargs['silent'] = not (branching or cb or future)\n            self.debug('EngineProxy: asynchronously sending {}'.format(kwargs))\n            self.send(self.pack(kwargs))\n            if branching:\n                # what happens if more than one branching call is happening at once?\n                return self._submit(self._branching, cb)\n            elif cb:\n                return self._submit(self._callback, cb)\n            if future:\n                return self._submit(self._unpack_recv)\n        self._handle_lock.release()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef pull(self, chars='all', cb=None, block=True):\n        if block:\n            deltas = self.handle('get_char_deltas', chars=chars)\n            self._upd_caches(deltas)\n            if cb:\n                cb(deltas)\n        else:\n            return self._submit(self._pull_async, chars, cb)", "response": "Update the state of all my proxy objects from the real objects."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef time_travel(self, branch, turn, tick=None, chars='all', cb=None, block=True):\n        if cb and not chars:\n            raise TypeError(\"Callbacks require chars\")\n        if cb is not None and not callable(cb):\n            raise TypeError(\"Uncallable callback\")\n        return self.handle(\n            'time_travel',\n            block=block,\n            branch=branch,\n            turn=turn,\n            tick=tick,\n            chars=chars,\n            cb=partial(self._upd_and_cb, cb)\n        )", "response": "Moves the current time in the timestream to a different point in the turn."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remove(self, branch, turn, tick):\n        for parent, entitys in list(self.parents.items()):\n            for entity, keys in list(entitys.items()):\n                for key, branchs in list(keys.items()):\n                    if branch in branchs:\n                        branhc = branchs[branch]\n                        if turn in branhc:\n                            trun = branhc[turn]\n                            if tick in trun:\n                                del trun[tick]\n                            trun.truncate(tick)\n                            if not trun:\n                                del branhc[turn]\n                        branhc.truncate(turn)\n                        if not branhc:\n                            del branchs[branch]\n                    if not branchs:\n                        del keys[key]\n                if not keys:\n                    del entitys[entity]\n            if not entitys:\n                del self.parents[parent]\n        for branchkey, branches in list(self.branches.items()):\n            if branch in branches:\n                branhc = branches[branch]\n                if turn in branhc:\n                    trun = branhc[turn]\n                    if tick in trun:\n                        del trun[tick]\n                    trun.truncate(tick)\n                    if not trun:\n                        del branhc[turn]\n                branhc.truncate(turn)\n                if not branhc:\n                    del branches[branch]\n            if not branches:\n                del self.branches[branchkey]\n        for keykey, keys in list(self.keys.items()):\n            for key, branchs in list(keys.items()):\n                if branch in branchs:\n                    branhc = branchs[branch]\n                    if turn in branhc:\n                        trun = branhc[turn]\n                        if tick in trun:\n                            del trun[tick]\n                        trun.truncate(tick)\n                        if not trun:\n                            del branhc[turn]\n                    branhc.truncate(turn)\n                    if not branhc:\n                        del branches[branch]\n                if not branchs:\n                    del keys[key]\n            if not keys:\n                del self.keys[keykey]\n        sets = self.settings[branch]\n        if turn in sets:\n            setsturn = sets[turn]\n            if tick in setsturn:\n                del setsturn[tick]\n            setsturn.truncate(tick)\n            if not setsturn:\n                del sets[turn]\n        sets.truncate(turn)\n        if not sets:\n            del self.settings[branch]\n        presets = self.presettings[branch]\n        if turn in presets:\n            presetsturn = presets[turn]\n            if tick in presetsturn:\n                del presetsturn[tick]\n            presetsturn.truncate(tick)\n            if not presetsturn:\n                del presets[turn]\n        presets.truncate(turn)\n        if not presets:\n            del self.presettings[branch]\n        for entity, brnch in list(self.keycache):\n            if brnch == branch:\n                kc = self.keycache[entity, brnch]\n                if turn in kc:\n                    kcturn = kc[turn]\n                    if tick in kcturn:\n                        del kcturn[tick]\n                    kcturn.truncate(tick)\n                    if not kcturn:\n                        del kc[turn]\n                kc.truncate(turn)\n                if not kc:\n                    del self.keycache[entity, brnch]\n        self.shallowest = OrderedDict()\n        self.send(self, branch=branch, turn=turn, tick=tick, action='remove')", "response": "Delete data on or after this tick."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nremove future data about a particular location. Return True if I deleted anything False otherwise.", "response": "def truncate_loc(self, character, location, branch, turn, tick):\n        \"\"\"Remove future data about a particular location\n\n        Return True if I deleted anything, False otherwise.\n\n        \"\"\"\n        r = False\n        branches_turns = self.branches[character, location][branch]\n        branches_turns.truncate(turn)\n        if turn in branches_turns:\n            bttrn = branches_turns[turn]\n            if bttrn.future(tick):\n                bttrn.truncate(tick)\n                r = True\n        keyses = self.keys[character, location]\n        for keysbranches in keyses.values():\n            if branch not in keysbranches:\n                continue\n            keysbranch = keysbranches[branch]\n            if keysbranch.future(turn):\n                keysbranch.truncate(turn)\n                r = True\n            if turn in keysbranch:\n                keysbranchturn = keysbranch[turn]\n                if keysbranchturn.future(tick):\n                    keysbranchturn.truncate(tick)\n                    r = True\n        if branch in self.settings:\n            for sets in (self.settings, self.presettings):\n                sets_branch = sets[branch]\n                if turn in sets_branch:\n                    sets_turn = sets_branch[turn]\n                    for tic, setting in list(sets_turn.future(tick).items()):\n                        if setting[:2] == (character, location):\n                            del sets_turn[tic]\n                            r = True\n                    if not sets_turn:\n                        del sets_branch[turn]\n                        assert r, \"Found an empty cache when I didn't delete anything\"\n                for trn, tics in list(sets_branch.future(turn).items()):\n                    for tic, setting in list(tics.future(tick).items()):\n                        if setting[:2] == (character, location):\n                            del tics[tic]\n                            r = True\n                    if not tics:\n                        del sets_branch[trn]\n                        assert r, \"Found an empty cache when I didn't delete anything\"\n                if not sets_branch:\n                    del sets[branch]\n                    assert r, \"Found an empty cache when I didn't delete anything\"\n        self.shallowest = OrderedDict()\n        return r"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the new stat on the currently selected item.", "response": "def new_stat(self):\n        \"\"\"Look at the key and value that the user has entered into the stat\n        configurator, and set them on the currently selected\n        entity.\n\n        \"\"\"\n        key = self.ids.newstatkey.text\n        value = self.ids.newstatval.text\n        if not (key and value):\n            # TODO implement some feedback to the effect that\n            # you need to enter things\n            return\n        try:\n            self.proxy[key] = self.engine.unpack(value)\n        except (TypeError, ValueError):\n            self.proxy[key] = value\n        self.ids.newstatkey.text = ''\n        self.ids.newstatval.text = ''"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncall this after you have created all the PawnSpot you need and are ready to add them to the board.", "response": "def finalize(self, initial=True):\n        \"\"\"Call this after you've created all the PawnSpot you need and are ready to add them to the board.\"\"\"\n        if getattr(self, '_finalized', False):\n            return\n        if (\n                self.proxy is None or\n                not hasattr(self.proxy, 'name')\n        ):\n            Clock.schedule_once(self.finalize, 0)\n            return\n        if initial:\n            self.name = self.proxy.name\n            self.paths = self.proxy.setdefault(\n                '_image_paths', self.default_image_paths\n            )\n            zeroes = [0] * len(self.paths)\n            self.offxs = self.proxy.setdefault('_offxs', zeroes)\n            self.offys = self.proxy.setdefault('_offys', zeroes)\n            self.proxy.connect(self._trigger_pull_from_proxy)\n        self.bind(\n            paths=self._trigger_push_image_paths,\n            offxs=self._trigger_push_offxs,\n            offys=self._trigger_push_offys\n        )\n        self._finalized = True\n        self.finalize_children()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef on_linecolor(self, *args):\n        if hasattr(self, 'color'):\n            self.color.rgba = self.linecolor\n            return\n\n        def upd_box_translate(*args):\n            self.box_translate.xy = self.pos\n\n        def upd_box_points(*args):\n            self.box.points = [0, 0, self.width, 0, self.width, self.height, 0, self.height, 0, 0]\n\n        self.boxgrp = boxgrp = InstructionGroup()\n        self.color = Color(*self.linecolor)\n        self.box_translate = Translate(*self.pos)\n        boxgrp.add(PushMatrix())\n        boxgrp.add(self.box_translate)\n        boxgrp.add(self.color)\n        self.box = Line()\n        upd_box_points()\n        self.bind(\n            size=upd_box_points,\n            pos=upd_box_translate\n        )\n        boxgrp.add(self.box)\n        boxgrp.add(Color(1., 1., 1.))\n        boxgrp.add(PopMatrix())", "response": "When the color of the current color is changed put them there."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef normalize_layout(l):\n    xs = []\n    ys = []\n    ks = []\n    for (k, (x, y)) in l.items():\n        xs.append(x)\n        ys.append(y)\n        ks.append(k)\n    minx = np.min(xs)\n    maxx = np.max(xs)\n    try:\n        xco = 0.98 / (maxx - minx)\n        xnorm = np.multiply(np.subtract(xs, [minx] * len(xs)), xco)\n    except ZeroDivisionError:\n        xnorm = np.array([0.5] * len(xs))\n    miny = np.min(ys)\n    maxy = np.max(ys)\n    try:\n        yco = 0.98 / (maxy - miny)\n        ynorm = np.multiply(np.subtract(ys, [miny] * len(ys)), yco)\n    except ZeroDivisionError:\n        ynorm = np.array([0.5] * len(ys))\n    return dict(zip(ks, zip(map(float, xnorm), map(float, ynorm))))", "response": "Make sure all the spots in a layout are within 0. 098."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking for collisions and select an appropriate entity.", "response": "def on_touch_down(self, touch):\n        \"\"\"Check for collisions and select an appropriate entity.\"\"\"\n        if hasattr(self, '_lasttouch') and self._lasttouch == touch:\n            return\n        if not self.collide_point(*touch.pos):\n            return\n        touch.push()\n        touch.apply_transform_2d(self.to_local)\n        if self.app.selection:\n            if self.app.selection.collide_point(*touch.pos):\n                Logger.debug(\"Board: hit selection\")\n                touch.grab(self.app.selection)\n        pawns = list(self.pawns_at(*touch.pos))\n        if pawns:\n            Logger.debug(\"Board: hit {} pawns\".format(len(pawns)))\n            self.selection_candidates = pawns\n            if self.app.selection in self.selection_candidates:\n                self.selection_candidates.remove(self.app.selection)\n            touch.pop()\n            return True\n        spots = list(self.spots_at(*touch.pos))\n        if spots:\n            Logger.debug(\"Board: hit {} spots\".format(len(spots)))\n            self.selection_candidates = spots\n            if self.adding_portal:\n                self.origspot = self.selection_candidates.pop(0)\n                self.protodest = Dummy(\n                    name='protodest',\n                    pos=touch.pos,\n                    size=(0, 0)\n                )\n                self.add_widget(self.protodest)\n                self.protodest.on_touch_down(touch)\n                self.protoportal = self.proto_arrow_cls(\n                    origin=self.origspot,\n                    destination=self.protodest\n                )\n                self.add_widget(self.protoportal)\n                if self.reciprocal_portal:\n                    self.protoportal2 = self.proto_arrow_cls(\n                        destination=self.origspot,\n                        origin=self.protodest\n                    )\n                    self.add_widget(self.protoportal2)\n            touch.pop()\n            return True\n        arrows = list(self.arrows_at(*touch.pos))\n        if arrows:\n            Logger.debug(\"Board: hit {} arrows\".format(len(arrows)))\n            self.selection_candidates = arrows\n            if self.app.selection in self.selection_candidates:\n                self.selection_candidates.remove(self.app.selection)\n            touch.pop()\n            return True\n        touch.pop()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef portal_touch_up(self, touch):\n        try:\n            # If the touch ended upon a spot, and there isn't\n            # already a portal between the origin and this\n            # destination, create one.\n            destspot = next(self.spots_at(*touch.pos))\n            orig = self.origspot.proxy\n            dest = destspot.proxy\n            if not(\n                orig.name in self.character.portal and\n                dest.name in self.character.portal[orig.name]\n            ):\n                port = self.character.new_portal(\n                    orig.name,\n                    dest.name\n                )\n                self.arrowlayout.add_widget(\n                    self.make_arrow(port)\n                )\n            # And another in the opposite direction if needed\n                if (\n                    hasattr(self, 'protoportal2') and not(\n                            orig.name in self.character.preportal and\n                            dest.name in self.character.preportal[orig.name]\n                        )\n                ):\n                    deport = self.character.new_portal(\n                        dest.name,\n                        orig.name\n                    )\n                    self.arrowlayout.add_widget(\n                        self.make_arrow(deport)\n                    )\n        except StopIteration:\n            pass\n        self.remove_widget(self.protoportal)\n        if hasattr(self, 'protoportal2'):\n            self.remove_widget(self.protoportal2)\n            del self.protoportal2\n        self.remove_widget(self.protodest)\n        del self.protoportal\n        del self.protodest", "response": "Try to create a portal between the spots the user chose."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef on_touch_up(self, touch):\n        if hasattr(self, '_lasttouch') and self._lasttouch == touch:\n            return\n        self._lasttouch = touch\n        touch.push()\n        touch.apply_transform_2d(self.to_local)\n        if hasattr(self, 'protodest'):\n            Logger.debug(\"Board: on_touch_up making a portal\")\n            touch.ungrab(self)\n            ret = self.portal_touch_up(touch)\n            touch.pop()\n            return ret\n        if self.app.selection and hasattr(self.app.selection, 'on_touch_up'):\n            self.app.selection.dispatch('on_touch_up', touch)\n        for candidate in self.selection_candidates:\n            if candidate == self.app.selection:\n                continue\n            if candidate.collide_point(*touch.pos):\n                Logger.debug(\"Board: selecting \" + repr(candidate))\n                if hasattr(candidate, 'selected'):\n                    candidate.selected = True\n                if hasattr(self.app.selection, 'selected'):\n                    self.app.selection.selected = False\n                self.app.selection = candidate\n                self.keep_selection = True\n                parent = candidate.parent\n                parent.remove_widget(candidate)\n                parent.add_widget(candidate)\n                break\n        if not self.keep_selection:\n            Logger.debug(\"Board: deselecting \" + repr(self.app.selection))\n            if hasattr(self.app.selection, 'selected'):\n                self.app.selection.selected = False\n            self.app.selection = None\n        self.keep_selection = False\n        touch.ungrab(self)\n        touch.pop()\n        return", "response": "Delegate touch handling if possible else select something."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef on_parent(self, *args):\n        if not self.parent or hasattr(self, '_parented'):\n            return\n        if not self.wallpaper_path:\n            Logger.debug(\"Board: waiting for wallpaper_path\")\n            Clock.schedule_once(self.on_parent, 0)\n            return\n        self._parented = True\n        self.wallpaper = Image(source=self.wallpaper_path)\n        self.bind(wallpaper_path=self._pull_image)\n        self._pull_size()\n        self.kvlayoutback = KvLayoutBack(**self.widkwargs)\n        self.arrowlayout = ArrowLayout(**self.widkwargs)\n        self.spotlayout = FinalLayout(**self.widkwargs)\n        self.kvlayoutfront = KvLayoutFront(**self.widkwargs)\n        for wid in self.wids:\n            self.add_widget(wid)\n            wid.pos = 0, 0\n            wid.size = self.size\n            if wid is not self.wallpaper:\n                self.bind(size=wid.setter('size'))\n        self.update()", "response": "Create some subwidgets and trigger the first update."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef make_pawn(self, thing):\n        if thing[\"name\"] in self.pawn:\n            raise KeyError(\"Already have a Pawn for this Thing\")\n        r = self.pawn_cls(\n            board=self,\n            thing=thing\n        )\n        self.pawn[thing[\"name\"]] = r\n        return r", "response": "Make a Pawn to represent a Thing store it and return it."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmaking a : class:`Spot` to represent a Place store it and return it.", "response": "def make_spot(self, place):\n        \"\"\"Make a :class:`Spot` to represent a :class:`Place`, store it, and\n        return it.\n\n        \"\"\"\n        if place[\"name\"] in self.spot:\n            raise KeyError(\"Already have a Spot for this Place\")\n        r = self.spot_cls(\n            board=self,\n            place=place\n        )\n        self.spot[place[\"name\"]] = r\n        if '_x' in place and '_y' in place:\n            r.pos = (\n                self.width * place['_x'],\n                self.height * place['_y']\n            )\n        return r"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef make_arrow(self, portal):\n        if (\n                portal[\"origin\"] not in self.spot or\n                portal[\"destination\"] not in self.spot\n        ):\n            raise ValueError(\n                \"An :class:`Arrow` should only be made after \"\n                \"the :class:`Spot`s it connects\"\n            )\n        if (\n                portal[\"origin\"] in self.arrow and\n                portal[\"destination\"] in self.arrow[portal[\"origin\"]]\n        ):\n            raise KeyError(\"Already have an Arrow for this Portal\")\n        return self._core_make_arrow(portal, self.spot[portal['origin']], self.spot[portal['destination']], self.arrow)", "response": "Make an Arrow to represent a portal store it and return it."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving the Pawn by the given name.", "response": "def rm_pawn(self, name, *args):\n        \"\"\"Remove the :class:`Pawn` by the given name.\"\"\"\n        if name not in self.pawn:\n            raise KeyError(\"No Pawn named {}\".format(name))\n        # Currently there's no way to connect Pawns with Arrows but I\n        # think there will be, so, insurance\n        self.rm_arrows_to_and_from(name)\n        pwn = self.pawn.pop(name)\n        if pwn in self.selection_candidates:\n            self.selection_candidates.remove(pwn)\n        pwn.parent.remove_widget(pwn)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef rm_spot(self, name, *args):\n        if name not in self.spot:\n            raise KeyError(\"No Spot named {}\".format(name))\n        spot = self.spot.pop(name)\n        if spot in self.selection_candidates:\n            self.selection_candidates.remove(spot)\n        pawns_here = list(spot.children)\n        self.rm_arrows_to_and_from(name)\n        self.spotlayout.remove_widget(spot)\n        spot.canvas.clear()\n        for pawn in pawns_here:\n            self.rm_pawn(pawn.name)", "response": "Removes the Spot by the given name."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nremoves the Arrow that goes from orig to dest.", "response": "def rm_arrow(self, orig, dest, *args):\n        \"\"\"Remove the :class:`Arrow` that goes from ``orig`` to ``dest``.\"\"\"\n        if (\n                orig not in self.arrow or\n                dest not in self.arrow[orig]\n        ):\n            raise KeyError(\"No Arrow from {} to {}\".format(orig, dest))\n        arr = self.arrow[orig].pop(dest)\n        if arr in self.selection_candidates:\n            self.selection_candidates.remove(arr)\n        self.arrowlayout.remove_widget(arr)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update(self, *args):\n\n        # remove widgets that don't represent anything anymore\n        Logger.debug(\"Board: updating\")\n        self.remove_absent_pawns()\n        self.remove_absent_spots()\n        self.remove_absent_arrows()\n        # add widgets to represent new stuff\n        self.add_new_spots()\n        if self.arrow_cls:\n            self.add_new_arrows()\n        self.add_new_pawns()\n        self.spotlayout.finalize_all()\n        Logger.debug(\"Board: updated\")", "response": "Force an update of the current state of my character."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nupdates the board from the delta dict.", "response": "def update_from_delta(self, delta, *args):\n        \"\"\"Apply the changes described in the dict ``delta``.\"\"\"\n        for (node, extant) in delta.get('nodes', {}).items():\n            if extant:\n                if node in delta.get('node_val', {}) \\\n                        and 'location' in delta['node_val'][node]\\\n                        and node not in self.pawn:\n                    self.add_pawn(node)\n                elif node not in self.spot:\n                    self.add_spot(node)\n                    spot = self.spot[node]\n                    if '_x' not in spot.place or '_y' not in spot.place:\n                        self.spots_unposd.append(spot)\n            else:\n                if node in self.pawn:\n                    self.rm_pawn(node)\n                if node in self.spot:\n                    self.rm_spot(node)\n        for (node, stats) in delta.get('node_val', {}).items():\n            if node in self.spot:\n                spot = self.spot[node]\n                x = stats.get('_x')\n                y = stats.get('_y')\n                if x is not None:\n                    spot.x = x * self.width\n                if y is not None:\n                    spot.y = y * self.height\n                if '_image_paths' in stats:\n                    spot.paths = stats['_image_paths'] or spot.default_image_paths\n            elif node in self.pawn:\n                pawn = self.pawn[node]\n                if 'location' in stats:\n                    pawn.loc_name = stats['location']\n                if '_image_paths' in stats:\n                    pawn.paths = stats['_image_paths'] or pawn.default_image_paths\n            else:\n                Logger.warning(\n                    \"Board: diff tried to change stats of node {} \"\n                    \"but I don't have a widget for it\".format(node)\n                )\n        for (orig, dests) in delta.get('edges', {}).items():\n            for (dest, extant) in dests.items():\n                if extant and (orig not in self.arrow or dest not in self.arrow[orig]):\n                    self.add_arrow(orig, dest)\n                elif not extant and orig in self.arrow and dest in self.arrow[orig]:\n                    self.rm_arrow(orig, dest)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef arrows(self):\n        for o in self.arrow.values():\n            for arro in o.values():\n                yield arro", "response": "Iterate over all my arrows."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\niterating over pawns that collide the given point.", "response": "def pawns_at(self, x, y):\n        \"\"\"Iterate over pawns that collide the given point.\"\"\"\n        for pawn in self.pawn.values():\n            if pawn.collide_point(x, y):\n                yield pawn"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef spots_at(self, x, y):\n        for spot in self.spot.values():\n            if spot.collide_point(x, y):\n                yield spot", "response": "Iterate over spots that collide the given point."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\niterates over the arrows that collide the given point.", "response": "def arrows_at(self, x, y):\n        \"\"\"Iterate over arrows that collide the given point.\"\"\"\n        for arrow in self.arrows():\n            if arrow.collide_point(x, y):\n                yield arrow"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef spot_from_dummy(self, dummy):\n        (x, y) = self.to_local(*dummy.pos_up)\n        x /= self.board.width\n        y /= self.board.height\n        self.board.spotlayout.add_widget(\n            self.board.make_spot(\n                self.board.character.new_place(\n                    dummy.name,\n                    _x=x,\n                    _y=y,\n                    _image_paths=list(dummy.paths)\n                )\n            )\n        )\n        dummy.num += 1", "response": "Make a real place and its spot from a dummy spot."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmake a real thing and its pawn from a dummy pawn.", "response": "def pawn_from_dummy(self, dummy):\n        \"\"\"Make a real thing and its pawn from a dummy pawn.\n\n        Create a new :class:`board.Pawn` instance, along with the\n        underlying :class:`LiSE.Place` instance, and give it the name,\n        location, and imagery of the provided dummy.\n\n        \"\"\"\n        dummy.pos = self.to_local(*dummy.pos)\n        for spot in self.board.spotlayout.children:\n            if spot.collide_widget(dummy):\n                whereat = spot\n                break\n        else:\n            return\n        whereat.add_widget(\n            self.board.make_pawn(\n                self.board.character.new_thing(\n                    dummy.name,\n                    whereat.place.name,\n                    _image_paths=list(dummy.paths)\n                )\n            )\n        )\n        dummy.num += 1"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmake a real portal and its dummy arrow from a widget.", "response": "def arrow_from_wid(self, wid):\n        \"\"\"Make a real portal and its arrow from a dummy arrow.\n\n        This doesn't handle touch events. It takes a widget as its\n        argument: the one the user has been dragging to indicate where\n        they want the arrow to go. Said widget ought to be invisible.\n        It checks if the dummy arrow connects two real spots first,\n        and does nothing if it doesn't.\n\n        \"\"\"\n        for spot in self.board.spotlayout.children:\n            if spot.collide_widget(wid):\n                whereto = spot\n                break\n        else:\n            return\n        self.board.arrowlayout.add_widget(\n            self.board.make_arrow(\n                self.board.character.new_portal(\n                    self.board.grabbed.place.name,\n                    whereto.place.name,\n                    reciprocal=self.reciprocal_portal\n                )\n            )\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a sorted list of the contents of a set", "response": "def sort_set(s):\n    \"\"\"Return a sorted list of the contents of a set\n\n    This is intended to be used to iterate over world state, where you just need keys\n    to be in some deterministic order, but the sort order should be obvious from the key.\n\n    Non-strings come before strings and then tuples. Tuples compare element-wise as normal.\n    But ultimately all comparisons are between values' ``repr``.\n\n    This is memoized.\n\n    \"\"\"\n    if not isinstance(s, Set):\n        raise TypeError(\"sets only\")\n    s = frozenset(s)\n    if s not in _sort_set_memo:\n        _sort_set_memo[s] = sorted(s, key=_sort_set_key)\n    return _sort_set_memo[s]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef reciprocal(self):\n        try:\n            return self.character.portal[self.dest][self.orig]\n        except KeyError:\n            raise KeyError(\"This portal has no reciprocal\")", "response": "Returns the reciprocal portal that is connected to the same origin and\n        destination that I do."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nupdates the internal dictionary with the contents of the old value.", "response": "def update(self, d):\n        \"\"\"Works like regular update, but only actually updates when the new\n        value and the old value differ. This is necessary to prevent\n        certain infinite loops.\n\n        :arg d: a dictionary\n\n        \"\"\"\n        for (k, v) in d.items():\n            if k not in self or self[k] != v:\n                self[k] = v"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves myself from my character.", "response": "def delete(self):\n        \"\"\"Remove myself from my :class:`Character`.\n\n        For symmetry with :class:`Thing` and :class`Place`.\n\n        \"\"\"\n        branch, turn, tick = self.engine._nbtt()\n        self.engine._edges_cache.store(\n            self.character.name,\n            self.origin.name,\n            self.destination.name,\n            0,\n            branch,\n            turn,\n            tick,\n            None\n        )\n        self.engine.query.exist_edge(\n            self.character.name,\n            self.origin.name,\n            self.destination.name,\n            branch, turn, tick, False\n        )\n        try:\n            del self.engine._edge_objs[\n                (self.graph.name, self.orig, self.dest)\n            ]\n        except KeyError:\n            pass\n        self.character.portal[self.origin.name].send(\n            self.character.portal[self.origin.name],\n            key='dest', val=None\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef toggle_rules(self, *args):\n        if self.app.manager.current != 'rules' and not isinstance(self.app.selected_proxy, CharStatProxy):\n            self.app.rules.entity = self.app.selected_proxy\n            self.app.rules.rulebook = self.app.selected_proxy.rulebook\n        if isinstance(self.app.selected_proxy, CharStatProxy):\n            self.app.charrules.character = self.app.selected_proxy\n            self.app.charrules.toggle()\n        else:\n            self.app.rules.toggle()", "response": "Display or hide the view for constructing rules out of cards."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nshows the dialog where you select graphics and a name for a place and hide it if already showing.", "response": "def toggle_spot_cfg(self):\n        \"\"\"Show the dialog where you select graphics and a name for a place,\n        or hide it if already showing.\n\n        \"\"\"\n        if self.app.manager.current == 'spotcfg':\n            dummyplace = self.screendummyplace\n            self.ids.placetab.remove_widget(dummyplace)\n            dummyplace.clear()\n            if self.app.spotcfg.prefix:\n                dummyplace.prefix = self.app.spotcfg.prefix\n                dummyplace.num = dummynum(\n                    self.app.character, dummyplace.prefix\n                ) + 1\n            dummyplace.paths = self.app.spotcfg.imgpaths\n            self.ids.placetab.add_widget(dummyplace)\n        else:\n            self.app.spotcfg.prefix = self.ids.dummyplace.prefix\n        self.app.spotcfg.toggle()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nshowing or hide the pop - over where you can configure the dummy pawn", "response": "def toggle_pawn_cfg(self):\n        \"\"\"Show or hide the pop-over where you can configure the dummy pawn\"\"\"\n        if self.app.manager.current == 'pawncfg':\n            dummything = self.app.dummything\n            self.ids.thingtab.remove_widget(dummything)\n            dummything.clear()\n            if self.app.pawncfg.prefix:\n                dummything.prefix = self.app.pawncfg.prefix\n                dummything.num = dummynum(\n                    self.app.character, dummything.prefix\n                ) + 1\n            if self.app.pawncfg.imgpaths:\n                dummything.paths = self.app.pawncfg.imgpaths\n            else:\n                dummything.paths = ['atlas://rltiles/base/unseen']\n            self.ids.thingtab.add_widget(dummything)\n        else:\n            self.app.pawncfg.prefix = self.ids.dummything.prefix\n        self.app.pawncfg.toggle()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef toggle_reciprocal(self):\n        self.screen.boardview.reciprocal_portal = not self.screen.boardview.reciprocal_portal\n        if self.screen.boardview.reciprocal_portal:\n            assert(self.revarrow is None)\n            self.revarrow = ArrowWidget(\n                board=self.screen.boardview.board,\n                origin=self.ids.emptyright,\n                destination=self.ids.emptyleft\n            )\n            self.ids.portaladdbut.add_widget(self.revarrow)\n        else:\n            if hasattr(self, 'revarrow'):\n                self.ids.portaladdbut.remove_widget(self.revarrow)\n                self.revarrow = None", "response": "Flip my reciprocal_portal boolean and draw or stop drawing"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dict_delta(old, new):\n    r = {}\n    oldkeys = set(old.keys())\n    newkeys = set(new.keys())\n    r.update((k, new[k]) for k in newkeys.difference(oldkeys))\n    r.update((k, None) for k in oldkeys.difference(newkeys))\n    for k in oldkeys.intersection(newkeys):\n        if old[k] != new[k]:\n            r[k] = new[k]\n    return r", "response": "Return a dictionary containing the items of new that are either\n    absent from old or whose values are different."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a dictionary describing a character.", "response": "def character_copy(self, char):\n        \"\"\"Return a dictionary describing character ``char``.\"\"\"\n        ret = self.character_stat_copy(char)\n        chara = self._real.character[char]\n        nv = self.character_nodes_stat_copy(char)\n        if nv:\n            ret['node_val'] = nv\n        ev = self.character_portals_stat_copy(char)\n        if ev:\n            ret['edge_val'] = ev\n        avs = self.character_avatars_copy(char)\n        if avs:\n            ret['avatars'] = avs\n        rbs = self.character_rulebooks_copy(char)\n        if rbs:\n            ret['rulebooks'] = rbs\n        nrbs = self.character_nodes_rulebooks_copy(char)\n        if nrbs:\n            for node, rb in nrbs.items():\n                assert node in chara.node\n                if 'node_val' not in ret:\n                    ret['node_val'] = {}\n                nv = ret['node_val']\n                if node not in nv:\n                    nv[node] = {}\n                nv[node]['rulebook'] = rb\n        porbs = self.character_portals_rulebooks_copy(char)\n        if porbs:\n            for orig, dests in porbs.items():\n                ports = chara.portal[orig]\n                for dest, rb in dests.items():\n                    assert dest in ports\n                    if 'edge_val' not in ret:\n                        ret['edge_val'] = {}\n                    ev = ret['edge_val']\n                    if orig not in ev:\n                        ev[orig] = {}\n                    ov = ev[orig]\n                    if dest not in ov:\n                        ov[dest] = {}\n                    ov[dest]['rulebook'] = rb\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a dictionary of changes to char since previous call.", "response": "def character_delta(self, char, *, store=True):\n        \"\"\"Return a dictionary of changes to ``char`` since previous call.\"\"\"\n        ret = self.character_stat_delta(char, store=store)\n        nodes = self.character_nodes_delta(char, store=store)\n        chara = self._real.character[char]\n        if nodes:\n            ret['nodes'] = nodes\n        edges = self.character_portals_delta(char, store=store)\n        if edges:\n            ret['edges'] = edges\n        avs = self.character_avatars_delta(char, store=store)\n        if avs:\n            ret['avatars'] = avs\n        rbs = self.character_rulebooks_delta(char, store=store)\n        if rbs:\n            ret['rulebooks'] = rbs\n        nrbs = self.character_nodes_rulebooks_delta(char, store=store)\n        if nrbs:\n            for node, rb in nrbs.items():\n                if node not in chara.node:\n                    continue\n                ret.setdefault('node_val', {}).setdefault(node, {})['rulebook'] = rb\n        porbs = self.character_portals_rulebooks_delta(char, store=store)\n        if porbs:\n            for orig, dests in porbs.items():\n                if orig not in chara.portal:\n                    continue\n                portals = chara.portal[orig]\n                for dest, rb in dests.items():\n                    if dest not in portals:\n                        continue\n                    ret.setdefault('edge_val', {}).setdefault(orig, {}).setdefault(dest, {})['rulebook'] = rb\n        nv = self.character_nodes_stat_delta(char, store=store)\n        if nv:\n            ret['node_val'] = nv\n        ev = self.character_portals_stat_delta(char, store=store)\n        if ev:\n            ret['edge_val'] = ev\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a node s stats prepared for pickling in a dictionary.", "response": "def node_stat_copy(self, node_or_char, node=None):\n        \"\"\"Return a node's stats, prepared for pickling, in a dictionary.\"\"\"\n        if node is None:\n            node = node_or_char\n        else:\n            node = self._real.character[node_or_char].node[node]\n        return {\n            k: v.unwrap() if hasattr(v, 'unwrap') and not hasattr(v, 'no_unwrap') else v\n            for (k, v) in node.items() if k not in {\n                'character',\n                'name',\n                'arrival_time',\n                'next_arrival_time'\n            }\n        }"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef node_stat_delta(self, char, node, *, store=True):\n        try:\n            old = self._node_stat_cache[char].get(node, {})\n            new = self.node_stat_copy(self._real.character[char].node[node])\n            if store:\n                self._node_stat_cache[char][node] = new\n            r = dict_delta(old, new)\n            return r\n        except KeyError:\n            return None", "response": "Return a dictionary describing changes to a node s stats since the last time you looked at it."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef character_nodes_stat_delta(self, char, *, store=True):\n        r = {}\n        nodes = set(self._real.character[char].node.keys())\n        for node in nodes:\n            delta = self.node_stat_delta(char, node, store=store)\n            if delta:\n                r[node] = delta\n        nsc = self._node_stat_cache[char]\n        for node in list(nsc.keys()):\n            if node not in nodes:\n                del nsc[node]\n        return r", "response": "Return a dictionary of node_stat_delta output for each node in a character."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_node(self, char, node, patch):\n        character = self._real.character[char]\n        if patch is None:\n            del character.node[node]\n        elif node not in character.node:\n            character.node[node] = patch\n            return\n        else:\n            character.node[node].update(patch)", "response": "Update a node s stats according to a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update_nodes(self, char, patch, backdate=False):\n        if backdate:\n            parbranch, parrev = self._real._parentbranch_rev.get(\n                self._real.branch, ('trunk', 0)\n            )\n            tick_now = self._real.tick\n            self._real.tick = parrev\n        for i, (n, npatch) in enumerate(patch.items(), 1):\n            self.update_node(char, n, npatch)\n        if backdate:\n            self._real.tick = tick_now", "response": "Update the stats of nodes in a character according to a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef del_node(self, char, node):\n        del self._real.character[char].node[node]\n        for cache in (\n                self._char_nodes_rulebooks_cache,\n                self._node_stat_cache,\n                self._node_successors_cache\n        ):\n            try:\n                del cache[char][node]\n            except KeyError:\n                pass\n        if char in self._char_nodes_cache and node in self._char_nodes_cache[char]:\n            self._char_nodes_cache[char] = self._char_nodes_cache[char] - frozenset([node])\n        if char in self._portal_stat_cache:\n            portal_stat_cache_char = self._portal_stat_cache[char]\n            if node in portal_stat_cache_char:\n                del portal_stat_cache_char[node]\n            for charo in portal_stat_cache_char.values():\n                if node in charo:\n                    del charo[node]\n        if char in self._char_portals_rulebooks_cache:\n            portal_rulebook_cache_char = self._char_portals_rulebooks_cache[char]\n            if node in portal_rulebook_cache_char:\n                del portal_rulebook_cache_char[node]\n            for porto in portal_rulebook_cache_char.values():\n                if node in porto:\n                    del porto[node]", "response": "Remove a node from a character."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef thing_travel_to(self, char, thing, dest, weight=None, graph=None):\n        return self._real.character[char].thing[thing].travel_to(dest, weight, graph)", "response": "Make something find a path to dest follow it."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef remake(self, *args):\n        if not self.config:\n            return\n        if not all((self.key, self.gett, self.sett, self.listen, self.unlisten)):\n            Clock.schedule_once(self.remake, 0)\n            return\n        if not hasattr(self, 'label'):\n            self.label = Label(text=str(self.key))\n\n            def updlabel(*args):\n                self.label.text = str(self.key)\n            self.bind(key=updlabel)\n            self.add_widget(self.label)\n        if hasattr(self, 'wid'):\n            self.remove_widget(self.wid)\n            del self.wid\n        control = self.config['control']\n        widkwargs = {\n            'key': self.key,\n            'gett': self.gett,\n            'sett': self.sett,\n            'listen': self.listen,\n            'unlisten': self.unlisten\n        }\n        if control == 'slider':\n            cls = StatRowSlider\n            try:\n               widkwargs['value'] = float(self.gett(self.key))\n               widkwargs['min'] = float(self.config['min'])\n               widkwargs['max'] = float(self.config['max'])\n            except (KeyError, ValueError):\n                return\n        elif control == 'togglebutton':\n            cls = StatRowToggleButton\n            try:\n                widkwargs['true_text'] = self.config['true_text']\n                widkwargs['false_text'] = self.config['false_text']\n            except KeyError:\n                return\n        elif control == 'textinput':\n            cls = StatRowTextInput\n        else:\n            cls = StatRowLabel\n        self.wid = cls(**widkwargs)\n        self.bind(\n            key=self.wid.setter('key'),\n            gett=self.wid.setter('gett'),\n            sett=self.wid.setter('sett'),\n            listen=self.wid.setter('listen'),\n            unlisten=self.wid.setter('unlisten')\n        )\n        if control == 'slider':\n            self.unbind(config=self._toggle_update_config)\n            self.bind(config=self._slider_update_config)\n        elif control == 'togglebutton':\n            self.unbind(config=self._slider_update_config)\n            self.bind(config=self._toggle_update_config)\n        else:\n            self.unbind(config=self._slider_update_config)\n            self.unbind(config=self._toggle_update_config)\n        self.add_widget(self.wid)", "response": "Replace any existing child widget with the one described by my config."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef del_key(self, k):\n        if k not in self.mirror:\n            raise KeyError\n        del self.proxy[k]\n        if '_config' in self.proxy and k in self.proxy['_config']:\n            del self.proxy['_config'][k]", "response": "Delete the key and any configuration for it"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting a value on the proxy parsing it to a useful datatype if possible", "response": "def set_value(self, k, v):\n        \"\"\"Set a value on the proxy, parsing it to a useful datatype if possible\"\"\"\n        from ast import literal_eval\n        if self.engine is None or self.proxy is None:\n            self._trigger_set_value(k, v)\n            return\n        if v is None:\n            del self.proxy[k]\n        else:\n            try:\n                vv = literal_eval(v)\n            except (TypeError, ValueError):\n                vv = v\n            self.proxy[k] = vv"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets a configuration option for a key", "response": "def set_config(self, key, option, value):\n        \"\"\"Set a configuration option for a key\"\"\"\n        if '_config' not in self.proxy:\n            newopt = dict(default_cfg)\n            newopt[option] = value\n            self.proxy['_config'] = {key: newopt}\n        else:\n            if key in self.proxy['_config']:\n                self.proxy['_config'][key][option] = value\n            else:\n                newopt = dict(default_cfg)\n                newopt[option] = value\n                self.proxy['_config'][key] = newopt"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_configs(self, key, d):\n        if '_config' in self.proxy:\n            self.proxy['_config'][key] = d\n        else:\n            self.proxy['_config'] = {key: d}", "response": "Set the whole configuration for a key"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef iter_data(self):\n        for (k, v) in self.proxy.items():\n            if (\n                not (isinstance(k, str) and k[0] == '_') and\n                k not in (\n                    'character',\n                    'name',\n                    'location'\n                )\n            ):\n                yield k, v", "response": "Iterate over key - value pairs that are really meant to be displayed"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nturning a key and value into a dictionary describing a widget to show", "response": "def munge(self, k, v):\n        \"\"\"Turn a key and value into a dictionary describing a widget to show\"\"\"\n        if '_config' in self.proxy and k in self.proxy['_config']:\n            config = self.proxy['_config'][k].unwrap()\n        else:\n            config = default_cfg\n        return {\n            'key': k,\n            'reg': self._reg_widget,\n            'unreg': self._unreg_widget,\n            'gett': self.proxy.__getitem__,\n            'sett': self.set_value,\n            'listen': self.proxy.connect,\n            'unlisten': self.proxy.disconnect,\n            'config': config\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating to match new entity data", "response": "def upd_data(self, *args):\n        \"\"\"Update to match new entity data\"\"\"\n        data = [self.munge(k, v) for k, v in self.iter_data()]\n        self.data = sorted(data, key=lambda d: d['key'])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef wait_travel(self, character, thing, dest, cb=None):\n        self.disable_input()\n        self.app.wait_travel(character, thing, dest, cb=partial(self.enable_input, cb))", "response": "Schedule a thing to travel someplace then wait for it to finish."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef wait_turns(self, turns, cb=None):\n        self.disable_input()\n        self.app.wait_turns(turns, cb=partial(self.enable_input, cb))", "response": "Call self. app. engine. next_turn n times waiting self. app. turn_length in between\n        Disables input for the duration."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef wait_command(self, start_func, turns=1, end_func=None):\n        self.disable_input()\n        start_func()\n        self.app.wait_turns(turns, cb=partial(self.enable_input, end_func))", "response": "Call start_func and then wait turns for the duration of the command."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef wait_travel_command(self, character, thing, dest, start_func, turns=1, end_func=lambda: None):\n        self.disable_input()\n        self.app.wait_travel_command(character, thing, dest, start_func, turns, partial(self.enable_input, end_func))", "response": "Schedule a thing to travel someplace and do something then wait for it to finish."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef wait_turns(self, turns, dt=None, *, cb=None):\n        if turns == 0:\n            if cb:\n                cb()\n            return\n        self.engine.next_turn()\n        turns -= 1\n        Clock.schedule_once(partial(self.wait_turns, turns, cb=cb), self.turn_length)", "response": "Wait for a given number of turns in between\n       . If cb is provided call cb when done."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nscheduling a thing to travel someplace then wait for it to finish and call cb when I re done", "response": "def wait_travel(self, character, thing, dest, cb=None):\n        \"\"\"Schedule a thing to travel someplace, then wait for it to finish, and call ``cb`` if provided\n\n        :param character: name of the character\n        :param thing: name of the thing\n        :param dest: name of the destination (a place)\n        :param cb: function to be called when I'm done\n        :return: ``None``\n        \n        \"\"\"\n        self.wait_turns(self.engine.character[character].thing[thing].travel_to(dest), cb=cb)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncall start_func and wait to call end_func after simulating turns", "response": "def wait_command(self, start_func, turns=1, end_func=None):\n        \"\"\"Call ``start_func``, and wait to call ``end_func`` after simulating ``turns`` (default 1)\n\n        :param start_func: function to call before waiting\n        :param turns: number of turns to wait\n        :param end_func: function to call after waiting\n        :return: ``None``\n\n        \"\"\"\n        start_func()\n        self.wait_turns(turns, cb=end_func)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef wait_travel_command(self, character, thing, dest, start_func, turns=1, end_func=None):\n        self.wait_travel(character, thing, dest, cb=partial(\n            self.wait_command, start_func, turns, end_func)\n        )", "response": "Schedule a thing to travel someplace and do something then wait for it to finish."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef on_state(self, *args):\n        # This really ought to be done with the selection behavior\n        if self.state == 'down':\n            self.rulesview.rule = self.rule\n            for button in self.ruleslist.children[0].children:\n                if button != self:\n                    button.state = 'normal'", "response": "Unpress all other buttons in the ruleslist"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmaking sure to update when the rulebook changes", "response": "def on_rulebook(self, *args):\n        \"\"\"Make sure to update when the rulebook changes\"\"\"\n        if self.rulebook is None:\n            return\n        self.rulebook.connect(self._trigger_redata, weak=False)\n        self.redata()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef redata(self, *args):\n        if self.rulesview is None:\n            Clock.schedule_once(self.redata, 0)\n            return\n        data = [\n            {'rulesview': self.rulesview, 'rule': rule, 'index': i, 'ruleslist': self}\n            for i, rule in enumerate(self.rulebook)\n        ]\n        self.data = data", "response": "Make my data represent what s in my rulebook right now"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef on_rule(self, *args):\n        if self.rule is None:\n            return\n        self.rule.connect(self._listen_to_rule)", "response": "Make sure to update when the rule changes"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef finalize(self, *args):\n        if not self.canvas:\n            Clock.schedule_once(self.finalize, 0)\n            return\n\n        deck_builder_kwargs = {\n            'pos_hint': {'x': 0, 'y': 0},\n            'starting_pos_hint': {'x': 0.05, 'top': 0.95},\n            'card_size_hint': (0.3, 0.4),\n            'card_hint_step': (0, -0.1),\n            'deck_x_hint_step': 0.4\n        }\n\n        self._tabs = TabbedPanel(\n            size=self.size,\n            pos=self.pos,\n            do_default_tab=False\n        )\n        self.bind(\n            size=self._tabs.setter('size'),\n            pos=self._tabs.setter('pos')\n        )\n        self.add_widget(self._tabs)\n\n        for functyp in 'trigger', 'prereq', 'action':\n            tab = TabbedPanelItem(text=functyp.capitalize())\n            setattr(self, '_{}_tab'.format(functyp), tab)\n            self._tabs.add_widget(getattr(self, '_{}_tab'.format(functyp)))\n            builder = DeckBuilderView(**deck_builder_kwargs)\n            setattr(self, '_{}_builder'.format(functyp), builder)\n            builder.bind(decks=getattr(self, '_trigger_push_{}s'.format(functyp)))\n            scroll_left = DeckBuilderScrollBar(\n                size_hint_x=0.01,\n                pos_hint={'x': 0, 'y': 0},\n                deckbuilder=builder,\n                deckidx=0,\n                scroll_min=0\n            )\n            setattr(self, '_scroll_left_' + functyp, scroll_left)\n            scroll_right = DeckBuilderScrollBar(\n                size_hint_x=0.01,\n                pos_hint={'right': 1, 'y': 0},\n                deckbuilder=builder,\n                deckidx=1,\n                scroll_min=0\n            )\n            setattr(self, '_scroll_right_' + functyp, scroll_right)\n            layout = FloatLayout()\n            setattr(self, '_{}_layout'.format(functyp), layout)\n            tab.add_widget(layout)\n            layout.add_widget(builder)\n            layout.add_widget(scroll_left)\n            layout.add_widget(scroll_right)\n            layout.add_widget(\n                Label(\n                    text='Used',\n                    pos_hint={'center_x': 0.1, 'center_y': 0.98},\n                    size_hint=(None, None)\n                )\n            )\n            layout.add_widget(\n                Label(\n                    text='Unused',\n                    pos_hint={'center_x': 0.5, 'center_y': 0.98},\n                    size_hint=(None, None)\n                )\n            )\n            self.bind(rule=getattr(self, '_trigger_pull_{}s'.format(functyp)))", "response": "Add my tabs and the decks."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a pair of lists of Card widgets for used and unused functions.", "response": "def get_functions_cards(self, what, allfuncs):\n        \"\"\"Return a pair of lists of Card widgets for used and unused functions.\n\n        :param what: a string: 'trigger', 'prereq', or 'action'\n        :param allfuncs: a sequence of functions' (name, sourcecode, signature)\n\n        \"\"\"\n        if not self.rule:\n            return [], []\n        rulefuncnames = getattr(self.rule, what+'s')\n        unused = [\n            Card(\n                ud={\n                    'type': what,\n                    'funcname': name,\n                    'signature': sig\n                },\n                headline_text=name,\n                show_art=False,\n                midline_text=what.capitalize(),\n                text=source\n            )\n            for (name, source, sig) in allfuncs if name not in rulefuncnames\n        ]\n        used = [\n            Card(\n                ud={\n                    'type': what,\n                    'funcname': name,\n                },\n                headline_text=name,\n                show_art=False,\n                midline_text=what.capitalize(),\n                text=str(getattr(getattr(self.engine, what), name))\n            )\n            for name in rulefuncnames\n        ]\n        return used, unused"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_functions(self, what, allfuncs):\n        setattr(getattr(self, '_{}_builder'.format(what)), 'decks', self.get_functions_cards(what, allfuncs))", "response": "Set the cards in the what builder to allfuncs"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef inspect_func(self, namesrc):\n        (name, src) = namesrc\n        glbls = {}\n        lcls = {}\n        exec(src, glbls, lcls)\n        assert name in lcls\n        func = lcls[name]\n        return name, src, signature(func)", "response": "Take a function s name sourcecode and return a triple of name sourcecode signature"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _upd_unused(self, what):\n        builder = getattr(self, '_{}_builder'.format(what))\n        updtrig = getattr(self, '_trigger_upd_unused_{}s'.format(what))\n        builder.unbind(decks=updtrig)\n        funcs = OrderedDict()\n        cards = list(self._action_builder.decks[1])\n        cards.reverse()\n        for card in cards:\n            funcs[card.ud['funcname']] = card\n        for card in self._action_builder.decks[0]:\n            if card.ud['funcname'] not in funcs:\n                funcs[card.ud['funcname']] = card.copy()\n        unused = list(funcs.values())\n        unused.reverse()\n        builder.decks[1] = unused\n        builder.bind(decks=updtrig)", "response": "Make sure to have exactly one copy of every valid function in the\n        \"unused\" pile on the right."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninstall the n_creatures sickles and sickles in the system", "response": "def install(\n        engine,\n        n_creatures=5,\n        n_sickles=3,\n        malaria_chance=.05,\n        mate_chance=.05,\n        mapsize=(1, 1),\n        startpos=(0, 0)\n):\n    \"\"\"Natural Selection on Sickle Cell Anemia\n\n    If anyone carries a pair of sickle betaglobin genes, they die of\n    sickle cell anemia.\n\n    Individuals with 1x betaglobin, 1x sickle betaglobin are immune to\n    malaria.\n\n    \"\"\"\n    initmap = nx.grid_2d_graph(*mapsize)\n    phys = engine.new_character(\"physical\", data=initmap)\n    species = engine.new_character(\n        \"species\",\n        mate_chance=mate_chance,\n        malaria_chance=malaria_chance,\n        n_creatures=n_creatures,\n    )\n    for n in range(0, n_creatures):\n        name = \"critter\" + str(n)\n        phys.add_thing(\n            name=name,\n            location=startpos,\n            sickle_a=(n < n_sickles),\n            sickle_b=False,\n            male=engine.coinflip(),\n            last_mate_turn=-1\n        )\n        assert name in phys.thing\n        assert name not in phys.place\n        assert name in phys.node, \"couldn't add node {} to phys.node\".format(name)\n        assert hasattr(phys.node[name], 'location')\n        species.add_avatar(\"physical\", name)\n        assert hasattr(species.avatar['physical'][name], 'location')\n\n    # putting dieoff earlier in the code than mate means that dieoff will\n    # be followed before mate is\n    @species.avatar.rule\n    def dieoff(critter):\n        critter.delete()\n        assert (critter.name not in critter.character.node)\n        if critter['from_malaria']:\n            return 'malaria'\n        else:\n            return 'anemia'\n\n    @species.avatar.rule\n    def mate(critter):\n        \"\"\"If I share my location with another critter, attempt to mate\"\"\"\n        suitors = list(\n            oc for oc in critter.location.contents()\n            if oc['male'] != critter['male']\n        )\n        assert (len(suitors) > 0)\n        other_critter = critter.engine.choice(suitors)\n        sickles = [\n            critter['sickle_a'],\n            critter['sickle_b'],\n            other_critter['sickle_a'],\n            other_critter['sickle_b']\n        ]\n        engine.shuffle(sickles)\n        name = \"critter\" + str(species.stat[\"n_creatures\"])\n        species.stat[\"n_creatures\"] += 1\n        engine.character[\"physical\"].add_thing(\n            name,\n            critter[\"location\"],\n            sickle_a=sickles.pop(),\n            sickle_b=sickles.pop(),\n            male=engine.coinflip(),\n            last_mate_turn=engine.turn\n        )\n        species.add_avatar(\"physical\", name)\n        critter['last_mate_turn'] = other_critter['last_mate_turn'] = \\\n            engine.turn\n        return 'mated'\n\n    @mate.prereq\n    def once_per_turn(critter):\n        return critter['last_mate_turn'] < critter.engine.turn\n\n    @mate.prereq\n    def mate_present(critter):\n        for oc in critter.location.contents():\n            if oc['male'] != critter['male']:\n                return True\n        return False\n\n    @mate.trigger\n    def in_the_mood(critter):\n        return critter.engine.random() < critter.user.stat['mate_chance']\n\n    @dieoff.trigger\n    def sickle2(critter):\n        r = critter['sickle_a'] and critter['sickle_b']\n        if r:\n            critter['from_malaria'] = False\n        return r\n\n    @dieoff.trigger\n    def malaria(critter):\n        r = (\n                critter.engine.random() < critter.user.stat['malaria_chance'] and not\n        (critter['sickle_a'] or critter['sickle_b'])\n        )\n        if r:\n            critter['from_malaria'] = True\n        return r\n\n    # it would make more sense to keep using species.avatar.rule, this\n    # is just a test\n    @phys.thing.rule\n    def wander(critter):\n        dests = list(critter.character.place.keys())\n        dests.remove(critter['location'])\n        dest = critter.engine.choice(dests)\n        critter.travel_to(dest)\n\n    @wander.trigger\n    def not_travelling(critter):\n        return critter.next_location is None\n\n    @wander.prereq\n    def big_map(critter):\n        return len(critter.character.place) > 1"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _fun_names_iter(self, functyp, val):\n        funcstore = getattr(self.engine, functyp)\n        for v in val:\n            if callable(v):\n                # Overwrites anything already on the funcstore, is that bad?\n                setattr(funcstore, v.__name__, v)\n                yield v.__name__\n            elif v not in funcstore:\n                raise KeyError(\"Function {} not present in {}\".format(\n                    v, funcstore._tab\n                ))\n            else:\n                yield v", "response": "Iterate over the names of the functions in val adding them to funcstore if they are not already in funcstore."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef duplicate(self, newname):\n        if self.engine.rule.query.haverule(newname):\n            raise KeyError(\"Already have a rule called {}\".format(newname))\n        return Rule(\n            self.engine,\n            newname,\n            list(self.triggers),\n            list(self.prereqs),\n            list(self.actions)\n        )", "response": "Return a new rule that s just like this one but under a new name."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef new_empty(self, name):\n        if name in self:\n            raise KeyError(\"Already have rule {}\".format(name))\n        new = Rule(self.engine, name)\n        self._cache[name] = new\n        self.send(self, rule=new, active=True)\n        return new", "response": "Make a new empty rule with no actions or anything and return it."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a dictionary of all the tables that are used for LiSE. Use the meta object as the base for LiSE.", "response": "def tables_for_meta(meta):\n    \"\"\"Return a dictionary full of all the tables I need for LiSE. Use the\n    provided metadata object.\n\n    \"\"\"\n    allegedb.alchemy.tables_for_meta(meta)\n\n    # Table for global variables that are not sensitive to sim-time.\n    Table(\n        'universals', meta,\n        Column('key', TEXT, primary_key=True),\n        Column(\n            'branch', TEXT, primary_key=True, default='trunk'\n        ),\n        Column('turn', INT, primary_key=True, default=0),\n        Column('tick', INT, primary_key=True, default=0),\n        Column('value', TEXT)\n    )\n\n    Table(\n        'rules', meta,\n        Column('rule', TEXT, primary_key=True)\n    )\n\n    # Table grouping rules into lists called rulebooks.\n    Table(\n        'rulebooks', meta,\n        Column('rulebook', TEXT, primary_key=True),\n        Column('branch', TEXT, primary_key=True, default='trunk'),\n        Column('turn', INT, primary_key=True, default=0),\n        Column('tick', INT, primary_key=True, default=0),\n        Column('rules', TEXT, default='[]')\n    )\n\n    # Table for rules' triggers, those functions that return True only\n    # when their rule should run (or at least check its prereqs).\n    Table(\n        'rule_triggers', meta,\n        Column('rule', TEXT, primary_key=True),\n        Column('branch', TEXT, primary_key=True, default='trunk'),\n        Column('turn', INT, primary_key=True, default=0),\n        Column('tick', INT, primary_key=True, default=0),\n        Column('triggers', TEXT, default='[]'),\n        ForeignKeyConstraint(\n            ['rule'], ['rules.rule']\n        )\n    )\n\n    # Table for rules' prereqs, functions with veto power over a rule\n    # being followed\n    Table(\n        'rule_prereqs', meta,\n        Column('rule', TEXT, primary_key=True),\n        Column('branch', TEXT, primary_key=True, default='trunk'),\n        Column('turn', INT, primary_key=True, default=0),\n        Column('tick', INT, primary_key=True, default=0),\n        Column('prereqs', TEXT, default='[]'),\n        ForeignKeyConstraint(\n            ['rule'], ['rules.rule']\n        )\n    )\n\n    # Table for rules' actions, the functions that do what the rule\n    # does.\n    Table(\n        'rule_actions', meta,\n        Column('rule', TEXT, primary_key=True),\n        Column('branch', TEXT, primary_key=True, default='trunk'),\n        Column('turn', INT, primary_key=True, default=0),\n        Column('tick', INT, primary_key=True, default=0),\n        Column('actions', TEXT, default='[]'),\n        ForeignKeyConstraint(\n            ['rule'], ['rules.rule']\n        )\n    )\n\n    # The top level of the LiSE world model, the character. Includes\n    # rulebooks for the character itself, its avatars, and all the things,\n    # places, and portals it contains--though those may have their own\n    # rulebooks as well.\n\n    for name in (\n        'character_rulebook',\n        'avatar_rulebook',\n        'character_thing_rulebook',\n        'character_place_rulebook',\n        'character_portal_rulebook'\n    ):\n        Table(\n            name, meta,\n            Column('character', TEXT, primary_key=True),\n            Column('branch', TEXT, primary_key=True, default='trunk'),\n            Column('turn', INT, primary_key=True, default=0),\n            Column('tick', INT, primary_key=True, default=0),\n            Column('rulebook', TEXT),\n            ForeignKeyConstraint(\n                ['character'], ['graphs.graph']\n            ),\n            ForeignKeyConstraint(\n                ['rulebook'], ['rulebooks.rulebook']\n            )\n        )\n\n    # Rules handled within the rulebook associated with one node in\n    # particular.\n    nrh = Table(\n        'node_rules_handled', meta,\n        Column('character', TEXT, primary_key=True),\n        Column('node', TEXT, primary_key=True),\n        Column('rulebook', TEXT, primary_key=True),\n        Column('rule', TEXT, primary_key=True),\n        Column('branch', TEXT, primary_key=True, default='trunk'),\n        Column('turn', INT, primary_key=True, default=0),\n        Column('tick', INT),\n        ForeignKeyConstraint(\n            ['character', 'node'], ['nodes.graph', 'nodes.node']\n        )\n    )\n\n    Table(\n        'node_rules_changes', meta,\n        Column('character', TEXT, primary_key=True),\n        Column('node', TEXT, primary_key=True),\n        Column('rulebook', TEXT, primary_key=True),\n        Column('rule', TEXT, primary_key=True),\n        Column('branch', TEXT, primary_key=True),\n        Column('turn', INT, primary_key=True),\n        Column('tick', INT, primary_key=True),\n        Column('handled_branch', TEXT),\n        Column('handled_turn', INT),\n        ForeignKeyConstraint(\n            ['character', 'node', 'rulebook', 'rule', 'handled_branch', 'handled_turn'],\n            [nrh.c.character, nrh.c.node, nrh.c.rulebook, nrh.c.rule, nrh.c.branch, nrh.c.turn]\n        )\n    )\n\n    # Rules handled within the rulebook associated with one portal in\n    # particular.\n    porh = Table(\n        'portal_rules_handled', meta,\n        Column('character', TEXT, primary_key=True),\n        Column('orig', TEXT, primary_key=True),\n        Column('dest', TEXT, primary_key=True),\n        Column('rulebook', TEXT, primary_key=True),\n        Column('rule', TEXT, primary_key=True),\n        Column('branch', TEXT, primary_key=True, default='trunk'),\n        Column('turn', INT, primary_key=True, default=0),\n        Column('tick', INT),\n        ForeignKeyConstraint(\n            ['character', 'orig', 'dest'], ['edges.graph', 'edges.orig', 'edges.dest']\n        )\n    )\n\n    Table(\n        'portal_rules_changes', meta,\n        Column('character', TEXT, primary_key=True),\n        Column('orig', TEXT, primary_key=True),\n        Column('dest', TEXT, primary_key=True),\n        Column('rulebook', TEXT, primary_key=True),\n        Column('rule', TEXT, primary_key=True),\n        Column('branch', TEXT, primary_key=True),\n        Column('turn', INT, primary_key=True),\n        Column('tick', INT, primary_key=True),\n        Column('handled_branch', TEXT),\n        Column('handled_turn', INT),\n        ForeignKeyConstraint(\n            ['character', 'orig', 'dest', 'rulebook', 'rule', 'handled_branch', 'handled_turn'],\n            [porh.c.character, porh.c.orig, porh.c.dest, porh.c.rulebook, porh.c.rule, porh.c.branch, porh.c.turn]\n        )\n    )\n\n    # The function to use for a given sense.\n    #\n    # Characters use senses to look at other characters. To model this,\n    # sense functions are called with a facade representing the\n    # character under observation; the function munges this facade to\n    # make it look as it does through the sense in question, and returns\n    # that.\n    Table(\n        'senses', meta,\n        # null character field means all characters have this sense\n        Column(\n            'character', TEXT, primary_key=True, nullable=True\n        ),\n        Column('sense', TEXT, primary_key=True),\n        Column(\n            'branch', TEXT, primary_key=True, default='trunk'\n        ),\n        Column('turn', INT, primary_key=True, default=0),\n        Column('tick', INT, primary_key=True, default=0),\n        Column('function', TEXT, nullable=True),\n        ForeignKeyConstraint(['character'], ['graphs.graph'])\n    )\n\n    # Table for Things, being those nodes in a Character graph that have\n    # locations.\n    #\n    # A Thing's location can be either a Place or another Thing, as long\n    # as it's in the same Character.\n    Table(\n        'things', meta,\n        Column('character', TEXT, primary_key=True),\n        Column('thing', TEXT, primary_key=True),\n        Column(\n            'branch', TEXT, primary_key=True, default='trunk'\n        ),\n        Column('turn', INT, primary_key=True, default=0),\n        Column('tick', INT, primary_key=True, default=0),\n        # when location is null, this node is not a thing, but a place\n        Column('location', TEXT),\n        ForeignKeyConstraint(\n            ['character', 'thing'], ['nodes.graph', 'nodes.node']\n        ),\n        ForeignKeyConstraint(\n            ['character', 'location'], ['nodes.graph', 'nodes.node']\n        )\n    )\n\n    # The rulebook followed by a given node.\n    Table(\n        'node_rulebook', meta,\n        Column('character', TEXT, primary_key=True),\n        Column('node', TEXT, primary_key=True),\n        Column('branch', TEXT, primary_key=True, default='trunk'),\n        Column('turn', INT, primary_key=True, default=0),\n        Column('tick', INT, primary_key=True, default=0),\n        Column('rulebook', TEXT),\n        ForeignKeyConstraint(\n            ['character', 'node'], ['nodes.graph', 'nodes.node']\n        )\n    )\n\n    # The rulebook followed by a given Portal.\n    #\n    # \"Portal\" is LiSE's term for an edge in any of the directed\n    # graphs it uses. The name is different to distinguish them from\n    # Edge objects, which exist in an underlying object-relational\n    # mapper called allegedb, and have a different API.\n    Table(\n        'portal_rulebook', meta,\n        Column('character', TEXT, primary_key=True),\n        Column('orig', TEXT, primary_key=True),\n        Column('dest', TEXT, primary_key=True),\n        Column('branch', TEXT, primary_key=True, default='trunk'),\n        Column('turn', INT, primary_key=True, default=0),\n        Column('tick', INT, primary_key=True, default=0),\n        Column('rulebook', TEXT),\n        ForeignKeyConstraint(\n            ['character', 'orig', 'dest'],\n            ['edges.graph', 'edges.orig', 'edges.dest']\n        )\n    )\n\n    # The avatars representing one Character in another.\n    #\n    # In the common situation where a Character, let's say Alice has her\n    # own stats and skill tree and social graph, and also has a location\n    # in physical space, you can represent this by creating a Thing in\n    # the Character that represents physical space, and then making that\n    # Thing an avatar of Alice. On its own this doesn't do anything,\n    # it's just a convenient way of indicating the relation -- but if\n    # you like, you can make rules that affect all avatars of some\n    # Character, irrespective of what Character the avatar is actually\n    # *in*.\n    Table(\n        'avatars', meta,\n        Column('character_graph', TEXT, primary_key=True),\n        Column('avatar_graph', TEXT, primary_key=True),\n        Column('avatar_node', TEXT, primary_key=True),\n        Column(\n            'branch', TEXT, primary_key=True, default='trunk'\n        ),\n        Column('turn', INT, primary_key=True, default=0),\n        Column('tick', INT, primary_key=True, default=0),\n        Column('is_avatar', BOOLEAN),\n        ForeignKeyConstraint(['character_graph'], ['graphs.graph']),\n        ForeignKeyConstraint(\n            ['avatar_graph', 'avatar_node'],\n            ['nodes.graph', 'nodes.node']\n        )\n    )\n\n    crh = Table(\n        'character_rules_handled', meta,\n        Column('character', TEXT, primary_key=True),\n        Column('rulebook', TEXT, primary_key=True),\n        Column('rule', TEXT, primary_key=True),\n        Column('branch', TEXT, primary_key=True, default='trunk'),\n        Column('turn', INT, primary_key=True),\n        Column('tick', INT),\n        ForeignKeyConstraint(\n            ['character', 'rulebook'], ['character_rulebook.character', 'character_rulebook.rulebook']\n        )\n    )\n\n    Table(\n        'character_rules_changes', meta,\n        Column('character', TEXT, primary_key=True),\n        Column('rulebook', TEXT, primary_key=True),\n        Column('rule', TEXT, primary_key=True),\n        Column('branch', TEXT, primary_key=True),\n        Column('turn', INT, primary_key=True),\n        Column('tick', INT, primary_key=True),\n        Column('handled_branch', TEXT),\n        Column('handled_turn', TEXT),\n        ForeignKeyConstraint(\n            ['character', 'rulebook', 'rule', 'handled_branch', 'handled_turn'],\n            [crh.c.character, crh.c.rulebook, crh.c.rule, crh.c.branch, crh.c.turn]\n        )\n    )\n\n    arh = Table(\n        'avatar_rules_handled', meta,\n        Column('character', TEXT, primary_key=True),\n        Column('rulebook', TEXT, primary_key=True),\n        Column('rule', TEXT, primary_key=True),\n        Column('graph', TEXT, primary_key=True),\n        Column('avatar', TEXT, primary_key=True),\n        Column('branch', TEXT, primary_key=True, default='trunk'),\n        Column('turn', INT, primary_key=True),\n        Column('tick', INT),\n        ForeignKeyConstraint(\n            ['character', 'rulebook'], ['avatar_rulebook.character', 'avatar_rulebook.rulebook']\n        )\n    )\n\n    Table(\n        'avatar_rules_changes', meta,\n        Column('character', TEXT, primary_key=True),\n        Column('rulebook', TEXT, primary_key=True),\n        Column('rule', TEXT, primary_key=True),\n        Column('graph', TEXT, primary_key=True),\n        Column('avatar', TEXT, primary_key=True),\n        Column('branch', TEXT, primary_key=True),\n        Column('turn', INT, primary_key=True),\n        Column('tick', INT, primary_key=True),\n        Column('handled_branch', TEXT),\n        Column('handled_turn', TEXT),\n        ForeignKeyConstraint(\n            ['character', 'rulebook', 'rule', 'graph', 'avatar', 'handled_branch', 'handled_turn'],\n            [arh.c.character, arh.c.rulebook, arh.c.rule, arh.c.graph, arh.c.avatar, arh.c.branch, arh.c.turn]\n        )\n    )\n\n    ctrh = Table(\n        'character_thing_rules_handled', meta,\n        Column('character', TEXT, primary_key=True),\n        Column('rulebook', TEXT, primary_key=True),\n        Column('rule', TEXT, primary_key=True),\n        Column('thing', TEXT, primary_key=True),\n        Column('branch', TEXT, primary_key=True, default='trunk'),\n        Column('turn', INT, primary_key=True),\n        Column('tick', INT),\n        ForeignKeyConstraint(\n            ['character', 'rulebook'], ['character_thing_rulebook.character', 'character_thing_rulebook.rulebook']\n        ),\n        ForeignKeyConstraint(\n            ['character', 'thing'], ['things.character', 'things.thing']\n        )\n    )\n\n    Table(\n        'character_thing_rules_changes', meta,\n        Column('character', TEXT, primary_key=True),\n        Column('rulebook', TEXT, primary_key=True),\n        Column('rule', TEXT, primary_key=True),\n        Column('thing', TEXT, primary_key=True),\n        Column('branch', TEXT, primary_key=True),\n        Column('turn', INT, primary_key=True),\n        Column('tick', INT, primary_key=True),\n        Column('handled_branch', TEXT),\n        Column('handled_turn', INT),\n        ForeignKeyConstraint(\n            ['character', 'rulebook', 'rule', 'thing', 'handled_branch', 'handled_turn'],\n            [ctrh.c.character, ctrh.c.rulebook, ctrh.c.rule, ctrh.c.thing, ctrh.c.branch, ctrh.c.turn]\n        )\n    )\n\n    cprh = Table(\n        'character_place_rules_handled', meta,\n        Column('character', TEXT, primary_key=True),\n        Column('rulebook', TEXT, primary_key=True),\n        Column('rule', TEXT, primary_key=True),\n        Column('place', TEXT, primary_key=True),\n        Column('branch', TEXT, primary_key=True, default='trunk'),\n        Column('turn', INT, primary_key=True),\n        Column('tick', INT),\n        ForeignKeyConstraint(\n            ['character', 'rulebook'], ['character_place_rulebook.character', 'character_place_rulebook.rulebook']\n        ),\n        ForeignKeyConstraint(\n            ['character', 'place'], ['nodes.graph', 'nodes.node']\n        )\n    )\n\n    Table(\n        'character_place_rules_changes', meta,\n        Column('character', TEXT, primary_key=True),\n        Column('rulebook', TEXT, primary_key=True),\n        Column('rule', TEXT, primary_key=True),\n        Column('place', TEXT, primary_key=True),\n        Column('branch', TEXT, primary_key=True),\n        Column('turn', INT, primary_key=True),\n        Column('tick', INT, primary_key=True),\n        Column('handled_branch', TEXT),\n        Column('handled_turn', INT),\n        ForeignKeyConstraint(\n            ['character', 'rulebook', 'rule', 'place', 'handled_branch', 'handled_turn'],\n            [cprh.c.character, cprh.c.rulebook, cprh.c.rule, cprh.c.place, cprh.c.branch, cprh.c.turn]\n        )\n    )\n\n    cporh = Table(\n        'character_portal_rules_handled', meta,\n        Column('character', TEXT, primary_key=True),\n        Column('rulebook', TEXT, primary_key=True),\n        Column('rule', TEXT, primary_key=True),\n        Column('orig', TEXT, primary_key=True),\n        Column('dest', TEXT, primary_key=True),\n        Column('branch', TEXT, primary_key=True, default='trunk'),\n        Column('turn', INT, primary_key=True),\n        Column('tick', INT),\n        ForeignKeyConstraint(\n            ['character', 'rulebook'], ['character_portal_rulebook.character', 'character_portal_rulebook.rulebook']\n        ),\n        ForeignKeyConstraint(\n            ['character', 'orig', 'dest'], ['edges.graph', 'edges.orig', 'edges.dest']\n        )\n    )\n\n    Table(\n        'character_portal_rules_changes', meta,\n        Column('character', TEXT, primary_key=True),\n        Column('rulebook', TEXT, primary_key=True),\n        Column('rule', TEXT, primary_key=True),\n        Column('orig', TEXT, primary_key=True),\n        Column('dest', TEXT, primary_key=True),\n        Column('branch', TEXT, primary_key=True),\n        Column('turn', INT, primary_key=True),\n        Column('tick', INT, primary_key=True),\n        Column('handled_branch', TEXT),\n        Column('handled_turn', INT),\n        ForeignKeyConstraint(\n            ['character', 'rulebook', 'rule', 'orig', 'dest', 'handled_branch', 'handled_turn'],\n            [cporh.c.character, cporh.c.rulebook, cporh.c.rule, cporh.c.orig, cporh.c.dest, cporh.c.branch, cporh.c.turn]\n        )\n    )\n\n    Table(\n        'turns_completed', meta,\n        Column('branch', TEXT, primary_key=True),\n        Column('turn', INT)\n    )\n\n    return meta.tables"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef queries(table):\n    def update_where(updcols, wherecols):\n        \"\"\"Return an ``UPDATE`` statement that updates the columns ``updcols``\n        when the ``wherecols`` match. Every column has a bound parameter of\n        the same name.\n\n        updcols are strings, wherecols are column objects\n\n        \"\"\"\n        vmap = OrderedDict()\n        for col in updcols:\n            vmap[col] = bindparam(col)\n        wheres = [\n            c == bindparam(c.name) for c in wherecols\n        ]\n        tab = wherecols[0].table\n        return tab.update().values(**vmap).where(and_(*wheres))\n\n    r = allegedb.alchemy.queries_for_table_dict(table)\n\n    rulebooks = table['rulebooks']\n    r['rulebooks_update'] = update_where(['rules'], [rulebooks.c.rulebook, rulebooks.c.branch, rulebooks.c.turn, rulebooks.c.tick])\n\n    for t in table.values():\n        key = list(t.primary_key)\n        if 'branch' in t.columns and 'turn' in t.columns and 'tick' in t.columns:\n            branch = t.columns['branch']\n            turn = t.columns['turn']\n            tick = t.columns['tick']\n            if branch in key and turn in key and tick in key:\n                key = [branch, turn, tick]\n        r[t.name + '_dump'] = select(list(t.c.values())).order_by(*key)\n        r[t.name + '_insert'] = t.insert().values(tuple(bindparam(cname) for cname in t.c.keys()))\n        r[t.name + '_count'] = select([func.COUNT('*')]).select_from(t)\n\n    r['del_char_things'] = table['things'].delete().where(\n        table['things'].c.character == bindparam('character')\n    )\n\n    r['del_char_avatars'] = table['avatars'].delete().where(\n        table['avatars'].c.character_graph == bindparam('character')\n    )\n    things = table['things']\n    r['del_things_after'] = things.delete().where(and_(\n        things.c.character == bindparam('character'),\n        things.c.thing == bindparam('thing'),\n        things.c.branch == bindparam('branch'),\n        or_(\n            things.c.turn > bindparam('turn'),\n            and_(\n                things.c.turn == bindparam('turn'),\n                things.c.tick >= bindparam('tick')\n            )\n        )\n    ))\n    avatars = table['avatars']\n    r['del_avatars_after'] = avatars.delete().where(and_(\n        avatars.c.character_graph == bindparam('character'),\n        avatars.c.avatar_graph == bindparam('graph'),\n        avatars.c.avatar_node == bindparam('avatar'),\n        avatars.c.branch == bindparam('branch'),\n        or_(\n            avatars.c.turn > bindparam('turn'),\n            and_(\n                avatars.c.turn == bindparam('turn'),\n                avatars.c.tick >= bindparam('tick')\n            )\n        )\n    ))\n\n    branches = table['branches']\n\n    r['branch_children'] = select(\n        [branches.c.branch]\n    ).where(\n        branches.c.parent == bindparam('branch')\n    )\n\n    tc = table['turns_completed']\n    r['turns_completed_update'] = update_where(['turn'], [tc.c.branch])\n\n    return r", "response": "Returns a dictionary of SQL queries that can be used to query the database for all the queries I need."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef windows_union(windows):\n    def fix_overlap(left, right):\n        if left == right:\n            return [left]\n        assert left[0] < right[0]\n        if left[1] >= right[0]:\n            if right[1] > left[1]:\n                return [(left[0], right[1])]\n            else:\n                return [left]\n        return [left, right]\n\n    if len(windows) == 1:\n        return windows\n    none_left = []\n    none_right = []\n    otherwise = []\n    for window in windows:\n        if window[0] is None:\n            none_left.append(window)\n        elif window[1] is None:\n            none_right.append(window)\n        else:\n            otherwise.append(window)\n\n    res = []\n    otherwise.sort()\n    for window in none_left:\n        if not res:\n            res.append(window)\n            continue\n        res.extend(fix_overlap(res.pop(), window))\n    while otherwise:\n        window = otherwise.pop(0)\n        if not res:\n            res.append(window)\n            continue\n        res.extend(fix_overlap(res.pop(), window))\n    for window in none_right:\n        if not res:\n            res.append(window)\n            continue\n        res.extend(fix_overlap(res.pop(), window))\n    return res", "response": "Given a list of windows returns a minimal version that contains the same ranges."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngives a list of windows return a list of where they overlap.", "response": "def windows_intersection(windows):\n    \"\"\"Given a list of (beginning, ending), return another describing where they overlap.\n\n    :rtype: list\n    \"\"\"\n\n    def intersect2(left, right):\n        if left == right:\n            return left\n        elif left is (None, None):\n            return right\n        elif right is (None, None):\n            return left\n        elif left[0] is None:\n            if right[0] is None:\n                return None, min((left[1], right[1]))\n            elif right[1] is None:\n                if left[1] <= right[0]:\n                    return left[1], right[0]\n                else:\n                    return None\n            elif right[0] <= left[1]:\n                return right[0], left[1]\n            else:\n                return None\n        elif left[1] is None:\n            if right[0] is None:\n                return left[0], right[1]\n            else:\n                return right  # assumes left[0] <= right[0]\n        # None not in left\n        elif right[0] is None:\n            return left[0], min((left[1], right[1]))\n        elif right[1] is None:\n            if left[1] >= right[0]:\n                return right[0], left[1]\n            else:\n                return None\n        assert None not in left and None not in right and left[0] < right[1]\n        if left[1] >= right[0]:\n            if right[1] > left[1]:\n                return right[0], left[1]\n            else:\n                return right\n        return None\n\n    if len(windows) == 1:\n        return windows\n    left_none = []\n    right_none = []\n    otherwise = []\n    for window in windows:\n        assert window is not None, None\n        if window[0] is None:\n            left_none.append(window)\n        elif window[1] is None:\n            right_none.append(window)\n        else:\n            otherwise.append(window)\n\n    done = []\n    todo = left_none + sorted(otherwise)\n    for window in todo:\n        if not done:\n            done.append(window)\n            continue\n        res = intersect2(done.pop(), window)\n        if res:\n            done.append(res)\n    return done"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\niterate over all turns on which a comparison holds.", "response": "def slow_iter_turns_eval_cmp(qry, oper, start_branch=None, engine=None):\n    \"\"\"Iterate over all turns on which a comparison holds.\n\n    This is expensive. It evaluates the query for every turn in history.\n\n    \"\"\"\n    def mungeside(side):\n        if isinstance(side, Query):\n            return side.iter_turns\n        elif isinstance(side, StatusAlias):\n            return EntityStatAccessor(\n                side.entity, side.stat, side.engine,\n                side.branch, side.turn, side.tick, side.current, side.mungers\n            )\n        elif isinstance(side, EntityStatAccessor):\n            return side\n        else:\n            return lambda: side\n    leftside = mungeside(qry.leftside)\n    rightside = mungeside(qry.rightside)\n    engine = engine or leftside.engine or rightside.engine\n\n    for (branch, _, _) in engine._iter_parent_btt(start_branch or engine.branch):\n        if branch is None:\n            return\n        parent, turn_start, tick_start, turn_end, tick_end = engine._branches[branch]\n        for turn in range(turn_start, engine.turn + 1):\n            if oper(leftside(branch, turn), rightside(branch, turn)):\n                yield branch, turn"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef on_enter(self, *args):\n        if self.text == '':\n            return\n        self.setter(self.text)\n        self.text = ''\n        self.focus = False", "response": "Call the setter and blank myself out so that everything s\n        working."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\napply the function to myself and return myself.", "response": "def do(self, func, *args, **kwargs):\n        \"\"\"Apply the function to myself, and return myself.\n\n        Look up the function in the database if needed. Pass it any\n        arguments given, keyword or positional.\n\n        Useful chiefly when chaining.\n\n        \"\"\"\n        if not callable(func):\n            func = getattr(self.engine.function, func)\n        func(self, *args, **kwargs)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\napply Perlin noise to my nodes and return myself.", "response": "def perlin(self, stat='perlin'):\n        \"\"\"Apply Perlin noise to my nodes, and return myself.\n\n        I'll try to use the name of the node as its spatial position\n        for this purpose, or use its stats 'x', 'y', and 'z', or skip\n        the node if neither are available. z is assumed 0 if not\n        provided for a node.\n\n        Result will be stored in a node stat named 'perlin' by default.\n        Supply the name of another stat to use it instead.\n\n        \"\"\"\n        from math import floor\n        p = self.engine.shuffle([\n            151, 160, 137, 91, 90, 15, 131, 13, 201, 95, 96, 53, 194, 233, 7,\n            225, 140, 36, 103, 30, 69, 142, 8, 99, 37, 240, 21, 10, 23, 190,\n            6, 148, 247, 120, 234, 75, 0, 26, 197, 62, 94, 252, 219, 203, 117,\n            35, 11, 32, 57, 177, 33, 88, 237, 149, 56, 87, 174, 20, 125, 136,\n            171, 168, 68, 175, 74, 165, 71, 134, 139, 48, 27, 166, 77, 146,\n            158, 231, 83, 111, 229, 122, 60, 211, 133, 230, 220, 105, 92, 41,\n            55, 46, 245, 40, 244, 102, 143, 54, 65, 25, 63, 161, 1, 216, 80,\n            73, 209, 76, 132, 187, 208, 89, 18, 169, 200, 196, 135, 130, 116,\n            188, 159, 86, 164, 100, 109, 198, 173, 186, 3, 64, 52, 217, 226,\n            250, 124, 123, 5, 202, 38, 147, 118, 126, 255, 82, 85, 212, 207,\n            206, 59, 227, 47, 16, 58, 17, 182, 189, 28, 42, 223, 183, 170, 213,\n            119, 248, 152, 2, 44, 154, 163, 70, 221, 153, 101, 155, 167, 43,\n            172, 9, 129, 22, 39, 253, 19, 98, 108, 110, 79, 113, 224, 232, 178,\n            185, 112, 104, 218, 246, 97, 228, 251, 34, 242, 193, 238, 210, 144,\n            12, 191, 179, 162, 241, 81, 51, 145, 235, 249, 14, 239, 107, 49,\n            192, 214, 31, 181, 199, 106, 157, 184, 84, 204, 176, 115, 121, 50,\n            45, 127, 4, 150, 254, 138, 236, 205, 93, 222, 114, 67, 29, 24, 72,\n            243, 141, 128, 195, 78, 66, 215, 61, 156, 180\n        ]) * 2\n\n        def fade(t):\n            return t * t * t * (t * (t * 6 - 15) + 10)\n\n        def lerp(t, a, b):\n            return a + t * (b - a)\n\n        def grad(hsh, x, y, z):\n            \"\"\"CONVERT LO 4 BITS OF HASH CODE INTO 12 GRADIENT DIRECTIONS.\"\"\"\n            h = hsh & 15\n            u = x if h < 8 else y\n            v = y if h < 4 else x if h == 12 or h == 14 else z\n            return (u if h & 1 == 0 else -u) + (v if h & 2 == 0 else -v)\n\n        def noise(x, y, z):\n            # FIND UNIT CUBE THAT CONTAINS POINT.\n            X = int(x) & 255\n            Y = int(y) & 255\n            Z = int(z) & 255\n            # FIND RELATIVE X, Y, Z OF POINT IN CUBE.\n            x -= floor(x)\n            y -= floor(y)\n            z -= floor(z)\n            # COMPUTE FADE CURVES FOR EACH OF X, Y, Z.\n            u = fade(x)\n            v = fade(y)\n            w = fade(z)\n            # HASH COORDINATES OF THE 8 CUBE CORNERS,\n            A = p[X] + Y\n            AA = p[A] + Z\n            AB = p[A+1] + Z\n            B = p[X+1] + y\n            BA = p[B] + Z\n            BB = p[B+1] + Z\n            # AND ADD BLENDED RESULTS FROM 8 CORNERS OF CUBE\n            return lerp(\n                w,\n                lerp(\n                    v,\n                    lerp(\n                        u,\n                        grad(p[AA], x, y, z),\n                        grad(p[BA], x-1, y, z)\n                    ),\n                    lerp(\n                        u,\n                        grad(p[AB], x, y-1, z),\n                        grad(p[BB], x-1, y-1, z)\n                    )\n                ),\n                lerp(\n                    v,\n                    lerp(\n                        u,\n                        grad(p[AA+1], x, y, z-1),\n                        grad(p[BA+1], x-1, y, z-1)\n                    ),\n                    lerp(\n                        u,\n                        grad(p[AB+1], x, y-1, z-1),\n                        grad(p[BB+1], x-1, y-1, z-1)\n                    )\n                )\n            )\n\n        for node in self.node.values():\n            try:\n                (x, y, z) = node.name\n            except ValueError:\n                try:\n                    (x, y) = node.name\n                    z = 0.0\n                except ValueError:\n                    try:\n                        x = node['x']\n                        y = node['y']\n                        z = node.get('z', 0.0)\n                    except KeyError:\n                        continue\n            x, y, z = map(float, (x, y, z))\n            node[stat] = noise(x, y, z)\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncopy all nodes and edges from the given graph into this.", "response": "def copy_from(self, g):\n        \"\"\"Copy all nodes and edges from the given graph into this.\n\n        Return myself.\n\n        \"\"\"\n        renamed = {}\n        for k, v in g.node.items():\n            ok = k\n            if k in self.place:\n                n = 0\n                while k in self.place:\n                    k = ok + (n,) if isinstance(ok, tuple) else (ok, n)\n                    n += 1\n            renamed[ok] = k\n            self.place[k] = v\n        if type(g) is nx.MultiDiGraph:\n            g = nx.DiGraph(g)\n        elif type(g) is nx.MultiGraph:\n            g = nx.Graph(g)\n        if type(g) is nx.DiGraph:\n            for u, v in g.edges:\n                self.edge[renamed[u]][renamed[v]] = g.adj[u][v]\n        else:\n            assert type(g) is nx.Graph\n            for u, v, d in g.edges.data():\n                self.add_portal(renamed[u], renamed[v], symmetrical=True, **d)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef grid_2d_8graph(self, m, n):\n        me = nx.Graph()\n        node = me.node\n        add_node = me.add_node\n        add_edge = me.add_edge\n        for i in range(m):\n            for j in range(n):\n                add_node((i, j))\n                if i > 0:\n                    add_edge((i, j), (i-1, j))\n                    if j > 0:\n                        add_edge((i, j), (i-1, j-1))\n                if j > 0:\n                    add_edge((i, j), (i, j-1))\n                if (i - 1, j + 1) in node:\n                    add_edge((i, j), (i-1, j+1))\n        return self.copy_from(me)", "response": "Make a 2d graph that s connected 8 ways enabling diagonal movement"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndeleting nodes whose stat < threshold.", "response": "def cull_nodes(self, stat, threshold=0.5, comparator=ge):\n        \"\"\"Delete nodes whose stat >= ``threshold`` (default 0.5).\n\n        Optional argument ``comparator`` will replace >= as the test\n        for whether to cull. You can use the name of a stored function.\n\n        \"\"\"\n        comparator = self._lookup_comparator(comparator)\n        dead = [\n            name for name, node in self.node.items()\n            if stat in node and comparator(node[stat], threshold)\n        ]\n        self.remove_nodes_from(dead)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndelete all portals whose stat < threshold.", "response": "def cull_portals(self, stat, threshold=0.5, comparator=ge):\n        \"\"\"Delete portals whose stat >= ``threshold`` (default 0.5).\n\n        Optional argument ``comparator`` will replace >= as the test\n        for whether to cull. You can use the name of a stored function.\n\n        \"\"\"\n        comparator = self._lookup_comparator(comparator)\n        dead = []\n        for u in self.portal:\n            for v in self.portal[u]:\n                if stat in self.portal[u][v] and comparator(\n                        self.portal[u][v][stat], threshold\n                ):\n                    dead.append((u, v))\n        self.remove_edges_from(dead)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndeletes edges whose stat > = threshold.", "response": "def cull_edges(self, stat, threshold=0.5, comparator=ge):\n        \"\"\"Delete edges whose stat >= ``threshold`` (default 0.5).\n\n        Optional argument ``comparator`` will replace >= as the test\n        for whether to cull. You can use the name of a stored function.\n\n        \"\"\"\n        return self.cull_portals(stat, threshold, comparator)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef func(self):\n        fn = self.engine.query.sense_func_get(\n            self.observer.name,\n            self.sensename,\n            *self.engine._btt()\n        )\n        if fn is not None:\n            return SenseFuncWrap(self.observer, fn)", "response": "Return the function most recently associated with this sense."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_thing(self, name, location, **kwargs):\n        if name in self.thing:\n            raise WorldIntegrityError(\n                \"Already have a Thing named {}\".format(name)\n            )\n        self.add_node(name, **kwargs)\n        if isinstance(location, Node):\n            location = location.name\n        self.place2thing(name, location,)", "response": "Create a Thing and set its location and initial attributes from the keyword arguments."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nturn a Place into a Thing with the given location.", "response": "def place2thing(self, name, location):\n        \"\"\"Turn a Place into a Thing with the given location.\n        \n        It will keep all its attached Portals.\n\n        \"\"\"\n        self.engine._set_thing_loc(\n            self.name, name, location\n        )\n        if (self.name, name) in self.engine._node_objs:\n            obj = self.engine._node_objs[self.name, name]\n            thing = Thing(self, name)\n            for port in obj.portals():\n                port.origin = thing\n            for port in obj.preportals():\n                port.destination = thing\n            self.engine._node_objs[self.name, name] = thing"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nturn a Thing s location into a Place.", "response": "def thing2place(self, name):\n        \"\"\"Unset a Thing's location, and thus turn it into a Place.\"\"\"\n        self.engine._set_thing_loc(\n            self.name, name, None\n        )\n        if (self.name, name) in self.engine._node_objs:\n            thing = self.engine._node_objs[self.name, name]\n            place = Place(self, name)\n            for port in thing.portals():\n                port.origin = place\n            for port in thing.preportals():\n                port.destination = place\n            self.engine._node_objs[self.name, name] = place"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_portal(self, origin, destination, symmetrical=False, **kwargs):\n        if isinstance(origin, Node):\n            origin = origin.name\n        if isinstance(destination, Node):\n            destination = destination.name\n        super().add_edge(origin, destination, **kwargs)\n        if symmetrical:\n            self.add_portal(destination, origin, is_mirror=True)", "response": "Connect the origin to the destination with a Portal."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a sequence of tuples containing origin and destination portal names.", "response": "def add_portals_from(self, seq, symmetrical=False):\n        \"\"\"Take a sequence of (origin, destination) pairs and make a\n        :class:`Portal` for each.\n\n        Actually, triples are acceptable too, in which case the third\n        item is a dictionary of stats for the new :class:`Portal`.\n\n        If optional argument ``symmetrical`` is set to ``True``, all\n        the :class:`Portal` instances will have a mirror portal going\n        in the opposite direction, which will always have the same\n        stats.\n\n        \"\"\"\n        for tup in seq:\n            orig = tup[0]\n            dest = tup[1]\n            kwargs = tup[2] if len(tup) > 2 else {}\n            if symmetrical:\n                kwargs['symmetrical'] = True\n            self.add_portal(orig, dest, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_avatar(self, a, b=None):\n        if self.engine._planning:\n            raise NotImplementedError(\"Currently can't add avatars within a plan\")\n        if b is None:\n            if not (\n                    isinstance(a, Place) or\n                    isinstance(a, Thing)\n            ):\n                raise TypeError(\n                    'when called with one argument, '\n                    'it must be a place or thing'\n                )\n            g = a.character.name\n            n = a.name\n        else:\n            if isinstance(a, Character):\n                g = a.name\n            elif not isinstance(a, str):\n                raise TypeError(\n                    'when called with two arguments, '\n                    'the first is a character or its name'\n                )\n            else:\n                g = a\n            if isinstance(b, Place) or isinstance(b, Thing):\n                n = b.name\n            elif not isinstance(b, str):\n                raise TypeError(\n                    'when called with two arguments, '\n                    'the second is a thing/place or its name'\n                )\n            else:\n                n = b\n        # This will create the node if it doesn't exist. Otherwise\n        # it's redundant but harmless.\n        self.engine._exist_node(g, n)\n        # Declare that the node is my avatar\n        branch, turn, tick = self.engine._nbtt()\n        self.engine._remember_avatarness(self.name, g, n, branch=branch, turn=turn, tick=tick)", "response": "Add an avatar to the node."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef portals(self):\n        char = self.character\n        make_edge = self.engine._get_edge\n        for (o, d) in self.engine._edges_cache.iter_keys(\n                self.character.name, *self.engine._btt()\n        ):\n            yield make_edge(char, o, d)", "response": "Iterate over all portals in the character."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef avatars(self):\n        charname = self.character.name\n        branch, turn, tick = self.engine._btt()\n        charmap = self.engine.character\n        avit = self.engine._avatarness_cache.iter_entities\n        makenode = self.engine._get_node\n        for graph in avit(\n                charname, branch, turn, tick\n        ):\n            for node in avit(\n                    charname, graph, branch, turn, tick\n            ):\n                try:\n                    yield makenode(charmap[graph], node)\n                except KeyError:\n                    continue", "response": "Iterate over all my avatars regardless of what character they are\n        in."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrap for various prewritten or compiled SQL calls.", "response": "def sql(self, stringname, *args, **kwargs):\n        \"\"\"Wrapper for the various prewritten or compiled SQL calls.\n\n        First argument is the name of the query, either a key in\n        ``sqlite.json`` or a method name in\n        ``allegedb.alchemy.Alchemist``. The rest of the arguments are\n        parameters to the query.\n\n        \"\"\"\n        if hasattr(self, 'alchemist'):\n            return getattr(self.alchemist, stringname)(*args, **kwargs)\n        else:\n            s = self.strings[stringname]\n            return self.connection.cursor().execute(\n                s.format(**kwargs) if kwargs else s, args\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sqlmany(self, stringname, *args):\n        if hasattr(self, 'alchemist'):\n            return getattr(self.alchemist.many, stringname)(*args)\n        s = self.strings[stringname]\n        return self.connection.cursor().executemany(s, args)", "response": "Wrapper for executing many SQL queries on my connection."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef have_graph(self, graph):\n        graph = self.pack(graph)\n        return bool(self.sql('graphs_named', graph).fetchone()[0])", "response": "Return whether I have a graph by this name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndeclaring a new graph by this name of this type.", "response": "def new_graph(self, graph, typ):\n        \"\"\"Declare a new graph by this name of this type.\"\"\"\n        graph = self.pack(graph)\n        return self.sql('new_graph', graph, typ)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef del_graph(self, graph):\n        g = self.pack(graph)\n        self.sql('del_edge_val_graph', g)\n        self.sql('del_node_val_graph', g)\n        self.sql('del_edge_val_graph', g)\n        self.sql('del_edges_graph', g)\n        self.sql('del_nodes_graph', g)\n        self.sql('del_graph', g)", "response": "Delete all records in the graph."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef graph_type(self, graph):\n        graph = self.pack(graph)\n        return self.sql('graph_type', graph).fetchone()[0]", "response": "What type of graph is this?"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef global_get(self, key):\n        key = self.pack(key)\n        r = self.sql('global_get', key).fetchone()\n        if r is None:\n            raise KeyError(\"Not set\")\n        return self.unpack(r[0])", "response": "Return the value for the given key in the globals table."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef global_items(self):\n        for (k, v) in self.sql('global_dump'):\n            yield (self.unpack(k), self.unpack(v))", "response": "Iterate over the key value pairs in the globals table."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef global_set(self, key, value):\n        (key, value) = map(self.pack, (key, value))\n        try:\n            return self.sql('global_insert', key, value)\n        except IntegrityError:\n            return self.sql('global_update', value, key)", "response": "Set the value of key to value globally"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef global_del(self, key):\n        key = self.pack(key)\n        return self.sql('global_del', key)", "response": "Delete the global record for the key."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef new_branch(self, branch, parent, parent_turn, parent_tick):\n        return self.sql('branches_insert', branch, parent, parent_turn, parent_tick, parent_turn, parent_tick)", "response": "Declare that the branch is descended from parent at\n        parent_turn parent_tick"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef graph_val_dump(self):\n        self._flush_graph_val()\n        for (graph, key, branch, turn, tick, value) in self.sql('graph_val_dump'):\n            yield (\n                self.unpack(graph),\n                self.unpack(key),\n                branch,\n                turn,\n                tick,\n                self.unpack(value)\n            )", "response": "Yields the entire contents of the graph_val table."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _flush_graph_val(self):\n        if not self._graphvals2set:\n            return\n        delafter = {}\n        for graph, key, branch, turn, tick, value in self._graphvals2set:\n            if (graph, key, branch) in delafter:\n                delafter[graph, key, branch] = min((\n                    (turn, tick),\n                    delafter[graph, key, branch]\n                ))\n            else:\n                delafter[graph, key, branch] = (turn, tick)\n        self.sqlmany(\n            'del_graph_val_after',\n            *((graph, key, branch, turn, turn, tick)\n              for ((graph, key, branch), (turn, tick)) in delafter.items())\n        )\n        self.sqlmany('graph_val_insert', *self._graphvals2set)\n        self._graphvals2set = []", "response": "Send all new and changed graph values to the database."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef exist_node(self, graph, node, branch, turn, tick, extant):\n        if (branch, turn, tick) in self._btts:\n            raise TimeError\n        self._btts.add((branch, turn, tick))\n        self._nodes2set.append((self.pack(graph), self.pack(node), branch, turn, tick, extant))", "response": "Declare that the node exists or doesn t."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndumping the entire contents of the nodes table.", "response": "def nodes_dump(self):\n        \"\"\"Dump the entire contents of the nodes table.\"\"\"\n        self._flush_nodes()\n        for (graph, node, branch, turn,tick, extant) in self.sql('nodes_dump'):\n            yield (\n                self.unpack(graph),\n                self.unpack(node),\n                branch,\n                turn,\n                tick,\n                bool(extant)\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef node_val_dump(self):\n        self._flush_node_val()\n        for (\n                graph, node, key, branch, turn, tick, value\n        ) in self.sql('node_val_dump'):\n            yield (\n                self.unpack(graph),\n                self.unpack(node),\n                self.unpack(key),\n                branch,\n                turn,\n                tick,\n                self.unpack(value)\n            )", "response": "Yields the entire contents of the node_val table."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef node_val_set(self, graph, node, key, branch, turn, tick, value):\n        if (branch, turn, tick) in self._btts:\n            raise TimeError\n        self._btts.add((branch, turn, tick))\n        graph, node, key, value = map(self.pack, (graph, node, key, value))\n        self._nodevals2set.append((graph, node, key, branch, turn, tick, value))", "response": "Set a key - value pair on a specific branch and revision."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef edges_dump(self):\n        self._flush_edges()\n        for (\n                graph, orig, dest, idx, branch, turn, tick, extant\n        ) in self.sql('edges_dump'):\n            yield (\n                self.unpack(graph),\n                self.unpack(orig),\n                self.unpack(dest),\n                idx,\n                branch,\n                turn,\n                tick,\n                bool(extant)\n            )", "response": "Dump the entire contents of the edges table."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef exist_edge(self, graph, orig, dest, idx, branch, turn, tick, extant):\n        if (branch, turn, tick) in self._btts:\n            raise TimeError\n        self._btts.add((branch, turn, tick))\n        graph, orig, dest = map(self.pack, (graph, orig, dest))\n        self._edges2set.append((graph, orig, dest, idx, branch, turn, tick, extant))", "response": "Declare whether or not this edge exists."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nyield the entire contents of the edge_val table.", "response": "def edge_val_dump(self):\n        \"\"\"Yield the entire contents of the edge_val table.\"\"\"\n        self._flush_edge_val()\n        for (\n                graph, orig, dest, idx, key, branch, turn, tick, value\n        ) in self.sql('edge_val_dump'):\n            yield (\n                self.unpack(graph),\n                self.unpack(orig),\n                self.unpack(dest),\n                idx,\n                self.unpack(key),\n                branch,\n                turn,\n                tick,\n                self.unpack(value)\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef edge_val_set(self, graph, orig, dest, idx, key, branch, turn, tick, value):\n        if (branch, turn, tick) in self._btts:\n            raise TimeError\n        self._btts.add((branch, turn, tick))\n        graph, orig, dest, key, value = map(self.pack, (graph, orig, dest, key, value))\n        self._edgevals2set.append(\n            (graph, orig, dest, idx, key, branch, turn, tick, value)\n        )", "response": "Set this key of this edge to this value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef initdb(self):\n        if hasattr(self, 'alchemist'):\n            self.alchemist.meta.create_all(self.engine)\n            if 'branch' not in self.globl:\n                self.globl['branch'] = 'trunk'\n            if 'rev' not in self.globl:\n                self.globl['rev'] = 0\n            return\n        from sqlite3 import OperationalError\n        cursor = self.connection.cursor()\n        try:\n            cursor.execute('SELECT * FROM global;')\n        except OperationalError:\n            cursor.execute(self.strings['create_global'])\n        if 'branch' not in self.globl:\n            self.globl['branch'] = 'trunk'\n        if 'turn' not in self.globl:\n            self.globl['turn'] = 0\n        if 'tick' not in self.globl:\n            self.globl['tick'] = 0\n        for table in (\n            'branches',\n            'turns',\n            'graphs',\n            'graph_val',\n            'nodes',\n            'node_val',\n            'edges',\n            'edge_val',\n            'plans',\n            'plan_ticks'\n        ):\n            try:\n                cursor.execute('SELECT * FROM ' + table + ';')\n            except OperationalError:\n                cursor.execute(self.strings['create_' + table])", "response": "Create tables and indices as needed."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nputs all pending changes into the SQL transaction.", "response": "def flush(self):\n        \"\"\"Put all pending changes into the SQL transaction.\"\"\"\n        self._flush_nodes()\n        self._flush_edges()\n        self._flush_graph_val()\n        self._flush_node_val()\n        self._flush_edge_val()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef path_exists(self, dest, weight=None):\n        try:\n            return bool(self.shortest_path_length(dest, weight))\n        except KeyError:\n            return False", "response": "Return whether there is a path leading from me to dest."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delete(self):\n        if self.name in self.character.portal:\n            del self.character.portal[self.name]\n        if self.name in self.character.preportal:\n            del self.character.preportal[self.name]\n        for contained in list(self.contents()):\n            contained.delete()\n        for user in list(self.users.values()):\n            user.del_avatar(self.character.name, self.name)\n        branch, turn, tick = self.engine._nbtt()\n        self.engine._nodes_cache.store(\n            self.character.name, self.name,\n            branch, turn, tick, False\n        )\n        self.engine.query.exist_node(\n            self.character.name, self.name,\n            branch, turn, tick, False\n        )\n        self.character.node.send(self.character.node, key=self.name, val=None)", "response": "Delete the node from the cache."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconnect a portal from here to another node and return it.", "response": "def one_way_portal(self, other, **stats):\n        \"\"\"Connect a portal from here to another node, and return it.\"\"\"\n        return self.character.new_portal(\n            self, other, symmetrical=False, **stats\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef two_way_portal(self, other, **stats):\n        return self.character.new_portal(\n            self, other, symmetrical=True, **stats\n        )", "response": "Connect these nodes with a two - way portal and return it."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef new_thing(self, name, **stats):\n        return self.character.new_thing(\n            name, self.name, **stats\n        )", "response": "Create a new thing located here and return it."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating the swatches widget for each texture.", "response": "def upd_textures(self, *args):\n        \"\"\"Create one :class:`SwatchButton` for each texture\"\"\"\n        if self.canvas is None:\n            Clock.schedule_once(self.upd_textures, 0)\n            return\n        for name in list(self.swatches.keys()):\n            if name not in self.atlas.textures:\n                self.remove_widget(self.swatches[name])\n                del self.swatches[name]\n        for (name, tex) in self.atlas.textures.items():\n            if name in self.swatches and self.swatches[name] != tex:\n                self.remove_widget(self.swatches[name])\n            if name not in self.swatches or self.swatches[name] != tex:\n                self.swatches[name] = SwatchButton(\n                    name=name,\n                    tex=tex,\n                    size_hint=(None, None),\n                    size=self.swatch_size\n                )\n                self.add_widget(self.swatches[name])"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the x value of the given sizehintx if available.", "response": "def get_pos_hint_x(poshints, sizehintx):\n    \"\"\"Return ``poshints['x']`` if available, or its computed equivalent\n    otherwise.\n\n    \"\"\"\n    if 'x' in poshints:\n        return poshints['x']\n    elif sizehintx is not None:\n        if 'center_x' in poshints:\n            return (\n                poshints['center_x'] -\n                sizehintx / 2\n            )\n        elif 'right' in poshints:\n            return (\n                poshints['right'] -\n                sizehintx\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns y if available or its computed equivalent otherwise.", "response": "def get_pos_hint_y(poshints, sizehinty):\n    \"\"\"Return ``poshints['y']`` if available, or its computed equivalent\n    otherwise.\n\n    \"\"\"\n    if 'y' in poshints:\n        return poshints['y']\n    elif sizehinty is not None:\n        if 'center_y' in poshints:\n            return (\n                poshints['center_y'] -\n                sizehinty / 2\n            )\n        elif 'top' in poshints:\n            return (\n                poshints['top'] -\n                sizehinty\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_pos_hint(poshints, sizehintx, sizehinty):\n    return (\n        get_pos_hint_x(poshints, sizehintx),\n        get_pos_hint_y(poshints, sizehinty)\n    )", "response": "Return a tuple of pos_hint_x pos_hint_y"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef on_background_source(self, *args):\n        if self.background_source:\n            self.background_image = Image(source=self.background_source)", "response": "When I get a new background_source load it as an anonymized Image and store that in self. background_image"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef on_foreground_image(self, *args):\n        if self.foreground_image is not None:\n            self.foreground_texture = self.foreground_image.texture", "response": "When I get a new foreground image store its texture in my\n       . foreground_texture"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef on_art_source(self, *args):\n        if self.art_source:\n            self.art_image = Image(source=self.art_source)", "response": "When I get a new art_source load it as an Image and store that in art_image"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef on_touch_down(self, touch):\n        if not self.collide_point(*touch.pos):\n            return\n        if 'card' in touch.ud:\n            return\n        touch.grab(self)\n        self.dragging = True\n        touch.ud['card'] = self\n        touch.ud['idx'] = self.idx\n        touch.ud['deck'] = self.deck\n        touch.ud['layout'] = self.parent\n        self.collide_x = touch.x - self.x\n        self.collide_y = touch.y - self.y", "response": "If I m the first card to collide this touch grab it and store the relative coords upon the card where the collision happened."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmoving the same tag relative to the touch.", "response": "def on_touch_move(self, touch):\n        \"\"\"If I'm being dragged, move so as to be always positioned the same\n        relative to the touch.\n\n        \"\"\"\n        if not self.dragging:\n            touch.ungrab(self)\n            return\n        self.pos = (\n            touch.x - self.collide_x,\n            touch.y - self.collide_y\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef on_touch_up(self, touch):\n        if not self.dragging:\n            return\n        touch.ungrab(self)\n        self.dragging = False", "response": "Stop dragging if needed."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a copy of this instance.", "response": "def copy(self):\n        \"\"\"Return a new :class:`Card` just like me.\"\"\"\n        d = {}\n        for att in (\n                'deck',\n                'idx',\n                'ud',\n                'foreground_source',\n                'foreground_color',\n                'foreground_image',\n                'foreground_texture',\n                'background_source',\n                'background_color',\n                'background_image',\n                'background_texture',\n                'outline_color',\n                'content_outline_color',\n                'foreground_outline_color',\n                'art_outline_color',\n                'art_source',\n                'art_color',\n                'art_image',\n                'art_texture',\n                'show_art',\n                'headline_text',\n                'headline_markup',\n                'headline_font_name',\n                'headline_font_size',\n                'headline_color',\n                'midline_text',\n                'midline_markup',\n                'midline_font_name',\n                'midline_font_size',\n                'midline_color',\n                'footer_text',\n                'footer_markup',\n                'footer_font_name',\n                'footer_font_size',\n                'footer_color',\n                'text',\n                'text_color',\n                'markup',\n                'font_name',\n                'font_size'\n        ):\n            v = getattr(self, att)\n            if v is not None:\n                d[att] = v\n        return Card(**d)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nasks the foundation where I should be based on what deck I m for.", "response": "def upd_pos(self, *args):\n        \"\"\"Ask the foundation where I should be, based on what deck I'm\n        for.\n\n        \"\"\"\n        self.pos = self.parent._get_foundation_pos(self.deck)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef upd_size(self, *args):\n        self.size = (\n            self.parent.card_size_hint_x * self.parent.width,\n            self.parent.card_size_hint_y * self.parent.height\n        )", "response": "Update the size of the current card in my : class : DepLayout."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef scroll_deck_x(self, decknum, scroll_x):\n        if decknum >= len(self.decks):\n            raise IndexError(\"I have no deck at {}\".format(decknum))\n        if decknum >= len(self.deck_x_hint_offsets):\n            self.deck_x_hint_offsets = list(self.deck_x_hint_offsets) + [0] * (\n                decknum - len(self.deck_x_hint_offsets) + 1\n            )\n        self.deck_x_hint_offsets[decknum] += scroll_x\n        self._trigger_layout()", "response": "Move a deck left or right."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef scroll_deck_y(self, decknum, scroll_y):\n        if decknum >= len(self.decks):\n            raise IndexError(\"I have no deck at {}\".format(decknum))\n        if decknum >= len(self.deck_y_hint_offsets):\n            self.deck_y_hint_offsets = list(self.deck_y_hint_offsets) + [0] * (\n                decknum - len(self.deck_y_hint_offsets) + 1\n            )\n        self.deck_y_hint_offsets[decknum] += scroll_y\n        self._trigger_layout()", "response": "Move a deck up or down."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_foundation_pos(self, i):\n        (phx, phy) = get_pos_hint(\n            self.starting_pos_hint, *self.card_size_hint\n        )\n        phx += self.deck_x_hint_step * i + self.deck_x_hint_offsets[i]\n        phy += self.deck_y_hint_step * i + self.deck_y_hint_offsets[i]\n        x = phx * self.width + self.x\n        y = phy * self.height + self.y\n        return (x, y)", "response": "Private. Get the absolute coordinates to use for a deck s foundation based on the starting_pos_hint and card_size_hint."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a Foundation for some deck creating it if needed.", "response": "def _get_foundation(self, i):\n        \"\"\"Return a :class:`Foundation` for some deck, creating it if\n        needed.\n\n        \"\"\"\n        if i >= len(self._foundations) or self._foundations[i] is None:\n            oldfound = list(self._foundations)\n            extend = i - len(oldfound) + 1\n            if extend > 0:\n                oldfound += [None] * extend\n            width = self.card_size_hint_x * self.width\n            height = self.card_size_hint_y * self.height\n            found = Foundation(\n                pos=self._get_foundation_pos(i), size=(width, height), deck=i\n            )\n            self.bind(\n                pos=found.upd_pos,\n                card_size_hint=found.upd_pos,\n                deck_hint_step=found.upd_pos,\n                size=found.upd_pos,\n                deck_x_hint_offsets=found.upd_pos,\n                deck_y_hint_offsets=found.upd_pos\n            )\n            self.bind(\n                size=found.upd_size,\n                card_size_hint=found.upd_size\n            )\n            oldfound[i] = found\n            self._foundations = oldfound\n        return self._foundations[i]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninforms the cards of their deck and their index within the deck ; extend the _hint_offsets properties as needed ; and trigger the layout as needed ; and trigger the layout as needed ; and trigger the layout as needed ;", "response": "def on_decks(self, *args):\n        \"\"\"Inform the cards of their deck and their index within the deck;\n        extend the ``_hint_offsets`` properties as needed; and trigger\n        a layout.\n\n        \"\"\"\n        if None in (\n                self.canvas,\n                self.decks,\n                self.deck_x_hint_offsets,\n                self.deck_y_hint_offsets\n        ):\n            Clock.schedule_once(self.on_decks, 0)\n            return\n        self.clear_widgets()\n        decknum = 0\n        for deck in self.decks:\n            cardnum = 0\n            for card in deck:\n                if not isinstance(card, Card):\n                    raise TypeError(\"You must only put Card in decks\")\n                if card not in self.children:\n                    self.add_widget(card)\n                if card.deck != decknum:\n                    card.deck = decknum\n                if card.idx != cardnum:\n                    card.idx = cardnum\n                cardnum += 1\n            decknum += 1\n        if len(self.deck_x_hint_offsets) < len(self.decks):\n            self.deck_x_hint_offsets = list(self.deck_x_hint_offsets) + [0] * (\n                len(self.decks) - len(self.deck_x_hint_offsets)\n            )\n        if len(self.deck_y_hint_offsets) < len(self.decks):\n            self.deck_y_hint_offsets = list(self.deck_y_hint_offsets) + [0] * (\n                len(self.decks) - len(self.deck_y_hint_offsets)\n            )\n        self._trigger_layout()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning whether the card is somewhere before the given point.", "response": "def point_before_card(self, card, x, y):\n        \"\"\"Return whether ``(x, y)`` is somewhere before ``card``, given how I\n        know cards to be arranged.\n\n        If the cards are being stacked down and to the right, that\n        means I'm testing whether ``(x, y)`` is above or to the left\n        of the card.\n\n        \"\"\"\n        def ycmp():\n            if self.card_y_hint_step == 0:\n                return False\n            elif self.card_y_hint_step > 0:\n                # stacking upward\n                return y < card.y\n            else:\n                # stacking downward\n                return y > card.top\n        if self.card_x_hint_step > 0:\n            # stacking to the right\n            if x < card.x:\n                return True\n            return ycmp()\n        elif self.card_x_hint_step == 0:\n            return ycmp()\n        else:\n            # stacking to the left\n            if x > card.right:\n                return True\n            return ycmp()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef on_touch_move(self, touch):\n        if (\n                'card' not in touch.ud or\n                'layout' not in touch.ud or\n                touch.ud['layout'] != self\n        ):\n            return\n        if (\n                touch.ud['layout'] == self and\n                not hasattr(touch.ud['card'], '_topdecked')\n        ):\n            touch.ud['card']._topdecked = InstructionGroup()\n            touch.ud['card']._topdecked.add(touch.ud['card'].canvas)\n            self.canvas.after.add(touch.ud['card']._topdecked)\n        for i, deck in enumerate(self.decks):\n            cards = [card for card in deck if not card.dragging]\n            maxidx = max(card.idx for card in cards) if cards else 0\n            if self.direction == 'descending':\n                cards.reverse()\n            cards_collided = [\n                card for card in cards if card.collide_point(*touch.pos)\n            ]\n            if cards_collided:\n                collided = cards_collided.pop()\n                for card in cards_collided:\n                    if card.idx > collided.idx:\n                        collided = card\n                if collided.deck == touch.ud['deck']:\n                    self.insertion_card = (\n                        1 if collided.idx == 0 else\n                        maxidx + 1 if collided.idx == maxidx else\n                        collided.idx + 1 if collided.idx > touch.ud['idx']\n                        else collided.idx\n                    )\n                else:\n                    dropdeck = self.decks[collided.deck]\n                    maxidx = max(card.idx for card in dropdeck)\n                    self.insertion_card = (\n                        1 if collided.idx == 0 else\n                        maxidx + 1 if collided.idx == maxidx else\n                        collided.idx + 1\n                    )\n                if self.insertion_deck != collided.deck:\n                    self.insertion_deck = collided.deck\n                return\n            else:\n                if self.insertion_deck == i:\n                    if self.insertion_card in (0, len(deck)):\n                        pass\n                    elif self.point_before_card(\n                            cards[0], *touch.pos\n                    ):\n                        self.insertion_card = 0\n                    elif self.point_after_card(\n                        cards[-1], *touch.pos\n                    ):\n                        self.insertion_card = cards[-1].idx\n                else:\n                    for j, found in enumerate(self._foundations):\n                        if (\n                                found is not None and\n                                found.collide_point(*touch.pos)\n                        ):\n                            self.insertion_deck = j\n                            self.insertion_card = 0\n                            return", "response": "Move other cards out of the way to show where the dragged card will go if you drop it."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nstacking the cards, starting at my deck's foundation, and proceeding by ``card_pos_hint``", "response": "def layout_deck(self, i):\n        \"\"\"Stack the cards, starting at my deck's foundation, and proceeding\n        by ``card_pos_hint``\n\n        \"\"\"\n        def get_dragidx(cards):\n            j = 0\n            for card in cards:\n                if card.dragging:\n                    return j\n                j += 1\n        # Put a None in the card list in place of the card you're\n        # hovering over, if you're dragging another card. This will\n        # result in an empty space where the card will go if you drop\n        # it now.\n        cards = list(self.decks[i])\n        dragidx = get_dragidx(cards)\n        if dragidx is not None:\n            del cards[dragidx]\n        if self.insertion_deck == i and self.insertion_card is not None:\n            insdx = self.insertion_card\n            if dragidx is not None and insdx > dragidx:\n                insdx -= 1\n            cards.insert(insdx, None)\n        if self.direction == 'descending':\n            cards.reverse()\n        # Work out the initial pos_hint for this deck\n        (phx, phy) = get_pos_hint(self.starting_pos_hint, *self.card_size_hint)\n        phx += self.deck_x_hint_step * i + self.deck_x_hint_offsets[i]\n        phy += self.deck_y_hint_step * i + self.deck_y_hint_offsets[i]\n        (w, h) = self.size\n        (x, y) = self.pos\n        # start assigning pos and size to cards\n        found = self._get_foundation(i)\n        if found in self.children:\n            self.remove_widget(found)\n        self.add_widget(found)\n        for card in cards:\n            if card is not None:\n                if card in self.children:\n                    self.remove_widget(card)\n                (shw, shh) = self.card_size_hint\n                card.pos = (\n                    x + phx * w,\n                    y + phy * h\n                )\n                card.size = (w * shw, h * shh)\n                self.add_widget(card)\n            phx += self.card_x_hint_step\n            phy += self.card_y_hint_step"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntelling my parent if I ve been touched", "response": "def on_touch_down(self, touch):\n        \"\"\"Tell my parent if I've been touched\"\"\"\n        if self.parent is None:\n            return\n        if self.collide_point(*touch.pos):\n            self.parent.bar_touched(self, touch)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef do_layout(self, *args):\n        if 'bar' not in self.ids:\n            Clock.schedule_once(self.do_layout)\n            return\n        if self.orientation == 'horizontal':\n            self.ids.bar.size_hint_x = self.hbar[1]\n            self.ids.bar.pos_hint = {'x': self.hbar[0], 'y': 0}\n        else:\n            self.ids.bar.size_hint_y = self.vbar[1]\n            self.ids.bar.pos_hint = {'x': 0, 'y': self.vbar[0]}\n        super().do_layout(*args)", "response": "Put the bar where it s supposed to be and size it in proportion to\n            the size of the scrollable area."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating my own scroll property to where my deck is actually scrolled.", "response": "def upd_scroll(self, *args):\n        \"\"\"Update my own ``scroll`` property to where my deck is actually\n        scrolled.\n\n        \"\"\"\n        att = 'deck_{}_hint_offsets'.format(\n            'x' if self.orientation == 'horizontal' else 'y'\n        )\n        self._scroll = getattr(self.deckbuilder, att)[self.deckidx]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbind my deckbuilder to update my scroll and my scroll to update my deckbuilder.", "response": "def on_deckbuilder(self, *args):\n        \"\"\"Bind my deckbuilder to update my ``scroll``, and my ``scroll`` to\n        update my deckbuilder.\n\n        \"\"\"\n        if self.deckbuilder is None:\n            return\n        att = 'deck_{}_hint_offsets'.format(\n            'x' if self.orientation == 'horizontal' else 'y'\n        )\n        offs = getattr(self.deckbuilder, att)\n        if len(offs) <= self.deckidx:\n            Clock.schedule_once(self.on_deckbuilder, 0)\n            return\n        self.bind(scroll=self.handle_scroll)\n        self.deckbuilder.bind(**{att: self.upd_scroll})\n        self.upd_scroll()\n        self.deckbuilder._trigger_layout()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef handle_scroll(self, *args):\n        if 'bar' not in self.ids:\n            Clock.schedule_once(self.handle_scroll, 0)\n            return\n        att = 'deck_{}_hint_offsets'.format(\n            'x' if self.orientation == 'horizontal' else 'y'\n        )\n        offs = list(getattr(self.deckbuilder, att))\n        if len(offs) <= self.deckidx:\n            Clock.schedule_once(self.on_scroll, 0)\n            return\n        offs[self.deckidx] = self._scroll\n        setattr(self.deckbuilder, att, offs)\n        self.deckbuilder._trigger_layout()", "response": "When my deckbuilder is scrolled tell my deckbuilder how it s scrolled."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nstarts scrolling and record where I started scrolling.", "response": "def bar_touched(self, bar, touch):\n        \"\"\"Start scrolling, and record where I started scrolling.\"\"\"\n        self.scrolling = True\n        self._start_bar_pos_hint = get_pos_hint(bar.pos_hint, *bar.size_hint)\n        self._start_touch_pos_hint = (\n            touch.x / self.width,\n            touch.y / self.height\n        )\n        self._start_bar_touch_hint = (\n            self._start_touch_pos_hint[0] - self._start_bar_pos_hint[0],\n            self._start_touch_pos_hint[1] - self._start_bar_pos_hint[1]\n        )\n        touch.grab(self)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef on_touch_move(self, touch):\n        if not self.scrolling or 'bar' not in self.ids:\n            touch.ungrab(self)\n            return\n        touch.push()\n        touch.apply_transform_2d(self.parent.to_local)\n        touch.apply_transform_2d(self.to_local)\n        if self.orientation == 'horizontal':\n            hint_right_of_bar = (touch.x - self.ids.bar.x) / self.width\n            hint_correction = hint_right_of_bar - self._start_bar_touch_hint[0]\n            self.scroll += hint_correction\n        else:  # self.orientation == 'vertical'\n            hint_above_bar = (touch.y - self.ids.bar.y) / self.height\n            hint_correction = hint_above_bar - self._start_bar_touch_hint[1]\n            self.scroll += hint_correction\n        touch.pop()", "response": "Move the scrollbar to the touch and update my scroll accordingly."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef unwrap(self):\n        return [v.unwrap() if hasattr(v, 'unwrap') and not hasattr(v, 'no_unwrap') else v for v in self]", "response": "Return a deep copy of myself and unwrap any wrapper objects in me."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_points(orig, dest, taillen):\n    # Adjust the start and end points so they're on the first non-transparent pixel.\n    # y = slope(x-ox) + oy\n    # x = (y - oy) / slope + ox\n    ox, oy = orig.center\n    ow, oh = orig.size\n    dx, dy = dest.center\n    dw, dh = dest.size\n    if ox < dx:\n        leftx = ox\n        rightx = dx\n        xco = 1\n    elif ox > dx:\n        leftx = ox * -1\n        rightx = dx * -1\n        xco = -1\n    else:\n        # straight up and down arrow\n        return up_and_down(orig, dest, taillen)\n    if oy < dy:\n        boty = oy\n        topy = dy\n        yco = 1\n    elif oy > dy:\n        boty = oy * -1\n        topy = dy * -1\n        yco = -1\n    else:\n        # straight left and right arrow\n        return left_and_right(orig, dest, taillen)\n    slope = (topy - boty) / (rightx - leftx)\n    # start from the earliest point that intersects the bounding box.\n    # work toward the center to find a non-transparent pixel\n    # y - boty = ((topy-boty)/(rightx-leftx))*(x - leftx)\n    if slope <= 1:\n        for rightx in range(\n                int(rightx - dw / 2),\n                int(rightx)+1\n        ):\n            topy = slope * (rightx - leftx) + boty\n            if dest.collide_point(rightx * xco, topy * yco):\n                rightx = float(rightx - 1)\n                for pip in range(10):\n                    rightx += 0.1 * pip\n                    topy = slope * (rightx - leftx) + boty\n                    if dest.collide_point(rightx * xco, topy * yco):\n                        break\n                break\n        for leftx in range(\n                int(leftx + ow / 2),\n                int(leftx)-1,\n                -1\n        ):\n            boty = slope * (leftx - rightx) + topy\n            if orig.collide_point(leftx * xco, boty * yco):\n                leftx = float(leftx + 1)\n                for pip in range(10):\n                    leftx -= 0.1 * pip\n                    boty = slope * (leftx - rightx) + topy\n                    if orig.collide_point(leftx * xco, boty * yco):\n                        break\n                break\n    else:\n        # x = leftx + ((rightx-leftx)(y - boty))/(topy-boty)\n        for topy in range(\n            int(topy - dh / 2),\n            int(topy) + 1\n        ):\n            rightx = leftx + (topy - boty) / slope\n            if dest.collide_point(rightx * xco, topy * yco):\n                topy = float(topy - 1)\n                for pip in range(10):\n                    topy += 0.1 * pip\n                    rightx = leftx + (topy - boty) / slope\n                    if dest.collide_point(rightx * xco, topy * yco):\n                        break\n                break\n        for boty in range(\n            int(boty + oh / 2),\n            int(boty) - 1,\n            -1\n        ):\n            leftx = (boty - topy) / slope + rightx\n            if orig.collide_point(leftx * xco, boty * yco):\n                boty = float(boty + 1)\n                for pip in range(10):\n                    boty -= 0.1 * pip\n                    leftx = (boty - topy) / slope + rightx\n                    if orig.collide_point(leftx * xco, boty * yco):\n                        break\n                break\n\n    rise = topy - boty\n    run = rightx - leftx\n\n    try:\n        start_theta = atan(rise/run)\n    except ZeroDivisionError:\n        return up_and_down(orig, dest, taillen)\n    try:\n        end_theta = atan(run/rise)\n    except ZeroDivisionError:\n        return left_and_right(orig, dest, taillen)\n\n    # make the little wedge at the end so you can tell which way the\n    # arrow's pointing, and flip it all back around to the way it was\n    top_theta = start_theta - fortyfive\n    bot_theta = pi - fortyfive - end_theta\n    xoff1 = cos(top_theta) * taillen\n    yoff1 = sin(top_theta) * taillen\n    xoff2 = cos(bot_theta) * taillen\n    yoff2 = sin(bot_theta) * taillen\n    x1 = (rightx - xoff1) * xco\n    x2 = (rightx - xoff2) * xco\n    y1 = (topy - yoff1) * yco\n    y2 = (topy - yoff2) * yco\n    startx = leftx * xco\n    starty = boty * yco\n    endx = rightx * xco\n    endy = topy * yco\n    return (\n        [startx, starty, endx, endy],\n        [x1, y1, endx, endy, x2, y2]\n    )", "response": "Return a pair of lists of points for use making an arrow."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets my name and instantiate my mirrormap as soon as I have the properties I need to do so.", "response": "def on_portal(self, *args):\n        \"\"\"Set my ``name`` and instantiate my ``mirrormap`` as soon as I have\n        the properties I need to do so.\n\n        \"\"\"\n        if not (\n                self.board and\n                self.origin and\n                self.destination and\n                self.origin.name in self.board.character.portal and\n                self.destination.name in self.board.character.portal\n        ):\n            Clock.schedule_once(self.on_portal, 0)\n            return\n        self.name = '{}->{}'.format(self.portal['origin'], self.portal['destination'])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef collide_point(self, x, y):\n        if not self.collider:\n            return False\n        return (x, y) in self.collider", "response": "Return True if the point is collide with another point."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmake sure to redraw whenever the origin moves.", "response": "def on_origin(self, *args):\n        \"\"\"Make sure to redraw whenever the origin moves.\"\"\"\n        if self.origin is None:\n            Clock.schedule_once(self.on_origin, 0)\n            return\n        self.origin.bind(\n            pos=self._trigger_repoint,\n            size=self._trigger_repoint\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef on_destination(self, *args):\n        if self.destination is None:\n            Clock.schedule_once(self.on_destination, 0)\n            return\n        self.destination.bind(\n            pos=self._trigger_repoint,\n            size=self._trigger_repoint\n        )", "response": "Make sure to redraw whenever the destination moves."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndrawing myself for the first time as soon as I have the properties I need to do so.", "response": "def on_board(self, *args):\n        \"\"\"Draw myself for the first time as soon as I have the properties I\n        need to do so.\n\n        \"\"\"\n        if None in (\n                self.board,\n                self.origin,\n                self.destination\n        ):\n            Clock.schedule_once(self.on_board, 0)\n            return\n        self._trigger_repoint()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_widget(self, wid, index=0, canvas=None):\n        super().add_widget(wid, index, canvas)\n        if not hasattr(wid, 'group'):\n            return\n        wid._no_use_canvas = True\n        mycanvas = (\n            self.canvas.before if canvas == 'before' else\n            self.canvas.after if canvas == 'after' else\n            self.canvas\n        )\n        mycanvas.remove(wid.canvas)\n        pawncanvas = (\n            self.board.spotlayout.canvas.before if canvas == 'before' else\n            self.board.spotlayout.canvas.after if canvas == 'after' else\n            self.board.spotlayout.canvas\n        )\n        for child in self.children:\n            if hasattr(child, 'group') and child.group in pawncanvas.children:\n                pawncanvas.remove(child.group)\n            pawncanvas.add(child.group)\n        self.pospawn(wid)", "response": "Add a widget to the list of available pawn areas."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning coordinates for where a Pawn should be if it has travelled along pct.", "response": "def pos_along(self, pct):\n        \"\"\"Return coordinates for where a Pawn should be if it has travelled\n        along ``pct`` of my length (between 0 and 1).\n\n        Might get complex when I switch over to using beziers for\n        arrows, but for now this is quite simple, using distance along\n        a line segment.\n\n        \"\"\"\n        if pct < 0 or pct > 1:\n            raise ValueError(\"Invalid portion\")\n        (ox, oy) = self.origin.center\n        (dx, dy) = self.destination.center\n        xdist = (dx - ox) * pct\n        ydist = (dy - oy) * pct\n        return (ox + xdist, oy + ydist)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npositioning a pawn that is my child so as to reflect how far its thing has gone along my portal.", "response": "def pospawn(self, pawn):\n        \"\"\"Position a :class:`Pawn` that is my child so as to reflect how far\n        its :class:`Thing` has gone along my :class:`Portal`.\n\n        \"\"\"\n        if self._turn < pawn.thing['arrival_time']:\n            # It's weird that the pawn is getting placed in me, but\n            # I'll do my best..\n            pawn.pos = self.pos_along(0)\n            return\n        elif (\n                pawn.thing['next_arrival_time'] and\n                self._turn >= pawn.thing['next_arrival_time']\n        ):\n            pawn.pos = self.pos_along(1)\n            return\n        try:\n            pawn.pos = self.pos_along(\n                (\n                    self._turn -\n                    pawn.thing['arrival_time']\n                ) / (\n                    pawn.thing['next_arrival_time'] -\n                    pawn.thing['arrival_time']\n                )\n            )\n        except (TypeError, ZeroDivisionError):\n            pawn.pos = self.pos_along(0)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_slope(self):\n        orig = self.origin\n        dest = self.destination\n        ox = orig.x\n        oy = orig.y\n        dx = dest.x\n        dy = dest.y\n        if oy == dy:\n            return 0\n        elif ox == dx:\n            return None\n        else:\n            rise = dy - oy\n            run = dx - ox\n            return rise / run", "response": "Return a float of the increase in y divided by the increase in x both from left to right."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_b(self):\n        orig = self.origin\n        dest = self.destination\n        (ox, oy) = orig.pos\n        (dx, dy) = dest.pos\n        denominator = dx - ox\n        x_numerator = (dy - oy) * ox\n        y_numerator = denominator * oy\n        return ((y_numerator - x_numerator), denominator)", "response": "Return my Y - intercept."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _repoint(self, *args):\n        if None in (self.origin, self.destination):\n            Clock.schedule_once(self._repoint, 0)\n            return\n        try:\n            (self.trunk_points, self.head_points) = self._get_points()\n        except ValueError:\n            self.trunk_points = self.head_points = []\n            return\n        (ox, oy, dx, dy) = self.trunk_points\n        r = self.w / 2\n        bgr = r * self.bg_scale_selected if self.selected \\\n            else self.bg_scale_unselected\n        self.trunk_quad_vertices_bg = get_thin_rect_vertices(\n            ox, oy, dx, dy, bgr\n        )\n        self.collider = Collide2DPoly(self.trunk_quad_vertices_bg)\n        self.trunk_quad_vertices_fg = get_thin_rect_vertices(ox, oy, dx, dy, r)\n        (x1, y1, endx, endy, x2, y2) = self.head_points\n        self.left_head_quad_vertices_bg = get_thin_rect_vertices(\n            x1, y1, endx, endy, bgr\n        )\n        self.right_head_quad_vertices_bg = get_thin_rect_vertices(\n            x2, y2, endx, endy, bgr\n        )\n        self.left_head_quad_vertices_fg = get_thin_rect_vertices(\n            x1, y1, endx, endy, r\n        )\n        self.right_head_quad_vertices_fg = get_thin_rect_vertices(\n            x2, y2, endx, endy, r\n        )\n        self.slope = self._get_slope()\n        self.y_intercept = self._get_b()\n        self.repointed = True", "response": "Recalculate points y - intercept and slope"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the Arrow that connects my origin and destination in the opposite direction.", "response": "def _get_reciprocal(self):\n        \"\"\"Return the :class:`Arrow` that connects my origin and destination\n        in the opposite direction, if it exists.\n\n        \"\"\"\n        orign = self.portal['origin']\n        destn = self.portal['destination']\n        if (\n                destn in self.board.arrow and\n                orign in self.board.arrow[destn]\n        ):\n            return self.board.arrow[destn][orign]\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef munge_source(v):\n    lines = v.split('\\n')\n    if not lines:\n        return tuple(), ''\n    firstline = lines[0].lstrip()\n    while firstline == '' or firstline[0] == '@':\n        del lines[0]\n        firstline = lines[0].lstrip()\n    if not lines:\n        return tuple(), ''\n    params = tuple(\n        parm.strip() for parm in\n        sig_ex.match(lines[0]).group(1).split(',')\n    )\n    del lines[0]\n    if not lines:\n        return params, ''\n    # hack to allow 'empty' functions\n    if lines and lines[-1].strip() == 'pass':\n        del lines[-1]\n    return params, dedent('\\n'.join(lines))", "response": "Take a Python source code return a pair of its parameters and the rest of it dedented"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating my data to match what s in my store", "response": "def redata(self, *args, **kwargs):\n        \"\"\"Update my ``data`` to match what's in my ``store``\"\"\"\n        select_name = kwargs.get('select_name')\n        if not self.store:\n            Clock.schedule_once(self.redata)\n            return\n        self.data = list(map(self.munge, enumerate(self._iter_keys())))\n        if select_name:\n            self._trigger_select_name(select_name)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nselect an item by its name highlighting", "response": "def select_name(self, name, *args):\n        \"\"\"Select an item by its name, highlighting\"\"\"\n        self.boxl.select_node(self._name2i[name])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef save(self, *args):\n        if self.name_wid is None or self.store is None:\n            Logger.debug(\"{}: Not saving, missing name_wid or store\".format(type(self).__name__))\n            return\n        if not (self.name_wid.text or self.name_wid.hint_text):\n            Logger.debug(\"{}: Not saving, no name\".format(type(self).__name__))\n            return\n        if self.name_wid.text and self.name_wid.text[0] in string.digits + string.whitespace + string.punctuation:\n            # TODO alert the user to invalid name\n            Logger.warning(\"{}: Not saving, invalid name\".format(type(self).__name__))\n            return\n        if hasattr(self, '_do_parse'):\n            try:\n                parse(self.source)\n            except SyntaxError:\n                # TODO alert user to invalid source\n                Logger.debug(\"{}: Not saving, couldn't parse\".format(type(self).__name__))\n                return\n        do_redata = False\n        if self.name_wid.text:\n            if (\n                self.name_wid.hint_text and\n                self.name_wid.hint_text != self.name_wid.text and\n                hasattr(self.store, self.name_wid.hint_text)\n            ):\n                delattr(self.store, self.name_wid.hint_text)\n                do_redata = True\n            if (\n                not hasattr(self.store, self.name_wid.text) or\n                getattr(self.store, self.name_wid.text) != self.source\n            ):\n                Logger.debug(\"{}: Saving!\".format(type(self).__name__))\n                setattr(self.store, self.name_wid.text, self.source)\n                do_redata = True\n        elif self.name_wid.hint_text:\n            if (\n                not hasattr(self.store, self.name_wid.hint_text) or\n                getattr(self.store, self.name_wid.hint_text) != self.source\n            ):\n                Logger.debug(\"{}: Saving!\".format(type(self).__name__))\n                setattr(self.store, self.name_wid.hint_text, self.source)\n                do_redata = True\n        return do_redata", "response": "Put text in my store return True if it changed"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef delete(self, *args):\n        key = self.name_wid.text or self.name_wid.hint_text\n        if not hasattr(self.store, key):\n            # TODO feedback about missing key\n            return\n        delattr(self.store, key)\n        try:\n            return min(kee for kee in dir(self.store) if kee > key)\n        except ValueError:\n            return '+'", "response": "Remove the currently selected item from my store"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef advance_dialog(self, *args):\n        self.clear_widgets()\n        try:\n            self._update_dialog(self.todo[self.idx])\n        except IndexError:\n            pass", "response": "Try to display the next dialog described in my todo."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nclear the dialog widgets call cb if provided and advance the dialog queue", "response": "def ok(self, *args, cb=None):\n        \"\"\"Clear dialog widgets, call ``cb`` if provided, and advance the dialog queue\"\"\"\n        self.clear_widgets()\n        if cb:\n            cb()\n        self.idx += 1\n        self.advance_dialog()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nyield pairs of id string for the given language.", "response": "def lang_items(self, lang=None):\n        \"\"\"Yield pairs of (id, string) for the given language.\"\"\"\n        if lang is None:\n            lang = self.language\n        yield from self.cache.setdefault(lang, {}).items()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a child to the object.", "response": "def add_child(self, child):\n        '''Add child to ``Node`` object\n\n        Args:\n            ``child`` (``Node``): The child ``Node`` to be added\n        '''\n        if not isinstance(child, Node):\n            raise TypeError(\"child must be a Node\")\n        self.children.append(child); child.parent = self"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncontracting this Node by directly connecting its children to its parent.", "response": "def contract(self):\n        '''Contract this ``Node`` by directly connecting its children to its parent'''\n        if self.is_root():\n            return\n        for c in self.children:\n            if self.edge_length is not None and c.edge_length is not None:\n                c.edge_length += self.edge_length\n            self.parent.add_child(c)\n        self.parent.remove_child(self)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a string representation of the node object.", "response": "def newick(self):\n        '''Newick string conversion starting at this ``Node`` object\n\n        Returns:\n            ``str``: Newick string conversion starting at this ``Node`` object\n        '''\n        node_to_str = dict()\n        for node in self.traverse_postorder():\n            if node.is_leaf():\n                if node.label is None:\n                    node_to_str[node] = ''\n                else:\n                    node_to_str[node] = str(node.label)\n            else:\n                out = ['(']\n                for c in node.children:\n                    out.append(node_to_str[c])\n                    if c.edge_length is not None:\n                        if isinstance(c.edge_length,int):\n                            l_str = str(c.edge_length)\n                        elif isinstance(c.edge_length,float) and c.edge_length.is_integer():\n                            l_str = str(int(c.edge_length))\n                        else:\n                            l_str = str(c.edge_length)\n                        out.append(':%s' % l_str)\n                    out.append(',')\n                    del node_to_str[c]\n                out.pop() # trailing comma\n                out.append(')')\n                if node.label is not None:\n                    out.append(str(node.label))\n                node_to_str[node] = ''.join(out)\n        return node_to_str[self]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remove_child(self, child):\n        '''Remove child from ``Node`` object\n\n        Args:\n            ``child`` (``Node``): The child to remove\n        '''\n        if not isinstance(child, Node):\n            raise TypeError(\"child must be a Node\")\n        try:\n            self.children.remove(child); child.parent = None\n        except:\n            raise RuntimeError(\"Attempting to remove non-existent child\")", "response": "Removes a child from the object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the parent of this Node object. Use this carefully otherwise you may damage the structure of this Node object.", "response": "def set_parent(self, parent):\n        '''Set the parent of this ``Node`` object. Use this carefully, otherwise you may damage the structure of this ``Tree`` object.\n\n        Args:\n            ``Node``: The new parent of this ``Node``\n        '''\n        if not isinstance(parent, Node):\n            raise TypeError(\"parent must be a Node\")\n        self.parent = parent"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef traverse_ancestors(self, include_self=True):\n        '''Traverse over the ancestors of this ``Node``\n\n        Args:\n            ``include_self`` (``bool``): ``True`` to include self in the traversal, otherwise ``False``\n        '''\n        if not isinstance(include_self, bool):\n            raise TypeError(\"include_self must be a bool\")\n        if include_self:\n            c = self\n        else:\n            c = self.parent\n        while c is not None:\n            yield c; c = c.parent", "response": "Traverse over the ancestors of this Node."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef traverse_bfs(self):\n        '''Perform a Breadth-First Search (BFS) starting at this ``Node`` object'. Yields (``Node``, distance) tuples\n        \n        Args:\n            ``include_self`` (``bool``): ``True`` to include self in the traversal, otherwise ``False``\n        '''\n        if not isinstance(include_self, bool):\n            raise TypeError(\"include_self must be a bool\")\n        q = deque(); dist = dict(); dist[self] = 0; q.append((self,0))\n        while len(q) != 0:\n            curr = q.popleft(); yield curr\n            for c in curr[0].children:\n                if c not in dist:\n                    if c.edge_length is None:\n                        el = 0\n                    else:\n                        el = c.edge_length\n                    dist[c] = dist[curr[0]] + el; q.append((c,dist[c]))\n            if curr[0].parent is not None and curr[0].parent not in dist:\n                if curr[0].edge_length is None:\n                    el = 0\n                else:\n                    el = curr[0].edge_length\n                dist[curr[0].parent] = dist[curr[0]] + el; q.append((curr[0].parent,dist[curr[0].parent]))", "response": "Perform a Breadth - First Search ( BFS ) starting at this object. Yields ( Node distance ) tuples\n        \n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef traverse_inorder(self, leaves=True, internal=True):\n        '''Perform an inorder traversal starting at this ``Node`` object\n\n        Args:\n            ``leaves`` (``bool``): ``True`` to include leaves, otherwise ``False``\n\n            ``internal`` (``bool``): ``True`` to include internal nodes, otherwise ``False``\n        '''\n        c = self; s = deque(); done = False\n        while not done:\n            if c is None:\n                if len(s) == 0:\n                    done = True\n                else:\n                    c = s.pop()\n                    if (leaves and c.is_leaf()) or (internal and not c.is_leaf()):\n                        yield c\n                    if len(c.children) == 0:\n                        c = None\n                    elif len(c.children) == 2:\n                        c = c.children[1]\n                    else:\n                        raise RuntimeError(INORDER_NONBINARY)\n            else:\n                s.append(c)\n                if len(c.children) == 0:\n                    c = None\n                elif len(c.children) == 2:\n                    c = c.children[0]\n                else:\n                    raise RuntimeError(INORDER_NONBINARY)", "response": "Perform an inorder traversal starting at this Node object\n           ."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nperforming a levelorder traversal starting at this node object", "response": "def traverse_levelorder(self, leaves=True, internal=True):\n        '''Perform a levelorder traversal starting at this ``Node`` object\n\n        Args:\n            ``leaves`` (``bool``): ``True`` to include leaves, otherwise ``False``\n\n            ``internal`` (``bool``): ``True`` to include internal nodes, otherwise ``False``\n        '''\n        q = deque(); q.append(self)\n        while len(q) != 0:\n            n = q.popleft()\n            if (leaves and n.is_leaf()) or (internal and not n.is_leaf()):\n                yield n\n            q.extend(n.children)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nperform a postorder traversal starting at this node object and yielding all the nodes that are in the tree.", "response": "def traverse_postorder(self, leaves=True, internal=True):\n        '''Perform a postorder traversal starting at this ``Node`` object\n\n        Args:\n            ``leaves`` (``bool``): ``True`` to include leaves, otherwise ``False``\n\n            ``internal`` (``bool``): ``True`` to include internal nodes, otherwise ``False``\n        '''\n        s1 = deque(); s2 = deque(); s1.append(self)\n        while len(s1) != 0:\n            n = s1.pop(); s2.append(n); s1.extend(n.children)\n        while len(s2) != 0:\n            n = s2.pop()\n            if (leaves and n.is_leaf()) or (internal and not n.is_leaf()):\n                yield n"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nperforming a preorder traversal starting at this Node object", "response": "def traverse_preorder(self, leaves=True, internal=True):\n        '''Perform a preorder traversal starting at this ``Node`` object\n\n        Args:\n            ``leaves`` (``bool``): ``True`` to include leaves, otherwise ``False``\n\n            ``internal`` (``bool``): ``True`` to include internal nodes, otherwise ``False``\n        '''\n        s = deque(); s.append(self)\n        while len(s) != 0:\n            n = s.pop()\n            if (leaves and n.is_leaf()) or (internal and not n.is_leaf()):\n                yield n\n            s.extend(n.children)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef traverse_rootdistorder(self, ascending=True, leaves=True, internal=True):\n        '''Perform a traversal of the ``Node`` objects in the subtree rooted at this ``Node`` in either ascending (``ascending=True``) or descending (``ascending=False``) order of distance from this ``Node``\n\n        Args:\n            ``ascending`` (``bool``): ``True`` to perform traversal in ascending distance from the root, otherwise ``False`` for descending\n\n            ``leaves`` (``bool``): ``True`` to include leaves, otherwise ``False``\n\n            ``internal`` (``bool``): ``True`` to include internal nodes, otherwise ``False``\n        '''\n        if not isinstance(ascending, bool):\n            raise TypeError(\"ascending must be a bool\")\n        nodes = list(); dist_from_root = dict()\n        for node in self.traverse_preorder():\n            if node == self:\n                d = 0\n            else:\n                d = dist_from_root[node.parent] + node.edge_length\n            dist_from_root[node] = d\n            if (leaves and node.is_leaf()) or (internal and not node.is_leaf()):\n                nodes.append((d,node))\n        nodes.sort(reverse=(not ascending))\n        for e in nodes:\n            yield e", "response": "Perform a traversal of the Node objects in the subtree rooted at this Node in either ascending or descending order of distance from the root."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef merge_with(self, other):\n        result = ValuesAggregation()\n        result.total = self.total + other.total\n        result.count = self.count + other.count\n        result.min = min(self.min, other.min)\n        result.max = max(self.max, other.max)\n        return result", "response": "Merge this ValuesAggregation with another ValuesAggregation"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nplotting the poloidal and toroidal projections of a list of Struct subclasses.", "response": "def Struct_plot(lS, lax=None, proj='all', element=None, dP=None,\n                dI=None, dBs=None, dBv=None,\n                dVect=None, dIHor=None, dBsHor=None, dBvHor=None,\n                Lim=None, Nstep=None, dLeg=None, indices=False,\n                draw=True, fs=None, wintit=None, tit=None, Test=True):\n    \"\"\" Plot the projections of a list of Struct subclass instances\n\n    D. VEZINET, Aug. 2014\n    Inputs :\n        V           A Ves instance\n        Nstep      An int (the number of points for evaluation of theta by np.linspace)\n        axP         A plt.Axes instance (if given) on which to plot the poloidal projection, otherwise ('None') a new figure/axes is created\n        axT         A plt.Axes instance (if given) on which to plot the toroidal projection, otherwise ('None') a new figure/axes is created\n        Tdict       A dictionnary specifying the style of the polygon plot\n        dLeg     A dictionnary specifying the style of the legend box (if None => no legend)\n    Outputs :\n        axP          The plt.Axes instance on which the poloidal plot was performed\n        axT          The plt.Axes instance on which the toroidal plot was performed\n    \"\"\"\n    proj = proj.lower()\n    if Test:\n        msg = \"Arg proj must be in ['cross','hor','all','3d'] !\"\n        assert proj in ['cross','hor','all','3d'], msg\n        lax, C0, C1, C2 = _check_Lax(lax,n=2)\n        assert type(draw) is bool, \"Arg draw must be a bool !\"\n\n    C0 = issubclass(lS.__class__, utils.ToFuObject)\n    C1 = (isinstance(lS,list)\n          and all([issubclass(ss.__class__, utils.ToFuObject) for ss in lS]))\n    msg = \"Arg lves must be a Struct subclass or a list of such !\"\n    assert C0 or C1, msg\n    if C0:\n        lS = [lS]\n    nS = len(lS)\n    if wintit is None:\n        wintit = _wintit\n\n    kwa = dict(fs=fs, wintit=wintit, Test=Test)\n    if proj=='3d':\n        # Temporary matplotlib issue\n        dLeg = None\n\n    for ii in  range(0,nS):\n        dplot = _Struct_plot_format(lS[ii], proj=proj, Elt=element,\n                                    dP=dP, dI=dI, dBs=dBs,\n                                    dBv=dBv, dVect=dVect, dIHor=dIHor,\n                                    dBsHor=dBsHor, dBvHor=dBvHor,\n                                    Lim=Lim, Nstep=Nstep)\n        for k in dplot.keys():\n            dplot[k].update(kwa)\n\n        if proj=='3d':\n            lax[0] = _Plot_3D_plt_Ves(lS[ii], ax=lax[0], LegDict=None,\n                                      draw=False, **dplot[proj])\n        else:\n            if proj=='cross':\n                lax[0] = _Plot_CrossProj_Ves(lS[ii], ax=lax[0],\n                                             indices=indices, LegDict=None,\n                                             draw=False, **dplot[proj])\n            elif proj=='hor':\n                lax[0] = _Plot_HorProj_Ves(lS[ii], ax=lax[0],\n                                           indices=indices, LegDict=None,\n                                           draw=False, **dplot[proj])\n            elif proj=='all':\n                if lax[0] is None or lax[1] is None:\n                    lax = list(_def.Plot_LOSProj_DefAxes('All', fs=fs,\n                                                         wintit=wintit,\n                                                         Type=lS[ii].Id.Type))\n                lax[0] = _Plot_CrossProj_Ves(lS[ii], ax=lax[0], LegDict=None,\n                                             indices=indices,\n                                             draw=False, **dplot['cross'])\n                lax[1] = _Plot_HorProj_Ves(lS[ii], ax=lax[1], LegDict=None,\n                                           indices=indices,\n                                           draw=False, **dplot['hor'])\n\n    # recompute the ax.dataLim\n    lax[0].relim()\n    if proj=='all':\n        lax[1].relim()\n    # update ax.viewLim using the new dataLim\n    lax[0].autoscale_view()\n    if proj=='all':\n        lax[1].autoscale_view()\n\n    if tit is not None:\n        lax[0].figure.suptitle(tit)\n\n    if not dLeg is None:\n        lax[0].legend(**dLeg)\n    if draw:\n        lax[0].relim()\n        lax[0].autoscale_view()\n        if len(lax)==2 and lax[1] is not None:\n            lax[1].relim()\n            lax[1].autoscale_view()\n        lax[0].figure.canvas.draw()\n    lax = lax if proj=='all' else lax[0]\n    return lax"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nplotting the poloidal projection of a single object.", "response": "def _Plot_CrossProj_Ves(V, ax=None, Elt='PIBsBvV',\n                        Pdict=_def.TorPd, Idict=_def.TorId, Bsdict=_def.TorBsd,\n                        Bvdict=_def.TorBvd, Vdict=_def.TorVind,\n                        LegDict=_def.TorLegd, indices=False,\n                        draw=True, fs=None, wintit=_wintit, Test=True):\n    \"\"\" Plot the poloidal projection of a Ves instance\n\n    Parameters\n    ----------\n        V       :   tfg.Ves / tfg.Struct\n            A Ves instance\n        ax      :   None / plt.Axes\n            A plt.Axes instance (if given) on which to plot, otherwise ('None') a new figure/axes is created\n        Pdict   :   dict\n            A dictionnary specifying the style of the polygon plot\n        LegDict :   None / dict\n            A dictionnary specifying the style of the legend box (if None => no legend)\n\n    Returns\n    -------\n        ax          The plt.Axes instance on which the plot was performed\n    \"\"\"\n    if Test:\n        ax, C0, C1, C2 = _check_Lax(ax,n=1)\n        assert type(Pdict) is dict, 'Arg Pdict should be a dictionary !'\n        assert type(Idict) is dict, \"Arg Idict should be a dictionary !\"\n        assert type(Bsdict) is dict, \"Arg Bsdict should be a dictionary !\"\n        assert type(Bvdict) is dict, \"Arg Bvdict should be a dictionary !\"\n        assert type(Vdict) is dict, \"Arg Vdict should be a dictionary !\"\n        assert type(LegDict) is dict or LegDict is None, 'Arg LegDict should be a dictionary !'\n        assert type(indices) is bool\n        if indices:\n            assert 'P' in Elt\n    if ax is None:\n        ax = _def.Plot_LOSProj_DefAxes('Cross', fs=fs,\n                                       wintit=wintit, Type=V.Id.Type)\n    if 'P' in Elt or 'V' in Elt:\n        P_closed = V.Poly_closed\n    if 'V' in Elt or indices:\n        midX = (P_closed[0,:-1]+P_closed[0,1:])/2.\n        midY = (P_closed[1,:-1]+P_closed[1,1:])/2.\n        VInX, VInY = V.dgeom['VIn'][0,:], V.dgeom['VIn'][1,:]\n    if 'P' in Elt:\n        if V._InOut=='in':\n            ax.plot(P_closed[0,:], P_closed[1,:],\n                    label=V.Id.NameLTX,**Pdict)\n        elif V._InOut=='out':\n            ax.add_patch(mPolygon(V.Poly.T, closed=True,\n                                  label=V.Id.NameLTX, **Pdict))\n        else:\n            msg = \"self._InOut not defined !\"\n            raise Exception(msg)\n    if 'I' in Elt:\n        ax.plot(V.dsino['RefPt'][0], V.dsino['RefPt'][1],\n                label=V.Id.NameLTX+\" Imp\", **Idict)\n    if 'Bs' in Elt:\n        ax.plot(V.dgeom['BaryS'][0], V.dgeom['BaryS'][1],\n                label=V.Id.NameLTX+\" Bs\", **Bsdict)\n    if 'Bv' in Elt and V.Id.Type=='Tor':\n        ax.plot(V.dgeom['BaryV'][0], V.dgeom['BaryV'][1],\n                label=V.Id.NameLTX+\" Bv\", **Bvdict)\n    if 'V' in Elt:\n        ax.quiver(midX, midY, VInX, VInY,\n                  angles='xy', scale_units='xy',\n                  label=V.Id.NameLTX+\" Vin\", **Vdict)\n    if indices:\n        for ii in range(0,V.dgeom['nP']):\n            ax.annotate(r\"{0}\".format(ii), size=10,\n                        xy = (midX[ii],midY[ii]),\n                        xytext = (midX[ii]-0.01*VInX[ii],\n                                  midY[ii]-0.01*VInY[ii]),\n                        horizontalalignment='center',\n                        verticalalignment='center')\n    if not LegDict is None:\n        ax.legend(**LegDict)\n    if draw:\n        ax.relim()\n        ax.autoscale_view()\n        ax.figure.canvas.draw()\n    return ax"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _Plot_HorProj_Ves(V, ax=None, Elt='PI', Nstep=_def.TorNTheta,\n                      Pdict=_def.TorPd, Idict=_def.TorITord,\n                      Bsdict=_def.TorBsTord, Bvdict=_def.TorBvTord,\n                      LegDict=_def.TorLegd, indices=False,\n                      draw=True, fs=None, wintit=_wintit, Test=True):\n    \"\"\" Plotting the toroidal projection of a Ves instance\n\n    Parameters\n    ----------\n        V           A Ves instance\n        Nstep      An int (the number of points for evaluation of theta by np.linspace)\n        ax          A plt.Axes instance (if given) on which to plot, otherwise ('None') a new figure/axes is created\n        Tdict       A dictionnary specifying the style of the polygon plot\n        LegDict     A dictionnary specifying the style of the legend box (if None => no legend)\n\n    Returns\n    -------\n        ax          The plt.Axes instance on which the plot was performed\n    \"\"\"\n    if Test:\n        assert type(Nstep) is int\n        ax, C0, C1, C2 = _check_Lax(ax,n=1)\n        assert type(Pdict) is dict, 'Arg Pdict should be a dictionary !'\n        assert type(Idict) is dict, 'Arg Idict should be a dictionary !'\n        assert type(LegDict) is dict or LegDict is None, 'Arg LegDict should be a dictionary !'\n\n    if ax is None:\n        ax = _def.Plot_LOSProj_DefAxes('Hor', Type=V.Id.Type,\n                                       fs=fs, wintit=wintit)\n    P1Min = V.dgeom['P1Min']\n    P1Max = V.dgeom['P1Max']\n    if 'P' in Elt:\n        if V._InOut=='in':\n            if V.Id.Type=='Tor':\n                Theta = np.linspace(0, 2*np.pi, num=Nstep,\n                                    endpoint=True, retstep=False)\n                lx = np.concatenate((P1Min[0]*np.cos(Theta),np.array([np.nan]),\n                                     P1Max[0]*np.cos(Theta)))\n                ly = np.concatenate((P1Min[0]*np.sin(Theta),np.array([np.nan]),\n                                     P1Max[0]*np.sin(Theta)))\n            elif V.Id.Type=='Lin':\n                lx = np.array([V.Lim[0,0],V.Lim[0,1],V.Lim[0,1],\n                               V.Lim[0,0],V.Lim[0,0]])\n                ly = np.array([P1Min[0],P1Min[0],P1Max[0],P1Max[0],P1Min[0]])\n            ax.plot(lx,ly,label=V.Id.NameLTX,**Pdict)\n        elif V._InOut=='out':\n            if V.Id.Type=='Tor':\n                Theta = np.linspace(0, 2*np.pi, num=Nstep,\n                                    endpoint=True, retstep=False)\n                if V.noccur==0:\n                    lx = np.concatenate((P1Min[0]*np.cos(Theta),\n                                         P1Max[0]*np.cos(Theta[::-1])))\n                    ly = np.concatenate((P1Min[0]*np.sin(Theta),\n                                         P1Max[0]*np.sin(Theta[::-1])))\n                    Lp = [mPolygon(np.array([lx,ly]).T, closed=True,\n                                   label=V.Id.NameLTX, **Pdict)]\n                else:\n                    Lp = [mWedge((0,0), P1Max[0],\n                                 V.Lim[ii][0]*180./np.pi,\n                                 V.Lim[ii][1]*180./np.pi,\n                                 width=P1Max[0]-P1Min[0],\n                                 label=V.Id.NameLTX, **Pdict)\n                          for ii in range(0,len(V.Lim))]\n            elif V.Id.Type=='Lin':\n                    ly = np.array([P1Min[0],P1Min[0],\n                                   P1Max[0],P1Max[0],P1Min[0]])\n                    Lp = []\n                    for ii in range(0,len(V.Lim)):\n                        lx = np.array([V.Lim[ii][0],V.Lim[ii][1],\n                                       V.Lim[ii][1],V.Lim[ii][0],\n                                       V.Lim[ii][0]])\n                        Lp.append(mPolygon(np.array([lx,ly]).T,\n                                           closed=True, label=V.Id.NameLTX,\n                                           **Pdict))\n            for pp in Lp:\n                ax.add_patch(pp)\n        else:\n            msg = \"Unknown self._InOut !\"\n            raise Exception(msg)\n\n    if 'I' in Elt:\n        if V.Id.Type=='Tor':\n            lx = V.dsino['RefPt'][0]*np.cos(Theta)\n            ly = V.dsino['RefPt'][0]*np.sin(Theta)\n        elif V.Id.Type=='Lin':\n            lx = np.array([np.min(V.Lim),np.max(V.Lim)])\n            ly = V.dsino['RefPt'][0]*np.ones((2,))\n        ax.plot(lx,ly,label=V.Id.NameLTX+\" Imp\",**Idict)\n    if 'Bs' in Elt:\n        if V.Id.Type=='Tor':\n            lx = V.dgeom['BaryS'][0]*np.cos(Theta)\n            ly = V.dgeom['BaryS'][0]*np.sin(Theta)\n        elif V.Id.Type=='Lin':\n            lx = np.array([np.min(V.Lim),np.max(V.Lim)])\n            ly = V.dgeom['BaryS'][0]*np.ones((2,))\n        ax.plot(lx,ly,label=V.Id.NameLTX+\" Bs\", **Bsdict)\n    if 'Bv' in Elt and V.Type=='Tor':\n        lx = V.dgeom['BaryV'][0]*np.cos(Theta)\n        ly = V.dgeom['BaryV'][0]*np.sin(Theta)\n        ax.plot(lx,ly,label=V.Id.NameLTX+\" Bv\", **Bvdict)\n\n    if indices and V.noccur>1:\n        if V.Id.Type=='Tor':\n            for ii in range(0,V.noccur):\n                R, theta = V.dgeom['P1Max'][0], np.mean(V.Lim[ii])\n                X, Y = R*np.cos(theta), R*np.sin(theta)\n                ax.annotate(r\"{0}\".format(ii), size=10,\n                            xy = (X,Y),\n                            xytext = (X+0.02*np.cos(theta),\n                                      Y+0.02*np.sin(theta)),\n                            horizontalalignment='center',\n                            verticalalignment='center')\n        elif V.Id.Type=='Lin':\n            for ii in range(0,V.noccur):\n                X, Y = np.mean(V.Lim[ii]), V.dgeom['P1Max'][0]\n                ax.annotate(r\"{0}\".format(ii), size=10,\n                            xy = (X,Y),\n                            xytext = (X, Y+0.02),\n                            horizontalalignment='center',\n                            verticalalignment='center')\n\n    if not LegDict is None:\n        ax.legend(**LegDict)\n    if draw:\n        ax.relim()\n        ax.autoscale_view()\n        ax.figure.canvas.draw()\n    return ax", "response": "Plots the toroidal projection of a Ves instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nplots the toroidal projection of a structure.", "response": "def Plot_Impact_PolProjPoly(lS, Leg=\"\", ax=None, Ang='theta', AngUnit='rad',\n                            Sketch=True, dP=None,\n                            dLeg=_def.TorLegd, draw=True, fs=None,\n                            wintit=None, tit=None, Test=True):\n    \"\"\" Plotting the toroidal projection of a Ves instance\n\n    D. VEZINET, Aug. 2014\n    Inputs :\n        T           A Ves instance\n        Leg         A str (the legend label to be used if T is not a Ves instance)\n        ax          A plt.Axes instance (if given) on which to plot the projection space, otherwise ('None') a new figure/axes is created\n        Dict        A dictionnary specifying the style of the boundary polygon plot\n        dLeg        A dictionnary specifying the style of the legend box\n    Outputs :\n        ax          The plt.Axes instance on which the poloidal plot was performed\n    \"\"\"\n    if Test:\n        Lax, C0, C1, C2 = _check_Lax(ax,n=1)\n        assert C0 or C1, 'Arg ax should a plt.Axes instance !'\n        assert dP is None or type(dP) is dict, \"Arg dP must be a dictionary !\"\n        assert dLeg is None or type(dLeg) is dict, \"Arg dLeg must be a dictionary !\"\n        assert Ang in ['theta','xi'], \"Arg Ang must be in ['theta','xi'] !\"\n        assert AngUnit in ['rad','deg'], \"Arg AngUnit must be in ['rad','deg'] !\"\n    C0 = issubclass(lS.__class__, utils.ToFuObject)\n    C1 = (isinstance(lS,list)\n          and all([issubclass(ss.__class__, utils.ToFuObject) for ss in lS]))\n    msg = \"Arg lves must be a Struct subclass or a list of such !\"\n    assert C0 or C1, msg\n    if C0:\n        lS = [lS]\n    nS = len(lS)\n\n    # Get Sketch\n    if ax is None:\n        if wintit is None:\n            wintit = _wintit\n        ax, axsketch = _def.Plot_Impact_DefAxes('Cross', fs=fs, wintit=wintit,\n                                                Ang=Ang, AngUnit=AngUnit,\n                                                Sketch=Sketch)\n\n    if dP is not None:\n        dp = dP\n\n    # Get up/down limits\n    pPmax, pPmin = 0, 0\n    for ss in lS:\n        pmax = np.max(ss.dsino['EnvMinMax'])\n        if pmax>pPmax:\n            pPmax = pmax\n        pmin = np.min(ss.dsino['EnvMinMax'])\n        if pmin<pPmin:\n            pPmin = pmin\n    if nS>0:\n        DoUp = (pPmin,pPmax)\n        nP = pmax.size\n\n    handles, labels = ax.get_legend_handles_labels()\n    for ii in range(0,nS):\n\n        Theta, pP = lS[ii].dsino['EnvTheta'], lS[ii].dsino['EnvMinMax'][0,:]\n        pN = lS[ii].dsino['EnvMinMax'][1,:]\n        if Ang=='xi':\n            Theta, pP, pN = _GG.ConvertImpact_Theta2Xi(Theta, pP, pN)\n        Theta = Theta.ravel()\n\n        if dP is None:\n            dp = {'facecolor':lS[ii].get_color(), 'edgecolor':'k',\n                  'linewidth':1., 'linestyle':'-'}\n\n        if lS[ii]._InOut=='in':\n            ax.fill_between(Theta, pP, DoUp[1]*np.ones((nP,)),**dp)\n            ax.fill_between(Theta, DoUp[0]*np.ones((nP,)), pN,**dp)\n        elif lS[ii]._InOut=='out':\n            ax.fill_between(Theta, pP, pN, **dp)\n        else:\n            msg = \"self._InOut not defined for {0}\".format(lS[ii].Id.Cls)\n            raise Exception(msg)\n        proxy = plt.Rectangle((0,0),1,1, fc=dp['facecolor'])\n        handles.append(proxy)\n        labels.append(lS[ii].Id.Cls+' '+lS[ii].Id.Name)\n\n    if nS>0:\n        ax.set_ylim(DoUp)\n    if not dLeg is None:\n        ax.legend(handles,labels,**dLeg)\n    if draw:\n        ax.figure.canvas.draw()\n    return ax"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef Plot_Impact_3DPoly(T, Leg=\"\", ax=None, Ang=_def.TorPAng,\n                       AngUnit=_def.TorPAngUnit, Pdict=_def.TorP3DFilld,\n                       dLeg=_def.TorLegd,\n                       draw=True, fs=None, wintit=_wintit, Test=True):\n    \"\"\" Plotting the toroidal projection of a Ves instance\n\n    D. VEZINET, Aug. 2014\n    Inputs :\n        T           A Ves instance\n        Leg         A str (the legend label to be used if T is not a Ves instance)\n        ax          A plt.Axes instance (if given) on which to plot the projection space, otherwise ('None') a new figure/axes is created\n        Dict        A dictionnary specifying the style of the boundary polygon plot\n        dLeg        A dictionnary specifying the style of the legend box\n    Outputs :\n        ax          The plt.Axes instance on which the poloidal plot was performed\n    \"\"\"\n\n    if Test:\n        assert T.Id.Cls in ['Ves','Struct'] or (isinstance(T,tuple) and len(T)==3), \"Arg T must be Ves instance or tuple with (Theta,pP,pN) 3 ndarrays !\"\n        assert isinstance(ax,Axes3D) or ax is None, \"Arg ax must be a Axes instance !\"\n        assert type(Pdict) is dict, \"Arg Pdict must be a dictionary !\"\n        assert type(dLeg) is dict or dLeg is None, \"Arg dLeg must be a dictionary !\"\n        assert Ang in ['theta','xi'], \"Arg Ang must be in ['theta','xi'] !\"\n        assert AngUnit in ['rad','deg'], \"Arg AngUnit must be in ['rad','deg'] !\"\n    if ax is None:\n        ax = _def.Plot_Impact_DefAxes('3D', fs=fs, wintit=wintit)\n    handles, labels = ax.get_legend_handles_labels()\n    if isinstance(T,Ves):\n        Leg = T.Id.NameLTX\n        Theta, pP, pN = T._Imp_EnvTheta, T._Imp_EnvMinMax[0,:], T._Imp_EnvMinMax[1,:]\n    else:\n        assert isinstance(T[0],np.ndarray) and isinstance(T[1],np.ndarray) and isinstance(T[2],np.ndarray), \"Args Theta, pP and pN should be np.ndarrays !\"\n        assert T[0].shape==T[1].shape==T[2].shape, \"Args Theta, pP and pN must have same shape !\"\n        Theta, pP, pN = T\n    AngName = r\"$\\theta$\"\n    if Ang=='xi':\n        Theta, pP, pN = _GG.ConvertImpact_Theta2Xi(Theta, pP, pN)\n        AngName = r\"$\\xi$\"\n    yDoUp, zDoUp = ax.get_ylim(), ax.get_zlim()\n    x = np.outer(Theta.flatten(),np.ones(zDoUp.shape))\n    yP = np.outer(pP.flatten(),np.ones(zDoUp.shape))\n    yN = np.outer(pN.flatten(),np.ones(zDoUp.shape))\n    z = np.outer(np.ones(pP.flatten().shape),zDoUp)\n    ax.plot_surface(x,yP,z,rstride=1,cstride=1,label=Leg,**Pdict)\n    ax.plot_surface(x,yN,z,rstride=1,cstride=1,label=Leg,**Pdict)\n    proxy = plt.Rectangle((0,0),1,1, fc=Pdict['color'])\n    handles.append(proxy)\n    labels.append(Leg)\n    ax.set_xticks([0,np.pi/4.,np.pi/2.,3.*np.pi/4.,np.pi])\n    ax.set_zticks([-np.pi/2.,-np.pi/4.,0.,np.pi/4.,np.pi/2.])\n    if AngUnit=='rad':\n        ax.set_xticklabels([r\"$0$\",r\"$\\pi/4$\",r\"$\\pi/2$\",r\"$3\\pi/4$\",r\"$\\pi$\"])\n        ax.set_zticklabels([r\"$-\\pi/2$\",r\"$-\\pi/4$\",r\"$0$\",r\"$\\pi/4$\",r\"$\\pi/2$\"])\n        AngUnit = r\"$(rad.)$\"\n    elif AngUnit=='deg':\n        ax.set_xticklabels([r\"$0$\",r\"$90$\",r\"$180$\",r\"$270$\",r\"$360$\"])\n        ax.set_zticklabels([r\"$-180$\",r\"$-90$\",r\"$0$\",r\"$90$\",r\"$180$\"])\n        AngUnit = r\"$(deg.)$\"\n    ax.set_xlabel(AngName+r\" \"+AngUnit)\n    ax.set_zlabel(r\"$\\phi$ \"+AngUnit)\n    if not dLeg is None:\n        ax.legend(handles,labels,**dLeg)\n    if draw:\n        ax.figure.canvas.draw()\n    return ax", "response": "Plots the toroidal projection of a Ves instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_url_rules(urls_fp):\n    url_rules = []\n    for line in urls_fp:\n        re_url = line.strip()\n        if re_url:\n            url_rules.append({'str': re_url, 're': re.compile(re_url)})\n    return url_rules", "response": "Parse the URL rules from given fp"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef force_bytes(s, encoding='utf-8', errors='strict'):\n    # Handle the common case first for performance reasons.\n    if isinstance(s, bytes):\n        if encoding == 'utf-8':\n            return s\n        else:\n            return s.decode('utf-8', errors).encode(encoding, errors)\n    else:\n        return s.encode(encoding, errors)", "response": "A function turns s into bytes object similar to django. utils. encoding. force_bytes\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef force_text(s, encoding='utf-8',  errors='strict'):\n    if issubclass(type(s), str):\n        return s\n    try:\n        if isinstance(s, bytes):\n            s = str(s, encoding, errors)\n        else:\n            s = str(s)\n    except UnicodeDecodeError as e:\n        raise DjangoUnicodeDecodeError(s, *e.args)\n    return s", "response": "A function turns s into text type similar to django. utils. encoding. force_text\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_notes():\n\n    notes = {'DPhi':{}, 'dPhi':{}}\n    # Toroidal width (mm, inner outer)\n    notes['DPhi']['In'] = 26.370\n    notes['DPhi']['Out'] = 31.929\n\n    # Inter tiles distance (mm, uniform)\n    notes['dl'] = 0.500\n\n    # Poloidal/Radial total length (mm)\n    notes['DL'] = 437.000\n\n    # Number of tiles radially\n    notes['nb'] = 35\n    notes['nbPhi'] = 19*2*12\n\n    # Radial length of a tile (mm, uniform)\n    notes['Dl'] = 12.000\n\n    # Vertical height of tiles (mm, uniform)\n    notes['DZ'] = 26.000\n\n    # Toroidal space between needles (mm, inner outer)\n    notes['dPhi']['In'] = 0.588\n    notes['dPhi']['Out'] = 0.612\n\n    # (X,Z,Y) polygon of one needle (mm) !!!!!! (X,Z,Y)\n    # 1 mm should be added towards Z>0 in the direction normal to the divertor's upper surface\n    notes['sampleXZY'] = [[-759.457, -625.500, -1797.591], # Old start point\n                          [-759.603, -624.572, -1797.936], # Only for pattern\n                          [-772.277, -620.864, -1794.112],\n                          [-761.681, -610.036, -1769.498], # Computed,tube/plane\n                          [-761.895, -620.231, -1764.921],\n                          [-751.095, -609.687, -1741.154],\n                          [-755.613, -580.944, -1751.852],\n                          [-766.413, -591.488, -1775.620], # Edge of plane\n                          [-763.902, -596.129, -1774.659], # Computed,tube/plane\n                          [-774.498, -606.956, -1799.274], # Middle top of tube\n                          [-763.246, -601.395, -1806.563],\n                          [-767.575, -605.891, -1816.813],\n                          [-763.932, -629.068, -1808.186],\n                          [-764.112, -629.255, -1808.613],\n                          [-767.755, -606.078, -1817.240],\n                          [-772.084, -610.573, -1827.490],\n                          [-768.441, -633.750, -1818.863],\n                          [-768.622, -633.938, -1819.290],\n                          [-772.265, -610.760, -1827.917],\n                          [-776.594, -615.256, -1838.167],\n                          [-772.950, -638.433, -1829.540],\n                          [-773.131, -638.620, -1829.967],\n                          [-776.774, -615.443, -1838.594],\n                          [-781.103, -619.938, -1848.844],\n                          [-777.460, -643.115, -1840.217],\n                          [-777.640, -643.303, -1840.644],\n                          [-781.283, -620.126, -1849.271],\n                          [-785.612, -624.621, -1859.520],\n                          [-781.969, -647.798, -1850.894],\n                          [-782.149, -647.985, -1851.321],\n                          [-785.793, -624.808, -1859.948],\n                          [-790.122, -629.303, -1870.197],\n                          [-786.478, -652.481, -1861.571],\n                          [-786.659, -652.668, -1861.998],\n                          [-790.302, -629.491, -1870.624],\n                          [-794.631, -633.986, -1880.874],\n                          [-790.988, -657.163, -1872.248],\n                          [-791.168, -657.351, -1872.675],\n                          [-794.811, -634.173, -1881.301]]\n    notes['sampleXZY'] = np.array(notes['sampleXZY'])\n\n    for kk in notes.keys():\n        if type(notes[kk]) is dict:\n            notes[kk]['In'] = notes[kk]['In']*1.e-3\n            notes[kk]['Out'] = notes[kk]['Out']*1.e-3\n        elif not 'nb' in kk:\n            notes[kk] = notes[kk]*1.e-3\n    return notes", "response": "Returns a dictionary of notes for a single base - order is important"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset or update the dchans dict of the nchans containing the current object.", "response": "def set_dchans(self, dchans=None, method='set'):\n        \"\"\" Set (or update) the dchans dict\n\n        dchans is a dict of np.ndarrays of len() = self.nch containing\n        channel-specific information\n\n        Use the kwarg 'method' to set / update the dict\n\n        \"\"\"\n        assert method in ['set','update']\n        dchans = self._checkformat_inputs_dchans(dchans=dchans)\n        if method == 'set':\n            self._dchans = dchans\n        else:\n            self._dchans.update(dchans)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets or update the dextra dict of the object", "response": "def set_dextra(self, dextra=None, method='set'):\n        \"\"\" Set (or update) the dextra dict\n\n        dextra is a dict of nested dict\n        It contains all extra signal that can help interpret the data\n            e.g.: heating power time traces, plasma current...\n        Each nested dict should have the following fields:\n            't'    : 1D np.ndarray (time vector)\n            'data' : 1D np.ndarray (data time trace)\n            'name' : str (used as label in legend)\n            'units': str (used n parenthesis in legend after name)\n\n        Use the kwarg 'method' to set / update the dict\n\n        \"\"\"\n        assert method in ['set','update']\n        dextra = self._checkformat_inputs_dextra(dextra=dextra)\n        if method == 'set':\n            self._dextra = dextra\n        else:\n            self._dextra.update(dextra)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_XRef(self, X=None, indtX=None, indtXlamb=None):\n        out = self._checkformat_inputs_XRef(X=X, indtX=indtX,\n                                            indXlamb=indtXlamb)\n        X, nnch, indtX, indXlamb, indtXlamb = out\n        self._ddataRef['X'] = X\n        self._ddataRef['nnch'] = nnch\n        self._ddataRef['indtX'] = indtX\n        self._ddataRef['indtXlamb'] = indtXlamb\n        self._ddata['uptodate'] = False", "response": "Reset the reference X to the given values."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_dtreat_indt(self, t=None, indt=None):\n        lC = [indt is not None, t is not None]\n        if all(lC):\n            msg = \"Please provide either t or indt (or none)!\"\n            raise Exception(msg)\n\n        if lC[1]:\n            ind = self.select_t(t=t, out=bool)\n        else:\n            ind = _format_ind(indt, n=self._ddataRef['nt'])\n        self._dtreat['indt'] = ind\n        self._ddata['uptodate'] = False", "response": "Store the desired index array for the time vector"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nstore the desired index array for the channels", "response": "def set_dtreat_indch(self, indch=None):\n        \"\"\" Store the desired index array for the channels\n\n        If None => all channels\n        Must be a 1d array\n\n        \"\"\"\n        if indch is not None:\n            indch = np.asarray(indch)\n            assert indch.ndim==1\n        indch = _format_ind(indch, n=self._ddataRef['nch'])\n        self._dtreat['indch'] = indch\n        self._ddata['uptodate'] = False"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nstores the desired index array for the DataSpectral object.", "response": "def set_dtreat_indlamb(self, indlamb=None):\n        \"\"\" Store the desired index array for the wavelength\n\n        If None => all wavelengths\n        Must be a 1d array\n\n        \"\"\"\n        if not self._isSpectral():\n            msg = \"The wavelength can only be set with DataSpectral object !\"\n            raise Exception(msg)\n        if indlamb is not None:\n            indlamb = np.asarray(indlamb)\n            assert indlamb.ndim==1\n            indlamb = _format_ind(indlamb, n=self._ddataRef['nlamb'])\n        self._dtreat['indlamb'] = indlamb\n        self._ddata['uptodate'] = False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_dtreat_interp_indt(self, indt=None):\n        lC = [indt is None, type(indt) in [np.ndarray,list], type(indt) is dict]\n        assert any(lC)\n        if lC[2]:\n            lc = [type(k) is int and k<self._ddataRef['nch'] for k in indt.keys()]\n            assert all(lc)\n            for k in indt.keys():\n                assert hasattr(indt[k],'__iter__')\n                indt[k] = _format_ind(indt[k], n=self._ddataRef['nt'])\n        elif lC[1]:\n            indt = np.asarray(indt)\n            assert indt.ndim==1\n            indt = _format_ind(indt, n=self._ddataRef['nt'])\n        self._dtreat['interp-indt'] = indt\n        self._ddata['uptodate'] = False", "response": "Set the indices of the data at which to interpolate data at the specified times."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_dtreat_interp_indch(self, indch=None):\n        lC = [indch is None, type(indch) in [np.ndarray,list], type(indch) is dict]\n        assert any(lC)\n        if lC[2]:\n            lc = [type(k) is int and k<self._ddataRef['nt'] for k in indch.keys()]\n            assert all(lc)\n            for k in indch.keys():\n                assert hasattr(indch[k],'__iter__')\n                indch[k] = _format_ind(indch[k], n=self._ddataRef['nch'])\n        elif lC[1]:\n            indch = np.asarray(indch)\n            assert indch.ndim==1\n            indch = _format_ind(indch, n=self._ddataRef['nch'])\n        self._dtreat['interp-indch'] = indch\n        self._ddata['uptodate'] = False", "response": "Set the indices of the channels which to interpolate at the times and channels for which to interpolate data at these channels."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the fitting dictionnary of the data for the current object", "response": "def set_dtreat_dfit(self, dfit=None):\n        \"\"\" Set the fitting dictionnary\n\n        A dict contaning all parameters for fitting the data\n        Valid dict content includes:\n            - 'type': str\n                'fft':  A fourier filtering\n                'svd':  A svd filtering\n\n        \"\"\"\n        warnings.warn(\"Not implemented yet !, dfit forced to None\")\n        dfit = None\n\n        assert dfit is None or isinstance(dfit,dict)\n        if isinstance(dfit,dict):\n            assert 'type' in dfit.keys()\n            assert dfit['type'] in ['svd','fft']\n\n        self._dtreat['dfit'] = dfit\n        self._ddata['uptodate'] = False"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_dtreat_interpt(self, t=None):\n        if t is not None:\n            t = np.unique(np.asarray(t, dtype=float).ravel())\n        self._dtreat['interp-t'] = t", "response": "Set the time vector on which to interpolate the data"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_dtreat_order(self, order=None):\n        if order is None:\n            order = list(self._ddef['dtreat']['order'])\n        assert type(order) is list and all([type(ss) is str for ss in order])\n        if not all([ss in ['indt','indch','indlamb'] for ss in order][-4:-1]):\n            msg = \"indt and indch must be the treatment steps -2 and -3 !\"\n            raise Exception(msg)\n        if not order[-1]=='interp-t':\n            msg = \"interp-t must be the last treatment step !\"\n            raise Exception(msg)\n        self._dtreat['order'] = order\n        self._ddata['uptodate'] = False", "response": "Set the order in which the data treatment steps should be performed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_treated_data(self):\n        # --------------------\n        # Copy reference data\n        d = self._ddataRef['data'].copy()\n        t, X = self._ddataRef['t'].copy(), self._ddataRef['X'].copy()\n        lamb = self._ddataRef['lamb']\n        if lamb is not None:\n            lamb = lamb.copy()\n\n        indtX = self._ddataRef['indtX']\n        if indtX is not None:\n            indtX = indtX.copy()\n        indtlamb = self._ddataRef['indtlamb']\n        if indtlamb is not None:\n            indtlamb = indtlamb.copy()\n        indXlamb = self._ddataRef['indXlamb']\n        if indXlamb is not None:\n            indXlamb = indXlamb.copy()\n        indtXlamb = self._ddataRef['indtXlamb']\n        if indtXlamb is not None:\n            indtXlamb = indtXlamb.copy()\n\n        # --------------------\n        # Apply data treatment\n        for kk in self._dtreat['order']:\n            # data only\n            if kk=='mask' and self._dtreat['mask-ind'] is not None:\n                d = self._mask(d, self._dtreat['mask-ind'],\n                               self._dtreat['mask-val'])\n            if kk=='interp_indt':\n                d = self._interp_indt(d, self._dtreat['interp-indt'],\n                                      self._ddataRef['t'])\n            if kk=='interp_indch':\n                d = self._interp_indch(d, self._dtreat['interp-indch'],\n                                       self._ddataRef['X'])\n            if kk=='data0':\n                d = self._data0(d, self._dtreat['data0-data'])\n            if kk=='dfit' and self._dtreat['dfit'] is not None:\n                d = self._dfit(d, **self._dtreat['dfit'])\n\n            # data + others\n            if kk=='indt' and self._dtreat['indt'] is not None:\n                d,t, indtX,indtlamb,indtXlamb = self._indt(d, t, indtX,\n                                                           indtlamb, indtXlamb,\n                                                           self._dtreat['indt'])\n            if kk=='indch' and self._dtreat['indch'] is not None:\n                d,X, indXlamb,indtXlamb = self._indch(d, X, indXlamb, indtXlamb,\n                                                      self._dtreat['indch'])\n            if kk=='indlamb' and self._dtreat['indlamb'] is not None:\n                d, lamb = self._indch(d, lamb, self._dtreat['indlamb'])\n            if kk=='interp_t' and self._dtreat['interp-t'] is not None:\n                d,t, indtX,indtlamb,indtXlamb\\\n                        = self._interp_t(d, t, indtX, indtlamb, indtXlamb,\n                                         self._dtreat['interp-t'], kind='linear')\n        # --------------------\n        # Safety check\n        if d.ndim==2:\n            (nt, nch), nlamb = d.shape, 0\n        else:\n            nt, nch, nlamb = d.shape\n        assert d.ndim in [2,3]\n        assert t.shape==(nt,)\n        assert X.shape==(self._ddataRef['nnch'], nch)\n        if lamb is not None:\n            assert lamb.shape==(self._ddataRef['nnlamb'], nlamb)\n\n        lout = [d, t, X, lamb, nt, nch, nlamb,\n                indtX, indtlamb, indXlamb, indtXlamb]\n        return lout", "response": "Return a copy of the data for the current treatment and the data for the current treatment."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nclear the working copy of data Harmless as it preserves the reference copy and the treatment of the memory", "response": "def clear_ddata(self):\n        \"\"\" Clear the working copy of data\n\n        Harmless, as it preserves the reference copy and the treatment dict\n        Use only to free some memory\n\n        \"\"\"\n        self._ddata = dict.fromkeys(self._get_keys_ddata())\n        self._ddata['uptodate'] = False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef clear_dtreat(self, force=False):\n        lC = [self._dtreat[k] is not None for k in self._dtreat.keys()\n              if k != 'order']\n        if any(lC) and not force:\n            msg = \"\"\"BEWARE : You are about to delete the data treatment\n                              i.e.: to clear self.dtreat (and also self.ddata)\n                              Are you sure ?\n                              If yes, use self.clear_dtreat(force=True)\"\"\"\n            raise Exception(msg)\n        dtreat = dict.fromkeys(self._get_keys_dtreat())\n        self._dtreat = self._checkformat_inputs_dtreat(dtreat)\n        self.clear_ddata()", "response": "Clear all treatment parameters in self. dtreat and also clear the working copy of data."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef dchans(self, key=None):\n        if self._dtreat['indch'] is None or np.all(self._dtreat['indch']):\n            dch = dict(self._dchans) if key is None else self._dchans[key]\n        else:\n            dch = {}\n            lk = self._dchans.keys() if key is None else [key]\n            for kk in lk:\n                if self._dchans[kk].ndim==1:\n                    dch[kk] = self._dchans[kk][self._dtreat['indch']]\n                elif self._dchans[kk].ndim==2:\n                    dch[kk] = self._dchans[kk][:,self._dtreat['indch']]\n                else:\n                    msg = \"Don't know how to treat self._dchans[%s]:\"%kk\n                    msg += \"\\n  shape = %s\"%(kk,str(self._dchans[kk].shape))\n                    warnings.warn(msg)\n            if key is not None:\n                dch = dch[key]\n        return dch", "response": "Return the dchans updated with indch\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef select_t(self, t=None, out=bool):\n        assert out in [bool,int]\n        ind = _select_ind(t, self._ddataRef['t'], self._ddataRef['nt'])\n        if out is int:\n            ind = ind.nonzero()[0]\n        return ind", "response": "Select a time index array or integer index array for a given time vector"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef select_ch(self, val=None, key=None, log='any', touch=None, out=bool):\n        assert out in [int,bool]\n        assert log in ['any','all','not']\n        lc = [val is None, key is None, touch is None]\n        lC = [all(lc), all(lc[:2]) and not lc[2],\n              not lc[0] and all(lc[1:]), not any(lc[:2]) and lc[2]]\n        assert np.sum(lC)==1\n\n        if lC[0]:\n            # get all channels\n            ind = np.ones((self._ddataRef['nch'],),dtype=bool)\n\n        elif lC[1]:\n            # get touch\n            if self._dgeom['lCam'] is None:\n                msg = \"self.dgeom['lCam'] must be set to use touch !\"\n                raise Exception(msg)\n            if any([type(cc) is str for cc in self._dgeom['lCam']]):\n                msg = \"self.dgeom['lCam'] contains pathfiles !\"\n                msg += \"\\n  => Run self.strip(0)\"\n                raise Exception(msg)\n            ind = []\n            for cc in self._dgeom['lCam']:\n                ind.append(cc.select(touch=touch, log=log, out=bool))\n            if len(ind)==1:\n                ind = ind[0]\n            else:\n                ind = np.concatenate(tuple(ind))\n\n        elif lC[2]:\n            # get values on X\n            if self._ddataRef['nnch']==1:\n                ind = _select_ind(val, self._ddataRef['X'], self._ddataRef['nch'])\n            else:\n                ind = np.zeros((self._ddataRef['nt'],self._ddataRef['nch']),dtype=bool)\n                for ii in range(0,self._ddataRef['nnch']):\n                    iind = self._ddataRef['indtX']==ii\n                    ind[iind,:] =  _select_ind(val, self._ddataRef['X'],\n                                               self._ddataRef['nch'])[np.newaxis,:]\n\n        else:\n            if not (type(key) is str and key in self._dchans.keys()):\n                msg = \"Provided key not valid!\\n\"\n                msg += \"    - key: %s\\n\"%str(key)\n                msg += \"Please provide a valid key of self.dchans():\\n\"\n                msg += \"    - \" + \"\\n    - \".join(self._dchans.keys())\n                raise Exception(msg)\n\n            ltypes = [str,int,float,np.int64,np.float64]\n            C0 = type(val) in ltypes\n            C1 = type(val) in [list,tuple,np.ndarray]\n            assert C0 or C1\n            if C0:\n                val = [val]\n            else:\n                assert all([type(vv) in ltypes for vv in val])\n            ind = np.vstack([self._dchans[key]==ii for ii in val])\n            if log=='any':\n                ind = np.any(ind,axis=0)\n            elif log=='all':\n                ind = np.all(ind,axis=0)\n            else:\n                ind = ~np.any(ind,axis=0)\n        if out is int:\n            ind = ind.nonzero()[0]\n        return ind", "response": "Select a channel from the data array."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nselects a wavelength index array for a specific time vector.", "response": "def select_lamb(self, lamb=None, out=bool):\n        \"\"\" Return a wavelength index array\n\n        Return a boolean or integer index array, hereafter called 'ind'\n        The array refers to the reference time vector self.ddataRef['lamb']\n\n        Parameters\n        ----------\n        lamb :     None / float / np.ndarray / list / tuple\n            The time values to be selected:\n                - None : ind matches all wavelength values\n                - float : ind is True only for the wavelength closest to lamb\n                - np.ndarray : ind True only for the wavelength closest to lamb\n                - list (len()==2): ind True for wavelength in [lamb[0],lamb[1]]\n                - tuple (len()==2): ind True for wavelength outside ]t[0];t[1][\n        out :   type\n            Specifies the type of the output index array:\n                - bool : return a boolean array of shape (self.ddataRef['nlamb'],)\n                - int : return the array as integers indices\n\n        Return\n        ------\n        ind :   np.ndarray\n            The array of indices, of dtype specified by keywordarg out\n\n        \"\"\"\n        if not self._isSpectral():\n            msg = \"\"\n            raise Exception(msg)\n        assert out in [bool,int]\n        ind = _select_ind(lamb, self._ddataRef['lamb'], self._ddataRef['nlamb'])\n        if out is int:\n            ind = ind.nonzero()[0]\n        return ind"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef plot(self, key=None,\n             cmap=None, ms=4, vmin=None, vmax=None,\n             vmin_map=None, vmax_map=None, cmap_map=None, normt_map=False,\n             ntMax=None, nchMax=None, nlbdMax=3,\n             lls=None, lct=None, lcch=None, lclbd=None, cbck=None,\n             inct=[1,10], incX=[1,5], inclbd=[1,10],\n             fmt_t='06.3f', fmt_X='01.0f',\n             invert=True, Lplot='In', dmarker=None,\n             Bck=True, fs=None, dmargin=None, wintit=None, tit=None,\n             fontsize=None, labelpad=None, draw=True, connect=True):\n        \"\"\" Plot the data content in a generic interactive figure  \"\"\"\n        kh = _plot.Data_plot(self, key=key, indref=0,\n                             cmap=cmap, ms=ms, vmin=vmin, vmax=vmax,\n                             vmin_map=vmin_map, vmax_map=vmax_map,\n                             cmap_map=cmap_map, normt_map=normt_map,\n                             ntMax=ntMax, nchMax=nchMax, nlbdMax=nlbdMax,\n                             lls=lls, lct=lct, lcch=lcch, lclbd=lclbd, cbck=cbck,\n                             inct=inct, incX=incX, inclbd=inclbd,\n                             fmt_t=fmt_t, fmt_X=fmt_X, Lplot=Lplot,\n                             invert=invert, dmarker=dmarker, Bck=Bck,\n                             fs=fs, dmargin=dmargin, wintit=wintit, tit=tit,\n                             fontsize=fontsize, labelpad=labelpad,\n                             draw=draw, connect=connect)\n        return kh", "response": "Plot the data content in a generic interactive figure"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef plot_compare(self, lD, key=None,\n                     cmap=None, ms=4, vmin=None, vmax=None,\n                     vmin_map=None, vmax_map=None, cmap_map=None, normt_map=False,\n                     ntMax=None, nchMax=None, nlbdMax=3,\n                     lls=None, lct=None, lcch=None, lclbd=None, cbck=None,\n                     inct=[1,10], incX=[1,5], inclbd=[1,10],\n                     fmt_t='06.3f', fmt_X='01.0f', fmt_l='07.3f',\n                     invert=True, Lplot='In', dmarker=None,\n                     sharey=True, sharelamb=True,\n                     Bck=True, fs=None, dmargin=None, wintit=None, tit=None,\n                     fontsize=None, labelpad=None, draw=True, connect=True):\n        \"\"\" Plot several Data instances of the same diag\n\n        Useful to compare :\n                - the diag data for 2 different shots\n                - experimental vs synthetic data for the same shot\n\n        \"\"\"\n        C0 = isinstance(lD,list)\n        C0 = C0 and all([issubclass(dd.__class__,DataAbstract) for dd in lD])\n        C1 = issubclass(lD.__class__,DataAbstract)\n        assert C0 or C1, 'Provided first arg. must be a tf.data.DataAbstract or list !'\n        lD = [lD] if C1 else lD\n        kh = _plot.Data_plot([self]+lD, key=key, indref=0,\n                             cmap=cmap, ms=ms, vmin=vmin, vmax=vmax,\n                             vmin_map=vmin_map, vmax_map=vmax_map,\n                             cmap_map=cmap_map, normt_map=normt_map,\n                             ntMax=ntMax, nchMax=nchMax, nlbdMax=nlbdMax,\n                             lls=lls, lct=lct, lcch=lcch, lclbd=lclbd, cbck=cbck,\n                             inct=inct, incX=incX, inclbd=inclbd,\n                             fmt_t=fmt_t, fmt_X=fmt_X, fmt_l=fmt_l, Lplot=Lplot,\n                             invert=invert, dmarker=dmarker, Bck=Bck,\n                             sharey=sharey, sharelamb=sharelamb,\n                             fs=fs, dmargin=dmargin, wintit=wintit, tit=tit,\n                             fontsize=fontsize, labelpad=labelpad,\n                             draw=draw, connect=connect)\n        return kh", "response": "Plot the data instances of the same diagnose and return the plot of the two data instances of the same diagnose."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef calc_spectrogram(self, fmin=None,\n                         method='scipy-fourier', deg=False,\n                         window='hann', detrend='linear',\n                         nperseg=None, noverlap=None,\n                         boundary='constant', padded=True,\n                         wave='morlet', warn=True):\n        \"\"\" Return the power spectrum density for each channel\n\n        The power spectrum density is computed with the chosen method\n\n        Parameters\n        ----------\n        fmin :  None / float\n            The minimum frequency of interest\n            If None, set to 5/T, where T is the whole time interval\n            Used to constrain the number of points per window\n        deg :   bool\n            Flag indicating whether to return the phase in deg (vs rad)\n        method : str\n            Flag indicating which method to use for computation:\n                - 'scipy-fourier':  uses scipy.signal.spectrogram()\n                    (windowed fast fourier transform)\n                - 'scipy-stft':     uses scipy.signal.stft()\n                    (short time fourier transform)\n                - 'scipy-wavelet':  uses scipy.signal.cwt()\n                    (continuous wavelet transform)\n            The following keyword args are fed to one of these scipy functions\n            See the corresponding online scipy documentation for details on\n            each function and its arguments\n        window : None / str / tuple\n            If method='scipy-fourier'\n            Flag indicating which type of window to use\n        detrend : None / str\n            If method='scipy-fourier'\n            Flag indicating whether and how to remove the trend of the signal\n        nperseg :   None / int\n            If method='scipy-fourier'\n            Number of points to the used for each window\n            If None, deduced from fmin\n        noverlap:\n            If method='scipy-fourier'\n            Number of points on which successive windows should overlap\n            If None, nperseg-1\n        boundary:\n            If method='scipy-stft'\n\n        padded :\n            If method='scipy-stft'\n            d\n        wave: None / str\n            If method='scipy-wavelet'\n\n        Return\n        ------\n        tf :    np.ndarray\n            Time vector of the spectrogram (1D)\n        f:      np.ndarray\n            frequency vector of the spectrogram (1D)\n        lspect: list of np.ndarrays\n            list of () spectrograms\n\n        \"\"\"\n        if self._isSpectral():\n            msg = \"spectrogram not implemented yet for spectral data class\"\n            raise Exception(msg)\n        tf, f, lpsd, lang = _comp.spectrogram(self.data, self.t,\n                                              fmin=fmin, deg=deg,\n                                              method=method, window=window,\n                                              detrend=detrend, nperseg=nperseg,\n                                              noverlap=noverlap, boundary=boundary,\n                                              padded=padded, wave=wave,\n                                              warn=warn)\n        return tf, f, lpsd, lang", "response": "Calculates the power spectrum density for each channel in the current state of the state of the channel."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef plot_spectrogram(self, fmin=None, fmax=None,\n                         method='scipy-fourier', deg=False,\n                         window='hann', detrend='linear',\n                         nperseg=None, noverlap=None,\n                         boundary='constant', padded=True, wave='morlet',\n                         invert=True, plotmethod='imshow',\n                         cmap_f=None, cmap_img=None,\n                         ms=4, ntMax=None, nfMax=None,\n                         Bck=True, fs=None, dmargin=None, wintit=None,\n                         tit=None, vmin=None, vmax=None, normt=False,\n                         draw=True, connect=True, returnspect=False, warn=True):\n        \"\"\" Plot the spectrogram of all channels with chosen method\n\n        All non-plotting arguments are fed to self.calc_spectrogram()\n        see self.calc_spectrogram? for details\n\n        Parameters\n        ----------\n\n        Return\n        ------\n        kh :    tofu.utils.HeyHandler\n            The tofu KeyHandler object handling figure interactivity\n        \"\"\"\n        if self._isSpectral():\n            msg = \"spectrogram not implemented yet for spectral data class\"\n            raise Exception(msg)\n        tf, f, lpsd, lang = _comp.spectrogram(self.data, self.t,\n                                              fmin=fmin, deg=deg,\n                                              method=method, window=window,\n                                              detrend=detrend, nperseg=nperseg,\n                                              noverlap=noverlap, boundary=boundary,\n                                              padded=padded, wave=wave,\n                                              warn=warn)\n        kh = _plot.Data_plot_spectrogram(self, tf, f, lpsd, lang, fmax=fmax,\n                                         invert=invert, plotmethod=plotmethod,\n                                         cmap_f=cmap_f, cmap_img=cmap_img,\n                                         ms=ms, ntMax=ntMax,\n                                         nfMax=nfMax, Bck=Bck, fs=fs,\n                                         dmargin=dmargin, wintit=wintit,\n                                         tit=tit, vmin=vmin, vmax=vmax,\n                                         normt=normt, draw=draw,\n                                         connect=connect)\n        if returnspect:\n            return kh, tf, f, lpsd, lang\n        else:\n            return kh", "response": "Plot the spectrogram of all channels with the specified method."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef calc_svd(self, lapack_driver='gesdd'):\n        if self._isSpectral():\n            msg = \"svd not implemented yet for spectral data class\"\n            raise Exception(msg)\n        chronos, s, topos = _comp.calc_svd(self.data, lapack_driver=lapack_driver)\n        return u, s, v", "response": "Calculates the SVD decomposition of the input data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef plot_svd(self, lapack_driver='gesdd', modes=None, key=None, Bck=True,\n                 Lplot='In', cmap=None, vmin=None, vmax=None,\n                 cmap_topos=None, vmin_topos=None, vmax_topos=None,\n                 ntMax=None, nchMax=None, ms=4,\n                 inct=[1,10], incX=[1,5], incm=[1,5],\n                 lls=None, lct=None, lcch=None, lcm=None, cbck=None,\n                 invert=False, fmt_t='06.3f', fmt_X='01.0f', fmt_m='03.0f',\n                 fs=None, dmargin=None, labelpad=None, wintit=None, tit=None,\n                 fontsize=None, draw=True, connect=True):\n        \"\"\" Plot the chosen modes of the svd decomposition\n\n        All modes will be plotted, the keyword 'modes' is only used to\n        determine the reference modes for computing a common scale for\n        vizualisation\n\n        Runs self.calc_svd() and then plots the result in an interactive figure\n\n        \"\"\"\n        if self._isSpectral():\n            msg = \"svd not implemented yet for spectral data class\"\n            raise Exception(msg)\n        # Computing (~0.2 s for 50 channels 1D and 1000 times)\n        chronos, s, topos = _comp.calc_svd(self.data, lapack_driver=lapack_driver)\n\n        # Plotting (~11 s for 50 channels 1D and 1000 times)\n        kh = _plot.Data_plot_svd(self, chronos, s, topos, modes=modes,\n                                 key=key, Bck=Bck, Lplot=Lplot,\n                                 cmap=cmap, vmin=vmin, vmax=vmax,\n                                 cmap_topos=cmap_topos, vmin_topos=vmin_topos,\n                                 vmax_topos=vmax_topos,\n                                 ntMax=ntMax, nchMax=nchMax, ms=ms,\n                                 inct=inct, incX=incX, incm=incm,\n                                 lls=lls, lct=lct, lcch=lcch, lcm=lcm, cbck=cbck,\n                                 invert=invert, fmt_t=fmt_t, fmt_X=fmt_X, fmt_m=fmt_m,\n                                 fs=fs, dmargin=dmargin, labelpad=labelpad, wintit=wintit,\n                                 tit=tit, fontsize=fontsize, draw=draw,\n                                 connect=connect)\n        return kh", "response": "Plots the svd decomposition of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef plot(self, key=None, invert=None, plotmethod='imshow',\n             cmap=plt.cm.gray, ms=4, Max=None,\n             fs=None, dmargin=None, wintit=None,\n             draw=True, connect=True):\n        \"\"\" Plot the data content in a predefined figure  \"\"\"\n        dax, KH = _plot.Data_plot(self, key=key, invert=invert, Max=Max,\n                                  plotmethod=plotmethod, cmap=cmap, ms=ms,\n                                  fs=fs, dmargin=dmargin, wintit=wintit,\n                                  draw=draw, connect=connect)\n        return dax, KH", "response": "Plot the data content in a predefined figure"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _Struct_set_Poly(Poly, pos=None, extent=None, arrayorder='C',\n                     Type='Tor', Clock=False):\n    \"\"\" Compute geometrical attributes of a Struct object \"\"\"\n\n    # Make Poly closed, counter-clockwise, with '(cc,N)' layout and arrayorder\n    Poly = _GG.Poly_Order(Poly, order='C', Clock=False,\n                          close=True, layout='(cc,N)', Test=True)\n    assert Poly.shape[0]==2, \"Arg Poly must be a 2D polygon !\"\n    fPfmt = np.ascontiguousarray if arrayorder=='C' else np.asfortranarray\n\n    # Get all remarkable points and moments\n    NP = Poly.shape[1]-1\n    P1Max = Poly[:,np.argmax(Poly[0,:])]\n    P1Min = Poly[:,np.argmin(Poly[0,:])]\n    P2Max = Poly[:,np.argmax(Poly[1,:])]\n    P2Min = Poly[:,np.argmin(Poly[1,:])]\n    BaryP = np.sum(Poly[:,:-1],axis=1,keepdims=False)/(Poly.shape[1]-1)\n    BaryL = np.array([(P1Max[0]+P1Min[0])/2., (P2Max[1]+P2Min[1])/2.])\n    TorP = plg.Polygon(Poly.T)\n    Surf = TorP.area()\n    BaryS = np.array(TorP.center()).flatten()\n\n    # Get lim-related indicators\n    noccur = int(pos.size)\n    Multi = noccur>1\n\n    # Get Tor-related quantities\n    if Type.lower()=='lin':\n        Vol, BaryV = None, None\n    else:\n        Vol, BaryV = _GG.Poly_VolAngTor(Poly)\n        msg = \"Pb. with volume computation for Ves object of type 'Tor' !\"\n        assert Vol>0., msg\n\n    # Compute the non-normalized vector of each side of the Poly\n    Vect = np.diff(Poly,n=1,axis=1)\n    Vect = fPfmt(Vect)\n\n    # Compute the normalised vectors directed inwards\n    Vin = np.array([Vect[1,:],-Vect[0,:]])\n    if not _GG.Poly_isClockwise(Poly):\n        Vin = -Vin\n    Vin = Vin/np.hypot(Vin[0,:],Vin[1,:])[np.newaxis,:]\n    Vin = fPfmt(Vin)\n\n    poly = _GG.Poly_Order(Poly, order=arrayorder, Clock=Clock,\n                          close=False, layout='(cc,N)', Test=True)\n\n    # Get bounding circle\n    circC = BaryS\n    r = np.sqrt(np.sum((poly-circC[:,np.newaxis])**2,axis=0))\n    circr = np.max(r)\n\n    dout = {'Poly':poly, 'pos':pos, 'extent':extent,\n            'noccur':noccur, 'Multi':Multi, 'nP':NP,\n            'P1Max':P1Max, 'P1Min':P1Min, 'P2Max':P2Max, 'P2Min':P2Min,\n            'BaryP':BaryP, 'BaryL':BaryL, 'BaryS':BaryS, 'BaryV':BaryV,\n            'Surf':Surf, 'VolAng':Vol, 'Vect':Vect, 'VIn':Vin,\n            'circ-C':circC, 'circ-r':circr, 'Clock':Clock}\n    return dout", "response": "Compute geometrical attributes of a Struct object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef LOS_PRMin(Ds, dus, kPOut=None, Eps=1.e-12, Test=True):\n    if Test:\n        assert Ds.ndim in [1,2] and 3 in Ds.shape and Ds.shape==dus.shape\n        assert kPOut is None or (Ds.ndim==1 and not hasattr(kPOut,'__iter__')) or (Ds.ndim==2 and kPOut.shape==(Ds.size/3,))\n\n    v = Ds.ndim==1\n    if v:\n        Ds = Ds.reshape((3,1))\n        dus = dus.reshape((3,1))\n        if kPOut is not None:\n            kPOut = np.array([kPOut])\n\n    kRMin = np.nan*np.ones((Ds.shape[1],))\n    uparN = np.sqrt(dus[0,:]**2 + dus[1,:]**2)\n\n    # Case with u vertical\n    ind = uparN>Eps\n    kRMin[~ind] = 0.\n\n    # Else\n    kRMin[ind] = -(dus[0,ind]*Ds[0,ind]+dus[1,ind]*Ds[1,ind])/uparN[ind]**2\n\n    # Check\n    kRMin[kRMin<=0.] = 0.\n    if kPOut is not None:\n        kRMin[kRMin>kPOut] = kPOut[kRMin>kPOut]\n\n    if v:\n        kRMin = kRMin[0]\n    return kRMin", "response": "Compute the point on the LOS where the major radius is minimum"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef LOS_CrossProj(VType, Ds, us, kPIns, kPOuts, kRMins,\n                  Lplot='In', proj='All', multi=False):\n    \"\"\" Compute the parameters to plot the poloidal projection of the LOS  \"\"\"\n    assert type(VType) is str and VType.lower() in ['tor','lin']\n    assert Lplot.lower() in ['tot','in']\n    assert type(proj) is str\n    proj = proj.lower()\n    assert proj in ['cross','hor','all','3d']\n    assert Ds.ndim==2 and Ds.shape==us.shape\n    nL = Ds.shape[1]\n    k0 = kPIns if Lplot.lower()=='in' else np.zeros((nL,))\n\n    if VType.lower()=='tor' and proj in ['cross','all']:\n        CrossProjAng = np.arccos(np.sqrt(us[0,:]**2+us[1,:]**2)\n                                 /np.sqrt(np.sum(us**2,axis=0)))\n        nkp = np.ceil(25.*(1 - (CrossProjAng/(np.pi/4)-1)**2) + 2)\n        ks = np.max([kRMins,kPIns],axis=0) if Lplot.lower()=='in' else kRMins\n        pts0 = []\n        if multi:\n            for ii in range(0,nL):\n                if np.isnan(kPOuts[ii]):\n                    pts0.append( np.array([[np.nan,np.nan],\n                                           [np.nan,np.nan]]) )\n                else:\n                    k = np.linspace(k0[ii],kPOuts[ii],nkp[ii],endpoint=True)\n                    k = np.unique(np.append(k,ks[ii]))\n                    pp = Ds[:,ii:ii+1] + k[np.newaxis,:]*us[:,ii:ii+1]\n                    pts0.append( np.array([np.hypot(pp[0,:],pp[1,:]),pp[2,:]])  )\n        else:\n            for ii in range(0,nL):\n                if np.isnan(kPOuts[ii]):\n                    pts0.append(np.array([[np.nan,np.nan,np.nan],\n                                          [np.nan,np.nan,np.nan],\n                                          [np.nan,np.nan,np.nan]]))\n                else:\n                    k = np.linspace(k0[ii],kPOuts[ii],nkp[ii],endpoint=True)\n                    k = np.append(np.unique(np.append(k,ks[ii])),np.nan)\n                    pts0.append( Ds[:,ii:ii+1] + k[np.newaxis,:]*us[:,ii:ii+1] )\n            pts0 = np.concatenate(tuple(pts0),axis=1)\n            pts0 = np.array([np.hypot(pts0[0,:],pts0[1,:]),pts0[2,:]])\n\n    if not (VType.lower()=='tor' and proj=='cross'):\n        pts = []\n        if multi:\n            for ii in range(0,nL):\n                if np.isnan(kPOuts[ii]):\n                    pts.append( np.array([[np.nan,np.nan],\n                                          [np.nan,np.nan],\n                                          [np.nan,np.nan]]) )\n                else:\n                    k = np.array([k0[ii],kPOuts[ii]])\n                    pts.append( Ds[:,ii:ii+1] + k[np.newaxis,:]*us[:,ii:ii+1] )\n        else:\n            for ii in range(0,nL):\n                if np.isnan(kPOuts[ii]):\n                    pts.append(np.array([[np.nan,np.nan,np.nan],\n                                         [np.nan,np.nan,np.nan],\n                                         [np.nan,np.nan,np.nan]]))\n                else:\n                    k = np.array([k0[ii],kPOuts[ii],np.nan])\n                    pts.append( Ds[:,ii:ii+1] + k[np.newaxis,:]*us[:,ii:ii+1] )\n            pts = np.concatenate(tuple(pts),axis=1)\n\n    if proj=='hor':\n        pts = [pp[:2,:] for pp in pts] if multi else pts[:2,:]\n    elif proj=='cross':\n        if VType.lower()=='tor':\n            pts = pts0\n        else:\n            pts = [pp[1:,:] for pp in pts] if multi else pts[1:,:]\n    elif proj=='all':\n        if multi:\n            if VType.lower()=='tor':\n                pts = [(p0,pp[:2,:]) for (p0,pp) in zip(*[pts0,pts])]\n            else:\n                pts = (pts[1:,:],pts[:2,:])\n        else:\n            pts = (pts0,pts[:2,:]) if VType.lower()=='tor' else (pts[1:,:],pts[:2,:])\n    return pts", "response": "Compute the parameters to plot the poloidal projection of the LOS."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the sampled line with the specified method", "response": "def LOS_get_sample(D, u, dL, DL=None, dLMode='abs', method='sum', Test=True):\n    \"\"\" Return the sampled line, with the specified method\n\n    'linspace': return the N+1 edges, including the first and last point\n    'sum' : return the N middle of the segments\n    'simps': return the N+1 egdes, where N has to be even (scipy.simpson requires an even number of intervals)\n    'romb' : return the N+1 edges, where N+1 = 2**k+1 (fed to scipy.romb for integration)\n    \"\"\"\n    if Test:\n        assert all([type(dd) is np.ndarray and dd.shape==(3,) for dd in [D,u]])\n        assert not hasattr(dL,'__iter__')\n        assert DL is None or all([hasattr(DL,'__iter__'), len(DL)==2, all([not hasattr(dd,'__iter__') for dd in DL])])\n        assert dLMode in ['abs','rel']\n        assert type(method) is str and method in ['linspace','sum','simps','romb']\n    # Compute the minimum number of intervals to satisfy the specified resolution\n    N = int(np.ceil((DL[1]-DL[0])/dL)) if dLMode=='abs' else int(np.ceil(1./dL))\n    # Modify N according to the desired method\n    if method=='simps':\n        N = N if N%2==0 else N+1\n    elif method=='romb':\n        N = 2**int(np.ceil(np.log(N)/np.log(2.)))\n\n    # Derive k and dLr\n    if method=='sum':\n        dLr = (DL[1]-DL[0])/N\n        k = DL[0] + (0.5+np.arange(0,N))*dLr\n    else:\n        k, dLr = np.linspace(DL[0], DL[1], N+1, endpoint=True, retstep=True, dtype=float)\n\n    Pts = D[:,np.newaxis] + k[np.newaxis,:]*u[:,np.newaxis]\n    return Pts, k, dLr"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntests whether each point lies inside the Struct object.", "response": "def isInside(self, pts, In='(X,Y,Z)'):\n        \"\"\" Return an array of booleans indicating whether each point lies\n        inside the Struct volume\n\n        Tests for each point whether it lies inside the Struct object.\n        The points coordinates can be provided in 2D or 3D\n        You must specify which coordinate system is used with 'In' kwdarg.\n        An array of boolean flags is returned.\n\n        Parameters\n        ----------\n        pts :   np.ndarray\n            (2,N) or (3,N) array, coordinates of the points to be tested\n        In :    str\n            Flag indicating the coordinate system in which pts are provided\n            e.g.: '(X,Y,Z)' or '(R,Z)'\n\n        Returns\n        -------\n        ind :   np.ndarray\n            (N,) array of booleans, True if a point is inside the volume\n\n        \"\"\"\n        ind = _GG._Ves_isInside(pts, self.Poly, Lim=self.Lim,\n                                nLim=self._dgeom['noccur'],\n                                VType=self.Id.Type,\n                                In=In, Test=True)\n        return ind"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_InsideConvexPoly(self, RelOff=_def.TorRelOff, ZLim='Def',\n                             Spline=True, Splprms=_def.TorSplprms,\n                             NP=_def.TorInsideNP, Plot=False, Test=True):\n        \"\"\" Return a polygon that is a smaller and smoothed approximation of Ves.Poly, useful for excluding the divertor region in a Tokamak\n\n        For some uses, it can be practical to approximate the polygon defining the Ves object (which can be non-convex, like with a divertor), by a simpler, sligthly smaller and convex polygon.\n        This method provides a fast solution for computing such a proxy.\n\n        Parameters\n        ----------\n        RelOff :    float\n            Fraction by which an homothetic polygon should be reduced (1.-RelOff)*(Poly-BaryS)\n        ZLim :      None / str / tuple\n            Flag indicating what limits shall be put to the height of the polygon (used for excluding divertor)\n        Spline :    bool\n            Flag indiating whether the reduced and truncated polygon shall be smoothed by 2D b-spline curves\n        Splprms :   list\n            List of 3 parameters to be used for the smoothing [weights,smoothness,b-spline order], fed to scipy.interpolate.splprep()\n        NP :        int\n            Number of points to be used to define the smoothed polygon\n        Plot :      bool\n            Flag indicating whether the result shall be plotted for visual inspection\n        Test :      bool\n            Flag indicating whether the inputs should be tested for conformity\n\n        Returns\n        -------\n        Poly :      np.ndarray\n            (2,N) polygon resulting from homothetic transform, truncating and optional smoothing\n\n        \"\"\"\n        return _comp._Ves_get_InsideConvexPoly(self.Poly_closed,\n                                               self.dgeom['P2Min'],\n                                               self.dgeom['P2Max'],\n                                               self.dgeom['BaryS'],\n                                               RelOff=RelOff, ZLim=ZLim,\n                                               Spline=Spline, Splprms=Splprms,\n                                               NP=NP, Plot=Plot, Test=Test)", "response": "Returns a smaller and smoothed approximation of Ves. Poly useful for excluding divertor region in a Tokamak object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_sampleEdge(self, res, DS=None, resMode='abs', offsetIn=0.):\n        pts, dlr, ind = _comp._Ves_get_sampleEdge(self.Poly, res, DS=DS,\n                                                  dLMode=resMode, DIn=offsetIn,\n                                                  VIn=self.dgeom['VIn'],\n                                                  margin=1.e-9)\n        return pts, dlr, ind", "response": "Sample the polygon edges with resolution res"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_sampleCross(self, res, DS=None, resMode='abs', ind=None):\n        args = [self.Poly, self.dgeom['P1Min'][0], self.dgeom['P1Max'][0],\n                self.dgeom['P2Min'][1], self.dgeom['P2Max'][1], res]\n        kwdargs = dict(DS=DS, dSMode=resMode, ind=ind, margin=1.e-9)\n        pts, dS, ind, reseff = _comp._Ves_get_sampleCross(*args, **kwdargs)\n        return pts, dS, ind, reseff", "response": "Sample with resolution res the 2D cross - section"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the sample of the object in the specified resolution.", "response": "def get_sampleS(self, res, DS=None, resMode='abs',\n                    ind=None, offsetIn=0., Out='(X,Y,Z)', Ind=None):\n        \"\"\" Sample, with resolution res, the surface defined by DS or ind\n\n        An optionnal offset perpendicular to the surface can be used\n        (offsetIn>0 => inwards)\n\n        Parameters\n        ----------\n        res     :   float / list of 2 floats\n            Desired resolution of the surfacic sample\n                float   : same resolution for all directions of the sample\n                list    : [dl,dXPhi] where:\n                    dl      : res. along polygon contours (cross-section)\n                    dXPhi   : res. along axis (toroidal/linear direction)\n        DS      :   None / list of 3 lists of 2 floats\n            Limits of the domain in which the sample should be computed\n                None : whole surface of the object\n                list : [D1,D2,D3], where Di is a len()=2 list\n                       (increasing floats, setting limits along coordinate i)\n                    [DR,DZ,DPhi]: in toroidal geometry (self.Id.Type=='Tor')\n                    [DX,DY,DZ]  : in linear geometry (self.Id.Type=='Lin')\n        resMode  :   str\n            Flag, specifies if res is absolute or relative to element sizes\n                'abs'   :   res is an absolute distance\n                'rel'   :   if res=0.1, each polygon segment is divided in 10,\n                            as is the toroidal/linear length\n        ind     :   None / np.ndarray of int\n            If provided, DS is ignored and the sample points corresponding to\n            the provided indices are returned\n            Example (assuming obj is a Ves object)\n                > # We create a 5x5 cm2 sample of the whole surface\n                > pts, dS, ind, reseff = obj.get_sample(0.05)\n                > # Perform operations, save only the points indices (save space)\n                > ...\n                > # Retrieve the points from their indices (requires same res)\n                > pts2, dS2, ind2, reseff2 = obj.get_sample(0.05, ind=ind)\n                > np.allclose(pts,pts2)\n                True\n        offsetIn:   float\n            Offset distance from the actual surface of the object\n            Inwards if positive\n            Useful to avoid numerical errors\n        Out     :   str\n            Flag indicating the coordinate system of returned points\n            e.g. : '(X,Y,Z)' or '(R,Z,Phi)'\n        Ind     :   None / iterable of ints\n            Array of indices of the entities to be considered\n            (only when multiple entities, i.e.: self.nLim>1)\n\n        Returns\n        -------\n        pts     :   np.ndarray / list of np.ndarrays\n            Sample points coordinates, as a (3,N) array.\n            A list is returned if the object has multiple entities\n        dS      :   np.ndarray / list of np.ndarrays\n            The surface (in m^2) associated to each point\n        ind     :   np.ndarray / list of np.ndarrays\n            The index of each point\n        reseff  :   np.ndarray / list of np.ndarrays\n            Effective resolution in both directions after sample computation\n        \"\"\"\n        if Ind is not None:\n            assert self.dgeom['Multi']\n        kwdargs = dict(DS=DS, dSMode=resMode, ind=ind, DIn=offsetIn,\n                       VIn=self.dgeom['VIn'], VType=self.Id.Type,\n                       VLim=np.ascontiguousarray(self.Lim), nVLim=self.noccur,\n                       Out=Out, margin=1.e-9,\n                       Multi=self.dgeom['Multi'], Ind=Ind)\n        args = [self.Poly, self.dgeom['P1Min'][0], self.dgeom['P1Max'][0],\n                self.dgeom['P2Min'][1], self.dgeom['P2Max'][1], res]\n        pts, dS, ind, reseff = _comp._Ves_get_sampleS(*args, **kwdargs)\n        return pts, dS, ind, reseff"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_sampleV(self, res, DV=None, resMode='abs', ind=None, Out='(X,Y,Z)'):\n\n        args = [self.Poly, self.dgeom['P1Min'][0], self.dgeom['P1Max'][0],\n                self.dgeom['P2Min'][1], self.dgeom['P2Max'][1], res]\n        kwdargs = dict(DV=DV, dVMode=resMode, ind=ind, VType=self.Id.Type,\n                      VLim=self.Lim, Out=Out, margin=1.e-9)\n        pts, dV, ind, reseff = _comp._Ves_get_sampleV(*args, **kwdargs)\n        return pts, dV, ind, reseff", "response": "Get the sample from the VES with resolution res the volume defined by DV or ind"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef plot(self, lax=None, proj='all', element='PIBsBvV',\n             dP=None, dI=_def.TorId, dBs=_def.TorBsd, dBv=_def.TorBvd,\n             dVect=_def.TorVind, dIHor=_def.TorITord, dBsHor=_def.TorBsTord,\n             dBvHor=_def.TorBvTord, Lim=None, Nstep=_def.TorNTheta,\n             dLeg=_def.TorLegd, indices=False,\n             draw=True, fs=None, wintit=None, Test=True):\n        \"\"\" Plot the polygon defining the vessel, in chosen projection\n\n        Generic method for plotting the Ves object\n        The projections to be plotted, the elements to plot can be specified\n        Dictionaries of properties for each elements can also be specified\n        If an ax is not provided a default one is created.\n\n        Parameters\n        ----------\n        Lax :       list or plt.Axes\n            The axes to be used for plotting\n            Provide a list of 2 axes if proj='All'\n            If None a new figure with axes is created\n        proj :      str\n            Flag specifying the kind of projection\n                - 'Cross' : cross-section projection\n                - 'Hor' : horizontal projection\n                - 'All' : both\n                - '3d' : a 3d matplotlib plot\n        element :   str\n            Flag specifying which elements to plot\n            Each capital letter corresponds to an element:\n                * 'P': polygon\n                * 'I': point used as a reference for impact parameters\n                * 'Bs': (surfacic) center of mass\n                * 'Bv': (volumic) center of mass for Tor type\n                * 'V': vector pointing inward perpendicular to each segment\n        dP :        dict / None\n            Dict of properties for plotting the polygon\n            Fed to plt.Axes.plot() or plt.plot_surface() if proj='3d'\n        dI :        dict / None\n            Dict of properties for plotting point 'I' in Cross-section projection\n        dIHor :     dict / None\n            Dict of properties for plotting point 'I' in horizontal projection\n        dBs :       dict / None\n            Dict of properties for plotting point 'Bs' in Cross-section projection\n        dBsHor :    dict / None\n            Dict of properties for plotting point 'Bs' in horizontal projection\n        dBv :       dict / None\n            Dict of properties for plotting point 'Bv' in Cross-section projection\n        dBvHor :    dict / None\n            Dict of properties for plotting point 'Bv' in horizontal projection\n        dVect :     dict / None\n            Dict of properties for plotting point 'V' in cross-section projection\n        dLeg :      dict / None\n            Dict of properties for plotting the legend, fed to plt.legend()\n            The legend is not plotted if None\n        Lim :       list or tuple\n            Array of a lower and upper limit of angle (rad.) or length for\n            plotting the '3d' proj\n        Nstep :     int\n            Number of points for sampling in ignorable coordinate (toroidal angle or length)\n        draw :      bool\n            Flag indicating whether the fig.canvas.draw() shall be called automatically\n        a4 :        bool\n            Flag indicating whether the figure should be plotted in a4 dimensions for printing\n        Test :      bool\n            Flag indicating whether the inputs should be tested for conformity\n\n        Returns\n        -------\n        La          list / plt.Axes\n            Handles of the axes used for plotting (list if several axes where used)\n\n        \"\"\"\n        kwdargs = locals()\n        lout = ['self']\n        for k in lout:\n            del kwdargs[k]\n        return _plot.Struct_plot(self, **kwdargs)", "response": "Plots the polygon defining the vessel in the specified axes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nplot the sinogram of the vessel polygon.", "response": "def plot_sino(self, ax=None, Ang=_def.LOSImpAng,\n                  AngUnit=_def.LOSImpAngUnit, Sketch=True, dP=None,\n                  dLeg=_def.TorLegd, draw=True, fs=None, wintit=None,\n                  Test=True):\n        \"\"\" Plot the sinogram of the vessel polygon, by computing its envelopp in a cross-section, can also plot a 3D version of it\n\n        The envelop of the polygon is computed using self.Sino_RefPt as a reference point in projection space,\n        and plotted using the provided dictionary of properties.\n        Optionaly a small sketch can be included illustrating how the angle\n        and the impact parameters are defined (if the axes is not provided).\n\n        Parameters\n        ----------\n        proj :      str\n            Flag indicating whether to plot a classic sinogram ('Cross') from the vessel cross-section (assuming 2D)\n            or an extended 3D version '3d' of it with additional angle\n        ax   :      None or plt.Axes\n            The axes on which the plot should be done, if None a new figure and axes is created\n        Ang  :      str\n            Flag indicating which angle to use for the impact parameter, the angle of the line itself (xi) or of its impact parameter (theta)\n        AngUnit :   str\n            Flag for the angle units to be displayed, 'rad' for radians or 'deg' for degrees\n        Sketch :    bool\n            Flag indicating whether a small skecth showing the definitions of angles 'theta' and 'xi' should be included or not\n        Pdict :     dict\n            Dictionary of properties used for plotting the polygon envelopp,\n            fed to plt.plot() if proj='Cross' and to plt.plot_surface() if proj='3d'\n        LegDict :   None or dict\n            Dictionary of properties used for plotting the legend, fed to plt.legend(), the legend is not plotted if None\n        draw :      bool\n            Flag indicating whether the fig.canvas.draw() shall be called automatically\n        a4 :        bool\n            Flag indicating whether the figure should be plotted in a4 dimensions for printing\n        Test :      bool\n            Flag indicating whether the inputs shall be tested for conformity\n\n        Returns\n        -------\n        ax :        plt.Axes\n            The axes used to plot\n\n        \"\"\"\n        if Test:\n            msg = \"The impact parameters must be set ! (self.set_dsino())\"\n            assert not self.dsino['RefPt'] is None, msg\n\n        # Only plot cross sino, from version 1.4.0\n        dP = _def.TorPFilld if dP is None else dP\n        ax = _plot.Plot_Impact_PolProjPoly(self, ax=ax, Ang=Ang,\n                                           AngUnit=AngUnit, Sketch=Sketch,\n                                           Leg=self.Id.NameLTX, dP=dP,\n                                           dLeg=dLeg, draw=False,\n                                           fs=fs, wintit=wintit, Test=Test)\n        # else:\n        # Pdict = _def.TorP3DFilld if Pdict is None else Pdict\n        # ax = _plot.Plot_Impact_3DPoly(self, ax=ax, Ang=Ang, AngUnit=AngUnit,\n                                      # Pdict=Pdict, dLeg=LegDict, draw=False,\n                                      # fs=fs, wintit=wintit, Test=Test)\n        if draw:\n            ax.figure.canvas.draw()\n        return ax"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef save_to_txt(self, path='./', name=None,\n                    include=['Mod','Cls','Exp','Name'],\n                    fmt='%.18e', delimiter=' ',\n                    footer='', encoding=None, verb=True, return_pfe=False):\n        \"\"\" Save the basic geometrical attributes only (polygon and pos/extent)\n\n        The attributes are saved to a txt file with chosen encoding\n        Usefu for easily sharing input with non-python users\n\n        BEWARE: doesn't save all attributes !!!\n        Only saves the basic geometrical inputs !!!\n        Not equivalent to full tofu save (using self.save()) !!!\n\n        The saving convention is:\n            * data is saved on 2 columns\n            * The first line gives 2 numbers: nP, no\n                - nP = Number of points in the polygon\n                    (i.e.: the number of following lines describing the polygon)\n                - no = Number of occurences (toroidal if in toroidal geometry)\n                    (i.e.: the nb. of pos/extent lines after the first nP lines)\n            * Hence, the data is a 2D array of shape (1 + nP + no, 2)\n            * The two columns of the nP lines describing the polygon represent:\n                - 1st: R (resp. Y) coordinate of polygon points\n                - 2nd: Z (resp. Z) coordinate of polygon points\n            * The two columns of the no lines representing the occurences are:\n                - 1st: pos, the tor. angle (resp. X) center of occurences\n                - 2nd: extent, the tor. angle (resp. X) extension of occurences\n\n        Hence, the polygon and pos/extent of the object can be retrieved with:\n        >>> import numpy as np\n        >>> out = np.loadtxt(filename)\n        >>> nP, no = out[0,:]\n        >>> poly = out[1:1+nP,:]\n        >>> pos, extent = out[1+nP:,0], out[1+nP:,1]\n\n        All parameters apart from path, name and include are fed to numpy.savetxt()\n\n        Parameters\n        ----------\n        path:   None / str\n            The path where to save the file\n            If None -> self.Id.SavePath\n        name:   None / str\n            The name to use for the saved file\n            If None -> self.Id.SaveName(include)\n        include:    list\n            List of attributes of to be used to built the default saving name\n            Fed to tf.utils.ID.generate_SaveName()\n            Recommended: ['Mod','Cls','Exp','Name']\n        \"\"\"\n        if name is None:\n            name = self.Id.generate_SaveName(include)\n        if path is None:\n            path = self.Id.SavePath\n        path = os.path.abspath(path)\n        pfe = os.path.join(path,name+'.txt')\n\n        nPno = np.r_[self.Poly.shape[1], self.noccur]\n        poly = self.Poly.T\n        posext = np.vstack((self.pos, self.extent)).T\n        out = np.vstack((nPno,poly,posext))\n\n        # default standards\n        newline = '\\n'\n        comments = '#'\n        header = ' Cls = %s\\n Exp = %s\\n Name = %s'%(self.__class__.__name__,\n                                                     self.Id.Exp, self.Id.Name)\n\n        kwds = dict(fmt=fmt, delimiter=delimiter, newline=newline,\n                   header=header, footer=footer, comments=comments)\n        if 'encoding' in inspect.signature(np.savetxt).parameters:\n            kwds['encoding'] = encoding\n        np.savetxt(pfe, out, **kwds)\n        if verb:\n            print(\"save_to_txt in:\\n\", pfe)\n        if return_pfe:\n            return pfe", "response": "Save the basic geometrical attributes of the object to a txt file."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the polygon and pos/extent stored in a .txt file The file must have been generated by the method save_to_txt() All arguments appart from pfe and out are: - fed to the relevant tofu.geom.Struct subclass to instanciate it - used only if out = 'object' Parameters ---------- pfe: str Unique string containing the path and file name to be read The file must be formatted as if generated by self.save_to_txt(): - Must contain a (N,2) array - Line 0 must contain 2 integers: - npts : the nb. of points of the polygon - noccur : the nb. of occurences (=0 if axisymmetric) - Hence the number of lines hould be N = npts + noccur + 1 - Lines 1:npts+1 contain the polygon points - Lines npts+1: contain positions and extent of each occurence out: str Flag indicating whether to return: - 'dict' : a dictionnary of np.ndarrays - 'object': a tofu.geom.Struct subclass, using the other kwdargs Return ------ obj: tf.geom.Struct sublass instance / dict Depending on the value of out, obj can be: - An instance of the relevant tofu.geom.Struct subclass - A dict with keys 'poly', 'pos' and 'extent'", "response": "def from_txt(cls, pfe, out='object',\n                 Exp=None, Name=None, shot=None, Type=None,\n                 mobile=False, color=None, SavePath=os.path.abspath('./')):\n        \"\"\" Return the polygon and pos/extent stored in a .txt file\n\n        The file must have been generated by the method save_to_txt()\n        All arguments appart from pfe and out are:\n            - fed to the relevant tofu.geom.Struct subclass to instanciate it\n            - used only if out = 'object'\n\n        Parameters\n        ----------\n        pfe:    str\n            Unique string containing the path and file name to be read\n            The file must be formatted as if generated by self.save_to_txt():\n                - Must contain a (N,2) array\n                - Line 0 must contain 2 integers:\n                    - npts : the nb. of points of the polygon\n                    - noccur : the nb. of occurences (=0 if axisymmetric)\n                - Hence the number of lines hould be N = npts + noccur + 1\n                - Lines 1:npts+1 contain the polygon points\n                - Lines npts+1: contain positions and extent of each occurence\n        out:    str\n            Flag indicating whether to return:\n                - 'dict'  : a dictionnary of np.ndarrays\n                - 'object': a tofu.geom.Struct subclass, using the other kwdargs\n\n        Return\n        ------\n        obj:    tf.geom.Struct sublass instance  / dict\n            Depending on the value of out, obj can be:\n                - An instance of the relevant tofu.geom.Struct subclass\n                - A dict with keys 'poly', 'pos' and 'extent'\n        \"\"\"\n\n        if not out in [object,'object','dict']:\n            msg = \"Arg out must be either:\"\n            msg += \"    - 'object': return a %s instance\\n\"%cls.__name__\n            msg += \"    - 'dict' : return a dict with polygon, pos and extent\"\n            raise Exception(msg)\n        if not pfe[-4:]=='.txt':\n            msg = \"Only accepts .txt files (fed to np.loadtxt) !\\n\"\n            msg += \"    file:  %s\"%pfe\n            raise Exception(msg)\n        oo = np.loadtxt(pfe)\n        if not (oo.ndim==2 and oo.shape[1]==2):\n            msg = \"The file should contain a (N,2) array !\\n\"\n            msg += \"    file : %s\\n\"%pfe\n            msg += \"    shape: {0}\".format(oo.shape)\n            raise Exception(msg)\n        C0 = (oo[0,0]==int(oo[0,0]) and oo[0,1]==int(oo[0,1]))\n        C1 = oo.shape == (oo[0,0] + oo[0,1] + 1, 2)\n        if not (C0 and C1):\n            sha = (oo[0,0]+oo[0,1]+1,2)\n            msg = \"The shape of the array is not as expected!\\n\"\n            msg += \"    file : %s\\n\"%pfe\n            if not C0:\n                msg += \"  The first line should contain integers!\\n\"\n                msg += \"    First line : {0}\".format(oo[0,:])\n            else:\n                msg += \"    Expected shape: {0}\".format(sha)\n                msg += \" = ({0} + {1} + 1, 2)\\n\".format(oo[0,0], oo[0,1])\n                msg += \"    Observed shape: {0}\".format(oo.shape)\n            raise Exception(msg)\n        npts, noccur = int(oo[0,0]), int(oo[0,1])\n        poly = oo[1:1+npts,:]\n        if noccur>0:\n            pos, extent = oo[1+npts:,0], oo[1+npts:,1]\n        else:\n            pos, extent = None, None\n\n         # Try reading Exp and Name if not provided\n        if Exp is None:\n            Exp = cls._from_txt_extract_params(pfe, 'Exp')\n        if Name is None:\n            Name = cls._from_txt_extract_params(pfe, 'Name')\n\n        if out=='dict':\n            return {'poly':poly, 'pos':pos, 'extent':extent}\n        else:\n            SavePath = os.path.abspath(SavePath)\n            obj = cls(Name=Name, Exp=Exp, shot=shot,\n                      Type=Type, mobile=mobile,\n                      Poly=poly, pos=pos, extent=extent,\n                      SavePath=SavePath, color=color)\n            return obj"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the current circulating on the coil A.", "response": "def set_I(self, I=None):\n        \"\"\" Set the current circulating on the coil (A) \"\"\"\n        C0 = I is None\n        C1 = type(I) in [int,float,np.int64,np.float64]\n        C2 = type(I) in [list,tuple,np.ndarray]\n        msg = \"Arg I must be None, a float or an 1D np.ndarray !\"\n        assert C0 or C1 or C2, msg\n        if C1:\n            I = np.array([I],dtype=float)\n        elif C2:\n            I = np.asarray(I,dtype=float).ravel()\n        self._dmag['I'] = I\n        if C0:\n            self._dmag['nI'] = 0\n        else:\n            self._dmag['nI'] = I.size"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef lStruct(self):\n        lStruct = []\n        for k in self._dStruct['lorder']:\n            k0, k1 = k.split('_')\n            lStruct.append(self._dStruct['dObj'][k0][k1])\n        return lStruct", "response": "Return the list of Struct that was used for creation"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef lStructIn(self):\n        lStruct = []\n        for k in self._dStruct['lorder']:\n            k0, k1 = k.split('_')\n            if type(self._dStruct['dObj'][k0][k1]) is str:\n                if any([ss in self._dStruct['dObj'][k0][k1]\n                        for ss in ['Ves','PlasmaDomain']]):\n                    lStruct.append(self._dStruct['dObj'][k0][k1])\n            elif issubclass(self._dStruct['dObj'][k0][k1].__class__, StructIn):\n                lStruct.append(self._dStruct['dObj'][k0][k1])\n        return lStruct", "response": "Return the list of StructIn contained in self. lStruct\n\n        As tofu objects or SavePath + SaveNames"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_Struct(self, struct=None,\n                   Cls=None, Name=None, Poly=None,\n                   mobile=False, shot=None,\n                   Lim=None, Type=None,\n                   dextraprop=None):\n        \"\"\" Add a Struct instance to the config\n\n        An already existing Struct subclass instance can be added\n        Or it will be created from the (Cls,Name,Poly,Lim) keyword args\n\n        \"\"\"\n        # Check inputs\n        C0a = struct is None\n        C1a = all([ss is None for ss in [Cls,Name,Poly,Lim,Type]])\n        if not np.sum([C0a,C1a])==1:\n            msg = \"Provide either:\"\n            msg += \"\\n    - struct: a Struct subclass instance\"\n            msg += \"\\n    - the keyword args to create one\"\n            msg += \"\\n        (Cls,Name,Poly,Lim,Type)\\n\"\n            msg += \"\\n You provded:\"\n            msg += \"\\n    - struct: {0}, {1}\".format(str(struct),\n                                                     type(struct))\n            raise Exception(msg)\n\n        # Create struct if not provided\n        if C0a:\n            if not (type(Cls) is str or issubclass(Cls,Struct)):\n                msg = \"Cls must be either:\"\n                msg += \"\\n    - a Struct subclass\"\n                msg += \"\\n    - the str Name of it (e.g.: 'PFC','CoilPF',...)\"\n                raise Exception(msg)\n            if type(Cls) is str:\n                Cls = eval('%s'%Cls)\n\n            # Preformat Lim and Type\n            if Lim is None:\n                Lim = self.Lim\n            if Type is None:\n                Type = self.Id.Type\n\n            # Create instance\n            struct = Cls(Poly=Poly, Name=Name, Lim=Lim, Type=Type,\n                         mobile=mobile, shot=shot, Exp=self.Id.Exp)\n\n        C0b = issubclass(struct.__class__, Struct)\n        assert C0b, \"struct must be a Struct subclass instance !\"\n\n        # Prepare dextraprop\n        dextra = self.dextraprop\n        lk = sorted([k[1:] for k in dextra.keys() if k!='lprop'])\n        if dextraprop is None:\n            if not dextra in [None,{}]:\n                msg = \"The current Config instance has the following extraprop:\"\n                msg += \"\\n    - \" + \"\\n    - \".join(lk)\n                msg += \"\\n  => Please specify a dextraprop for struct !\"\n                msg += \"\\n     (using the same keys !)\"\n                raise Exception(msg)\n        else:\n            assert isinstance(dextraprop,dict)\n            assert all([k in lk for k in dextraprop.keys()])\n            assert all([k in dextraprop.keys() for k in lk])\n            dx = {}\n            for k in lk:\n                dk = 'd'+k\n                dx[k] = {}\n                for k0 in dextra[dk].keys():\n                    dx[k][k0] = {}\n                    for k1 in dextra[dk][k0].keys():\n                        dx[k][k0][k1] = dextra[dk][k0][k1]\n                if not struct.Id.Cls in dx[k].keys():\n                    dx[k][struct.Id.Cls] = {struct.Id.Name:dextraprop[k]}\n                else:\n                    dx[k][struct.Id.Cls][struct.Id.Name] = dextraprop[k]\n\n        # Set self.lStruct\n        lS = self.lStruct + [struct]\n        self._init(lStruct=lS, Lim=self.Lim, dextraprop=dx)", "response": "Add a Struct instance to the config."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_color(self):\n        col = np.full((self._dStruct['nObj'],4), np.nan)\n        ii = 0\n        for k in self._dStruct['lorder']:\n            k0, k1 = k.split('_')\n            col[ii,:] = self._dStruct['dObj'][k0][k1].get_color()\n            ii += 1\n        return col", "response": "Return the array of rgba colors"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_summary(self, verb=False, max_columns=100, width=1000):\n        # Make sure the data is accessible\n        msg = \"The data is not accessible because self.strip(2) was used !\"\n        assert self._dstrip['strip']<2, msg\n\n        # Build the list\n        d = self._dStruct['dObj']\n        data = []\n        for k in self._ddef['dStruct']['order']:\n            if k not in d.keys():\n                continue\n            for kk in d[k].keys():\n                lu = [k,\n                      self._dStruct['dObj'][k][kk]._Id._dall['Name'],\n                      self._dStruct['dObj'][k][kk]._Id._dall['SaveName'],\n                      self._dStruct['dObj'][k][kk]._dgeom['nP'],\n                      self._dStruct['dObj'][k][kk]._dgeom['noccur'],\n                      self._dStruct['dObj'][k][kk]._dgeom['mobile'],\n                      self._dStruct['dObj'][k][kk]._dmisc['color']]\n                for pp in self._dextraprop['lprop']:\n                    lu.append(self._dextraprop['d'+pp][k][kk])\n                data.append(lu)\n\n        # Build the pandas DataFrame\n        col = ['class', 'Name', 'SaveName', 'nP', 'noccur',\n               'mobile', 'color'] + self._dextraprop['lprop']\n        df = pd.DataFrame(data, columns=col)\n        pd.set_option('display.max_columns',max_columns)\n        pd.set_option('display.width',width)\n\n        if verb:\n            print(df)\n        return df", "response": "Return the summary description of the object content as a pandas DataFrame"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a 2D array of bools where each element in the array is inside the specified point.", "response": "def isInside(self, pts, In='(X,Y,Z)', log='any'):\n        \"\"\" Return a 2D array of bool\n\n        Equivalent to applying isInside to each Struct\n        Check self.lStruct[0].isInside? for details\n\n        Arg log determines how Struct with multiple Limits are treated\n            - 'all' : True only if pts belong to all elements\n            - 'any' : True if pts belong to any element\n        \"\"\"\n        msg = \"Arg pts must be a 1D or 2D np.ndarray !\"\n        assert isinstance(pts,np.ndarray) and pts.ndim in [1,2], msg\n        msg = \"Arg log must be in ['any','all']\"\n        assert log in ['any','all'], msg\n        if pts.ndim==1:\n            msg = \"Arg pts must contain the coordinates of a point !\"\n            assert pts.size in [2,3], msg\n            pts = pts.reshape((pts.size,1)).astype(float)\n        else:\n            msg = \"Arg pts must contain the coordinates of points !\"\n            assert pts.shape[0] in [2,3], pts\n        nP = pts.shape[1]\n\n        ind = np.zeros((self._dStruct['nObj'],nP), dtype=bool)\n        lStruct = self.lStruct\n        for ii in range(0,self._dStruct['nObj']):\n            indi = _GG._Ves_isInside(pts,\n                                     lStruct[ii].Poly,\n                                     Lim=lStruct[ii].Lim,\n                                     nLim=lStruct[ii].noccur,\n                                     VType=lStruct[ii].Id.Type,\n                                     In=In, Test=True)\n            if lStruct[ii].noccur>1:\n                if log=='any':\n                    indi = np.any(indi,axis=0)\n                else:\n                    indi = np.all(indi,axis=0)\n            ind[ii,:] = indi\n        return ind"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef select(self, key=None, val=None, touch=None, log='any', out=int):\n        assert out in [int,bool]\n        assert log in ['any','all','not']\n        C = [key is None,touch is None]\n        assert np.sum(C)>=1\n        if np.sum(C)==2:\n            ind = np.ones((self.nRays,),dtype=bool)\n        else:\n            if key is not None:\n                assert type(key) is str and key in self._dchans.keys()\n                ltypes = [str,int,float,np.int64,np.float64]\n                C0 = type(val) in ltypes\n                C1 = type(val) in [list,tuple,np.ndarray]\n                assert C0 or C1\n                if C0:\n                    val = [val]\n                else:\n                    assert all([type(vv) in ltypes for vv in val])\n                ind = np.vstack([self._dchans[key]==ii for ii in val])\n                if log=='any':\n                    ind = np.any(ind,axis=0)\n                elif log=='all':\n                    ind = np.all(ind,axis=0)\n                else:\n                    ind = ~np.any(ind,axis=0)\n\n            elif touch is not None:\n                lint = [int,np.int64]\n                larr = [list,tuple,np.ndarray]\n                touch = [touch] if not type(touch) is list else touch\n                assert len(touch) in [1,2,3]\n\n                def _check_touch(tt):\n                    cS = type(tt) is str and len(tt.split('_'))==2\n                    c0 = type(tt) in lint\n                    c1 = type(tt) in larr and len(tt)>=0\n                    c1 = c1 and all([type(t) in lint for t in tt])\n                    return cS, c0, c1\n                for ii in range(0,3-len(touch)):\n                    touch.append([])\n\n                ntouch = len(touch)\n                assert ntouch == 3\n\n                for ii in range(0,ntouch):\n                    cS, c0, c1 = _check_touch(touch[ii])\n\n                    if not (cS or c0 or c1):\n                        msg = \"Provided touch is not valid:\\n\"%touch\n                        msg += \"    - Provided: %s\\n\"%str(touch)\n                        msg += \"Please provide either:\\n\"\n                        msg += \"    - str in the form 'Cls_Name'\\n\"\n                        msg += \"    - int (index)\\n\"\n                        msg += \"    - array of int indices\"\n                        raise Exception(msg)\n\n                    if cS:\n                        lS = self.lStruct_computeInOut\n                        k0, k1 = touch[ii].split('_')\n                        ind = [jj for jj in range(0,len(lS))\n                               if lS[jj].Id.Cls==k0 and lS[jj].Id.Name==k1]\n                        assert len(ind)==1\n                        touch[ii] = [ind[0]]\n                    elif c0:\n                        touch[ii] = [touch[ii]]\n\n                # Common part\n                ind = np.zeros((ntouch,self.nRays),dtype=bool)\n                for i in range(0,ntouch):\n                    if len(touch[i])==0:\n                        ind[i,:] = True\n                    else:\n                        for n in range(0,len(touch[i])):\n                            ind[i,:] = np.logical_or(ind[i,:],\n                                                     self._dgeom['indout'][i,:]==touch[i][n])\n                ind = np.all(ind,axis=0)\n                if log=='not':\n                    ind[:] = ~ind\n        if out is int:\n            ind = ind.nonzero()[0]\n        return ind", "response": "Select the indices of the rays that are in the current set of elements in the current set of elements."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the L coordinates of the cross - section projections", "response": "def _get_plotL(self, Lplot='Tot', proj='All', ind=None, multi=False):\n        \"\"\" Get the (R,Z) coordinates of the cross-section projections \"\"\"\n        ind = self._check_indch(ind)\n        if ind.size>0:\n            Ds, us = self.D[:,ind], self.u[:,ind]\n            if ind.size==1:\n                Ds, us = Ds.reshape((3,1)), us.reshape((3,1))\n            kPIn, kPOut = self.kIn[ind], self.kOut[ind]\n            if self.config.Id.Type=='Tor':\n                kRMin = self._dgeom['kRMin'][ind]\n            else:\n                kRMin = None\n            pts = _comp.LOS_CrossProj(self.config.Id.Type, Ds, us,\n                                      kPIn, kPOut, kRMin, proj=proj,\n                                      Lplot=Lplot, multi=multi)\n        else:\n            pts = None\n        return pts"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a linear sampling of the LOS into a series of points and segments lengths and a specific method", "response": "def get_sample(self, res, resMode='abs', DL=None, method='sum', ind=None,\n                  compact=False):\n        \"\"\" Return a linear sampling of the LOS\n\n        The LOS is sampled into a series a points and segments lengths\n        The resolution (segments length) is <= res\n        The sampling can be done according to different methods\n        It is possible to sample only a subset of the LOS\n\n        Parameters\n        ----------\n        res:     float\n            Desired resolution\n        resMode: str\n            Flag indicating res should be understood as:\n                - 'abs':    an absolute distance in meters\n                - 'rel':    a relative distance (fraction of the LOS length)\n        DL:     None / iterable\n            The fraction [L1;L2] of the LOS that should be sampled, where\n            L1 and L2 are distances from the starting point of the LOS (LOS.D)\n        method: str\n            Flag indicating which to use for sampling:\n                - 'sum':    the LOS is sampled into N segments of equal length,\n                            where N is the smallest int such that:\n                                * segment length <= resolution(res,resMode)\n                            The points returned are the center of each segment\n                - 'simps':  the LOS is sampled into N segments of equal length,\n                            where N is the smallest int such that:\n                                * segment length <= resolution(res,resMode)\n                                * N is even\n                            The points returned are the egdes of each segment\n                - 'romb':   the LOS is sampled into N segments of equal length,\n                            where N is the smallest int such that:\n                                * segment length <= resolution(res,resMode)\n                                * N = 2^k + 1\n                            The points returned are the egdes of each segment\n\n        Returns\n        -------\n        pts:    np.ndarray\n            A (3,NP) array of NP points along the LOS in (X,Y,Z) coordinates\n        k:      np.ndarray\n            A (NP,) array of the points distances from the LOS starting point\n        reseff: float\n            The effective resolution (<= res input), as an absolute distance\n\n        \"\"\"\n        ind = self._check_indch(ind)\n        # preload k\n        kIn = self.kIn\n        kOut = self.kOut\n\n        # Preformat DL\n        if DL is None:\n            DL = np.array([kIn[ind], kOut[ind]])\n        elif np.asarray(DL).size==2:\n            DL = np.tile(np.asarray(DL).ravel(),(len(ind),1)).T\n        DL = np.ascontiguousarray(DL).astype(float)\n        assert type(DL) is np.ndarray and DL.ndim==2\n        assert DL.shape==(2,len(ind)), \"Arg DL has wrong shape !\"\n\n        # Check consistency of limits\n        ii = DL[0,:] < kIn[ind]\n        DL[0,ii] = kIn[ind][ii]\n        ii[:] = DL[0,:] >= kOut[ind]\n        DL[0,ii] = kOut[ind][ii]\n        ii[:] = DL[1,:] > kOut[ind]\n        DL[1,ii] = kOut[ind][ii]\n        ii[:] = DL[1,:] <= kIn[ind]\n        DL[1,ii] = kIn[ind][ii]\n\n        # Preformat Ds, us\n        Ds, us = self.D[:,ind], self.u[:,ind]\n        if len(ind)==1:\n            Ds, us = Ds.reshape((3,1)), us.reshape((3,1))\n        Ds, us = np.ascontiguousarray(Ds), np.ascontiguousarray(us)\n\n        # Launch    # NB : find a way to exclude cases with DL[0,:]>=DL[1,:] !!\n        # Todo : reverse in _GG : make compact default for faster computation !\n        lpts, k, reseff = _GG.LOS_get_sample(Ds, us, res, DL,\n                                             dLMode=resMode, method=method)\n        if compact:\n            pts = np.concatenate(lpts, axis=1)\n            ind = np.array([pt.shape[1] for pt in lpts], dtype=int)\n            ind = np.cumsum(ind)[:-1]\n            return pts, k, reseff, ind\n        else:\n            return lpts, k, reseff"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the intersection points of each ray with each isoflux and returns the intersection points of each isoflux and the intersection points of each isoflux.", "response": "def calc_kInkOut_IsoFlux(self, lPoly, lVIn=None, Lim=None,\n                             kInOut=True):\n        \"\"\" Calculate the intersection points of each ray with each isoflux\n\n        The isofluxes are provided as a list of 2D closed polygons\n\n        The intersections are the inward and outward intersections\n        They are retruned as two np.ndarrays: kIn and kOut\n        Each array contains the length parameter along the ray for each isoflux\n\n        Parameters\n        ----------\n\n\n        Returns\n        -------\n\n        \"\"\"\n\n        # Preformat input\n        nPoly, lPoly, lVIn = self._kInOut_IsoFlux_inputs_usr(lPoly, lVIn=lVIn)\n\n        # Prepare output\n        kIn = np.full((self.nRays,nPoly), np.nan)\n        kOut = np.full((self.nRays,nPoly), np.nan)\n\n        # Compute intersections\n        assert(self._method in ['ref', 'optimized'])\n        if self._method=='ref':\n            for ii in range(0,nPoly):\n                largs, dkwd = self._kInOut_IsoFlux_inputs([lPoly[ii]],\n                                                          lVIn=[lVIn[ii]])\n                out = _GG.SLOW_LOS_Calc_PInOut_VesStruct(*largs, **dkwd)\n                PIn, POut, kin, kout, VperpIn, vperp, IIn, indout = out\n                kIn[:,ii], kOut[:,ii] = kin, kout\n        elif self._method==\"optimized\":\n            for ii in range(0,nPoly):\n                largs, dkwd = self._kInOut_IsoFlux_inputs([lPoly[ii]],\n                                                          lVIn=[lVIn[ii]])\n\n                out = _GG.LOS_Calc_PInOut_VesStruct(*largs, **dkwd)\n                kin, kout, _, _ = out\n                kIn[:,ii], kOut[:,ii] = kin, kout\n        if kInOut:\n            indok = ~np.isnan(kIn)\n            ind = np.zeros((self.nRays,nPoly), dtype=bool)\n            kInref = np.tile(self.kIn[:,np.newaxis],nPoly)\n            kOutref = np.tile(self.kOut[:,np.newaxis],nPoly)\n            ind[indok] = (kIn[indok]<kInref[indok]) | (kIn[indok]>kOutref[indok])\n            kIn[ind] = np.nan\n\n            ind[:] = False\n            indok[:] = ~np.isnan(kOut)\n            ind[indok] = (kOut[indok]<kInref[indok]) | (kOut[indok]>kOutref[indok])\n            kOut[ind] = np.nan\n\n        return kIn, kOut"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates the signal of a given time - vector and returns it.", "response": "def calc_signal(self, ff, t=None, ani=None, fkwdargs={}, Brightness=True,\n                    res=0.005, DL=None, resMode='abs', method='sum',\n                    ind=None, out=object, plot=True, dataname=None,\n                    fs=None, dmargin=None, wintit=None, invert=True,\n                    units=None, draw=True, connect=True):\n        \"\"\" Return the line-integrated emissivity\n\n        Beware, by default, Brightness=True and it is only a line-integral !\n\n        Indeed, to get the received power, you need an estimate of the Etendue\n        (previously set using self.set_Etendues()) and use Brightness=False.\n\n        Hence, if Brightness=True and if\n        the emissivity is provided in W/m3 (resp. W/m3/sr),\n        => the method returns W/m2 (resp. W/m2/sr)\n        The line is sampled using :meth:`~tofu.geom.LOS.get_sample`,\n\n        The integral can be computed using three different methods:\n            - 'sum':    A numpy.sum() on the local values (x segments lengths)\n            - 'simps':  using :meth:`scipy.integrate.simps`\n            - 'romb':   using :meth:`scipy.integrate.romb`\n\n        Except ff, arguments common to :meth:`~tofu.geom.LOS.get_sample`\n\n        Parameters\n        ----------\n        ff :    callable\n            The user-provided\n\n        Returns\n        -------\n        sig :   np.ndarray\n            The computed signal, a 1d or 2d array depending on whether a time\n            vector was provided.\n        units:  str\n            Units of the result\n\n        \"\"\"\n        msg = \"Arg out must be in [object,np.ndarray]\"\n        assert out in [object,np.ndarray], msg\n        assert type(Brightness) is bool, \"Arg Brightness must be a bool !\"\n        if Brightness is False and self.Etendues is None:\n            msg = \"Etendue must be set if Brightness is False !\"\n            raise Exception(msg)\n\n        # Preformat ind\n        ind = self._check_indch(ind)\n        # Preformat DL\n        kIn, kOut = self.kIn, self.kOut\n        if DL is None:\n            DL = np.array([kIn[ind], kOut[ind]])\n        elif np.asarray(DL).size==2:\n            DL = np.tile(np.asarray(DL).ravel()[:,np.newaxis],len(ind))\n        DL = np.ascontiguousarray(DL).astype(float)\n        assert type(DL) is np.ndarray and DL.ndim==2\n        assert DL.shape==(2,len(ind)), \"Arg DL has wrong shape !\"\n\n        # check limits\n        ii = DL[0,:] < kIn[ind]\n        DL[0,ii] = kIn[ind][ii]\n        ii[:] = DL[0,:] >= kOut[ind]\n        DL[0,ii] = kOut[ind][ii]\n        ii[:] = DL[1,:] > kOut[ind]\n        DL[1,ii] = kOut[ind][ii]\n        ii[:] = DL[1,:] <= kIn[ind]\n        DL[1,ii] = kIn[ind][ii]\n\n        # Preformat Ds, us and Etendue\n        Ds, us = self.D[:,ind], self.u[:,ind]\n        if Brightness is False:\n            E = self.Etendues\n            if E.size==self.nRays:\n                E = E[ind]\n\n        # Preformat signal\n        if len(ind)==1:\n            Ds, us = Ds.reshape((3,1)), us.reshape((3,1))\n        if t is None or len(t)==1:\n            sig = np.full((Ds.shape[1],),np.nan)\n        else:\n            sig = np.full((len(t),Ds.shape[1]),np.nan)\n        indok = ~(np.any(np.isnan(DL),axis=0) | np.any(np.isinf(DL),axis=0)\n                  | ((DL[1,:]-DL[0,:])<=0.))\n\n        if np.any(indok):\n            Ds, us, DL = Ds[:,indok], us[:,indok], DL[:,indok]\n            if indok.sum()==1:\n                Ds, us = Ds.reshape((3,1)), us.reshape((3,1))\n                DL = DL.reshape((2,1))\n            Ds, us = np.ascontiguousarray(Ds), np.ascontiguousarray(us)\n            DL = np.ascontiguousarray(DL)\n            # Launch    # NB : find a way to exclude cases with DL[0,:]>=DL[1,:] !!\n            # Exclude Rays not seeing the plasma\n            s = _GG.LOS_calc_signal(ff, Ds, us, res, DL,\n                                    dLMode=resMode, method=method,\n                                    t=t, Ani=ani, fkwdargs=fkwdargs, Test=True)\n            if t is None or len(t)==1:\n                sig[indok] = s\n            else:\n                sig[:,indok] = s\n\n        # Format output\n        if Brightness is False:\n            if dataname is None:\n                dataname = r\"LOS-integral x Etendue\"\n            if t is None or len(t)==1 or E.size==1:\n                sig = sig*E\n            else:\n                sig = sig*E[np.newaxis,:]\n            if units is None:\n                units = r\"origin x $m^3.sr$\"\n        else:\n            if dataname is None:\n                dataname = r\"LOS-integral\"\n            if units is None:\n                units = r\"origin x m\"\n\n        if plot or out in [object,'object']:\n            kwdargs = dict(data=sig, t=t, lCam=self, Name=self.Id.Name,\n                           dlabels={'data':{'units':units, 'name':dataname}},\n                           Exp=self.Id.Exp, Diag=self.Id.Diag)\n            import tofu.data as tfd\n            if self._is2D():\n                osig = tfd.DataCam2D(**kwdargs)\n            else:\n                osig = tfd.DataCam1D(**kwdargs)\n            if plot:\n                kh = osig.plot(fs=fs, dmargin=dmargin, wintit=wintit,\n                               plotmethod=plotmethod, invert=invert,\n                               draw=draw, connect=connect)\n\n        if out in [object, 'object']:\n            return osig\n        else:\n            return sig, units"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nplotting the Rays and LOS in the chosen projection.", "response": "def plot(self, lax=None, proj='all', Lplot=_def.LOSLplot, element='L',\n             element_config='P', Leg='', dL=None, dPtD=_def.LOSMd,\n             dPtI=_def.LOSMd, dPtO=_def.LOSMd, dPtR=_def.LOSMd,\n             dPtP=_def.LOSMd, dLeg=_def.TorLegd, multi=False, ind=None,\n             fs=None, wintit=None, draw=True, Test=True):\n        \"\"\" Plot the Rays / LOS, in the chosen projection(s)\n\n        Optionnally also plot associated :class:`~tofu.geom.Ves` and Struct\n        The plot can also include:\n            - special points\n            - the unit directing vector\n\n        Parameters\n        ----------\n        Lax :       list / plt.Axes\n            The axes for plotting (list of 2 axes if Proj='All')\n            If None a new figure with new axes is created\n        Proj :      str\n            Flag specifying the kind of projection:\n                - 'Cross' : cross-section\n                - 'Hor' : horizontal\n                - 'All' : both cross-section and horizontal (on 2 axes)\n                - '3d' : a (matplotlib) 3d plot\n        element :   str\n            Flag specifying which elements to plot\n            Each capital letter corresponds to an element:\n                * 'L': LOS\n                * 'D': Starting point of the LOS\n                * 'I': Input point (i.e.: where the LOS enters the Vessel)\n                * 'O': Output point (i.e.: where the LOS exits the Vessel)\n                * 'R': Point of minimal major radius R (only if Ves.Type='Tor')\n                * 'P': Point of used for impact parameter (i.e.: with minimal\n                        distance to reference point Sino_RefPt)\n        Lplot :     str\n            Flag specifying the length to plot:\n                - 'Tot': total length, from starting point (D) to output point\n                - 'In' : only the in-vessel fraction (from input to output)\n        element_config : str\n            Fed to self.config.plot()\n        Leg :       str\n            Legend, if Leg='' the LOS name is used\n        dL :     dict / None\n            Dictionary of properties for plotting the lines\n            Fed to plt.Axes.plot(), set to default if None\n        dPtD :      dict\n            Dictionary of properties for plotting point 'D'\n        dPtI :      dict\n            Dictionary of properties for plotting point 'I'\n        dPtO :      dict\n            Dictionary of properties for plotting point 'O'\n        dPtR :      dict\n            Dictionary of properties for plotting point 'R'\n        dPtP :      dict\n            Dictionary of properties for plotting point 'P'\n        dLeg :      dict or None\n            Dictionary of properties for plotting the legend\n            Fed to plt.legend(), the legend is not plotted if None\n        draw :      bool\n            Flag indicating whether fig.canvas.draw() shall be called\n        a4 :        bool\n            Flag indicating whether to plot the figure in a4 dimensions\n        Test :      bool\n        a4 :        bool\n            Flag indicating whether to plot the figure in a4 dimensions\n        Test :      bool\n        a4 :        bool\n            Flag indicating whether to plot the figure in a4 dimensions\n        Test :      bool\n        a4 :        bool\n            Flag indicating whether to plot the figure in a4 dimensions\n        Test :      bool\n        Test :      bool\n            Flag indicating whether the inputs should be tested for conformity\n\n        Returns\n        -------\n        La :        list / plt.Axes\n            Handles of the axes used for plotting (list if Proj='All')\n\n        \"\"\"\n\n        return _plot.Rays_plot(self, Lax=lax, Proj=proj, Lplot=Lplot,\n                               element=element, element_config=element_config, Leg=Leg,\n                               dL=dL, dPtD=dPtD, dPtI=dPtI, dPtO=dPtO, dPtR=dPtR,\n                               dPtP=dPtP, dLeg=dLeg, multi=multi, ind=ind,\n                               fs=fs, wintit=wintit, draw=draw, Test=Test)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nplot the sinograms in the cross - section of the current class.", "response": "def plot_sino(self, ax=None, element=_def.LOSImpElt, Sketch=True,\n                  Ang=_def.LOSImpAng, AngUnit=_def.LOSImpAngUnit, Leg=None,\n                  dL=_def.LOSMImpd, dVes=_def.TorPFilld, dLeg=_def.TorLegd,\n                  ind=None, multi=False,\n                  fs=None, wintit=None, draw=True, Test=True):\n        \"\"\" Plot the LOS in projection space (sinogram)\n\n        Plot the Rays in projection space (cf. sinograms) as points.\n        Can also optionnally plot the associated :class:`~tofu.geom.Ves`\n\n        Can plot the conventional projection-space (in 2D in a cross-section),\n        or a 3D extrapolation of it, where the third coordinate is provided by\n        the angle that the LOS makes with the cross-section plane\n        (useful in case of multiple LOS with a partially tangential view)\n\n        Parameters\n        ----------\n        Proj :      str\n            Flag indicating whether to plot:\n                - 'Cross':  a classic sinogram (vessel cross-section)\n                - '3d': an extended 3D version ('3d'), with an additional angle\n        ax :        None / plt.Axes\n            The axes on which to plot, if None a new figure is created\n        Elt :       str\n            Flag indicating which elements to plot (one per capital letter):\n                * 'L': LOS\n                * 'V': Vessel\n        Ang  :      str\n            Flag indicating which angle to use for the impact parameter:\n                - 'xi': the angle of the line itself\n                - 'theta': its impact parameter (theta)\n        AngUnit :   str\n            Flag for the angle units to be displayed:\n                - 'rad': for radians\n                - 'deg': for degrees\n        Sketch :    bool\n            Flag indicating whether to plot a skecth with angles definitions\n        dL :        dict\n            Dictionary of properties for plotting the Rays points\n        dV :        dict\n            Dictionary of properties for plotting the vessel envelopp\n        dLeg :      None / dict\n            Dictionary of properties for plotting the legend\n            The legend is not plotted if None\n        draw :      bool\n            Flag indicating whether to draw the figure\n        a4 :        bool\n            Flag indicating whether the figure should be a4\n        Test :      bool\n            Flag indicating whether the inputs shall be tested for conformity\n\n        Returns\n        -------\n        ax :        plt.Axes\n            The axes used to plot\n\n        \"\"\"\n        if self._dsino['RefPt'] is None:\n            msg = \"The sinogram ref. point is not set !\"\n            msg += \"\\n  => run self.set_dsino()\"\n            raise Exception(msg)\n        return _plot.GLOS_plot_Sino(self, Proj='Cross', ax=ax, Elt=element, Leg=Leg,\n                                    Sketch=Sketch, Ang=Ang, AngUnit=AngUnit,\n                                    dL=dL, dVes=dVes, dLeg=dLeg,\n                                    ind=ind, fs=fs, wintit=wintit,\n                                    draw=draw, Test=Test)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting a dictionnary of Cls_Name struct with indices of Rays touching and in - out structures", "response": "def get_touch_dict(self, ind=None, out=bool):\n        \"\"\" Get a dictionnary of Cls_Name struct with indices of Rays touching\n\n        Only includes Struct object with compute = True\n            (as returned by self.lStruct__computeInOut_computeInOut)\n        Also return the associated colors\n        If in is not None, the indices for each Struct are split between:\n            - indok : rays touching Struct and in ind\n            - indout: rays touching Struct but not in ind\n\n        \"\"\"\n        if self.config is None:\n            msg = \"Config must be set in order to get touch dict !\"\n            raise Exception(msg)\n\n        dElt = {}\n        ind = self._check_indch(ind, out=bool)\n        for ss in self.lStruct_computeInOut:\n            kn = \"%s_%s\"%(ss.__class__.__name__, ss.Id.Name)\n            indtouch = self.select(touch=kn, out=bool)\n            if np.any(indtouch):\n                indok  = indtouch & ind\n                indout = indtouch & ~ind\n                if np.any(indok) or np.any(indout):\n                    if out == int:\n                        indok  = indok.nonzero()[0]\n                        indout = indout.nonzero()[0]\n                    dElt[kn] = {'indok':indok, 'indout':indout,\n                                'col':ss.get_color()}\n        return dElt"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef merge_urls_data_to(to, food={}):\n    if not to:\n        to.update(food)\n\n    for url, data in food.items():\n        if url not in to:\n            to[url] = data\n        else:\n            to[url] = to[url].merge_with(data)", "response": "Merge urls data into to"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmerging a small analyzed result to a big one, this function will modify the original ``to``", "response": "def merge_requests_data_to(to, food={}):\n    \"\"\"Merge a small analyzed result to a big one, this function will modify the \n    original ``to``\"\"\"\n    if not to:\n        to.update(food)\n\n    to['requests_counter']['normal'] += food['requests_counter']['normal']\n    to['requests_counter']['slow'] += food['requests_counter']['slow']\n    to['total_slow_duration'] += food['total_slow_duration']\n\n    for group_name, urls in food['data_details'].items():\n        if group_name not in to['data_details']:\n            to['data_details'][group_name] = urls\n        else:\n            to_urls = to['data_details'][group_name]\n            to_urls['duration_agr_data'] = to_urls['duration_agr_data'].merge_with(\n                    urls['duration_agr_data'])\n\n            # Merge urls data\n            merge_urls_data_to(to_urls['urls'], urls['urls'])"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nformat data from LogAnalyzer for render purpose", "response": "def format_data(raw_data, limit_per_url_group=LIMIT_PER_URL_GROUP, limit_url_groups=LIMIT_URL_GROUPS):\n    \"\"\"Fomat data from LogAnalyzer for render purpose\"\"\"\n    data = copy.deepcopy(raw_data)\n    for k, v in list(data['data_details'].items()):\n        # Only reserve first ``limit_per_url_group`` items\n        v['urls'] = sorted(list(v['urls'].items()), key=lambda k_v: k_v[1].total,\n                           reverse=True)[:limit_per_url_group]\n\n    data_details = sorted(iter(data['data_details'].items()),\n                          key=lambda k_v1: k_v1[1][\"duration_agr_data\"].total, \n                          reverse=True)[:limit_url_groups]\n\n    if data['requests_counter']['normal']:\n        slow_rate = format(data['requests_counter']['slow'] / \\\n                           float(data['requests_counter']['normal']), '.2%')\n    else:\n        slow_rate = '-'\n    data.update({\n        'slow_rate': slow_rate,\n        'data_details': data_details,\n    })\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_result_group_names(self, request_datetime):\n        request_date = request_datetime.date()\n        today = datetime.date.today()\n        yesterday = datetime.date.today() - datetime.timedelta(days=1)\n        result = []\n        if total_seconds(datetime.datetime.now() - request_datetime) < REALTIME_UPDATE_INTERVAL:\n            result.append('last_interval')\n        if request_date == today:\n            result.append(today.isoformat())\n        elif request_date == yesterday:\n            result.append(yesterday.isoformat())\n        return result", "response": "Get the list of result group names for the current time period"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef annotate_rule_violation(self, rule: ValidationRule) -> None:\n        if self.errors.get(rule.label) is None:\n            self.errors[rule.label] = []\n        self.errors[rule.label].append(rule.get_error_message())", "response": "Adds a rule violation to the error message list."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef annotate_exception(self, exception: Exception, rule: ValidationRule = None) -> None:\n        error_key = rule.label if isinstance(rule, ValidationRule) else 'get_rules'\n        if self.errors.get(error_key) is None:\n            self.errors[error_key] = []\n        self.errors[error_key].append(str(exception))", "response": "Adds an exception to the error list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nvalidate the current state of the current state of the current state and return a ValidationResult object.", "response": "def validate(self) -> ValidationResult:\n        \"\"\"\n        Apply the configured ValidationRule(s) (in the given order) and return a ValidationResult object.\n\n        :return: validation result\n        :rtype: ValidationResult\n        \"\"\"\n        result = ValidationResult()\n        try:\n            for rule in self.get_rules():\n                try:\n                    if not rule.apply():\n                        result.annotate_rule_violation(rule)\n                        if rule.stop_if_invalid:\n                            break\n                except Exception as e:\n                    result.annotate_exception(e, rule)\n        except Exception as e:\n            result.annotate_exception(e, None)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a polygon as a np. ndarray extracted from a txt file or a ToFu object with appropriate units", "response": "def get_PolyFromPolyFileObj(PolyFileObj, SavePathInp=None, units='m', comments='#', skiprows=0, shape0=2):\n    \"\"\" Return a polygon as a np.ndarray, extracted from a txt file or from a ToFu object, with appropriate units\n\n    Useful for :meth:`tofu.plugins.AUG.Ves._create()`\n\n    Parameters\n    ----------\n    PolyFileObj :   str / :mod:`tofu.geom` object / np.ndarray\n        The source where the polygon is to be found, either:\n            - str: the name of a file containing the coorindates of a polygon to be loaded with :meth:`numpy.loadtxt()`\n            - A :mod:`tofu.geom` object: with attribute 'Poly'\n            - np.ndarray: an 2-dimensional array containing the 2D cartesian coordinates of a polygon\n    SavePathInp :   str / None\n        The absolute path where the input file is stored\n    units :         str\n        Flag indicating in which units the polygon coordinates is expressed in the input file / object / array (will be converted to meters)\n    comments :      str\n        Parameter to be fed to :meth:`numpy.loadtxt()` if PolyFileObj is a file name\n    skiprows :      int\n        Parameter to be fed to :meth:`numpy.loadtxt()` if PolyFileObj is a file name\n    shape0 :          int\n        Specifies whether the loaded array is a (2,N) or (3,N) array (transposed it if necessary)\n\n    Returns\n    -------\n    Poly :          np.ndarray\n        (2,N) np.ndarray containing the 2D cartesian coordinates of the polygon, where N is the number of points\n    addInfo :       dict\n        Dictionaryb containing information on the origin of the polygon, for the record (e.g.: the name and absolute path of the file from which it was extracted)\n\n    \"\"\"\n    assert type(PolyFileObj) in [list,str] or hasattr(PolyFileObj,\"Poly\") or np.asarray(PolyFileObj).ndim==2, \"Arg PolyFileObj must be str (PathFileExt), a ToFu object with attribute Poly or an iterable convertible to 2d np.ndarray !\"\n\n    # Load PolyFileObj if file and check shape\n    addInfo = {}\n    if type(PolyFileObj) in [list,str]:\n        PathFileExt = get_FileFromInfos(Path=SavePathInp, Name=PolyFileObj)\n        # Include PathFileExt in ID for tracability\n        addInfo = {'Input':PathFileExt}\n        PolyFileObj = np.loadtxt(PathFileExt, dtype=float, comments=comments, delimiter=None, converters=None, skiprows=skiprows, usecols=None, unpack=False, ndmin=2)\n    elif hasattr(PolyFileObj,\"Poly\"):\n        addInfo = {'Input':PolyFileObj.Id.SaveName}\n        PolyFileObj = PolyFileObj.Poly\n\n    Poly = np.asarray(PolyFileObj)\n    assert Poly.ndim==2 and shape0 in Poly.shape and max(Poly.shape)>=3 and not np.any(np.isnan(Poly)), \"Arg np.asarray(PolyFileObj) must be a (2,N) or (N,2) np.ndarray with non NaNs !\"\n    Poly = Poly if Poly.shape[0]==shape0 else Poly.T\n    Poly = convert_units(Poly, In=units, Out='m')\n    return Poly, addInfo"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef SaveName_Conv(Mod=None, Cls=None, Type=None, Name=None, Deg=None,\n                  Exp=None, Diag=None, shot=None, version=None, usr=None,\n                  Include=defInclude):\n    \"\"\" Return a default name for saving the object\n\n    Includes key info for fast identification of the object from file name\n    Used on object creation by :class:`~tofu.pathfile.ID`\n    It is recommended to use this default name.\n\n    \"\"\"\n    Modstr = dModes[Mod] if Mod is not None else None\n    Include = defInclude if Include is None else Include\n    if Cls is not None and Type is not None and 'Type' in Include:\n        Clsstr = Cls+Type\n    else:\n        Clsstr = Cls\n    Dict = {'Mod':Modstr, 'Cls':Clsstr, 'Name':Name}\n    for ii in Include:\n        if not ii in ['Mod','Cls','Type','Name']:\n            Dict[ii] = None\n        if ii=='Deg' and Deg is not None:\n            Dict[ii] = dPref[ii]+'{0:02.0f}'.format(Deg)\n        elif ii=='shot' and shot is not None:\n            Dict[ii] = dPref[ii]+'{0:05.0f}'.format(shot)\n        elif not ii in ['Mod','Cls','Type','Name'] and eval(ii+' is not None'):\n            Dict[ii] = dPref[ii]+eval(ii)\n    if 'Data' in Cls:\n        Order = ['Mod','Cls','Exp','Deg','Diag','shot','Name','version','usr']\n    else:\n        Order = ['Mod','Cls','Exp','Deg','Diag','Name','shot','version','usr']\n\n    SVN = \"\"\n    for ii in range(0,len(Order)):\n        if Order[ii] in Include and Dict[Order[ii]] is not None:\n            SVN += '_' + Dict[Order[ii]]\n    SVN = SVN.replace('__','_')\n    if SVN[0]=='_':\n        SVN = SVN[1:]\n    return SVN", "response": "Save the object name for fast identification of the object from file name."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if two objects are the same instance of a ToFu object.", "response": "def CheckSameObj(obj0, obj1, LFields=None):\n    \"\"\" Check if two variables are the same instance of a ToFu class\n\n    Checks a list of attributes, provided by LField\n\n    Parameters\n    ----------\n    obj0 :      tofu object\n        A variable refering to a ToFu object of any class\n    obj1 :      tofu object\n        A variable refering to a ToFu object of the same class as obj0\n    LFields :   None / str / list\n        The criteria against which the two objects are evaluated:\n            - None: True is returned\n            - str or list: tests whether all listed attributes have the same value\n\n    Returns\n    -------\n    A :     bool\n        True only is LField is None or a list of attributes that all match\n\n    \"\"\"\n    A = True\n    if LField is not None and obj0.__class__==obj1.__class__:\n        assert type(LFields) in [str,list]\n        if type(LFields) is str:\n            LFields = [LFields]\n        assert all([type(s) is str for s in LFields])\n        ind = [False for ii in range(0,len(LFields))]\n        Dir0 = dir(obj0.Id)+dir(obj0)\n        Dir1 = dir(obj1.Id)+dir(obj1)\n        for ii in range(0,len(LFields)):\n            assert LFields[ii] in Dir0, LFields[ii]+\" not in \"+obj0.Id.Name\n            assert LFields[ii] in Dir1, LFields[ii]+\" not in \"+obj1.Id.Name\n            if hasattr(obj0,LFields[ii]):\n                ind[ii] = np.all(getattr(obj0,LFields[ii])==getattr(obj1,LFields[ii]))\n            else:\n                ind[ii] = getattr(obj0.Id,LFields[ii])==getattr(obj1.Id,LFields[ii])\n        A = all(ind)\n    return A"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef SelectFromListId(LId, Val=None, Crit='Name',\n                     PreExp=None, PostExp=None, Log='any',\n                     InOut='In', Out=bool):\n    \"\"\" Return the indices or instances of all LOS matching criteria\n\n    The selection can be done according to 2 different mechanisms\n\n    Mechanism (1): provide the value (Val) a criterion (Crit) should match\n    The criteria are typically attributes of :class:`~tofu.pathfile.ID`\n    (i.e.: name, or user-defined attributes like the camera head...)\n\n    Mechanism (2): (used if Val=None)\n    Provide a str expression (or a list of such) to be fed to eval()\n    Used to check on quantitative criteria.\n        - PreExp: placed before the criterion value (e.g.: 'not ' or '<=')\n        - PostExp: placed after the criterion value\n        - you can use both\n\n    Other parameters are used to specify logical operators for the selection\n    (match any or all the criterion...) and the type of output.\n\n    Parameters\n    ----------\n    Crit :      str\n        Flag indicating which criterion to use for discrimination\n        Can be set to:\n            - any attribute of :class:`~tofu.pathfile.ID`\n              (e.g.: 'Name','SaveName','SavePath'...)\n            - any key of ID.USRdict (e.g.: 'Exp'...)\n    Val :       None / list / str\n        The value to match for the chosen criterion, can be a list\n        Used for selection mechanism (1)\n    PreExp :    None / list / str\n        A str (or list of such) expression to be fed to eval(),\n        Placed before the criterion value\n        Used for selection mechanism (2)\n    PostExp :   None / list / str\n        A str (or list of such) expression to be fed to eval()\n        Placed after the criterion value\n        Used for selection mechanism (2)\n    Log :       str\n        Flag indicating whether the criterion shall match:\n            - 'all': all provided values\n            - 'any': at least one of them\n    InOut :     str\n        Flag indicating whether the returned indices are:\n            - 'In': the ones matching the criterion\n            - 'Out': the ones not matching it\n    Out :       type / str\n        Flag indicating in which form to return the result:\n            - int: as an array of integer indices\n            - bool: as an array of boolean indices\n            - 'Name': as a list of names\n            - 'LOS': as a list of :class:`~tofu.geom.LOS` instances\n\n    Returns\n    -------\n    ind :       list / np.ndarray\n        The computed output, of nature defined by parameter Out\n\n    \"\"\"\n    C0 = type(Crit) is str\n    C1 = type(Crit) is list and all([type(cc) is str for cc in Crit])\n    assert C0 or C1, \"Arg Crit must be a str or list of str !\"\n    for rr in [PreExp,PostExp]:\n        if rr is not None:\n            C0 = type(rr) is str\n            C1 = type(rr) is list and all([type(ee) is str for ee in rr])\n            assert C0 or C1,  \"Args %S must be a str or list of str !\"%rr\n    assert Log in ['any','all'], \"Arg Log must be in ['any','all'] !\"\n    assert InOut in ['In','Out'], \"Arg InOut must be in ['In','Out'] !\"\n    if Val is None and PreExp is None and PostExp is None:\n        ind = np.ones((1,len(LId)),dtype=bool)\n    elif not Val is None:\n        if type(Val) is str:\n            Val=[Val]\n        N = len(Val)\n        ind = np.zeros((N,len(LId)),dtype=bool)\n        if Crit in dir(ID):\n            for ii in range(0,N):\n                ind[ii,:] = np.asarray([getattr(iid,Crit)==Val[ii]\n                                        for iid in LId],dtype=bool)\n        else:\n            for ii in range(0,N):\n                ind[ii,:] = np.asarray([iid.USRdict[Crit]==Val[ii]\n                                        for iid in LId],dtype=bool)\n    else:\n        if type(PreExp) is str:\n            PreExp = [PreExp]\n        if type(PostExp) is str:\n            PostExp = [PostExp]\n        if PreExp is None:\n            PreExp = [\"\" for ss in PostExp]\n        if PostExp is None:\n            PostExp = [\"\" for ss in PreExp]\n        assert len(PreExp)==len(PostExp), \"len(PreExp) should be =len(PostExp)\"\n        N = len(PreExp)\n        ind = np.zeros((N,len(LId)),dtype=bool)\n        if Crit in dir(ID):\n            for ii in range(0,N):\n                List = [eval(PreExp[ii]+\" getattr(iid,'%s') \"%Crit+PostExp[ii])\n                        for iid in LId]\n                ind[ii,:] = np.array(List,dtype=bool)\n        else:\n            for ii in range(0,N):\n                List = [eval(PreExp[ii]+\" iid.USRdict['%s'] \"%Crit+PostExp[ii])\n                        for iid in LId]\n                ind[ii,:] = np.asarray(List,dtype=bool)\n    ind = np.any(ind,axis=0) if Log=='any' else np.all(ind,axis=0)\n    if InOut=='Out':\n        ind = ~ind\n    if Out==int:\n        ind = ind.nonzero()[0]\n    elif Out is not bool and hasattr(ID,Out):\n        ind = [getattr(LId[ii],Out) for ii in ind.nonzero()[0]]\n    elif Out is not bool and Out in LId[0].USRdict.keys():\n        ind = [LId[ii].USRdict[Out] for ii in ind.nonzero()[0]]\n    return ind", "response": "Select the indices or instances of a list with the given list ID."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef FindSolFile(shot=0, t=0, Dt=None, Mesh='Rough1', Deg=2, Deriv='D2N2', Sep=True, Pos=True, OutPath='/afs/ipp-garching.mpg.de/home/d/didiv/Python/tofu/src/Outputs_AUG/'):\n    assert None in [t,Dt] and not (t is None and Dt is None), \"Arg t or Dt must be None, but not both !\"\n    LF = [ff for ff in os.listdir(OutPath) if 'TFI_Sol2D_AUG_SXR' in ff]\n    LF = [ff for ff in LF if all([ss in ff for ss in ['_'+str(shot)+'_', '_'+Mesh+'_D'+str(Deg), '_Deriv'+Deriv+'_Sep'+str(Sep)+'_Pos'+str(Pos)]])]\n    if len(LF)==0:\n        print(\"No matching Sol2D file in \", OutPath)\n        out = None\n    LDTstr = [ff[ff.index('_Dt')+3:ff.index('s_')] for ff in LF]\n    LDTstr = [(ss[:7],ss[8:]) for ss in LDTstr]\n    if t is None:\n        LF = [LF[ii] for ii in range(0,len(LF)) if LDTstr[ii][0]+'-'+LDTstr[ii][1]=='{0:07.4f}-{1:07.4f}'.format(Dt[0],Dt[1])]\n    elif Dt is None:\n        LF = [LF[ii] for ii in range(0,len(LF)) if t>=float(LDTstr[ii][0]) and t<=float(LDTstr[ii][1])]\n    if len(LF)==0:\n        print(\"No matching Sol2D file in \", OutPath)\n        out = None\n    elif len(LF)>1:\n        print(\"Several matching Sol2D files in \", OutPath)\n        print(LF)\n        out = None\n    else:\n        out = LF[0]\n    return out", "response": "This function finds a good Sol2D saved file in a given folder."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef Save_Generic(obj, SaveName=None, Path='./',\n                 Mode='npz', compressed=False, Print=True):\n    \"\"\" Save a ToFu object under file name SaveName, in folder Path\n\n    ToFu provides built-in saving and loading functions for ToFu objects.\n    There is now only one saving mode:\n        - 'npz': saves a dict of key attributes using :meth:`numpy.savez`\n\n    Good practices are:\n        - save :class:`~tofu.geom.Ves` and :class:`~tofu.geom.Struct`\n        - intermediate optics (:class:`~tofu.geom.Apert` and\n          :class:`~tofu.geom.Lens`) generally do not need to be saved\n          Indeed, they will be autoamtically included in larger objects\n          like Detect or Cam objects\n\n    Parameters\n    ----------\n    SaveName :      str\n        The file name, if None (recommended) uses obj.Id.SaveName\n    Path :          str\n        Path where to save the file\n    Mode :          str\n        Flag specifying the saving mode\n            - 'npz': Only mode currently available ('pck' deprecated)\n    compressed :    bool\n        Indicate whether to use np.savez_compressed (slower but smaller files)\n\n    \"\"\"\n    assert type(obj.__class__) is type\n    if SaveName is not None:\n        C = type(SaveName) is str and not (SaveName[-4]=='.')\n        assert C, \"SaveName should not include the extension !\"\n    assert Path is None or type(Path) is str\n    assert Mode in ['npz']\n    assert type(compressed) is bool\n    assert type(Print) is bool\n    if Path is None:\n        Path = obj.Id.SavePath\n    else:\n        obj._Id._SavePath = Path\n    if Mode=='npz':\n        Ext = '.npz'\n    if SaveName is None:\n        SaveName = obj.Id.SaveName\n    else:\n        obj._Id.set_SaveName(SaveName)\n    pathfileext = os.path.join(Path,SaveName+Ext)\n    if Ext=='.npz':\n        _save_np(obj, pathfileext, compressed=compressed)\n    if Print:\n        print(\"Saved in :  \"+pathfileext)", "response": "Save a generic ToFu object under file name SaveName in folder Path and Mode."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _save_np(obj, pathfileext, compressed=False):\n\n    func = np.savez_compressed if compressed else np.savez\n    dId = obj.Id._todict()\n\n    # tofu.geom\n    if obj.Id.Cls=='Ves':\n        func(pathfileext, Id=dId, arrayorder=obj._arrayorder, Clock=obj._Clock,\n             Poly=obj.Poly, Lim=obj.Lim, Sino_RefPt=obj.sino['RefPt'],\n             Sino_NP=obj.sino['NP'])\n\n    elif obj.Id.Cls=='Struct':\n        func(pathfileext, Id=dId, arrayorder=obj._arrayorder, Clock=obj._Clock,\n             Poly=obj.Poly, Lim=obj.Lim, mobile=obj._mobile)\n\n    elif obj.Id.Cls in ['Rays','LOS','LOSCam1D','LOSCam2D']:\n        func(pathfileext, Id=dId, extra=obj._extra,\n             geom=obj.geom, sino=obj.sino, dchans=obj.dchans)\n\n    elif obj.Id.Cls in ['Data','Data1D','Data2D']:\n        dsave = obj._todict()\n        if dsave['geom'] is not None and dsave['geom']['LCam'] is not None:\n            LCam = []\n            for cc in dsave['geom']['LCam']:\n                pathS = cc['Id']['SavePath']\n                pathN = cc['Id']['SaveName']\n                LCam.append(os.path.join(pathS,pathN+'.npz'))\n            dsave['geom'] = LCam\n        elif dsave['geom'] is not None:\n            geom = []\n            if dsave['geom']['Ves'] is not None:\n                pathS = dsave['geom']['Ves']['Id']['SavePath']\n                pathN = dsave['geom']['Ves']['Id']['SaveName']\n                Ves = os.path.join(pathS,pathN+'.npz')\n                geom += [Ves]\n            if dsave['geom']['LStruct'] is not None:\n                for ss in dsave['geom']['LStruct']:\n                    sf = os.path.join(ss['Id']['SavePath'],\n                                      ss['Id']['SaveName']+'.npz')\n                    geom += [sf]\n            dsave['geom'] = geom\n        func(pathfileext, **dsave)\n\n    \"\"\"\n    elif obj.Id.Cls=='GLOS':\n        LIdLOS = [ll.Id.todict() for ll in obj.LLOS]\n        LDs, Lus = np.array([ll.D for ll in obj.LLOS]).T, np.array([ll.u for ll in obj.LLOS]).T\n        func(pathfileext, Idsave=Idsave, LIdLOS=LIdLOS, LDs=LDs, Lus=Lus, Sino_RefPt=obj.Sino_RefPt, arrayorder=obj._arrayorder, Clock=obj._Clock)\n\n    elif obj.Id.Cls=='Lens':\n        func(pathfileext, Idsave=Idsave, arrayorder=obj._arrayorder, Clock=obj._Clock, O=obj.O, nIn=obj.nIn, Rad=[obj.Rad], F1=[obj.F1], F2=[obj.F2], R1=[obj.R1], R2=[obj.R2], dd=[obj.dd])\n\n    elif obj.Id.Cls=='Apert':\n        func(pathfileext, Idsave=Idsave, arrayorder=obj._arrayorder, Clock=obj._Clock, Poly=obj.Poly)\n\n    elif obj.Id.Cls=='Detect':\n        LOSprops, Sino, Span, Cone, SAng, SynthDiag, Res, Optics = _convert_Detect2Ldict(obj)\n        VesCalc = {'SavePath':None} if (not hasattr(obj,'_VesCalc') or obj._VesCalc is None) else {'SavePath':obj._VesCalc.Id.SavePath, 'SaveName':obj._VesCalc.Id.SaveName}\n        func(pathfileext, Idsave=Idsave, Poly=obj.Poly, Rad=obj.Rad, BaryS=obj.BaryS, nIn=obj.nIn, arrayorder=obj._arrayorder, Clock=obj._Clock, Sino_RefPt=obj.Sino_RefPt, LOSNP=[obj._LOS_NP],\n                LOSprops=[LOSprops], Sino=[Sino], Span=[Span], Cone=[Cone], SAng=[SAng], SynthDiag=[SynthDiag], Res=[Res], Optics=[Optics], VesCalc=[VesCalc])\n\n    elif obj.Id.Cls=='GDetect':\n        LDetsave, LDetSynthRes = [], []\n        for ii in range(0,obj.nDetect):\n            ddIdsave = obj.LDetect[ii].Id.todict()\n            LOSprops, Sino, Span, Cone, SAng, SynthDiag, Res, Optics = _convert_Detect2Ldict(obj.LDetect[ii])\n            VesCalc = {'SavePath':None} if (not hasattr(obj.LDetect[ii],'_VesCalc') or obj.LDetect[ii]._VesCalc is None) else {'SavePath':obj.LDetect[ii]._VesCalc.Id.SavePath, 'SaveName':obj.LDetect[ii]._VesCalc.Id.SaveName}\n            dd = dict(Idsave=ddIdsave, Poly=obj.LDetect[ii].Poly, Rad=obj.LDetect[ii].Rad, BaryS=obj.LDetect[ii].BaryS, nIn=obj.LDetect[ii].nIn, arrayorder=obj._arrayorder, Clock=obj._Clock, Sino_RefPt=obj.Sino_RefPt,\n                      LOSNP=[obj.LDetect[ii]._LOS_NP], LOSprops=[LOSprops], Sino=[Sino], Span=[Span], Cone=[Cone], SAng=[SAng], Optics=[Optics], VesCalc=[VesCalc])\n            LDetsave.append(dd)\n            LDetSynthRes.append({'SynthDiag':[SynthDiag],'Res':[Res]})\n        Res, lAttr = {}, dir(obj)\n        for pp in lAttr:\n            if not inspect.ismethod(getattr(obj,pp)) and '_Res' in pp:\n                Res[pp] = getattr(obj,pp)\n        func(pathfileext, Idsave=Idsave, arrayorder=obj._arrayorder, Clock=obj._Clock, Sino_RefPt=obj.Sino_RefPt, LOSRef=obj._LOSRef, Res=[Res], LDetsave=LDetsave, LDetSynthRes=LDetSynthRes)\n\n    # tofu.Eq\n    elif obj.Id.Cls=='Eq2D':\n        np.savez(pathfileext, Idsave=Idsave, **obj._Tab)\n\n    # tofu.mesh\n    elif obj.Id.Cls=='Mesh1D':\n        func(pathfileext, Idsave=Idsave, Knots=obj.Knots)\n\n    elif obj.Id.Cls=='Mesh2D':\n        SubMinds = [{'Name':kk, 'ind':obj._SubMesh[kk]['ind']} for kk in obj._SubMesh.keys()]\n        func(pathfileext, Idsave=Idsave, Knots=[obj.MeshX1.Knots,obj.MeshX2.Knots], SubMinds=SubMinds, IndBg=obj._get_CentBckg()[1])\n\n    elif obj.Id.Cls=='BF2D':\n        Id = np.array(['BF2D',obj.Id.Name,obj.Id.SaveName,obj.Id.SavePath,obj.Id._dtFormat,obj.Id._Diag,str(obj.Id._shot), [obj.Id.Type], obj.Id.Exp],dtype=str)\n        IdMesh = np.array(['Mesh2D',obj.Mesh.Id.Name,obj.Mesh.Id.SaveName,obj.Mesh.Id.SavePath,obj.Mesh.Id._dtFormat],dtype=str)\n        dtime, dtimeMesh = np.array([obj.Id._dtime],dtype=object), np.array([obj.Mesh.Id._dtime],dtype=object)\n        USR = np.asarray(obj.Id.USRdict)\n        func(pathfileext, Id=Id, IdMesh=IdMesh, dtime=dtime, IdUSR=USR, dtimeMesh=dtimeMesh, KnotsR=obj.Mesh.MeshR.Knots, KnotsZ=obj.Mesh.MeshZ.Knots, Deg=np.array([obj.Deg],dtype=int), Ind=obj.Mesh._get_CentBckg()[1])\n\n    # tofu.matcomp\n    elif obj.Id.Cls=='GMat2D':\n        Id = np.array(['GMat2D',obj.Id.Name,obj.Id.SaveName,obj.Id.SavePath,obj.Id._dtFormat,obj.Id._Diag,str(obj.Id._shot), [obj.Id.Type], obj.Id.Exp],dtype=str)\n        dtime = np.array([obj.Id._dtime],dtype=object)\n        USR = np.asarray(obj.Id.USRdict)\n        IdObj, IdObjUSR = save_np_IdObj(obj.Id)\n        CompParamVal = np.array([obj._Mat_epsrel, obj._Mat_SubP, obj._Mat_SubTheta, obj._indMat_SubP, obj._MatLOS_epsrel, obj._MatLOS_SubP, int(obj._Mat_Fast)])\n        CompParamStr = np.array([obj._Mat_Mode, obj._Mat_SubMode, obj._Mat_SubThetaMode, obj._MatLOS_Mode, obj._MatLOS_SubMode])\n        func(pathfileext, Id=Id, dtime=dtime, IdUSR=USR, Ves=IdObj[2], VesUSR=IdObjUSR[2], LDetect=IdObj[1], BF2=IdObj[0], BF2USR=IdObjUSR[0], LDetectUSR=IdObjUSR[1], CompParamVal=CompParamVal,\n                CompParamStr=CompParamStr, indMat=obj._indMat, Matdata=obj._Mat_csr.data, Matind=obj._Mat_csr.indices, Matindpr=obj._Mat_csr.indptr, Matshape=obj._Mat_csr.shape,\n                MatLOSdata=obj._MatLOS_csr.data, MatLOSind=obj._MatLOS_csr.indices, MatLOSindpr=obj._MatLOS_csr.indptr, MatLOSshape=obj._MatLOS_csr.shape,\n                BF2Par=np.array([obj._BF2_Deg,obj._BF2_NFunc,obj._BF2_NCents]), LD_nD=obj._LD_nDetect)\n\n    # tofu.treat\n    elif obj.Id.Cls=='PreData':\n        Init, Update = _convert_PreData2Ldict(obj)\n        func(pathfileext, Idsave=Idsave, Init=[Init], Update=[Update])\n\n        #Id = np.array(['PreData',obj.Id.Name,obj.Id.SaveName,obj.Id.SavePath,obj.Id._dtFormat,obj.Id._Diag,str(obj.Id._shot), [obj.Id.Type], obj.Id.Exp],dtype=str)\n        #dtime = np.array([obj.Id._dtime],dtype=object)\n        #USR = np.asarray(obj.Id.USRdict)\n        #IdObj, IdObjUSR = save_np_IdObj(obj.Id)\n        #StrPar = np.asarray([obj._Exp, obj._interpkind])\n        #func(pathfileext, Id=Id, dtime=dtime, IdUSR=USR, LDetect=IdObj[0], LDetectUSR=IdObjUSR[0],\n        #        DLPar=obj._DLPar, shot=obj._shot, StrPar=StrPar, Dt=obj._Dt, DtMarg=obj._DtMargin, MovMeanfreq=obj._MovMeanfreq, Resamp=obj._Resamp,\n        #        indOut=obj._indOut, indCorr=obj._indCorr, PhysNoise=obj._PhysNoise, NoiseMod=obj._NoiseModel, interp_lt=obj._interp_lt, interp_lN=obj._interp_lNames)\n\n    # tofu.inv\n    elif obj.Id.Cls=='Sol2D':\n        Id = np.array(['Sol2D',obj.Id.Name,obj.Id.SaveName,obj.Id.SavePath,obj.Id._dtFormat,obj.Id._Diag,str(obj.Id._shot), [obj.Id.Type], obj.Id.Exp],dtype=str)\n        dtime = np.array([obj.Id._dtime],dtype=object)\n        USR = np.asarray(obj.Id.USRdict)\n        IdObj, IdObjUSR = save_np_IdObj(obj.Id)\n        try:\n            timing = obj._timing\n        except Exception:\n            timing = obj._t2\n        func(pathfileext, Id=Id, dtime=dtime, IdUSR=USR, PreData=IdObj[2], PreDataUSR=IdObjUSR[2], GMat2D=IdObj[1], GMatUSR=IdObjUSR[1], BF2D=IdObj[0], BF2DUSR=IdObjUSR[0],\n                InvParam=obj.InvParam, shot=obj.shot, LNames=obj._LNames, Run=obj._run,\n                LOS=obj._LOS, data=obj._data, t=obj._t, Coefs=obj._Coefs, sigma=obj._sigma, Mu=obj._Mu, Chi2N=obj._Chi2N, R = obj._R, Nit=obj._Nit, Spec=obj._Spec, t2=timing, PostTreat=obj._PostTreat)\n    \"\"\"", "response": "Save the object to a numpy file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsaving the list of all the LObj and LObjUSR in a single object.", "response": "def save_np_IdObj(Id):\n    \"\"\" (to do) \"\"\"\n    LObj, LObjUSR = [], []\n    Keys = sorted(Id.LObj.keys())\n    for ii in range(0,len(Keys)):\n        kk = sorted(Id.LObj[Keys[ii]].keys())\n        Larr, LarrUSR = [], []\n        for jj in range(0,len(kk)):\n            if kk[jj]=='USRdict':\n                LarrUSR.append(np.asarray([Id.LObj[Keys[ii]][kk[jj]]],dtype=object))\n            else:\n                Larr.append(np.asarray([Id.LObj[Keys[ii]][kk[jj]]],dtype=str))\n        LObj.append( np.concatenate(tuple(Larr),axis=0) )\n        LObjUSR.append( np.concatenate(tuple(LarrUSR),axis=0) )\n    return LObj, LObjUSR"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nopen a ToFu object saved file This generic open function identifies the required loading routine by detecting how the object was saved from the file name extension. Also, it uses :meth:`~tofu.pathfile.FindSolFile()` to identify the relevant file in case key criteria such as shot, Deg... are provided instead of the file name itself. Finally, once all the relevant data is loaded from the file, a ToFu object is re-created, if necessary by implicitly loading all other objects it may depend on (i.e.: vessel, apertures...) If pathfileext is not provided (None), then the following keyword arguments are fed to :meth:`~tofu.pathfile.FindSolFile()`: shot, t, Dt, Mesh, Deg, Deriv, Sep, Pos Parameters ---------- pathfileext : None / str If provided, the name of the file to load OutPath : None / str If provided, the absolute path where the file is to be found ReplacePath : str If provided, ? (to finish) Ves : None / If provided, the :class:`tofu.geom.Ves` object that shall be used to reconstruct the object (if not provided, the appropriate vessel will be loaded). out : str Flag indicating whether the object should be loaded completely ('full'), in a light dismissing the heaviest attributes ('light') or whether only the Id or a list of Id should be returned ('Id'), valid only for '.npz' Verb : bool Flag indicating whether to pring intermediate comments on the loading procedure Returns ------- obj ToFu object The loaded and re-created ToFu object", "response": "def Open(pathfileext=None,\n         shot=None, t=None, Dt=None, Mesh=None, Deg=None, Deriv=None,\n         Sep=True, Pos=True, OutPath=None, ReplacePath=None, Ves=None,\n         out='full', Verb=False, Print=True):\n    \"\"\" Open a ToFu object saved file\n\n    This generic open function identifies the required loading routine by detecting how the object was saved from the file name extension.\n    Also, it uses :meth:`~tofu.pathfile.FindSolFile()` to identify the relevant file in case key criteria such as shot, Deg... are provided instead of the file name itself.\n    Finally, once all the relevant data is loaded from the file, a ToFu object is re-created, if necessary by implicitly loading all other objects it may depend on (i.e.: vessel, apertures...)\n\n    If pathfileext is not provided (None), then the following keyword arguments are fed to :meth:`~tofu.pathfile.FindSolFile()`: shot, t, Dt, Mesh, Deg, Deriv, Sep, Pos\n\n    Parameters\n    ----------\n    pathfileext :   None / str\n        If provided, the name of the file to load\n    OutPath :       None / str\n        If provided, the absolute path where the file is to be found\n    ReplacePath :   str\n        If provided, ? (to finish)\n    Ves :           None /\n        If provided, the :class:`tofu.geom.Ves` object that shall be used to reconstruct the object (if not provided, the appropriate vessel will be loaded).\n    out :           str\n        Flag indicating whether the object should be loaded completely ('full'), in a light dismissing the heaviest attributes ('light') or whether only the Id or a list of Id should be returned ('Id'), valid only for '.npz'\n    Verb :          bool\n        Flag indicating whether to pring intermediate comments on the loading procedure\n\n    Returns\n    -------\n    obj             ToFu object\n        The loaded and re-created ToFu object\n\n    \"\"\"\n    assert None in [pathfileext,shot] and not (pathfileext is None and shot is None), \"Arg pathfileext or shot must be None, but not both !\"\n    if pathfileext is None:\n        File = FindSolFile(shot=shot, t=t, Dt=Dt, Mesh=Mesh, Deg=Deg,\n                           Deriv=Deriv, Sep=Sep, Pos=Pos, OutPath=OutPath)\n        if File is None:\n            return File\n        pathfileext = os.path.join(OutPath,File)\n    C = any([ss in pathfileext for ss in ['.npz']])\n    assert C, \"Arg pathfileext must contain '.npz' !\"\n\n    if '.npz' in pathfileext:\n        obj = _open_np(pathfileext, Ves=Ves, ReplacePath=ReplacePath,\n                       out=out, Verb=Verb, Print=Print)\n    if Print:\n        print(\"Loaded :  \"+pathfileext)\n    return obj"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _open_np(pathfileext, Ves=None,\n             ReplacePath=None, out='full', Verb=False, Print=True):\n\n    if 'TFG' in pathfileext:\n        import tofu.geom as tfg\n    elif 'TFD' in pathfileext:\n        import tofu.data as tfd\n    #elif 'TFEq' in pathfileext:\n    #    import tofu.Eq as tfEq\n    #elif 'TFM' in pathfileext:\n    #    import tofu.mesh as TFM\n    #elif 'TFMC' in pathfileext:\n    #    import tofu.matcomp as TFMC\n    #elif 'TFT' in pathfileext:\n    #    import tofu.treat as tft\n    #elif 'TFI' in pathfileext:\n    #    import tofu.inv as TFI\n\n    try:\n        Out = np.load(pathfileext,mmap_mode=None)\n    except UnicodeError:\n        Out = np.load(pathfileext,mmap_mode=None, encoding='latin1')\n    Id = ID(fromdict=Out['Id'].tolist())\n    if out=='Id':\n        return Id\n\n    if Id.Cls == 'Ves':\n        Lim = None if Out['Lim'].tolist() is None else Out['Lim']\n        obj = tfg.Ves(Id, Out['Poly'], Lim=Lim, Type=Id.Type,\n                      Clock=bool(Out['Clock']),\n                      arrayorder=str(Out['arrayorder']),\n                      Sino_RefPt=Out['Sino_RefPt'], Sino_NP=int(Out['Sino_NP']))\n\n    elif Id.Cls == 'Struct':\n        Lim = None if Out['Lim'].tolist() is None else Out['Lim']\n        obj = tfg.Struct(Id, Out['Poly'], Type=Id.Type, Lim=Lim,\n                         Clock=bool(Out['Clock']),\n                         arrayorder=str(Out['arrayorder']),\n                         mobile=Out['mobile'].tolist())\n\n    elif Id.Cls in ['Rays','LOS','LOSCam1D','LOSCam2D']:\n        Ves, LStruct = _tryloadVesStruct(Id, Print=Print)\n        dobj = {'Id':Id._todict(), 'dchans':Out['dchans'].tolist(),\n                'geom':Out['geom'].tolist(),\n                'sino':Out['sino'].tolist()}\n        if 'extra' in Out.keys():\n            dobj['extra'] = Out['extra'].tolist()\n        if Ves is None:\n            dobj['Ves'] = None\n        else:\n            dobj['Ves'] = Ves._todict()\n        if LStruct is None:\n            dobj['LStruct'] = None\n        else:\n            dobj['LStruct'] = [ss._todict() for ss in LStruct]\n        if Id.Cls=='Rays':\n            obj = tfg.Rays(fromdict=dobj)\n        elif Id.Cls=='LOSCam1D':\n            obj = tfg.LOSCam1D(fromdict=dobj)\n        elif Id.Cls=='LOSCam2D':\n            obj = tfg.LOSCam2D(fromdict=dobj)\n\n    elif Id.Cls in ['Data1D','Data2D']:\n        dobj = {'Id':Id._todict(), 'Ref':Out['Ref'].tolist(),\n                'dunits':Out['dunits'].tolist(), 'fft':Out['fft'].tolist(),\n                'data0':Out['data0'].tolist(), 'CamCls':Out['CamCls'].tolist()}\n        indt = None if Out['indt'].tolist() is None else Out['indt']\n        indch = None if Out['indch'].tolist() is None else Out['indch']\n        if Out['geom'].tolist() is None:\n            geom = None\n        else:\n            if 'Cam' in Out['geom'][0]:\n                LCam = [Open(ss)._todict() for ss in Out['geom']]\n                geom = {'LCam':LCam}\n            else:\n                Ves = Open(Out['geom'][0])._todict()\n                if len(Out['geom'])>1:\n                    LStruct = [Open(ss)._todict() for ss in Out['geom'][1:]]\n                else:\n                    LStruct = None\n                    geom = {'LCam':None, 'Ves':Ves, 'LStruct':LStruct}\n        dobj['indt'] = indt\n        dobj['indch'] = indch\n        dobj['geom'] = geom\n        if 'dMag' in Out.keys():\n            dMag = Out['dMag'].tolist()\n        else:\n            dMag = None\n        dobj['dMag'] = dMag\n        if Id.Cls=='Data1D':\n            obj = tfd.Data1D(fromdict=dobj)\n        elif Id.Cls=='Data2D':\n            obj = tfd.Data2D(fromdict=dobj)\n\n    \"\"\"\n    elif Id.Cls == 'GLOS':\n        Ves = _tryloadVes(Id)\n        LLOS, IdLOS = [], Id.LObj['LOS']\n        for ii in range(0,len(IdLOS['Name'])):\n            Idl = _Id_recreateFromdict(Out['LIdLOS'][ii])\n            ll = TFG.LOS(Idl, Du=(Out['LDs'][:,ii],Out['Lus'][:,ii]), Ves=Ves, Sino_RefPt=Out['Sino_RefPt'], arrayorder=str(Out['arrayorder']))\n            LLOS.append(ll)\n        obj = TFG.GLOS(Id, LLOS, Ves=Ves, Type=Id.Type, Exp=Id.Exp, Diag=Id.Diag, shot=Id.shot, Sino_RefPt=Out['Sino_RefPt'], SavePath=Id.SavePath, arrayorder=str(Out['arrayorder']), Clock=bool(Out['Clock']),\n                       dtime=Id.dtime)\n\n    elif Id.Cls == 'Lens':\n        Ves = _tryloadVes(Id, Ves=Ves)\n        obj = TFG.Lens(Id, Out['O'], Out['nIn'], Out['Rad'][0], Out['F1'][0], F2=Out['F2'][0], Type=Id.Type, R1=Out['R1'][0], R2=Out['R2'][0], dd=Out['dd'][0], Ves=Ves,\n                Exp=Id.Exp, Clock=bool(Out['Clock']), Diag=Id.Diag, shot=Id.shot, arrayorder=str(Out['arrayorder']), SavePath=Id.SavePath, dtime=Id.dtime)\n\n    elif Id.Cls == 'Apert':\n        Ves = _tryloadVes(Id, Ves=Ves)\n        obj = TFG.Apert(Id, Out['Poly'], Clock=bool(Out['Clock']), arrayorder=str(Out['arrayorder']), Ves=Ves, Exp=Id.Exp, Diag=Id.Diag, shot=Id.shot, dtime=Id.dtime)\n\n    elif Id.Cls == 'Detect':\n        Ves = _tryloadVes(Id, Ves=Ves)\n        if 'VesCalc'in Out.keys() and Out['VesCalc'][0]['SavePath'] is not None:\n            VesCalc = Open(Out['VesCalc'][0]['SavePath']+Out['VesCalc'][0]['SaveName']+'.npz')\n        else:\n            VesCalc = None\n        LOSprops, Sino, Span, Cone, SAng, Opt = Out['LOSprops'][0], Out['Sino'][0], Out['Span'][0], Out['Cone'][0], Out['SAng'][0], Out['Optics'][0]\n        (SynthDiag,Res) = (Out['SynthDiag'][0],Out['Res'][0]) if out=='full' else _get_light_SynthDiag_Res()\n        Optics = _tryLoadOpticsElseCreate(Id, Opt=Opt, Ves=Ves, Verb=Verb)\n\n        Poly = Out['Poly'] if type(Optics) is list else dict(Rad=float(Out['Rad']),O=Out['BaryS'],nIn=Out['nIn'])\n        obj = TFG.Detect(Id, Poly, Optics=Optics, Ves=Ves, VesCalc=VesCalc, Sino_RefPt=Sino['_Sino_RefPt'], CalcEtend=False, CalcSpanImp=False, CalcCone=False, CalcPreComp=False, Calc=True, Verb=Verb,\n                         arrayorder=str(Out['arrayorder']), Clock=bool(Out['Clock']))\n        obj = _resetDetectAttr(obj, {'LOSprops':LOSprops, 'Sino':Sino, 'Span':Span, 'Cone':Cone, 'SAng':SAng, 'SynthDiag':SynthDiag, 'Res':Res, 'Optics':Opt})\n        obj._LOS_NP = Out['LOSNP']\n        if obj._SynthDiag_Done and obj._SynthDiag_Points is None:\n            obj.set_SigPrecomp()\n\n    elif Id.Cls == 'GDetect':\n        LDetsave = list(Out['LDetsave'])\n        LDet = []\n        Ves = _tryloadVes(Id, Ves=Ves)\n        if out=='light':\n            SynthDiag, Res = _get_light_SynthDiag_Res()\n        else:\n            LDetSynthRes = Out['LDetSynthRes']\n        for ii in range(0,len(LDetsave)):\n            ddIdsave = _Id_recreateFromdict(LDetsave[ii]['Idsave'])\n            if 'VesCalc'in LDetsave[ii].keys() and LDetsave[ii]['VesCalc'][0]['SavePath'] is not None:\n                VesCalc = Open(LDetsave[ii]['VesCalc'][0]['SavePath']+LDetsave[ii]['VesCalc'][0]['SaveName']+'.npz')\n            else:\n                VesCalc = None\n            LOSprops, Sino, Span, Cone, SAng, Opt = LDetsave[ii]['LOSprops'][0], LDetsave[ii]['Sino'][0], LDetsave[ii]['Span'][0], LDetsave[ii]['Cone'][0], LDetsave[ii]['SAng'][0], LDetsave[ii]['Optics'][0]\n            if out=='full':\n                SynthDiag, Res = LDetSynthRes[ii]['SynthDiag'][0], LDetSynthRes[ii]['Res'][0]\n            Optics = _tryLoadOpticsElseCreate(ddIdsave, Opt=Opt, Ves=Ves, Verb=Verb)\n            Poly = LDetsave[ii]['Poly'] if type(Optics) is list else dict(Rad=float(LDetsave[ii]['Rad']),O=LDetsave[ii]['BaryS'],nIn=LDetsave[ii]['nIn'])\n            Sino_RefPt = None if Out['Sino_RefPt'].shape==() else Out['Sino_RefPt']\n            dd = TFG.Detect(ddIdsave, Poly, Optics=Optics, Ves=Ves, VesCalc=VesCalc, Sino_RefPt=Sino_RefPt, CalcEtend=False, CalcSpanImp=False, CalcCone=False, CalcPreComp=False, Calc=True, Verb=Verb,\n                            arrayorder=str(Out['arrayorder']), Clock=bool(Out['Clock']))\n            dd = _resetDetectAttr(dd, {'LOSprops':LOSprops, 'Sino':Sino, 'Span':Span, 'Cone':Cone, 'SAng':SAng, 'SynthDiag':SynthDiag, 'Res':Res, 'Optics':Opt})\n            dd._LOS_NP = LDetsave[ii]['LOSNP']\n            if dd._SynthDiag_Done and dd._SynthDiag_Points is None:\n                dd.set_SigPrecomp()\n            LDet.append(dd)\n        obj = TFG.GDetect(Id, LDet, Type=Id.Type, Exp=Id.Exp, Diag=Id.Diag, shot=Id.shot, dtime=Id.dtime, Sino_RefPt=Out['Sino_RefPt'], LOSRef=str(Out['LOSRef']),\n                          arrayorder=str(Out['arrayorder']), Clock=bool(Out['Clock']), SavePath=Id.SavePath)\n        Res = Out['Res'][0] if out=='full' else Res\n        for kk in Res.keys():\n            setattr(obj,kk,Res[kk])\n\n    elif Id.Cls=='Eq2D':\n        Sep = [np.array(ss) for ss in Out['Sep'].tolist()]\n        obj = tfEq.Eq2D(Id, Out['PtsCross'], t=Out['t'], MagAx=Out['MagAx'], Sep=Sep, rho_p=Out['rho_p'].tolist(), rho_t=Out['rho_t'].tolist(), surf=Out['surf'].tolist(), vol=Out['vol'].tolist(),\n                        q=Out['q'].tolist(), jp=Out['jp'].tolist(), pf=Out['pf'].tolist(), tf=Out['tf'].tolist(), theta=Out['theta'].tolist(), thetastar=Out['thetastar'].tolist(),\n                        BTX=Out['BTX'].tolist(), BRY=Out['BRY'].tolist(), BZ=Out['BZ'].tolist(), Ref=str(Out['Ref']))\n\n    elif Id.Cls=='Mesh1D':\n        obj = TFM.Mesh1D(Id, Out['Knots'])\n\n    elif Id.Cls=='Mesh2D':\n        obj = TFM.Mesh2D(Id, [Out['Knots'][0],Out['Knots'][1]])\n        obj = TFM.Mesh2D(Id, Knots=obj, ind=Out['IndBg'])\n        for ii in range(0,len(Out['SubMinds'])):\n            obj.add_SubMesh(Name=Out['SubMinds'][ii]['Name'], ind=Out['SubMinds'][ii]['ind'])\n\n    elif Id.Cls=='Metric1D':\n        obj = TFM.Metric1D(Id)\n\n    elif Id.Cls=='Metric2D':\n        obj = TFM.Metric2D(Id)\n\n\n    elif Id.Cls in 'BF2D':\n        IdMesh = ID(str(Out['IdMesh'][0]), str(Out['IdMesh'][1]), SaveName=str(Out['IdMesh'][2]), SavePath=str(Out['IdMesh'][3]), dtime=Out['dtimeMesh'][0], dtFormat=str(Out['IdMesh'][4]))\n        M2 = TFM.Mesh2D(IdMesh, Knots=[Out['KnotsR'],Out['KnotsZ']])\n        M2bis = TFM.Mesh2D(IdMesh,Knots=M2,Ind=Out['Ind'])\n        obj = TFM.BF2D(Id, M2bis, int(Out['Deg'][0]))\n    elif Id.Cls=='GMat2D':\n        import ToFu_MatComp as TFMC\n        import scipy.sparse as scpsp\n        Id.set_LObj(open_np_IdObj(['Ves','BF2D','Detect'], [Out['Ves'],Out['BF2'],Out['LDetect']], [Out['VesUSR'],Out['BF2USR'],Out['LDetectUSR']]))\n        Mat = scpsp.csr_matrix((Out['Matdata'], Out['Matind'], Out['Matindpr']), shape=Out['Matshape'])\n        MatLOS = scpsp.csr_matrix((Out['MatLOSdata'], Out['MatLOSind'], Out['MatLOSindpr']), shape=Out['MatLOSshape'])\n        obj = TFMC.GMat2D(Id, None, None, Mat=None, indMat=None, MatLOS=None, Calcind=False, Calc=False, CalcLOS=False)\n        obj._init_CompParam(Mode=str(Out['CompParamStr'][0]), epsrel=Out['CompParamVal'][0], SubP=Out['CompParamVal'][1], SubMode=str(Out['CompParamStr'][1]), SubTheta=Out['CompParamVal'][2], SubThetaMode=str(Out['CompParamStr'][2]), Fast=bool(Out['CompParamVal'][-1]), SubPind=Out['CompParamVal'][3], ModeLOS=str(Out['CompParamStr'][3]), epsrelLOS=Out['CompParamVal'][4], SubPLOS=Out['CompParamVal'][5], SubModeLOS=str(Out['CompParamStr'][4]))\n        obj._BF2 = None\n        obj._BF2_Deg = int(Out['BF2Par'][0])\n        obj._BF2_NCents = int(Out['BF2Par'][2])\n        obj._BF2_NFunc = int(Out['BF2Par'][1])\n        obj._Ves = None\n        obj._LD = None\n        obj._LD_nDetect = int(Out['LD_nD'])\n        obj._set_indMat(indMat=Out['indMat'], Verb=False)\n        obj._set_MatLOS(MatLOS=MatLOS, Verb=False)\n        obj._set_Mat(Mat=Mat, Verb=False)\n\n\n\n    elif Id.Cls=='PreData':\n        LIdDet = Id.get_LObjasLId('Detect') if 'Detect' in Id.LObj.keys() else None\n        Init, Update = Out['Init'][0], Out['Update'][0]\n        obj = tft.PreData(Init['data'], Id=Id, t=Init['t'], Chans=Init['Chans'], DtRef=Init['DtRef'], LIdDet=LIdDet)\n        obj.set_Dt(Update['Dt'], Calc=False)\n        obj.set_Resamp(t=Update['Resamp_t'], f=Update['Resamp_f'], Method=Update['Resamp_Method'], interpkind=Update['Resamp_interpkind'], Calc=False)\n        obj.Out_add(indOut=Update['indOut'], Calc=False)\n        obj.Corr_add(indCorr=Update['indCorr'], Calc=False)\n        obj.interp(lt=Update['interp_lt'], lNames=Update['interp_lNames'], Calc=False)\n        obj.substract_Dt(tsub=Update['Subtract_tsub'], Calc=False)\n        obj.set_fft(Calc=True, **Update['FFTPar'])\n        if not Update['PhysNoiseParam'] is None:\n            Method = 'svd' if 'Modes' in Update['PhysNoiseParam'].keys() else 'fft'\n            obj.set_PhysNoise(**Update['PhysNoiseParam'].update({'Method':Method}))\n\n\n        #Id.set_LObj(open_np_IdObj(['Detect'],[Out['LDetect']], [Out['LDetectUSR']]))\n        #obj = TFT.PreData(Id=Id, shot=int(Out['shot']), DLPar=Out['DLPar'].item(), Exp=str(Out['StrPar'][0]), Dt=list(Out['Dt']), DtMargin=float(Out['DtMarg']), MovMeanfreq=float(Out['MovMeanfreq']), Resamp=bool(Out['Resamp']),\n        #        interpkind=str(Out['StrPar'][1]), indOut=Out['indOut'], indCorr=Out['indCorr'], lt=Out['interp_lt'], lNames=Out['interp_lN'].tolist(), Test=True)\n        #if not Out['PhysNoise'].item() is None:\n        #    obj.set_PhysNoise(Deg=int(Out['NoiseMod'].item()['Deg']), Nbin=int(Out['NoiseMod'].item()['Nbin']), LimRatio=float(Out['NoiseMod'].item()['LimRatio']), **Out['PhysNoise'].item()['Param'])\n\n\n    elif Id.Cls=='Sol2D':\n        Id.set_LObj(open_np_IdObj(['PreData','GMat2D','BF2D'],[Out['PreData'], Out['GMat2D'], Out['BF2D']], [Out['PreDataUSR'],Out['GMatUSR'],Out['BF2DUSR']]))\n        GMSaveName = Id.LObj['GMat2D']['SaveName'][0]\n        try:\n            GMat = Open(Id.LObj['GMat2D']['SavePath'][0]+GMSaveName+'.npz')\n        except Exception:\n            GMSaveName = GMSaveName[:GMSaveName.index('All_')+4]+'sh'+GMSaveName[GMSaveName.index('All_')+4:]\n            GMat = Open(Id.LObj['GMat2D']['SavePath'][0]+GMSaveName+'.npz')\n        obj = TFI.Sol2D(Id, PreData=None, GMat=GMat, InvParam=Out['InvParam'].item(), SVesePreData=False, SVeseGMat=True, SVeseBF=True)\n        obj._PreData = None\n        obj._GMat = obj.GMat.get_SubGMat2D(Val=list(Out['LNames']), Crit='Name',InOut='In')\n        obj._shot = int(Out['shot'])\n        try:\n            obj._LNames = Out['LNames'].tolist()\n        except Exception:\n            obj._LNames = obj.PreData.In_list()\n        obj._run = bool(Out['Run'])\n        if bool(Out['Run']):\n            obj._LOS = bool(Out['LOS'])\n            obj._t, obj._data = Out['t'], Out['data']\n            obj._Coefs, obj._sigma = Out['Coefs'], Out['sigma']\n            obj._Mu, obj._Chi2N, obj._R, obj._Nit = Out['Mu'], Out['Chi2N'], Out['R'], Out['Nit']\n            obj._Spec = list(Out['Spec'])\n            obj._timing = Out['t2']\n            obj._PostTreat = list(Out['PostTreat'])\n    \"\"\"\n    return obj", "response": "Open a TFG file and return the ID of the next TFG object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the name of the instance automatically updating the SaveName", "response": "def set_Name(self, Name, SaveName=None,\n                 Include=defInclude,\n                 ForceUpdate=False):\n        \"\"\" Set the Name of the instance, automatically updating the SaveName\n\n        The name should be a str without spaces or underscores (removed)\n        When the name is changed, if SaveName (i.e. the name used for saving)\n        was not user-defined, it is automatically updated\n\n        Parameters\n        ----------\n        Name :      str\n            Name of the instance, without ' ' or '_' (automatically removed)\n        SaveName :  None / str\n            If provided, overrides the default name for saving (not recommended)\n        Include:    list\n            Controls how te default SaveName is generated\n            Each element of the list is a key str indicating whether an element\n            should be present in the SaveName\n\n        \"\"\"\n        self._check_inputs(Name=Name, SaveName=SaveName, Include=Include)\n        self._Name = Name\n        self.set_SaveName(SaveName=SaveName, Include=Include,\n                          ForceUpdate=ForceUpdate)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_SaveName(self,SaveName=None,\n                     Include=defInclude,\n                     ForceUpdate=False):\n        \"\"\" Set the name for saving the instance (SaveName)\n\n        SaveName can be either:\n            - provided by the user (no constraint) - not recommended\n            - automatically generated from Name and key attributes (cf. Include)\n\n        Parameters\n        ----------\n        SaveName :      None / str\n            If provided, overrides the default name for saving (not recommended)\n        Include :       list\n            Controls how te default SaveName is generated\n            Each element of the list is a key str indicating whether an element\n            should be present in the SaveName\n        ForceUpdate :   bool\n            Flag indicating the behaviour when SaveName=None:\n                - True : A new SaveName is generated, overriding the old one\n                - False : The former SaveName is preserved (default)\n        \"\"\"\n        self._check_inputs(SaveName=SaveName, Include=Include)\n        if not hasattr(self,'_SaveName_usr'):\n            self._SaveName_usr = (SaveName is not None)\n        # If SaveName provided by user, override\n        if SaveName is not None:\n            self._SaveName = SaveName\n            self._SaveName_usr = True\n        else:\n            # Don't update if former is user-defined and ForceUpdate is False\n            # Override if previous was:\n            # automatic or (user-defined but ForceUpdate is True)\n            if (not self._SaveName_usr) or (self._SaveName_usr and ForceUpdate):\n                SN = SaveName_Conv(Mod=self._Mod, Cls=self.Cls, Type=self.Type,\n                                   Name=self.Name, Deg=self._Deg, Exp=self.Exp,\n                                   Diag=self.Diag, shot=self.shot,\n                                   version=self._version, usr=self._usr,\n                                   Include=Include)\n                self._SaveName = SN\n                self._SaveName_usr = False", "response": "Set the name for saving the instance of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the attribute _LObj which stores the objects that depend on the object.", "response": "def set_LObj(self,LObj=None):\n        \"\"\" Set the LObj attribute, storing objects the instance depends on\n\n        For example:\n        A Detect object depends on a vessel and some apertures\n        That link between should be stored somewhere (for saving/loading).\n        LObj does this: it stores the ID (as dict) of all objects depended on.\n\n        Parameters\n        ----------\n        LObj :  None / dict / :class:`~tofu.pathfile.ID` / list of such\n            Provide either:\n                - A dict (derived from :meth:`~tofu.pathfile.ID._todict`)\n                - A :class:`~tofu.pathfile.ID` instance\n                - A list of dict or :class:`~tofu.pathfile.ID` instances\n\n        \"\"\"\n        self._LObj = {}\n        if LObj is not None:\n            if type(LObj) is not list:\n                LObj = [LObj]\n            for ii in range(0,len(LObj)):\n                if type(LObj[ii]) is ID:\n                    LObj[ii] = LObj[ii]._todict()\n            ClsU = list(set([oo['Cls'] for oo in LObj]))\n            for c in ClsU:\n                self._LObj[c] = [oo for oo in LObj if oo['Cls']==c]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the USRdict containing user - defined info about the instance holding the current instance holding the current instance holding the current instance holding the current instance holding the current instance holding the current instance holding the USRdict.", "response": "def set_USRdict(self,USRdict={}):\n        \"\"\" Set the USRdict, containing user-defined info about the instance\n\n        Useful for arbitrary info (e.g.: manufacturing date, material...)\n\n        Parameters\n        ----------\n        USRdict :   dict\n            A user-defined dictionary containing info about the instance\n\n        \"\"\"\n        self._check_inputs(USRdict=USRdict)\n        self._USRdict = USRdict"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_config(case=None, Exp='Dummy', Type='Tor',\n                  Lim=None, Bump_posextent=[np.pi/4., np.pi/4],\n                  R=2.4, r=1., elong=0., Dshape=0.,\n                  divlow=True, divup=True, nP=200,\n                  out='object', SavePath='./'):\n    \"\"\" Create easily a tofu.geom.Config object\n\n    In tofu, a Config (short for geometrical configuration) refers to the 3D\n    geometry of a fusion device.\n    It includes, at least, a simple 2D polygon describing the first wall of the\n    fusion chamber, and can also include other structural elements (tiles,\n    limiters...) that can be non-axisymmetric.\n\n    To create a simple Config, provide either the name of a reference test\n    case, of a set of geometrical parameters (major radius, elongation...).\n\n    This is just a tool for fast testing, if you want to create a custom\n    config, use directly tofu.geom.Config and provide the parameters you want.\n\n    Parameters\n    ----------\n    case :      str\n        The name of a reference test case, if provided, this arguments is\n        sufficient, the others are ignored\n    Exp  :      str\n        The name of the experiment\n    Type :      str\n        The type of configuration (toroidal 'Tor' or linear 'Lin')\n    Lim_Bump:   list\n        The angular (poloidal) limits, in the cross-section of the extension of\n        the outer bumper\n    R   :       float\n        The major radius of the center of the cross-section\n    r   :       float\n        The minor radius of the cross-section\n    elong:      float\n        An elongation parameter (in [-1;1])\n    Dshape:     float\n        A parameter specifying the D-shape of the cross-section (in [-1;1])\n    divlow:     bool\n        A flag specifying whether to include a lower divertor-like shape\n    divup:     bool\n        A flag specifying whether to include an upper divertor-like shape\n    nP:         int\n        Number of points used to describe the cross-section polygon\n    out:        str\n        FLag indicating whether to return:\n            - 'dict'  : the polygons as a dictionary of np.ndarrays\n            - 'object': the configuration as a tofu.geom.Config instance\n\n    Return\n    ------\n    conf:   tofu.geom.Config / dict\n        Depending on the value of parameter out, either:\n            - the tofu.geom.Config object created\n            - a dictionary of the polygons and their pos/extent (if any)\n    \"\"\"\n\n    if case is not None:\n        conf = _create_config_testcase(config=case, out=out)\n    else:\n        poly, pbump, pbaffle = _compute_VesPoly(R=R, r=r, elong=elong, Dshape=Dshape,\n                                                divlow=divlow, divup=divup, nP=nP)\n\n        if out=='dict':\n            conf = {'Ves':{'Poly':poly},\n                    'Baffle':{'Poly':pbaffle},\n                    'Bumper':{'Poly':pbump,\n                              'pos':Bump_posextent[0],\n                              'extent':Bump_posextent[1]}}\n        else:\n            ves = _core.Ves(Poly=poly, Type=Type, Lim=Lim, Exp=Exp, Name='Ves',\n                            SavePath=SavePath)\n            baf = _core.PFC(Poly=pbaffle, Type=Type, Lim=Lim,\n                            Exp=Exp, Name='Baffle', color='b', SavePath=SavePath)\n            bump = _core.PFC(Poly=pbump, Type=Type,\n                             pos=Bump_posextent[0], extent=Bump_posextent[1],\n                             Exp=Exp, Name='Bumper', color='g', SavePath=SavePath)\n\n            conf = _core.Config(Name='Dummy', Exp=Exp, lStruct=[ves,baf,bump],\n                                SavePath=SavePath)\n    return conf", "response": "Create a simple Config object for a 3D object."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nanalyzes a log file.", "response": "def analyze_log(fp, configs, url_rules):\n    \"\"\"Analyze log file\"\"\"\n    url_classifier = URLClassifier(url_rules)\n    analyzer = LogAnalyzer(url_classifier=url_classifier, min_msecs=configs.min_msecs)\n    for line in fp:\n        analyzer.analyze_line(line)\n    return analyzer.get_data()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_subcommand(subparsers):\n    parser_analyze = subparsers.add_parser('analyze', help='Analyze uwsgi log to get report')\n    parser_analyze.add_argument('-f', '--filepath', type=argparse.FileType('r'), dest='filepath',\n                                help='Path of uwsgi log file', required=True)\n    parser_analyze.add_argument('--output', dest=\"output\", type=argparse.FileType('w'), default=sys.stdout, \n                                help='HTML report file path')\n    parser_analyze.add_argument('--min-msecs', dest=\"min_msecs\", type=int, default=200,\n                                help='Request serve time lower than this value will not be counted, default: 200')\n    parser_analyze.add_argument('--domain', dest=\"domain\", type=str, required=False,\n                                help='Make url in report become a hyper-link by settings a domain')\n    parser_analyze.add_argument('--url-file', dest=\"url_file\", type=argparse.FileType('r'), required=False, \n                                help='Customized url rules in regular expression')\n    parser_analyze.add_argument('--limit-url-groups', dest=\"limit_url_groups\", type=int, required=False, \n                                default=LIMIT_URL_GROUPS, help='Number of url groups considered, default: 200')\n    parser_analyze.add_argument('--limit-per-url-group', dest=\"limit_per_url_group\", type=int,\n                                required=False, default=LIMIT_PER_URL_GROUP,\n                                help='Number of urls per group considered, default: 20')\n    parser_analyze.set_defaults(func=analyze)", "response": "Load this subcommand into subparsers"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a spectrogram of the data for each channel in the file system.", "response": "def _spectrogram_scipy_fourier(data, fs, nt, nch, fmin=None,\n                               window=('tukey', 0.25), deg=False,\n                               nperseg=None, noverlap=None,\n                               detrend='linear', stft=False,\n                               boundary='constant', padded=True, warn=True):\n    \"\"\" Return a spectrogram for each channel, and a common frequency vector\n\n    The min frequency of interest fmin fixes the nb. of pt. per seg. (if None)\n    The number of overlapping points is set to nperseg-1 if None\n    The choice of the window type is a trade-off between:\n        Spectral resolution between similar frequencies/amplitudes:\n            =>\n        Dynamic range (lots of != frequencies of != amplitudes):\n            =>\n        Compromise:\n            => 'hann'\n    \"\"\"\n\n    # Check inputs\n    if nperseg is None and fmin is None:\n        fmin = _fmin_coef*(fs/nt)\n        if warn:\n            msg = \"nperseg and fmin were not provided\\n\"\n            msg += \"    => fmin automatically set to 10.*fs/nt:\\n\"\n            msg += \"       fmin = 10.*{0} / {1} = {2} Hz\".format(fs,nt,fmin)\n            warnings.warn(msg)\n\n    # Format inputs\n    if nperseg is None:\n        assert fmin > fs/nt\n        nperseg = int(np.ceil(fs/fmin))\n\n    if nperseg%2==1:\n        nperseg = nperseg + 1\n    if noverlap is None:\n        noverlap = nperseg - 1\n    n = int(np.ceil(np.log(nperseg)/np.log(2)))\n    nfft = 2**n\n\n    # Prepare output\n    if stft:\n        f, tf, ssx = scpsig.stft(data, fs=fs,\n                                 window=window, nperseg=nperseg,\n                                 noverlap=noverlap, nfft=nfft, detrend=detrend,\n                                 return_onesided=True, boundary=boundary,\n                                 padded=padded, axis=0)\n    else:\n        f, tf, ssx = scpsig.spectrogram(data, fs=fs,\n                                        window=window, nperseg=nperseg,\n                                        noverlap=noverlap, nfft=nfft,\n                                        detrend=detrend, return_onesided=True,\n                                        scaling='density', axis=0,\n                                        mode='complex')\n\n    # Split in list (per channel)\n    lssx = np.split(ssx, np.arange(1,nch), axis=1)\n    lssx = [ss.squeeze().T for ss in lssx]\n    lpsd = [np.abs(ss)**2 for ss in lssx]\n    lang = [np.angle(ss, deg=deg) for ss in lssx]\n\n    return f, tf, lpsd, lang"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfiltering the data array with the bandpass FFT - filtered signal", "response": "def filter_bandpass_fourier(t, data, method='stft', detrend='linear',\n                            df=None, harm=True,\n                            df_out=None, harm_out=True):\n    \"\"\" Return bandpass FFT-filtered signal (and the rest)\n\n    Optionnally include all higher harmonics\n    Can also exclude a frequency interval and its higher harmonics\n\n    Parameters\n    ----------\n    t :         np.ndarray\n        1D array, monotonously increasing time vector with regular spacing\n    data :      np.ndarray\n        1 or 2D array, with shape[0]=t.size, the data to be filtered\n    method:     str\n        Flag indicating which method to use:\n            - 'rfft':   scipy.fftpack.rfft\n            - 'stft':   scipy.signal.stft\n    df :        None / list\n        List or tuple of len()=2, containing the bandpass lower / upper bounds\n    harm :      bool\n        If True all the higher harmonics of df will also be included\n    df_out :    None / list\n        List or tuple of len()=2, containing the bandpass lower / upper bounds\n        to be excluded from filtering (if it overlaps with high harmonics of df)\n    harm_out :  bool\n        If True, the higher harmonics of the interval df_out are also excluded\n    Test :      bool\n        If True tests all the input arguments for any mistake\n\n    Returns\n    -------\n    In :        np.ndarray\n        Array with shape=data.shape, filtered signal retrieved from inverse FFT\n    Out :       np.ndarray\n        Array with shape=data.shape, excluded signal from filtering\n\n    \"\"\"\n    # Check / format inputs\n    assert data.ndim==2 and t.ndim==1\n    assert data.shape[0]==(t.size,)\n    assert np.allclose(t,np.unique(t)) and np.std(np.diff(t))<=1.e-12\n    assert method in ['rfft','stft']\n    lC = [df is None, df_out is None]\n    assert np.sum(lC)<=1, \"At least one of df or df_out must be provided !\"\n    assert type(harm) is bool and type(harm_out) is bool\n    if df is not None:\n        df = np.unique(df)\n        assert df.shape==(2,)\n    if df_out is not None:\n        df_out = np.unique(df_out)\n        assert df_out.shape==(2,)\n\n    nt, nch = data.shape\n    dt = np.mean(np.diff(t))\n    fs = 1./dt\n\n    if method=='rfft':\n        data_in, data_out = _filter_bandpass_rfft(data, t, dt, fs, nt, nch,\n                                                  df=df, harm=harm,\n                                                  df_out=df_out,\n                                                  harm_out=harm_out)\n    elif methd=='stft':\n        data_in, data_out = _filter_bandpass_stft(data, t, dt, fs, nt, nch,\n                                                  df=df, harm=harm,\n                                                  df_out=df_out,\n                                                  harm_out=harm_out)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef filter_svd(data, lapack_driver='gesdd', modes=[]):\n    # Check input\n    modes = np.asarray(modes,dtype=int)\n    assert modes.ndim==1\n    assert modes.size>=1, \"No modes selected !\"\n\n    u, s, v = scplin.svd(data, full_matrices=False, compute_uv=True,\n                         overwrite_a=False, check_finite=True,\n                         lapack_driver=lapack_driver)\n    indout = np.arange(0,s.size)\n    indout = np.delete(indout, modes)\n\n    data_in = np.dot(u[:,modes]*s[modes],v[modes,:])\n    data_out = np.dot(u[:,indout]*s[indout],v[indout,:])\n    return data_in, data_out", "response": "Filter the svd - filtered signal using only the selected modes"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef render_requests_data_to_html(self, data, file_name, context={}):\n        file_path = os.path.join(self.html_dir, file_name)\n        logger.info('Rendering HTML file %s...' % file_path)\n        data = format_data(data)\n        data.update(context)\n        data.update(domain=self.domain)\n        with open(file_path, 'w') as fp:\n            fp.write(render_template('realtime.html', data))", "response": "Render the data to HTML file"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _set_arrayorder(obj, arrayorder='C'):\n    msg = \"Arg arrayorder must be in ['C','F']\"\n    assert arrayorder in ['C','F'], msg\n\n    d = obj.to_dict(strip=-1)\n    account = {'Success':[], 'Failed':[]}\n    for k, v in d.items():\n        if type(v) is np.array and v.ndim>1:\n            try:\n                if arrayorder=='C':\n                    d[k] = np.ascontiguousarray(v)\n                else:\n                    d[k] = np.asfortranarray(v)\n                account['Success'].append(k)\n            except Exception as err:\n                warnings.warn(str(err))\n                account['Failed'].append(k)\n\n    return d, account", "response": "Set the memory order of all np. ndarrays in a tofu object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef save(obj, path=None, name=None, sep=_sep, deep=False, mode='npz',\n         strip=None, compressed=False, verb=True, return_pfe=False):\n    \"\"\" Save the ToFu object\n\n    ToFu provides built-in saving and loading functions for ToFu objects.\n    Specifying saving path ad name is optional (defaults recommended)\n    The mode refers to the file format\n\n    Good practices are:\n        - save all struct objects\n\n    Parameters\n    ----------\n    obj  :      ToFuObject subclass instance\n        The object to be saved\n    path :      None / str\n        The folder where to save, if None (recommended), uses obj.Id.SavePath\n    name :      None / str\n        The file name, if None (recommended), uses obj.Id.SaveName\n    mode :      str\n        Flag specifying the saving mode\n            - 'npz': numpy file\n            - 'mat': matlab file\n    strip:      int\n        Flag indicating how stripped the saved object should be\n        See docstring of self.strip()\n    deep:       bool\n        Flag, used when the object has other tofu objects as attributes\n        Indicated whether these attribute object should be:\n            - True: converted to dict themselves in order to be saved inside\n                the same file as attributes\n                (-> uses self.to_dict(deep='dict'))\n            - False: not converted, in that the strategy would be to save them\n                separately and store only the reference to the saved files\n                instead of the objects themselves.\n                To do this, you must:\n                    1/ Save all object attributes independently\n                    2/ Store only the reference by doing self.strip(-1)\n                       The strip() method will check they have been saved\n                       before removing them, and throw an Exception otherwise\n                    3/ self.save(deep=False)\n    compressed :    bool\n        Flag indicating whether to compress the file (slower, not recommended)\n    verb :          bool\n        Flag indicating whether to print a summary (recommended)\n\n    \"\"\"\n    msg = \"Arg obj must be a tofu subclass instance !\"\n    assert issubclass(obj.__class__, ToFuObject), msg\n    msg = \"Arg path must be None or a str (folder) !\"\n    assert path is None or isinstance(path,str), msg\n    msg = \"Arg name must be None or a str (file name) !\"\n    assert name is None or isinstance(name,str), msg\n    msg = \"Arg mode must be in ['npz','mat'] !\"\n    assert mode in ['npz','mat'], msg\n    msg = \"Arg compressed must be a bool !\"\n    assert type(compressed) is bool, msg\n    msg = \"Arg verb must be a bool !\"\n    assert type(verb) is bool, msg\n\n    # Check path, name, mode\n    path, name, mode = get_pathfileext(path=path, name=name,\n                                       path_def=obj.Id.SavePath,\n                                       name_def=obj.Id.SaveName, mode=mode)\n\n    # Update self._Id fields\n    obj._Id._SavePath = path\n    if name!=obj.Id.SaveName:\n        obj._Id.set_SaveName(name)\n\n    # Get stripped dictionnary\n    deep = 'dict' if deep else 'ref'\n    dd = obj.to_dict(strip=strip, sep=sep, deep=deep)\n\n    pathfileext = os.path.join(path,name+'.'+mode)\n\n    if mode=='npz':\n        _save_npz(dd, pathfileext, compressed=compressed)\n    elif mode=='mat':\n        _save_mat(dd, pathfileext, compressed=compressed)\n\n    # print\n    if verb:\n        msg = \"Saved in :\\n\"\n        msg += \"    \"+pathfileext\n        print(msg)\n    if return_pfe:\n        return pathfileext", "response": "Save the object to a file in the specified folder."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load(name, path=None, strip=None, verb=True):\n\n    lmodes = ['.npz','.mat','.txt']\n    name, mode, pfe = _filefind(name=name, path=path, lmodes=lmodes)\n\n    if mode == 'txt':\n        obj = _load_from_txt(name, pfe)\n    else:\n        if mode == 'npz':\n            dd = _load_npz(pfe)\n        elif mode == 'mat':\n            dd = _load_mat(pfe)\n\n        # Recreate from dict\n        exec(\"import tofu.{0} as mod\".format(dd['dId_dall_Mod']))\n        obj = eval(\"mod.{0}(fromdict=dd)\".format(dd['dId_dall_Cls']))\n\n    if strip is not None:\n        obj.strip(strip=strip)\n\n    # print\n    if verb:\n        msg = \"Loaded from:\\n\"\n        msg += \"    \"+pfe\n        print(msg)\n    return obj", "response": "Loads a tofu object from a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef strip(self, strip=0, **kwdargs):\n        msg = \"Only allowed strip values are:\\n\"\n        msg += \"    \"+ \", \".join([\"{0}\".format(ii)\n                                  for ii in self._dstrip['allowed']])\n        assert strip in [-1]+self._dstrip['allowed'], msg\n        strip = self._dstrip['allowed'][strip]\n\n        # --------------------------------\n        # Call class-specific strip method\n        self._strip(strip, **kwdargs)\n        # --------------------------------\n\n        self._dstrip['strip'] = strip", "response": "Remove non - essential attributes from the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_dict(self, strip=None, sep=_sep, deep='ref'):\n        if deep not in ['ref','copy','dict']:\n            msg = \"Arg deep must be a flag in ['ref','copy','dict'] !\"\n            raise Exception(msg)\n        if strip is None:\n            strip = self._dstrip['strip']\n        if self._dstrip['strip'] != strip:\n            self.strip(strip)\n\n        # ---------------------\n        # Call class-specific\n        dd = self._to_dict()\n        # ---------------------\n        dd['dId'] = self._get_dId()\n        dd['dstrip'] = {'dict':self._dstrip, 'lexcept':None}\n\n        dout = {}\n        for k, v in dd.items():\n            lexcept_key = v.get('lexcept_key', None)\n            try:\n                d = flatten_dict(v['dict'],\n                                 parent_key='', sep=sep, deep=deep,\n                                 lexcept_key=lexcept_key)\n            except Exception as err:\n                msg = str(err)\n                msg += \"\\nIssue flattening dict %s\"%k\n                msg += \"\\n\\n\\n\" + str(v['dict'])\n                raise Exception(msg)\n            dout[k] = d\n        dout = flatten_dict(dout, parent_key='', sep=sep, deep=deep)\n        return dout", "response": "Return a flat dict view of the object s attributes and their associated field values."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef from_dict(self, fd, sep=_sep, strip=None):\n\n        self._reset()\n        dd = reshape_dict(fd)\n\n        # ---------------------\n        # Call class-specific\n        self._from_dict(dd)\n        # ---------------------\n        self._dstrip.update(**dd['dstrip'])\n        if 'dId' in dd.keys():\n            self._set_Id(Id=ID(fromdict=dd['dId']))\n\n        if strip is None:\n            strip = self._dstrip['strip']\n        if self._dstrip['strip'] != strip:\n            self.strip(strip, verb=verb)", "response": "Populate the instance attributes using a dict."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn an instance of the object with the same attributes.", "response": "def copy(self, strip=None, deep='ref'):\n        \"\"\" Return another instance of the object, with the same attributes\n\n        If deep=True, all attributes themselves are also copies\n        \"\"\"\n        dd = self.to_dict(strip=strip, deep=deep)\n        return self.__class__(fromdict=dd)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_nbytes(self):\n        dd = self.to_dict()\n        dsize = dd.fromkeys(dd.keys(),0)\n        total = 0\n        for k, v in dd.items():\n            if issubclass(v.__class__, ToFuObjectBase):\n                dsize[k] = v.get_nbytes()[0]\n            else:\n                dsize[k] = np.asarray(v).nbytes\n            total += dsize[k]\n        return total, dsize", "response": "Compute and return the object size in bytes"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a default name for saving the object from file name.", "response": "def SaveName_Conv(Mod=None, Cls=None, Type=None, Name=None, Deg=None,\n                      Exp=None, Diag=None, shot=None, version=None, usr=None,\n                      include=None):\n        \"\"\" Return a default name for saving the object\n\n        Includes key info for fast identification of the object from file name\n        Used on object creation by :class:`~tofu.pathfile.ID`\n        It is recommended to use this default name.\n\n        \"\"\"\n        Modstr = ID._dModes[Mod] if Mod is not None else None\n        include = ID._defInclude if include is None else include\n        if Cls is not None and Type is not None and 'Type' in include:\n            Clsstr = Cls+Type\n        else:\n            Clsstr = Cls\n        Dict = {'Mod':Modstr, 'Cls':Clsstr, 'Name':Name}\n        for ii in include:\n            if not ii in ['Mod','Cls','Type','Name']:\n                Dict[ii] = None\n            if ii=='Deg' and Deg is not None:\n                Dict[ii] = ID._dPref[ii]+'{0:02.0f}'.format(Deg)\n            elif ii=='shot' and shot is not None:\n                Dict[ii] = ID._dPref[ii]+'{0:05.0f}'.format(shot)\n            elif not (ii in ['Mod','Cls','Type','Name'] or eval(ii+' is None')):\n                Dict[ii] = ID._dPref[ii]+eval(ii)\n        if 'Data' in Cls:\n            Order = ['Mod','Cls','Exp','Deg','Diag','shot',\n                     'Name','version','usr']\n        else:\n            Order = ['Mod','Cls','Exp','Deg','Diag','Name',\n                     'shot','version','usr']\n\n        SVN = \"\"\n        for ii in range(0,len(Order)):\n            if Order[ii] in include and Dict[Order[ii]] is not None:\n                SVN += '_' + Dict[Order[ii]]\n        SVN = SVN.replace('__','_')\n        if SVN[0]=='_':\n            SVN = SVN[1:]\n        return SVN"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_Name(self, Name, SaveName=None,\n                 include=None,\n                 ForceUpdate=False):\n        \"\"\" Set the Name of the instance, automatically updating the SaveName\n\n        The name should be a str without spaces or underscores (removed)\n        When the name is changed, if SaveName (i.e. the name used for saving)\n        was not user-defined, it is automatically updated\n\n        Parameters\n        ----------\n        Name :      str\n            Name of the instance, without ' ' or '_' (automatically removed)\n        SaveName :  None / str\n            If provided, overrides the default name for saving (not recommended)\n        include:    list\n            Controls how te default SaveName is generated\n            Each element of the list is a key str indicating whether an element\n            should be present in the SaveName\n\n        \"\"\"\n        self._dall['Name'] = Name\n        self.set_SaveName(SaveName=SaveName, include=include,\n                          ForceUpdate=ForceUpdate)", "response": "Set the name of the instance automatically updating the SaveName\n       "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_SaveName(self,SaveName=None,\n                     include=None,\n                     ForceUpdate=False):\n        \"\"\" Set the name for saving the instance (SaveName)\n\n        SaveName can be either:\n            - provided by the user (no constraint) - not recommended\n            - automatically generated from Name and key attributes (cf. include)\n\n        Parameters\n        ----------\n        SaveName :      None / str\n            If provided, overrides the default name for saving (not recommended)\n        include :       list\n            Controls how te default SaveName is generated\n            Each element of the list is a key str indicating whether an element\n            should be present in the SaveName\n        ForceUpdate :   bool\n            Flag indicating the behaviour when SaveName=None:\n                - True : A new SaveName is generated, overriding the old one\n                - False : The former SaveName is preserved (default)\n        \"\"\"\n        if not 'SaveName-usr' in self.dall.keys():\n            self._dall['SaveName-usr'] = (SaveName is not None)\n        # If SaveName provided by user, override\n        if SaveName is not None:\n            self._dall['SaveName'] = SaveName\n            self._dall['SaveName-usr'] = True\n        else:\n            # Don't update if former is user-defined and ForceUpdate is False\n            # Override if previous was:\n            # automatic or (user-defined but ForceUpdate is True)\n            C0 = self._dall['SaveName-usr']\n            C1 = self._dall['SaveName-usr'] and ForceUpdate\n            if (not C0) or C1:\n                SN = ID.SaveName_Conv(Mod=self.Mod, Cls=self.Cls,\n                                      Type=self.Type, Name=self.Name,\n                                      Deg=self.Deg, Exp=self.Exp,\n                                      Diag=self.Diag, shot=self.shot,\n                                      version=self.version, usr=self.usr,\n                                      include=include)\n                self._dall['SaveName'] = SN\n                self._dall['SaveName-usr'] = False", "response": "Set the name for saving the instance of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the lObj attribute of the object that depends on the object.", "response": "def set_lObj(self, lObj=None):\n        \"\"\" Set the lObj attribute, storing objects the instance depends on\n\n        For example:\n        A Detect object depends on a vessel and some apertures\n        That link between should be stored somewhere (for saving/loading).\n        lObj does this: it stores the ID (as dict) of all objects depended on.\n\n        Parameters\n        ----------\n        lObj :  None / dict / :class:`~tofu.pathfile.ID` / list of such\n            Provide either:\n                - A dict (derived from :meth:`~tofu.pathfile.ID._todict`)\n                - A :class:`~tofu.pathfile.ID` instance\n                - A list of dict or :class:`~tofu.pathfile.ID` instances\n\n        \"\"\"\n        if self.lObj is None and lObj is not None:\n            self._dall['lObj'] = {}\n        if lObj is not None:\n            if type(lObj) is not list:\n                lObj = [lObj]\n            for ii in range(0,len(lObj)):\n                if type(lObj[ii]) is ID:\n                    lObj[ii] = lObj[ii].to_dict()\n            ClsU = list(set([oo['Cls'] for oo in lObj]))\n            for c in ClsU:\n                self._dall['lObj'][c] = [oo for oo in lObj if oo['Cls']==c]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef select(self, key=None, val=None, log='any', out=bool):\n        assert out in [bool, int], \"Arg out is not valid (int or bool) !\"\n        C0 = key is None or val is None\n        if C0:\n            if out is bool:\n                ind = np.ones((self._nch,), dtype=bool)\n            else:\n                ind = np.arange(0, self._nch)\n            return ind\n\n        lt0 = [list, tuple, np.ndarray]\n        lt1 = [str, int, float, np.int64, np.float64, bool]\n        C0 = log in ['any', 'all']\n        C1 = type(log) in lt0 and all([ll in ['any', 'all'] for ll in log])\n        assert C0 or C1, \"Arg out is not valid ('any','all' or an iterable) !\"\n        C2 = isinstance(key, str) and key in self._dchans.keys()\n        assert C2, \"Arg key not valid: provide key of self.dchans\"\n        C4 = type(val) in lt1\n        C5 = type(val) in lt0 and all([type(vv) in lt1 for vv in val])\n        assert C4 or C5, \"Arg val not valid, should be in %s !\"%str(lt1)\n        if C4:\n            val = [val]\n        nv = len(val)\n        ind = np.vstack([self._dchans[key] == vv for vv in val])\n        if log == 'any':\n            ind = np.any(ind,axis=0)\n        else:\n            ind = np.all(ind,axis=0)\n\n        # To be finsihed: add operators and str operations + not\n\n        return ind", "response": "Returns the indices of all channels matching the key and val pairs."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef seek_line(self):\n        pos = end_pos = self.file.tell()\n\n        read_size = self.read_size\n        if pos > read_size:\n            pos -= read_size\n        else:\n            pos = 0\n            read_size = end_pos\n\n        self.seek(pos)\n\n        bytes_read, read_str = self.read(read_size)\n\n        if bytes_read and read_str[-1] in self.line_terminators:\n            # The last charachter is a line terminator, don't count this one\n            bytes_read -= 1\n\n            if read_str[-2:] == '\\r\\n' and '\\r\\n' in self.line_terminators:\n                # found crlf\n                bytes_read -= 1\n\n        while bytes_read > 0:          \n            # Scan backward, counting the newlines in this bufferfull\n            i = bytes_read - 1\n            while i >= 0:\n                if read_str[i] in self.line_terminators:\n                    self.seek(pos + i + 1)\n                    return self.file.tell()\n                i -= 1\n\n            if pos == 0 or pos - self.read_size < 0:\n                # Not enought lines in the buffer, send the whole file\n                self.seek(0)\n                return None\n\n            pos -= self.read_size\n            self.seek(pos)\n\n            bytes_read, read_str = self.read(self.read_size)\n\n        return None", "response": "\\ Returns the position of the next line terminator in the file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef follow(self, delay=1.0):\n        # TODO: Handle log file rotation\n        self.trailing = True\n        unchanged_stats = 0\n        \n        while not self.should_stop_follow:\n            where = self.file.tell()\n            line = self.file.readline()\n            if line:    \n                if self.trailing and line in self.line_terminators:\n                    # This is just the line terminator added to the end of the file\n                    # before a new line, ignore.\n                    self.trailing = False\n                    continue\n\n                if line[-1] in self.line_terminators:\n                    line = line[:-1]\n                    if line[-1:] == '\\r\\n' and '\\r\\n' in self.line_terminators:\n                        # found crlf\n                        line = line[:-1]\n\n                self.trailing = False\n                unchanged_stats = 0\n                yield line\n            else:\n                self.trailing = True\n                self.seek(where)\n                yield no_new_line\n                # Try to catch up rotated log file\n                unchanged_stats += 1\n                if unchanged_stats >= self.MAX_UNCHANGED_STATS and \\\n                        where != os.stat(self.file.name).st_size:\n                    logger.info('Reopen log file because file may has been rotated.')\n                    self.reopen_file()\n\n                time.sleep(delay)", "response": "Iterator generator that returns lines as data is added to the log file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_notes():\n\n    # CreoVew coordinates : (-X,Y,Y)\n    notes = {}\n\n    # Low Divertor coils\n    notes['DivLow11'] = [[ 544.028, -707.593, -1904.619],\n                         [ 544.028, -739.007, -1904.619],\n                         [ 552.107, -739.007, -1932.902],\n                         [ 552.107, -707.593, -1932.902]]\n\n    notes['DivLow12'] = [[ 506.027, -741.593, -1915.066],\n                         [ 506.027, -773.007, -1915.066],\n                         [ 513.541, -773.007, -1943.504],\n                         [ 513.541, -741.593, -1943.504]]\n\n    notes['DivLow13'] = [[ 591.045, -707.593, -1924.058],\n                         [ 591.045, -739.007, -1924.058],\n                         [ 599.682, -739.007, -1952.176],\n                         [ 599.682, -707.593, -1952.176]]\n\n    notes['DivLow14'] = [[ 629.245, -741.593, -1911.906],\n                         [ 629.245, -773.007, -1911.906],\n                         [ 638.441, -773.007, -1939.846],\n                         [ 638.441, -741.593, -1939.846]]\n\n    notes['DivLow15'] = [[ 719.986, -741.593, -1913.844],\n                         [ 719.986, -773.007, -1913.844],\n                         [ 730.343, -773.007, -1941.374],\n                         [ 730.343, -741.593, -1941.374]]\n\n    notes['DivLow16'] = [[866.309, -775.593, -1852.211],\n                         [866.309, -807.007, -1852.211],\n                         [878.771, -807.007, -1878.855],\n                         [878.771, -775.593, -1878.855]]\n\n    notes['DivLow17'] = [[782.893, -741.593, -1923.577],\n                         [782.893, -773.007, -1923.577],\n                         [793.981, -773.007, -1950.821],\n                         [793.981, -741.593, -1950.821]]\n\n    notes['DivLow18'] = [[832.314, -775.593, -1902.715],\n                         [832.314, -807.007, -1902.715],\n                         [844.102, -807.007, -1929.663],\n                         [844.102, -775.593, -1929.663]]\n\n    notes['DivLow1'] = [notes['DivLow14'][3],\n                        notes['DivLow13'][3],\n                        notes['DivLow11'][0],\n                        notes['DivLow12'][1],\n                        notes['DivLow14'][2],\n                        notes['DivLow15'][1],\n                        notes['DivLow16'][1],\n                        notes['DivLow18'][2],\n                        notes['DivLow17'][3],\n                        notes['DivLow15'][0]]\n\n    #\u00a0Div Low 2\n\n    notes['DivLow21'] = [[2070.226, -770.213, -525.144],\n                         [2070.226, -801.627, -525.144],\n                         [2098.737, -801.627, -532.377],\n                         [2098.737, -770.213, -532.377]]\n\n    notes['DivLow22'] = [[2059.565, -804.213, -565.509],\n                         [2059.565, -835.627, -565.509],\n                         [2087.930, -835.627, -573.298],\n                         [2087.930, -804.213, -573.298]]\n\n    notes['DivLow23'] = [[2111.217, -770.213, -492.023],\n                         [2111.217, -801.627, -492.023],\n                         [2139.864, -801.627, -498.699],\n                         [2139.864, -770.213, -498.699]]\n\n    notes['DivLow24'] = [[2036.407, -804.213, 743.216],\n                         [2036.407, -835.627, 743.216],\n                         [2064.039, -835.627, 753.301],\n                         [2064.039, -804.213, 753.301]]\n\n    notes['DivLow25'] = [[2160.173, -804.213, -415.621],\n                         [2160.173, -835.627, -415.621],\n                         [2189.057, -835.627, -421.178],\n                         [2189.057, -804.213, -421.178]]\n\n    notes['DivLow26'] = [[2185.593, -838.213, -249.547],\n                         [2185.593, -869.627, -249.547],\n                         [2214.817, -869.627, -252.884],\n                         [2214.817, -838.213, -252.884]]\n\n    notes['DivLow27'] = [[2221.021, -838.213, -219.014],\n                         [2221.021, -869.627, -219.014],\n                         [2250.293, -869.627, -221.901],\n                         [2250.293, -838.213, -221.901]]\n\n    notes['DivLow28'] = [[2214.719, -804.213, -275.533],\n                         [2214.719, -835.627, -275.533],\n                         [2243.908, -835.627, -279.164],\n                         [2243.908, -804.213, -279.164]]\n\n    notes['DivLow2'] = [notes['DivLow24'][3],\n                        notes['DivLow23'][3],\n                        notes['DivLow21'][0],\n                        notes['DivLow22'][1],\n                        notes['DivLow24'][2],\n                        notes['DivLow25'][1],\n                        notes['DivLow26'][1],\n                        notes['DivLow27'][2],\n                        notes['DivLow28'][3],\n                        notes['DivLow25'][0]]\n\n    # Upper Divertor coils\n\n    notes['DivUp11'] = [[1915.066, 775.907, -506.027],\n                        [1915.066, 744.493, -506.027],\n                        [1943.504, 744.493, -513.541],\n                        [1943.504, 775.907, -513.541]]\n\n    notes['DivUp12'] = [[1904.619, 741.907, -544.028],\n                        [1904.619, 710.493, -544.028],\n                        [1932.902, 710.493, -552.107],\n                        [1932.902, 741.907, -552.107]]\n\n    notes['DivUp13'] = [[1911.907, 775.907, -629.241],\n                        [1911.907, 744.493, -629.241],\n                        [1939.848, 744.493, -638.437],\n                        [1939.848, 775.907, -638.437]]\n\n    notes['DivUp14'] = [[1924.058, 741.907, -591.045],\n                        [1924.058, 710.493, -591.045],\n                        [1952.176, 710.493, -599.682],\n                        [1952.176, 741.907, -599.682]]\n\n    notes['DivUp15'] = [[1852.211, 809.907, -866.309],\n                        [1852.211, 778.493, -866.309],\n                        [1878.855, 778.493, -878.771],\n                        [1878.855, 809.907, -878.771]]\n\n    notes['DivUp16'] = [[1913.844, 775.907, -719.986],\n                        [1913.844, 744.493, -719.986],\n                        [1941.374, 744.493, -730.343],\n                        [1941.374, 775.907, -730.343]]\n\n    notes['DivUp17'] = [[1902.715, 809.907, -832.314],\n                        [1902.715, 778.493, -832.314],\n                        [1929.663, 778.493, -844.102],\n                        [1929.663, 809.907, -844.102]]\n\n    notes['DivUp18'] = [[1923.577, 775.907, -782.893],\n                        [1923.577, 744.493, -782.893],\n                        [1950.821, 744.493, -793.981],\n                        [1950.821, 775.907, -793.981]]\n\n    notes['DivUp1'] = [notes['DivUp13'][3],\n                       notes['DivUp11'][0],\n                       notes['DivUp12'][1],\n                       notes['DivUp14'][2],\n                       notes['DivUp13'][2],\n                       notes['DivUp16'][1],\n                       notes['DivUp18'][2],\n                       notes['DivUp17'][3],\n                       notes['DivUp15'][0],\n                       notes['DivUp16'][0]]\n\n    # Div Up 2\n\n    notes['DivUp21'] = [[565.509, 838.527, -2059.565],\n                        [565.509, 807.113, -2059.565],\n                        [573.298, 807.113, -2087.930],\n                        [573.298, 838.527, -2087.930]]\n\n    notes['DivUp22'] = [[525.144, 804.527, -2070.226],\n                        [525.144, 773.113, -2070.226],\n                        [532.377, 773.113, -2098.737],\n                        [532.377, 804.527, -2098.737]]\n\n    notes['DivUp23'] = [[492.023, 804.527, -2111.217],\n                        [492.023, 773.113, -2111.217],\n                        [498.699, 773.113, -2139.864],\n                        [498.699, 804.527, -2139.864]]\n\n    notes['DivUp24'] = [[-743.216, 838.527, -2036.407],\n                        [-743.216, 807.113, -2036.407],\n                        [-753.301, 807.113, -2064.039],\n                        [-753.301, 838.527, -2064.039]]\n\n    notes['DivUp25'] = [[249.547, 872.527, -2185.593],\n                        [249.547, 841.113, -2185.593],\n                        [252.884, 841.113, -2214.817],\n                        [252.884, 872.527, -2214.817]]\n\n    notes['DivUp26'] = [[415.621, 838.527, -2160.173],\n                        [415.621, 807.113, -2160.173],\n                        [421.178, 807.113, -2189.057],\n                        [421.178, 838.527, -2189.057]]\n\n    notes['DivUp27'] = [[275.533, 838.527, -2214.719],\n                        [275.533, 807.113, -2214.719],\n                        [279.164, 807.113, -2243.908],\n                        [279.164, 838.527, -2243.908]]\n\n    notes['DivUp28'] = [[219.014, 872.527, -2221.021],\n                        [219.014, 841.113, -2221.021],\n                        [221.901, 841.113, -2250.293],\n                        [221.901, 872.527, -2250.293]]\n\n    notes['DivUp2'] = [notes['DivUp24'][3],\n                       notes['DivUp21'][0],\n                       notes['DivUp22'][1],\n                       notes['DivUp23'][2],\n                       notes['DivUp24'][2],\n                       notes['DivUp26'][1],\n                       notes['DivUp27'][2],\n                       notes['DivUp28'][3],\n                       notes['DivUp25'][0],\n                       notes['DivUp26'][0]]\n\n    # Central Solenoid\n\n    notes['CS'] = [[0.,  875., 669.],\n                   [0., -845., 669.],\n                   [0., -845., 812.],\n                   [0.,  875., 812.]]\n\n    # External PF coils\n\n    notes['Bu'] = [[0., 2400., 1082.],\n                   [0., 1200., 1082.],\n                   [-577.5, 1200., 1000.259],\n                   [0., 2400., 1155.]]\n\n    notes['Bl'] = [[-1083., -1200., 0.],\n                   [-1083., -2400., 0.],\n                   [-1155., -2400., 0.],\n                   [-816.708, -1200., -816.708]]\n\n    notes['Du'] = [[-1374., 2109., 2379.838],\n                   [-1374., 1746., 2379.838],\n                   [-1507.5, 1746., 2611.067],\n                   [-1507.5, 2109., 2611.067]]\n\n    notes['Dl'] = [[-1374., -1746., 2379.838],\n                   [-1374., -2109., 2379.838],\n                   [-1507.5, -2109., 2611.067],\n                   [-1507.5, -1746., 2611.067]]\n\n    notes['Eu'] = [[-1814., 1736., 3141.94],\n                   [-1814., 1346., 3141.94],\n                   [-1957.5, 1346., 3390.489],\n                   [-1957.5, 1736., 3390.489]]\n\n    notes['El'] = [[-1814., -1346., 3141.94],\n                   [-1814., -1736., 3141.94],\n                   [-1957.5, -1736., 3390.489],\n                   [-1957.5, -1346., 3390.489]]\n\n    notes['Fu'] = [[-2115., 838., 3663.287],\n                   [-2115., 450., 3663.287],\n                   [-2259.5, 450., 3913.569],\n                   [-2259.5, 838., 3913.569]]\n\n    notes['Fl'] = [[-2115., -452., 3663.287],\n                   [-2115., -840., 3663.287],\n                   [-2259.5, -840., 3913.569],\n                   [-2259.5, -452., 3913.569]]\n\n    for k in notes.keys():\n        notes[k] = np.array(notes[k])*1.e-3\n\n    return notes", "response": "Returns a list of notes for the given CreoVew coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbuilds and return a frequency distribution over xpath occurrences.", "response": "def get_xpath_frequencydistribution(paths):\r\n    \"\"\" Build and return a frequency distribution over xpath occurrences.\"\"\"\r\n    # \"html/body/div/div/text\" -> [ \"html\", \"body\", \"div\", \"div\", \"text\" ]\r\n    splitpaths = [p.split('/') for p in paths]\r\n\r\n    # get list of \"parentpaths\" by right-stripping off the last xpath-node,\r\n    # effectively getting the parent path\r\n    parentpaths = ['/'.join(p[:-1]) for p in splitpaths]\r\n\r\n    # build frequency distribution\r\n    parentpaths_counter = Counter(parentpaths)\r\n    return parentpaths_counter.most_common()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef calc_avgstrlen_pathstextnodes(pars_tnodes, dbg=False):\r\n    ttl = 0\r\n    for _, tnodes in pars_tnodes:\r\n        ttl += tnodes[3]        # index #3 holds the avg strlen\r\n\r\n    crd = len(pars_tnodes)\r\n    avg = ttl/crd\r\n    if dbg is True:\r\n        print(avg)\r\n    #       avg = ttl/crd\r\n    return (avg, ttl, crd)", "response": "Calculates the total length of path - text nodes in a sequence of TNode objects."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngive a url path or filelike obj returns a list of parent paths and children textnodes and feature tuples.", "response": "def get_parent_xpaths_and_textnodes(filename_url_or_filelike,\r\n                                    xpath_to_text=TEXT_FINDER_XPATH):\r\n    \"\"\"Provided a url, path or filelike obj., we construct an html tree,\r\n    and build a list of parent paths and children textnodes & \"feature\"\r\n    tuples.\r\n    The features - descriptive values used for gathering statistics that\r\n    attempts to describe this artificial environment I've created (parent\r\n    paths and children textnodes) - are initialized to '0'\r\n\r\n    Modifications of eatiht.get_sentence_xpath_tuples: some code was\r\n    refactored-out, variable names are slightly different. This function\r\n    does wrap the ltml.tree construction, so a file path, file-like\r\n    structure, or URL is required.\r\n    \"\"\"\r\n    html_tree = get_html_tree(filename_url_or_filelike)\r\n\r\n    xpath_finder = html_tree.getroot().getroottree().getpath\r\n\r\n    nodes_with_text = html_tree.xpath(xpath_to_text)\r\n\r\n    # read note 5\r\n    parentpaths_textnodes = [\r\n        (xpath_finder(n),\r\n         [n.xpath('.//text()'),   # list of text from textnode\r\n          0,                      # number of texts (cardinality)\r\n          0,                      # total string length in list of texts\r\n          0])                     # average string length\r\n        for n in nodes_with_text\r\n        ]\r\n\r\n    if len(parentpaths_textnodes) is 0:\r\n        raise Exception(\"No text nodes satisfied the xpath:\\n\\n\" +\r\n                        xpath_to_text + \"\\n\\nThis can be due to user's\" +\r\n                        \" custom xpath, min_str_length value, or both\")\r\n\r\n    return parentpaths_textnodes"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbuilding and return a frequency distribution over xpath occurrences.", "response": "def get_xpath_frequencydistribution(paths):\r\n    \"\"\" Build and return a frequency distribution over xpath occurrences.\"\"\"\r\n    # \"html/body/div/div/text\" -> [ \"html\", \"body\", \"div\", \"div\", \"text\" ]\r\n    splitpaths = [p.rsplit('/', 1) for p in paths]\r\n\r\n    # get list of \"parentpaths\" by right-stripping off the last xpath-node,\r\n    # effectively getting the parent path\r\n    # thanks to eugene-eeo for optimization\r\n    parentpaths = [p[0] for p in splitpaths]\r\n\r\n    # build frequency distribution\r\n    parentpaths_counter = Counter(parentpaths)\r\n    return parentpaths_counter.most_common()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates the average length of a list of subtrees", "response": "def calcavg_avgstrlen_subtrees(subtrees, dbg=False):\r\n    \"\"\"In the effort of not using external libraries (like scipy, numpy, etc),\r\n    I've written some harmless code for basic statistical calculations\r\n    \"\"\"\r\n    ttl = 0\r\n    for subtree in subtrees:\r\n        ttl += subtree.avg_strlen\r\n\r\n    crd = len(subtrees)\r\n    avg = ttl/crd\r\n    if dbg is True:\r\n        print(avg)\r\n    #       avg = ttl/crd\r\n    return (avg, ttl, crd)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef extract(filename_url_filelike_or_htmlstring):\r\n    html_tree = get_html_tree(filename_url_filelike_or_htmlstring)\r\n\r\n    subtrees = get_textnode_subtrees(html_tree)\r\n    #[iterable, cardinality, ttl across iterable, avg across iterable.])\r\n\r\n    # calculate AABSL\r\n    avg, _, _ = calcavg_avgstrlen_subtrees(subtrees)\r\n\r\n    # \"high-pass\" filter\r\n    filtered = [subtree for subtree in subtrees\r\n                if subtree.ttl_strlen > avg]\r\n\r\n    paths = [subtree.parent_path for subtree in filtered]\r\n\r\n    hist = get_xpath_frequencydistribution(paths)\r\n\r\n    target_subtrees = [stree for stree in subtrees\r\n                       if hist[0][0] in stree.parent_path]\r\n\r\n    title = html_tree.find(\".//title\")\r\n\r\n    return TextNodeTree(title.text_content(), target_subtrees, hist)", "response": "An improved algorithm over the original eatiht algorithm\r\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncalculate cardinality total and average string length", "response": "def __learn_oneself(self):\r\n        \"\"\"calculate cardinality, total and average string length\"\"\"\r\n        if not self.__parent_path or not self.__text_nodes:\r\n            raise Exception(\"This error occurred because the step constructor\\\r\n                            had insufficient textnodes or it had empty string\\\r\n                            for its parent xpath\")\r\n        # Iterate through text nodes and sum up text length\r\n        # TODO: consider naming this child_count or cardinality\r\n        # or branch_cnt\r\n        self.tnodes_cnt = len(self.__text_nodes)\r\n        # consider naming this total\r\n        self.ttl_strlen = sum([len(tnode) for tnode in self.__text_nodes])\r\n        # consider naming this average\r\n        self.avg_strlen = self.ttl_strlen/self.tnodes_cnt"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __make_tree(self):\r\n\r\n        # create div with \"container\" class\r\n        div = E.DIV(E.CLASS(\"container\"))\r\n\r\n        # append header with title\r\n        div.append(E.H2(self.__title))\r\n\r\n        # next, iterate through subtrees appending each tree to div\r\n        for subtree in self.__subtrees:\r\n            div.append(subtree.get_html())\r\n\r\n        # Connect div to body\r\n        body = E.BODY(div)\r\n\r\n        # attach body to html\r\n        self.__htmltree = E.HTML(\r\n            E.HEAD(\r\n                E.TITLE(self.__title)\r\n                ),\r\n            body\r\n            )", "response": "Create a tree using lxml. html. builder and our subtrees"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_html(self):\r\n        if self.__htmltree is not None:\r\n            return self.__htmltree\r\n        else:\r\n            self.__make_tree()\r\n            return self.__htmltree", "response": "Generates if need be and returns a simpler html document with text"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates if need be and returns a simpler html string with extracted text", "response": "def get_html_string(self):\r\n        \"\"\"Generates if need be and returns a simpler html string with\r\n        extracted text\"\"\"\r\n        if self.__htmltree is not None:\r\n            return htmltostring(self.__htmltree)\r\n        else:\r\n            self.__make_tree()\r\n            return htmltostring(self.__htmltree)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning all joined text from each subtree", "response": "def get_text(self):\r\n        \"\"\"Return all joined text from each subtree\"\"\"\r\n        if self.__fulltext:\r\n            return self.__fulltext\r\n        else:\r\n            self.__fulltext = \"\\n\\n\".join(text.get_text()\r\n                                          for text in self.__subtrees)\r\n            return self.__fulltext"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef bootstrapify(self):\r\n        if self.__htmltree is None:\r\n            #raise Exception(\"HtmlTree has not been made yet\")\r\n            self.__make_tree()\r\n\r\n        # add bootstrap cdn to head\r\n        self.__htmltree.find('head').append(\r\n            E.LINK(rel=\"stylesheet\",\r\n                   href=\"//maxcdn.bootstrapcdn.com/bootstrap/3.3.1/css/bootstrap.min.css\",\r\n                   type=\"text/css\"))\r\n\r\n        # center images\r\n        for img_parent in self.__htmltree.xpath(\"//img/..\"):\r\n            # the space before the class to insert is CRITICAL!\r\n            img_parent.attrib[\"class\"] += \" text-center\"\r\n\r\n        # make images responsive\r\n        for img in self.__htmltree.xpath(\"//img\"):\r\n            # the space before the class to insert is CRITICAL!\r\n            img.attrib[\"class\"] += \" img-responsive\"", "response": "Add bootstrap cdn to headers of html"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngiving a url and xpath this function will download parse then extract a list of ( text exact - xpath ) tuples.", "response": "def get_sentence_xpath_tuples(filename_url_or_filelike,\r\n                              xpath_to_text=TEXT_FINDER_XPATH):\r\n    \"\"\"\r\n    Given a url and xpath, this function will download, parse, then\r\n    iterate though queried text-nodes. From the resulting text-nodes,\r\n    extract a list of (text, exact-xpath) tuples.\r\n    \"\"\"\r\n\r\n    parsed_html = get_html_tree(filename_url_or_filelike)\r\n\r\n    try:\r\n        xpath_finder = parsed_html.getroot().getroottree().getpath\r\n    except(AttributeError):\r\n        xpath_finder = parsed_html.getroottree().getpath\r\n\r\n    nodes_with_text = parsed_html.xpath(xpath_to_text)\r\n\r\n    sent_xpath_pairs = [\r\n        # hard-code paragraph breaks (there has to be a better way)\r\n        ('\\n\\n' + s, xpath_finder(n)) if e == 0\r\n        else (s, xpath_finder(n))\r\n        for n in nodes_with_text\r\n        for e, s in enumerate(SENTENCE_TOKEN_PATTERN.split(\r\n            BRACKET_PATTERN.sub('', ''.join(n.xpath('.//text()')))))\r\n        if s.endswith(tuple(SENTENCE_ENDING))\r\n        ]\r\n\r\n    return sent_xpath_pairs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwrapping function for extracting the main article from html document. A crappy flowchart/state-diagram: start: url[,xpath] -> xpaths of text-nodes -> frequency distribution -> argmax( freq. dist. ) = likely xpath leading to article's content", "response": "def extract(url_or_htmlstring, xpath_to_text=TEXT_FINDER_XPATH):\r\n    \"\"\"\r\n    Wrapper function for extracting the main article from html document.\r\n\r\n    A crappy flowchart/state-diagram:\r\n    start: url[,xpath] -> xpaths of text-nodes -> frequency distribution\r\n    -> argmax( freq. dist. ) = likely xpath leading to article's content\r\n    \"\"\"\r\n    sent_xpath_pairs = get_sentence_xpath_tuples(url_or_htmlstring, xpath_to_text)\r\n\r\n    hist = get_xpath_frequencydistribution(\r\n        [x for (s, x) in sent_xpath_pairs])\r\n\r\n    max_path = hist[0]\r\n\r\n    article_text = ' '.join([s for (s, x) in sent_xpath_pairs\r\n                             if max_path[0] in x])\r\n\r\n    # starting from index 2 because of the two extra newlines in front\r\n    return article_text[2:]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rar3_type(btype):\n    if btype < rf.RAR_BLOCK_MARK or btype > rf.RAR_BLOCK_ENDARC:\n        return \"*UNKNOWN*\"\n    return block_strs[btype - rf.RAR_BLOCK_MARK]", "response": "RAR3 type code as string."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef xprint(m, *args):\n    if sys.hexversion < 0x3000000:\n        m = m.decode('utf8')\n    if args:\n        m = m % args\n    if sys.hexversion < 0x3000000:\n        m = m.encode('utf8')\n    sys.stdout.write(m)\n    sys.stdout.write('\\n')", "response": "Print string to stdout."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nshowing flag names and handle dict size.", "response": "def get_file_flags(flags):\n    \"\"\"Show flag names and handle dict size.\n    \"\"\"\n    res = render_flags(flags & ~rf.RAR_FILE_DICTMASK, file_bits)\n\n    xf = (flags & rf.RAR_FILE_DICTMASK) >> 5\n    res += \",\" + file_parms[xf]\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef show_item(h):\n    if isinstance(h, rf.Rar3Info):\n        show_item_v3(h)\n    elif isinstance(h, rf.Rar5Info):\n        show_item_v5(h)\n    else:\n        xprint('Unknown info record')", "response": "Show any RAR3 or RAR5 record."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nshowing any RAR3 record.", "response": "def show_item_v3(h):\n    \"\"\"Show any RAR3 record.\n    \"\"\"\n    st = rar3_type(h.type)\n    xprint(\"%s: hdrlen=%d datlen=%d\", st, h.header_size, h.add_size)\n    if h.type in (rf.RAR_BLOCK_FILE, rf.RAR_BLOCK_SUB):\n        if h.host_os == rf.RAR_OS_UNIX:\n            s_mode = \"0%o\" % h.mode\n        else:\n            s_mode = \"0x%x\" % h.mode\n        xprint(\"  flags=0x%04x:%s\", h.flags, get_file_flags(h.flags))\n        if h.host_os >= 0 and h.host_os < len(os_list):\n            s_os = os_list[h.host_os]\n        else:\n            s_os = \"?\"\n        xprint(\"  os=%d:%s ver=%d mode=%s meth=%c cmp=%d dec=%d vol=%d\",\n               h.host_os, s_os,\n               h.extract_version, s_mode, h.compress_type,\n               h.compress_size, h.file_size, h.volume)\n        ucrc = (h.CRC + (1 << 32)) & ((1 << 32) - 1)\n        xprint(\"  crc=0x%08x (%d) date_time=%s\", ucrc, h.CRC, fmt_time(h.date_time))\n        xprint(\"  name=%s\", h.filename)\n        if h.mtime:\n            xprint(\"  mtime=%s\", fmt_time(h.mtime))\n        if h.ctime:\n            xprint(\"  ctime=%s\", fmt_time(h.ctime))\n        if h.atime:\n            xprint(\"  atime=%s\", fmt_time(h.atime))\n        if h.arctime:\n            xprint(\"  arctime=%s\", fmt_time(h.arctime))\n    elif h.type == rf.RAR_BLOCK_MAIN:\n        xprint(\"  flags=0x%04x:%s\", h.flags, render_flags(h.flags, main_bits))\n    elif h.type == rf.RAR_BLOCK_ENDARC:\n        xprint(\"  flags=0x%04x:%s\", h.flags, render_flags(h.flags, endarc_bits))\n    elif h.type == rf.RAR_BLOCK_MARK:\n        xprint(\"  flags=0x%04x:\", h.flags)\n    else:\n        xprint(\"  flags=0x%04x:%s\", h.flags, render_flags(h.flags, generic_bits))\n\n    if h.comment is not None:\n        cm = repr(h.comment)\n        if cm[0] == 'u':\n            cm = cm[1:]\n        xprint(\"  comment=%s\", cm)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef show_item_v5(h):\n    st = rar5_type(h.block_type)\n    xprint(\"%s: hdrlen=%d datlen=%d hdr_extra=%d\", st, h.header_size,\n           h.compress_size, h.block_extra_size)\n    xprint(\"  block_flags=0x%04x:%s\", h.block_flags, render_flags(h.block_flags, r5_block_flags))\n    if h.block_type in (rf.RAR5_BLOCK_FILE, rf.RAR5_BLOCK_SERVICE):\n        xprint(\"  name=%s\", h.filename)\n        if h.file_host_os == rf.RAR5_OS_UNIX:\n            s_os = 'UNIX'\n            s_mode = \"0%o\" % h.mode\n        else:\n            s_os = 'WINDOWS'\n            s_mode = \"0x%x\" % h.mode\n        xprint(\"  file_flags=0x%04x:%s\", h.file_flags, render_flags(h.file_flags, r5_file_flags))\n\n        cmp_flags = h.file_compress_flags\n        xprint(\"  cmp_algo=%d cmp_meth=%d dict=%d solid=%r\",\n               cmp_flags & 0x3f,\n               (cmp_flags >> 7) & 0x07,\n               cmp_flags >> 10,\n               cmp_flags & rf.RAR5_COMPR_SOLID > 0)\n        xprint(\"  os=%d:%s mode=%s cmp=%r dec=%r vol=%r\",\n               h.file_host_os, s_os, s_mode,\n               h.compress_size, h.file_size, h.volume)\n        if h.CRC is not None:\n            xprint(\"  crc=0x%08x (%d)\", h.CRC, h.CRC)\n        if h.blake2sp_hash is not None:\n            xprint(\"  blake2sp=%s\", rf.tohex(h.blake2sp_hash))\n        if h.date_time is not None:\n            xprint(\"  date_time=%s\", fmt_time(h.date_time))\n        if h.mtime:\n            xprint(\"  mtime=%s\", fmt_time(h.mtime))\n        if h.ctime:\n            xprint(\"  ctime=%s\", fmt_time(h.ctime))\n        if h.atime:\n            xprint(\"  atime=%s\", fmt_time(h.atime))\n        if h.arctime:\n            xprint(\"  arctime=%s\", fmt_time(h.arctime))\n        if h.flags & rf.RAR_FILE_PASSWORD:\n            enc_algo, enc_flags, kdf_count, salt, iv, checkval = h.file_encryption\n            algo_name = 'AES256' if enc_algo == rf.RAR5_XENC_CIPHER_AES256 else 'UnknownAlgo'\n            xprint('  algo=%d:%s enc_flags=%04x:%s kdf_lg=%d kdf_count=%d salt=%s iv=%s checkval=%s',\n                   enc_algo, algo_name, enc_flags, render_flags(enc_flags, r5_file_enc_flags),\n                   kdf_count, 1 << kdf_count, rf.tohex(salt), rf.tohex(iv),\n                   checkval and rf.tohex(checkval) or '-')\n        if h.file_redir:\n            redir_type, redir_flags, redir_name = h.file_redir\n            xprint('  redir: type=%s flags=%d:%s destination=%s',\n                   r5_file_redir_types.get(redir_type, 'Unknown'),\n                   redir_flags, render_flags(redir_flags, r5_file_redir_flags),\n                   redir_name)\n        if h.file_owner:\n            uname, gname, uid, gid = h.file_owner\n            xprint('  owner: name=%r group=%r uid=%r gid=%r',\n                   uname, gname, uid, gid)\n        if h.file_version:\n            flags, version = h.file_version\n            xprint('  version: flags=%r version=%r', flags, version)\n    elif h.block_type == rf.RAR5_BLOCK_MAIN:\n        xprint(\"  flags=0x%04x:%s\", h.flags, render_flags(h.main_flags, r5_main_flags))\n    elif h.block_type == rf.RAR5_BLOCK_ENDARC:\n        xprint(\"  flags=0x%04x:%s\", h.flags, render_flags(h.endarc_flags, r5_endarc_flags))\n    elif h.block_type == rf.RAR5_BLOCK_ENCRYPTION:\n        algo_name = 'AES256' if h.encryption_algo == rf.RAR5_XENC_CIPHER_AES256 else 'UnknownAlgo'\n        xprint(\"  algo=%d:%s flags=0x%04x:%s\", h.encryption_algo, algo_name, h.flags,\n               render_flags(h.encryption_flags, r5_enc_flags))\n        xprint(\"  kdf_lg=%d kdf_count=%d\", h.encryption_kdf_count, 1 << h.encryption_kdf_count)\n        xprint(\"  salt=%s\", rf.tohex(h.encryption_salt))\n    else:\n        xprint(\"  - missing info -\")\n\n    if h.comment is not None:\n        cm = repr(h.comment)\n        if cm[0] == 'u':\n            cm = cm[1:]\n        xprint(\"  comment=%s\", cm)", "response": "Show any RAR5 record."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_crc(f, inf, desc):\n    exp = inf._md_expect\n    if exp is None:\n        return\n    ucrc = f._md_context.digest()\n    if ucrc != exp:\n        print('crc error - %s - exp=%r got=%r' % (desc, exp, ucrc))", "response": "Compare result crc to expected value."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_vint(buf, pos):\n    limit = min(pos + 11, len(buf))\n    res = ofs = 0\n    while pos < limit:\n        b = _byte_code(buf[pos])\n        res += ((b & 0x7F) << ofs)\n        pos += 1\n        ofs += 7\n        if b < 0x80:\n            return res, pos\n    raise BadRarFile('cannot load vint')", "response": "Load variable - size int."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_byte(buf, pos):\n    end = pos + 1\n    if end > len(buf):\n        raise BadRarFile('cannot load byte')\n    return S_BYTE.unpack_from(buf, pos)[0], end", "response": "Load a single byte from a buffer."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload little - endian 32 - bit integer from buffer at the given position.", "response": "def load_le32(buf, pos):\n    \"\"\"Load little-endian 32-bit integer\"\"\"\n    end = pos + 4\n    if end > len(buf):\n        raise BadRarFile('cannot load le32')\n    return S_LONG.unpack_from(buf, pos)[0], pos + 4"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload a sequence of bytes from a buffer.", "response": "def load_bytes(buf, num, pos):\n    \"\"\"Load sequence of bytes\"\"\"\n    end = pos + num\n    if end > len(buf):\n        raise BadRarFile('cannot load bytes')\n    return buf[pos : end], end"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load_vstr(buf, pos):\n    slen, pos = load_vint(buf, pos)\n    return load_bytes(buf, slen, pos)", "response": "Load bytes prefixed by vint length and return them as a list of bytes."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_dostime(buf, pos):\n    stamp, pos = load_le32(buf, pos)\n    tup = parse_dos_time(stamp)\n    return to_datetime(tup), pos", "response": "Load dos timestamp from buffer at given position."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nloading LE32 unix timestamp", "response": "def load_unixtime(buf, pos):\n    \"\"\"Load LE32 unix timestamp\"\"\"\n    secs, pos = load_le32(buf, pos)\n    dt = datetime.fromtimestamp(secs, UTC)\n    return dt, pos"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading LE64 windows timestamp", "response": "def load_windowstime(buf, pos):\n    \"\"\"Load LE64 windows timestamp\"\"\"\n    # unix epoch (1970) in seconds from windows epoch (1601)\n    unix_epoch = 11644473600\n    val1, pos = load_le32(buf, pos)\n    val2, pos = load_le32(buf, pos)\n    secs, n1secs = divmod((val2 << 32) | val1, 10000000)\n    dt = datetime.fromtimestamp(secs - unix_epoch, UTC)\n    dt = dt.replace(microsecond=n1secs // 10)\n    return dt, pos"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_filelike(obj):\n    if isinstance(obj, (bytes, unicode)):\n        return False\n    res = True\n    for a in ('read', 'tell', 'seek'):\n        res = res and hasattr(obj, a)\n    if not res:\n        raise ValueError(\"Invalid object passed as file\")\n    return True", "response": "Check if object is a file object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef rar3_s2k(psw, salt):\n    if not isinstance(psw, unicode):\n        psw = psw.decode('utf8')\n    seed = bytearray(psw.encode('utf-16le') + salt)\n    h = Rar3Sha1(rarbug=True)\n    iv = EMPTY\n    for i in range(16):\n        for j in range(0x4000):\n            cnt = S_LONG.pack(i * 0x4000 + j)\n            h.update(seed)\n            h.update(cnt[:3])\n            if j == 0:\n                iv += h.digest()[19:20]\n    key_be = h.digest()[:16]\n    key_le = pack(\"<LLLL\", *unpack(\">LLLL\", key_be))\n    return key_le, iv", "response": "String - to - key hash for RAR3."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndecompresses a blob of compressed data.", "response": "def rar3_decompress(vers, meth, data, declen=0, flags=0, crc=0, psw=None, salt=None):\n    \"\"\"Decompress blob of compressed data.\n\n    Used for data with non-standard header - eg. comments.\n    \"\"\"\n    # already uncompressed?\n    if meth == RAR_M0 and (flags & RAR_FILE_PASSWORD) == 0:\n        return data\n\n    # take only necessary flags\n    flags = flags & (RAR_FILE_PASSWORD | RAR_FILE_SALT | RAR_FILE_DICTMASK)\n    flags |= RAR_LONG_BLOCK\n\n    # file header\n    fname = b'data'\n    date = 0\n    mode = 0x20\n    fhdr = S_FILE_HDR.pack(len(data), declen, RAR_OS_MSDOS, crc,\n                           date, vers, meth, len(fname), mode)\n    fhdr += fname\n    if flags & RAR_FILE_SALT:\n        if not salt:\n            return EMPTY\n        fhdr += salt\n\n    # full header\n    hlen = S_BLK_HDR.size + len(fhdr)\n    hdr = S_BLK_HDR.pack(0, RAR_BLOCK_FILE, flags, hlen) + fhdr\n    hcrc = rar_crc32(hdr[2:]) & 0xFFFF\n    hdr = S_BLK_HDR.pack(hcrc, RAR_BLOCK_FILE, flags, hlen) + fhdr\n\n    # archive main header\n    mh = S_BLK_HDR.pack(0x90CF, RAR_BLOCK_MAIN, 0, 13) + ZERO * (2 + 4)\n\n    # decompress via temp rar\n    tmpfd, tmpname = mkstemp(suffix='.rar')\n    tmpf = os.fdopen(tmpfd, \"wb\")\n    try:\n        tmpf.write(RAR_ID + mh + hdr + data)\n        tmpf.close()\n\n        cmd = [UNRAR_TOOL] + list(OPEN_ARGS)\n        add_password_arg(cmd, psw, (flags & RAR_FILE_PASSWORD))\n        cmd.append(tmpname)\n\n        p = custom_popen(cmd)\n        return p.communicate()[0]\n    finally:\n        tmpf.close()\n        os.unlink(tmpname)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting 6 - part time tuple into datetime object.", "response": "def to_datetime(t):\n    \"\"\"Convert 6-part time tuple into datetime object.\n    \"\"\"\n    if t is None:\n        return None\n\n    # extract values\n    year, mon, day, h, m, s = t\n\n    # assume the values are valid\n    try:\n        return datetime(year, mon, day, h, m, s)\n    except ValueError:\n        pass\n\n    # sanitize invalid values\n    mday = (0, 31, 29, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31)\n    if mon < 1:\n        mon = 1\n    if mon > 12:\n        mon = 12\n    if day < 1:\n        day = 1\n    if day > mday[mon]:\n        day = mday[mon]\n    if h > 23:\n        h = 23\n    if m > 59:\n        m = 59\n    if s > 59:\n        s = 59\n    if mon == 2 and day == 29:\n        try:\n            return datetime(year, mon, day, h, m, s)\n        except ValueError:\n            day = 28\n    return datetime(year, mon, day, h, m, s)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing standard 32-bit DOS timestamp.", "response": "def parse_dos_time(stamp):\n    \"\"\"Parse standard 32-bit DOS timestamp.\n    \"\"\"\n    sec, stamp = stamp & 0x1F, stamp >> 5\n    mn,  stamp = stamp & 0x3F, stamp >> 6\n    hr,  stamp = stamp & 0x1F, stamp >> 5\n    day, stamp = stamp & 0x1F, stamp >> 5\n    mon, stamp = stamp & 0x0F, stamp >> 4\n    yr = (stamp & 0x7F) + 1980\n    return (yr, mon, day, hr, mn, sec * 2)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef custom_popen(cmd):\n    # needed for py2exe\n    creationflags = 0\n    if sys.platform == 'win32':\n        creationflags = 0x08000000   # CREATE_NO_WINDOW\n\n    # run command\n    try:\n        p = Popen(cmd, bufsize=0, stdout=PIPE, stdin=PIPE, stderr=STDOUT,\n                  creationflags=creationflags)\n    except OSError as ex:\n        if ex.errno == errno.ENOENT:\n            raise RarCannotExec(\"Unrar not installed? (rarfile.UNRAR_TOOL=%r)\" % UNRAR_TOOL)\n        if ex.errno == errno.EACCES or ex.errno == errno.EPERM:\n            raise RarCannotExec(\"Cannot execute unrar (rarfile.UNRAR_TOOL=%r)\" % UNRAR_TOOL)\n        raise\n    return p", "response": "Custom Popen wrapper for the Unrar command."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrun command and collect output raise error if needed.", "response": "def custom_check(cmd, ignore_retcode=False):\n    \"\"\"Run command, collect output, raise error if needed.\n    \"\"\"\n    p = custom_popen(cmd)\n    out, _ = p.communicate()\n    if p.returncode and not ignore_retcode:\n        raise RarExecError(\"Check-run failed\")\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_password_arg(cmd, psw, ___required=False):\n    if UNRAR_TOOL == ALT_TOOL:\n        return\n    if psw is not None:\n        cmd.append('-p' + psw)\n    else:\n        cmd.append('-p-')", "response": "Append password switch to commandline."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nraise exception according to unrar exit code.", "response": "def check_returncode(p, out):\n    \"\"\"Raise exception according to unrar exit code.\n    \"\"\"\n    code = p.returncode\n    if code == 0:\n        return\n\n    # map return code to exception class, codes from rar.txt\n    errmap = [None,\n              RarWarning, RarFatalError, RarCRCError, RarLockedArchiveError,    # 1..4\n              RarWriteError, RarOpenError, RarUserError, RarMemoryError,        # 5..8\n              RarCreateError, RarNoFilesError, RarWrongPassword]                # 9..11\n    if UNRAR_TOOL == ALT_TOOL:\n        errmap = [None]\n    if code > 0 and code < len(errmap):\n        exc = errmap[code]\n    elif code == 255:\n        exc = RarUserBreak\n    elif code < 0:\n        exc = RarSignalExit\n    else:\n        exc = RarUnknownError\n\n    # format message\n    if out:\n        msg = \"%s [%d]: %s\" % (exc.__doc__, p.returncode, out)\n    else:\n        msg = \"%s [%d]\" % (exc.__doc__, p.returncode)\n\n    raise exc(msg)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef membuf_tempfile(memfile):\n    memfile.seek(0, 0)\n\n    tmpfd, tmpname = mkstemp(suffix='.rar')\n    tmpf = os.fdopen(tmpfd, \"wb\")\n\n    try:\n        while True:\n            buf = memfile.read(BSIZE)\n            if not buf:\n                break\n            tmpf.write(buf)\n        tmpf.close()\n    except:\n        tmpf.close()\n        os.unlink(tmpname)\n        raise\n    return tmpname", "response": "Write in - memory file object to real file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef isdir(self):\n        if self.type == RAR_BLOCK_FILE:\n            return (self.flags & RAR_FILE_DIRECTORY) == RAR_FILE_DIRECTORY\n        return False", "response": "Returns True if entry is a directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef setpassword(self, password):\n        self._password = password\n        if self._file_parser:\n            if self._file_parser.has_header_encryption():\n                self._file_parser = None\n        if not self._file_parser:\n            self._parse()\n        else:\n            self._file_parser.setpassword(self._password)", "response": "Sets the password to use when extracting."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef open(self, fname, mode='r', psw=None):\n\n        if mode != 'r':\n            raise NotImplementedError(\"RarFile.open() supports only mode=r\")\n\n        # entry lookup\n        inf = self.getinfo(fname)\n        if inf.isdir():\n            raise TypeError(\"Directory does not have any data: \" + inf.filename)\n\n        # check password\n        if inf.needs_password():\n            psw = psw or self._password\n            if psw is None:\n                raise PasswordRequired(\"File %s requires password\" % inf.filename)\n        else:\n            psw = None\n\n        return self._file_parser.open(inf, psw)", "response": "Returns file - like object from where the data can be read."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading uncompressed data for a specific file.", "response": "def read(self, fname, psw=None):\n        \"\"\"Return uncompressed data for archive entry.\n\n        For longer files using :meth:`RarFile.open` may be better idea.\n\n        Parameters:\n\n            fname\n                filename or RarInfo instance\n            psw\n                password to use for extracting.\n        \"\"\"\n\n        with self.open(fname, 'r', psw) as f:\n            return f.read()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef extract(self, member, path=None, pwd=None):\n        if isinstance(member, RarInfo):\n            fname = member.filename\n        else:\n            fname = member\n        self._extract([fname], path, pwd)", "response": "Extracts a single file into the current directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef extractall(self, path=None, members=None, pwd=None):\n        fnlist = []\n        if members is not None:\n            for m in members:\n                if isinstance(m, RarInfo):\n                    fnlist.append(m.filename)\n                else:\n                    fnlist.append(m)\n        self._extract(fnlist, path, pwd)", "response": "Extract all files into current directory."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning True if headers are encrypted", "response": "def has_header_encryption(self):\n        \"\"\"Returns True if headers are encrypted\n        \"\"\"\n        if self._hdrenc_main:\n            return True\n        if self._main:\n            if self._main.flags & RAR_MAIN_PASSWORD:\n                return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning RarInfo for filename", "response": "def getinfo(self, member):\n        \"\"\"Return RarInfo for filename\n        \"\"\"\n        if isinstance(member, RarInfo):\n            fname = member.filename\n        else:\n            fname = member\n\n        # accept both ways here\n        if PATH_SEP == '/':\n            fname2 = fname.replace(\"\\\\\", \"/\")\n        else:\n            fname2 = fname.replace(\"/\", \"\\\\\")\n\n        try:\n            return self._info_map[fname]\n        except KeyError:\n            try:\n                return self._info_map[fname2]\n            except KeyError:\n                raise NoRarEntry(\"No such file: %s\" % fname)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning stream object for file data.", "response": "def open(self, inf, psw):\n        \"\"\"Return stream object for file data.\"\"\"\n\n        if inf.file_redir:\n            # cannot leave to unrar as it expects copied file to exist\n            if inf.file_redir[0] in (RAR5_XREDIR_FILE_COPY, RAR5_XREDIR_HARD_LINK):\n                inf = self.getinfo(inf.file_redir[2])\n                if not inf:\n                    raise BadRarFile('cannot find copied file')\n\n        if inf.flags & RAR_FILE_SPLIT_BEFORE:\n            raise NeedFirstVolume(\"Partial file, please start from first volume: \" + inf.filename)\n\n        # is temp write usable?\n        use_hack = 1\n        if not self._main:\n            use_hack = 0\n        elif self._main._must_disable_hack():\n            use_hack = 0\n        elif inf._must_disable_hack():\n            use_hack = 0\n        elif is_filelike(self._rarfile):\n            pass\n        elif inf.file_size > HACK_SIZE_LIMIT:\n            use_hack = 0\n        elif not USE_EXTRACT_HACK:\n            use_hack = 0\n\n        # now extract\n        if inf.compress_type == RAR_M0 and (inf.flags & RAR_FILE_PASSWORD) == 0 and inf.file_redir is None:\n            return self._open_clear(inf)\n        elif use_hack:\n            return self._open_hack(inf, psw)\n        elif is_filelike(self._rarfile):\n            return self._open_unrar_membuf(self._rarfile, inf, psw)\n        else:\n            return self._open_unrar(self._rarfile, inf, psw)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef std_byte(self):\n        try:\n            return self.std_name[self.pos]\n        except IndexError:\n            self.failed = 1\n            return ord('?')", "response": "Copy byte from 8 - bit representation."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef put(self, lo, hi):\n        self.buf.append(lo)\n        self.buf.append(hi)\n        self.pos += 1", "response": "Copy 16 - bit value to result."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndecompressing compressed UTF16 value.", "response": "def decode(self):\n        \"\"\"Decompress compressed UTF16 value.\"\"\"\n        hi = self.enc_byte()\n        flagbits = 0\n        while self.encpos < len(self.encdata):\n            if flagbits == 0:\n                flags = self.enc_byte()\n                flagbits = 8\n            flagbits -= 2\n            t = (flags >> flagbits) & 3\n            if t == 0:\n                self.put(self.enc_byte(), 0)\n            elif t == 1:\n                self.put(self.enc_byte(), hi)\n            elif t == 2:\n                self.put(self.enc_byte(), self.enc_byte())\n            else:\n                n = self.enc_byte()\n                if n & 0x80:\n                    c = self.enc_byte()\n                    for _ in range((n & 0x7f) + 2):\n                        lo = (self.std_byte() + c) & 0xFF\n                        self.put(lo, hi)\n                else:\n                    for _ in range(n + 2):\n                        self.put(self.std_byte(), 0)\n        return self.buf.decode(\"utf-16le\", \"replace\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads all or specified amount of data from archive entry.", "response": "def read(self, cnt=None):\n        \"\"\"Read all or specified amount of data from archive entry.\"\"\"\n\n        # sanitize cnt\n        if cnt is None or cnt < 0:\n            cnt = self._remain\n        elif cnt > self._remain:\n            cnt = self._remain\n        if cnt == 0:\n            return EMPTY\n\n        # actual read\n        data = self._read(cnt)\n        if data:\n            self._md_context.update(data)\n            self._remain -= len(data)\n        if len(data) != cnt:\n            raise BadRarFile(\"Failed the read enough data\")\n\n        # done?\n        if not data or self._remain == 0:\n            # self.close()\n            self._check()\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nseeks in the file.", "response": "def seek(self, ofs, whence=0):\n        \"\"\"Seek in data.\n\n        On uncompressed files, the seeking works by actual\n        seeks so it's fast.  On compresses files its slow\n        - forward seeking happends by reading ahead,\n        backwards by re-opening and decompressing from the start.\n        \"\"\"\n\n        # disable crc check when seeking\n        self._md_context = NoHashContext()\n\n        fsize = self._inf.file_size\n        cur_ofs = self.tell()\n\n        if whence == 0:     # seek from beginning of file\n            new_ofs = ofs\n        elif whence == 1:   # seek from current position\n            new_ofs = cur_ofs + ofs\n        elif whence == 2:   # seek from end of file\n            new_ofs = fsize + ofs\n        else:\n            raise ValueError('Invalid value for whence')\n\n        # sanity check\n        if new_ofs < 0:\n            new_ofs = 0\n        elif new_ofs > fsize:\n            new_ofs = fsize\n\n        # do the actual seek\n        if new_ofs >= cur_ofs:\n            self._skip(new_ofs - cur_ofs)\n        else:\n            # reopen and seek\n            self._open()\n            self._skip(new_ofs)\n        return self.tell()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading and discard data.", "response": "def _skip(self, cnt):\n        \"\"\"Read and discard data\"\"\"\n        while cnt > 0:\n            if cnt > 8192:\n                buf = self.read(8192)\n            else:\n                buf = self.read(cnt)\n            if not buf:\n                break\n            cnt -= len(buf)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _skip(self, cnt):\n\n        while cnt > 0:\n            # next vol needed?\n            if self._cur_avail == 0:\n                if not self._open_next():\n                    break\n\n            # fd is in read pos, do the read\n            if cnt > self._cur_avail:\n                cnt -= self._cur_avail\n                self._remain -= self._cur_avail\n                self._cur_avail = 0\n            else:\n                self._fd.seek(cnt, 1)\n                self._cur_avail -= cnt\n                self._remain -= cnt\n                cnt = 0", "response": "Skips through the rar files and returns the position of the next available item."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads from potentially multi - volume archive.", "response": "def _read(self, cnt):\n        \"\"\"Read from potentially multi-volume archive.\"\"\"\n\n        buf = []\n        while cnt > 0:\n            # next vol needed?\n            if self._cur_avail == 0:\n                if not self._open_next():\n                    break\n\n            # fd is in read pos, do the read\n            if cnt > self._cur_avail:\n                data = self._fd.read(self._cur_avail)\n            else:\n                data = self._fd.read(cnt)\n            if not data:\n                break\n\n            # got some data\n            cnt -= len(data)\n            self._cur_avail -= len(data)\n            buf.append(data)\n\n        if len(buf) == 1:\n            return buf[0]\n        return EMPTY.join(buf)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef readinto(self, buf):\n        got = 0\n        vbuf = memoryview(buf)\n        while got < len(buf):\n            # next vol needed?\n            if self._cur_avail == 0:\n                if not self._open_next():\n                    break\n\n            # length for next read\n            cnt = len(buf) - got\n            if cnt > self._cur_avail:\n                cnt = self._cur_avail\n\n            # read into temp view\n            res = self._fd.readinto(vbuf[got : got + cnt])\n            if not res:\n                break\n            self._md_context.update(vbuf[got : got + res])\n            self._cur_avail -= res\n            self._remain -= res\n            got += res\n        return got", "response": "Zero - copy read directly into buffer."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating the internal state of the internal state of the object.", "response": "def update(self, data):\n        \"\"\"Hash data.\n        \"\"\"\n        view = memoryview(data)\n        bs = self.block_size\n        if self._buf:\n            need = bs - len(self._buf)\n            if len(view) < need:\n                self._buf += view.tobytes()\n                return\n            self._add_block(self._buf + view[:need].tobytes())\n            view = view[need:]\n        while len(view) >= bs:\n            self._add_block(view[:bs])\n            view = view[bs:]\n        self._buf = view.tobytes()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef digest(self):\n        if self._digest is None:\n            if self._buf:\n                self._add_block(self._buf)\n                self._buf = EMPTY\n            ctx = self._blake2s(0, 1, True)\n            for t in self._thread:\n                ctx.update(t.digest())\n            self._digest = ctx.digest()\n        return self._digest", "response": "Return final digest value."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_tags_recommendation(request):\n    query = request.GET.get('query')\n    limit = settings.TAGGIT_SELECTIZE['RECOMMENDATION_LIMIT']\n\n    try:\n        cls = import_string(settings.TAGGIT_SELECTIZE_THROUGH)\n    except AttributeError:\n        cls = Tag\n\n    if query:\n        tags = cls.objects.filter(name__icontains=query).values('name')[:limit]\n    else:\n        tags = cls.objects.values()[:limit]\n\n    data = json.dumps({\n        'tags': list(tags)\n    })\n\n    return HttpResponse(data, content_type='application/json')", "response": "Taggit autocomplete ajax view."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_tags(tagstring):\n    if not tagstring:\n        return []\n\n    tagstring = force_text(tagstring)\n\n    words = []\n    buffer = []\n    # Defer splitting of non-quoted sections until we know if there are\n    # any unquoted commas.\n    to_be_split = []\n    i = iter(tagstring)\n    try:\n        while True:\n            c = six.next(i)\n            if c == '\"':\n                if buffer:\n                    to_be_split.append(''.join(buffer))\n                    buffer = []\n                c = six.next(i)\n                while c != '\"':\n                    buffer.append(c)\n                    c = six.next(i)\n                if buffer:\n                    word = ''.join(buffer).strip()\n                    if word:\n                        words.append(word)\n                    buffer = []\n            else:\n                buffer.append(c)\n    except StopIteration:\n        # If we were parsing an open quote which was never closed treat\n        # the buffer as unquoted.\n        if buffer:\n            to_be_split.append(''.join(buffer))\n    if to_be_split:\n        for chunk in to_be_split:\n            words.extend(split_strip(chunk, settings.TAGGIT_SELECTIZE['DELIMITER']))\n    words = list(set(words))\n    words.sort()\n    return words", "response": "Parses a tag string into a list of unique tag names."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngives a list of Tag instances creates a string representation of the list of Tag instances suitable for editing by the user.", "response": "def join_tags(tags):\n    \"\"\"\n    Given list of ``Tag`` instances, creates a string representation of\n    the list suitable for editing by the user, such that submitting the\n    given string representation back without changing it will give the\n    same list of tags.\n\n    Tag names which contain DELIMITER will be double quoted.\n\n    Adapted from Taggit's _edit_string_for_tags()\n\n    Ported from Jonathan Buchanan's `django-tagging\n    <http://django-tagging.googlecode.com/>`_\n    \"\"\"\n    names = []\n    delimiter = settings.TAGGIT_SELECTIZE['DELIMITER']\n    for tag in tags:\n        name = tag.name\n        if delimiter in name or ' ' in name:\n            names.append('\"%s\"' % name)\n        else:\n            names.append(name)\n    return delimiter.join(sorted(names))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_logger(name, namespace='{{project.package}}',\n               log_level=DEFAULT_LOG_LEVEL, log_dir=DEFAULT_LOG_DIR):\n    \"\"\"Build a logger that outputs to a file and to the console,\"\"\"\n\n    log_level = (os.getenv('{}_LOG_LEVEL'.format(namespace.upper())) or\n                 os.getenv('LOG_LEVEL', log_level))\n    log_dir = (os.getenv('{}_LOG_DIR'.format(namespace.upper())) or\n               os.getenv('LOG_DIR', log_dir))\n\n    logger = logging.getLogger('{}.{}'.format(namespace, name))\n    logger.setLevel(log_level)\n\n    formatter = logging.Formatter(\n        '%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\n    # Create a console stream handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(log_level)\n    console_handler.setFormatter(formatter)\n    logger.addHandler(console_handler)\n\n    try:\n        if log_dir:\n            log_path = os.path.abspath(log_dir)\n            log_filename = '{name}.{pid}.log'.format(\n                name=namespace, pid=os.getpid())\n\n            file_path = str(os.path.join(log_path, log_filename))\n\n            if not os.path.exists(log_path):\n                os.makedirs(log_path, mode=774)\n\n            # Create a file handler\n            file_handler = logging.FileHandler(file_path)\n            file_handler.setLevel(log_level)\n            file_handler.setFormatter(formatter)\n            logger.addHandler(file_handler)\n    except OSError as e:\n        logger.error('Could not create log file {file}: {error}'.format(\n            file=file_path, error=e.strerror))\n\n    return logger", "response": "Build a logger that outputs to a file and to the console."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get(cls, key, section=None, **kwargs):\n        section = section or cls._default_sect\n        if section not in cls._conf:\n            cls._load(section=section)\n\n        value = cls._conf[section].get(key)\n\n        # if not found in context read default\n        if not value and section != cls._default_sect:\n            value = cls._conf[cls._default_sect].get(key) if cls._default_sect in cls._conf else None\n\n        if value is None:\n            if 'default' in kwargs:  # behave as {}.get(x, default='fallback')\n                _def_value = kwargs['default']\n                logger.warn(\"Static configuration [{}] was not found. Using the default value [{}].\".format(key, _def_value))\n                return _def_value\n            else:\n                raise InvalidConfigException(u'Not found entry: {}'.format(key))\n\n        try:\n            value = from_json(value)  # parse value\n        except (TypeError, ValueError):\n            pass  # if not json parseable, then keep the string value\n\n        return value", "response": "Retrieves a config value from dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef keys(cls, section=None):\n        section = section or cls._default_sect\n        if section not in cls._conf:\n            cls._load(section=section)\n        return cls._conf[section].keys()", "response": "Get a list with all config keys"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get(self):\n        args = self.get_parser.parse_args()\n        cred = self.manager.get_credential(args)\n        if cred is None:\n            return abort(http_client.BAD_REQUEST,\n                         message='Unable to decrypt credential value.')\n        else:\n            return cred", "response": "Get a credential by file path"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating a credential by file path", "response": "def put(self):\n        \"\"\"Update a credential by file path\"\"\"\n        cred_payload = utils.uni_to_str(json.loads(request.get_data()))\n        return self.manager.update_credential(cred_payload)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the project version from VERSION file.", "response": "def _get_version():\n    \"\"\"Return the project version from VERSION file.\"\"\"\n\n    with open(join(dirname(__file__), '{{project.package}}/VERSION'), 'rb') as f:\n        version = f.read().decode('ascii').strip()\n    return version"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_response(self, response):\n        status = response.status_code\n        if response.ok:\n            data = response.json()\n            return HttpResponse(ok=response.ok, status=status, errors=None, data=data)\n        else:\n            try:\n                errors = response.json()\n            except ValueError:\n                errors = response.content\n            return HttpResponse(ok=response.ok, status=status, errors=errors, data=None)", "response": "Parse the response and build a list of objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_all(self, path, data=None, limit=100):\n        return ListResultSet(path=path, data=data or {}, limit=limit)", "response": "Encapsulates GET all requests"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the project version from VERSION file.", "response": "def _get_version():\n    \"\"\"Return the project version from VERSION file.\"\"\"\n    with open(os.path.join(os.path.dirname(__file__), PACKAGE_NAME, 'VERSION'), 'rb') as f:\n        version = f.read().decode('ascii').strip()\n    return version"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_data_path(cls):\n        marvin_path = os.environ.get(cls._key)\n        if not marvin_path:\n            raise InvalidConfigException('Data path not set!')\n\n        is_path_created = check_path(marvin_path, create=True)\n        if not is_path_created:\n            raise InvalidConfigException('Data path does not exist!')\n\n        return marvin_path", "response": "Read the data path from the following sources."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load_data(cls, relpath):\n        filepath = os.path.join(cls.data_path, relpath)\n        with open(filepath) as fp:\n            content = fp.read()\n\n        return content", "response": "Load data from the sources in order of priority 1. Filesystem\n\n       "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef download_file(cls, url, local_file_name=None, force=False, chunk_size=1024):\n\n        local_file_name = local_file_name if local_file_name else url.split('/')[-1]\n        filepath = os.path.join(cls.data_path, local_file_name)\n\n        if not os.path.exists(filepath) or force:\n            try:\n                headers = requests.head(url, allow_redirects=True).headers\n                length = headers.get('Content-Length')\n\n                logger.info(\"Starting download of {} file with {} bytes ...\".format(url, length))\n\n                widgets = [\n                    'Downloading file please wait...', progressbar.Percentage(),\n                    ' ', progressbar.Bar(),\n                    ' ', progressbar.ETA(),\n                    ' ', progressbar.FileTransferSpeed(),\n                ]\n                bar = progressbar.ProgressBar(widgets=widgets, max_value=int(length) + chunk_size).start()\n\n                r = requests.get(url, stream=True)\n\n                with open(filepath, 'wb') as f:\n                    total_chunk = 0\n\n                    for chunk in r.iter_content(chunk_size):\n                        if chunk:\n                            f.write(chunk)\n                            total_chunk += chunk_size\n                            bar.update(total_chunk)\n\n                bar.finish()\n\n            except:\n                if os.path.exists(filepath):\n                    os.remove(filepath)\n\n                raise\n\n        return filepath", "response": "Download a file from a given url and save it to a local file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nyield successive n - sized chunks from lst.", "response": "def chunks(lst, size):\n    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n    for i in xrange(0, len(lst), size):\n        yield lst[i:i + size]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _from_json_object_hook(obj):\n\n    for key, value in obj.items():\n        # Check for datetime objects\n        if isinstance(value, str):\n            dt_result = datetime_regex.match(value)\n            if dt_result:\n                year, month, day, hour, minute, second = map(\n                    lambda x: int(x), dt_result.groups())\n                obj[key] = datetime.datetime(\n                    year, month, day, hour, minute, second)\n            else:\n                dt_result = uuid_regex.match(value)\n                if dt_result:\n                    obj[key] = uuid.UUID(value)\n    return obj", "response": "Converts a json string where datetime and UUID objects were converted\n    into strings using the _to_json_default method."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if a path exists on filesystem", "response": "def check_path(path, create=False):\n    \"\"\"\n    Check for a path on filesystem\n\n    :param path: str - path name\n    :param create: bool - create if do not exist\n    :return: bool - path exists\n    \"\"\"\n    if not os.path.exists(path):\n        if create:\n            os.makedirs(path)\n            return os.path.exists(path)\n        else:\n            return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert special characters using %xx escape.", "response": "def url_encode(url):\n    \"\"\"\n    Convert special characters using %xx escape.\n\n    :param url: str\n    :return: str - encoded url\n    \"\"\"\n    if isinstance(url, text_type):\n        url = url.encode('utf8')\n    return quote(url, ':/%?&=')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget run by id", "response": "def get(self, id):\n        \"\"\"Get run by id\"\"\"\n        run = self.backend_store.get_run(id)\n        if not run:\n            return abort(http_client.NOT_FOUND,\n                         message=\"Run {} doesn't exist\".format(id))\n        return run_model.format_response(run)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndeletes a run by id", "response": "def delete(self, id):\n        \"\"\"Delete run by id\"\"\"\n        run = self.backend_store.get_run(id)\n        if not run:\n            return abort(http_client.NOT_FOUND,\n                         message=\"Run {} doesn't exist\".format(id))\n        if not self.manager.delete_run(run):\n            return abort(http_client.BAD_REQUEST,\n                         message=\"Failed to find the task queue \"\n                                 \"manager of run {}.\".format(id))\n        return '', http_client.NO_CONTENT"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget all ansible runs", "response": "def get(self):\n        \"\"\"Get run list\"\"\"\n        LOG.info('Returning all ansible runs')\n        response = []\n        for run in self.backend_store.list_runs():\n            response.append(run_model.format_response(run))\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntrigger a new run", "response": "def post(self):\n        \"\"\"Trigger a new run\"\"\"\n        run_payload = utils.uni_to_str(json.loads(request.get_data()))\n        run_payload['id'] = str(uuid.uuid4())\n        LOG.info('Triggering new ansible run %s', run_payload['id'])\n        run = self.manager.create_run(run_payload)\n        return run_model.format_response(run), http_client.CREATED"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_spark_session(enable_hive=False, app_name='marvin-engine', configs=[]):\n\n    # Prepare spark context to be used\n    import findspark\n    findspark.init()\n    from pyspark.sql import SparkSession\n\n    # prepare spark sesseion to be returned\n    spark = SparkSession.builder\n\n    spark = spark.appName(app_name)\n    spark = spark.enableHiveSupport() if enable_hive else spark\n\n    # if has configs\n    for config in configs:\n        spark = spark.config(config)\n\n    return spark.getOrCreate()", "response": "Return a Spark Session object"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the project version from VERSION file.", "response": "def get_version(path):\n    \"\"\"Return the project version from VERSION file.\"\"\"\n\n    with open(os.path.join(path, 'VERSION'), 'rb') as f:\n        version = f.read().decode('ascii').strip()\n    return version.strip()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a suffix mag string suffix and conversion factor for the given measured value in seconds.", "response": "def _choose_unit(value, unit=None, asciimode=None):\n    \"\"\"\n    Finds a good unit to print seconds in\n\n    Args:\n        value (float): measured value in seconds\n        unit (str): if specified, overrides heuristic decision\n        asciimode (bool): if True, forces ascii for microseconds\n\n    Returns:\n        tuple[(str, float)]: suffix, mag:\n            string suffix and conversion factor\n\n    Example:\n        >>> assert _choose_unit(1.1, unit=None)[0] == 's'\n        >>> assert _choose_unit(1e-2, unit=None)[0] == 'ms'\n        >>> assert _choose_unit(1e-4, unit=None, asciimode=True)[0] == 'us'\n        >>> assert _choose_unit(1.1, unit='ns')[0] == 'ns'\n    \"\"\"\n    from collections import OrderedDict\n    micro = _trychar('\u00b5s', 'us', asciimode)\n    units = OrderedDict([\n        ('s', ('s', 1e0)),\n        ('ms', ('ms', 1e-3)),\n        ('us', (micro, 1e-6)),\n        ('ns', ('ns', 1e-9)),\n    ])\n    if unit is None:\n        for suffix, mag in units.values():  # pragma: nobranch\n            if value > mag:\n                break\n    else:\n        suffix, mag = units[unit]\n    return suffix, mag"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntrying to use a character in the current language.", "response": "def _trychar(char, fallback, asciimode=None):  # nocover\n    \"\"\"\n    Logic from IPython timeit to handle terminals that cant show mu\n\n    Args:\n        char (str): character, typically unicode, to try to use\n        fallback (str): ascii character to use if stdout cannot encode char\n        asciimode (bool): if True, always use fallback\n\n    Example:\n        >>> char = _trychar('\u00b5s', 'us')\n        >>> print('char = {}'.format(char))\n        >>> assert _trychar('\u00b5s', 'us', asciimode=True) == 'us'\n\n    \"\"\"\n    if asciimode is True:\n        # If we request ascii mode simply return it\n        return fallback\n    if hasattr(sys.stdout, 'encoding') and sys.stdout.encoding:  # pragma: nobranch\n        try:\n            char.encode(sys.stdout.encoding)\n        except Exception:  # nocover\n            pass\n        else:\n            return char\n    return fallback"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nclearing all measurements allowing the object to be reused", "response": "def reset(self, label=None):\n        \"\"\"\n        clears all measurements, allowing the object to be reused\n\n        Args:\n            label (str, optional) : optionally change the label\n\n        Example:\n            >>> from timerit import Timerit\n            >>> import math\n            >>> ti = Timerit(num=10, unit='us', verbose=True)\n            >>> _ = ti.reset(label='10!').call(math.factorial, 10)\n            Timed best=...s, mean=...s for 10!\n            >>> _ = ti.reset(label='20!').call(math.factorial, 20)\n            Timed best=...s, mean=...s for 20!\n            >>> _ = ti.reset().call(math.factorial, 20)\n            Timed best=...s, mean=...s for 20!\n        \"\"\"\n        if label:\n            self.label = label\n        self.times = []\n        self.n_loops = None\n        self.total_time = None\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef call(self, func, *args, **kwargs):\n        for timer in self:\n            with timer:\n                func(*args, **kwargs)\n        return self", "response": "A simple function that calls the function with the given arguments and returns the timer that was called."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef mean(self):\n        chunk_iter = chunks(self.times, self.bestof)\n        times = list(map(min, chunk_iter))\n        mean = sum(times) / len(times)\n        return mean", "response": "Returns the mean of the best results of each trial."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef std(self):\n        import math\n        chunk_iter = chunks(self.times, self.bestof)\n        times = list(map(min, chunk_iter))\n        mean = sum(times) / len(times)\n        std = math.sqrt(sum((t - mean) ** 2 for t in times) / len(times))\n        return std", "response": "Returns the standard deviation of the best results of each trial."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _seconds_str(self):\n        mean = self.mean()\n        unit, mag = _choose_unit(mean, self.unit, self._asciimode)\n\n        unit_min = self.min() / mag\n        unit_mean = mean / mag\n\n        # Is showing the std useful? It probably doesn't hurt.\n        std = self.std()\n        unit_std = std / mag\n        pm = _trychar('\u00b1', '+-', self._asciimode)\n        fmtstr = ('best={min:.{pr1}{t}} {unit}, '\n                  'mean={mean:.{pr1}{t}} {pm} {std:.{pr2}{t}} {unit}')\n        pr1 = pr2 = self._precision\n        if isinstance(self._precision, int):  # pragma: nobranch\n            pr2 = max(self._precision - 2, 1)\n        unit_str = fmtstr.format(min=unit_min, unit=unit, mean=unit_mean,\n                                 t=self._precision_type, pm=pm, std=unit_std,\n                                 pr1=pr1, pr2=pr2)\n        return unit_str", "response": "Returns a human readable text of the current object s time."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a status line for the current state of the object.", "response": "def _status_line(self, tense='past'):\n        \"\"\"\n        Text indicating what has been / is being done.\n\n        Doctest:\n            >>> print(Timerit()._status_line(tense='past'))\n            Timed for: 1 loops, best of 1\n            >>> print(Timerit()._status_line(tense='present'))\n            Timing for: 1 loops, best of 1\n        \"\"\"\n        action = {'past': 'Timed',  'present': 'Timing'}[tense]\n        line = '{action} {label}for: {num:d} loops, best of {bestof:d}'.format(\n            label=self.label + ' ' if self.label else '',\n            action=action, num=self.num, bestof=min(self.bestof, self.num))\n        return line"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef report(self, verbose=1):\n        lines = []\n        if verbose >= 2:\n            # use a multi-line format for high verbosity\n            lines.append(self._status_line(tense='past'))\n            if verbose >= 3:\n                unit, mag = _choose_unit(self.total_time, self.unit,\n                                         self._asciimode)\n                lines.append('    body took: {total:.{pr}{t}} {unit}'.format(\n                    total=self.total_time / mag,\n                    t=self._precision_type,\n                    pr=self._precision, unit=unit))\n            lines.append('    time per loop: {}'.format(self._seconds_str()))\n        else:\n            # use a single-line format for low verbosity\n            line = 'Timed ' + self._seconds_str()\n            if self.label:\n                line += ' for ' + self.label\n            lines.append(line)\n        text = '\\n'.join(lines)\n        return text", "response": "Creates a human readable report of the current state of the object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_description():\n    from os.path import dirname, join, exists\n    readme_fpath = join(dirname(__file__), 'README.rst')\n    # This breaks on pip install, so check that it exists.\n    if exists(readme_fpath):\n        textlines = []\n        with open(readme_fpath, 'r') as f:\n            textlines = f.readlines()\n        text = ''.join(textlines).strip()\n        return text\n    return ''", "response": "Parse the description in the README file and return the text."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_requirements(fname='requirements.txt'):\n    from os.path import dirname, join, exists\n    import re\n    require_fpath = join(dirname(__file__), fname)\n    # This breaks on pip install, so check that it exists.\n    if exists(require_fpath):\n        with open(require_fpath, 'r') as f:\n            packages = []\n            for line in f.readlines():\n                line = line.strip()\n                if line and not line.startswith('#'):\n                    if line.startswith('-e '):\n                        package = line.split('#egg=')[1]\n                        packages.append(package)\n                    else:\n                        pat = '|'.join(['>', '>=', '=='])\n                        package = re.split(pat, line)[0]\n                        packages.append(package)\n            return packages\n    return []", "response": "Parse the package dependencies listed in a requirements file but strips versioning information."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nbenchmarks cyordereddict. OrderedDict against collections. OrderedDict", "response": "def benchmark(repeat=10):\n    \"\"\"Benchmark cyordereddict.OrderedDict against collections.OrderedDict\n    \"\"\"\n    columns = ['Test', 'Code', 'Ratio (stdlib / cython)']\n    res = _calculate_benchmarks(repeat)\n    try:\n        from tabulate import tabulate\n        print(tabulate(res, columns, 'rst'))\n    except ImportError:\n        print(columns)\n        print(res)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntiming execution of a Python statement or expression Usage:\\\\ %timeit [-n<N> -r<R> [-t|-c]] statement Time execution of a Python statement or expression using the timeit module. Options: -n<N>: execute the given statement <N> times in a loop. If this value is not given, a fitting value is chosen. -r<R>: repeat the loop iteration <R> times and take the best result. Default: 3 -t: use time.time to measure the time, which is the default on Unix. This function measures wall time. -c: use time.clock to measure the time, which is the default on Windows and measures wall time. On Unix, resource.getrusage is used instead and returns the CPU user time. -p<P>: use a precision of <P> digits to display the timing result. Default: 3 Examples: In [1]: %timeit pass 10000000 loops, best of 3: 53.3 ns per loop In [2]: u = None In [3]: %timeit u is None 10000000 loops, best of 3: 184 ns per loop In [4]: %timeit -r 4 u == None 1000000 loops, best of 4: 242 ns per loop In [5]: import time In [6]: %timeit -n1 time.sleep(2) 1 loops, best of 3: 2 s per loop The times reported by %timeit will be slightly higher than those reported by the timeit.py script when variables are accessed. This is due to the fact that %timeit executes the statement in the namespace of the shell, compared with timeit.py, which uses a single setup statement to import function or create variables. Generally, the bias does not matter as long as results from timeit.py are not mixed with those from %timeit.", "response": "def magic_timeit(setup, stmt, ncalls=None, repeat=3, force_ms=False):\n    \"\"\"Time execution of a Python statement or expression\n    Usage:\\\\\n      %timeit [-n<N> -r<R> [-t|-c]] statement\n    Time execution of a Python statement or expression using the timeit\n    module.\n    Options:\n    -n<N>: execute the given statement <N> times in a loop. If this value\n    is not given, a fitting value is chosen.\n    -r<R>: repeat the loop iteration <R> times and take the best result.\n    Default: 3\n    -t: use time.time to measure the time, which is the default on Unix.\n    This function measures wall time.\n    -c: use time.clock to measure the time, which is the default on\n    Windows and measures wall time. On Unix, resource.getrusage is used\n    instead and returns the CPU user time.\n    -p<P>: use a precision of <P> digits to display the timing result.\n    Default: 3\n    Examples:\n      In [1]: %timeit pass\n      10000000 loops, best of 3: 53.3 ns per loop\n      In [2]: u = None\n      In [3]: %timeit u is None\n      10000000 loops, best of 3: 184 ns per loop\n      In [4]: %timeit -r 4 u == None\n      1000000 loops, best of 4: 242 ns per loop\n      In [5]: import time\n      In [6]: %timeit -n1 time.sleep(2)\n      1 loops, best of 3: 2 s per loop\n    The times reported by %timeit will be slightly higher than those\n    reported by the timeit.py script when variables are accessed. This is\n    due to the fact that %timeit executes the statement in the namespace\n    of the shell, compared with timeit.py, which uses a single setup\n    statement to import function or create variables. Generally, the bias\n    does not matter as long as results from timeit.py are not mixed with\n    those from %timeit.\"\"\"\n\n    import timeit\n    import math\n\n    units = [\"s\", \"ms\", 'us', \"ns\"]\n    scaling = [1, 1e3, 1e6, 1e9]\n\n    timer = timeit.Timer(stmt, setup)\n\n    if ncalls is None:\n        # determine number so that 0.2 <= total time < 2.0\n        number = 1\n        for _ in range(1, 10):\n            if timer.timeit(number) >= 0.1:\n                break\n            number *= 10\n    else:\n        number = ncalls\n\n    best = min(timer.repeat(repeat, number)) / number\n\n    if force_ms:\n        order = 1\n    else:\n        if best > 0.0 and best < 1000.0:\n            order = min(-int(math.floor(math.log10(best)) // 3), 3)\n        elif best >= 1000.0:\n            order = 0\n        else:\n            order = 3\n\n    return {'loops': number,\n            'repeat': repeat,\n            'timing': best * scaling[order],\n            'units': units[order]}"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef configure_settings():\n    if not settings.configured:\n        # Determine the database settings depending on if a test_db var is set in CI mode or not\n        test_db = os.environ.get('DB', None)\n        if test_db is None:\n            db_config = {\n                'ENGINE': 'django.db.backends.postgresql_psycopg2',\n                'NAME': 'ambition',\n                'USER': 'ambition',\n                'PASSWORD': 'ambition',\n                'HOST': 'db'\n            }\n        elif test_db == 'postgres':\n            db_config = {\n                'ENGINE': 'django.db.backends.postgresql_psycopg2',\n                'USER': 'postgres',\n                'NAME': 'manager_utils',\n                }\n        elif test_db == 'sqlite':\n            db_config = {\n                'ENGINE': 'django.db.backends.sqlite3',\n                'NAME': 'manager_utils',\n                }\n        else:\n            raise RuntimeError('Unsupported test DB {0}'.format(test_db))\n\n        settings.configure(\n            TEST_RUNNER='django_nose.NoseTestSuiteRunner',\n            NOSE_ARGS=['--nocapture', '--nologcapture', '--verbosity=1'],\n            MIDDLEWARE_CLASSES={},\n            DATABASES={\n                'default': db_config,\n            },\n            INSTALLED_APPS=(\n                'django.contrib.auth',\n                'django.contrib.contenttypes',\n                'django.contrib.sessions',\n                'django.contrib.admin',\n                'manager_utils',\n                'manager_utils.tests',\n            ),\n            ROOT_URLCONF='manager_utils.urls',\n            DEBUG=False,\n        )", "response": "Configure settings for manage. py and run_tests. py."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_update_fields(model, uniques, to_update):\n    fields = {\n        field.attname: field\n        for field in model._meta.fields\n    }\n\n    if to_update is None:\n        to_update = [\n            field.attname for field in model._meta.fields\n        ]\n\n    to_update = [\n        attname for attname in to_update\n        if (attname not in uniques\n            and not getattr(fields[attname], 'auto_now_add', False)\n            and not fields[attname].auto_created)\n    ]\n\n    return to_update", "response": "Get the fields to be updated in an upsert."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _fill_auto_fields(model, values):\n    auto_field_names = [\n        f.attname\n        for f in model._meta.fields\n        if getattr(f, 'auto_now', False) or getattr(f, 'auto_now_add', False)\n    ]\n    now = timezone.now()\n    for value in values:\n        for f in auto_field_names:\n            setattr(value, f, now)\n\n    return values", "response": "Fill in auto_now and auto_now_add fields\n    Given a list of models fill in auto_now and auto_now_add fields\n    for upserts."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _sort_by_unique_fields(model, model_objs, unique_fields):\n    unique_fields = [\n        field for field in model._meta.fields\n        if field.attname in unique_fields\n    ]\n\n    def sort_key(model_obj):\n        return tuple(\n            field.get_db_prep_save(getattr(model_obj, field.attname),\n                                   connection)\n            for field in unique_fields\n        )\n    return sorted(model_objs, key=sort_key)", "response": "Sort a list of models by their unique fields."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_upsert_sql(queryset, model_objs, unique_fields, update_fields, returning,\n                    ignore_duplicate_updates=True, return_untouched=False):\n    \"\"\"\n    Generates the postgres specific sql necessary to perform an upsert (ON CONFLICT)\n    INSERT INTO table_name (field1, field2)\n    VALUES (1, 'two')\n    ON CONFLICT (unique_field) DO UPDATE SET field2 = EXCLUDED.field2;\n    \"\"\"\n    model = queryset.model\n\n    # Use all fields except pk unless the uniqueness constraint is the pk field\n    all_fields = [\n        field for field in model._meta.fields\n        if field.column != model._meta.pk.name or not field.auto_created\n    ]\n\n    all_field_names = [field.column for field in all_fields]\n    returning = returning if returning is not True else [f.column for f in model._meta.fields]\n    all_field_names_sql = ', '.join([_quote(field) for field in all_field_names])\n\n    # Convert field names to db column names\n    unique_fields = [\n        model._meta.get_field(unique_field)\n        for unique_field in unique_fields\n    ]\n    update_fields = [\n        model._meta.get_field(update_field)\n        for update_field in update_fields\n    ]\n\n    unique_field_names_sql = ', '.join([\n        _quote(field.column) for field in unique_fields\n    ])\n    update_fields_sql = ', '.join([\n        '{0} = EXCLUDED.{0}'.format(_quote(field.column))\n        for field in update_fields\n    ])\n\n    row_values, sql_args = _get_values_for_rows(model_objs, all_fields)\n\n    return_sql = 'RETURNING ' + _get_return_fields_sql(returning, return_status=True) if returning else ''\n    ignore_duplicates_sql = ''\n    if ignore_duplicate_updates:\n        ignore_duplicates_sql = (\n            ' WHERE ({update_fields_sql}) IS DISTINCT FROM ({excluded_update_fields_sql}) '\n        ).format(\n            update_fields_sql=', '.join(\n                '{0}.{1}'.format(model._meta.db_table, _quote(field.column))\n                for field in update_fields\n            ),\n            excluded_update_fields_sql=', '.join(\n                'EXCLUDED.' + _quote(field.column)\n                for field in update_fields\n            )\n        )\n\n    on_conflict = (\n        'DO UPDATE SET {0} {1}'.format(update_fields_sql, ignore_duplicates_sql) if update_fields else 'DO NOTHING'\n    )\n\n    if return_untouched:\n        row_values_sql = ', '.join([\n            '(\\'{0}\\', {1})'.format(i, row_value[1:-1])\n            for i, row_value in enumerate(row_values)\n        ])\n        sql = (\n            ' WITH input_rows(\"temp_id_\", {all_field_names_sql}) AS ('\n            '     VALUES {row_values_sql}'\n            ' ), ins AS ( '\n            '     INSERT INTO {table_name} ({all_field_names_sql})'\n            '     SELECT {all_field_names_sql} FROM input_rows ORDER BY temp_id_'\n            '     ON CONFLICT ({unique_field_names_sql}) {on_conflict} {return_sql}'\n            ' )'\n            ' SELECT DISTINCT ON ({table_pk_name}) * FROM ('\n            '     SELECT status_, {return_fields_sql}'\n            '     FROM   ins'\n            '     UNION  ALL'\n            '     SELECT \\'n\\' AS status_, {aliased_return_fields_sql}'\n            '     FROM input_rows'\n            '     JOIN {table_name} c USING ({unique_field_names_sql})'\n            ' ) as results'\n            ' ORDER BY results.\"{table_pk_name}\", CASE WHEN(status_ = \\'n\\') THEN 1 ELSE 0 END;'\n        ).format(\n            all_field_names_sql=all_field_names_sql,\n            row_values_sql=row_values_sql,\n            table_name=model._meta.db_table,\n            unique_field_names_sql=unique_field_names_sql,\n            on_conflict=on_conflict,\n            return_sql=return_sql,\n            table_pk_name=model._meta.pk.name,\n            return_fields_sql=_get_return_fields_sql(returning),\n            aliased_return_fields_sql=_get_return_fields_sql(returning, alias='c')\n        )\n    else:\n        row_values_sql = ', '.join(row_values)\n        sql = (\n            ' INSERT INTO {table_name} ({all_field_names_sql})'\n            ' VALUES {row_values_sql}'\n            ' ON CONFLICT ({unique_field_names_sql}) {on_conflict} {return_sql}'\n        ).format(\n            table_name=model._meta.db_table,\n            all_field_names_sql=all_field_names_sql,\n            row_values_sql=row_values_sql,\n            unique_field_names_sql=unique_field_names_sql,\n            on_conflict=on_conflict,\n            return_sql=return_sql\n        )\n\n    return sql, sql_args", "response": "Generates the SQL to perform an upsert on the database."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef upsert(\n    queryset, model_objs, unique_fields,\n    update_fields=None, returning=False, sync=False,\n    ignore_duplicate_updates=True,\n    return_untouched=False\n):\n    \"\"\"\n    Perform a bulk upsert on a table, optionally syncing the results.\n\n    Args:\n        queryset (Model|QuerySet): A model or a queryset that defines the collection to sync\n        model_objs (List[Model]): A list of Django models to sync. All models in this list\n            will be bulk upserted and any models not in the table (or queryset) will be deleted\n            if sync=True.\n        unique_fields (List[str]): A list of fields that define the uniqueness of the model. The\n            model must have a unique constraint on these fields\n        update_fields (List[str], default=None): A list of fields to update whenever objects\n            already exist. If an empty list is provided, it is equivalent to doing a bulk\n            insert on the objects that don't exist. If `None`, all fields will be updated.\n        returning (bool|List[str]): If True, returns all fields. If a list, only returns\n            fields in the list\n        sync (bool, default=False): Perform a sync operation on the queryset\n        ignore_duplicate_updates (bool, default=False): Don't perform an update if the row is\n            a duplicate.\n        return_untouched (bool, default=False): Return untouched rows by the operation\n    \"\"\"\n    queryset = queryset if isinstance(queryset, models.QuerySet) else queryset.objects.all()\n    model = queryset.model\n\n    # Populate automatically generated fields in the rows like date times\n    _fill_auto_fields(model, model_objs)\n\n    # Sort the rows to reduce the chances of deadlock during concurrent upserts\n    model_objs = _sort_by_unique_fields(model, model_objs, unique_fields)\n    update_fields = _get_update_fields(model, unique_fields, update_fields)\n\n    return _fetch(queryset, model_objs, unique_fields, update_fields, returning, sync,\n                  ignore_duplicate_updates=ignore_duplicate_updates,\n                  return_untouched=return_untouched)", "response": "Perform a bulk upsert on a list of objects in a tree."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_upserts_distinct(queryset, model_objs_updated, model_objs_created, unique_fields):\n\n    # Keep track of the created models\n    created_models = []\n\n    # If we created new models query for them\n    if model_objs_created:\n        created_models.extend(\n            queryset.extra(\n                where=['({unique_fields_sql}) in %s'.format(\n                    unique_fields_sql=', '.join(unique_fields)\n                )],\n                params=[\n                    tuple([\n                        tuple([\n                            getattr(model_obj, field)\n                            for field in unique_fields\n                        ])\n                        for model_obj in model_objs_created\n                    ])\n                ]\n            )\n        )\n\n    # Return the models\n    return model_objs_updated, created_models", "response": "Given a list of model objects that were updated and model objects that were created and unique_fields return the pks of the newly created models and the lists of the models that were created."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_upserts(queryset, model_objs_updated, model_objs_created, unique_fields):\n    updated, created = _get_upserts_distinct(queryset, model_objs_updated, model_objs_created, unique_fields)\n    return updated + created", "response": "Get the list of model objects that were updated and model objects that were created."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngiving a list of models to update and create return a list of models to update and create.", "response": "def _get_model_objs_to_update_and_create(model_objs, unique_fields, update_fields, extant_model_objs):\n    \"\"\"\n    Used by bulk_upsert to gather lists of models that should be updated and created.\n    \"\"\"\n\n    # Find all of the objects to update and all of the objects to create\n    model_objs_to_update, model_objs_to_create = list(), list()\n    for model_obj in model_objs:\n        extant_model_obj = extant_model_objs.get(tuple(getattr(model_obj, field) for field in unique_fields), None)\n        if extant_model_obj is None:\n            # If the object needs to be created, make a new instance of it\n            model_objs_to_create.append(model_obj)\n        else:\n            # If the object needs to be updated, update its fields\n            for field in update_fields:\n                setattr(extant_model_obj, field, getattr(model_obj, field))\n            model_objs_to_update.append(extant_model_obj)\n\n    return model_objs_to_update, model_objs_to_create"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the value of a field of a model obj that is prepared for the db.", "response": "def _get_prepped_model_field(model_obj, field):\n    \"\"\"\n    Gets the value of a field of a model obj that is prepared for the db.\n    \"\"\"\n\n    # Get the field\n    field = model_obj._meta.get_field(field)\n\n    # Get the value\n    value = field.get_db_prep_save(getattr(model_obj, field.attname), connection)\n\n    # Return the value\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef bulk_upsert(\n    queryset, model_objs, unique_fields, update_fields=None, return_upserts=False, return_upserts_distinct=False,\n    sync=False, native=False\n):\n    \"\"\"\n    Performs a bulk update or insert on a list of model objects. Matches all objects in the queryset\n    with the objs provided using the field values in unique_fields.\n    If an existing object is matched, it is updated with the values from the provided objects. Objects\n    that don't match anything are bulk created.\n    A user can provide a list update_fields so that any changed values on those fields will be updated.\n    However, if update_fields is not provided, this function reduces down to performing a bulk_create\n    on any non extant objects.\n\n    :type model_objs: list of dict\n    :param model_objs: A list of dictionaries that have fields corresponding to the model in the manager.\n\n    :type unique_fields: list of str\n    :param unique_fields: A list of fields that are used to determine if an object in objs matches a model\n            from the queryset.\n\n    :type update_fields: list of str\n    :param update_fields: A list of fields used from the objects in objs as fields when updating existing\n            models. If None, this function will only perform a bulk create for model_objs that do not\n            currently exist in the database.\n\n    :type return_upserts_distinct: bool\n    :param return_upserts_distinct: A flag specifying whether to return the upserted values as a list of distinct lists,\n            one containing the updated models and the other containing the new models. If True, this performs an\n            additional query to fetch any bulk created values.\n\n    :type return_upserts: bool\n    :param return_upserts: A flag specifying whether to return the upserted values. If True, this performs\n            an additional query to fetch any bulk created values.\n\n    :type sync: bool\n    :param sync: A flag specifying whether a sync operation should be applied to the bulk_upsert. If this\n            is True, all values in the queryset that were not updated will be deleted such that the\n            entire list of model objects is synced to the queryset.\n\n    :type native: bool\n    :param native: A flag specifying whether to use postgres insert on conflict (upsert).\n\n    :signals: Emits a post_bulk_operation when a bulk_update or a bulk_create occurs.\n\n    Examples:\n\n    .. code-block:: python\n\n        # Start off with no objects in the database. Call a bulk_upsert on the TestModel, which includes\n        # a char_field, int_field, and float_field\n        bulk_upsert(TestModel.objects.all(), [\n            TestModel(float_field=1.0, char_field='1', int_field=1),\n            TestModel(float_field=2.0, char_field='2', int_field=2),\n            TestModel(float_field=3.0, char_field='3', int_field=3),\n        ], ['int_field'], ['char_field'])\n\n        # All objects should have been created\n        print(TestModel.objects.count())\n        3\n\n        # Now perform a bulk upsert on all the char_field values. Since the objects existed previously\n        # (known by the int_field uniqueness constraint), the char fields should be updated\n        bulk_upsert(TestModel.objects.all(), [\n            TestModel(float_field=1.0, char_field='0', int_field=1),\n            TestModel(float_field=2.0, char_field='0', int_field=2),\n            TestModel(float_field=3.0, char_field='0', int_field=3),\n        ], ['int_field'], ['char_field'])\n\n        # No more new objects should have been created, and every char field should be 0\n        print(TestModel.objects.count(), TestModel.objects.filter(char_field='-1').count())\n        3, 3\n\n        # Do the exact same operation, but this time add an additional object that is not already\n        # stored. It will be created.\n        bulk_upsert(TestModel.objects.all(), [\n            TestModel(float_field=1.0, char_field='1', int_field=1),\n            TestModel(float_field=2.0, char_field='2', int_field=2),\n            TestModel(float_field=3.0, char_field='3', int_field=3),\n            TestModel(float_field=4.0, char_field='4', int_field=4),\n        ], ['int_field'], ['char_field'])\n\n        # There should be one more object\n        print(TestModel.objects.count())\n        4\n\n        # Note that one can also do the upsert on a queryset. Perform the same data upsert on a\n        # filter for int_field=1. In this case, only one object has the ability to be updated.\n        # All of the other objects will be created\n        bulk_upsert(TestModel.objects.filter(int_field=1), [\n            TestModel(float_field=1.0, char_field='1', int_field=1),\n            TestModel(float_field=2.0, char_field='2', int_field=2),\n            TestModel(float_field=3.0, char_field='3', int_field=3),\n            TestModel(float_field=4.0, char_field='4', int_field=4),\n        ], ['int_field'], ['char_field'])\n\n        # There should be three more objects\n        print(TestModel.objects.count())\n        7\n\n    \"\"\"\n    if not unique_fields:\n        raise ValueError('Must provide unique_fields argument')\n    update_fields = update_fields or []\n\n    if native:\n        if return_upserts_distinct:\n            raise NotImplementedError('return upserts distinct not supported with native postgres upsert')\n        return_value = Query().from_table(table=queryset.model).upsert(\n            model_objs, unique_fields, update_fields, return_models=return_upserts or sync\n        ) or []\n        if sync:\n            orig_ids = frozenset(queryset.values_list('pk', flat=True))\n            queryset.filter(pk__in=orig_ids - frozenset([m.pk for m in return_value])).delete()\n\n        post_bulk_operation.send(sender=queryset.model, model=queryset.model)\n\n        return return_value\n\n    # Create a look up table for all of the objects in the queryset keyed on the unique_fields\n    extant_model_objs = {\n        tuple(getattr(extant_model_obj, field) for field in unique_fields): extant_model_obj\n        for extant_model_obj in queryset\n    }\n\n    # Find all of the objects to update and all of the objects to create\n    model_objs_to_update, model_objs_to_create = _get_model_objs_to_update_and_create(\n        model_objs, unique_fields, update_fields, extant_model_objs)\n\n    # Find all objects in the queryset that will not be updated. These will be deleted if the sync option is\n    # True\n    if sync:\n        model_objs_to_update_set = frozenset(model_objs_to_update)\n        model_objs_to_delete = [\n            model_obj.pk for model_obj in extant_model_objs.values() if model_obj not in model_objs_to_update_set\n        ]\n        if model_objs_to_delete:\n            queryset.filter(pk__in=model_objs_to_delete).delete()\n\n    # Apply bulk updates and creates\n    if update_fields:\n        bulk_update(queryset, model_objs_to_update, update_fields)\n    queryset.bulk_create(model_objs_to_create)\n\n    # Optionally return the bulk upserted values\n    if return_upserts_distinct:\n        # return a list of lists, the first being the updated models, the second being the newly created objects\n        return _get_upserts_distinct(queryset, model_objs_to_update, model_objs_to_create, unique_fields)\n    if return_upserts:\n        return _get_upserts(queryset, model_objs_to_update, model_objs_to_create, unique_fields)", "response": "Bulk update or insert on a list of model objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef bulk_upsert2(\n    queryset, model_objs, unique_fields, update_fields=None, returning=False,\n    ignore_duplicate_updates=True, return_untouched=False\n):\n    \"\"\"\n    Performs a bulk update or insert on a list of model objects. Matches all objects in the queryset\n    with the objs provided using the field values in unique_fields.\n    If an existing object is matched, it is updated with the values from the provided objects. Objects\n    that don't match anything are bulk created.\n    A user can provide a list update_fields so that any changed values on those fields will be updated.\n    However, if update_fields is not provided, this function reduces down to performing a bulk_create\n    on any non extant objects.\n\n    Args:\n        queryset (Model|QuerySet): A model or a queryset that defines the collection to sync\n        model_objs (List[Model]): A list of Django models to sync. All models in this list\n            will be bulk upserted and any models not in the table (or queryset) will be deleted\n            if sync=True.\n        unique_fields (List[str]): A list of fields that define the uniqueness of the model. The\n            model must have a unique constraint on these fields\n        update_fields (List[str], default=None): A list of fields to update whenever objects\n            already exist. If an empty list is provided, it is equivalent to doing a bulk\n            insert on the objects that don't exist. If ``None``, all fields will be updated.\n        returning (bool|List[str]): If ``True``, returns all fields. If a list, only returns\n            fields in the list. Return values are split in a tuple of created and updated models\n        ignore_duplicate_updates (bool, default=False): Ignore updating a row in the upsert if all of the update fields\n            are duplicates\n        return_untouched (bool, default=False): Return values that were not touched by the upsert operation\n\n    Returns:\n        UpsertResult: A list of results if ``returning`` is not ``False``. created, updated, and untouched,\n            results can be obtained by accessing the ``created``, ``updated``, and ``untouched`` properties\n            of the result.\n\n    Examples:\n\n    .. code-block:: python\n\n        # Start off with no objects in the database. Call a bulk_upsert on the TestModel, which includes\n        # a char_field, int_field, and float_field\n        bulk_upsert2(TestModel.objects.all(), [\n            TestModel(float_field=1.0, char_field='1', int_field=1),\n            TestModel(float_field=2.0, char_field='2', int_field=2),\n            TestModel(float_field=3.0, char_field='3', int_field=3),\n        ], ['int_field'], ['char_field'])\n\n        # All objects should have been created\n        print(TestModel.objects.count())\n        3\n\n        # Now perform a bulk upsert on all the char_field values. Since the objects existed previously\n        # (known by the int_field uniqueness constraint), the char fields should be updated\n        bulk_upsert2(TestModel.objects.all(), [\n            TestModel(float_field=1.0, char_field='0', int_field=1),\n            TestModel(float_field=2.0, char_field='0', int_field=2),\n            TestModel(float_field=3.0, char_field='0', int_field=3),\n        ], ['int_field'], ['char_field'])\n\n        # No more new objects should have been created, and every char field should be 0\n        print(TestModel.objects.count(), TestModel.objects.filter(char_field='-1').count())\n        3, 3\n\n        # Do the exact same operation, but this time add an additional object that is not already\n        # stored. It will be created.\n        bulk_upsert2(TestModel.objects.all(), [\n            TestModel(float_field=1.0, char_field='1', int_field=1),\n            TestModel(float_field=2.0, char_field='2', int_field=2),\n            TestModel(float_field=3.0, char_field='3', int_field=3),\n            TestModel(float_field=4.0, char_field='4', int_field=4),\n        ], ['int_field'], ['char_field'])\n\n        # There should be one more object\n        print(TestModel.objects.count())\n        4\n\n        # Note that one can also do the upsert on a queryset. Perform the same data upsert on a\n        # filter for int_field=1. In this case, only one object has the ability to be updated.\n        # All of the other objects will be created\n        bulk_upsert2(TestModel.objects.filter(int_field=1), [\n            TestModel(float_field=1.0, char_field='1', int_field=1),\n            TestModel(float_field=2.0, char_field='2', int_field=2),\n            TestModel(float_field=3.0, char_field='3', int_field=3),\n            TestModel(float_field=4.0, char_field='4', int_field=4),\n        ], ['int_field'], ['char_field'])\n\n        # There should be three more objects\n        print(TestModel.objects.count())\n        7\n\n        # Return creates and updates on the same set of models\n        created, updated = bulk_upsert2(TestModel.objects.filter(int_field=1), [\n            TestModel(float_field=1.0, char_field='1', int_field=1),\n            TestModel(float_field=2.0, char_field='2', int_field=2),\n            TestModel(float_field=3.0, char_field='3', int_field=3),\n            TestModel(float_field=4.0, char_field='4', int_field=4),\n        ], ['int_field'], ['char_field'])\n\n        # All four objects should be updated\n        print(len(updated))\n        4\n    \"\"\"\n    results = upsert2.upsert(queryset, model_objs, unique_fields,\n                             update_fields=update_fields, returning=returning,\n                             ignore_duplicate_updates=ignore_duplicate_updates,\n                             return_untouched=return_untouched)\n    post_bulk_operation.send(sender=queryset.model, model=queryset.model)\n    return results", "response": "Bulk upserts a list of objects in the base base"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sync(queryset, model_objs, unique_fields, update_fields=None, **kwargs):\n    return bulk_upsert(queryset, model_objs, unique_fields, update_fields=update_fields, sync=True, **kwargs)", "response": "Syncs a list of model_objs with the contents of the queryset."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nperforming a sync operation on a queryset, making the contents of the queryset match the contents of model_objs. Note: The definition of a sync requires that we return untouched rows from the upsert opertion. There is no way to turn off returning untouched rows in a sync. Args: queryset (Model|QuerySet): A model or a queryset that defines the collection to sync model_objs (List[Model]): A list of Django models to sync. All models in this list will be bulk upserted and any models not in the table (or queryset) will be deleted if sync=True. unique_fields (List[str]): A list of fields that define the uniqueness of the model. The model must have a unique constraint on these fields update_fields (List[str], default=None): A list of fields to update whenever objects already exist. If an empty list is provided, it is equivalent to doing a bulk insert on the objects that don't exist. If `None`, all fields will be updated. returning (bool|List[str]): If True, returns all fields. If a list, only returns fields in the list. Return values are split in a tuple of created, updated, and deleted models. ignore_duplicate_updates (bool, default=False): Ignore updating a row in the upsert if all of the update fields are duplicates Returns: UpsertResult: A list of results if ``returning`` is not ``False``. created, updated, untouched, and deleted results can be obtained by accessing the ``created``, ``updated``, ``untouched``, and ``deleted`` properties of the result.", "response": "def sync2(queryset, model_objs, unique_fields, update_fields=None, returning=False, ignore_duplicate_updates=True):\n    \"\"\"\n    Performs a sync operation on a queryset, making the contents of the\n    queryset match the contents of model_objs.\n\n    Note: The definition of a sync requires that we return untouched rows from the upsert opertion. There is\n    no way to turn off returning untouched rows in a sync.\n\n    Args:\n        queryset (Model|QuerySet): A model or a queryset that defines the collection to sync\n        model_objs (List[Model]): A list of Django models to sync. All models in this list\n            will be bulk upserted and any models not in the table (or queryset) will be deleted\n            if sync=True.\n        unique_fields (List[str]): A list of fields that define the uniqueness of the model. The\n            model must have a unique constraint on these fields\n        update_fields (List[str], default=None): A list of fields to update whenever objects\n            already exist. If an empty list is provided, it is equivalent to doing a bulk\n            insert on the objects that don't exist. If `None`, all fields will be updated.\n        returning (bool|List[str]): If True, returns all fields. If a list, only returns\n            fields in the list. Return values are split in a tuple of created, updated, and\n            deleted models.\n        ignore_duplicate_updates (bool, default=False): Ignore updating a row in the upsert if all\n            of the update fields are duplicates\n\n    Returns:\n        UpsertResult: A list of results if ``returning`` is not ``False``. created, updated, untouched,\n            and deleted results can be obtained by accessing the ``created``, ``updated``, ``untouched``,\n            and ``deleted`` properties of the result.\n    \"\"\"\n    results = upsert2.upsert(queryset, model_objs, unique_fields,\n                             update_fields=update_fields, returning=returning, sync=True,\n                             ignore_duplicate_updates=ignore_duplicate_updates)\n    post_bulk_operation.send(sender=queryset.model, model=queryset.model)\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_or_none(queryset, **query_params):\n    try:\n        obj = queryset.get(**query_params)\n    except queryset.model.DoesNotExist:\n        obj = None\n    return obj", "response": "Get an object or return None if it doesn t exist."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nbulks updates a list of models that have been saved.", "response": "def bulk_update(manager, model_objs, fields_to_update):\n    \"\"\"\n    Bulk updates a list of model objects that are already saved.\n\n    :type model_objs: list of :class:`Models<django:django.db.models.Model>`\n    :param model_objs: A list of model objects that have been updated.\n        fields_to_update: A list of fields to be updated. Only these fields will be updated\n\n\n    :signals: Emits a post_bulk_operation signal when completed.\n\n    Examples:\n\n    .. code-block:: python\n\n        # Create a couple test models\n        model_obj1 = TestModel.objects.create(int_field=1, float_field=2.0, char_field='Hi')\n        model_obj2 = TestModel.objects.create(int_field=3, float_field=4.0, char_field='Hello')\n\n        # Change their fields and do a bulk update\n        model_obj1.int_field = 10\n        model_obj1.float_field = 20.0\n        model_obj2.int_field = 30\n        model_obj2.float_field = 40.0\n        bulk_update(TestModel.objects, [model_obj1, model_obj2], ['int_field', 'float_field'])\n\n        # Reload the models and view their changes\n        model_obj1 = TestModel.objects.get(id=model_obj1.id)\n        print(model_obj1.int_field, model_obj1.float_field)\n        10, 20.0\n\n        model_obj2 = TestModel.objects.get(id=model_obj2.id)\n        print(model_obj2.int_field, model_obj2.float_field)\n        10, 20.0\n\n    \"\"\"\n\n    # Add the pk to the value fields so we can join\n    value_fields = [manager.model._meta.pk.attname] + fields_to_update\n\n    # Build the row values\n    row_values = [\n        [_get_prepped_model_field(model_obj, field_name) for field_name in value_fields]\n        for model_obj in model_objs\n    ]\n\n    # If we do not have any values or fields to update just return\n    if len(row_values) == 0 or len(fields_to_update) == 0:\n        return\n\n    # Create a map of db types\n    db_types = [\n        manager.model._meta.get_field(field).db_type(connection)\n        for field in value_fields\n    ]\n\n    # Build the value fields sql\n    value_fields_sql = ', '.join(\n        '\"{field}\"'.format(field=manager.model._meta.get_field(field).column)\n        for field in value_fields\n    )\n\n    # Build the set sql\n    update_fields_sql = ', '.join([\n        '\"{field}\" = \"new_values\".\"{field}\"'.format(\n            field=manager.model._meta.get_field(field).column\n        )\n        for field in fields_to_update\n    ])\n\n    # Build the values sql\n    values_sql = ', '.join([\n        '({0})'.format(\n            ', '.join([\n                '%s::{0}'.format(\n                    db_types[i]\n                ) if not row_number and i else '%s'\n                for i, _ in enumerate(row)\n            ])\n        )\n        for row_number, row in enumerate(row_values)\n    ])\n\n    # Start building the query\n    update_sql = (\n        'UPDATE {table} '\n        'SET {update_fields_sql} '\n        'FROM (VALUES {values_sql}) AS new_values ({value_fields_sql}) '\n        'WHERE \"{table}\".\"{pk_field}\" = \"new_values\".\"{pk_field}\"'\n    ).format(\n        table=manager.model._meta.db_table,\n        pk_field=manager.model._meta.pk.column,\n        update_fields_sql=update_fields_sql,\n        values_sql=values_sql,\n        value_fields_sql=value_fields_sql\n    )\n\n    # Combine all the row values\n    update_sql_params = list(itertools.chain(*row_values))\n\n    # Run the update query\n    with connection.cursor() as cursor:\n        cursor.execute(update_sql, update_sql_params)\n\n    # call the bulk operation signal\n    post_bulk_operation.send(sender=manager.model, model=manager.model)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nperforms an upsert on a single object or an insert if the object does not exist.", "response": "def upsert(manager, defaults=None, updates=None, **kwargs):\n    \"\"\"\n    Performs an update on an object or an insert if the object does not exist.\n\n    :type defaults: dict\n    :param defaults: These values are set when the object is created, but are irrelevant\n            when the object already exists. This field should only be used when values only need to\n            be set during creation.\n\n    :type updates: dict\n    :param updates: These values are updated when the object is updated. They also override any\n            values provided in the defaults when inserting the object.\n\n    :param kwargs: These values provide the arguments used when checking for the existence of\n            the object. They are used in a similar manner to Django's get_or_create function.\n\n    :returns: A tuple of the upserted object and a Boolean that is True if it was created (False otherwise)\n\n    Examples:\n\n    .. code-block:: python\n\n        # Upsert a test model with an int value of 1. Use default values that will be given to it when created\n        model_obj, created = upsert(TestModel.objects, int_field=1, defaults={'float_field': 2.0})\n        print(created)\n        True\n        print(model_obj.int_field, model_obj.float_field)\n        1, 2.0\n\n        # Do an upsert on that same model with different default fields. Since it already exists, the defaults\n        # are not used\n        model_obj, created = upsert(TestModel.objects, int_field=1, defaults={'float_field': 3.0})\n        print(created)\n        False\n        print(model_obj.int_field, model_obj.float_field)\n        1, 2.0\n\n        # In order to update the float field in an existing object, use the updates dictionary\n        model_obj, created = upsert(TestModel.objects, int_field=1, updates={'float_field': 3.0})\n        print(created)\n        False\n        print(model_obj.int_field, model_obj.float_field)\n        1, 3.0\n\n        # You can use updates on a newly created object that will also be used as initial values.\n        model_obj, created = upsert(TestModel.objects, int_field=2, updates={'float_field': 4.0})\n        print(created)\n        True\n        print(model_obj.int_field, model_obj.float_field)\n        2, 4.0\n\n    \"\"\"\n    defaults = defaults or {}\n    # Override any defaults with updates\n    defaults.update(updates or {})\n\n    # Do a get or create\n    obj, created = manager.get_or_create(defaults=defaults, **kwargs)\n\n    # Update any necessary fields\n    if updates is not None and not created and any(getattr(obj, k) != updates[k] for k in updates):\n        for k, v in updates.items():\n            setattr(obj, k, v)\n        obj.save(update_fields=updates)\n\n    return obj, created"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\noverride Django s bulk_create function to emit a post_bulk_operation signal when bulk_create is finished.", "response": "def bulk_create(self, *args, **kwargs):\n        \"\"\"\n        Overrides Django's bulk_create function to emit a post_bulk_operation signal when bulk_create\n        is finished.\n        \"\"\"\n        ret_val = super(ManagerUtilsQuerySet, self).bulk_create(*args, **kwargs)\n        post_bulk_operation.send(sender=self.model, model=self.model)\n        return ret_val"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\noverride Django s update method to emit a post_bulk_operation signal when it completes.", "response": "def update(self, **kwargs):\n        \"\"\"\n        Overrides Django's update method to emit a post_bulk_operation signal when it completes.\n        \"\"\"\n        ret_val = super(ManagerUtilsQuerySet, self).update(**kwargs)\n        post_bulk_operation.send(sender=self.model, model=self.model)\n        return ret_val"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef process_request(self, request):\n        token = request.GET.get(TOKEN_NAME)\n        user = None if token is None else authenticate(url_auth_token=token)\n\n        # If the sessions framework is enabled and the token is valid,\n        # persist the login in session.\n        if hasattr(request, 'session') and user is not None:\n            login(request, user)\n            # Once we persist the login in the session, if the authentication\n            # middleware is enabled, it will set request.user in future\n            # requests. We can get rid of the token in the URL by redirecting\n            # to the same URL with the token removed. We only do this for GET\n            # requests because redirecting POST requests doesn't work well. We\n            # don't do this on Safari because it triggers the over-zealous\n            # \"Protection Against First Party Bounce Trackers\" of ITP 2.0.\n            if (\n                hasattr(request, 'user') and\n                request.method == 'GET' and\n                not self.is_safari(request)\n            ):\n                return self.get_redirect(request)\n\n        # If the authentication middleware isn't enabled, set request.user.\n        # (This attribute is overwritten by the authentication middleware\n        # if it runs after this one.)\n        if not hasattr(request, 'user'):\n            request.user = user if user is not None else AnonymousUser()", "response": "Process the request and return a HTTP redirect response that removes the token from the URL if the user is not logged in."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_redirect(request):\n        params = request.GET.copy()\n        params.pop(TOKEN_NAME)\n        url = request.path\n        if params:\n            url += '?' + urlencode(params)\n        return redirect(url)", "response": "Create a HTTP redirect response that removes the token from the URL."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate an URL - safe signed token from data.", "response": "def sign(self, data):\n        \"\"\"\n        Create an URL-safe, signed token from ``data``.\n\n        \"\"\"\n        data = signing.b64_encode(data).decode()\n        return self.signer.sign(data)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nextracting the data from a signed token.", "response": "def unsign(self, token):\n        \"\"\"\n        Extract the data from a signed ``token``.\n\n        \"\"\"\n        if self.max_age is None:\n            data = self.signer.unsign(token)\n        else:\n            data = self.signer.unsign(token, max_age=self.max_age)\n        return signing.b64_decode(data.encode())"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_revocation_key(self, user):\n        value = ''\n        if self.invalidate_on_password_change:\n            value += user.password\n        if self.one_time:\n            value += str(user.last_login)\n        return value", "response": "Returns the key that is used to revocate the user s password and last login."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_token(self, user):\n        # The password is expected to be a secure hash but we hash it again\n        # for additional safety. We default to MD5 to minimize the length of\n        # the token. (Remember, if an attacker obtains the URL, he can already\n        # log in. This isn't high security.)\n        h = crypto.pbkdf2(\n            self.get_revocation_key(user),\n            self.salt,\n            self.iterations,\n            digest=self.digest,\n        )\n        return self.sign(self.packer.pack_pk(user.pk) + h)", "response": "Create a signed token from a user."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse a signed token and return a user object.", "response": "def parse_token(self, token):\n        \"\"\"\n        Obtain a user from a signed token.\n\n        \"\"\"\n        try:\n            data = self.unsign(token)\n        except signing.SignatureExpired:\n            logger.debug(\"Expired token: %s\", token)\n            return\n        except signing.BadSignature:\n            logger.debug(\"Bad token: %s\", token)\n            return\n        except Exception:\n            logger.exception(\n                \"Valid signature but unexpected token - if you changed \"\n                \"django-sesame settings, you must regenerate tokens\")\n            return\n        user_pk, data = self.packer.unpack_pk(data)\n        user = self.get_user(user_pk)\n        if user is None:\n            logger.debug(\"Unknown token: %s\", token)\n            return\n        h = crypto.pbkdf2(\n            self.get_revocation_key(user),\n            self.salt,\n            self.iterations,\n            digest=self.digest,\n        )\n        if not crypto.constant_time_compare(data, h):\n            logger.debug(\"Invalid token: %s\", token)\n            return\n        logger.debug(\"Valid token for user %s: %s\", user, token)\n        return user"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks the token and return the corresponding user.", "response": "def authenticate(self, request, url_auth_token=None):\n        \"\"\"\n        Check the token and return the corresponding user.\n\n        \"\"\"\n        try:\n            return self.parse_token(url_auth_token)\n        except TypeError:\n            backend = \"%s.%s\" % (self.__module__, self.__class__.__name__)\n            logger.exception(\"TypeError in %s, here's the traceback before \"\n                             \"Django swallows it:\", backend)\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninterpreting the output of the frog parser.", "response": "def parse_frog(lines):\n    \"\"\"\n    Interpret the output of the frog parser.\n    Input should be an iterable of lines (i.e. the output of call_frog)\n    Result is a sequence of dicts representing the tokens\n    \"\"\"\n    sid = 0\n    for i, line in enumerate(lines):\n        if not line:\n            # end of sentence marker\n            sid += 1\n        else:\n            parts = line.split(\"\\t\")\n            tid, token, lemma, morph, pos, conf, ne, _, parent, rel = parts\n            if rel:\n                rel = (rel, int(parent) - 1)\n            word = u' '.join(token.split(u'_'))\n            result = dict(id=i, sentence=sid, word=word, lemma=lemma, pos=pos,\n                          pos_confidence=float(conf), rel=rel)\n            if ne != 'O':\n                # NER label from BIO tags\n                result[\"ne\"] = ne.split('_', 1)[0][2:]\n            yield result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a pos1 element to a frog token.", "response": "def _add_pos1(token):\n    \"\"\"\n    Adds a 'pos1' element to a frog token.\n    \"\"\"\n    result = token.copy()\n    result['pos1'] = _POSMAP[token['pos'].split(\"(\")[0]]\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert frog tokens into a new SAF document", "response": "def frog_to_saf(tokens):\n    \"\"\"\n    Convert frog tokens into a new SAF document\n    \"\"\"\n    tokens = [_add_pos1(token) for token in tokens]\n    module = {'module': \"frog\",\n              \"started\": datetime.datetime.now().isoformat()}\n    return {\"header\": {'format': \"SAF\",\n                       'format-version': \"0.0\",\n                       'processed': [module]\n                       }, \"tokens\": tokens}"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef copy_cwl_files(from_dir=CWL_PATH, to_dir=None):\n    cwl_files = glob.glob('{}{}*.cwl'.format(from_dir, os.sep))\n    # if no files are found, the output directory should not be created\n    if len(cwl_files) > 0:\n        create_dirs(to_dir)\n    for fi in cwl_files:\n        fo = os.path.join(to_dir, os.path.basename(fi))\n        shutil.copy2(fi, fo)\n\n    return len(cwl_files)", "response": "Copy cwl files to a directory where the cwl - runner can find them."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef main(to_dir, from_dir):\n    num = copy_cwl_files(from_dir=from_dir, to_dir=to_dir)\n    if num > 0:\n        click.echo('Copied {} CWL files to \"{}\".'.format(num, to_dir))\n    else:\n        msg = 'No CWL files found in \"{}\". Copied 0 files'.format(from_dir)\n        click.echo(msg)", "response": "Copy CWL files to a new directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns sorted value of This if list or dict.", "response": "def find(self, datum):\n        \"\"\"Return sorted value of This if list or dict.\"\"\"\n        if isinstance(datum.value, dict) and self.expressions:\n            return datum\n\n        if isinstance(datum.value, dict) or isinstance(datum.value, list):\n            key = (functools.cmp_to_key(self._compare)\n                   if self.expressions else None)\n            return [jsonpath_rw.DatumInContext.wrap(\n                [value for value in sorted(datum.value, key=key)])]\n        return datum"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsaving the current object to file fname.", "response": "def save(self, fname, mode=None, validate=True, wd=False, inline=False,\n             relative=True, pack=False, encoding='utf-8'):\n        \"\"\"Save workflow to file\n\n        For nlppln, the default is to save workflows with relative paths.\n        \"\"\"\n        super(WorkflowGenerator, self).save(fname,\n                                            mode=mode,\n                                            validate=validate,\n                                            wd=wd,\n                                            inline=inline,\n                                            relative=relative,\n                                            pack=pack,\n                                            encoding=encoding)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef match(pattern, data, **parse_kwargs):\n    return [m.value for m in parse(pattern, **parse_kwargs).find(data)]", "response": "Returns all values of pattern in data"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef match1(pattern, data, **parse_kwargs):\n    matches = match(pattern, data, **parse_kwargs)\n    return matches[0] if matches else None", "response": "Returns first value of pattern in data or None if no matches"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef p_expression(self, p):\n        if len(p) == 2:\n            left, op, right = p[1], None, None\n        else:\n            __, left, op, right = p\n        p[0] = _filter.Expression(left, op, right)", "response": "expression - > expression"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_chunked_list(in_dir, size, out_dir, out_name):\n    create_dirs(out_dir)\n\n    in_files = get_files(in_dir)\n    chunks = chunk(in_files, size)\n\n    division = {}\n\n    for i, files in enumerate(chunks):\n        division[i] = [os.path.basename(f) for f in files]\n\n    out_file = os.path.join(out_dir, out_name)\n    with codecs.open(out_file, 'wb', encoding='utf-8') as f:\n        json.dump(division, f, indent=4)", "response": "Create a division of the input files in chunks."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remove_ext(fname):\n    bn = os.path.basename(fname)\n    return os.path.splitext(bn)[0]", "response": "Removes the extension from a filename returning the filename"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating ( output ) directories if they don t exist.", "response": "def create_dirs(fname, is_file=False):\n    \"\"\"Create (output) directories if they don't exist\n\n    Removes the file name part if is_file is set to True.\n    \"\"\"\n    fname = os.path.abspath(fname)\n    if is_file:\n        fname = os.path.dirname(fname)\n\n    if not os.path.exists(fname):\n        os.makedirs(fname)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef out_file_name(out_dir, fname, ext=None):\n    if ext is None:\n        return os.path.join(out_dir, os.path.basename(fname))\n\n    fname = remove_ext(fname)\n    return os.path.join(out_dir, '{}.{}'.format(fname, ext))", "response": "Return path of output file given a directory file name and extension."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_files(directory, recursive=False):\n    files_out = []\n    if recursive:\n        for root, dirs, files in os.walk(os.path.abspath(directory)):\n            files = [os.path.join(root, f) for f in files]\n            files_out.append(files)\n        files_out = list(itertools.chain(*files_out))\n    else:\n        files_out = [os.path.join(directory, f) for f in os.listdir(directory)]\n        files_out = list(filter(lambda f: os.path.isfile(f), files_out))\n\n    # order alphabetically on file name\n    return sorted(files_out)", "response": "Return a list of all files in the directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef run(self):\n        with open('src/lsqfit/__init__.py', 'a') as lsfile:\n            lsfile.write(\"\\n__version__ = '%s'\\n\" % LSQFIT_VERSION)\n        _build_py.run(self)", "response": "Append version number to lsqfit. py"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _reformat(p, buf):\n    if numpy.ndim(buf) != 1:\n        raise ValueError(\"Buffer ``buf`` must be 1-d.\")\n    if hasattr(p, 'keys'):\n        ans = _gvar.BufferDict(p)\n        if ans.size != len(buf):\n            raise ValueError(       #\n                \"p, buf size mismatch: %d, %d\"%(ans.size, len(buf)))\n        ans = _gvar.BufferDict(ans, buf=buf)\n    else:\n        if numpy.size(p) != len(buf):\n            raise ValueError(       #\n                \"p, buf size mismatch: %d, %d\"%(numpy.size(p), len(buf)))\n        ans = numpy.array(buf).reshape(numpy.shape(p))\n    return ans", "response": "Apply format of p to data in 1 - d array buf."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nunpacking data and prior into x y prior fdata.", "response": "def _unpack_data(data, prior, svdcut, uncorrelated_data, add_svdnoise):\n    \"\"\" Unpack data and prior into ``(x, y, prior, fdata)``.\n\n    This routine unpacks ``data`` and ``prior`` into ``x, y, prior, fdata``\n    where ``x`` is the independent data, ``y`` is the fit data,\n    ``prior`` is the collection of priors for the fit, and ``fdata``\n    contains the information about the data and prior needed for the\n    fit function. Both ``y`` and ``prior`` are modified to account\n    for SVD cuts if ``svdcut>0``.\n\n    Allowed layouts for ``data`` are: ``x, y, ycov``, ``x, y, ysdev``,\n    ``x, y``, and ``y``. In the last two case, ``y`` can be either an array\n    of |GVar|\\s or a dictionary whose values are |GVar|\\s or arrays of\n    |GVar|\\s. In the last case it is assumed that the fit function is a\n    function of only the parameters: ``fcn(p)`` --- no ``x``. (This is also\n    assumed if ``x = False``.)\n\n    Output data in ``fdata`` is: ``fdata.mean`` containing the mean values of\n    ``y.flat`` and ``prior.flat`` (if there is a prior);\n    ``fdata.svdcorrection``  containing the sum of the SVD corrections to\n    ``y.flat`` and ``prior.flat``; ``fdata.logdet`` containing  the logarithm\n    of the  determinant of the covariance matrix of ``y.flat`` and\n    ``prior.flat``; and ``fdata.inv_wgts`` containing a representation of the\n    inverse of the covariance matrix, after SVD cuts (see :func:`gvar.svd`\n    for a description of the format).\n    \"\"\"\n    # unpack data tuple\n    if not isinstance(data, tuple):\n        x = False                   # no x in fit fcn\n        y = _unpack_gvars(data)\n    elif len(data) == 3:\n        x, ym, ycov = data\n        ym = numpy.asarray(ym)\n        ycov = numpy.asarray(ycov)\n        y = _gvar.gvar(ym, ycov)\n    elif len(data) == 2:\n        x, y = data\n        y = _unpack_gvars(y)\n    else:\n        raise ValueError(\"data tuple wrong length: \"+str(len(data)))\n\n    # clean up\n    if prior is not None:\n        prior = _unpack_gvars(prior)\n\n    def _apply_svd(data, svdcut=svdcut):\n        ans, inv_wgts = _gvar.svd(\n            data, wgts=-1, svdcut=svdcut, add_svdnoise=add_svdnoise,\n            )\n        fdata = _FDATA(\n            mean=_gvar.mean(ans.flat),\n            inv_wgts=inv_wgts,\n            svdcorrection=numpy.sum(ans.svdcorrection.flat),\n            logdet=ans.logdet,\n            nblocks=ans.nblocks,\n            svdn=ans.nmod,\n            )\n        del ans.nblocks\n        # del ans.svdcorrection\n        return ans, fdata\n\n    if uncorrelated_data:\n        ysdev = _gvar.sdev(y.flat)\n        if prior is None:\n            pfdata = _FDATA(\n                mean=[], inv_wgts=[([],[])], svdcorrection=_gvar.gvar(0,0),\n                logdet=0.0, nblocks={1:0}, svdn=0,\n                )\n        else:\n            prior, pfdata = _apply_svd(prior)\n        inv_wgts = [(numpy.arange(y.size, dtype=numpy.intp), 1. / ysdev)]\n        i, wgt = pfdata.inv_wgts[0]\n        if len(i) > 0:\n            inv_wgts = [(\n                numpy.concatenate((inv_wgts[0][0], i + y.size)),\n                numpy.concatenate((inv_wgts[0][1], wgt))\n                )]\n        for i, wgt in pfdata.inv_wgts[1:]:\n            inv_wgts.append((\n                i + y.size, wgt\n                ))\n        pfdata.nblocks[1] = pfdata.nblocks.get(1, 0) + y.size\n        fdata = _FDATA(\n            mean=numpy.concatenate((_gvar.mean(y.flat), pfdata.mean)),\n            inv_wgts=inv_wgts,\n            svdcorrection=pfdata.svdcorrection,\n            logdet=2 * numpy.sum(numpy.log(ysdev)) + pfdata.logdet,\n            nblocks=pfdata.nblocks,\n            svdn=pfdata.svdn,\n            )\n    elif prior is None:\n        y, fdata = _apply_svd(y)\n    else:\n        yp, fdata = _apply_svd(numpy.concatenate((y.flat, prior.flat)))\n        y.flat = yp[:y.size]\n        prior.flat = yp[y.size:]\n    return x, y, prior, fdata"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _unpack_gvars(g):\n    if g is not None:\n        g = _gvar.gvar(g)\n        if not hasattr(g, 'flat'):\n            # must be a scalar (ie, not an array and not a dictionary)\n            g = numpy.asarray(g)\n    return g", "response": "Unpack collection of GVars to BufferDict or numpy array."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nunpacks a single p0 from a file.", "response": "def _unpack_p0(p0, p0file, prior):\n    \"\"\" Create proper p0.\n\n    Try to read from a file. If that doesn't work, try using p0,\n    and then, finally, the prior. If the p0 is from the file, it is\n    checked against the prior to make sure that all elements have the\n    right shape; if not the p0 elements are adjusted (using info from\n    the prior) to be the correct shape. If p0 is a dictionary,\n    keys in p0 that are not in prior are discarded. If p0 is True,\n    then a random p0 is generated from the prior.\n    \"\"\"\n    if p0file is not None:\n        # p0 is a filename; read in values\n        try:\n            with open(p0file, \"rb\") as f:\n                p0 = pickle.load(f)\n        except (IOError, EOFError):\n            if prior is None:\n                raise IOError(\n                    \"No prior and can't read parameters from \" + p0file\n                    )\n            else:\n                p0 = None\n    if p0 is not None:\n        # repackage as BufferDict or numpy array\n        if p0 is True:\n            p0 = next(_gvar.raniter(prior))\n        if hasattr(p0, 'keys'):\n            p0 = _gvar.BufferDict(p0)\n            if p0.dtype != float:\n                p0.buf = numpy.asarray(p0.buf, dtype=float)\n        else:\n            p0 = numpy.array(p0, float)\n    if prior is not None:\n        # build new p0 from p0, plus the prior as needed\n        pp = _reformat(prior, buf=[x.mean if x.mean != 0.0\n                        else x.mean + 0.1 * x.sdev for x in prior.flat])\n        if p0 is None:\n            p0 = pp\n        else:\n            if pp.shape is not None:\n                # pp and p0 are arrays\n                pp_shape = pp.shape\n                p0_shape = p0.shape\n                if len(pp_shape)!=len(p0_shape):\n                    raise ValueError(       #\n                        \"p0 and prior shapes incompatible: %s, %s\"\n                        % (str(p0_shape), str(pp_shape)))\n                idx = []\n                for npp, np0 in zip(pp_shape, p0_shape):\n                    idx.append(slice(0, min(npp, np0)))\n                idx = tuple(idx)    # overlapping slices in each dir\n                pp[idx] = p0[idx]\n                p0 = pp\n            else:\n                # pp and p0 are dicts\n                # if set(pp.keys()) != set(p0.keys()):\n                #     # mismatch in keys between prior and p0\n                #     raise ValueError(\"Key mismatch between prior and p0: \"\n                #                      + ' '.join(str(k) for k in\n                #                      set(prior.keys()) ^ set(p0.keys())))\n                # adjust p0[k] to be compatible with shape of prior[k]\n                for k in pp:\n                    if k not in p0:\n                        continue\n                    pp_shape = numpy.shape(pp[k])\n                    p0_shape = numpy.shape(p0[k])\n                    if len(pp_shape)!=len(p0_shape):\n                        raise ValueError(\"p0 and prior incompatible: \"\n                                         +str(k))\n                    if pp_shape == p0_shape:\n                        pp[k] = p0[k]\n                    else:\n                        # find overlap between p0 and pp\n                        pp_shape = pp[k].shape\n                        p0_shape = p0[k].shape\n                        if len(pp_shape)!=len(p0_shape):\n                            raise ValueError(       #\n                                \"p0 and prior incompatible: \"+str(k))\n                        idx = []\n                        for npp, np0 in zip(pp_shape, p0_shape):\n                            idx.append(slice(0, min(npp, np0)))\n                        idx = tuple(idx)    # overlapping slices in each dir\n                        pp[k][idx] = p0[k][idx]\n                p0 = pp\n    if p0 is None:\n        raise ValueError(\"No starting values for parameters\")\n    return p0"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreconfiguring fitting fcn so inputs = flat arrays ; hide x", "response": "def _unpack_fcn(fcn, p0, y, x):\n    \"\"\" reconfigure fitting fcn so inputs, outputs = flat arrays; hide x \"\"\"\n    if y.shape is not None:\n        if p0.shape is not None:\n            def nfcn(p, x=x, fcn=fcn, pshape=p0.shape):\n                po = p.reshape(pshape)\n                ans = fcn(po) if x is False else fcn(x, po)\n                if hasattr(ans, 'flat'):\n                    return ans.flat\n                else:\n                    return numpy.array(ans).flat\n        else:\n            po = _gvar.BufferDict(p0, buf=numpy.zeros(p0.size, float))\n            def nfcn(p, x=x, fcn=fcn, po=po):\n                po.buf = p\n                ans = fcn(po) if x is False else fcn(x, po)\n                if hasattr(ans, 'flat'):\n                    return ans.flat\n                else:\n                    return numpy.array(ans).flat\n    else:\n        yo = _gvar.BufferDict(y, buf=y.size*[None])\n        if p0.shape is not None:\n            def nfcn(p, x=x, fcn=fcn, pshape=p0.shape, yo=yo):\n                po = p.reshape(pshape)\n                fxp = fcn(po) if x is False else fcn(x, po)\n                for k in yo:\n                    yo[k] = fxp[k]\n                return yo.flat\n        else:\n            po = _gvar.BufferDict(p0, buf=numpy.zeros(p0.size, float))\n            def nfcn(p, x=x, fcn=fcn, po=po, yo=yo):\n                po.buf = p\n                fxp = fcn(po) if x is False else fcn(x, po)\n                for k in yo:\n                    yo[k] = fxp[k]\n                return yo.flat\n    return nfcn"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set(clear=False, **defaults):\n        old_defaults = dict(nonlinear_fit.DEFAULTS)\n        if clear:\n            nonlinear_fit.DEFAULTS = {}\n        for k in defaults:\n            if k == 'fitter' and defaults[k] not in nonlinear_fit.FITTERS:\n                raise ValueError('unknown fitter: ' + str(defaults[k]))\n            nonlinear_fit.DEFAULTS[k] = defaults[k]\n        return old_defaults", "response": "Set default parameters for lsqfit. nonlinear_fit."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check_roundoff(self, rtol=0.25, atol=1e-6):\n        psdev = _gvar.sdev(self.p.flat)\n        paltsdev = _gvar.sdev(self.palt.flat)\n        if not numpy.allclose(psdev, paltsdev, rtol=rtol, atol=atol):\n            warnings.warn(\"Possible roundoff errors in fit.p; try svd cut.\")", "response": "Checks for roundoff errors in fit. p."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nbuilding a list of GVar objects for best fit parameters.", "response": "def _getp(self):\n        \"\"\" Build :class:`gvar.GVar`\\s for best-fit parameters. \"\"\"\n        if self._p is not None:\n            return self._p\n        # buf = [y,prior]; D[a,i] = dp[a]/dbuf[i]\n        pmean = _gvar.mean(self.palt).flat\n        buf = (\n            self.y.flat if self.prior is None else\n            numpy.concatenate((self.y.flat, self.prior.flat))\n            )\n        D = numpy.zeros((self.cov.shape[0], len(buf)), float)\n        for i, chivw_i in enumerate(self._chivw(_gvar.valder(pmean))):\n            for a in range(D.shape[0]):\n                D[a, i] = chivw_i.dotder(self.cov[a])\n\n        # p[a].mean=pmean[a]; p[a].der[j] = sum_i D[a,i]*buf[i].der[j]\n        p = []\n        for a in range(D.shape[0]): # der[a] = sum_i D[a,i]*buf[i].der\n            p.append(_gvar.gvar(pmean[a], _gvar.wsum_der(D[a], buf),\n                     buf[0].cov))\n        self._p = _reformat(self.p0, p)\n        return self._p"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef plot_residuals(self, plot=None):\n        if plot is None:\n            import matplotlib.pyplot as plot\n        x = numpy.arange(1, len(self.residuals) + 1)\n        y = _gvar.mean(self.residuals)\n        yerr = _gvar.sdev(self.residuals)\n        plot.errorbar(x=x, y=y, yerr=yerr, fmt='o', color='b')\n        plot.ylabel('normalized residuals')\n        xr = [x[0], x[-1]]\n        plot.plot([x[0], x[-1]], [0, 0], 'r-')\n        plot.fill_between(\n            x=xr, y1=[-1,-1], y2=[1,1], color='r', alpha=0.075\n            )\n        return plot", "response": "Plot normalized fit residuals."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nformats fit output details into a string for printing.", "response": "def format(self, maxline=0, pstyle='v', nline=None, extend=True):\n        \"\"\" Formats fit output details into a string for printing.\n\n        The output tabulates the ``chi**2`` per degree of freedom of the fit\n        (``chi2/dof``), the number of degrees of freedom, the ``Q``  value of\n        the fit (ie, p-value), and the logarithm of the Gaussian Bayes Factor\n        for the fit (``logGBF``). At the end it lists the SVD cut, the number\n        of eigenmodes modified by the SVD cut, the tolerances used in the fit,\n        and the time in seconds needed to do the fit. The tolerance used to\n        terminate the fit is marked with an asterisk. It also lists\n        information about the fitter used if it is other than the standard\n        choice.\n\n        Optionally, ``format`` will also list the best-fit values\n        for the fit parameters together with the prior for each (in ``[]`` on\n        each line). Lines for parameters that deviate from their prior by more\n        than one (prior) standard deviation are marked with asterisks, with\n        the number of asterisks equal to the number of standard deviations (up\n        to five). Lines for parameters designated as linear (see ``linear``\n        keyword) are marked with a minus sign after their prior.\n\n        ``format`` can also list all of the data and the corresponding values\n        from the fit, again with asterisks on lines  where there is a\n        significant discrepancy.\n\n        Args:\n            maxline (int or bool): Maximum number of data points for which\n                fit results and input data are tabulated. ``maxline<0``\n                implies that only ``chi2``, ``Q``, ``logGBF``, and ``itns``\n                are tabulated; no parameter values are included. Setting\n                ``maxline=True`` prints all data points; setting it\n                equal ``None`` or ``False`` is the same as setting\n                it equal to ``-1``. Default is ``maxline=0``.\n            pstyle (str or None): Style used for parameter list. Supported\n                values are 'vv' for very verbose, 'v' for verbose, and 'm' for\n                minimal. When 'm' is set, only parameters whose values differ\n                from their prior values are listed. Setting ``pstyle=None``\n                implies no parameters are listed.\n            extend (bool): If ``True``, extend the parameter list to\n                include values derived from log-normal or other\n                non-Gaussian parameters. So values for fit parameter\n                ``p['log(a)']``, for example, are listed together with\n                values ``p['a']`` for the exponential of the fit parameter.\n                Setting ``extend=False`` means that only the value\n                for ``p['log(a)']`` is listed. Default is ``True``.\n\n        Returns:\n            String containing detailed information about fit.\n        \"\"\"\n        # unpack arguments\n        if nline is not None and maxline == 0:\n            maxline = nline         # for legacy code (old name)\n        if maxline is True:\n            # print all data\n            maxline = sys.maxsize\n        if maxline is False or maxline is None:\n            maxline = -1\n        if pstyle is not None:\n            if pstyle[:2] == 'vv':\n                pstyle = 'vv'\n            elif pstyle[:1] == 'v':\n                pstyle = 'v'\n            elif pstyle[:1] == 'm':\n                pstyle = 'm'\n            else:\n                raise ValueError(\"Invalid pstyle: \"+str(pstyle))\n\n        def collect(v1, v2, style='v', stride=1, extend=False):\n            \"\"\" Collect data from v1 and v2 into table.\n\n            Returns list of [label,v1fmt,v2fmt]s for each entry in v1 and\n            v2. Here v1fmt and v2fmt are strings representing entries in v1\n            and v2, while label is assembled from the key/index of the\n            entry.\n            \"\"\"\n            def nstar(v1, v2):\n                sdev = max(v1.sdev, v2.sdev)\n                nstar = int(abs(v1.mean - v2.mean) / sdev)\n                if nstar > 5:\n                    nstar = 5\n                elif nstar < 1:\n                    nstar = 0\n                return '  ' + nstar * '*'\n            ct = 0\n            ans = []\n            width = [0,0,0]\n            stars = []\n            if v1.shape is None:\n                # BufferDict\n                keys = list(v1.keys())\n                if extend:\n                    v1 = _gvar.BufferDict(v1)\n                    v2 = _gvar.BufferDict(v2)\n                    ekeys = v1.extension_keys()\n                    if len(ekeys) > 0:\n                        first_ekey = ekeys[0]\n                        keys += ekeys\n                    else:\n                        extend = False\n                for k in keys:\n                    if extend and k == first_ekey:\n                        # marker indicating beginning of extra keys\n                        stars.append(None)\n                        ans.append(None)\n                    ktag = str(k)\n                    if numpy.shape(v1[k]) == ():\n                        if ct%stride != 0:\n                            ct += 1\n                            continue\n                        if style in ['v','m']:\n                            v1fmt = v1[k].fmt(sep=' ')\n                            v2fmt = v2[k].fmt(sep=' ')\n                        else:\n                            v1fmt = v1[k].fmt(-1)\n                            v2fmt = v2[k].fmt(-1)\n                        if style == 'm' and v1fmt == v2fmt:\n                            ct += 1\n                            continue\n                        stars.append(nstar(v1[k], v2[k]))\n                        ans.append([ktag, v1fmt, v2fmt])\n                        w = [len(ai) for ai in ans[-1]]\n                        for i, (wo, wn) in enumerate(zip(width, w)):\n                            if wn > wo:\n                                width[i] = wn\n                        ct += 1\n                    else:\n                        ktag = ktag + \" \"\n                        for i in numpy.ndindex(v1[k].shape):\n                            if ct%stride != 0:\n                                ct += 1\n                                continue\n                            ifmt = (len(i)*\"%d,\")[:-1] % i\n                            if style in ['v','m']:\n                                v1fmt = v1[k][i].fmt(sep=' ')\n                                v2fmt = v2[k][i].fmt(sep=' ')\n                            else:\n                                v1fmt = v1[k][i].fmt(-1)\n                                v2fmt = v2[k][i].fmt(-1)\n                            if style == 'm' and v1fmt == v2fmt:\n                                ct += 1\n                                continue\n                            stars.append(nstar(v1[k][i], v2[k][i]))\n                            ans.append([ktag+ifmt, v1fmt, v2fmt])\n                            w = [len(ai) for ai in ans[-1]]\n                            for i, (wo, wn) in enumerate(zip(width, w)):\n                                if wn > wo:\n                                    width[i] = wn\n                            ct += 1\n                            ktag = \"\"\n            else:\n                # numpy array\n                v2 = numpy.asarray(v2)\n                for k in numpy.ndindex(v1.shape):\n                    # convert array(GVar) to GVar\n                    v1k = v1[k] if hasattr(v1[k], 'fmt') else v1[k].flat[0]\n                    v2k = v2[k] if hasattr(v2[k], 'fmt') else v2[k].flat[0]\n                    if ct%stride != 0:\n                        ct += 1\n                        continue\n                    kfmt = (len(k) * \"%d,\")[:-1] % k\n                    if style in ['v','m']:\n                        v1fmt = v1k.fmt(sep=' ')\n                        v2fmt = v2k.fmt(sep=' ')\n                    else:\n                        v1fmt = v1k.fmt(-1)\n                        v2fmt = v2k.fmt(-1)\n                    if style == 'm' and v1fmt == v2fmt:\n                        ct += 1\n                        continue\n                    stars.append(nstar(v1k, v2k)) ###\n                    ans.append([kfmt, v1fmt, v2fmt])\n                    w = [len(ai) for ai in ans[-1]]\n                    for i, (wo, wn) in enumerate(zip(width, w)):\n                        if wn > wo:\n                            width[i] = wn\n                    ct += 1\n\n            collect.width = width\n            collect.stars = stars\n            return ans\n\n        # build header\n        dof = self.dof\n        if dof > 0:\n            chi2_dof = self.chi2/self.dof\n        else:\n            chi2_dof = self.chi2\n        try:\n            Q = 'Q = %.2g' % self.Q\n        except:\n            Q = ''\n        try:\n            logGBF = 'logGBF = %.5g' % self.logGBF\n        except:\n            logGBF = ''\n        if self.prior is None:\n            descr = ' (no prior)'\n        else:\n            descr = ''\n        table = ('Least Square Fit%s:\\n  chi2/dof [dof] = %.2g [%d]    %s'\n                 '    %s\\n' % (descr, chi2_dof, dof, Q, logGBF))\n        if maxline < 0:\n            return table\n\n        # create parameter table\n        if pstyle is not None:\n            table = table + '\\nParameters:\\n'\n            prior = self.prior\n            if prior is None:\n                if self.p0.shape is None:\n                    prior = _gvar.BufferDict(\n                        self.p0, buf=self.p0.flatten() + _gvar.gvar(0,float('inf')))\n                else:\n                    prior = self.p0 + _gvar.gvar(0,float('inf'))\n            data = collect(self.palt, prior, style=pstyle, stride=1, extend=extend)\n            w1, w2, w3 = collect.width\n            fst = \"%%%ds%s%%%ds%s[ %%%ds ]\" % (\n                max(w1, 15), 3 * ' ',\n                max(w2, 10), int(max(w2,10)/2) * ' ', max(w3,10)\n                )\n            if len(self.linear) > 0:\n                spacer = [' ', '-']\n            else:\n                spacer = ['', '']\n            for i, (di, stars) in enumerate(zip(data, collect.stars)):\n                if di is None:\n                    # marker for boundary between true fit parameters and derived parameters\n                    ndashes = (\n                        max(w1, 15) + 3 + max(w2, 10) + int(max(w2, 10)/2)\n                        + 4 + max(w3, 10)\n                        )\n                    table += ndashes * '-' + '\\n'\n                    continue\n                table += (\n                    (fst % tuple(di)) +\n                    spacer[i in self.linear] +\n                    stars + '\\n'\n                    )\n\n        # settings\n        settings = \"\\nSettings:\"\n        if not self.add_svdnoise or self.svdcut is None or self.svdcut < 0:\n            settings += \"\\n  svdcut/n = {svdcut:.2g}/{svdn}\".format(\n                svdcut=self.svdcut if self.svdcut is not None else 0.0,\n                svdn=self.svdn\n                )\n        else:\n            settings += \"\\n  svdcut/n = {svdcut:.2g}/{svdn}*\".format(\n                    svdcut=self.svdcut, svdn=self.svdn\n                    )\n        criterion = self.stopping_criterion\n        try:\n            fmtstr = [\n                \"    tol = ({:.2g},{:.2g},{:.2g})\",\n                \"    tol = ({:.2g}*,{:.2g},{:.2g})\",\n                \"    tol = ({:.2g},{:.2g}*,{:.2g})\",\n                \"    tol = ({:.2g},{:.2g},{:.2g}*)\",\n                ][criterion if criterion is not None else 0]\n            settings += fmtstr.format(*self.tol)\n        except:\n            pass\n        if criterion is not None and criterion == 0:\n            settings +=\"    (itns/time = {itns}*/{time:.1f})\".format(\n                itns=self.nit, time=self.time\n                )\n        else:\n            settings +=\"    (itns/time = {itns}/{time:.1f})\".format(\n                itns=self.nit, time=self.time\n                )\n        default_line = '\\n  fitter = gsl_multifit    methods = lm/more/qr\\n'\n        newline = \"\\n  fitter = {}    {}\\n\".format(\n            self.fitter, self.description\n            )\n        if newline != default_line:\n            settings += newline\n        else:\n            settings += '\\n'\n        if maxline <= 0 or self.data is None:\n            return table + settings\n        # create table comparing fit results to data\n        ny = self.y.size\n        stride = 1 if maxline >= ny else (int(ny/maxline) + 1)\n        if hasattr(self, 'fcn_p'):\n            f = self.fcn_p\n        elif self.x is False:\n            f = self.fcn(self.p)\n        else:\n            f = self.fcn(self.x, self.p)\n        if hasattr(f, 'keys'):\n            f = _gvar.BufferDict(f)\n        else:\n            f = numpy.array(f)\n        data = collect(self.y, f, style='v', stride=stride, extend=False)\n        w1,w2,w3 = collect.width\n        clabels = (\"key\",\"y[key]\",\"f(p)[key]\")\n        if self.y.shape is not None and self.x is not False and self.x is not None:\n            # use x[k] to label lines in table?\n            try:\n                x = numpy.array(self.x)\n                xlist = []\n                ct = 0\n                for k in numpy.ndindex(x.shape):\n                    if ct%stride != 0:\n                        ct += 1\n                        continue\n                    xlist.append(\"%g\" % x[k])\n                assert len(xlist) == len(data)\n            except:\n                xlist = None\n            if xlist is not None:\n                for i,(d1,d2,d3) in enumerate(data):\n                    data[i] = (xlist[i],d2,d3)\n                clabels = (\"x[k]\",\"y[k]\",\"f(x[k],p)\")\n\n        w1,w2,w3 = max(9,w1+4), max(9,w2+4), max(9,w3+4)\n        table += \"\\nFit:\\n\"\n        fst = \"%%%ds%%%ds%%%ds\\n\" % (w1, w2, w3)\n        table += fst % clabels\n        table += (w1 + w2 + w3) * \"-\" + \"\\n\"\n        for di, stars in zip(data, collect.stars):\n            table += fst[:-1] % tuple(di) + stars + '\\n'\n\n        return table + settings"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads parameters stored in file filename.", "response": "def load_parameters(filename):\n        \"\"\" Load parameters stored in file ``filename``.\n\n        ``p = nonlinear_fit.load_p(filename)`` is used to recover the\n        values of fit parameters dumped using ``fit.dump_p(filename)`` (or\n        ``fit.dump_pmean(filename)``) where ``fit`` is of type\n        :class:`lsqfit.nonlinear_fit`. The layout of the returned\n        parameters ``p`` is the same as that of ``fit.p`` (or\n        ``fit.pmean``).\n        \"\"\"\n        warnings.warn(\n            \"nonlinear_fit.load_parameters deprecated; use pickle.load or gvar.load instead\",\n            DeprecationWarning,\n            )\n        with open(filename,\"rb\") as f:\n            return pickle.load(f)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndumping the best - fit parameter values into a file.", "response": "def dump_p(self, filename):\n        \"\"\" Dump parameter values (``fit.p``) into file ``filename``.\n\n        ``fit.dump_p(filename)`` saves the best-fit parameter values\n        (``fit.p``) from a ``nonlinear_fit`` called ``fit``. These values\n        are recovered using\n        ``p = nonlinear_fit.load_parameters(filename)``\n        where ``p``'s layout is the same as that of ``fit.p``.\n        \"\"\"\n        warnings.warn(\n            \"nonlinear_fit.dump_p deprecated; use gvar.dump instead\",\n            DeprecationWarning\n            )\n        with open(filename, \"wb\") as f:\n            pickle.dump(self.palt, f)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dump_pmean(self, filename):\n        warnings.warn(\n            \"nonlinear_fit.dump_pmean deprecated; use pickle.dump instead\",\n            DeprecationWarning,\n            )\n        with open(filename, \"wb\") as f:\n            if self.p0.shape is not None:\n                pickle.dump(numpy.array(self.pmean), f)\n            else:\n                pickle.dump(collections.OrderedDict(self.pmean), f)", "response": "Dump the parameter means of the best - fit to a file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef simulated_data_iter(\n        self, n=None, pexact=None, add_priornoise=False, bootstrap=None\n        ):\n        \"\"\" Iterator that returns simulated data based upon a fit's data.\n\n        Simulated data is generated from a fit's data ``fit.y`` by\n        replacing the mean values in that data with random numbers\n        drawn from a distribution whose mean is ``self.fcn(pexact)``\n        and whose covariance matrix is the same as that of ``self.y``.\n        Each iteration of the iterator returns new simulated data,\n        with different random numbers for the means and a covariance\n        matrix equal to that of ``self.y``. This iterator is used by\n        ``self.simulated_fit_iter``.\n\n        Typical usage::\n\n            fit = nonlinear_fit(data=(x,y), prior=prior, fcn=fcn)\n            ...\n            for ysim, priorsim in fit.simulate_data_iter(n=10):\n                fitsim = nonlinear_fit(data=(x, ysim), prior=priorsim, fcn=fcn)\n                print(fitsim)\n                print('chi2 =', gv.chi2(fit.p, fitsim.p))\n\n        This code tests the fitting protocol on simulated data, comparing the\n        best fit parameters in each case with the correct values (``fit.p``).\n        The loop in this code is functionally the same as (but probably not\n        as fast as)::\n\n            for fitsim in fit.simulated_fit_iter(n=10):\n                print(fitsim)\n                print('chi2 =', gv.chi2(fit.p, fitsim.p))\n\n        Args:\n            n (int or None): Maximum number of iterations (equals\n                infinity if ``None``).\n\n            pexact (None or dict/array of numbers): Fit-parameter values for\n                the underlying distribution used to generate simulated data;\n                replaced by ``self.pmean`` if is ``None`` (default).\n\n            add_priornoise (bool): Vary prior means if ``True``; otherwise\n                vary only the means in ``self.y`` (default).\n\n        Returns:\n            An iterator that returns a 2-tuple containing simulated\n            versions of self.y and self.prior: ``(ysim, priorsim)``.\n        \"\"\"\n        pexact = self.pmean if pexact is None else pexact\n        # bootstrap is old name for add_priornoise; keep for legacy code\n        if bootstrap is not None:\n            add_priornoise = bootstrap\n        f = self.fcn(pexact) if self.x is False else self.fcn(self.x, pexact)\n        y = copy.deepcopy(self.y)\n        if isinstance(y, _gvar.BufferDict):\n            # y,f dictionaries; fresh copy of y, reorder f\n            tmp_f = _gvar.BufferDict([(k, f[k]) for k in y])\n            y.buf += tmp_f.buf - _gvar.mean(y.buf)\n        else:\n            # y,f arrays; fresh copy of y\n            y += numpy.asarray(f) - _gvar.mean(y)\n        prior = copy.deepcopy(self.prior)\n        if prior is None or not add_priornoise:\n            yiter = _gvar.bootstrap_iter(y, n)\n            for ysim in _gvar.bootstrap_iter(y, n):\n                yield ysim, prior\n        else:\n            yp = numpy.empty(y.size + prior.size, object)\n            yp[:y.size] = y.flat\n            yp[y.size:] = prior.flat\n            for ypsim in _gvar.bootstrap_iter(yp, n):\n                y.flat = ypsim[:y.size]\n                prior.flat = ypsim[y.size:]\n                yield y, prior", "response": "This function returns an iterator that returns simulated data based upon a fit s data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef bootstrapped_fit_iter(self, n=None, datalist=None, **kargs):\n        fargs = dict(fitter=self.fitter, fcn=self.fcn)\n        fargs.update(self.fitterargs)\n        fargs['p0'] = self.pmean\n        fargs['prior'] = self.prior\n        for k in kargs:\n            fargs[k] = kargs[k]\n        prior = fargs['prior']\n        del fargs['prior']\n        if datalist is None:\n            x = self.x\n            y = self.y\n            if prior is None:\n                for yb in _gvar.bootstrap_iter(y, n):\n                    fit = nonlinear_fit(data=(x, yb), prior=None, **fargs)\n                    yield fit\n            else:\n                g = _gvar.BufferDict(y=y.flat, prior=prior.flat)\n                for gb in _gvar.bootstrap_iter(g, n):\n                    yb = _reformat(y, buf=gb['y'])\n                    priorb = _reformat(prior, buf=gb['prior'])\n                    fit = nonlinear_fit(data=(x, yb), prior=priorb, **fargs)\n                    yield fit\n        else:\n            if prior is None:\n                i = 0\n                for datab in datalist:\n                    i += 1\n                    if n is not None and i > n:\n                        break\n                    fit = nonlinear_fit(data=datab, prior=None, **fargs)\n                    yield fit\n            else:\n                piter = _gvar.bootstrap_iter(prior)\n                i = 0\n                for datab in datalist:\n                    i += 1\n                    if n is not None and i > n:\n                        break\n                    fit = nonlinear_fit(data=datab, prior=next(piter), **fargs)\n                    yield fit", "response": "This method returns an iterator that returns bootstrap copies of a fit."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef logpdf(self, p, lognorm=None):\n        if lognorm is None:\n            lognorm = self.log_norm\n        if hasattr(p, 'keys'):\n            if isinstance(p, _gvar.BufferDict):\n                p = p.buf\n            else:\n                p = _gvar.BufferDict(p).buf\n        else:\n            p = numpy.asarray(p).reshape(-1)\n        return (\n            -0.5 * (numpy.sum(self.chiv(p) ** 2) - self.chi2)\n            - self.log_gnorm - self.log_norm\n            )", "response": "Logarithm of the probability density function evaluated at p."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef empbayes_fit(z0, fitargs, **minargs):\n    save = dict(lastz=None, lastp0=None)\n    if hasattr(z0, 'keys'):\n        # z is a dictionary\n        if not isinstance(z0, gvar.BufferDict):\n            z0 = gvar.BufferDict(z0)\n        z0buf = z0.buf\n        def convert(zbuf):\n            return gvar.BufferDict(z0, buf=zbuf)\n    elif numpy.shape(z0) == ():\n        # z is a number\n        z0buf = numpy.array([z0])\n        def convert(zbuf):\n            return zbuf[0]\n    else:\n        # z is an array\n        z0 = numpy.asarray(z0)\n        z0buf = z0\n        def convert(zbuf):\n            return zbuf\n    def minfcn(zbuf, save=save, convert=convert):\n        z = convert(zbuf)\n        args = fitargs(z)\n        if not hasattr(args, 'keys'):\n            args, plausibility = args\n        else:\n            plausibility = 0.0\n        if save['lastp0'] is not None:\n            args['p0'] = save['lastp0']\n        fit = lsqfit.nonlinear_fit(**args)\n        if numpy.isnan(fit.logGBF):\n            raise ValueError\n        else:\n            save['lastz'] = z\n            save['lastp0'] = fit.pmean\n        return -fit.logGBF - plausibility\n    try:\n        z = convert(_multiminex(z0buf, minfcn, **minargs).x)\n    except ValueError:\n        print('*** empbayes_fit warning: null logGBF')\n        z = save['lastz']\n    args = fitargs(z)\n    if not hasattr(args, 'keys'):\n        args, plausibility = args\n    if save['lastp0'] is not None:\n        args['p0'] = save['lastp0']\n    return lsqfit.nonlinear_fit(**args), z", "response": "This function calculates the fit and the data corresponding to the fit\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef wavg(datalist, fast=False, prior=None, **fitterargs):\n    if prior is not None:\n        datalist = list(datalist) + [prior]\n        warnings.warn(\n            'use of prior in lsqfit.wavg is deprecated',\n            DeprecationWarning\n            )\n    if len(datalist) <= 0:\n        return None\n    elif len(datalist) == 1:\n        if hasattr(datalist[0], 'keys'):\n            return BufferDictWAvg(datalist[0], None)\n        if numpy.shape(datalist[0]) == ():\n            return GVarWAvg(datalist[0], None)\n        else:\n            return ArrayWAvg(numpy.asarray(datalist[0]), None)\n    if fast:\n        chi2 = dof = time = svdcorrection = 0\n        ans = datalist[0]\n        for i, di in enumerate(datalist[1:]):\n            ans = wavg([ans, di], fast=False, **fitterargs)\n            chi2 += ans.chi2\n            dof += ans.dof\n            time += ans.time\n            svdcorrection += ans.svdcorrection\n        ans.fit.dof = dof\n        ans.fit.Q = _gammaQ(dof / 2., chi2 / 2.)\n        ans.fit.chi2 = chi2\n        ans.fit.time = time\n        ans.fit.svdcorrection = svdcorrection\n        return ans\n    if hasattr(datalist[0], 'keys'):\n        datashape = None\n    else:\n        datashape = numpy.shape(datalist[0])\n        datalist = [{None:di} for di in datalist]\n    # repack as a single dictionary\n    p0shape = {}\n    p0index = {}\n    data = gvar.BufferDict()\n    for i, di in enumerate(datalist):\n        for k in di:\n            data[k, i] = di[k]\n            shape = numpy.shape(di[k])\n            p0index[k, i] = tuple(slice(0, j) for j in shape)\n            if k not in p0shape:\n                p0shape[k] = shape\n            elif p0shape[k] != shape:\n                p0shape[k] = tuple(\n                    max(j1, j2) for j1, j2 in zip(shape, p0shape[k])\n                    )\n    # calculate p0\n    p0 = gvar.BufferDict()\n    p0count = {}\n    for k, i in data:\n        if k not in p0:\n            p0[k] = numpy.zeros(p0shape[k], float)\n            p0count[k] = numpy.zeros(p0shape[k], float)\n        if p0index[k, i] == ():\n            p0[k] += data[k, i].mean\n            p0count[k] += 1\n        else:\n            p0[k][p0index[k, i]] += gvar.mean(data[k, i])\n            p0count[k][p0index[k, i]] += 1.\n    for k in p0:\n        p0[k] /= p0count[k]\n    # set up fit\n    def fcn(p):\n        ans = gvar.BufferDict()\n        for k, i in data:\n            shape = data[k, i].shape\n            if shape == ():\n                ans[k, i] = p[k]\n            else:\n                ans[k, i] = p[k][p0index[k, i]]\n        return ans\n    fit = lsqfit.nonlinear_fit(data=data, fcn=fcn, p0=p0, **fitterargs)\n    if datashape is None:\n        return BufferDictWAvg(fit.p, fit)\n    elif datashape == ():\n        return GVarWAvg(fit.p[None], fit)\n    else:\n        return ArrayWAvg(fit.p[None], fit)", "response": "Calculates the weighted average of the |GVar|s or arrays of |GVar|s or dicts of |GVar|s and the one - parameter fit function."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef formatall(self, *args, **kargs):\n        \" Add-on method for fits returned by chained_nonlinear_fit. \"\n        ans = ''\n        for x in self.chained_fits:\n            ans += 10 * '=' + ' ' + str(x) + '\\n'\n            ans += self.chained_fits[x].format(*args, **kargs)\n            ans += '\\n'\n        return ans[:-1]", "response": "Add - on method for fits returned by chained_nonlinear_fit."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the default values for the fitter s keyword parameters.", "response": "def set(self, **kargs):\n        \"\"\" Reset default keyword parameters.\n\n        Assigns new default values from dictionary ``kargs`` to the fitter's\n        keyword parameters. Keywords for the underlying :mod:`lsqfit` fitters\n        can also be  included (or grouped together in dictionary\n        ``fitterargs``).\n\n        Returns tuple ``(kargs, oldkargs)`` where ``kargs`` is a dictionary\n        containing all :class:`lsqfit.MultiFitter` keywords after they have\n        been updated, and ``oldkargs`` contains the  original values for these\n        keywords. Use ``fitter.set(**oldkargs)`` to restore the original\n        values.\n        \"\"\"\n        kwords = set([\n            'mopt', 'fast', 'ratio', 'wavg_kargs', 'wavg_all',\n            'fitterargs', 'fitname',\n            ])\n        kargs = dict(kargs)\n        oldkargs = {}\n        fargs = {}\n        # changed\n        for k in list(kargs.keys()):  # list() needed since changing kargs\n            if k in kwords:\n                oldkargs[k] = getattr(self, k)\n                setattr(self, k, kargs[k])\n                kwords.remove(k)\n            else:\n                fargs[k] = kargs[k]\n                del kargs[k]\n        # unchanged\n        for k in kwords:\n            kargs[k] = getattr(self, k)\n        # manage fitterargs\n        if 'fitterargs' in kwords:\n            # means wasn't in kargs initially\n            oldkargs['fitterargs'] = self.fitterargs\n            self.fitterargs = dict(self.fitterargs)\n        if len(fargs) > 0:\n            self.fitterargs.update(fargs)\n        kargs['fitterargs'] = dict(self.fitterargs)\n        return kargs, oldkargs"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating fit function to fit models in list models.", "response": "def buildfitfcn(self):\n        \"\"\" Create fit function to fit models in list ``models``. \"\"\"\n        def _fitfcn(p, flatmodels=self.flatmodels):\n            ans = gvar.BufferDict()\n            for m in flatmodels:\n                ans[m.datatag] = (\n                    m.fitfcn(p) if m.ncg <= 1 else\n                    MultiFitter.coarse_grain(m.fitfcn(p), m.ncg)\n                    )\n            return ans\n        return _fitfcn"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef builddata(self, mopt=None, data=None, pdata=None, prior=None):\n        if pdata is None:\n            if data is None:\n                raise ValueError('no data or pdata')\n            pdata = gvar.BufferDict()\n            for m in self.flatmodels:\n                pdata[m.datatag] = (\n                    m.builddata(data) if m.ncg <= 1 else\n                    MultiFitter.coarse_grain(m.builddata(data), m.ncg)\n                    )\n        else:\n            npdata = gvar.BufferDict()\n            for m in self.flatmodels:\n                npdata[m.datatag] = pdata[m.datatag]\n            pdata = npdata\n        if mopt is not None:\n            fitfcn = self.buildfitfcn()\n            p_all = self.buildprior(prior=prior, mopt=None)\n            f_all = fitfcn(p_all)\n\n            # fcn with part we want to keep\n            p_trunc = self.buildprior(prior=prior, mopt=mopt)\n            f_trunc = fitfcn(p_trunc)\n\n            # correct pdata\n            pdata = gvar.BufferDict(pdata)\n            if not self.ratio:\n                for m in self.flatmodels:\n                    pdata[m.datatag] += f_trunc[m.datatag] - f_all[m.datatag]\n            else:\n                for m in self.flatmodels:\n                    ii = (gvar.mean(f_all[m.datatag]) != 0)\n                    ratio = f_trunc[m.datatag][ii] / f_all[m.datatag][ii]\n                    pdata[m.datatag][ii] *= ratio\n        return pdata", "response": "Rebuild pdata to account for marginalization."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate prior to fit models in list models.", "response": "def buildprior(self, prior, mopt=None):\n        \"\"\" Create prior to fit models in list ``models``. \"\"\"\n        nprior = gvar.BufferDict()\n        for m in self.flatmodels:\n            nprior.update(m.buildprior(\n                prior, mopt=mopt,\n                ))\n        if not self.fast:\n            for k in prior:\n                if k not in nprior:\n                    nprior[k] = prior[k]\n        return nprior"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _flatten_models(tasklist):\n        \" Create 1d-array containing all disctinct models from ``tasklist``. \"\n        ans = gvar.BufferDict()\n        for task, mlist in tasklist:\n            if task != 'fit':\n                continue\n            for m in mlist:\n                id_m = id(m)\n                if id_m not in ans:\n                    ans[id_m] = m\n        return ans.buf.tolist()", "response": "Create 1d - array containing all disctinct models from tasklist."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef flatten_models(models):\n        \" Create 1d-array containing all disctinct models from ``models``. \"\n        if isinstance(models, MultiFitterModel):\n            ans = [models]\n        else:\n            tasklist = MultiFitter._compile_models(models)\n            ans = MultiFitter._flatten_models(tasklist)\n        return ans", "response": "Create 1d - array containing all disctinct models from models."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute least-squares fit of models to data. :meth:`MultiFitter.lsqfit` fits all of the models together, in a single fit. It returns the |nonlinear_fit| object from the fit. To see plots of the fit data divided by the fit function with the best-fit parameters use fit.show_plots() This method has optional keyword arguments ``save`` and ``view``; see documentation for :class:`lsqfit.MultiFitter.show_plots` for more information. Plotting requires module :mod:`matplotlib`. To bootstrap a fit, use ``fit.bootstrapped_fit_iter(...)``; see :meth:`lsqfit.nonlinear_fit.bootstrapped_fit_iter` for more information. Args: data: Input data. One of ``data`` or ``pdata`` must be specified but not both. ``pdata`` is obtained from ``data`` by collecting the output from ``m.builddata(data)`` for each model ``m`` and storing it in a dictionary with key ``m.datatag``. pdata: Input data that has been processed by the models using :meth:`MultiFitter.process_data` or :meth:`MultiFitter.process_dataset`. One of ``data`` or ``pdata`` must be specified but not both. prior (dict): Bayesian prior for fit parameters used by the models. p0: Dictionary , indexed by parameter labels, containing initial values for the parameters in the fit. Setting ``p0=None`` implies that initial values are extracted from the prior. Setting ``p0=\"filename\"`` causes the fitter to look in the file with name ``\"filename\"`` for initial values and to write out best-fit parameter values after the fit (for the next call to ``self.lsqfit()``). kargs: Arguments that (temporarily) override parameters specified when the :class:`MultiFitter` was created. Can also include additional arguments to be passed through to the :mod:`lsqfit` fitter.", "response": "def lsqfit(self, data=None, pdata=None, prior=None, p0=None, **kargs):\n        \"\"\" Compute least-squares fit of models to data.\n\n        :meth:`MultiFitter.lsqfit` fits all of the models together, in\n        a single fit. It returns the |nonlinear_fit| object from the fit.\n\n        To see plots of the fit data divided by the fit function\n        with the best-fit parameters use\n\n            fit.show_plots()\n\n        This method has optional keyword arguments ``save`` and ``view``;\n        see documentation for :class:`lsqfit.MultiFitter.show_plots`\n        for more information. Plotting requires module :mod:`matplotlib`.\n\n        To bootstrap a fit, use ``fit.bootstrapped_fit_iter(...)``;\n        see :meth:`lsqfit.nonlinear_fit.bootstrapped_fit_iter` for more\n        information.\n\n        Args:\n            data: Input data. One of ``data`` or ``pdata`` must be\n                specified but not both. ``pdata`` is obtained from ``data``\n                by collecting the output from ``m.builddata(data)``\n                for each model ``m`` and storing it in a dictionary\n                with key ``m.datatag``.\n            pdata: Input data that has been processed by the\n                models using :meth:`MultiFitter.process_data` or\n                :meth:`MultiFitter.process_dataset`. One of\n                ``data`` or ``pdata`` must be  specified but not both.\n            prior (dict): Bayesian prior for fit parameters used by the models.\n            p0: Dictionary , indexed by parameter labels, containing\n                initial values for the parameters in the fit. Setting\n                ``p0=None`` implies that initial values are extracted from the\n                prior. Setting ``p0=\"filename\"`` causes the fitter to look in\n                the file with name ``\"filename\"`` for initial values and to\n                write out best-fit parameter values after the fit (for the\n                next call to ``self.lsqfit()``).\n            kargs: Arguments that (temporarily) override parameters specified\n                when the :class:`MultiFitter` was created. Can also include\n                additional arguments to be passed through to the :mod:`lsqfit`\n                fitter.\n        \"\"\"\n        # gather parameters\n        if prior is None:\n            raise ValueError('no prior')\n        kargs, oldargs = self.set(**kargs)\n\n        # save parameters for bootstrap (in case needed)\n        fitter_args_kargs = (\n            self.chained_lsqfit,\n            dict(data=data, prior=prior, pdata=pdata, models=self.models),\n            dict(kargs),\n            )\n\n        # build prior, data and function\n        fitprior = self.buildprior(prior=prior, mopt=self.mopt)\n        fitdata = self.builddata(\n            mopt=self.mopt, data=data, pdata=pdata, prior=prior\n            )\n        fitfcn = self.buildfitfcn()\n\n        # fit\n        self.fit = lsqfit.nonlinear_fit(\n            data=fitdata, prior=fitprior, fcn=fitfcn, p0=p0,\n            **self.fitterargs\n            )\n        if len(self.flatmodels) > 1:\n            fname = self.fitname(\n                '(' +\n                ','.join([self.fitname(k.datatag) for k in self.flatmodels])\n                + ')'\n                )\n        else:\n            fname = self.fitname(self.flatmodels[0].datatag)\n        self.fit.chained_fits = collections.OrderedDict([(fname, self.fit)])\n\n        # add methods for printing and plotting\n        def _formatall(*args, **kargs):\n            \" Add-on method for fits returned by chained_lsqfit. \"\n            ans = ''\n            for x in self.fit.chained_fits:\n                ans += 10 * '=' + ' ' + str(x) + '\\n'\n                ans += self.fit.chained_fits[x].format(*args, **kargs)\n                ans += '\\n'\n            return ans[:-1]\n        self.fit.formatall = _formatall\n        def _show_plots(save=False, view='ratio'):\n            MultiFitter.show_plots(\n                fitdata=fitdata, fitval=fitfcn(self.fit.p),\n                save=save, view=view,\n                )\n        self.fit.show_plots = _show_plots\n\n        # restore default keywords\n        self.set(**oldargs)\n\n        # add bootstrap method\n        fitter_args_kargs[1]['p0'] = self.fit.pmean\n        def _bstrap_iter(\n            n=None, datalist=None, pdatalist=None, **kargs\n            ):\n            return MultiFitter._bootstrapped_fit_iter(\n                fitter_args_kargs,\n                n=n, datalist=datalist, pdatalist=pdatalist, **kargs\n                )\n        self.fit.bootstrapped_fit_iter = _bstrap_iter\n        return self.fit"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes the chained least - squares fit of the specified set of items in the specified data.", "response": "def chained_lsqfit(\n        self, data=None, pdata=None, prior=None, p0=None, add_priornoise=None,\n        **kargs\n        ):\n        \"\"\" Compute chained least-squares fit of models to data.\n\n        In a chained fit to models ``[s1, s2, ...]``, the models are fit one\n        at a time, with the fit output from one being fed into the prior for\n        the next. This can be much faster than  fitting the models together,\n        simultaneously. The final result comes from the last fit in the chain,\n        and includes parameters from all of the models.\n\n        The most general chain has the structure ``[s1, s2, s3 ...]``\n        where each ``sn`` is one of:\n\n            1) A model (derived from :class:`multifitter.MultiFitterModel`).\n\n            2) A tuple ``(m1, m2, m3)`` of models, to be fit together in\n                a single fit (i.e., simultaneously). Simultaneous fits\n                are useful for closely related models.\n\n            3) A list ``[p1, p2, p3 ...]`` where each ``pn`` is either\n                a model or a tuple of models (see #2). The ``pn`` are fit\n                separately: the fit output from one fit is *not* fed into the\n                prior of the next (i.e., the fits are effectively in\n                parallel). Results from the separate fits are averaged at the\n                end to provide a single composite result for the collection of\n                fits. Parallel fits are effective (and fast) when the\n                different fits have few or no fit parameters in common.\n\n            4) A dictionary that (temporarily) resets default values for\n                fitter keywords. The new values, specified in the dictionary,\n                apply to subsequent fits in the chain. Any number of such\n                dictionaries can be included in the model chain.\n\n\n        Fit results are returned in a :class:`lsqfit.MultiFitter.chained_fit`\n        object ``fit``, which is very similar to a :class:`nonlinear_fit`\n        object (see documentation for more information). Object ``fit`` has an\n        extra attribute ``fit.chained_fits`` which is an ordered dictionary\n        containing fit results for each link in the chain of fits, indexed by\n        fit names built from the corresponding data tags.\n\n        To list results from all of the fits in the chain, use ::\n\n            print(fit.formatall())\n\n        This method has optional keyword arguments ``maxline``,\n        ``pstyle``, and ``nline``; see the documentation for\n        :meth:`lsqfit.nonlinear_fit.format` for more\n        information.\n\n        To view plots of each fit use\n\n            fit.show_plots()\n\n        This method has optional keyword arguments ``save`` and ``view``;\n        see documentation for :class:`lsqfit.MultiFitter.show_plots`\n        for more information. Plotting requires module :mod:`matplotlib`.\n\n        To bootstrap a fit, use ``fit.bootstrapped_fit_iter(...)``;\n        see :meth:`lsqfit.nonlinear_fit.bootstrapped_fit_iter` for more\n        information.\n\n        Args:\n            data: Input data. One of ``data`` or ``pdata`` must be\n                specified but not both. ``pdata`` is obtained from ``data``\n                by collecting the output from ``m.builddata(data)``\n                for each model ``m`` and storing it in a dictionary\n                with key ``m.datatag``.\n            pdata: Input data that has been processed by the\n                models using :meth:`MultiFitter.process_data` or\n                :meth:`MultiFitter.process_dataset`. One of\n                ``data`` or ``pdata`` must be  specified but not both.\n            prior: Bayesian prior for fit parameters used by the models.\n            p0: Dictionary , indexed by parameter labels, containing\n                initial values for the parameters in the fit. Setting\n                ``p0=None`` implies that initial values are extracted from the\n                prior. Setting ``p0=\"filename\"`` causes the fitter to look in\n                the file with name ``\"filename\"`` for initial values and to\n                write out best-fit parameter values after the fit (for the\n                next call to ``self.lsqfit()``).\n            kargs: Arguments that override parameters specified when\n                the :class:`MultiFitter` was created. Can also include\n                additional arguments to be passed through to\n                the :mod:`lsqfit` fitter.\n        \"\"\"\n        if prior is None:\n            raise ValueError('no prior')\n        if add_priornoise:\n            # intercept at top level so same treatment for all fits\n            prior = prior + (gvar.sample(prior) - gvar.mean(prior))\n        kargs, oldargs = self.set(**kargs)\n\n        # parameters for bootstrap (see below)\n        fitter_args_kargs = (\n            self.chained_lsqfit,\n            dict(data=data, prior=prior, pdata=pdata, models=self.models),\n            dict(kargs),\n            )\n\n        # local copy of prior\n        if self.fast:\n            prior = self.buildprior(prior)\n        else:\n            prior = gvar.BufferDict(prior)\n\n        # execute tasks in self.tasklist\n        chained_fits = collections.OrderedDict()\n        all_fnames = []\n        all_fitp = []\n        for tasktype, taskdata in self.tasklist:\n            if tasktype == 'fit':\n                fitter = self.__class__(models=taskdata, **kargs)\n                fit = fitter.lsqfit(\n                    data=data, pdata=pdata, prior=prior, p0=p0\n                    )\n                fname = list(fit.chained_fits.keys())[0]\n                if fname in chained_fits:\n                    raise ValueError('duplicate fits in chain: ' + str(fname))\n                elif fname[:5] == 'wavg(' and fname[-1] == ')':\n                    raise ValueError('bad fit name: ' + fname)\n                else:\n                    all_fnames.append(fname)\n                    chained_fits[fname] = fit\n                    all_fitp.append(fit.p)\n            elif tasktype == 'update-prior':\n                lastfit = chained_fits[all_fnames[-1]]\n                lastfit_p = lastfit.p\n                for k in lastfit_p:\n                    idx = tuple(\n                        slice(None, i) for i in numpy.shape(lastfit.p[k])\n                        )\n                    if idx != ():\n                        prior[k][idx] = lastfit.p[k]\n                    else:\n                        prior[k] = lastfit.p[k]\n            elif tasktype == 'wavg':\n                    nlist = all_fnames[-taskdata:]\n                    plist = [chained_fits[k].p for k in nlist]\n                    fit = lsqfit.wavg(plist, **self.wavg_kargs).fit\n                    fname = self.fitname('wavg({})'.format(','.join(nlist)))\n                    all_fnames.append(fname)\n                    chained_fits[fname] = fit\n            elif tasktype == 'update-kargs':\n                kargs.update(taskdata)\n            else:\n                raise RuntimeError('unknown task: ' + tasktype)\n\n        if self.fast and self.wavg_all:\n            fit = lsqfit.wavg(all_fitp, **self.wavg_kargs).fit\n            fname = self.fitname('wavg(all)')\n            chained_fits[fname] = fit\n            prior = fit.p\n\n        # build output class\n        self.fit = chained_nonlinear_fit(\n            p=prior, chained_fits=chained_fits,\n            multifitter=self, prior=fitter_args_kargs[1]['prior'],\n            )\n\n        # add bootstrap method\n        fitter_args_kargs[1]['p0'] = self.fit.pmean\n        def _bstrap_iter(\n            n=None, datalist=None, pdatalist=None, **kargs\n            ):\n            return MultiFitter._bootstrapped_fit_iter(\n                fitter_args_kargs,\n                n=n, datalist=datalist, pdatalist=pdatalist, **kargs\n                )\n        self.fit.bootstrapped_fit_iter = _bstrap_iter\n\n        # restore default keywords\n        self.set(**oldargs)\n        return self.fit"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts a list of models into a list of tasks.", "response": "def _compile_models(models):\n        \"\"\" Convert ``models`` into a list of tasks.\n\n        Each task is tuple ``(name, data)`` where ``name`` indicates the task\n        task and ``data`` is the relevant data for that task.\n\n        Supported tasks and data:\n\n            - ``'fit'`` and list of models\n            -  ``'update-kargs'`` and ``None``\n            - ``'update-prior'`` and ``None``\n            - ``'wavg'`` and number of (previous) fits to average\n\n        \"\"\"\n        tasklist = []\n        for m in models:\n            if isinstance(m, MultiFitterModel):\n                tasklist += [('fit', [m])]\n                tasklist += [('update-prior', None)]\n            elif hasattr(m, 'keys'):\n                tasklist += [('update-kargs', m)]\n            elif isinstance(m, tuple):\n                tasklist += [('fit', list(m))]\n                tasklist += [('update-prior', None)]\n            elif isinstance(m, list):\n                for sm in m:\n                    if isinstance(sm, MultiFitterModel):\n                        tasklist += [('fit', [sm])]\n                    elif isinstance(sm, tuple):\n                        tasklist += [('fit', list(sm))]\n                    else:\n                        raise ValueError(\n                            'type {} not allowed in sublists '.format(\n                                str(type(sm))\n                                )\n                            )\n                tasklist += [('wavg', len(m))]\n                tasklist += [('update-prior', None)]\n            else:\n                raise RuntimeError('bad model list')\n        return tasklist"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _bootstrapped_fit_iter(\n        fitter_args_kargs, n=None, datalist=None, pdatalist=None, **kargs\n        ):\n        \"\"\" Iterator that returns bootstrap copies of a fit.\n\n        Bootstrap iterator for |MultiFitter| fits analogous to\n        :meth:`lsqfit.bootstrapped_fit_iter`. The bootstrap uses the\n        same parameters as the last fit done by the fitter unless they\n        are overridden by ``kargs``.\n\n        Args:\n            n (int): Maximum number of iterations if ``n`` is not ``None``;\n                otherwise there is no maximum. Default is ``None``.\n            datalist (iter): Collection of bootstrap data sets for fitter.\n            pdatalist (iter): Collection of bootstrap processed data sets for\n                fitter.\n            kargs (dict): Overrides arguments in original fit.\n\n        Returns:\n            Iterator that returns an |nonlinear_fit| object\n            containing results from the fit to the next data set in\n            ``datalist``.\n\n        \"\"\"\n        fitter, args, okargs = fitter_args_kargs\n        for k in okargs:\n            if k not in kargs:\n                kargs[k] = okargs[k]\n        if 'p0' not in kargs:\n            kargs['p0'] = args['p0']\n        if datalist is not None:\n            pdatalist = (\n                MultiFitter.process_data(d, args['models']) for d in datalist\n                )\n        elif pdatalist is None:\n            pdata = args['pdata']\n            if pdata is None:\n                pdata = MultiFitter.process_data(args['data'], args['models'])\n            pdatalist = gvar.bootstrap_iter(pdata, n)\n        i = 0\n        for pdata in pdatalist:\n            i += 1\n            if n is not None and i > n:\n                break\n            fit = fitter(pdata=pdata, prior=args['prior'], **kargs)\n            yield fit", "response": "A generator that returns a bootstrap copy of a fit."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert data to processed data using models.", "response": "def process_data(data, models):\n        \"\"\" Convert ``data`` to processed data using ``models``.\n\n        Data from dictionary ``data`` is processed by each model\n        in list ``models``, and the results collected into a new\n        dictionary ``pdata`` for use in :meth:`MultiFitter.lsqfit`\n        and :meth:`MultiFitter.chained_lsqft`.\n        \"\"\"\n        pdata = gvar.BufferDict()\n        for m in MultiFitter.flatten_models(models):\n            pdata[m.datatag] = (\n                m.builddata(data) if m.ncg <= 1 else\n                MultiFitter.coarse_grain(m.builddata(data), ncg=m.ncg)\n                )\n        return pdata"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert a dataset to processed data using models.", "response": "def process_dataset(dataset, models, **kargs):\n        \"\"\" Convert ``dataset`` to processed data using ``models``.\n\n        :class:`gvar.dataset.Dataset` (or similar dictionary) object\n        ``dataset`` is processed by each model in list ``models``,\n        and the results collected into a new dictionary ``pdata`` for use in\n        :meth:`MultiFitter.lsqfit` and :meth:`MultiFitter.chained_lsqft`.\n        Assumes that the models have defined method\n        :meth:`MultiFitterModel.builddataset`. Keyword arguments\n        ``kargs`` are passed on to :func:`gvar.dataset.avg_data` when\n        averaging the data.\n        \"\"\"\n        dset = collections.OrderedDict()\n        for m in MultiFitter.flatten_models(models):\n            dset[m.datatag] = (\n                m.builddataset(dataset) if m.ncg <= 1 else\n                MultiFitter.coarse_grain(m.builddataset(dataset), ncg=m.ncg)\n                )\n        return gvar.dataset.avg_data(dset, **kargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nshow plots comparing fitdata and fitval.", "response": "def show_plots(fitdata, fitval, x=None, save=False, view='ratio'):\n        \"\"\" Show plots comparing ``fitdata[k],fitval[k]`` for each key ``k`` in ``fitval``.\n\n        Assumes :mod:`matplotlib` is installed (to make the plots). Plots\n        are shown for one correlator at a time. Press key ``n`` to see the\n        next correlator; press key ``p`` to see the previous one; press key\n        ``q`` to quit the plot and return control to the calling program;\n        press a digit to go directly to one of the first ten plots. Zoom,\n        pan and save using the window controls.\n\n        There are several different views available for each plot,\n        specified by parameter ``view``:\n\n            ``view='ratio'``: Data divided by fit (default).\n\n            ``view='diff'``: Data minus fit, divided by data's standard deviation.\n\n            ``view='std'``: Data and fit.\n\n            ``view='log'``: ``'std'`` with log scale on the vertical axis.\n\n            ``view='loglog'``: `'std'`` with log scale on both axes.\n\n        Press key ``v`` to cycle through these  views; or press keys\n        ``r``, ``d``, or ``l`` for the ``'ratio'``, ``'diff'``,\n        or ``'log'`` views, respectively.\n\n        Copies of the plots that are viewed can be saved by setting parameter\n        ``save=fmt`` where ``fmt`` is a string used to create\n        file names: the file name for the plot corresponding to key\n        ``k`` is ``fmt.format(k)``. It is important that the\n        filename end with a suffix indicating the type of plot file\n        desired: e.g., ``fmt='plot-{}.pdf'``.\n        \"\"\"\n        import matplotlib.pyplot as plt\n        # collect plotinfo\n        plotinfo = collections.OrderedDict()\n        for tag in fitval:\n            d = fitdata[tag]\n            f = fitval[tag]\n            plotinfo[tag] = (\n                numpy.arange(len(d))+1 if x is None else x[tag],\n                gvar.mean(d), gvar.sdev(d),  gvar.mean(f), gvar.sdev(f)\n                )\n        plotinfo_keys = list(plotinfo.keys())\n        fig = plt.figure()\n        viewlist = ['ratio', 'diff', 'std', 'log', 'loglog']\n        def onpress(event):\n            if event is not None:\n                try:    # digit?\n                    onpress.idx = int(event.key)\n                except ValueError:\n                    if event.key == 'n':\n                        onpress.idx += 1\n                    elif event.key == 'p':\n                        onpress.idx -= 1\n                    elif event.key == 'v':\n                        onpress.view = (onpress.view + 1) % len(viewlist)\n                    elif event.key == 'r':\n                        onpress.view = viewlist.index('ratio')\n                    elif event.key == 'd':\n                        onpress.view = viewlist.index('diff')\n                    elif event.key == 'l':\n                        onpress.view = viewlist.index('log')\n                    # elif event.key == 'q':  # unnecessary\n                    #     plt.close()\n                    #     return\n                    else:\n                        return\n            else:\n                onpress.idx = 0\n\n            # do the plot\n            if onpress.idx >= len(plotinfo_keys):\n                onpress.idx = len(plotinfo_keys)-1\n            elif onpress.idx < 0:\n                onpress.idx = 0\n            i = onpress.idx\n            k = plotinfo_keys[i]\n            x, g, dg, gth, dgth = plotinfo[k]\n            fig.clear()\n            plt.title(\"%d) %s   (press 'n', 'p', 'q', 'v' or a digit)\"\n                        % (i, k))\n            dx = (max(x) - min(x)) / 50.\n            plt.xlim(min(x)-dx, max(x)+dx)\n            plotview = viewlist[onpress.view]\n            if plotview in ['std', 'log', 'loglog']:\n                if plotview in ['log', 'loglog']:\n                    plt.yscale('log', nonposy='clip')\n                if plotview == 'loglog':\n                    plt.xscale('log', nonposx='clip')\n                plt.ylabel(str(k) + '   [%s]' % plotview)\n                if len(x) > 0:\n                    if len(x) > 1:\n                        plt.plot(x, gth, 'r-')\n                        plt.fill_between(\n                            x, y2=gth + dgth, y1=gth - dgth,\n                            color='r', alpha=0.075,\n                            )\n                    else:\n                        extra_x = [x[0] * 0.5, x[0] * 1.5]\n                        plt.plot(extra_x, 2 * [gth[0]], 'r-')\n                        plt.fill_between(\n                            extra_x, y2=2 * [gth[0] + dgth[0]],\n                            y1=2 * [gth[0] - dgth[0]],\n                            color='r', alpha=0.075,\n                            )\n                    plt.errorbar(x, g, dg, fmt='o')\n            elif plotview == 'ratio':\n                plt.ylabel(str(k)+' / '+'fit'  + '   [%s]' % plotview)\n                ii = (gth != 0.0)       # check for exact zeros (eg, antiperiodic)\n                if len(x[ii]) > 0:\n                    if len(x[ii]) > 1:\n                        plt.fill_between(\n                            x[ii], y2=1 + dgth[ii] / gth[ii],\n                            y1=1 - dgth[ii] / gth[ii],\n                            color='r', alpha=0.075,\n                            )\n                        plt.plot(x, numpy.ones(len(x), float), 'r-')\n                    else:\n                        extra_x = [x[ii][0] * 0.5, x[ii][0] * 1.5]\n                        plt.fill_between(\n                            extra_x, y2=2 * [1 + dgth[ii][0]/gth[ii][0]],\n                            y1=2 * [1 - dgth[ii][0]/gth[ii][0]],\n                            color='r', alpha=0.075,\n                            )\n                        plt.plot(extra_x, numpy.ones(2, float), 'r-')\n                    plt.errorbar(x[ii], g[ii]/gth[ii], dg[ii]/gth[ii], fmt='o')\n            elif plotview == 'diff':\n                plt.ylabel('({} - fit) / sigma'.format(str(k))  + '   [%s]' % plotview)\n                ii = (dg != 0.0)       # check for exact zeros\n                if len(x[ii]) > 0:\n                    if len(x[ii]) > 1:\n                        plt.fill_between(\n                            x[ii], y2=dgth[ii] / dg[ii],\n                            y1=-dgth[ii] / dg[ii],\n                            color='r', alpha=0.075\n                            )\n                        plt.plot(x, numpy.zeros(len(x), float), 'r-')\n                    else:\n                        extra_x = [x[ii][0] * 0.5, x[ii][0] * 1.5]\n                        plt.fill_between(\n                            extra_x, y2=2 * [dgth[ii][0] / dg[ii][0]],\n                            y1=2 * [-dgth[ii][0] / dg[ii][0]],\n                            color='r', alpha=0.075\n                            )\n                        plt.plot(extra_x, numpy.zeros(2, float), 'r-')\n                    plt.errorbar(\n                        x[ii], (g[ii] - gth[ii]) / dg[ii], dg[ii] / dg[ii],\n                        fmt='o'\n                        )\n            if save:\n                plt.savefig(save.format(k), bbox_inches='tight')\n            else:\n                plt.draw()\n        onpress.idx = 0\n        try:\n            onpress.view = viewlist.index(view)\n        except ValueError:\n            raise ValueError('unknow view: ' + str(view))\n        fig.canvas.mpl_connect('key_press_event', onpress)\n        onpress(None)\n        plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef make_script():\n    # tests\n    easy = [\n        'misra1a', 'chwirut2', 'chwirut1', 'lanczos3',\n        'gauss1', 'gauss2', 'danwood', 'misra1b',\n        ]\n    medium = [\n        'kirby2', 'hahn1', 'nelson', 'mgh17', 'lanczos1',\n        'lanczos2', 'gauss3', 'misra1c', 'misra1d', 'roszman1', 'enso',\n        ]\n    hard = [\n        'mgh09', 'thurber', 'boxbod', 'rat42', 'mgh10',\n        'eckerle4', 'rat43', 'bennett5',\n        ]\n\n    # beginning material\n    text = \\\n\"\"\"\nfrom __future__ import print_function\n\nimport gvar as gv\nimport numpy as np\nimport lsqfit\n\nlog = np.log\nexp = np.exp\narctan = np.arctan\ncos = np.cos\nsin = np.sin\npi = np.pi\n\n\"\"\"\n    if USE_2ND_STARTING_VALUE:\n        text += '# 2nd starting values\\n\\n'\n    else:\n        text += '# 1st starting values\\n\\n'\n    # main() program\n    text += 'def main():\\n'\n    text += '    # easy\\n'\n    for n in easy:\n        text += '    ' + n + '()\\n'\n    text += '\\n    # medium\\n'\n    for n in medium:\n        text += '    ' + n + '()\\n'\n    text += '\\n    # hard\\n'\n    for n in hard:\n        text += '    ' + n + '()\\n'\n\n    # add test-fit functions\n    for n in easy:\n        text += '\\n'\n        text += make_fcn(n + '.txt')\n    for n in medium:\n        text += '\\n'\n        text += make_fcn(n + '.txt')\n    for n in hard:\n        text += '\\n'\n        text += make_fcn(n + '.txt')\n\n    # ending material\n    text += \\\n\"\"\"\n\nif __name__ == '__main__':\n    main()\n\"\"\"\n    return text", "response": "Return the full testing script with all the tests."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the fit - test function corresponding to file filename.", "response": "def make_fcn(filename):\n    \"\"\" Return the fit-test function corresponding to file ``filename``.  \"\"\"\n    # collect lines and parse first several lines for fit info\n    with open(filename, 'r') as ifile:\n        lines = ifile.readlines()\n    name = filename.split('.')[0]\n    _certified_values = re.compile('.*Certified Values\\s*\\(lines\\s*([0-9]*)\\s*to\\s*([0-9]*)\\)')\n    _data = re.compile('.*Data\\s*\\(lines\\s*([0-9]*)\\s*to\\s*([0-9]*)\\)')\n    _model = re.compile('^Model:')\n    values = None\n    data = None\n    fcn = None\n    for i,line in enumerate(lines):\n        m = _certified_values.match(line)\n        if m is not None:\n            values = (int(m.group(1)), int(m.group(2)))\n        m = _data.match(line)\n        if m is not None:\n            data = (int(m.group(1)), int(m.group(2)))\n        m = _model.match(line)\n        if m is not None:\n            fcn = \"\"\n            for j in range(i+3,values[0] - 1):\n                s = lines[j].strip()\n                if s == '':\n                    break\n                fcn += s\n        if None not in [values, data, fcn]:\n            break\n    # get rid of + e\n    fcn = fcn[:-4]\n    # fix [] brackets\n    s = fcn.split('[')\n    if len(s) > 1:\n        _bracket = re.compile('(.*)\\](.*)')\n        nfcn = s[0]\n        for si in s[1:]:\n            m = _bracket.match(si)\n            if m is not None:\n                si = m.group(1) + ')' + m.group(2)\n                si = '(' + m.group(1) + ')' + m.group(2)\n            nfcn += si\n        fcn = nfcn\n    # parameter names\n    nparam = values[1] - values[0] + 1 - 5\n    pnames = ','.join(['b' + str(i+1) for i in range(nparam)])\n    p = []\n    p0 = []\n    prior = []\n    for i in range(values[0] - 1, values[0] - 1 + nparam):\n        s = lines[i].split()\n        p.append(gv.gvar(s[-2] + ' +- ' + s[-1]))\n        if USE_2ND_STARTING_VALUE:\n            p0.append(float(s[3]))\n        else:\n            p0.append(float(s[2]))\n        prior.append(gv.gvar('0 +- ' + str(200 * p[-1].mean)))\n    p = np.array(p)\n    p0 = np.array(p0)\n    prior = list(gv.fmt(prior))\n    nistp = answers.get(name, str(p))\n    # error on y\n    s = lines[values[1] - 3].split()\n    yerr = '0 +- ' + s[-1]\n    # collect x, y\n    x = []\n    y = []\n    for i in range(data[0] - 1, data[1]):\n        s = lines[i].split()\n        if len(s) == 2:\n            x.append(float(s[-1]))\n            y.append(float(s[-2]))\n        elif len(s) > 2:\n            y.append(float(s[0]))\n            x.append([float(si) for si in s[1:]])\n    x = np.array(x)\n    if len(x.shape) > 1:\n        xnames = ','.join(['x' + str(i+1) for i in range(x.shape[1])])\n        x = x.T\n    else:\n        xnames = 'x'\n    y = np.array(y)\n    if len(nistp.split('\\n')) > 1:\n        nistp = '\\\\n'.join(nistp.split('\\n'))\n    if name == 'nelson':\n        fcn = fcn.replace('log(y)', 'y')\n        yname = 'log(y)'\n    else:\n        yname = 'y'\n    if name == 'mgh10' and not USE_2ND_STARTING_VALUE:\n        p0 = gv.sdev(gv.gvar(prior)) / 100.\n    template = \\\n\"\"\"\ndef {name}():\n    print(20 * '=', '{name}')\n    x = np.{x}\n    y = np.{y}\n    y = {yname} + gv.gvar(len(y) * ['{yerr}'])\n    def fcn(x, b):\n        {xnames} = x\n        {pnames} = b\n        {fcn}\n        return y\n    prior = gv.gvar({prior})\n    p0 = np.{p0}\n    fit = lsqfit.nonlinear_fit(\n        prior=prior, data=(x,y), fcn=fcn, p0=p0, tol=1e-10,\n        )\n    print(fit)\n    assert str(fit.p) == '{nistp}'\n\"\"\"\n    return template.format(\n        name=name,\n        x=repr(x),\n        y=repr(y),\n        p0=repr(p0),\n        yerr=yerr,\n        fcn=fcn,\n        prior=repr(prior),\n        nistp=nistp,\n        pnames=pnames,\n        xnames=xnames,\n        yname=yname,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef buildprior(self, prior, mopt=None, extend=False):\n        \" Extract the model's parameters from prior. \"\n        newprior = {}\n        # allow for log-normal, etc priors\n        intercept, slope = gv.get_dictkeys(\n            prior, [self.intercept, self.slope]\n            )\n        newprior[intercept] = prior[intercept]\n        if mopt is None:\n            # slope parameter marginalized if mopt is not None\n            newprior[slope] = prior[slope]\n        return newprior", "response": "Extract the model s parameters from prior."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndisplays theta vs t plot.", "response": "def show_plot(t_array, th_array):\n    \"\"\" Display theta vs t plot. \"\"\"\n    th_mean = gv.mean(th_array)\n    th_sdev = gv.sdev(th_array)\n    thp = th_mean + th_sdev\n    thm = th_mean - th_sdev\n    plt.fill_between(t_array, thp, thm, color='0.8')\n    plt.plot(t_array, th_mean, linewidth=0.5)\n    plt.xlabel('$t$')\n    plt.ylabel(r'$\\theta(t)$')\n    plt.savefig('pendulum.pdf', bbox_inches='tight')\n    plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncalculate [ dtheta / dt d2theta / dt2 from [ theta dtheta / dt2 ].", "response": "def deriv(self, t, y, data=None):\n        \" Calculate [dtheta/dt, d2theta/dt2] from [theta, dtheta/dt].\"\n        theta, dtheta_dt = y\n        return np.array([dtheta_dt, - self.g_l * gv.sin(theta)])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntaking in 4 - digit year for first half of season and returns API appropriate formatted code for the given season", "response": "def _nbaSeason(x):\n    \"\"\"Takes in 4-digit year for first half of season and returns API appropriate formatted code\n\n    Input Values: YYYY \n\n    Used in: _Draft.Anthro(), _Draft.Agility(), _Draft.NonStationaryShooting(), \n    _Draft.SpotUpShooting(), _Draft.Combine()\n\n    \"\"\"\n    if len(str(x)) == 4:\n        try:\n            return '{0}-{1}'.format(x, str(int(x) % 100 + 1)[-2:].zfill(2))\n        except ValueError: \n            raise ValueError(\"Enter the four digit year for the first half of the desired season\")\n    else: raise ValueError(\"Enter the four digit year for the first half of the desired season\")"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _seasonID(x):\n    if len(str(x)) == 4:\n        try:\n            return \"\".join([\"2\",str(x)])\n        except ValueError:\n            raise ValueError(\"Enter the four digit year for the first half of the desired season\")\n    else: raise ValueError(\"Enter the four digit year for the first half of the desired season\")", "response": "takes in 4 - digit years and returns API formatted seasonID"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntakes in 4 - digit year for first half of season and returns API appropriate formatted code for the nba season.", "response": "def nba_season(x):\n    \"\"\"Takes in 4-digit year for first half of season and returns API appropriate formatted code\n\n    Input Values: YYYY \n\n    Used in: _Draft.Anthro(), _Draft.Agility(), _Draft.NonStationaryShooting(), \n    _Draft.SpotUpShooting(), _Draft.Combine()\n\n    \"\"\"\n    if len(str(x)) == 4:\n        try:\n            return '{0}-{1}'.format(x, str(int(x) % 100 + 1)[-2:].zfill(2))\n        except ValueError:\n            raise ValueError(\"Enter the four digit year for the first half of the desired season\")\n    else:\n        raise ValueError(\"Enter the four digit year for the first half of the desired season\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntaking in 4 - digit years and returns API formatted seasonID Used in the base class", "response": "def season_id(x):\n    \"\"\"takes in 4-digit years and returns API formatted seasonID\n\n    Input Values: YYYY \n\n    Used in:\n\n    \"\"\"\n    if len(str(x)) == 4:\n        try:\n            return \"\".join([\"2\", str(x)])\n        except ValueError:\n            raise ValueError(\"Enter the four digit year for the first half of the desired season\")\n    else:\n        raise ValueError(\"Enter the four digit year for the first half of the desired season\")"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef aggregated_records(all_records, key_fields=KEY_FIELDS):\n    flow_table = defaultdict(_FlowStats)\n    for flow_record in all_records:\n        key = tuple(getattr(flow_record, attr) for attr in key_fields)\n        if any(x is None for x in key):\n            continue\n        flow_table[key].update(flow_record)\n\n    for key in flow_table:\n        item = {k: v for k, v in zip(key_fields, key)}\n        item.update(flow_table[key].to_dict())\n        yield item", "response": "Yields all_records dicts that correspond to aggregates of the FlowRecords in all_records."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nshows the set of IPs seen in Flow Log records.", "response": "def action_ipset(reader, *args):\n    \"\"\"Show the set of IPs seen in Flow Log records.\"\"\"\n    ip_set = set()\n    for record in reader:\n        if record.log_status in (SKIPDATA, NODATA):\n            continue\n        ip_set.add(record.srcaddr)\n        ip_set.add(record.dstaddr)\n\n    for ip in ip_set:\n        print(ip)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef action_findip(reader, *args):\n    target_ips = set(args)\n    for record in reader:\n        if (record.srcaddr in target_ips) or (record.dstaddr in target_ips):\n            print(record.to_message())", "response": "Find Flow Log records involving a specific IP or IPs."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef action_aggregate(reader, *args):\n    all_aggregated = aggregated_records(reader)\n    first_row = next(all_aggregated)\n    keys = sorted(first_row.keys())\n    print(*keys, sep='\\t')\n\n    # Join the first row with the rest of the rows and print them\n    iterable = chain([first_row], all_aggregated)\n    for item in iterable:\n        print(*[item[k] for k in keys], sep='\\t')", "response": "Aggregate flow records by 5 - tuple and print a tab - separated stream"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef generate_return_periods(qout_file,\n                            return_period_file,\n                            num_cpus=multiprocessing.cpu_count(),\n                            storm_duration_days=7,\n                            method='weibull'):\n    \"\"\"\n    Generate return period from RAPID Qout file\n    \"\"\"\n    # get ERA Interim Data Analyzed\n    with RAPIDDataset(qout_file) as qout_nc_file:\n        print(\"Setting up Return Periods File ...\")\n        return_period_nc = Dataset(return_period_file, 'w')\n\n        return_period_nc.createDimension('rivid', qout_nc_file.size_river_id)\n\n        timeSeries_var = \\\n            return_period_nc.createVariable('rivid', 'i4', ('rivid',))\n        timeSeries_var.long_name = (\n            'unique identifier for each river reach')\n\n        max_flow_var = \\\n            return_period_nc.createVariable('max_flow', 'f8', ('rivid',))\n        max_flow_var.long_name = 'maximum streamflow'\n        max_flow_var.units = 'm3/s'\n\n        if method == 'weibull':\n\n            return_period_20_var = \\\n                return_period_nc.createVariable('return_period_20',\n                                                'f8', ('rivid',))\n            return_period_20_var.long_name = '20 year return period flow'\n            return_period_20_var.units = 'm3/s'\n\n        if method == 'gumble':\n\n            return_period_100_var = \\\n                return_period_nc.createVariable('return_period_100',\n                                                'f8', ('rivid',))\n            return_period_100_var.long_name = '100 year return period flow'\n            return_period_100_var.units = 'm3/s'\n\n            return_period_50_var = \\\n                return_period_nc.createVariable('return_period_50',\n                                                'f8', ('rivid',))\n            return_period_50_var.long_name = '50 year return period flow'\n            return_period_50_var.units = 'm3/s'\n\n            return_period_20_var = \\\n                return_period_nc.createVariable('return_period_20',\n                                                'f8', ('rivid',))\n            return_period_20_var.long_name = '20 year return period flow'\n            return_period_20_var.units = 'm3/s'\n\n        if method == 'log_pearson':\n\n            return_period_100_var = \\\n                return_period_nc.createVariable('return_period_100',\n                                                'f8', ('rivid',))\n            return_period_100_var.long_name = '100 year return period flow'\n            return_period_100_var.units = 'm3/s'\n\n            return_period_50_var = \\\n                return_period_nc.createVariable('return_period_50',\n                                                'f8', ('rivid',))\n            return_period_50_var.long_name = '50 year return period flow'\n            return_period_50_var.units = 'm3/s'\n\n            return_period_25_var = \\\n                return_period_nc.createVariable('return_period_25',\n                                                'f8', ('rivid',))\n            return_period_25_var.long_name = '25 year return period flow'\n            return_period_25_var.units = 'm3/s'\n\n        return_period_10_var = \\\n            return_period_nc.createVariable('return_period_10',\n                                            'f8', ('rivid',))\n        return_period_10_var.long_name = '10 year return period flow'\n        return_period_10_var.units = 'm3/s'\n\n        return_period_2_var = \\\n            return_period_nc.createVariable('return_period_2',\n                                            'f8', ('rivid',))\n        return_period_2_var.long_name = '2 year return period flow'\n        return_period_2_var.units = 'm3/s'\n\n        lat_var = return_period_nc.createVariable('lat', 'f8', ('rivid',),\n                                                  fill_value=-9999.0)\n\n        lon_var = return_period_nc.createVariable('lon', 'f8', ('rivid',),\n                                                  fill_value=-9999.0)\n\n        add_latlon_metadata(lat_var, lon_var)\n\n        return_period_nc.variables['lat'][:] = \\\n            qout_nc_file.qout_nc.variables['lat'][:]\n        return_period_nc.variables['lon'][:] = \\\n            qout_nc_file.qout_nc.variables['lon'][:]\n\n        river_id_list = qout_nc_file.get_river_id_array()\n        return_period_nc.variables['rivid'][:] = river_id_list\n\n        return_period_nc.return_period_method = method\n\n        return_period_nc.close()\n\n        time_array = qout_nc_file.get_time_array()\n\n    log(\"Extracting Data and Generating Return Periods ...\")\n    num_years = int((datetime.utcfromtimestamp(time_array[-1]) -\n                     datetime.utcfromtimestamp(time_array[0])).days/365.2425)\n    time_steps_per_day = (24 * 3600) / float(\n        (datetime.utcfromtimestamp(time_array[1]) -\n         datetime.utcfromtimestamp(time_array[0])).total_seconds())\n    step = max(1, int(time_steps_per_day * storm_duration_days))\n\n    # generate multiprocessing jobs\n    # pylint: disable=no-member\n    mp_lock = multiprocessing.Manager().Lock()\n    job_combinations = []\n    partition_index_list = partition(river_id_list, num_cpus*2)[1]\n    for sub_partition_index_list in partition_index_list:\n        # pylint: disable=len-as-condition\n        if len(sub_partition_index_list) > 0:\n            job_combinations.append((qout_file,\n                                     return_period_file,\n                                     sub_partition_index_list,\n                                     step,\n                                     num_years,\n                                     method,\n                                     mp_lock\n                                     ))\n\n    pool = multiprocessing.Pool(num_cpus)\n    pool.map(generate_single_return_period,\n             job_combinations)\n    pool.close()\n    pool.join()", "response": "Generate return periods from RAPID Qout file"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_poly_area_geo(poly):\n    minx, miny, maxx, maxy = poly.bounds\n    # reproject polygon to get area\n    reprojected_for_area = Proj(\"+proj=aea +lat_1={0} +lat_1={1} \"\n                                \"+lat_0={2} +lon_0={3}\"\n                                .format(miny,\n                                        maxy,\n                                        (miny + maxy) / 2.0,\n                                        (minx + maxx) / 2.0))\n    geographic_proj = Proj(init='epsg:4326')\n    project_func = partial(transform,\n                           geographic_proj,\n                           reprojected_for_area)\n    reprojected_poly = shapely_transform(project_func, poly)\n    return reprojected_poly.area", "response": "Calculates the area in meters squared of the individual polygon"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_lat_lon_indices(lsm_lat_array, lsm_lon_array, lat, lon):\n    if lsm_lat_array.ndim == 2 and lsm_lon_array.ndim == 2:\n        lsm_lat_indices_from_lat, lsm_lon_indices_from_lat = \\\n            np.where((lsm_lat_array == lat))\n        lsm_lat_indices_from_lon, lsm_lon_indices_from_lon = \\\n            np.where((lsm_lon_array == lon))\n\n        index_lsm_grid_lat = np.intersect1d(lsm_lat_indices_from_lat,\n                                            lsm_lat_indices_from_lon)[0]\n        index_lsm_grid_lon = np.intersect1d(lsm_lon_indices_from_lat,\n                                            lsm_lon_indices_from_lon)[0]\n\n    elif lsm_lat_array.ndim == 1 and lsm_lon_array.ndim == 1:\n        index_lsm_grid_lon = np.where(lsm_lon_array == lon)[0][0]\n        index_lsm_grid_lat = np.where(lsm_lat_array == lat)[0][0]\n    else:\n        raise IndexError(\"Lat/Lon lists have invalid dimensions. \"\n                         \"Only 1D or 2D arrays allowed ...\")\n\n    return index_lsm_grid_lat, index_lsm_grid_lon", "response": "Determines the index in the array where the lat and lon point are the LSM modules."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates Weight Table for Land Surface Model Grids", "response": "def rtree_create_weight_table(lsm_grid_lat, lsm_grid_lon,\n                              in_catchment_shapefile, river_id,\n                              in_rapid_connect, out_weight_table,\n                              file_geodatabase=None, area_id=None):\n    \"\"\"\n    Create Weight Table for Land Surface Model Grids\n    \"\"\"\n    time_start_all = datetime.utcnow()\n\n    if lsm_grid_lat.ndim == 3 and lsm_grid_lon.ndim == 3:\n        # assume first dimension is time\n        lsm_grid_lat = lsm_grid_lat[0]\n        lsm_grid_lon = lsm_grid_lon[0]\n\n    log(\"Generating LSM Grid Thiessen Array ...\")\n    if file_geodatabase:\n        gdb_driver = ogr.GetDriverByName(\"OpenFileGDB\")\n        ogr_file_geodatabase = gdb_driver.Open(file_geodatabase, 0)\n        ogr_catchment_shapefile_lyr = \\\n            ogr_file_geodatabase.GetLayer(in_catchment_shapefile)\n    else:\n        ogr_catchment_shapefile = ogr.Open(in_catchment_shapefile)\n        ogr_catchment_shapefile_lyr = ogr_catchment_shapefile.GetLayer()\n\n    ogr_catchment_shapefile_lyr_proj = \\\n        ogr_catchment_shapefile_lyr.GetSpatialRef()\n    original_catchment_proj = \\\n        Proj(ogr_catchment_shapefile_lyr_proj.ExportToProj4())\n    geographic_proj = Proj(init='EPSG:4326')\n    extent = ogr_catchment_shapefile_lyr.GetExtent()\n    if original_catchment_proj != geographic_proj:\n        x, y = transform(original_catchment_proj,\n                         geographic_proj,\n                         [extent[0], extent[1]],\n                         [extent[2], extent[3]])\n        extent = [min(x), max(x), min(y), max(y)]\n\n    lsm_grid_feature_list = \\\n        pointsToVoronoiGridArray(lsm_grid_lat, lsm_grid_lon, extent)\n\n#    ##COMMENTED LINES FOR TESTING\n#    import os\n#    from .voronoi import pointsToVoronoiGridShapefile\n#    vor_shp_path = \\\n#        os.path.join(os.path.dirname(in_catchment_shapefile), \"test_grid.shp\")\n#    pointsToVoronoiGridShapefile(lsm_grid_lat, lsm_grid_lon,\n#                                 vor_shp_path, extent)\n\n    time_end_lsm_grid_thiessen = datetime.utcnow()\n    log(time_end_lsm_grid_thiessen - time_start_all)\n\n    log(\"Generating LSM Grid Rtree ...\")\n    rtree_idx = rtree.index.Index()\n    # Populate R-tree index with bounds of ECMWF grid cells\n    for lsm_grid_pos, lsm_grid_feature in enumerate(lsm_grid_feature_list):\n        rtree_idx.insert(lsm_grid_pos, lsm_grid_feature['polygon'].bounds)\n\n    time_end_lsm_grid_rtree = datetime.utcnow()\n    log(time_end_lsm_grid_rtree - time_end_lsm_grid_thiessen)\n\n    log(\"Retrieving catchment river id list ...\")\n    number_of_catchment_features = \\\n        ogr_catchment_shapefile_lyr.GetFeatureCount()\n    catchment_rivid_list = \\\n        np.zeros(number_of_catchment_features, dtype=np.int32)\n    for feature_idx, catchment_feature in \\\n            enumerate(ogr_catchment_shapefile_lyr):\n        catchment_rivid_list[feature_idx] = \\\n            catchment_feature.GetField(river_id)\n\n    log(\"Reading in RAPID connect file ...\")\n    rapid_connect_rivid_list = np.loadtxt(in_rapid_connect,\n                                          delimiter=\",\",\n                                          usecols=(0,),\n                                          ndmin=1,\n                                          dtype=int)\n    log(\"Find LSM grid cells that intersect with each catchment\")\n    log(\"and write out weight table ...\")\n\n    dummy_lat_index, dummy_lon_index = \\\n        _get_lat_lon_indices(lsm_grid_lat,\n                             lsm_grid_lon,\n                             lsm_grid_feature_list[0]['lat'],\n                             lsm_grid_feature_list[0]['lon'])\n    dummy_row_end = [\n        0,\n        dummy_lon_index,\n        dummy_lat_index,\n        1,\n        lsm_grid_feature_list[0]['lon'],\n        lsm_grid_feature_list[0]['lat']\n    ]\n\n    with open_csv(out_weight_table, 'w') as csvfile:\n        connectwriter = csv.writer(csvfile)\n        connectwriter.writerow(['rivid', 'area_sqm', 'lon_index', 'lat_index',\n                                'npoints', 'lsm_grid_lon', 'lsm_grid_lat'])\n        geographic_proj = Proj(init='EPSG:4326')\n        osr_geographic_proj = osr.SpatialReference()\n        osr_geographic_proj.ImportFromEPSG(4326)\n        proj_transform = None\n        if original_catchment_proj != geographic_proj:\n            proj_transform = \\\n                osr.CoordinateTransformation(ogr_catchment_shapefile_lyr_proj,\n                                             osr_geographic_proj)\n\n        for rapid_connect_rivid in rapid_connect_rivid_list:\n            intersect_grid_info_list = []\n            try:\n                catchment_pos = \\\n                    np.where(catchment_rivid_list == rapid_connect_rivid)[0][0]\n            except IndexError:\n                # if it is not in the catchment, add dummy row in its place\n                connectwriter.writerow([rapid_connect_rivid] + dummy_row_end)\n                continue\n\n            get_catchment_feature = \\\n                ogr_catchment_shapefile_lyr.GetFeature(catchment_pos)\n            feat_geom = get_catchment_feature.GetGeometryRef()\n            # make sure coordinates are geographic\n            if proj_transform:\n                feat_geom.Transform(proj_transform)\n            catchment_polygon = shapely_loads(feat_geom.ExportToWkb())\n\n            for sub_lsm_grid_pos in \\\n                    rtree_idx.intersection(catchment_polygon.bounds):\n                lsm_grid_polygon = \\\n                    lsm_grid_feature_list[sub_lsm_grid_pos]['polygon']\n                if catchment_polygon.intersects(lsm_grid_polygon):\n                    try:\n                        intersect_poly = \\\n                            catchment_polygon.intersection(lsm_grid_polygon)\n                    except TopologicalError:\n                        log('The catchment polygon with id {0} was '\n                            'invalid. Attempting to self clean...'\n                            .format(rapid_connect_rivid))\n                        original_area = catchment_polygon.area\n                        catchment_polygon = catchment_polygon.buffer(0)\n                        area_ratio = original_area/catchment_polygon.area\n                        log('AREA_RATIO: {0}'.format(area_ratio))\n                        msg_level = \"INFO\"\n                        if round(area_ratio, 5) != 1:\n                            msg_level = \"WARNING\"\n                        log('The cleaned catchment polygon area '\n                            'differs from the original area by {1}%.'\n                            .format(abs(area_ratio - 1)), severity=msg_level)\n                        intersect_poly = \\\n                            catchment_polygon.intersection(lsm_grid_polygon)\n                    if not area_id:\n                        # attempt to calculate AREA\n                        poly_area = get_poly_area_geo(intersect_poly)\n                    else:\n                        poly_area = \\\n                            float(get_catchment_feature.GetField(area_id)) * \\\n                            intersect_poly.area/catchment_polygon.area\n\n                    index_lsm_grid_lat, index_lsm_grid_lon = \\\n                        _get_lat_lon_indices(\n                            lsm_grid_lat,\n                            lsm_grid_lon,\n                            lsm_grid_feature_list[sub_lsm_grid_pos]['lat'],\n                            lsm_grid_feature_list[sub_lsm_grid_pos]['lon'])\n                    intersect_grid_info_list.append({\n                        'rivid': rapid_connect_rivid,\n                        'area': poly_area,\n                        'lsm_grid_lat':\n                            lsm_grid_feature_list[sub_lsm_grid_pos]['lat'],\n                        'lsm_grid_lon':\n                            lsm_grid_feature_list[sub_lsm_grid_pos]['lon'],\n                        'index_lsm_grid_lon': index_lsm_grid_lon,\n                        'index_lsm_grid_lat': index_lsm_grid_lat\n                    })\n\n            npoints = len(intersect_grid_info_list)\n            # If no intersection found, add dummy row\n            if npoints <= 0:\n                connectwriter.writerow([rapid_connect_rivid] + dummy_row_end)\n\n            for intersect_grid_info in intersect_grid_info_list:\n                connectwriter.writerow([\n                    intersect_grid_info['rivid'],\n                    intersect_grid_info['area'],\n                    intersect_grid_info['index_lsm_grid_lon'],\n                    intersect_grid_info['index_lsm_grid_lat'],\n                    npoints,\n                    intersect_grid_info['lsm_grid_lon'],\n                    intersect_grid_info['lsm_grid_lat']\n                ])\n\n    time_end_all = datetime.utcnow()\n    log(time_end_all - time_end_lsm_grid_rtree)\n    log(\"TOTAL TIME: {0}\".format(time_end_all - time_start_all))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef CreateWeightTableECMWF(in_ecmwf_nc,\n                           in_catchment_shapefile,\n                           river_id,\n                           in_connectivity_file,\n                           out_weight_table,\n                           area_id=None,\n                           file_geodatabase=None):\n    \"\"\"\n    Create Weight Table for ECMWF Grids\n\n    .. note:: The grids are in the RAPIDpy package under\n              the gis/lsm_grids folder.\n\n    Parameters\n    ----------\n    in_ecmwf_nc: str\n        Path to the ECMWF NetCDF grid.\n    in_catchment_shapefile: str\n        Path to the Catchment shapefile.\n    river_id: str\n        The name of the field with the river ID (Ex. 'DrainLnID' or 'LINKNO').\n    in_connectivity_file: str\n        The path to the RAPID connectivity file.\n    out_weight_table: str\n        The path to the output weight table file.\n    area_id: str, optional\n        The name of the field with the area of each catchment stored in meters\n        squared. Default is it calculate the area.\n    file_geodatabase: str, optional\n        Path to the file geodatabase. If you use this option, in_drainage_line\n        is the name of the stream network feature class.\n        (WARNING: Not always stable with GDAL.)\n\n\n    Example:\n\n    .. code:: python\n\n        from RAPIDpy.gis.weight import CreateWeightTableECMWF\n\n        CreateWeightTableECMWF(\n            in_ecmwf_nc='/path/to/runoff_ecmwf_grid.nc'\n            in_catchment_shapefile='/path/to/catchment.shp',\n            river_id='LINKNO',\n            in_connectivity_file='/path/to/rapid_connect.csv',\n            out_weight_table='/path/to/ecmwf_weight.csv',\n        )\n\n    \"\"\"\n    # extract ECMWF GRID\n    data_ecmwf_nc = Dataset(in_ecmwf_nc)\n    variables_list = data_ecmwf_nc.variables.keys()\n    in_ecmwf_lat_var = 'lat'\n    if 'latitude' in variables_list:\n        in_ecmwf_lat_var = 'latitude'\n    in_ecmwf_lon_var = 'lon'\n    if 'longitude' in variables_list:\n        in_ecmwf_lon_var = 'longitude'\n\n    # convert [0, 360] to [-180, 180]\n    ecmwf_lon = \\\n        (data_ecmwf_nc.variables[in_ecmwf_lon_var][:] + 180) % 360 - 180\n    # assume [-90, 90]\n    ecmwf_lat = data_ecmwf_nc.variables[in_ecmwf_lat_var][:]\n    data_ecmwf_nc.close()\n\n    rtree_create_weight_table(ecmwf_lat, ecmwf_lon,\n                              in_catchment_shapefile, river_id,\n                              in_connectivity_file, out_weight_table,\n                              file_geodatabase, area_id)", "response": "Creates a weight table for the given ECMWF grid."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef CreateWeightTableLDAS(in_ldas_nc,\n                          in_nc_lon_var,\n                          in_nc_lat_var,\n                          in_catchment_shapefile,\n                          river_id,\n                          in_connectivity_file,\n                          out_weight_table,\n                          area_id=None,\n                          file_geodatabase=None):\n    \"\"\"\n    Create Weight Table for NLDAS, GLDAS grids as well as for 2D Joules,\n    or LIS Grids\n\n    Parameters\n    ----------\n    in_ldas_nc: str\n        Path to the land surface model NetCDF grid.\n    in_nc_lon_var: str\n        The variable name in the NetCDF file for the longitude.\n    in_nc_lat_var: str\n        The variable name in the NetCDF file for the latitude.\n    in_catchment_shapefile: str\n        Path to the Catchment shapefile.\n    river_id: str\n        The name of the field with the river ID (Ex. 'DrainLnID' or 'LINKNO').\n    in_connectivity_file: str\n        The path to the RAPID connectivity file.\n    out_weight_table: str\n        The path to the output weight table file.\n    area_id: str, optional\n        The name of the field with the area of each catchment stored in meters\n        squared. Default is it calculate the area.\n    file_geodatabase: str, optional\n        Path to the file geodatabase. If you use this option, in_drainage_line\n        is the name of the stream network feature class.\n        (WARNING: Not always stable with GDAL.)\n\n\n    Example:\n\n    .. code:: python\n\n        from RAPIDpy.gis.weight import CreateWeightTableLDAS\n\n        CreateWeightTableLDAS(\n            in_ldas_nc='/path/to/runoff_grid.nc',\n            in_nc_lon_var=\"lon_110\",\n            in_nc_lat_var=\"lat_110\",\n            in_catchment_shapefile='/path/to/catchment.shp',\n            river_id='LINKNO',\n            in_connectivity_file='/path/to/rapid_connect.csv',\n            out_weight_table='/path/to/ldas_weight.csv',\n        )\n    \"\"\"\n    # extract LDAS GRID\n    data_ldas_nc = Dataset(in_ldas_nc)\n    variables_list = data_ldas_nc.variables.keys()\n    if in_nc_lon_var not in variables_list:\n        raise Exception(\"Invalid longitude variable. Choose from: {0}\"\n                        .format(variables_list))\n    if in_nc_lat_var not in variables_list:\n        raise Exception(\"Invalid latitude variable. Choose from: {0}\"\n                        .format(variables_list))\n    ldas_lon = data_ldas_nc.variables[in_nc_lon_var][:]  # assume [-180, 180]\n    ldas_lat = data_ldas_nc.variables[in_nc_lat_var][:]  # assume [-90,90]\n    data_ldas_nc.close()\n\n    rtree_create_weight_table(ldas_lat, ldas_lon,\n                              in_catchment_shapefile, river_id,\n                              in_connectivity_file, out_weight_table,\n                              file_geodatabase, area_id)", "response": "Creates a weight table for NLDAS and LIS grids."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef generate_single_seasonal_average(args):\n    qout_file = args[0]\n    seasonal_average_file = args[1]\n    day_of_year = args[2]\n    mp_lock = args[3]\n\n    min_day = day_of_year - 3\n    max_day = day_of_year + 3\n\n    with RAPIDDataset(qout_file) as qout_nc_file:\n        time_indices = []\n        for idx, t in enumerate(qout_nc_file.get_time_array()):\n            var_time = gmtime(t)\n            compare_yday = var_time.tm_yday\n            # move day back one past because of leap year adds\n            # a day after feb 29 (day 60)\n            if isleap(var_time.tm_year) and compare_yday > 60:\n                compare_yday -= 1\n            # check if date within range of season\n            if max_day > compare_yday >= min_day:\n                time_indices.append(idx)\n\n        if not time_indices:\n            raise IndexError(\"No time steps found within range ...\")\n\n        streamflow_array = qout_nc_file.get_qout(time_index_array=time_indices)\n\n    avg_streamflow_array = np.mean(streamflow_array, axis=1)\n    std_streamflow_array = np.std(streamflow_array, axis=1)\n    max_streamflow_array = np.amax(streamflow_array, axis=1)\n    min_streamflow_array = np.min(streamflow_array, axis=1)\n\n    mp_lock.acquire()\n    seasonal_avg_nc = Dataset(seasonal_average_file, 'a')\n    seasonal_avg_nc.variables['average_flow'][:, day_of_year-1] = \\\n        avg_streamflow_array\n    seasonal_avg_nc.variables['std_dev_flow'][:, day_of_year-1] = \\\n        std_streamflow_array\n    seasonal_avg_nc.variables['max_flow'][:, day_of_year-1] = \\\n        max_streamflow_array\n    seasonal_avg_nc.variables['min_flow'][:, day_of_year-1] = \\\n        min_streamflow_array\n    seasonal_avg_nc.close()\n    mp_lock.release()", "response": "This function calculates the seasonal average for a single day of the year."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef generate_seasonal_averages(qout_file, seasonal_average_file,\n                               num_cpus=multiprocessing.cpu_count()):\n    \"\"\"\n    This function loops through a CF compliant rapid streamflow\n    file to produce a netCDF file with a seasonal average for\n    365 days a year\n    \"\"\"\n    with RAPIDDataset(qout_file) as qout_nc_file:\n        print(\"Generating seasonal average file ...\")\n        seasonal_avg_nc = Dataset(seasonal_average_file, 'w')\n\n        seasonal_avg_nc.createDimension('rivid', qout_nc_file.size_river_id)\n        seasonal_avg_nc.createDimension('day_of_year', 365)\n\n        time_series_var = seasonal_avg_nc.createVariable('rivid', 'i4',\n                                                         ('rivid',))\n        time_series_var.long_name = (\n            'unique identifier for each river reach')\n\n        average_flow_var = \\\n            seasonal_avg_nc.createVariable('average_flow', 'f8',\n                                           ('rivid', 'day_of_year'))\n        average_flow_var.long_name = 'seasonal average streamflow'\n        average_flow_var.units = 'm3/s'\n\n        std_dev_flow_var = \\\n            seasonal_avg_nc.createVariable('std_dev_flow', 'f8',\n                                           ('rivid', 'day_of_year'))\n        std_dev_flow_var.long_name = 'seasonal std. dev. streamflow'\n        std_dev_flow_var.units = 'm3/s'\n\n        std_dev_flow_var = \\\n            seasonal_avg_nc.createVariable('max_flow', 'f8',\n                                           ('rivid', 'day_of_year'))\n        std_dev_flow_var.long_name = 'seasonal max streamflow'\n        std_dev_flow_var.units = 'm3/s'\n\n        std_dev_flow_var = \\\n            seasonal_avg_nc.createVariable('min_flow', 'f8',\n                                           ('rivid', 'day_of_year'))\n        std_dev_flow_var.long_name = 'seasonal min streamflow'\n        std_dev_flow_var.units = 'm3/s'\n\n        lat_var = seasonal_avg_nc.createVariable('lat', 'f8', ('rivid',),\n                                                 fill_value=-9999.0)\n\n        lon_var = seasonal_avg_nc.createVariable('lon', 'f8', ('rivid',),\n                                                 fill_value=-9999.0)\n        add_latlon_metadata(lat_var, lon_var)\n\n        seasonal_avg_nc.variables['lat'][:] = \\\n            qout_nc_file.qout_nc.variables['lat'][:]\n        seasonal_avg_nc.variables['lon'][:] = \\\n            qout_nc_file.qout_nc.variables['lon'][:]\n\n        river_id_list = qout_nc_file.get_river_id_array()\n        seasonal_avg_nc.variables['rivid'][:] = river_id_list\n        seasonal_avg_nc.close()\n\n    # generate multiprocessing jobs\n    mp_lock = multiprocessing.Manager().Lock()  # pylint: disable=no-member\n    job_combinations = []\n    for day_of_year in range(1, 366):\n        job_combinations.append((qout_file,\n                                 seasonal_average_file,\n                                 day_of_year,\n                                 mp_lock\n                                 ))\n\n    pool = multiprocessing.Pool(num_cpus)\n    pool.map(generate_single_seasonal_average,\n             job_combinations)\n    pool.close()\n    pool.join()", "response": "This function loops through a CF compliant rapid streamflow file and generates a seasonal average for the first 365 days a year and a seasonal average for the second 365 days a year."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef open_shapefile(shapefile_path, file_geodatabase=None):\n    if file_geodatabase:\n        gdb_driver = ogr.GetDriverByName(\"OpenFileGDB\")\n        ogr_shapefile = gdb_driver.Open(file_geodatabase)\n        ogr_shapefile_lyr = ogr_shapefile.GetLayer(shapefile_path)\n    else:\n        ogr_shapefile = ogr.Open(shapefile_path)\n        ogr_shapefile_lyr = ogr_shapefile.GetLayer()\n    return ogr_shapefile_lyr, ogr_shapefile", "response": "Opens a shapefile using either a shapefile path or a file geodatabase"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting windows path to cygpath", "response": "def _get_cygwin_path(self, windows_path):\n        \"\"\"\n        Convert windows path to cygpath\n        \"\"\"\n        conv_cmd = [os.path.join(self._cygwin_bin_location, \"cygpath.exe\"),\n                    \"-u\", windows_path]\n        process = Popen(conv_cmd,\n                        stdout=PIPE, stderr=PIPE, shell=False)\n        out, err = process.communicate()\n        if err:\n            print(err)\n            raise Exception(err)\n\n        return out.strip()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _create_symlink_cygwin(self, initial_path, final_path):\n        symlink_cmd = [os.path.join(self._cygwin_bin_location, \"ln.exe\"),\n                       \"-s\", self._get_cygwin_path(initial_path),\n                       self._get_cygwin_path(final_path)]\n        process = Popen(symlink_cmd,\n                        stdout=PIPE, stderr=PIPE, shell=False)\n        out, err = process.communicate()\n        if err:\n            print(err)\n            raise Exception(err)\n\n        return out.strip()", "response": "Create a symbolic link using cygqin to generate a new file."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nuses cygwin to convert a DOS file to unix format", "response": "def _dos2unix_cygwin(self, file_path):\n        \"\"\"\n        Use cygwin to convert file to unix format\n        \"\"\"\n        dos2unix_cmd = \\\n            [os.path.join(self._cygwin_bin_location, \"dos2unix.exe\"),\n             self._get_cygwin_path(file_path)]\n        process = Popen(dos2unix_cmd,\n                        stdout=PIPE, stderr=PIPE, shell=False)\n        process.communicate()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update_parameters(self, **kwargs):\n        # set arguments based off of user input\n        for key, value in list(kwargs.items()):\n            if key in dir(self) and not key.startswith('_'):\n                setattr(self, key, value)\n            else:\n                log(\"Invalid RAPID parameter %s.\" % key,\n                    \"ERROR\")", "response": "Update the parameters of the current object with the values of the input parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_reach_number_data(self):\n        if not self.rapid_connect_file:\n            log(\"Missing rapid_connect_file. \"\n                \"Please set before running this function ...\",\n                \"ERROR\")\n\n        if not self.riv_bas_id_file:\n            log(\"Missing riv_bas_id_file. \"\n                \"Please set before running this function ...\",\n                \"ERROR\")\n\n        # get rapid connect info\n        rapid_connect_table = np.loadtxt(self.rapid_connect_file,\n                                         ndmin=2, delimiter=\",\", dtype=int)\n\n        self.IS_riv_tot = int(rapid_connect_table.shape[0])\n        self.IS_max_up = int(rapid_connect_table[:, 2].max())\n\n        # get riv_bas_id info\n        riv_bas_id_table = np.loadtxt(self.riv_bas_id_file,\n                                      ndmin=1, delimiter=\",\",\n                                      usecols=(0,), dtype=int)\n        self.IS_riv_bas = int(riv_bas_id_table.size)\n\n        # add the forcing files\n        if not self.for_tot_id_file:\n            self.IS_for_tot = 0\n            log(\"Missing for_tot_id_file. Skipping ...\",\n                \"WARNING\")\n        else:\n            # get riv_bas_id info\n            for_tot_id_table = np.loadtxt(self.for_tot_id_file,\n                                          ndmin=1, delimiter=\",\",\n                                          usecols=(0,), dtype=int)\n            self.IS_for_tot = int(for_tot_id_table.size)\n\n        if not self.for_use_id_file:\n            self.IS_for_use = 0\n            log(\"Missing for_use_id_file. Skipping ...\",\n                \"WARNING\")\n        else:\n            # get riv_bas_id info\n            for_use_id_table = np.loadtxt(self.for_use_id_file,\n                                          ndmin=1, delimiter=\",\",\n                                          usecols=(0,), dtype=int)\n            self.IS_for_use = int(for_use_id_table.size)", "response": "Update the reach number data for the namelist based on input files."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update_simulation_runtime(self):\n        if not self.Vlat_file or not os.path.exists(self.Vlat_file):\n            log(\"Need Vlat_file to proceed ...\",\n                \"ERROR\")\n\n        if self.ZS_TauR <= 0:\n            log(\"Missing routing time step ...\",\n                \"ERROR\")\n\n        try:\n            self.ZS_TauR = int(self.ZS_TauR)\n        except ValueError:\n            log(\"Invalid routing time step: {0} ...\".format(self.ZS_TauR),\n                \"ERROR\")\n\n        with RAPIDDataset(self.Vlat_file) as m3_nc:\n            self.ZS_TauM = m3_nc.size_time*self.ZS_TauR\n            self.ZS_TauO = m3_nc.size_time*self.ZS_TauR", "response": "Updates the total simulation duration from the total simulation duration of the current system."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef generate_namelist_file(self, rapid_namelist_file):\n        log(\"Generating RAPID namelist file ...\",\n            \"INFO\")\n        try:\n            os.remove(rapid_namelist_file)\n        except OSError:\n            pass\n\n        with open(rapid_namelist_file, 'w') as new_file:\n            new_file.write('&NL_namelist\\n')\n            for attr, value in sorted(list(self.__dict__.items())):\n                if not attr.startswith('_'):\n                    if attr.startswith('BS'):\n                        new_file.write(\"{0} = .{1}.\\n\"\n                                       .format(attr, str(value).lower()))\n                    elif isinstance(value, int):\n                        new_file.write(\"%s = %s\\n\" % (attr, value))\n                    else:\n                        if value:\n                            if os.name == \"nt\":\n                                # if windows generate file with cygpath\n                                value = self._get_cygwin_path(value)\n                            new_file.write(\"%s = \\'%s\\'\\n\" % (attr, value))\n            new_file.write(\"/\\n\")", "response": "Generate rapid_namelist file.\n\n        Parameters\n        ----------\n        rapid_namelist_file: str\n            Path of namelist file to generate from\n            parameters added to the RAPID manager."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_namelist_file(self, rapid_namelist_file,\n                             new_namelist_file=None):\n        \"\"\"\n        Update existing namelist file with new parameters\n\n        Parameters\n        ----------\n        rapid_namelist_file: str\n            Path of namelist file to use in the simulation. It will be\n            updated with any parameters added to the RAPID manager.\n        new_namelist_file: str, optional\n            Path to output the updated namelist file.\n        \"\"\"\n        if os.path.exists(rapid_namelist_file) and rapid_namelist_file:\n            log(\"Adding missing inputs from RAPID input file ...\",\n                \"INFO\")\n            with open(rapid_namelist_file, 'r') as old_file:\n                for line in old_file:\n                    line = line.strip()\n                    if not line[:1].isalpha() or not line:\n                        continue\n                    line_split = line.split(\"=\")\n                    attr = line_split[0].strip()\n                    value = None\n                    if len(line_split) > 1:\n                        value = line_split[1].strip()\\\n                            .replace(\"'\", \"\").replace('\"', \"\")\n                        # convert integers to integers\n                        try:\n                            value = int(value)\n                        except ValueError:\n                            pass\n                        # remove dots from beginning & end of value\n                        if attr.startswith('BS'):\n                            value = value.replace(\".\", \"\")\n                    # add attribute if exists\n                    if attr in dir(self) and not attr.startswith('_'):\n                        # set attribute if not set already\n                        if not getattr(self, attr):\n                            setattr(self, attr, value)\n                    else:\n                        log(\"Invalid argument {0}. Skipping ...\".format(attr),\n                            \"INFO\")\n\n            if new_namelist_file is None:\n                new_namelist_file = rapid_namelist_file\n\n            self.generate_namelist_file(new_namelist_file)\n        else:\n            log(\"RAPID namelist file to update not found.\",\n                \"ERROR\")", "response": "Update the namelist file with new parameters."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrunning RAPID program and generate file based on inputs This will generate your rapid_namelist file and run RAPID from wherever you call this script (your working directory). Parameters ---------- rapid_namelist_file: str, optional Path of namelist file to use in the simulation. It will be updated with any parameters added to the RAPID manager. Linux Example: .. code:: python from RAPIDpy import RAPID rapid_manager = RAPID( rapid_executable_location='~/work/rapid/src/rapid' use_all_processors=True, ) rapid_manager.update_parameters( rapid_connect_file='../rapid-io/input/rapid_connect.csv', Vlat_file='../rapid-io/input/m3_riv.nc', riv_bas_id_file='../rapid-io/input/riv_bas_id.csv', k_file='../rapid-io/input/k.csv', x_file='../rapid-io/input/x.csv', Qout_file='../rapid-io/output/Qout.nc', ) rapid_manager.update_reach_number_data() rapid_manager.update_simulation_runtime() rapid_manager.run( rapid_namelist_file='../rapid-io/input/rapid_namelist') Linux Reservoir Forcing Flows Example: .. code:: python from RAPIDpy import RAPID rapid_manager = RAPID( rapid_executable_location='~/work/rapid/src/rapid', num_processors=4, IS_for_tot=4, IS_for_use=4, for_tot_id_file='../rapid-io/input/dam_id.csv', for_use_id_file='../rapid-io/input/dam_id.csv', Qfor_file='../rapid-io/input/qout_dams.csv', ZS_dtF=86400, BS_opt_for=True, ) rapid_manager.run( rapid_namelist_file='../rapid-io/input/rapid_namelist_regular') Windows with Cygwin Example: .. code:: python from RAPIDpy import RAPID from os import path rapid_exe_path = 'C:/cygwin64/home/username/rapid/run/rapid', rapid_manager = RAPID( rapid_executable_location=rapid_exe_path, cygwin_bin_location='C:/cygwin64/bin', use_all_processors=True, ZS_TauR=24*3600, ZS_dtR=15*60, ZS_TauM=365*24*3600, ZS_dtM=24*3600 ) rapid_input = 'C:/cygwin64/home/username/rapid-io/input' rapid_output = 'C:/cygwin64/home/username/rapid-io/output' rapid_manager.update_parameters( rapid_connect_file=path.join(rapid_input, 'rapid_connect.csv'), Vlat_file=path.join(rapid_input, 'm3_riv.nc'), riv_bas_id_file=path.join(rapid_input, 'riv_bas_id.csv'), k_file=path.join(rapid_input, 'k.csv'), x_file=path.join(rapid_input, 'x.csv'), Qout_file=path.join(rapid_output, 'Qout.nc'), ) rapid_manager.update_reach_number_data() rapid_manager.update_simulation_runtime() rapid_manager.run()", "response": "def run(self, rapid_namelist_file=\"\"):\n        \"\"\"\n        Run RAPID program and generate file based on inputs\n        This will generate your rapid_namelist file and run RAPID from wherever\n        you call this script (your working directory).\n\n        Parameters\n        ----------\n        rapid_namelist_file: str, optional\n            Path of namelist file to use in the simulation.\n            It will be updated with any parameters added to the RAPID manager.\n\n\n        Linux Example:\n\n        .. code:: python\n\n            from RAPIDpy import RAPID\n\n            rapid_manager = RAPID(\n                rapid_executable_location='~/work/rapid/src/rapid'\n                use_all_processors=True,\n            )\n\n            rapid_manager.update_parameters(\n                rapid_connect_file='../rapid-io/input/rapid_connect.csv',\n                Vlat_file='../rapid-io/input/m3_riv.nc',\n                riv_bas_id_file='../rapid-io/input/riv_bas_id.csv',\n                k_file='../rapid-io/input/k.csv',\n                x_file='../rapid-io/input/x.csv',\n                Qout_file='../rapid-io/output/Qout.nc',\n            )\n\n            rapid_manager.update_reach_number_data()\n            rapid_manager.update_simulation_runtime()\n            rapid_manager.run(\n                rapid_namelist_file='../rapid-io/input/rapid_namelist')\n\n\n        Linux Reservoir Forcing Flows Example:\n\n        .. code:: python\n\n            from RAPIDpy import RAPID\n\n            rapid_manager = RAPID(\n                rapid_executable_location='~/work/rapid/src/rapid',\n                num_processors=4,\n                IS_for_tot=4,\n                IS_for_use=4,\n                for_tot_id_file='../rapid-io/input/dam_id.csv',\n                for_use_id_file='../rapid-io/input/dam_id.csv',\n                Qfor_file='../rapid-io/input/qout_dams.csv',\n                ZS_dtF=86400,\n                BS_opt_for=True,\n            )\n\n            rapid_manager.run(\n                rapid_namelist_file='../rapid-io/input/rapid_namelist_regular')\n\n        Windows with Cygwin Example:\n\n        .. code:: python\n\n            from RAPIDpy import RAPID\n            from os import path\n\n            rapid_exe_path = 'C:/cygwin64/home/username/rapid/run/rapid',\n            rapid_manager = RAPID(\n                rapid_executable_location=rapid_exe_path,\n                cygwin_bin_location='C:/cygwin64/bin',\n                use_all_processors=True,\n                ZS_TauR=24*3600,\n                ZS_dtR=15*60,\n                ZS_TauM=365*24*3600,\n                ZS_dtM=24*3600\n            )\n\n            rapid_input = 'C:/cygwin64/home/username/rapid-io/input'\n            rapid_output = 'C:/cygwin64/home/username/rapid-io/output'\n            rapid_manager.update_parameters(\n                rapid_connect_file=path.join(rapid_input, 'rapid_connect.csv'),\n                Vlat_file=path.join(rapid_input, 'm3_riv.nc'),\n                riv_bas_id_file=path.join(rapid_input, 'riv_bas_id.csv'),\n                k_file=path.join(rapid_input, 'k.csv'),\n                x_file=path.join(rapid_input, 'x.csv'),\n                Qout_file=path.join(rapid_output, 'Qout.nc'),\n            )\n\n            rapid_manager.update_reach_number_data()\n            rapid_manager.update_simulation_runtime()\n            rapid_manager.run()\n        \"\"\"\n        if not self._rapid_executable_location:\n            log(\"Missing rapid_executable_location. \"\n                \"Please set before running this function ...\",\n                \"ERROR\")\n\n        time_start = datetime.datetime.utcnow()\n        temp_rapid_namelist_file = os.path.join(os.getcwd(), \"rapid_namelist\")\n\n        if not rapid_namelist_file or not os.path.exists(rapid_namelist_file):\n            # generate input file if it does not exist\n            self.generate_namelist_file(temp_rapid_namelist_file)\n        else:\n            # update existing file\n            self.update_namelist_file(rapid_namelist_file,\n                                      temp_rapid_namelist_file)\n\n        local_rapid_executable_location = \\\n            os.path.join(os.path.dirname(temp_rapid_namelist_file),\n                         \"rapid_exe_symlink\")\n\n        def rapid_cleanup(*args):\n            \"\"\"\n            Cleans up the rapid files generated by the process\n            \"\"\"\n            for arg in args:\n                # remove files\n                try:\n                    os.remove(arg)\n                except OSError:\n                    pass\n\n        # create link to RAPID if needed\n        temp_link_to_rapid = \"\"\n        # pylint: disable=no-member\n        if self._rapid_executable_location != \\\n                local_rapid_executable_location:\n            rapid_cleanup(local_rapid_executable_location)\n            if os.name == \"nt\":\n                self._create_symlink_cygwin(self._rapid_executable_location,\n                                            local_rapid_executable_location)\n            else:\n                os.symlink(self._rapid_executable_location,\n                           local_rapid_executable_location)\n            temp_link_to_rapid = local_rapid_executable_location\n\n        # run RAPID\n        log(\"Running RAPID ...\",\n            \"INFO\")\n        if os.name == \"nt\":\n            local_rapid_executable_location = \\\n                self._get_cygwin_path(local_rapid_executable_location)\n\n        # htcondor will not allow mpiexec for single processor jobs\n        # this was added for that purpose\n        run_rapid_command = [local_rapid_executable_location,\n                             \"-ksp_type\", self._ksp_type]\n\n        if self._num_processors > 1:\n            run_rapid_command = [self._mpiexec_command,\n                                 \"-n\", str(self._num_processors)] \\\n                                + run_rapid_command\n\n        process = Popen(run_rapid_command,\n                        stdout=PIPE, stderr=PIPE, shell=False)\n        out, err = process.communicate()\n        if err:\n            rapid_cleanup(temp_link_to_rapid, temp_rapid_namelist_file)\n            raise Exception(err)\n        else:\n            log('RAPID output:',\n                \"INFO\")\n            for line in out.split(b'\\n'):\n                print(line)\n        rapid_cleanup(temp_link_to_rapid, temp_rapid_namelist_file)\n        log(\"Time to run RAPID: %s\" % (datetime.datetime.utcnow()-time_start),\n            \"INFO\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates a qinit file from a RAPID qout file.", "response": "def generate_qinit_from_past_qout(self, qinit_file, time_index=-1,\n                                      out_datetime=None):\n        \"\"\"\n        Generate qinit from a RAPID qout file\n\n        Parameters\n        ----------\n        qinit_file: str\n            Path to output qinit_file.\n        time_index: int, optional\n            Index of simulation to generate initial flow file.\n            Default is the last index.\n        out_datetime: :obj:`datetime.datetime`, optional\n            Datetime object containing time of initialization.\n\n\n        Example:\n\n        .. code:: python\n\n            from RAPIDpy import RAPID\n\n            rapid_manager = RAPID(\n                Qout_file='/output_mississippi-nfie/Qout_k2v1_2005to2009.nc',\n                rapid_connect_file='/input_mississippi_nfie/rapid_connect.csv'\n            )\n\n            rapid_manager.generate_qinit_from_past_qout(\n                qinit_file='/input_mississippi_nfie/Qinit_2008_flood.csv',\n                time_index=10162\n            )\n\n        \"\"\"\n        if not self.Qout_file or not os.path.exists(self.Qout_file):\n            log('Missing Qout_file. '\n                'Please set before running this function ...',\n                \"ERROR\")\n\n        if not self.rapid_connect_file or not self.rapid_connect_file:\n            log('Missing rapid_connect file. '\n                'Please set before running this function ...',\n                \"ERROR\")\n\n        log(\"Generating qinit file from qout file ...\",\n            \"INFO\")\n        # get information from dataset\n        with xarray.open_dataset(self.Qout_file) as qds:\n            rivid_array = qds.rivid.values\n            if out_datetime is None:\n                streamflow_values = qds.isel(time=time_index).Qout.values\n            else:\n                streamflow_values = qds.sel(time=str(out_datetime)).Qout.values\n\n        log(\"Reordering data ...\",\n            \"INFO\")\n\n        stream_id_array = np.loadtxt(self.rapid_connect_file,\n                                     ndmin=1, delimiter=\",\",\n                                     usecols=(0,), dtype=int)\n        init_flows_array = np.zeros(stream_id_array.size)\n        for riv_bas_index, riv_bas_id in enumerate(rivid_array):\n            try:\n                data_index = np.where(stream_id_array == riv_bas_id)[0][0]\n                init_flows_array[data_index] = streamflow_values[riv_bas_index]\n            except IndexError:\n                log('riv bas id {0} not found in connectivity list.'\n                    .format(riv_bas_id),\n                    \"WARNING\")\n\n        log(\"Writing to file ...\",\n            \"INFO\")\n        with open_csv(qinit_file, 'w') as qinit_out:\n            for init_flow in init_flows_array:\n                qinit_out.write('{0}\\n'.format(init_flow))\n\n        self.Qinit_file = qinit_file\n        self.BS_opt_Qinit = True\n        log(\"Initialization Complete!\",\n            \"INFO\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate daily streamflow file and stream id file required for calibration or for substituting flows based on USGS gage ids associated with stream ids. Parameters ---------- reach_id_gage_id_file: str Path to reach_id_gage_id file. start_datetime: datetime A datetime object with the start date to download data. end_datetime: datetime A datetime object with the end date to download data. out_streamflow_file: str The path to output the streamflow file for RAPID. out_stream_id_file: str The path to output the stream ID file associated with the streamflow file for RAPID. Example *reach_id_gage_id_file*:: COMID, USGS_GAGE_ID 2000, 503944 ... .. warning:: Overuse will get you blocked from downloading data from USGS. .. warning:: This code does not clean the data in any way. Thus, you are likely to run into issues if you simply use the raw data. .. warning:: The code skips gages that do not have data for the entire time period. Simple Example: .. code:: python import datetime from os.path import join from RAPIDpy import RAPID main_path = \"/home/username/data\" rapid_manager = RAPID() rapid_manager.generate_usgs_avg_daily_flows_opt( reach_id_gage_id_file=join(main_path, \"usgsgage_id_comid.csv\"), start_datetime=datetime.datetime(2000,1,1), end_datetime=datetime.datetime(2014,12,31), out_streamflow_file=join(main_path,\"streamflow_2000_2014.csv\"), out_stream_id_file=join(main_path,\"streamid_2000_2014.csv\") ) Complex Example: .. code:: python import datetime from os.path import join from RAPIDpy import RAPID main_path = \"/home/username/data\" rapid_manager = RAPID( rapid_executable_location='~/work/rapid/run/rapid' use_all_processors=True, ZS_TauR=24*3600, ZS_dtR=15*60, ZS_TauM=365*24*3600, ZS_dtM=24*3600 ) rapid_manager.update_parameters( rapid_connect_file='../rapid-io/input/rapid_connect.csv', Vlat_file='../rapid-io/input/m3_riv.nc', riv_bas_id_file='../rapid-io/input/riv_bas_id.csv', k_file='../rapid-io/input/k.csv', x_file='../rapid-io/input/x.csv', Qout_file='../rapid-io/output/Qout.nc', ) rapid_manager.update_reach_number_data() rapid_manager.update_simulation_runtime() rapid_manager.generate_usgs_avg_daily_flows_opt( reach_id_gage_id_file=join(main_path, \"usgsgage_id_comid.csv\"), start_datetime=datetime.datetime(2000,1,1), end_datetime=datetime.datetime(2014,12,31), out_streamflow_file=join(main_path,\"streamflow_2000_2014.csv\"), out_stream_id_file=join(main_path,\"streamid_2000_2014.csv\") ) rapid_manager.run()", "response": "def generate_usgs_avg_daily_flows_opt(self,\n                                          reach_id_gage_id_file,\n                                          start_datetime,\n                                          end_datetime,\n                                          out_streamflow_file,\n                                          out_stream_id_file):\n        \"\"\"\n        Generate daily streamflow file and stream id file required for\n        calibration or for substituting flows based on USGS gage ids\n        associated with stream ids.\n\n        Parameters\n        ----------\n        reach_id_gage_id_file: str\n            Path to reach_id_gage_id file.\n        start_datetime: datetime\n            A datetime object with the start date to download data.\n        end_datetime: datetime\n            A datetime object with the end date to download data.\n        out_streamflow_file: str\n            The path to output the streamflow file for RAPID.\n        out_stream_id_file: str\n            The path to output the stream ID file associated with the\n            streamflow file for RAPID.\n\n\n        Example *reach_id_gage_id_file*::\n\n            COMID, USGS_GAGE_ID\n            2000, 503944\n            ...\n\n        .. warning:: Overuse will get you blocked from downloading data from\n                     USGS.\n\n        .. warning:: This code does not clean the data in any way. Thus, you\n                     are likely to run into issues if you simply use the raw\n                     data.\n\n        .. warning:: The code skips gages that do not have data\n                     for the entire time period.\n\n\n        Simple Example:\n\n        .. code:: python\n\n            import datetime\n            from os.path import join\n            from RAPIDpy import RAPID\n\n            main_path = \"/home/username/data\"\n\n            rapid_manager = RAPID()\n            rapid_manager.generate_usgs_avg_daily_flows_opt(\n                reach_id_gage_id_file=join(main_path, \"usgsgage_id_comid.csv\"),\n                start_datetime=datetime.datetime(2000,1,1),\n                end_datetime=datetime.datetime(2014,12,31),\n                out_streamflow_file=join(main_path,\"streamflow_2000_2014.csv\"),\n                out_stream_id_file=join(main_path,\"streamid_2000_2014.csv\")\n            )\n\n\n        Complex Example:\n\n        .. code:: python\n\n            import datetime\n            from os.path import join\n            from RAPIDpy import RAPID\n\n            main_path = \"/home/username/data\"\n\n            rapid_manager = RAPID(\n                rapid_executable_location='~/work/rapid/run/rapid'\n                use_all_processors=True,\n                ZS_TauR=24*3600,\n                ZS_dtR=15*60,\n                ZS_TauM=365*24*3600,\n                ZS_dtM=24*3600\n            )\n\n            rapid_manager.update_parameters(\n                rapid_connect_file='../rapid-io/input/rapid_connect.csv',\n                Vlat_file='../rapid-io/input/m3_riv.nc',\n                riv_bas_id_file='../rapid-io/input/riv_bas_id.csv',\n                k_file='../rapid-io/input/k.csv',\n                x_file='../rapid-io/input/x.csv',\n                Qout_file='../rapid-io/output/Qout.nc',\n            )\n\n            rapid_manager.update_reach_number_data()\n            rapid_manager.update_simulation_runtime()\n            rapid_manager.generate_usgs_avg_daily_flows_opt(\n                reach_id_gage_id_file=join(main_path, \"usgsgage_id_comid.csv\"),\n                start_datetime=datetime.datetime(2000,1,1),\n                end_datetime=datetime.datetime(2014,12,31),\n                out_streamflow_file=join(main_path,\"streamflow_2000_2014.csv\"),\n                out_stream_id_file=join(main_path,\"streamid_2000_2014.csv\")\n                )\n            rapid_manager.run()\n\n        \"\"\"\n        log(\"Generating avg streamflow file and stream id file \"\n            \"required for calibration ...\",\n            \"INFO\")\n        log(\"Generating avg streamflow file and stream id file \"\n            \"required for calibration ...\",\n            \"INFO\")\n        reach_id_gage_id_list = csv_to_list(reach_id_gage_id_file)\n        gage_data_matrix = []\n        valid_comid_list = []\n\n        # add extra day as it includes the start date\n        # (e.g. 7-5 is 2 days, but have data for 5,6,7, so +1)\n        num_days_needed = (end_datetime-start_datetime).days + 1\n\n        gage_id_list = []\n        for row in reach_id_gage_id_list[1:]:\n            station_id = row[1]\n            if len(row[1]) == 7:\n                station_id = '0' + row[1]\n            gage_id_list.append(station_id)\n\n        num_gage_id_list = np.array(gage_id_list, dtype=np.int32)\n        log(\"Querying Server for Data ...\",\n            \"INFO\")\n\n        query_params = {\n                        'format': 'json',\n                        'sites': \",\".join(gage_id_list),\n                        'startDT': start_datetime.strftime(\"%Y-%m-%d\"),\n                        'endDT': end_datetime.strftime(\"%Y-%m-%d\"),\n                        'parameterCd': '00060',  # streamflow\n                        'statCd': '00003'  # average\n                       }\n        response = get(\"http://waterservices.usgs.gov/nwis/dv\",\n                       params=query_params)\n\n        if not response.ok:\n            log(\"USGS query error ...\",\n                \"WARNING\")\n            return\n\n        requested_data = None\n        try:\n            requested_data = response.json()['value']['timeSeries']\n        except IndexError:\n            pass\n\n        if requested_data is not None:\n            for time_series in enumerate(requested_data):\n                usgs_station_full_name = time_series[1]['name']\n                usgs_station_id = usgs_station_full_name.split(\":\")[1]\n                gage_data = []\n                for time_step in time_series[1]['values'][0]['value']:\n                    local_datetime = parse(time_step['dateTime'])\n                    if local_datetime > end_datetime:\n                        break\n\n                    if local_datetime >= start_datetime:\n                        if not time_step['value']:\n                            log(\"MISSING DATA for USGS Station {0} {1} {2}\"\n                                .format(usgs_station_id,\n                                        local_datetime,\n                                        time_step['value']),\n                                \"WARNING\")\n                        gage_data.append(\n                            float(time_step['value']) / 35.3146667)\n\n                try:\n                    # get where streamids associated with USGS station ID\n                    streamid_index = \\\n                        np.where(num_gage_id_list ==\n                                 int(float(usgs_station_id)))[0][0]+1\n                except (IndexError, ValueError):\n                    log(\"USGS Station {0} not found in list ...\"\n                        .format(usgs_station_id),\n                        \"WARNING\")\n                    raise\n\n                if len(gage_data) == num_days_needed:\n                    gage_data_matrix.append(gage_data)\n                    valid_comid_list.append(\n                        reach_id_gage_id_list[streamid_index][0])\n                else:\n                    log(\"StreamID {0} USGS Station {1} MISSING {2} \"\n                        \"DATA VALUES\".format(\n                            reach_id_gage_id_list[streamid_index][0],\n                            usgs_station_id,\n                            num_days_needed-len(gage_data)),\n                        \"WARNING\")\n\n        if gage_data_matrix and valid_comid_list:\n            log(\"Writing Output ...\",\n                \"INFO\")\n            np_array = np.array(gage_data_matrix).transpose()\n            with open_csv(out_streamflow_file, 'w') as gage_data:\n                wgd = csvwriter(gage_data)\n                for row in np_array:\n                    wgd.writerow(row)\n\n            with open_csv(out_stream_id_file, 'w') as comid_data:\n                wcd = csvwriter(comid_data)\n                for row in valid_comid_list:\n                    wcd.writerow([int(float(row))])\n\n            # set parameters for RAPID run\n            self.IS_obs_tot = len(valid_comid_list)\n            self.obs_tot_id_file = out_stream_id_file\n            self.Qobs_file = out_streamflow_file\n            self.IS_obs_use = len(valid_comid_list)\n            self.obs_use_id_file = out_stream_id_file\n        else:\n            log(\"No valid data returned ...\",\n                \"WARNING\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts a flowline feature to a list of centroid points with their rivid in EPSG 4326.", "response": "def FlowlineToPoint(in_drainage_line,\n                    river_id,\n                    out_csv_file,\n                    file_geodatabase=None):\n    \"\"\"\n    Converts flowline feature to a list of centroid points with their rivid\n    in EPSG:4326.\n\n    Parameters\n    ----------\n    in_drainage_line: str\n        Path to the stream network (i.e. Drainage Line) shapefile.\n    river_id: str\n        The name of the field with the river ID\n        (Ex. 'HydroID', 'COMID', or 'LINKNO').\n    out_csv_file: str\n        Path to the output csv file with the centroid points.\n    file_geodatabase: str, optional\n        Path to the file geodatabase. If you use this option, in_drainage_line\n         is the name of the stream network feature class\n         (WARNING: Not always stable with GDAL).\n\n\n    Example::\n\n        from RAPIDpy.gis.centroid import FlowlineToPoint\n\n        FlowlineToPoint(\n            in_drainage_line='/path/to/drainageline.shp',\n            river_id='LINKNO',\n            out_csv_file='/path/to/comid_lat_lon_z.csv')\n\n    \"\"\"\n    ogr_drainage_line_shapefile_lyr, ogr_drainage_line_shapefile = \\\n        open_shapefile(in_drainage_line, file_geodatabase)\n\n    ogr_drainage_line_shapefile_lyr_proj = \\\n        ogr_drainage_line_shapefile_lyr.GetSpatialRef()\n    osr_geographic_proj = osr.SpatialReference()\n    osr_geographic_proj.ImportFromEPSG(4326)\n    proj_transform = None\n    if ogr_drainage_line_shapefile_lyr_proj != osr_geographic_proj:\n        proj_transform = osr.CoordinateTransformation(\n            ogr_drainage_line_shapefile_lyr_proj, osr_geographic_proj)\n\n    # print valid field names to table\n    with open_csv(out_csv_file, 'w') as outfile:\n        writer = csv_writer(outfile)\n        writer.writerow(['rivid', 'lat', 'lon', 'z'])\n        for feature in ogr_drainage_line_shapefile_lyr:\n            feat_geom = feature.GetGeometryRef()\n            if proj_transform:\n                feat_geom.Transform(proj_transform)\n            centroid = feat_geom.Centroid()\n            centroid_pt = centroid.GetPoint(0)\n            writer.writerow([\n                feature.GetField(river_id),\n                centroid_pt[1],\n                centroid_pt[0],\n                centroid_pt[2]\n            ])\n\n    del ogr_drainage_line_shapefile"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read_in_weight_table(self, in_weight_table):\r\n        print(\"Reading the weight table...\")\r\n        with open_csv(in_weight_table, \"r\") as csvfile:\r\n            reader = csv.reader(csvfile)\r\n            header_row = next(reader)\r\n            # check number of columns in the weight table\r\n            if len(header_row) < len(self.header_wt):\r\n                raise Exception(self.error_messages[4])\r\n            # check header\r\n            if header_row[1:len(self.header_wt)] != self.header_wt[1:]:\r\n                raise Exception(self.error_messages[5])\r\n\r\n        self.dict_list = \\\r\n            np.loadtxt(\r\n                in_weight_table,\r\n                delimiter=\",\",\r\n                usecols=(0, 1, 2, 3, 4),\r\n                skiprows=1,\r\n                dtype={\r\n                    'names': (self.header_wt[0],\r\n                              self.header_wt[1],\r\n                              self.header_wt[2],\r\n                              self.header_wt[3],\r\n                              self.header_wt[4]),\r\n                    'formats': ('i8', 'f8', 'i8', 'i8', 'i8')\r\n                },\r\n            )\r\n\r\n        self.count = self.dict_list.shape[0]\r\n        self.size_stream_id = \\\r\n            len(np.unique(np.array(self.dict_list[self.header_wt[0]],\r\n                                   dtype=np.int32)))", "response": "Reads in the weight table and populates the internal attributes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _write_lat_lon(data_out_nc, rivid_lat_lon_z_file):\r\n        # only add if user adds\r\n        if rivid_lat_lon_z_file and os.path.exists(rivid_lat_lon_z_file):\r\n            # get list of COMIDS\r\n            lookup_table = np.loadtxt(\r\n                rivid_lat_lon_z_file,\r\n                delimiter=\",\",\r\n                usecols=(0, 1, 2),\r\n                skiprows=1,\r\n                dtype={\r\n                    'names': ('rivid', 'lat', 'lon'),\r\n                    'formats': ('i8', 'f8', 'f8'),\r\n                },\r\n            )\r\n\r\n            # Get relevant arrays while we update them\r\n            nc_rivids = data_out_nc.variables['rivid'][:]\r\n            lats = data_out_nc.variables['lat'][:]\r\n            lons = data_out_nc.variables['lon'][:]\r\n\r\n            lat_min = None\r\n            lat_max = None\r\n            lon_min = None\r\n            lon_max = None\r\n\r\n            # Process each row in the lookup table\r\n            for nc_index, nc_rivid in enumerate(nc_rivids):\r\n                try:\r\n                    lookup_index = \\\r\n                        np.where(lookup_table['rivid'] == nc_rivid)[0][0]\r\n                except Exception:\r\n                    raise Exception('rivid {0} misssing in '\r\n                                    'comid_lat_lon_z file'.format(nc_rivid))\r\n\r\n                lat = float(lookup_table['lat'][lookup_index])\r\n                lats[nc_index] = lat\r\n                if lat_min is None or lat < lat_min:\r\n                    lat_min = lat\r\n                if lat_max is None or lat > lat_max:\r\n                    lat_max = lat\r\n\r\n                lon = float(lookup_table['lon'][lookup_index])\r\n                lons[nc_index] = lon\r\n                if lon_min is None or lon < lon_min:\r\n                    lon_min = lon\r\n                if lon_max is None or lon > lon_max:\r\n                    lon_max = lon\r\n\r\n            # Overwrite netCDF variable values\r\n            data_out_nc.variables['lat'][:] = lats\r\n            data_out_nc.variables['lon'][:] = lons\r\n\r\n            # Update metadata\r\n            if lat_min is not None:\r\n                data_out_nc.geospatial_lat_min = lat_min\r\n            if lat_max is not None:\r\n                data_out_nc.geospatial_lat_max = lat_max\r\n            if lon_min is not None:\r\n                data_out_nc.geospatial_lon_min = lon_min\r\n            if lon_max is not None:\r\n                data_out_nc.geospatial_lon_max = lon_max\r\n        else:\r\n            print('No comid_lat_lon_z file. Not adding values ...')", "response": "Add latitude and longitude each netCDF feature with rivid Lat and Lon columns."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating the inflow file for the RAPID simulation", "response": "def generateOutputInflowFile(self,\r\n                                 out_nc,\r\n                                 start_datetime_utc,\r\n                                 number_of_timesteps,\r\n                                 simulation_time_step_seconds,\r\n                                 in_rapid_connect_file,\r\n                                 in_rivid_lat_lon_z_file,\r\n                                 land_surface_model_description,\r\n                                 modeling_institution\r\n                                 ):\r\n        \"\"\"\r\n        Generate inflow file for RAPID\r\n        \"\"\"\r\n        self.simulation_time_step_seconds = simulation_time_step_seconds\r\n\r\n        # Create output inflow netcdf data\r\n        print(\"Generating inflow file ...\")\r\n        data_out_nc = Dataset(out_nc, \"w\", format=\"NETCDF3_CLASSIC\")\r\n        rivid_list = np.loadtxt(in_rapid_connect_file,\r\n                                delimiter=\",\",\r\n                                ndmin=1,\r\n                                usecols=(0,),\r\n                                dtype=int)\r\n        # create dimensions\r\n        data_out_nc.createDimension('time', number_of_timesteps)\r\n        data_out_nc.createDimension('rivid', len(rivid_list))\r\n        data_out_nc.createDimension('nv', 2)\r\n        # create variables\r\n        # m3_riv\r\n        m3_riv_var = data_out_nc.createVariable('m3_riv', 'f4',\r\n                                                ('time', 'rivid'),\r\n                                                fill_value=0)\r\n        m3_riv_var.long_name = 'accumulated external water volume ' \\\r\n                               'inflow upstream of each river reach'\r\n        m3_riv_var.units = 'm3'\r\n        m3_riv_var.coordinates = 'lon lat'\r\n        m3_riv_var.grid_mapping = 'crs'\r\n        m3_riv_var.cell_methods = \"time: sum\"\r\n        data_out_nc.close()\r\n\r\n        try:\r\n            data_out_nc = Dataset(out_nc, \"a\", format=\"NETCDF3_CLASSIC\")\r\n            # rivid\r\n            rivid_var = data_out_nc.createVariable('rivid', 'i4',\r\n                                                   ('rivid',))\r\n            rivid_var.long_name = 'unique identifier for each river reach'\r\n            rivid_var.units = '1'\r\n            rivid_var.cf_role = 'timeseries_id'\r\n            rivid_var[:] = rivid_list\r\n\r\n            # time\r\n            time_var = data_out_nc.createVariable('time', 'i4',\r\n                                                  ('time',))\r\n            time_var.long_name = 'time'\r\n            time_var.standard_name = 'time'\r\n            time_var.units = 'seconds since 1970-01-01 00:00:00+00:00'\r\n            time_var.axis = 'T'\r\n            time_var.calendar = 'gregorian'\r\n            time_var.bounds = 'time_bnds'\r\n\r\n            initial_time_seconds = \\\r\n                (start_datetime_utc.replace(tzinfo=utc) -\r\n                 datetime(1970, 1, 1, tzinfo=utc)).total_seconds()\r\n            final_time_seconds = \\\r\n                initial_time_seconds + number_of_timesteps\\\r\n                * simulation_time_step_seconds\r\n            time_array = np.arange(initial_time_seconds, final_time_seconds,\r\n                                   simulation_time_step_seconds)\r\n            time_var[:] = time_array\r\n\r\n            # time_bnds\r\n            time_bnds_var = data_out_nc.createVariable('time_bnds', 'i4',\r\n                                                       ('time', 'nv',))\r\n            for time_index, time_element in enumerate(time_array):\r\n                time_bnds_var[time_index, 0] = time_element\r\n                time_bnds_var[time_index, 1] = \\\r\n                    time_element + simulation_time_step_seconds\r\n\r\n            # longitude\r\n            lon_var = data_out_nc.createVariable('lon', 'f8', ('rivid',),\r\n                                                 fill_value=-9999.0)\r\n            lon_var.long_name = \\\r\n                'longitude of a point related to each river reach'\r\n            lon_var.standard_name = 'longitude'\r\n            lon_var.units = 'degrees_east'\r\n            lon_var.axis = 'X'\r\n\r\n            # latitude\r\n            lat_var = data_out_nc.createVariable('lat', 'f8', ('rivid',),\r\n                                                 fill_value=-9999.0)\r\n            lat_var.long_name = \\\r\n                'latitude of a point related to each river reach'\r\n            lat_var.standard_name = 'latitude'\r\n            lat_var.units = 'degrees_north'\r\n            lat_var.axis = 'Y'\r\n\r\n            crs_var = data_out_nc.createVariable('crs', 'i4')\r\n            crs_var.grid_mapping_name = 'latitude_longitude'\r\n            crs_var.epsg_code = 'EPSG:4326'  # WGS 84\r\n            crs_var.semi_major_axis = 6378137.0\r\n            crs_var.inverse_flattening = 298.257223563\r\n\r\n            # add global attributes\r\n            data_out_nc.Conventions = 'CF-1.6'\r\n            data_out_nc.title = 'RAPID Inflow from {0}'\\\r\n                .format(land_surface_model_description)\r\n            data_out_nc.history = 'date_created: {0}'\\\r\n                .format(datetime.utcnow().replace(tzinfo=utc))\r\n            data_out_nc.featureType = 'timeSeries'\r\n            data_out_nc.institution = modeling_institution\r\n\r\n            # write lat lon data\r\n            self._write_lat_lon(data_out_nc, in_rivid_lat_lon_z_file)\r\n\r\n            # close file\r\n            data_out_nc.close()\r\n        except RuntimeError:\r\n            print(\"File size too big to add data beforehand.\"\r\n                  \" Performing conversion after ...\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexecutes the main method of the tool.", "response": "def execute(self, nc_file_list, index_list, in_weight_table,\r\n                out_nc, grid_type, mp_lock):\r\n\r\n        \"\"\"The source code of the tool.\"\"\"\r\n        if not os.path.exists(out_nc):\r\n            raise Exception(\"Outfile has not been created. \"\r\n                            \"You need to run: generateOutputInflowFile \"\r\n                            \"function ...\")\r\n\r\n        if len(nc_file_list) != len(index_list):\r\n            raise Exception(\"ERROR: Number of runoff files not equal to \"\r\n                            \"number of indices ...\")\r\n\r\n        demo_file_list = nc_file_list[0]\r\n        if not isinstance(nc_file_list[0], list):\r\n            demo_file_list = [demo_file_list]\r\n\r\n        self.data_validation(demo_file_list[0])\r\n        self.read_in_weight_table(in_weight_table)\r\n\r\n        conversion_factor = self.get_conversion_factor(demo_file_list[0],\r\n                                                       len(demo_file_list))\r\n\r\n        # get indices of subset of data\r\n        lon_ind_all = [int(i) for i in self.dict_list[self.header_wt[2]]]\r\n        lat_ind_all = [int(j) for j in self.dict_list[self.header_wt[3]]]\r\n\r\n        # Obtain a subset of  runoff data based on the indices in the\r\n        # weight table\r\n        min_lon_ind_all = min(lon_ind_all)\r\n        max_lon_ind_all = max(lon_ind_all)\r\n        min_lat_ind_all = min(lat_ind_all)\r\n        max_lat_ind_all = max(lat_ind_all)\r\n        lon_slice = slice(min_lon_ind_all, max_lon_ind_all + 1)\r\n        lat_slice = slice(min_lat_ind_all, max_lat_ind_all + 1)\r\n        index_new = []\r\n\r\n        # combine inflow data\r\n        for nc_file_array_index, nc_file_array in enumerate(nc_file_list):\r\n\r\n            index = index_list[nc_file_array_index]\r\n\r\n            if not isinstance(nc_file_array, list):\r\n                nc_file_array = [nc_file_array]\r\n\r\n            data_subset_all = None\r\n            for nc_file in nc_file_array:\r\n                # Validate the netcdf dataset\r\n                self.data_validation(nc_file)\r\n\r\n                # Read the netcdf dataset\r\n                data_in_nc = Dataset(nc_file)\r\n\r\n                # Calculate water inflows\r\n                runoff_dimension_size = \\\r\n                    len(data_in_nc.variables[self.runoff_vars[0]].dimensions)\r\n                if runoff_dimension_size == 2:\r\n                    # obtain subset of surface and subsurface runoff\r\n                    data_subset_runoff = \\\r\n                        data_in_nc.variables[self.runoff_vars[0]][\r\n                            lat_slice, lon_slice]\r\n                    for var_name in self.runoff_vars[1:]:\r\n                        data_subset_runoff += \\\r\n                            data_in_nc.variables[var_name][\r\n                                lat_slice, lon_slice]\r\n\r\n                    # get runoff dims\r\n                    len_time_subset = 1\r\n                    len_lat_subset = data_subset_runoff.shape[0]\r\n                    len_lon_subset = data_subset_runoff.shape[1]\r\n\r\n                    # reshape the runoff\r\n                    data_subset_runoff = data_subset_runoff.reshape(\r\n                        len_lat_subset * len_lon_subset)\r\n\r\n                elif runoff_dimension_size == 3:\r\n                    # obtain subset of surface and subsurface runoff\r\n                    data_subset_runoff = \\\r\n                        data_in_nc.variables[self.runoff_vars[0]][\r\n                            :, lat_slice, lon_slice]\r\n                    for var_name in self.runoff_vars[1:]:\r\n                        data_subset_runoff += \\\r\n                            data_in_nc.variables[var_name][\r\n                                :, lat_slice, lon_slice]\r\n\r\n                    # get runoff dims\r\n                    len_time_subset = data_subset_runoff.shape[0]\r\n                    len_lat_subset = data_subset_runoff.shape[1]\r\n                    len_lon_subset = data_subset_runoff.shape[2]\r\n                    # reshape the runoff\r\n                    data_subset_runoff = \\\r\n                        data_subset_runoff.reshape(\r\n                            len_time_subset,\r\n                            (len_lat_subset * len_lon_subset))\r\n\r\n                data_in_nc.close()\r\n\r\n                if not index_new:\r\n                    # compute new indices based on the data_subset_surface\r\n                    for r in range(0, self.count):\r\n                        ind_lat_orig = lat_ind_all[r]\r\n                        ind_lon_orig = lon_ind_all[r]\r\n                        index_new.append(\r\n                            (ind_lat_orig - min_lat_ind_all) * len_lon_subset\r\n                            + (ind_lon_orig - min_lon_ind_all))\r\n\r\n                # obtain a new subset of data\r\n                if runoff_dimension_size == 2:\r\n                    data_subset_new = data_subset_runoff[index_new]\r\n                elif runoff_dimension_size == 3:\r\n                    data_subset_new = data_subset_runoff[:, index_new]\r\n\r\n                # FILTER DATA\r\n                try:\r\n                    # set masked values to zero\r\n                    data_subset_new = data_subset_new.filled(fill_value=0)\r\n                except AttributeError:\r\n                    pass\r\n                # set negative values to zero\r\n                data_subset_new[data_subset_new < 0] = 0\r\n\r\n                # combine data\r\n                if data_subset_all is None:\r\n                    data_subset_all = data_subset_new\r\n                else:\r\n                    data_subset_all = np.add(data_subset_all, data_subset_new)\r\n\r\n            if runoff_dimension_size == 3 and len_time_subset > 1:\r\n                inflow_data = np.zeros((len_time_subset, self.size_stream_id))\r\n            else:\r\n                inflow_data = np.zeros(self.size_stream_id)\r\n\r\n            pointer = 0\r\n            for stream_index in xrange(self.size_stream_id):\r\n                npoints = int(self.dict_list[self.header_wt[4]][pointer])\r\n                # Check if all npoints points correspond to the same streamID\r\n                if len(set(self.dict_list[self.header_wt[0]][\r\n                           pointer: (pointer + npoints)])) != 1:\r\n                    print(\"ROW INDEX {0}\".format(pointer))\r\n                    print(\"COMID {0}\".format(\r\n                        self.dict_list[self.header_wt[0]][pointer]))\r\n                    raise Exception(self.error_messages[2])\r\n\r\n                area_sqm_npoints = \\\r\n                    np.array([float(k) for k in\r\n                              self.dict_list[self.header_wt[1]][\r\n                              pointer: (pointer + npoints)]])\r\n\r\n                # assume data is incremental\r\n                if runoff_dimension_size == 3:\r\n                    data_goal = data_subset_all[:, pointer:(pointer + npoints)]\r\n                else:\r\n                    data_goal = data_subset_all[pointer:(pointer + npoints)]\r\n\r\n                if grid_type == 't255':\r\n                    # A) ERA Interim Low Res (T255) - data is cumulative\r\n                    # from time 3/6/9/12\r\n                    # (time zero not included, so assumed to be zero)\r\n                    ro_first_half = \\\r\n                        np.concatenate([data_goal[0:1, ],\r\n                                        np.subtract(data_goal[1:4, ],\r\n                                                    data_goal[0:3, ])])\r\n                    # from time 15/18/21/24\r\n                    # (time restarts at time 12, assumed to be zero)\r\n                    ro_second_half = \\\r\n                        np.concatenate([data_goal[4:5, ],\r\n                                        np.subtract(data_goal[5:, ],\r\n                                                    data_goal[4:7, ])])\r\n                    ro_stream = \\\r\n                        np.multiply(\r\n                            np.concatenate([ro_first_half, ro_second_half]),\r\n                            area_sqm_npoints)\r\n\r\n                else:\r\n                    ro_stream = data_goal * area_sqm_npoints * \\\r\n                                conversion_factor\r\n\r\n                # filter nan\r\n                ro_stream[np.isnan(ro_stream)] = 0\r\n\r\n                if ro_stream.any():\r\n                    if runoff_dimension_size == 3 and len_time_subset > 1:\r\n                        inflow_data[:, stream_index] = ro_stream.sum(axis=1)\r\n                    else:\r\n                        inflow_data[stream_index] = ro_stream.sum()\r\n\r\n                pointer += npoints\r\n\r\n            # only one process is allowed to write at a time to netcdf file\r\n            mp_lock.acquire()\r\n            data_out_nc = Dataset(out_nc, \"a\", format=\"NETCDF3_CLASSIC\")\r\n            if runoff_dimension_size == 3 and len_time_subset > 1:\r\n                data_out_nc.variables['m3_riv'][\r\n                    index*len_time_subset:(index+1)*len_time_subset, :] = \\\r\n                    inflow_data\r\n            else:\r\n                data_out_nc.variables['m3_riv'][index] = inflow_data\r\n            data_out_nc.close()\r\n            mp_lock.release()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef filter_nan(s, o):\n    data = np.array([s.flatten(), o.flatten()])\n    data = np.transpose(data)\n    data = data[~np.isnan(data).any(1)]\n    return data[:, 0], data[:, 1]", "response": "This function removes the data from simulated and observed data from simulated and observed data and returns the nan as a tuple of the two arrays"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pc_bias(s, o):\n    # s,o = filter_nan(s,o)\n    return 100.0*np.sum(s-o)/np.sum(o)", "response": "Percent Bias\n        input:\n        s: simulated\n        o: observed\n        output:\n        pc_bias: percent bias"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef apb(s, o):\n    # s,o = filter_nan(s,o)\n    return 100.0*np.sum(np.abs(s-o))/np.sum(o)", "response": "calculate apb of a set of items"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef NS(s, o):\n    # s,o = filter_nan(s,o)\n    return 1 - np.sum((s-o)**2)/np.sum((o-np.mean(o))**2)", "response": "Nash Sutcliffe efficient coefficient of a set of simulated and observed items."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef L(s, o, N=5):\n    # s,o = filter_nan(s,o)\n    return np.exp(-N*np.sum((s-o)**2)/np.sum((o-np.mean(o))**2))", "response": "Compute the likelihood of a set of simulated and observed items."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef correlation(s, o):\n    # s,o = filter_nan(s,o)\n    if s.size == 0:\n        corr = np.NaN\n    else:\n        corr = np.corrcoef(o, s)[0, 1]\n    return corr", "response": "compute the correlation coefficient of a simulated and observed item set"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef index_agreement(s, o):\n    # s,o = filter_nan(s,o)\n    ia = 1 - (np.sum((o-s)**2)) /\\\n             (np.sum((np.abs(s-np.mean(o))+np.abs(o-np.mean(o)))**2))\n    return ia", "response": "calculate the index of the agreement between two simulated and observed items"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef KGE(s, o):\n    # s,o = filter_nan(s, o)\n    cc = correlation(s, o)\n    alpha = np.std(s)/np.std(o)\n    beta = np.sum(s)/np.sum(o)\n    kge = 1 - np.sqrt((cc-1)**2 + (alpha-1)**2 + (beta-1)**2)\n    return kge, cc, alpha, beta", "response": "Compute the Kling - Gupta Efficiency\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef find_goodness_of_fit(rapid_qout_file, reach_id_file, observed_file,\n                         out_analysis_file, daily=False):\n    \"\"\"\n    Finds the goodness of fit comparing observed streamflow in a rapid Qout\n    file with simulated flows in a csv file.\n\n    Parameters\n    ----------\n    rapid_qout_file: str\n        Path to the RAPID Qout file.\n    reach_id_file: str\n        ath to file with river reach ID's associate with the RAPID Qout file.\n        It is in the format of the RAPID observed flows reach ID file.\n    observed_file: str\n        Path to input csv with with observed flows corresponding to the\n        RAPID Qout. It is in the format of the RAPID observed flows file.\n    out_analysis_file: str\n        Path to the analysis output csv file.\n    daily: bool, optional\n        If True and the file is CF-Compliant, it will compare the\n        *observed_file* with daily average flow from Qout. Default is False.\n\n\n    Example with CF-Compliant RAPID Qout file:\n\n    .. code:: python\n\n        import os\n        from RAPIDpy.postprocess import find_goodness_of_fit\n\n        INPUT_DATA_PATH = '/path/to/data'\n        reach_id_file = os.path.join(INPUT_DATA_PATH, 'obs_reach_id.csv')\n        observed_file = os.path.join(INPUT_DATA_PATH, 'obs_flow.csv')\n\n        cf_input_qout_file = os.path.join(COMPARE_DATA_PATH,\n                                         'Qout_nasa_lis_3hr_20020830_CF.nc')\n        cf_out_analysis_file = \\\n            os.path.join(OUTPUT_DATA_PATH,\n                        'cf_goodness_of_fit_results-daily.csv')\n        find_goodness_of_fit(cf_input_qout_file,\n                             reach_id_file,\n                             observed_file,\n                             cf_out_analysis_file,\n                             daily=True)\n\n    \"\"\"\n    reach_id_list = np.loadtxt(reach_id_file,\n                               delimiter=\",\", usecols=(0,),\n                               ndmin=1, dtype=np.int32)\n\n    data_nc = RAPIDDataset(rapid_qout_file)\n\n    # analyze and write\n    observed_table = np.loadtxt(observed_file,\n                                ndmin=2, delimiter=\",\",\n                                usecols=tuple(range(reach_id_list.size)))\n    with open(out_analysis_file, 'w') as outcsv:\n        writer = csvwriter(outcsv)\n        writer.writerow([\"reach_id\",\n                         \"percent_bias\",\n                         \"abs_percent_bias\",\n                         \"rmse\",\n                         \"mae\",\n                         \"bias\",\n                         \"NSE\",\n                         \"likelihood\",\n                         \"correlation_coeff\",\n                         \"index_agreement\",\n                         \"KGE\"])\n\n        for index, reach_id in enumerate(reach_id_list):\n            observed_array = observed_table[:, index]\n            simulated_array = data_nc.get_qout(reach_id, daily=daily)\n            # make sure they are the same length\n            simulated_array = simulated_array[:len(observed_array)]\n            observed_array = observed_array[:len(simulated_array)]\n            simulated_array, observed_array = \\\n                filter_nan(simulated_array, observed_array)\n            writer.writerow([reach_id,\n                             pc_bias(simulated_array, observed_array),\n                             apb(simulated_array, observed_array),\n                             rmse(simulated_array, observed_array),\n                             mae(simulated_array, observed_array),\n                             bias(simulated_array, observed_array),\n                             NS(simulated_array, observed_array),\n                             L(simulated_array, observed_array),\n                             correlation(simulated_array, observed_array),\n                             index_agreement(simulated_array, observed_array),\n                             KGE(simulated_array, observed_array)[0]])", "response": "This function finds the goodness of fit comparing observed streamflow in a rapid Qout file with simulated flows in a csv file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind the goodness of fit comparing observed and simulated flows In the file, the first column is the observed flows and the second column is the simulated flows. Example:: 33.5, 77.2 34.7, 73.0 Parameters ---------- observed_simulated_file: str Path to the csv file with the observed and simulated flows. out_file: str, optional Path to output file. If not provided, it will print to console. Example: .. code:: python from RAPIDpy.postprocess import find_goodness_of_fit_csv find_goodness_of_fit_csv(' /united_kingdom-thames/flows_kingston_gage_noah.csv')", "response": "def find_goodness_of_fit_csv(observed_simulated_file, out_file=None):\n    \"\"\"\n    Finds the goodness of fit comparing observed and simulated flows\n    In the file, the first column is the observed flows and the\n    second column is the simulated flows.\n\n    Example::\n\n        33.5, 77.2\n        34.7, 73.0\n\n    Parameters\n    ----------\n    observed_simulated_file: str\n        Path to the csv file with the observed and simulated flows.\n    out_file: str, optional\n        Path to output file. If not provided, it will print to console.\n\n\n    Example:\n\n    .. code:: python\n\n        from RAPIDpy.postprocess import find_goodness_of_fit_csv\n\n        find_goodness_of_fit_csv('\n            /united_kingdom-thames/flows_kingston_gage_noah.csv')\n\n    \"\"\"\n    observed_simulated_table = np.loadtxt(observed_simulated_file,\n                                          ndmin=2, delimiter=\",\",\n                                          usecols=(0, 1))\n\n    observed_array, simulated_array = \\\n        filter_nan(observed_simulated_table[:, 0],\n                   observed_simulated_table[:, 1])\n\n    # print error indices\n    if out_file:\n        print_file = open(out_file, 'w')\n    else:\n        print_file = None\n\n    print(\"\\n\".join([\n               \"Percent Bias: {0:.4f}\"\n               .format(pc_bias(simulated_array, observed_array)),\n               \"Absolute Percent Bias: {0:.4f}\"\n               .format(apb(simulated_array, observed_array)),\n               \"Root Mean Squared Error: {0:.4f}\"\n               .format(rmse(simulated_array, observed_array)),\n               \"Mean Absolute Error: {0:.4f}\"\n               .format(mae(simulated_array, observed_array)),\n               \"Bias: {0}\".format(bias(simulated_array, observed_array)),\n               \"Nash Sutcliffe efficiency coefficient: {0:.4f}\"\n               .format(NS(simulated_array, observed_array)),\n               \"Likelihood: {0:.4f}\"\n               .format(L(simulated_array, observed_array)),\n               \"correlation coefficient: {0:.4f}\"\n               .format(correlation(simulated_array, observed_array)),\n               \"index of agreement: {0:.4f}\"\n               .format(index_agreement(simulated_array, observed_array)),\n               \"Kling-Gupta Efficiency: {0:.4f}\"\n               .format(KGE(simulated_array, observed_array)[0]),\n               ]),\n          file=print_file)\n\n    if print_file:\n        print_file.close()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck the necessary dimensions and variables in the input netcdf data", "response": "def data_validation(self, in_nc):\r\n        \"\"\"Check the necessary dimensions and variables in the input\r\n        netcdf data\"\"\"\r\n        data_nc = Dataset(in_nc)\r\n\r\n        dims = list(data_nc.dimensions)\r\n\r\n        if dims not in self.dims_oi:\r\n            data_nc.close()\r\n            raise Exception(\"{0} {1}\".format(self.error_messages[1], dims))\r\n\r\n        nc_vars = list(data_nc.variables)\r\n\r\n        if nc_vars == self.vars_oi[0]:\r\n            self.runoff_vars = [self.vars_oi[0][-1]]\r\n        elif nc_vars == self.vars_oi[1]:\r\n            self.runoff_vars = [self.vars_oi[1][-1]]\r\n        else:\r\n            data_nc.close()\r\n            raise Exception(\"{0} {1}\".format(self.error_messages[2], nc_vars))\r\n        data_nc.close()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking the necessary dimensions and variables in the Taxonomy input netcdf data", "response": "def data_validation(self, in_nc):\n        \"\"\"Check the necessary dimensions and variables in the\n        input netcdf data\"\"\"\n        data_nc = Dataset(in_nc)\n        for dim in self.dims_oi:\n            if dim not in data_nc.dimensions.keys():\n                data_nc.close()\n                raise Exception(self.error_messages[1])\n\n        for var in self.vars_oi:\n            if var not in data_nc.variables.keys():\n                data_nc.close()\n                raise Exception(self.error_messages[2])\n\n        data_nc.close()\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nopens a CSV file with the correct mode", "response": "def open_csv(csv_file, mode='r'):\n    \"\"\"\n    Get mode depending on Python version\n    Based on: http://stackoverflow.com/questions/29840849/writing-a-csv-file-in-python-that-works-for-both-python-2-7-and-python-3-3-in\n    \"\"\"  # noqa\n    if version_info[0] == 2:  # Not named on 2.6\n        access = '{0}b'.format(mode)\n        kwargs = {}\n    else:\n        access = '{0}t'.format(mode)\n        kwargs = {'newline': ''}\n\n    return open(csv_file, access, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlog or raises a message.", "response": "def log(message, severity=\"INFO\", print_debug=True):\n    \"\"\"Logs, prints, or raises a message.\n\n    Arguments:\n        message -- message to report\n        severity -- string of one of these values:\n            CRITICAL|ERROR|WARNING|INFO|DEBUG\n    \"\"\"\n\n    print_me = ['WARNING', 'INFO', 'DEBUG']\n    if severity in print_me:\n        if severity == 'DEBUG':\n            if print_debug:\n                print(\"{0}: {1}\".format(severity, message))\n        else:\n            print(\"{0}: {1}\".format(severity, message))\n    else:\n        raise Exception(\"{0}: {1}\".format(severity, message))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreads in a CSV file and returns the contents as a list.", "response": "def csv_to_list(csv_file, delimiter=','):\n    \"\"\"\n    Reads in a CSV file and returns the contents as list,\n    where every row is stored as a sublist, and each element\n    in the sublist represents 1 cell in the table.\n    \"\"\"\n    with open_csv(csv_file) as csv_con:\n        if len(delimiter) > 1:\n            dialect = csv.Sniffer().sniff(csv_con.read(1024),\n                                          delimiters=delimiter)\n            csv_con.seek(0)\n            reader = csv.reader(csv_con, dialect)\n        else:\n            reader = csv.reader(csv_con, delimiter=delimiter)\n        return list(reader)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef compare_csv_decimal_files(file1, file2, header=True, timeseries=False):\n    # CHECK NUM LINES\n    with open_csv(file1) as fh1, \\\n            open_csv(file2) as fh2:\n        assert sum(1 for _ in fh1) == sum(1 for _ in fh2)\n\n    with open_csv(file1) as fh1, \\\n            open_csv(file2) as fh2:\n        csv1 = csv.reader(fh1)\n        csv2 = csv.reader(fh2)\n\n        if header:\n            assert next(csv1) == next(csv2)  # header\n\n        while True:\n            try:\n                row1 = next(csv1)\n                row2 = next(csv2)\n                compare_start_index = 0\n                if timeseries:\n                    assert row1[0] == row2[0]  # check dates\n                    compare_start_index = 1\n\n                assert_almost_equal(\n                    np_array(row1[compare_start_index:], dtype=np_float32),\n                    np_array(row2[compare_start_index:], dtype=np_float32),\n                    decimal=2)\n            except StopIteration:\n                break\n    return True", "response": "This function compares two csv files and returns True if they are equal."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_latlon_metadata(lat_var, lon_var):\n    lat_var.long_name = 'latitude'\n    lat_var.standard_name = 'latitude'\n    lat_var.units = 'degrees_north'\n    lat_var.axis = 'Y'\n\n    lon_var.long_name = 'longitude'\n    lon_var.standard_name = 'longitude'\n    lon_var.units = 'degrees_east'\n    lon_var.axis = 'X'", "response": "Adds latitude and longitude metadata to lat_var and lon_var."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generate_inflows_from_runoff(args):\n    runoff_file_list = args[0]\n    file_index_list = args[1]\n    weight_table_file = args[2]\n    grid_type = args[3]\n    rapid_inflow_file = args[4]\n    rapid_inflow_tool = args[5]\n    mp_lock = args[6]\n\n    time_start_all = datetime.utcnow()\n\n    if not isinstance(runoff_file_list, list):\n        runoff_file_list = [runoff_file_list]\n    else:\n        runoff_file_list = runoff_file_list\n\n    if not isinstance(file_index_list, list):\n        file_index_list = [file_index_list]\n    else:\n        file_index_list = file_index_list\n    if runoff_file_list and file_index_list:\n        # prepare ECMWF file for RAPID\n        index_string = \"Index: {0}\".format(file_index_list[0])\n        if len(file_index_list) > 1:\n            index_string += \" to {0}\".format(file_index_list[-1])\n        print(index_string)\n        runoff_string = \"File(s): {0}\".format(runoff_file_list[0])\n        if len(runoff_file_list) > 1:\n            runoff_string += \" to {0}\".format(runoff_file_list[-1])\n        print(runoff_string)\n        print(\"Converting inflow ...\")\n        try:\n            rapid_inflow_tool.execute(nc_file_list=runoff_file_list,\n                                      index_list=file_index_list,\n                                      in_weight_table=weight_table_file,\n                                      out_nc=rapid_inflow_file,\n                                      grid_type=grid_type,\n                                      mp_lock=mp_lock)\n        except Exception:\n            # This prints the type, value, and stack trace of the\n            # current exception being handled.\n            traceback.print_exc()\n            raise\n\n        time_finish_ecmwf = datetime.utcnow()\n        print(\"Time to convert inflows: {0}\"\n              .format(time_finish_ecmwf-time_start_all))", "response": "generate inflows from runoff file"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nidentifies the LSM grid by looking for the correct dimensions.", "response": "def identify_lsm_grid(lsm_grid_path):\n    \"\"\"\n    This is used to idenfity the input LSM grid\n    \"\"\"\n    # check to see what kind of file we are dealing with\n    lsm_example_file = Dataset(lsm_grid_path)\n\n    # INDENTIFY LAT/LON DIMENSIONS\n    dim_list = lsm_example_file.dimensions.keys()\n\n    latitude_dim = \"lat\"\n    if 'latitude' in dim_list:\n        latitude_dim = 'latitude'\n    elif 'g0_lat_0' in dim_list:\n        # GLDAS/NLDAS MOSAIC\n        latitude_dim = 'g0_lat_0'\n    elif 'lat_110' in dim_list:\n        # NLDAS NOAH/VIC\n        latitude_dim = 'lat_110'\n    elif 'north_south' in dim_list:\n        # LIS/Joules\n        latitude_dim = 'north_south'\n    elif 'south_north' in dim_list:\n        # WRF Hydro\n        latitude_dim = 'south_north'\n    elif 'Y' in dim_list:\n        # FLDAS\n        latitude_dim = 'Y'\n\n    longitude_dim = \"lon\"\n    if 'longitude' in dim_list:\n        longitude_dim = 'longitude'\n    elif 'g0_lon_1' in dim_list:\n        # GLDAS/NLDAS MOSAIC\n        longitude_dim = 'g0_lon_1'\n    elif 'lon_110' in dim_list:\n        # NLDAS NOAH/VIC\n        longitude_dim = 'lon_110'\n    elif 'east_west' in dim_list:\n        # LIS/Joules\n        longitude_dim = 'east_west'\n    elif 'west_east' in dim_list:\n        # WRF Hydro\n        longitude_dim = 'west_east'\n    elif 'X' in dim_list:\n        # FLDAS\n        longitude_dim = 'X'\n\n    time_dim = None\n    if 'time' in dim_list:\n        time_dim = 'time'\n    elif 'Time' in dim_list:\n        time_dim = 'Time'\n    elif 'Times' in dim_list:\n        time_dim = 'Times'\n    elif 'times' in dim_list:\n        time_dim = 'times'\n\n    lat_dim_size = len(lsm_example_file.dimensions[latitude_dim])\n    lon_dim_size = len(lsm_example_file.dimensions[longitude_dim])\n\n    # IDENTIFY VARIABLES\n    var_list = lsm_example_file.variables.keys()\n\n    latitude_var = \"lat\"\n    if 'latitude' in var_list:\n        latitude_var = 'latitude'\n    elif 'g0_lat_0' in var_list:\n        latitude_var = 'g0_lat_0'\n    elif 'lat_110' in var_list:\n        latitude_var = 'lat_110'\n    elif 'north_south' in var_list:\n        latitude_var = 'north_south'\n    elif 'XLAT' in var_list:\n        # WRF\n        latitude_var = 'XLAT'\n    elif 'Y' in var_list:\n        # FLDAS\n        latitude_var = 'Y'\n\n    longitude_var = \"lon\"\n    if 'longitude' in var_list:\n        longitude_var = 'longitude'\n    elif 'g0_lon_1' in var_list:\n        longitude_var = 'g0_lon_1'\n    elif 'lon_110' in var_list:\n        longitude_var = 'lon_110'\n    elif 'east_west' in var_list:\n        longitude_var = 'east_west'\n    elif 'XLONG' in var_list:\n        # WRF\n        longitude_var = 'XLONG'\n    elif 'X' in var_list:\n        # FLDAS\n        longitude_var = 'X'\n\n    time_var = None\n    if 'time' in var_list:\n        time_var = 'time'\n    elif 'Time' in var_list:\n        time_var = 'Time'\n    elif 'Times' in var_list:\n        time_var = 'Times'\n    elif 'times' in var_list:\n        time_var = 'times'\n\n    surface_runoff_var = \"\"\n    subsurface_runoff_var = \"\"\n    total_runoff_var = \"\"\n    for var in var_list:\n        if var.startswith(\"SSRUN\"):\n            # NLDAS/GLDAS\n            surface_runoff_var = var\n        elif var.startswith(\"BGRUN\"):\n            # NLDAS/GLDAS\n            subsurface_runoff_var = var\n        elif var == \"Qs_acc\":\n            # GLDAS v2\n            surface_runoff_var = var\n        elif var == \"Qsb_acc\":\n            # GLDAS v2\n            subsurface_runoff_var = var\n        elif var == \"Qs_tavg\":\n            # FLDAS\n            surface_runoff_var = var\n        elif var == \"Qsb_tavg\":\n            # FLDAS\n            subsurface_runoff_var = var\n        elif var == \"Qs_inst\":\n            # LIS\n            surface_runoff_var = var\n        elif var == \"Qsb_inst\":\n            # LIS\n            subsurface_runoff_var = var\n        elif var == \"SFROFF\":\n            # WRF Hydro\n            surface_runoff_var = var\n        elif var == \"UDROFF\":\n            # WRF Hydro\n            subsurface_runoff_var = var\n        elif var.lower() == \"ro\":\n            # ERA Interim\n            total_runoff_var = var\n        elif var == \"total runoff\":\n            # CMIP5 data\n            total_runoff_var = var\n\n    # IDENTIFY GRID TYPE\n    lsm_file_data = {\n        \"weight_file_name\": \"\",\n        \"grid_type\": \"\",\n        \"model_name\": \"\",\n        \"description\": \"\",\n        \"rapid_inflow_tool\": None,\n        \"latitude_var\": latitude_var,\n        \"longitude_var\": longitude_var,\n        \"time_var\": time_var,\n        \"latitude_dim\": latitude_dim,\n        \"longitude_dim\": longitude_dim,\n        \"time_dim\": time_dim,\n    }\n\n    institution = \"\"\n    title = \"\"\n    try:\n        institution = lsm_example_file.getncattr(\"institution\")\n    except AttributeError:\n        pass\n    try:\n        title = lsm_example_file.getncattr(\"title\")\n    except AttributeError:\n        pass\n\n    runoff_vars = [surface_runoff_var, subsurface_runoff_var]\n\n    if institution == \"European Centre for Medium-Range Weather Forecasts\" \\\n            or total_runoff_var.lower() == \"ro\":\n        # these are the ECMWF models\n        if lat_dim_size == 361 and lon_dim_size == 720:\n            print(\"Runoff file identified as ERA Interim Low Res (T255) GRID\")\n            # A) ERA Interim Low Res (T255)\n            # Downloaded as 0.5 degree grid\n            #  dimensions:\n            # \t longitude = 720 ;\n            # \t latitude = 361 ;\n            lsm_file_data[\"description\"] = \"ERA Interim (T255 Grid)\"\n            lsm_file_data[\"model_name\"] = \"erai\"\n            lsm_file_data[\"weight_file_name\"] = r'weight_era_t255\\.csv'\n            lsm_file_data[\"grid_type\"] = 't255'\n\n        elif lat_dim_size == 512 and lon_dim_size == 1024:\n            print(\"Runoff file identified as ERA Interim High Res (T511) GRID\")\n            # B) ERA Interim High Res (T511)\n            #  dimensions:\n            #   lon = 1024 ;\n            #   lat = 512 ;\n            lsm_file_data[\"description\"] = \"ERA Interim (T511 Grid)\"\n            lsm_file_data[\"weight_file_name\"] = r'weight_era_t511\\.csv'\n            lsm_file_data[\"model_name\"] = \"erai\"\n            lsm_file_data[\"grid_type\"] = 't511'\n        elif lat_dim_size == 161 and lon_dim_size == 320:\n            print(\"Runoff file identified as ERA 20CM (T159) GRID\")\n            # C) ERA 20CM (T159) - 3hr - 10 ensembles\n            # Downloaded as 1.125 degree grid\n            #  dimensions:\n            #   longitude = 320 ;\n            #   latitude = 161 ;\n            lsm_file_data[\"description\"] = \"ERA 20CM (T159 Grid)\"\n            lsm_file_data[\"weight_file_name\"] = r'weight_era_t159\\.csv'\n            lsm_file_data[\"model_name\"] = \"era_20cm\"\n            lsm_file_data[\"grid_type\"] = 't159'\n        else:\n            lsm_example_file.close()\n            raise Exception(\"Unsupported ECMWF grid.\")\n\n        lsm_file_data[\"rapid_inflow_tool\"] = \\\n            CreateInflowFileFromERAInterimRunoff()\n\n    elif institution == \"NASA GSFC\":\n        if title == \"GLDAS2.0 LIS land surface model output\":\n            print(\"Runoff file identified as GLDAS v2 LIS GRID\")\n            # this is the LIS model\n            lsm_file_data[\"weight_file_name\"] = r'weight_gldas2\\.csv'\n            lsm_file_data[\"grid_type\"] = 'gldas2'\n            lsm_file_data[\"description\"] = \"GLDAS2.0 LIS\"\n            lsm_file_data[\"model_name\"] = \"nasa\"\n\n        else:\n            print(\"Runoff file identified as LIS GRID\")\n            # this is the LIS model (can be FLDAS)\n            # THIS CASE CAN ALSO BE FOR FLDAS, however you will need to add\n            # the file_datetime_pattern && file_datetime_re_pattern for it to\n            # work if it is not 3-hourly time step.\n            lsm_file_data[\"weight_file_name\"] = r'weight_lis\\.csv'\n            lsm_file_data[\"grid_type\"] = 'lis'\n            lsm_file_data[\"description\"] = \"NASA GSFC LIS\"\n            lsm_file_data[\"model_name\"] = \"nasa\"\n\n    elif institution == \"Met Office, UK\":\n        print(\"Runoff file identified as Joules GRID\")\n        lsm_file_data[\"weight_file_name\"] = r'weight_joules\\.csv'\n        lsm_file_data[\"grid_type\"] = 'joules'\n        lsm_file_data[\"description\"] = \"Met Office Joules\"\n        lsm_file_data[\"model_name\"] = \"met_office\"\n\n    elif institution == \"NCAR, USACE, USBR\":\n        print(\"Runoff file identified as CMIP5\")\n        lsm_file_data[\"weight_file_name\"] = r'weight_cmip5\\.csv'\n        lsm_file_data[\"grid_type\"] = 'cmip5'\n        lsm_file_data[\"description\"] = \"CMIP5 Runoff\"\n        lsm_file_data[\"model_name\"] = \"cmip5\"\n\n        runoff_vars = [total_runoff_var]\n\n    elif surface_runoff_var.startswith(\"SSRUN\") \\\n            and subsurface_runoff_var.startswith(\"BGRUN\"):\n\n        lsm_file_data[\"model_name\"] = \"nasa\"\n        if lat_dim_size == 600 and lon_dim_size == 1440:\n            print(\"Runoff file identified as GLDAS GRID\")\n            # GLDAS NC FILE\n            # dimensions:\n            #     g0_lat_0 = 600 ;\n            #     g0_lon_1 = 1440 ;\n            # variables\n            # SSRUN_GDS0_SFC_ave1h (surface)\n            # BGRUN_GDS0_SFC_ave1h (subsurface)\n            #  or\n            # SSRUNsfc_GDS0_SFC_ave1h (surface)\n            # BGRUNsfc_GDS0_SFC_ave1h (subsurface)\n            lsm_file_data[\"description\"] = \"GLDAS\"\n            lsm_file_data[\"weight_file_name\"] = r'weight_gldas\\.csv'\n            lsm_file_data[\"grid_type\"] = 'gldas'\n\n        elif lat_dim_size <= 224 and lon_dim_size <= 464:\n            print(\"Runoff file identified as NLDAS GRID\")\n            # NLDAS MOSAIC FILE\n            # dimensions:\n            #     g0_lat_0 = 224 ;\n            #     g0_lon_1 = 464 ;\n            # NLDAS NOAH/VIC FILE\n            # dimensions:\n            #     lat_110 = 224 ;\n            #     lon_110 = 464 ;\n\n            lsm_file_data[\"description\"] = \"NLDAS\"\n            lsm_file_data[\"weight_file_name\"] = r'weight_nldas\\.csv'\n            lsm_file_data[\"grid_type\"] = 'nldas'\n        else:\n            lsm_example_file.close()\n            raise Exception(\"Unsupported runoff grid.\")\n\n    else:\n        title = \"\"\n        try:\n            title = lsm_example_file.getncattr(\"TITLE\")\n        except AttributeError:\n            pass\n\n        if \"WRF\" in title:\n            lsm_file_data[\"description\"] = \"WRF/WRF-Hydro Runoff\"\n            lsm_file_data[\"weight_file_name\"] = r'weight_wrf\\.csv'\n            lsm_file_data[\"model_name\"] = 'wrf'\n            lsm_file_data[\"grid_type\"] = 'wrf'\n\n            lsm_file_data['rapid_inflow_tool'] = \\\n                CreateInflowFileFromWRFHydroRunoff(\n                    latitude_dim,\n                    longitude_dim,\n                    latitude_var,\n                    longitude_var,\n                    surface_runoff_var,\n                    subsurface_runoff_var,\n                )\n        else:\n            lsm_example_file.close()\n            raise Exception(\"Unsupported LSM grid.\")\n\n    lsm_example_file.close()\n\n    # set the inflow tool to use the LDAS tool by default\n    if lsm_file_data[\"rapid_inflow_tool\"] is None:\n        lsm_file_data[\"rapid_inflow_tool\"] = \\\n            CreateInflowFileFromLDASRunoff(\n                latitude_dim,\n                longitude_dim,\n                latitude_var,\n                longitude_var,\n                runoff_vars,\n            )\n\n    return lsm_file_data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef determine_start_end_timestep(lsm_file_list,\n                                 file_re_match=None,\n                                 file_datetime_pattern=None,\n                                 expected_time_step=None,\n                                 lsm_grid_info=None):\n    \"\"\"\n    Determine the start and end date from LSM input files\n    \"\"\"\n    if lsm_grid_info is None:\n        lsm_grid_info = identify_lsm_grid(lsm_file_list[0])\n\n    if None in (lsm_grid_info['time_var'], lsm_grid_info['time_dim'])\\\n            or lsm_grid_info['model_name'] in ('era_20cm', 'erai'):\n        # NOTE: the ERA20CM and ERA 24hr time variables\n        # in the tests are erroneous\n        if None in (file_re_match, file_datetime_pattern):\n            raise ValueError(\"LSM files missing time dimension and/or \"\n                             \"variable.To mitigate this, add the \"\n                             \"'file_re_match' and 'file_datetime_pattern' \"\n                             \"arguments.\")\n\n        if lsm_grid_info['time_dim'] is None:\n            print(\"Assuming time dimension is 1\")\n            file_size_time = 1\n        else:\n            lsm_example_file = Dataset(lsm_file_list[0])\n            file_size_time = \\\n                len(lsm_example_file.dimensions[lsm_grid_info['time_dim']])\n            lsm_example_file.close()\n\n        total_num_time_steps = int(file_size_time * len(lsm_file_list))\n\n        # determine the start time from the existing files\n        actual_simulation_start_datetime = \\\n            datetime.strptime(file_re_match.search(lsm_file_list[0]).group(0),\n                              file_datetime_pattern)\n\n        # check to see if the time step matches expected\n        if len(lsm_file_list) > 1:\n            time_step = \\\n                int((datetime.strptime(\n                    file_re_match.search(lsm_file_list[1]).group(0),\n                    file_datetime_pattern) -\n                    actual_simulation_start_datetime).total_seconds()\n                    / float(file_size_time))\n\n        elif expected_time_step is not None:\n            time_step = int(expected_time_step)\n        else:\n            raise ValueError(\"Only one LSM file with one timestep present. \"\n                             \"'expected_time_step' parameter required to \"\n                             \"continue.\")\n\n        # determine the end datetime\n        actual_simulation_end_datetime = \\\n            datetime.strptime(file_re_match.search(lsm_file_list[-1]).group(0),\n                              file_datetime_pattern) \\\n            + timedelta(seconds=(file_size_time-1) * time_step)\n    else:\n        with pangaea.open_mfdataset(lsm_file_list,\n                                    lat_var=lsm_grid_info['latitude_var'],\n                                    lon_var=lsm_grid_info['longitude_var'],\n                                    time_var=lsm_grid_info['time_var'],\n                                    lat_dim=lsm_grid_info['latitude_dim'],\n                                    lon_dim=lsm_grid_info['longitude_dim'],\n                                    time_dim=lsm_grid_info['time_dim']) as xds:\n\n            datetime_arr = [pd.to_datetime(dval) for dval in\n                            xds.lsm.datetime.values]\n            actual_simulation_start_datetime = datetime_arr[0]\n            actual_simulation_end_datetime = datetime_arr[-1]\n            total_num_time_steps = len(datetime_arr)\n\n            if total_num_time_steps <= 1:\n                if expected_time_step is not None:\n                    time_step = int(expected_time_step)\n                else:\n                    raise ValueError(\"Only one LSM file with one timestep \"\n                                     \"present. 'expected_time_step' parameter \"\n                                     \"required to continue.\")\n            else:\n                time_step = int(np.diff(xds.lsm.datetime.values)[0]\n                                / np.timedelta64(1, 's'))\n\n    if expected_time_step is not None:\n        if time_step != int(expected_time_step):\n            print(\"WARNING: The time step used {0} is different than \"\n                  \"expected {1}\".format(time_step, expected_time_step))\n\n    return (actual_simulation_start_datetime, actual_simulation_end_datetime,\n            time_step, total_num_time_steps)", "response": "Determines the start and end date of a single LSM file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pointsToVoronoiGridShapefile(lat, lon, vor_shp_path, extent=None):\n    voronoi_centroids = _get_voronoi_centroid_array(lat, lon, extent)\n\n    # set-up output polygon shp\n    log(\"Creating output polygon shp {0}\"\n        .format(os.path.basename(vor_shp_path)))\n    if os.path.exists(vor_shp_path):\n        os.remove(vor_shp_path)\n    drv = ogr.GetDriverByName('ESRI Shapefile')\n    outShp = drv.CreateDataSource(vor_shp_path)\n    osr_geographic_proj = osr.SpatialReference()\n    osr_geographic_proj.ImportFromEPSG(4326)\n    layer = outShp.CreateLayer('', osr_geographic_proj, ogr.wkbPolygon)\n    layer.CreateField(ogr.FieldDefn('GRID_LAT', ogr.OFTReal))\n    layer.CreateField(ogr.FieldDefn('GRID_LON', ogr.OFTReal))\n    layerDefn = layer.GetLayerDefn()\n\n    # find nodes surrounding polygon centroid\n    # sort nodes in counterclockwise order\n    # create polygon perimeter through nodes\n    log(\"Building Voronoi polygons...\")\n    # compute voronoi\n    voronoi_manager = Voronoi(voronoi_centroids)\n    voronoi_vertices = voronoi_manager.vertices\n    voronoi_regions = voronoi_manager.regions\n    for point_id, region_index in enumerate(voronoi_manager.point_region):\n        vert_index_list = np.array(voronoi_regions[region_index])\n        voronoi_centroid = voronoi_centroids[point_id]\n        voronoi_poly_points = _get_voronoi_poly_points(vert_index_list,\n                                                       voronoi_vertices,\n                                                       voronoi_centroid)\n        if len(voronoi_poly_points) == 4:\n            poly = ogr.Geometry(ogr.wkbPolygon)\n            ring = ogr.Geometry(ogr.wkbLinearRing)\n            for node in voronoi_poly_points:\n                ring.AddPoint(node[0], node[1])\n\n            # grab first node to close ring\n            ring.AddPoint(voronoi_poly_points[0][0], voronoi_poly_points[0][1])\n\n            poly.AddGeometry(ring)\n            feat = ogr.Feature(layerDefn)\n            feat.SetField('GRID_LON', float(voronoi_centroid[0]))\n            feat.SetField('GRID_LAT', float(voronoi_centroid[1]))\n            feat.SetGeometry(poly)\n            layer.CreateFeature(feat)", "response": "Converts points to voronoi grid shapefile"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts points to voronoi grid array via voronoi", "response": "def pointsToVoronoiGridArray(lat, lon, extent=None):\n    \"\"\"\n    Converts points to grid array via voronoi\n    \"\"\"\n    voronoi_centroids = _get_voronoi_centroid_array(lat, lon, extent)\n    # find nodes surrounding polygon centroid\n    # sort nodes in counterclockwise order\n    # create polygon perimeter through nodes\n    log(\"Building Voronoi polygons...\")\n    # compute voronoi\n    voronoi_manager = Voronoi(voronoi_centroids)\n    voronoi_vertices = voronoi_manager.vertices\n    voronoi_regions = voronoi_manager.regions\n    feature_list = []\n    for point_id, region_index in enumerate(voronoi_manager.point_region):\n        vert_index_list = np.array(voronoi_regions[region_index])\n        voronoi_centroid = voronoi_centroids[point_id]\n        voronoi_poly_points = _get_voronoi_poly_points(vert_index_list,\n                                                       voronoi_vertices,\n                                                       voronoi_centroid)\n\n        if len(voronoi_poly_points) == 4:\n            feature_list.append({'polygon': Polygon(voronoi_poly_points),\n                                 'lon': voronoi_centroid[0],\n                                 'lat': voronoi_centroid[1]})\n\n    return feature_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _validate_raw_nc(self):\r\n        self.raw_nc_list = []\r\n        # add one for the first flow value RAPID\r\n        # does not include\r\n        total_time_len = 1\r\n        id_len_list = []\r\n        for rapid_output_file in self.rapid_output_file_list:\r\n            qout_nc = RAPIDDataset(rapid_output_file)\r\n            id_len_list.append(qout_nc.size_river_id)\r\n            total_time_len += qout_nc.size_time\r\n            self.raw_nc_list.append(qout_nc)\r\n\r\n        # make sure river id lists are the same\r\n        for id_len_undex in range(1, len(id_len_list)):\r\n            if id_len_list[id_len_undex] != id_len_list[0]:\r\n                raise Exception(\"River ID size is different in \"\r\n                                \"one of the files ...\")\r\n\r\n        for raw_nc_index in range(1, len(self.raw_nc_list)):\r\n            if not (self.raw_nc_list[raw_nc_index].get_river_id_array() ==\r\n                    self.raw_nc_list[0].get_river_id_array()).all():\r\n                raise Exception(\"River IDs are different in \"\r\n                                \"files ...\")\r\n\r\n        return id_len_list[0], total_time_len", "response": "Checks that the raw netCDF file has the right dimensions and variables and returns the list of RAPID output files."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates the netCDF file with CF dimensions and variables but no data.", "response": "def _initialize_output(self, time_len, id_len):\r\n        \"\"\"Creates netCDF file with CF dimensions and variables, but no data.\r\n\r\n        Arguments\r\n        ---------\r\n        time_len: int\r\n            Length of time dimension (number of time steps).\r\n        id_len: int\r\n            Length of Id dimension (number of time series).\r\n\r\n        \"\"\"\r\n        log('Initializing new file %s' % self.cf_compliant_file, 'INFO')\r\n\r\n        self.cf_nc = Dataset(self.cf_compliant_file, 'w',\r\n                             format='NETCDF3_CLASSIC')\r\n\r\n        # Create global attributes\r\n        log('    globals', 'DEBUG', self.print_debug)\r\n        self.cf_nc.featureType = 'timeSeries'\r\n        self.cf_nc.Metadata_Conventions = 'Unidata Dataset Discovery v1.0'\r\n        self.cf_nc.Conventions = 'CF-1.6'\r\n        self.cf_nc.cdm_data_type = 'Station'\r\n        self.cf_nc.nodc_template_version = (\r\n            'NODC_NetCDF_TimeSeries_Orthogonal_Template_v1.1')\r\n        self.cf_nc.standard_name_vocabulary = \\\r\n            ('NetCDF Climate and Forecast (CF) '\r\n             'Metadata Convention Standard Name '\r\n             'Table v28')\r\n        self.cf_nc.title = 'RAPID Result'\r\n        self.cf_nc.summary =\\\r\n            (\"Results of RAPID river routing simulation. Each river \"\r\n             \"reach (i.e., feature) is represented by a point \"\r\n             \"feature at its midpoint, and is identified by the \"\r\n             \"reach's unique NHDPlus COMID identifier.\")\r\n        self.cf_nc.time_coverage_resolution = 'point'\r\n        self.cf_nc.geospatial_lat_min = 0.0\r\n        self.cf_nc.geospatial_lat_max = 0.0\r\n        self.cf_nc.geospatial_lat_units = 'degrees_north'\r\n        self.cf_nc.geospatial_lat_resolution = 'midpoint of stream feature'\r\n        self.cf_nc.geospatial_lon_min = 0.0\r\n        self.cf_nc.geospatial_lon_max = 0.0\r\n        self.cf_nc.geospatial_lon_units = 'degrees_east'\r\n        self.cf_nc.geospatial_lon_resolution = 'midpoint of stream feature'\r\n        self.cf_nc.geospatial_vertical_min = 0.0\r\n        self.cf_nc.geospatial_vertical_max = 0.0\r\n        self.cf_nc.geospatial_vertical_units = 'm'\r\n        self.cf_nc.geospatial_vertical_resolution = \\\r\n            'midpoint of stream feature'\r\n        self.cf_nc.geospatial_vertical_positive = 'up'\r\n        self.cf_nc.project = self.project_name\r\n        self.cf_nc.processing_level = 'Raw simulation result'\r\n        self.cf_nc.keywords_vocabulary = \\\r\n            ('NASA/Global Change Master Directory '\r\n             '(GCMD) Earth Science Keywords. Version '\r\n             '8.0.0.0.0')\r\n        self.cf_nc.keywords = 'DISCHARGE/FLOW'\r\n        self.cf_nc.comment = \\\r\n            'Result time step(s) (seconds): ' + str(self.time_step_array)\r\n\r\n        timestamp = datetime.utcnow().isoformat() + 'Z'\r\n        self.cf_nc.date_created = timestamp\r\n        self.cf_nc.history = \\\r\n            (timestamp + '; added time, lat, lon, z, crs variables; '\r\n             'added metadata to conform to NODC_NetCDF_TimeSeries_'\r\n             'Orthogonal_Template_v1.1')\r\n\r\n        # Create dimensions\r\n        log('    dimming', 'DEBUG', self.print_debug)\r\n        self.cf_nc.createDimension('time', time_len)\r\n        self.cf_nc.createDimension(self.output_id_dim_name, id_len)\r\n\r\n        # Create variables\r\n        log('    time_series_var', 'DEBUG', self.print_debug)\r\n        time_series_var = \\\r\n            self.cf_nc.createVariable(self.output_id_dim_name, 'i4',\r\n                                      (self.output_id_dim_name,))\r\n        time_series_var.long_name = (\r\n            'Unique NHDPlus COMID identifier for each river reach feature')\r\n        time_series_var.cf_role = 'timeseries_id'\r\n\r\n        log('    time_var', 'DEBUG', self.print_debug)\r\n        time_var = self.cf_nc.createVariable('time', 'i4', ('time',))\r\n        time_var.long_name = 'time'\r\n        time_var.standard_name = 'time'\r\n        time_var.units = 'seconds since 1970-01-01 00:00:00 0:00'\r\n        time_var.axis = 'T'\r\n\r\n        # only add if user adds\r\n        if self.comid_lat_lon_z_file and \\\r\n                os.path.exists(self.comid_lat_lon_z_file):\r\n            log('    lat_var', 'DEBUG', self.print_debug)\r\n            lat_var = self.cf_nc.createVariable('lat', 'f8',\r\n                                                (self.output_id_dim_name,),\r\n                                                fill_value=-9999.0)\r\n\r\n            log('    lon_var', 'DEBUG', self.print_debug)\r\n            lon_var = self.cf_nc.createVariable('lon', 'f8',\r\n                                                (self.output_id_dim_name,),\r\n                                                fill_value=-9999.0)\r\n\r\n            add_latlon_metadata(lat_var, lon_var)\r\n\r\n            log('    z_var', 'DEBUG', self.print_debug)\r\n            z_var = self.cf_nc.createVariable('z', 'f8',\r\n                                              (self.output_id_dim_name,),\r\n                                              fill_value=-9999.0)\r\n            z_var.long_name = ('Elevation referenced to the North American '\r\n                               'Vertical Datum of 1988 (NAVD88)')\r\n            z_var.standard_name = 'surface_altitude'\r\n            z_var.units = 'm'\r\n            z_var.axis = 'Z'\r\n            z_var.positive = 'up'\r\n\r\n            log('    crs_var', 'DEBUG', self.print_debug)\r\n            crs_var = self.cf_nc.createVariable('crs', 'i4')\r\n            crs_var.grid_mapping_name = 'latitude_longitude'\r\n            crs_var.epsg_code = 'EPSG:4326'  # WGS 84\r\n            crs_var.semi_major_axis = 6378137.0\r\n            crs_var.inverse_flattening = 298.257223563"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _write_comid_lat_lon_z(self):\r\n        # only add if user adds\r\n        if self.comid_lat_lon_z_file and \\\r\n                os.path.exists(self.comid_lat_lon_z_file):\r\n            # get list of COMIDS\r\n            lookup_table = csv_to_list(self.comid_lat_lon_z_file)\r\n            lookup_comids = np.array([int(float(row[0])) for row in\r\n                                      lookup_table[1:]])\r\n\r\n            # Get relevant arrays while we update them\r\n            nc_comids = self.cf_nc.variables[self.output_id_dim_name][:]\r\n            lats = self.cf_nc.variables['lat'][:]\r\n            lons = self.cf_nc.variables['lon'][:]\r\n            zs = self.cf_nc.variables['z'][:]\r\n\r\n            min_lat = None\r\n            max_lat = None\r\n            min_lon = None\r\n            max_lon = None\r\n            z_min = None\r\n            z_max = None\r\n\r\n            # Process each row in the lookup table\r\n            for nc_index, nc_comid in enumerate(nc_comids):\r\n                try:\r\n                    lookup_index = \\\r\n                        np.where(lookup_comids == nc_comid)[0][0] + 1\r\n                except IndexError:\r\n                    log('rivid %s missing in comid_lat_lon_z file' % nc_comid,\r\n                        'ERROR')\r\n\r\n                lat = float(lookup_table[lookup_index][1])\r\n                lats[nc_index] = lat\r\n                if min_lat is None or lat < min_lat:\r\n                    min_lat = lat\r\n                if max_lat is None or lat > max_lat:\r\n                    max_lat = lat\r\n\r\n                lon = float(lookup_table[lookup_index][2])\r\n                lons[nc_index] = lon\r\n                if min_lon is None or lon < min_lon:\r\n                    min_lon = lon\r\n                if max_lon is None or lon > max_lon:\r\n                    max_lon = lon\r\n\r\n                z = float(lookup_table[lookup_index][3])\r\n                zs[nc_index] = z\r\n                if z_min is None or z < z_min:\r\n                    z_min = z\r\n                if z_max is None or z > z_max:\r\n                    z_max = z\r\n\r\n            # Overwrite netCDF variable values\r\n            self.cf_nc.variables['lat'][:] = lats\r\n            self.cf_nc.variables['lon'][:] = lons\r\n            self.cf_nc.variables['z'][:] = zs\r\n\r\n            # Update metadata\r\n            if min_lat is not None:\r\n                self.cf_nc.geospatial_lat_min = min_lat\r\n            if max_lat is not None:\r\n                self.cf_nc.geospatial_lat_max = max_lat\r\n            if min_lon is not None:\r\n                self.cf_nc.geospatial_lon_min = min_lon\r\n            if max_lon is not None:\r\n                self.cf_nc.geospatial_lon_max = max_lon\r\n            if z_min is not None:\r\n                self.cf_nc.geospatial_vertical_min = z_min\r\n            if z_max is not None:\r\n                self.cf_nc.geospatial_vertical_max = z_max\r\n        else:\r\n            log('No comid_lat_lon_z file. Not adding values ...', 'INFO')", "response": "Add latitude longitude and z values for each netCDF feature"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _generate_time_values(self):\r\n        # Populate time values\r\n        log('writing times', 'INFO')\r\n        d1970 = datetime(1970, 1, 1, tzinfo=utc)\r\n        time_array = [[int((self.start_datetime - d1970).total_seconds())]]\r\n\r\n        datetime_nc_start_simulation = self.start_datetime\r\n        for raw_nc_index, raw_nc in enumerate(self.raw_nc_list):\r\n            raw_nc_time = raw_nc.get_time_array(\r\n                datetime_simulation_start=datetime_nc_start_simulation,\r\n                simulation_time_step_seconds=self.time_step_array[\r\n                    raw_nc_index])\r\n\r\n            time_array.append(raw_nc_time)\r\n            datetime_nc_start_simulation = \\\r\n                datetime.utcfromtimestamp(raw_nc_time[-1])\r\n\r\n        self.cf_nc.variables['time'][:] = np.concatenate(time_array)\r\n        end_date = datetime.utcfromtimestamp(self.cf_nc.variables['time'][-1])\r\n        self.cf_nc.time_coverage_start = self.start_datetime.isoformat() + 'Z'\r\n        self.cf_nc.time_coverage_end = end_date.isoformat() + 'Z'", "response": "Generate time values for out nc file"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _copy_streamflow_values(self):\r\n        log('Creating streamflow variable', 'INFO')\r\n        q_var = self.cf_nc.createVariable(\r\n            self.output_flow_var_name, 'f4', (self.output_id_dim_name, 'time'))\r\n        q_var.long_name = 'Discharge'\r\n        q_var.units = 'm^3/s'\r\n        q_var.coordinates = 'time lat lon z'\r\n        q_var.grid_mapping = 'crs'\r\n        q_var.source = ('Generated by the Routing Application for Parallel '\r\n                        'computatIon of Discharge (RAPID) river routing '\r\n                        'model.')\r\n        q_var.references = 'http://rapid-hub.org/'\r\n        q_var.comment = ('lat, lon, and z values taken at midpoint of river '\r\n                         'reach feature')\r\n\r\n        log('Copying streamflow values', 'INFO')\r\n        master_begin_time_step_index = 1\r\n        # to reduce RAM, copy by chunks\r\n        max_2d_dimension = 1000000000  # ~8GB Max\r\n        for raw_nc in self.raw_nc_list:\r\n            max_time_step_size = min(raw_nc.size_time,\r\n                                     max(1, int(float(max_2d_dimension) /\r\n                                                float(raw_nc.size_river_id))))\r\n            raw_nc_begin_time_step_index = 0\r\n            for raw_nc_time_index in \\\r\n                    xrange(0, raw_nc.size_time, max_time_step_size):\r\n                time_interval_size = \\\r\n                    max(1, min(raw_nc.size_time - raw_nc_time_index,\r\n                               max_time_step_size))\r\n\r\n                raw_nc_end_time_step_index = \\\r\n                    raw_nc_begin_time_step_index + time_interval_size\r\n                master_end_time_step_index = \\\r\n                    master_begin_time_step_index + time_interval_size\r\n\r\n                q_var[:,\r\n                      master_begin_time_step_index:master_end_time_step_index]\\\r\n                    = raw_nc.get_qout(\r\n                        time_index_start=raw_nc_begin_time_step_index,\r\n                        time_index_end=raw_nc_end_time_step_index)\r\n\r\n                master_begin_time_step_index = master_end_time_step_index\r\n                raw_nc_begin_time_step_index = raw_nc_end_time_step_index\r\n\r\n        log('Adding initial streamflow values', 'INFO')\r\n        # add initial flow to RAPID output file\r\n        if self.qinit_file and self.rapid_connect_file:\r\n            lookup_table = csv_to_list(self.rapid_connect_file)\r\n            lookup_comids = np.array([int(float(row[0])) for row\r\n                                      in lookup_table])\r\n\r\n            init_flow_table = csv_to_list(self.qinit_file)\r\n\r\n            for index, comid in enumerate(\r\n                    self.cf_nc.variables[self.output_id_dim_name][:]):\r\n                try:\r\n                    lookup_index = np.where(lookup_comids == comid)[0][0]\r\n                except IndexError:\r\n                    log('COMID %s misssing in rapid_connect file' % comid,\r\n                        'ERROR')\r\n                q_var[index, 0] = float(init_flow_table[lookup_index][0])\r\n        else:\r\n            for index, comid in enumerate(\r\n                    self.cf_nc.variables[self.output_id_dim_name][:]):\r\n                q_var[index, 0] = 0", "response": "Copies streamflow values from raw output to CF file"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef convert(self):\r\n        try:\r\n            log('Processing %s ...' % self.rapid_output_file_list[0])\r\n            time_start_conversion = datetime.utcnow()\r\n\r\n            # Validate the raw netCDF file\r\n            log('validating input netCDF file', 'INFO')\r\n            id_len, time_len = self._validate_raw_nc()\r\n\r\n            # Initialize the output file (create dimensions and variables)\r\n            log('initializing output', 'INFO')\r\n            self._initialize_output(time_len, id_len)\r\n\r\n            self._generate_time_values()\r\n\r\n            # copy river ids over\r\n            self.cf_nc.variables[self.output_id_dim_name][:] = \\\r\n                self.raw_nc_list[0].get_river_id_array()\r\n\r\n            # Populate comid, lat, lon, z\r\n            log('writing comid lat lon z')\r\n            lookup_start = datetime.now()\r\n            self._write_comid_lat_lon_z()\r\n            duration = str((datetime.now() - lookup_start).total_seconds())\r\n            log('Lookup Duration (s): ' + duration)\r\n\r\n            # Create a variable for streamflow. This is big, and slows down\r\n            # previous steps if we do it earlier.\r\n            self._copy_streamflow_values()\r\n\r\n            # close files\r\n            for raw_nc in self.raw_nc_list:\r\n                raw_nc.close()\r\n            self.cf_nc.close()\r\n\r\n            # delete original RAPID output\r\n            remove_files(*self.rapid_output_file_list)\r\n\r\n            # rename nc compliant file to original name\r\n            os.rename(self.cf_compliant_file, self.rapid_output_file_list[0])\r\n            log('Time to process %s' %\r\n                (datetime.utcnow()-time_start_conversion))\r\n        except Exception:\r\n            # delete cf RAPID output\r\n            remove_files(self.cf_compliant_file)\r\n            raise", "response": "Converts the RAPID netCDF output to CF - compliant netCDF output."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef CreateMuskingumKfacFile(in_drainage_line,\n                            river_id,\n                            length_id,\n                            slope_id,\n                            celerity,\n                            formula_type,\n                            in_connectivity_file,\n                            out_kfac_file,\n                            length_units=\"km\",\n                            slope_percentage=False,\n                            file_geodatabase=None):\n    r\"\"\"\n    Creates the Kfac file for calibration.\n\n    The improved methods using slope to generate values\n    for Kfac were used here:\n\n    Tavakoly, A. A., A. D. Snow, C. H. David, M. L. Follum, D. R. Maidment,\n    and Z.-L. Yang, (2016) \"Continental-Scale River Flow Modeling of the\n    Mississippi River Basin Using High-Resolution NHDPlus Dataset\",\n    Journal of the American Water Resources Association (JAWRA) 1-22.\n    DOI: 10.1111/1752-1688.12456\n\n    Formula Type Options:\n\n    1. :math:`Kfac_n = \\frac{RiverLength_n}{Celerity_n}`\n    2. :math:`Kfac_n = \\eta*\\frac{RiverLength_n}{\\sqrt{RiverSlope_n}}`\n    3. :math:`Kfac_n = \\eta*\\frac{RiverLength_n}{\\sqrt{RiverSlope_n}}\\left[0.05, 0.95\\right]`\n\n\n    Where:\n\n    :math:`a = \\frac{\\sum_{n=1}^{r} \\frac{RiverLength_n}{Celerity_n}}{r}`\n\n    :math:`b = \\frac{\\sum_{n=1}^{r} \\frac{RiverLength_n}{\\sqrt{RiverSlope_n}}}{r}`\n\n    :math:`\\eta = \\frac{a}{b}`\n\n    r = Number of river segments.\n\n\n    Parameters\n    ----------\n    in_drainage_line: str\n        Path to the stream network (i.e. Drainage Line) shapefile.\n    river_id: str\n        The name of the field with the river ID\n        (Ex. 'HydroID', 'COMID', or 'LINKNO').\n    length_id: str\n        The field name containging the length of the river segment\n        (Ex. 'LENGTHKM' or 'Length').\n    slope_id: str\n        The field name containging the slope of the river segment\n        (Ex. 'Avg_Slope' or 'Slope').\n    celerity: float\n        The flow wave celerity for the watershed in meters per second.\n        1 km/hr or 1000.0/3600.0 m/s is a reasonable value if unknown.\n    formula_type: int\n        An integer representing the formula type to use when calculating kfac.\n    in_connectivity_file: str\n        The path to the RAPID connectivity file.\n    out_kfac_file: str\n        The path to the output kfac file.\n    length_units: str, optional\n        The units for the length_id field. Supported types are \"m\" for meters\n        and \"km\" for kilometers.\n    slope_percentage: bool, optional\n        If True, it assumes the slope given is in percentage and will\n        divide by 100. Default is False.\n    file_geodatabase: str, optional\n        Path to the file geodatabase. If you use this option, in_drainage_line\n         is the name of the stream network feature class\n         (WARNING: Not always stable with GDAL).\n\n\n    Example::\n\n        from RAPIDpy.gis.muskingum import CreateMuskingumKfacFile\n\n        CreateMuskingumKfacFile(\n            in_drainage_line='/path/to/drainageline.shp',\n            river_id='LINKNO',\n            length_id='Length',\n            slope_id='Slope',\n            celerity=1000.0/3600.0,\n            formula_type=3,\n            in_connectivity_file='/path/to/rapid_connect.csv',\n            out_kfac_file='/path/to/kfac.csv',\n            length_units=\"m\",\n        )\n    \"\"\"  # noqa\n    ogr_drainage_line_shapefile_lyr, ogr_drainage_line_shapefile = \\\n        open_shapefile(in_drainage_line, file_geodatabase)\n\n    number_of_features = ogr_drainage_line_shapefile_lyr.GetFeatureCount()\n    river_id_list = np.zeros(number_of_features, dtype=np.int32)\n\n    length_list = \\\n        np.zeros(number_of_features, dtype=np.float32)\n    slope_list = np.zeros(number_of_features, dtype=np.float32)\n    for feature_idx, drainage_line_feature in \\\n            enumerate(ogr_drainage_line_shapefile_lyr):\n        river_id_list[feature_idx] = drainage_line_feature.GetField(river_id)\n        length = drainage_line_feature.GetField(length_id)\n        if length is not None:\n            length_list[feature_idx] = length\n        slope = drainage_line_feature.GetField(slope_id)\n        if slope is not None:\n            slope_list[feature_idx] = slope\n\n    del ogr_drainage_line_shapefile\n\n    if slope_percentage:\n        slope_list /= 100.0\n\n    if length_units == \"m\":\n        length_list /= 1000.0\n    elif length_units != \"km\":\n        raise Exception(\"Invalid length units supplied. \"\n                        \"Supported units are m and km.\")\n\n    connectivity_table = np.loadtxt(in_connectivity_file,\n                                    delimiter=\",\",\n                                    ndmin=2,\n                                    dtype=int)\n\n    length_slope_array = []\n    kfac2_array = []\n    if formula_type == 1:\n        log(\"River Length/Celerity\")\n    elif formula_type == 2:\n        log(\"Eta*River Length/Sqrt(River Slope)\")\n    elif formula_type == 3:\n        log(\"Eta*River Length/Sqrt(River Slope) [0.05, 0.95]\")\n    else:\n        raise Exception(\"Invalid formula type. Valid range: 1-3 ...\")\n\n    with open_csv(out_kfac_file, 'w') as kfacfile:\n        kfac_writer = csv_writer(kfacfile)\n        for row in connectivity_table:\n            stream_id = int(float(row[0]))\n\n            stream_id_index = river_id_list == stream_id\n            # find the length\n            stream_length = length_list[stream_id_index] * 1000.0\n\n            if formula_type >= 2:\n                # find the slope\n                stream_slope = slope_list[stream_id_index]\n\n                if stream_slope <= 0:\n                    # if no slope, take average of upstream\n                    # and downstream to get it\n                    next_down_id = int(float(row[1]))\n                    next_down_slope = 0\n                    try:\n                        next_down_index = \\\n                            np.where(river_id_list == next_down_id)[0][0]\n                        next_down_slope = slope_list[next_down_index]\n                    except IndexError:\n                        pass\n\n                    next_up_id = int(float(row[3]))\n                    next_up_slope = 0\n                    try:\n                        next_up_index = \\\n                            np.where(river_id_list == next_up_id)[0][0]\n                        next_up_slope = slope_list[next_up_index]\n                    except IndexError:\n                        pass\n\n                    stream_slope = (next_down_slope + next_up_slope) / 2.0\n                    if stream_slope <= 0:\n                        # if still no slope, set to 0.001\n                        stream_slope = 0.001\n\n                length_slope_array.append(stream_length / stream_slope**0.5)\n                kfac2_array.append(stream_length / celerity)\n            else:\n                kfac = stream_length / celerity\n                kfac_writer.writerow(kfac)\n\n        if formula_type >= 2:\n            if formula_type == 3:\n                log(\"Filtering Data by 5th and 95th Percentiles ...\")\n                length_slope_array = np.array(length_slope_array)\n                percentile_5 = np.percentile(length_slope_array, 5)\n                percentile_95 = np.percentile(length_slope_array, 95)\n\n                length_slope_array[length_slope_array < percentile_5] = \\\n                    percentile_5\n                length_slope_array[length_slope_array > percentile_95] = \\\n                    percentile_95\n\n            eta = np.mean(kfac2_array) / np.mean(length_slope_array)\n            log(\"Kfac2_Avg {0}\".format(np.mean(kfac2_array)))\n            log(\"Length_Slope Avg {0}\".format(np.mean(length_slope_array)))\n            log(\"Eta {0}\".format(eta))\n            log(\"Writing Data ...\")\n            for len_slope in length_slope_array:\n                kfac_writer.writerow(eta*len_slope)", "response": "r Creates a Muskingum Kfac file for calibration."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef CreateMuskingumKFile(lambda_k,\n                         in_kfac_file,\n                         out_k_file):\n    \"\"\"\n    Creates muskingum k file from kfac file.\n\n    Parameters\n    ----------\n    lambda_k: float\n        The value for lambda given from RAPID after the calibration process.\n        If no calibration has been performed, 0.35 is reasonable.\n    in_kfac_file: str\n        The path to the input kfac file.\n    out_k_file: str\n        The path to the output k file.\n\n\n    Example::\n\n        from RAPIDpy.gis.muskingum import CreateMuskingumKFile\n\n        CreateMuskingumKFile(\n            lambda_k=0.35,\n            in_kfac_file='/path/to/kfac.csv',\n            out_k_file='/path/to/k.csv')\n\n    \"\"\"\n    kfac_table = csv_to_list(in_kfac_file)\n\n    with open_csv(out_k_file, 'w') as kfile:\n        k_writer = csv_writer(kfile)\n        for row in kfac_table:\n            k_writer.writerow([lambda_k * float(row[0])])", "response": "Creates muskingum k file from kfac file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a muskingum X file from a drainage line.", "response": "def CreateMuskingumXFileFromDranageLine(in_drainage_line,\n                                        x_id,\n                                        out_x_file,\n                                        file_geodatabase=None):\n    \"\"\"\n    Create muskingum X file from drainage line.\n\n    Parameters\n    ----------\n    in_drainage_line: str\n        Path to the stream network (i.e. Drainage Line) shapefile.\n    x_id: str\n        The name of the muksingum X field (i.e. 'Musk_x').\n    out_x_file: str\n        The path to the output x file.\n    file_geodatabase: str, optional\n        Path to the file geodatabase. If you use this option,\n        in_drainage_line is the name of the stream network feature class\n        (WARNING: Not always stable with GDAL).\n\n\n    Example::\n\n        from RAPIDpy.gis.muskingum import CreateMuskingumXFileFromDranageLine\n\n        CreateMuskingumXFileFromDranageLine(\n            in_drainage_line='/path/to/drainageline.shp',\n            x_id='Musk_x',\n            out_x_file='/path/to/x.csv')\n\n    \"\"\"\n    ogr_drainage_line_shapefile_lyr, ogr_drainage_line_shapefile = \\\n        open_shapefile(in_drainage_line, file_geodatabase)\n\n    with open_csv(out_x_file, 'w') as kfile:\n        x_writer = csv_writer(kfile)\n        for drainage_line_feature in ogr_drainage_line_shapefile_lyr:\n            x_writer.writerow([drainage_line_feature.GetField(x_id)])\n\n    del ogr_drainage_line_shapefile"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef CreateConstMuskingumXFile(x_value,\n                              in_connectivity_file,\n                              out_x_file):\n    \"\"\"\n    Create muskingum X file from value that is constant all the way through\n    for each river segment.\n\n    Parameters\n    ----------\n    x_value: float\n        Value for the muskingum X parameter [0-0.5].\n    in_connectivity_file: str\n        The path to the RAPID connectivity file.\n    out_x_file: str\n        The path to the output x file.\n\n\n    Example::\n\n        from RAPIDpy.gis.muskingum import CreateConstMuskingumXFile\n\n        CreateConstMuskingumXFile(\n            x_value=0.3,\n            in_connectivity_file='/path/to/rapid_connect.csv',\n            out_x_file='/path/to/x.csv')\n\n    \"\"\"\n    num_rivers = 0\n    with open_csv(in_connectivity_file, \"r\") as csvfile:\n        reader = csv_reader(csvfile)\n        for _ in reader:\n            num_rivers += 1\n\n    with open_csv(out_x_file, 'w') as kfile:\n        x_writer = csv_writer(kfile)\n        for _ in xrange(num_rivers):\n            x_writer.writerow([x_value])", "response": "Creates a muskingum X file from value that is constant all the way through\notope."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef StreamIDNextDownIDToConnectivity(stream_id_array,\n                                     next_down_id_array,\n                                     out_csv_file):\n    \"\"\"\n    Creates RAPID connect file from stream_id array and next down id array\n    \"\"\"\n    list_all = []\n    max_count_upstream = 0\n\n    for hydroid in np.sort(stream_id_array):\n        # find the HydroID of the upstreams\n        list_upstreamID = stream_id_array[next_down_id_array == hydroid]\n        # count the total number of the upstreams\n        count_upstream = len(list_upstreamID)\n        if count_upstream > max_count_upstream:\n            max_count_upstream = count_upstream\n        nextDownID = next_down_id_array[stream_id_array == hydroid][0]\n        # append the list of Stream HydroID, NextDownID, Count of Upstream ID,\n        # and  HydroID of each Upstream into a larger list\n        list_all.append(\n            np.concatenate(\n                [np.array([hydroid, nextDownID, count_upstream]),\n                 list_upstreamID]\n            ).astype(int))\n\n    with open_csv(out_csv_file, 'w') as csvfile:\n        connectwriter = csv_writer(csvfile)\n        for row_list in list_all:\n            out = np.concatenate([\n                row_list,\n                np.array([0 for _ in xrange(max_count_upstream - row_list[2])])\n            ])\n            connectwriter.writerow(out.astype(int))", "response": "Creates RAPID connect file from stream_id array and next down id array."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef CreateNetworkConnectivity(in_drainage_line,\n                              river_id,\n                              next_down_id,\n                              out_connectivity_file,\n                              file_geodatabase=None):\n    \"\"\"\n    Creates Network Connectivity input CSV file for RAPID\n    based on the Drainage Line shapefile with river ID and\n    next downstream ID fields.\n\n    Parameters\n    ----------\n    in_drainage_line: str\n        Path to the stream network (i.e. Drainage Line) shapefile.\n    river_id: str\n        The name of the field with the river ID\n        (Ex. 'HydroID', 'COMID', or 'LINKNO').\n    next_down_id: str\n        The name of the field with the river ID of the next downstream\n        river segment (Ex. 'NextDownID' or 'DSLINKNO').\n    out_connectivity_file: str\n        The path to the output connectivity file.\n    file_geodatabase\n        Path to the file geodatabase. If you use this option, in_drainage_line\n        is the name of the stream network feature class.\n        (WARNING: Not always stable with GDAL.)\n\n\n    Example::\n\n        from RAPIDpy.gis.network import CreateNetworkConnectivity\n\n        CreateNetworkConnectivity(\n            in_drainage_line='/path/to/drainageline.shp',\n            river_id='LINKNO',\n            next_down_id='DSLINKNO',\n            out_connectivity_file='/path/to/rapid_connect.csv')\n\n    \"\"\"\n    ogr_drainage_line_shapefile_lyr, ogr_drainage_line_shapefile = \\\n        open_shapefile(in_drainage_line, file_geodatabase)\n\n    stream_id_array = []\n    next_down_id_array = []\n    for drainage_line_feature in ogr_drainage_line_shapefile_lyr:\n        stream_id_array.append(drainage_line_feature.GetField(river_id))\n        next_down_id_array.append(drainage_line_feature.GetField(next_down_id))\n\n    stream_id_array = np.array(stream_id_array, dtype=np.int32)\n    next_down_id_array = np.array(next_down_id_array, dtype=np.int32)\n\n    StreamIDNextDownIDToConnectivity(stream_id_array,\n                                     next_down_id_array,\n                                     out_connectivity_file)\n\n    del ogr_drainage_line_shapefile", "response": "Creates Network Connectivity input CSV file for RAPID."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef CreateNetworkConnectivityTauDEMTree(network_connectivity_tree_file,\n                                        out_csv_file):\n    \"\"\"\n    Creates Network Connectivity input CSV file for RAPID\n    based on the TauDEM network connectivity tree file\n    \"\"\"\n    stream_id_array = []\n    next_down_id_array = []\n    with open_csv(network_connectivity_tree_file, \"r\") as csvfile:\n        for row in csvfile:\n            split_row = row.split()\n            # link number\n            stream_id_array.append(split_row[0].strip())\n            # next downstream link number\n            next_down_id_array.append(split_row[3].strip())\n\n    stream_id_array = np.array(stream_id_array, dtype=np.int32)\n    next_down_id_array = np.array(next_down_id_array, dtype=np.int32)\n\n    StreamIDNextDownIDToConnectivity(stream_id_array,\n                                     next_down_id_array,\n                                     out_csv_file)", "response": "Creates Network Connectivity input CSV file for RAPID\n    based on the TauDEM network connectivity tree file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating Network Connectivity input CSV file for NHDPlus drainage lines.", "response": "def CreateNetworkConnectivityNHDPlus(in_drainage_line,\n                                     out_connectivity_file,\n                                     file_geodatabase=None):\n    \"\"\"\n    Creates Network Connectivity input CSV file for RAPID\n    based on the NHDPlus drainage lines with\n    COMID, FROMNODE, TONODE, and DIVERGENCE fields.\n\n    Parameters\n    ----------\n    in_drainage_line: str\n        Path to the stream network (i.e. Drainage Line) shapefile.\n    out_connectivity_file: str\n        The path to the output connectivity file.\n    file_geodatabase: str, optional\n        Path to the file geodatabase. If you use this option,\n        in_drainage_line is the name of the stream network feature class\n        (WARNING: Not always stable with GDAL).\n\n\n    Example::\n\n        from RAPIDpy.gis.network import CreateNetworkConnectivityNHDPlus\n\n        CreateNetworkConnectivityNHDPlus(\n            in_drainage_line='/path/to/drainageline.shp',\n            out_connectivity_file='/path/to/rapid_connect.csv')\n\n    \"\"\"\n    ogr_drainage_line_shapefile_lyr, ogr_drainage_line_shapefile = \\\n        open_shapefile(in_drainage_line, file_geodatabase)\n\n    ogr_drainage_line_definition = \\\n        ogr_drainage_line_shapefile_lyr.GetLayerDefn()\n\n    orig_field_names = []\n    for idx in xrange(ogr_drainage_line_definition.GetFieldCount()):\n        orig_field_names.append(\n            ogr_drainage_line_definition.GetFieldDefn(idx).GetName())\n\n    upper_field_names = [field.upper() for field in orig_field_names]\n\n    def get_field_name_index(upper_field_name, _upper_field_names):\n        \"\"\"\n        returns index of field name\n        \"\"\"\n        try:\n            return _upper_field_names.index(upper_field_name)\n        except ValueError:\n            raise IndexError(\"{0} not found in shapefile ..\"\n                             .format(_upper_field_names))\n\n    rivid_field = \\\n        orig_field_names[get_field_name_index('COMID', upper_field_names)]\n    fromnode_field = \\\n        orig_field_names[get_field_name_index('FROMNODE', upper_field_names)]\n    tonode_field = \\\n        orig_field_names[get_field_name_index('TONODE', upper_field_names)]\n    divergence_field =\\\n        orig_field_names[get_field_name_index('DIVERGENCE', upper_field_names)]\n\n    number_of_features = ogr_drainage_line_shapefile_lyr.GetFeatureCount()\n    rivid_list = np.zeros(number_of_features, dtype=np.int32)\n    fromnode_list = np.zeros(number_of_features, dtype=np.int32)\n    tonode_list = np.zeros(number_of_features, dtype=np.int32)\n    divergence_list = np.zeros(number_of_features, dtype=np.int32)\n    for feature_idx, catchment_feature in \\\n            enumerate(ogr_drainage_line_shapefile_lyr):\n        rivid_list[feature_idx] = catchment_feature.GetField(rivid_field)\n        fromnode_list[feature_idx] = catchment_feature.GetField(fromnode_field)\n        tonode_list[feature_idx] = catchment_feature.GetField(tonode_field)\n        divergence_list[feature_idx] = \\\n            catchment_feature.GetField(divergence_field)\n\n    del ogr_drainage_line_shapefile\n    # -------------------------------------------------------------------------\n    # Compute connectivity, based on:\n    # https://github.com/c-h-david/rrr/blob/master/src/rrr_riv_tot_gen_all_nhdplus.py\n    # -------------------------------------------------------------------------\n    fromnode_list[fromnode_list == 0] = -9999\n    # Some NHDPlus v1 reaches have FLOWDIR='With Digitized'\n    # but no info in VAA table\n\n    fromnode_list[divergence_list == 2] = -9999\n    # Virtually disconnect the upstream node of all minor divergences\n    del divergence_list  # delete information in list\n\n    next_down_id_list = np.zeros(number_of_features, dtype=np.int32)\n    for rivid_index in xrange(len(rivid_list)):\n        try:\n            next_down_id_list[rivid_index] = \\\n                rivid_list[\n                    np.where(fromnode_list == tonode_list[rivid_index])[0][0]]\n        except IndexError:\n            # this is an outlet\n            next_down_id_list[rivid_index] = -1\n\n    # determine the downstream reach for each reach\n\n    # empty unecessary lists\n    del fromnode_list\n    del tonode_list\n\n    StreamIDNextDownIDToConnectivity(rivid_list,\n                                     next_down_id_list,\n                                     out_connectivity_file)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef CreateSubsetFile(in_drainage_line,\n                     river_id,\n                     out_riv_bas_id_file,\n                     file_geodatabase=None):\n    \"\"\"\n    Creates River Basin ID subset input CSV file for RAPID\n    based on the Drainage Line shapefile with river ID and\n    next downstream ID fields\n\n    Parameters\n    ----------\n    in_drainage_line: str\n        Path to the stream network (i.e. Drainage Line) shapefile.\n    river_id: str\n        The name of the field with the river ID\n        (Ex. 'HydroID', 'COMID', or 'LINKNO').\n    out_riv_bas_id_file: str\n        The path to the output river basin ID subset file.\n    file_geodatabase: str, optional\n        Path to the file geodatabase. If you use this option,\n        in_drainage_line is the name of the stream network feature class\n        (WARNING: Not always stable with GDAL).\n\n\n    Example::\n\n        from RAPIDpy.gis.network import CreateSubsetFile\n\n        CreateSubsetFile(\n            in_drainage_line='/path/to/drainageline.shp',\n            river_id='LINKNO',\n            out_riv_bas_id_file='/path/to/riv_bas_id.csv')\n\n    \"\"\"\n    ogr_drainage_line_shapefile_lyr, ogr_drainage_line_shapefile = \\\n        open_shapefile(in_drainage_line, file_geodatabase)\n\n    ogr_drainage_line_definition = \\\n        ogr_drainage_line_shapefile_lyr.GetLayerDefn()\n\n    orig_field_names = []\n    for idx in xrange(ogr_drainage_line_definition.GetFieldCount()):\n        orig_field_names.append(\n            ogr_drainage_line_definition.GetFieldDefn(idx).GetName())\n\n    upper_field_names = [field.upper() for field in orig_field_names]\n    sort_field = None\n\n    # Sort by HYDROSEQ order if the option exists\n    if 'HYDROSEQ' in upper_field_names:\n        # with this method, smaller is downstream\n        sort_field = orig_field_names[upper_field_names.index('HYDROSEQ')]\n        log(\"Sorting by {0}\".format(sort_field))\n\n    hydroseq_list = []\n    hydroid_list = []\n    # The script line below makes sure that rows in the subset file are\n    # arranged in descending order of NextDownID of stream segements\n    for drainage_line_feature in ogr_drainage_line_shapefile_lyr:\n        hydroid_list.append(drainage_line_feature.GetField(river_id))\n        if sort_field:\n            hydroseq_list.append(drainage_line_feature.GetField(sort_field))\n\n    del ogr_drainage_line_shapefile\n\n    hydroid_list = np.array(hydroid_list, dtype=np.int32)\n    if hydroseq_list:\n        hydroseq_list = np.array(hydroseq_list, dtype=np.int32)\n        sort_order = hydroseq_list.argsort()[::-1]\n        hydroid_list = hydroid_list[sort_order]\n    else:\n        hydroid_list = np.sort(hydroid_list)\n\n    with open_csv(out_riv_bas_id_file, 'w') as csvfile:\n        connectwriter = csv_writer(csvfile)\n        for hydroid in hydroid_list:\n            connectwriter.writerow([hydroid])", "response": "Creates a RAPID base ID subset file for the given Drainage Line shapefile."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef CreateAllStaticRAPIDFiles(in_drainage_line,\n                              river_id,\n                              length_id,\n                              slope_id,\n                              next_down_id,\n                              rapid_output_folder,\n                              kfac_celerity=1000.0/3600.0,\n                              kfac_formula_type=3,\n                              kfac_length_units=\"km\",\n                              lambda_k=0.35,\n                              x_value=0.3,\n                              nhdplus=False,\n                              taudem_network_connectivity_tree_file=None,\n                              file_geodatabase=None):\n    \"\"\"\n    To generate the static RAPID files (rapid_connect.csv, riv_bas_id.csv,\n    kfac.csv, k.csv, x.csv, comid_lat_lon_z.csv) with default values.\n\n    Parameters\n    ----------\n    in_drainage_line: str\n        Path to the stream network (i.e. Drainage Line) shapefile.\n    river_id: str\n        The name of the field with the river ID\n        (Ex. 'HydroID', 'COMID', or 'LINKNO').\n    length_id: str\n        The field name containging the length of the river segment\n        (Ex. 'LENGTHKM' or 'Length').\n    slope_id: str\n        The field name containging the slope of the river segment\n        (Ex. 'Avg_Slope' or 'Slope').\n    next_down_id: str\n        The name of the field with the river ID of the next downstream river\n        segment (Ex. 'NextDownID' or 'DSLINKNO').\n    rapid_output_folder: str\n        The path to the folder where all of the RAPID output will be generated.\n    kfac_celerity: float, optional\n        The flow wave celerity for the watershed in meters per second.\n        1 km/hr or 1000.0/3600.0 m/s is a reasonable value if unknown.\n    kfac_formula_type: int, optional\n        An integer representing the formula type to use when calculating kfac.\n        Default is 3.\n    kfac_length_units: str, optional\n        The units for the length_id field. Supported types are \"m\" for meters\n        and \"km\" for kilometers. Default is \"km\".\n    lambda_k: float, optional\n        The value for lambda given from RAPID after the calibration process.\n        Default is 0.35.\n    x_value: float, optional\n        Value for the muskingum X parameter [0-0.5]. Default is 0.3.\n    nhdplus: bool, optional\n        If True, the drainage line is from the NHDPlus dataset with the VAA\n        fields COMID, FROMNODE, TONODE, and DIVERGENCE. Default is False.\n    taudem_network_connectivity_tree_file: str, optional\n        If set, the connectivity file will be generated from the TauDEM\n        connectivity tree file.\n    file_geodatabase: str, optional\n        Path to the file geodatabase. If you use this option,\n        in_drainage_line is the name of the stream network feature class.\n        (WARNING: Not always stable with GDAL.)\n\n\n    Example::\n\n        from RAPIDpy.gis.workflow import CreateAllStaticRAPIDFiles\n\n        CreateAllStaticRAPIDFiles(\n            in_drainage_line=\"/path/to/drainage_line.shp\",\n            river_id=\"HydroID\",\n            length_id=\"LENGTHKM\",\n            slope_id=\"SLOPE\",\n            next_down_river_id=\"NextDownID\",\n            rapid_output_folder=\"/path/to/rapid/output\",\n        )\n    \"\"\"\n    # RAPID connect file\n    rapid_connect_file = os.path.join(rapid_output_folder, 'rapid_connect.csv')\n    if nhdplus:\n        CreateNetworkConnectivityNHDPlus(in_drainage_line,\n                                         rapid_connect_file,\n                                         file_geodatabase)\n    elif taudem_network_connectivity_tree_file:\n        CreateNetworkConnectivityTauDEMTree(\n            taudem_network_connectivity_tree_file,\n            rapid_connect_file)\n    else:\n        CreateNetworkConnectivity(in_drainage_line,\n                                  river_id,\n                                  next_down_id,\n                                  rapid_connect_file,\n                                  file_geodatabase)\n\n    # river basin id file\n    riv_bas_id_file = os.path.join(rapid_output_folder, 'riv_bas_id.csv')\n    CreateSubsetFile(in_drainage_line,\n                     river_id,\n                     riv_bas_id_file,\n                     file_geodatabase)\n    # kfac file\n    kfac_file = os.path.join(rapid_output_folder, 'kfac.csv')\n    CreateMuskingumKfacFile(in_drainage_line,\n                            river_id,\n                            length_id,\n                            slope_id,\n                            kfac_celerity,\n                            kfac_formula_type,\n                            rapid_connect_file,\n                            kfac_file,\n                            length_units=kfac_length_units,\n                            file_geodatabase=file_geodatabase)\n    # k file\n    k_file = os.path.join(rapid_output_folder, 'k.csv')\n    CreateMuskingumKFile(lambda_k,\n                         kfac_file,\n                         k_file)\n    # x file\n    x_file = os.path.join(rapid_output_folder, 'x.csv')\n    CreateConstMuskingumXFile(x_value,\n                              rapid_connect_file,\n                              x_file)\n    # comid lat lon z file\n    comid_lat_lon_z_file = \\\n        os.path.join(rapid_output_folder, 'comid_lat_lon_z.csv')\n    FlowlineToPoint(in_drainage_line,\n                    river_id,\n                    comid_lat_lon_z_file,\n                    file_geodatabase)", "response": "Creates all of the static RAPID files."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef CreateAllStaticECMWFFiles(in_catchment,\n                              catchment_river_id,\n                              rapid_output_folder,\n                              rapid_connect_file,\n                              file_geodatabase=None\n                              ):\n    \"\"\"\n    This creates all of the ECMWF grid weight tables using an area\n    weighted method based on Esri's RAPID_Toolbox.\n\n    Parameters\n    ----------\n    in_catchment: str\n        Path to the Catchment shapefile.\n    catchment_river_id: str\n        The name of the field with the river ID (Ex. 'DrainLnID' or 'LINKNO').\n    rapid_output_folder: str\n        The path to the folder where all of the RAPID output will be generated.\n    rapid_connect_file: str\n        The path to the RAPID connectivity file.\n    file_geodatabase: str, optional\n        Path to the file geodatabase. If you use this option,\n        in_drainage_line is the name of the stream network feature class.\n        (WARNING: Not always stable with GDAL.)\n\n\n    Example::\n\n        from RAPIDpy.gis.workflow import CreateAllStaticECMWFFiles\n\n        CreateAllStaticECMWFFiles(\n            in_catchment=\"/path/to/catchment.shp\",\n            catchment_river_id=\"DrainLnID\",\n            rapid_output_folder=\"/path/to/rapid/output\",\n            rapid_connect_file=\"/path/to/rapid_connect.csv\",\n        )\n\n    \"\"\"\n    lsm_grid_folder = \\\n        os.path.join(os.path.dirname(os.path.realpath(__file__)), 'lsm_grids')\n\n    # create from ECMWF high reslution grid\n    ecmwf_t1279_grid_file = \\\n        os.path.join(lsm_grid_folder, 'runoff_ecmwf_t1279_grid.nc')\n    weight_ecmwf_t1279_file = \\\n        os.path.join(rapid_output_folder, 'weight_ecmwf_t1279.csv')\n    CreateWeightTableECMWF(ecmwf_t1279_grid_file,\n                           in_catchment,\n                           catchment_river_id,\n                           rapid_connect_file,\n                           weight_ecmwf_t1279_file,\n                           file_geodatabase=file_geodatabase)\n\n    # create from ECMWF low reslution grid\n    ecmwf_tco639_grid_file = \\\n        os.path.join(lsm_grid_folder, 'runoff_ecmwf_tco639_grid.nc')\n    weight_ecmwf_tco639_file = \\\n        os.path.join(rapid_output_folder, 'weight_ecmwf_tco639.csv')\n    CreateWeightTableECMWF(ecmwf_tco639_grid_file,\n                           in_catchment,\n                           catchment_river_id,\n                           rapid_connect_file,\n                           weight_ecmwf_tco639_file,\n                           file_geodatabase=file_geodatabase)\n\n    # create from ERA Interim grid\n    era_t511_grid_file = \\\n        os.path.join(lsm_grid_folder, 'runoff_era_t511_grid.nc')\n    weight_era_t511_file = \\\n        os.path.join(rapid_output_folder, 'weight_era_t511.csv')\n    CreateWeightTableECMWF(era_t511_grid_file,\n                           in_catchment,\n                           catchment_river_id,\n                           rapid_connect_file,\n                           weight_era_t511_file,\n                           file_geodatabase=file_geodatabase)", "response": "Creates all of the static ECMWF grid weight tables using a high reslution grid."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef compare_qout_files(dataset1_path, dataset2_path):\n    qout_same = False\n\n    d1 = RAPIDDataset(dataset1_path)\n    d2 = RAPIDDataset(dataset2_path)\n\n    if len(d1.get_river_id_array()) != len(d2.get_river_id_array()):\n        log(\"Length of COMID/rivid input not the same.\",\n            \"ERROR\")\n\n    if not (d1.get_river_id_array() == d2.get_river_id_array()).all():\n        log(\"COMID/rivid order is different in each dataset.\"\n            \" Reordering data for comparison.\",\n            \"WARNING\")\n\n        d2_reordered_river_index_list = []\n        for rivid in d1.get_river_id_array():\n            reordered_index = np.where(d2.get_river_id_array() == rivid)[0][0]\n            d2_reordered_river_index_list.append(reordered_index)\n        d2_reordered_qout = d2.get_qout_index(d2_reordered_river_index_list)\n    else:\n        d2_reordered_qout = d2.get_qout()\n\n    # get where the files are different\n    d1_qout = d1.get_qout()\n    where_diff = np.where(d1_qout != d2_reordered_qout)\n    un_where_diff = np.unique(where_diff[0])\n\n    # if different, check to see how different\n    if un_where_diff.any():\n        decimal_test = 7\n        while decimal_test > 0:\n            try:\n                np.testing.assert_almost_equal(d1_qout,\n                                               d2_reordered_qout,\n                                               decimal=decimal_test)\n                log(\"ALMOST EQUAL to {0} decimal places.\".format(decimal_test),\n                    \"INFO\")\n                qout_same = True\n                decimal_test = -1\n            except AssertionError as ex:\n                if decimal_test <= 1:\n                    log(ex, \"WARNING\")\n                decimal_test -= 1\n\n        log(\"Number of different timeseries: {0}\".format(len(un_where_diff)),\n            \"INFO\")\n        log(\"COMID idexes where different: {0}\".format(un_where_diff),\n            \"INFO\")\n        log(\"COMID idexes where different: {0}\".format(un_where_diff),\n            \"INFO\")\n        index = un_where_diff[0]\n        log(\"Dataset 1 example. COMID index: \"\n            \"{0}\".format(d1.get_qout_index(index)),\n            \"INFO\")\n        log(\"Dataset 2 example. COMID index: \"\n            \"{0}\".format(d2_reordered_qout[index, :]),\n            \"INFO\")\n\n    else:\n        qout_same = True\n        log(\"Output Qout data is the same.\",\n            \"INFO\")\n\n    d1.close()\n    d2.close()\n    return qout_same", "response": "This function compares the output of RAPID Qout and tells you where they are different."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_time_variable_valid(self):\n        # pylint: disable=len-as-condition\n        time_var_valid = False\n        if 'time' in self.qout_nc.variables.keys():\n            if len(self.qout_nc.dimensions['time']) > 0:\n                if not is_masked(self.qout_nc.variables['time'][:]):\n                    try:\n                        timestep = (datetime.datetime\n                                    .utcfromtimestamp(\n                                        self.qout_nc.variables['time'][1]\n                                    ) -\n                                    datetime.datetime\n                                    .utcfromtimestamp(\n                                        self.qout_nc.variables['time'][0]\n                                    )).total_seconds()\n                        if timestep > 0:\n                            time_var_valid = True\n                    except ValueError:\n                        pass\n\n        return time_var_valid", "response": "This function returns whether or not the time variable is valid."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_time_array(self,\n                       datetime_simulation_start=None,\n                       simulation_time_step_seconds=None,\n                       return_datetime=False,\n                       time_index_array=None):\n        \"\"\"\n        This method extracts or generates an array of time.\n        The new version of RAPID output has the time array stored.\n        However, the old version requires the user to know when the\n        simulation began and the time step of the output.\n\n        Parameters\n        ----------\n        datetime_simulation_start: :obj:`datetime.datetime`, optional\n            The start datetime of the simulation. Only required if the time\n            variable is not included in the file.\n        simulation_time_step_seconds: int, optional\n            The time step of the file in seconds. Only required if the time\n            variable is not included in the file.\n        return_datetime: bool, optional\n            If true, it converts the data to a list of datetime objects.\n            Default is False.\n        time_index_array: list or :obj:`numpy.array`, optional\n            This is used to extract the datetime values by index from the main\n            list. This can be from the *get_time_index_range* function.\n\n        Returns\n        -------\n        list:\n            An array of integers representing seconds since Jan 1, 1970 UTC\n            or datetime objects if *return_datetime* is set to True.\n\n        These examples demonstrates how to retrieve or generate a time array\n        to go along with your RAPID streamflow series.\n\n\n        CF-Compliant Qout File Example:\n\n        .. code:: python\n\n            from RAPIDpy import RAPIDDataset\n\n            path_to_rapid_qout = '/path/to/Qout.nc'\n            with RAPIDDataset(path_to_rapid_qout) as qout_nc:\n                #retrieve integer timestamp array\n                time_array = qout_nc.get_time_array()\n\n                #or, to get datetime array\n                time_datetime = qout_nc.get_time_array(return_datetime=True)\n\n\n        Legacy Qout File Example:\n\n        .. code:: python\n\n            from RAPIDpy import RAPIDDataset\n\n            path_to_rapid_qout = '/path/to/Qout.nc'\n            with RAPIDDataset(path_to_rapid_qout,\n                              datetime_simulation_start=datetime(1980, 1, 1),\n                              simulation_time_step_seconds=3 * 3600)\\\n                    as qout_nc:\n\n                #retrieve integer timestamp array\n                time_array = qout_nc.get_time_array()\n\n                #or, to get datetime array\n                time_datetime = qout_nc.get_time_array(return_datetime=True)\n\n        \"\"\"\n        # Original Qout file\n        if datetime_simulation_start is not None:\n            self.datetime_simulation_start = datetime_simulation_start\n        if simulation_time_step_seconds is not None:\n            self.simulation_time_step_seconds = simulation_time_step_seconds\n\n        epoch = datetime.datetime(1970, 1, 1, tzinfo=utc)\n        time_units = \"seconds since {0}\".format(epoch)\n\n        # CF-1.6 compliant file\n        if self.is_time_variable_valid():\n            time_array = self.qout_nc.variables['time'][:]\n            if self.qout_nc.variables['time'].units:\n                time_units = self.qout_nc.variables['time'].units\n\n        # Original Qout file\n        elif self._is_legacy_time_valid():\n            initial_time_seconds = ((self.datetime_simulation_start\n                                    .replace(tzinfo=utc) - epoch)\n                                    .total_seconds() +\n                                    self.simulation_time_step_seconds)\n            final_time_seconds = (initial_time_seconds +\n                                  self.size_time *\n                                  self.simulation_time_step_seconds)\n            time_array = np.arange(initial_time_seconds,\n                                   final_time_seconds,\n                                   self.simulation_time_step_seconds)\n        else:\n            raise ValueError(\"This file does not contain the time\"\n                             \" variable. To get time array, add\"\n                             \" datetime_simulation_start and\"\n                             \" simulation_time_step_seconds\")\n\n        if time_index_array is not None:\n            time_array = time_array[time_index_array]\n\n        if return_datetime:\n            time_array = num2date(time_array, time_units)\n\n            if self.out_tzinfo is not None:\n                for i in xrange(len(time_array)):\n                    # convert time to output timezone\n                    time_array[i] = utc.localize(time_array[i]) \\\n                                       .astimezone(self.out_tzinfo) \\\n                                       .replace(tzinfo=None)\n\n        return time_array", "response": "This method extracts or generates a time array from the RAPID output file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate a time index range based on time bounds given. This is useful for subset data extraction. Parameters ---------- date_search_start: :obj:`datetime.datetime`, optional This is a datetime object with the date of the minimum date for starting. date_search_end: :obj:`datetime.datetime`, optional This is a datetime object with the date of the maximum date for ending. time_index_start: int, optional This is the index of the start of the time array subset. Useful for the old file version. time_index_end: int, optional This is the index of the end of the time array subset. Useful for the old file version. time_index: int, optional This is the index of time to return in the case that your code only wants one index. Used internally. Returns ------- :obj:`numpy.array`: This is an array of time indices used to extract a subset of data. CF-Compliant Qout File Example: .. code:: python from datetime import datetime from RAPIDpy import RAPIDDataset path_to_rapid_qout = '/path/to/Qout.nc' with RAPIDDataset(path_to_rapid_qout) as qout_nc: time_index_range = qout_nc.get_time_index_range( date_search_start=datetime(1980, 1, 1), date_search_end=datetime(1980, 12, 11)) Legacy Qout File Example: .. code:: python from datetime import datetime from RAPIDpy import RAPIDDataset path_to_rapid_qout = '/path/to/Qout.nc' with RAPIDDataset(path_to_rapid_qout, datetime_simulation_start=datetime(1980, 1, 1), simulation_time_step_seconds=3600) as qout_nc: time_index_range = qout_nc.get_time_index_range( date_search_start=datetime(1980, 1, 1), date_search_end=datetime(1980, 12, 11))", "response": "def get_time_index_range(self,\n                             date_search_start=None,\n                             date_search_end=None,\n                             time_index_start=None,\n                             time_index_end=None,\n                             time_index=None):\n        \"\"\"\n        Generates a time index range based on time bounds given.\n        This is useful for subset data extraction.\n\n        Parameters\n        ----------\n        date_search_start: :obj:`datetime.datetime`, optional\n            This is a datetime object with the date of the minimum date for\n            starting.\n        date_search_end: :obj:`datetime.datetime`, optional\n            This is a datetime object with the date of the maximum date\n            for ending.\n        time_index_start: int, optional\n            This is the index of the start of the time array subset.\n            Useful for the old file version.\n        time_index_end: int, optional\n            This is the index of the end of the time array subset.\n            Useful for the old file version.\n        time_index: int, optional\n            This is the index of time to return in the case that your\n            code only wants one index. Used internally.\n\n        Returns\n        -------\n        :obj:`numpy.array`:\n            This is an array of time indices used to extract a subset of data.\n\n\n        CF-Compliant Qout File Example:\n\n        .. code:: python\n\n            from datetime import datetime\n            from RAPIDpy import RAPIDDataset\n\n            path_to_rapid_qout = '/path/to/Qout.nc'\n            with RAPIDDataset(path_to_rapid_qout) as qout_nc:\n                time_index_range = qout_nc.get_time_index_range(\n                    date_search_start=datetime(1980, 1, 1),\n                    date_search_end=datetime(1980, 12, 11))\n\n\n        Legacy Qout File Example:\n\n        .. code:: python\n\n            from datetime import datetime\n            from RAPIDpy import RAPIDDataset\n\n            path_to_rapid_qout = '/path/to/Qout.nc'\n            with RAPIDDataset(path_to_rapid_qout,\n                              datetime_simulation_start=datetime(1980, 1, 1),\n                              simulation_time_step_seconds=3600) as qout_nc:\n\n                time_index_range = qout_nc.get_time_index_range(\n                    date_search_start=datetime(1980, 1, 1),\n                    date_search_end=datetime(1980, 12, 11))\n\n        \"\"\"\n        # get the range of time based on datetime range\n        time_range = None\n        if ((self.is_time_variable_valid() or self._is_legacy_time_valid()) and\n                (date_search_start is not None or\n                 date_search_end is not None)):\n\n            log(\"Determining time range ({0} to {1})\"\n                \"...\".format(date_search_start, date_search_end),\n                \"INFO\")\n            time_array = self.get_time_array()\n            if date_search_start is not None:\n                date_search_start_utc = date_search_start\n                if self.out_tzinfo is not None:\n                    date_search_start_utc = self.out_tzinfo \\\n                                                .localize(date_search_start) \\\n                                                .astimezone(utc) \\\n                                                .replace(tzinfo=None)\n                seconds_start = (date_search_start_utc -\n                                 datetime.datetime(1970, 1, 1)).total_seconds()\n                time_range = np.where(time_array >= seconds_start)[0]\n\n            if date_search_end is not None:\n                date_search_end_utc = date_search_end\n                if self.out_tzinfo is not None:\n                    date_search_end_utc = self.out_tzinfo \\\n                                              .localize(date_search_end) \\\n                                              .astimezone(utc) \\\n                                              .replace(tzinfo=None)\n\n                seconds_end = (date_search_end_utc -\n                               datetime.datetime(1970, 1, 1)).total_seconds()\n                if time_range is not None:\n                    time_range = np.intersect1d(time_range,\n                                                np.where(time_array <=\n                                                         seconds_end)[0])\n                else:\n                    time_range = np.where(time_array <= seconds_end)[0]\n\n        # get the range of time based on time index range\n        elif time_index_start is not None or time_index_end is not None:\n            if time_index_start is None:\n                time_index_start = 0\n            if time_index_end is None:\n                time_index_end = self.size_time\n            time_range = range(time_index_start, time_index_end)\n\n        # get only one time step\n        elif time_index is not None:\n            time_range = [time_index]\n        # return all\n        else:\n            time_range = range(self.size_time)\n\n        return time_range"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_river_index(self, river_id):\n        try:\n            return np.where(self.get_river_id_array() == river_id)[0][0]\n        except IndexError:\n            raise IndexError(\"ERROR: River ID {0} not found in dataset \"\n                             \"...\".format(river_id))", "response": "This method retrieves the index of the river segment corresponding to the river ID."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the subset riverid_list from the netcdf file that contains the river segments that are not in the dataset.", "response": "def get_subset_riverid_index_list(self, river_id_list):\n        \"\"\"\n        Gets the subset riverid_list from the netcdf file\n        Optional returns include the list of valid river ids in the dataset\n        as well as a list of missing rive rids\n\n        Parameters\n        ----------\n        river_id_list: list or :obj:`numpy.array`\n            Array of river ID's for the river segments you want the index of.\n\n        Returns\n        -------\n        :obj:`numpy.array`\n            A sorted array of the river index in the NetCDF file that\n            were found.\n        :obj:`numpy.array`\n            A sorted array of the river IDs that were found.\n        list\n            An array of the missing river ids.\n\n        \"\"\"\n        netcdf_river_indices_list = []\n        valid_river_ids = []\n        missing_river_ids = []\n        for river_id in river_id_list:\n            # get where streamids are in netcdf file\n            try:\n                netcdf_river_indices_list \\\n                    .append(self.get_river_index(river_id))\n                valid_river_ids.append(river_id)\n            except IndexError:\n                log(\"ReachID {0} not found in netCDF dataset.\"\n                    \" Skipping ...\".format(river_id),\n                    \"WARNING\")\n                missing_river_ids.append(river_id)\n\n        np_valid_river_indices_list = np.array(netcdf_river_indices_list)\n        np_valid_river_ids = np.array(valid_river_ids)\n        sorted_indexes = np.argsort(np_valid_river_indices_list)\n\n        return(np_valid_river_indices_list[sorted_indexes],\n               np_valid_river_ids[sorted_indexes],\n               np.array(missing_river_ids))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_qout_index(self,\n                       river_index_array=None,\n                       date_search_start=None,\n                       date_search_end=None,\n                       time_index_start=None,\n                       time_index_end=None,\n                       time_index=None,\n                       time_index_array=None,\n                       daily=False,\n                       pd_filter=None,\n                       filter_mode=\"mean\",\n                       as_dataframe=False):\n        \"\"\"\n        This method extracts streamflow data by river index.\n        It allows for extracting single or multiple river streamflow arrays\n        It has options to extract by date or by date index.\n\n        See: :meth:`RAPIDpy.RAPIDDataset.get_qout`\n        \"\"\"\n        if river_index_array is not None:\n            if hasattr(river_index_array, \"__len__\"):\n                if len(river_index_array) == 1:\n                    river_index_array = river_index_array[0]\n\n        if time_index_array is None:\n            time_index_array = self.get_time_index_range(date_search_start,\n                                                         date_search_end,\n                                                         time_index_start,\n                                                         time_index_end,\n                                                         time_index)\n\n        qout_variable = self.qout_nc.variables[self.q_var_name]\n        qout_dimensions = qout_variable.dimensions\n        if qout_dimensions[0].lower() == 'time' and \\\n                qout_dimensions[1].lower() == self.river_id_dimension.lower():\n            if time_index_array is not None and river_index_array is not None:\n                streamflow_array = qout_variable[time_index_array,\n                                                 river_index_array].transpose()\n            elif time_index_array is not None:\n                streamflow_array = qout_variable[time_index_array, :] \\\n                                   .transpose()\n            elif river_index_array is not None:\n                streamflow_array = qout_variable[:, river_index_array] \\\n                                   .transpose()\n            else:\n                streamflow_array = qout_variable[:].transpose()\n        elif qout_dimensions[1].lower() == 'time' and \\\n                qout_dimensions[0].lower() == self.river_id_dimension.lower():\n            if time_index_array is not None and river_index_array is not None:\n                streamflow_array = qout_variable[river_index_array,\n                                                 time_index_array]\n            elif time_index_array is not None:\n                streamflow_array = qout_variable[:, time_index_array]\n            elif river_index_array is not None:\n                streamflow_array = qout_variable[river_index_array, :]\n            else:\n                streamflow_array = qout_variable[:]\n        else:\n            raise Exception(\"Invalid RAPID Qout file dimensions ...\")\n\n        if daily:\n            pd_filter = \"D\"\n\n        if pd_filter is not None or as_dataframe:\n            time_array = self.get_time_array(return_datetime=True,\n                                             time_index_array=time_index_array)\n            qout_df = pd.DataFrame(streamflow_array.T, index=time_array)\n\n            if pd_filter is not None:\n                qout_df = qout_df.resample(pd_filter)\n                if filter_mode == \"mean\":\n                    qout_df = qout_df.mean()\n                elif filter_mode == \"max\":\n                    qout_df = qout_df.max()\n                else:\n                    raise Exception(\"Invalid filter_mode ...\")\n\n            if as_dataframe:\n                return qout_df\n\n            streamflow_array = qout_df.as_matrix().T\n\n            if streamflow_array.ndim > 0 and streamflow_array.shape[0] == 1:\n                streamflow_array = streamflow_array[0]\n\n        return streamflow_array", "response": "This method extracts streamflow data by river index."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write_flows_to_csv(self, path_to_output_file,\n                           river_index=None,\n                           river_id=None,\n                           date_search_start=None,\n                           date_search_end=None,\n                           daily=False,\n                           filter_mode=\"mean\"):\n        \"\"\"\n        Write out RAPID output to CSV file.\n\n        .. note:: Need either *reach_id* or *reach_index* parameter,\n                  but either can be used.\n\n        Parameters\n        ----------\n        path_to_output_file: str\n            Path to the output csv file.\n        river_index: :obj:`datetime.datetime`, optional\n            This is the index of the river in the file you want the\n            streamflow for.\n        river_id: :obj:`datetime.datetime`, optional\n            This is the river ID that you want the streamflow for.\n        date_search_start: :obj:`datetime.datetime`, optional\n            This is a datetime object with the date of the minimum date\n            for starting.\n        date_search_end: :obj:`datetime.datetime`, optional\n            This is a datetime object with the date of the maximum date\n            for ending.\n        daily: bool, optional\n            If True and the file is CF-Compliant, write out daily flows.\n        filter_mode: str, optional\n            You can get the daily average \"mean\" or the maximum \"max\".\n            Default is \"mean\".\n\n\n        Example writing entire time series to file:\n\n        .. code:: python\n\n            from RAPIDpy import RAPIDDataset\n\n            river_id = 3624735\n            path_to_rapid_qout = '/path/to/Qout.nc'\n\n            with RAPIDDataset(path_to_rapid_qout) as qout_nc:\n                #for writing entire time series to file\n                qout_nc.write_flows_to_csv('/timeseries/Qout_3624735.csv',\n                                           river_id=river_id,\n                                           )\n\n\n                #if file is CF compliant, you can write out daily average\n\n                #NOTE: Getting the river index is not necessary\n                #this is just an example of how to use this\n                river_index = qout_nc.get_river_index(river_id)\n                qout_nc.write_flows_to_csv('/timeseries/Qout_daily.csv',\n                                           river_index=river_index,\n                                           daily=True,\n                                           )\n\n        Example writing entire time series as daily average to file:\n\n        .. code:: python\n\n            from RAPIDpy import RAPIDDataset\n\n            river_id = 3624735\n            path_to_rapid_qout = '/path/to/Qout.nc'\n\n            with RAPIDDataset(path_to_rapid_qout) as qout_nc:\n                #NOTE: Getting the river index is not necessary\n                #this is just an example of how to use this\n                river_index = qout_nc.get_river_index(river_id)\n\n                #if file is CF compliant, you can write out daily average\n                qout_nc.write_flows_to_csv('/timeseries/Qout_daily.csv',\n                                           river_index=river_index,\n                                           daily=True,\n                                           )\n\n        Example writing entire time series as daily average to file:\n\n        .. code:: python\n\n            from datetime import datetime\n            from RAPIDpy import RAPIDDataset\n\n            river_id = 3624735\n            path_to_rapid_qout = '/path/to/Qout.nc'\n\n            with RAPIDDataset(path_to_rapid_qout) as qout_nc:\n                # if file is CF compliant, you can filter by date\n                qout_nc.write_flows_to_csv(\n                    '/timeseries/Qout_daily_date_filter.csv',\n                    river_id=river_id,\n                    daily=True,\n                    date_search_start=datetime(2002, 8, 31),\n                    date_search_end=datetime(2002, 9, 15),\n                    filter_mode=\"max\"\n                )\n        \"\"\"\n        if river_id is not None:\n            river_index = self.get_river_index(river_id)\n        elif river_id is None and river_index is None:\n            raise ValueError(\"Need reach id or reach index ...\")\n\n        # analyze and write\n        if self.is_time_variable_valid() or self._is_legacy_time_valid():\n            qout_df = self.get_qout_index(river_index,\n                                          date_search_start=date_search_start,\n                                          date_search_end=date_search_end,\n                                          daily=daily,\n                                          filter_mode=filter_mode,\n                                          as_dataframe=True)\n\n            qout_df.to_csv(path_to_output_file, header=False)\n\n        else:\n            log(\"Valid time variable not found. Printing values only ...\",\n                \"WARNING\")\n            qout_arr = self.get_qout_index(river_index)\n            with open_csv(path_to_output_file, 'w') as outcsv:\n                writer = csv_writer(outcsv)\n                for index in xrange(len(qout_arr)):\n                    writer.writerow([index, \"{0:.5f}\".format(qout_arr[index])])", "response": "Writes out the flows of the river in the file to a CSV file."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwrites out flows to GSSHA time series xys file.", "response": "def write_flows_to_gssha_time_series_xys(self,\n                                             path_to_output_file,\n                                             series_name,\n                                             series_id,\n                                             river_index=None,\n                                             river_id=None,\n                                             date_search_start=None,\n                                             date_search_end=None,\n                                             daily=False,\n                                             filter_mode=\"mean\"):\n        \"\"\"\n        Write out RAPID output to GSSHA WMS time series xys file.\n\n        Parameters\n        ----------\n        path_to_output_file: str\n            Path to the output xys file.\n        series_name: str\n            The name for the series.\n        series_id: int\n            The ID to give the series.\n        river_index: :obj:`datetime.datetime`, optional\n            This is the index of the river in the file you want the\n            streamflow for.\n        river_id: :obj:`datetime.datetime`, optional\n            This is the river ID that you want the streamflow for.\n        date_search_start: :obj:`datetime.datetime`, optional\n            This is a datetime object with the date of the minimum date for\n            starting.\n        date_search_end: :obj:`datetime.datetime`, optional\n            This is a datetime object with the date of the maximum date for\n            ending.\n        daily: bool, optional\n            If True and the file is CF-Compliant, write out daily flows.\n        filter_mode: str, optional\n            You can get the daily average \"mean\" or the maximum \"max\".\n            Defauls is \"mean\".\n\n\n        Example writing entire time series to file:\n\n        .. code:: python\n\n            from RAPIDpy import RAPIDDataset\n\n            river_id = 3624735\n            path_to_rapid_qout = '/path/to/Qout.nc'\n\n            with RAPIDDataset(path_to_rapid_qout) as qout_nc:\n                qout_nc.write_flows_to_gssha_time_series_xys(\n                    '/timeseries/Qout_{0}.xys'.format(river_id),\n                    series_name=\"RAPID_TO_GSSHA_{0}\".format(river_id),\n                    series_id=34,\n                    river_id=river_id)\n\n\n        Example writing entire time series as daily average to file:\n\n        .. code:: python\n\n            from RAPIDpy import RAPIDDataset\n\n            river_id = 3624735\n            path_to_rapid_qout = '/path/to/Qout.nc'\n\n            with RAPIDDataset(path_to_rapid_qout) as qout_nc:\n                # NOTE: Getting the river index is not necessary\n                # this is just an example of how to use this\n                river_index = qout_nc.get_river_index(river_id)\n\n                # if file is CF compliant, you can write out daily average\n                qout_nc.write_flows_to_gssha_time_series_xys(\n                    '/timeseries/Qout_daily.xys',\n                    series_name=\"RAPID_TO_GSSHA_{0}\".format(river_id),\n                    series_id=34,\n                    river_index=river_index,\n                    daily=True)\n\n\n        Example writing subset of time series as daily maximum to file:\n\n        .. code:: python\n\n            from datetime import datetime\n            from RAPIDpy import RAPIDDataset\n\n            river_id = 3624735\n            path_to_rapid_qout = '/path/to/Qout.nc'\n\n            with RAPIDDataset(path_to_rapid_qout) as qout_nc:\n                # NOTE: Getting the river index is not necessary\n                # this is just an example of how to use this\n                river_index = qout_nc.get_river_index(river_id)\n\n                # if file is CF compliant, you can filter by date and\n                # get daily values\n                qout_nc.write_flows_to_gssha_time_series_xys(\n                    '/timeseries/Qout_daily_date_filter.xys',\n                    series_name=\"RAPID_TO_GSSHA_{0}\".format(river_id),\n                    series_id=34,\n                    river_index=river_index,\n                    date_search_start=datetime(2002, 8, 31),\n                    date_search_end=datetime(2002, 9, 15),\n                    daily=True,\n                    filter_mode=\"max\")\n\n        \"\"\"\n        if river_id is not None:\n            river_index = self.get_river_index(river_id)\n        elif river_id is None and river_index is None:\n            raise ValueError(\" Need reach id or reach index ...\")\n\n        self.raise_time_valid()\n\n        # analyze and write\n        qout_df = self.get_qout_index(river_index,\n                                      date_search_start=date_search_start,\n                                      date_search_end=date_search_end,\n                                      daily=daily,\n                                      filter_mode=filter_mode,\n                                      as_dataframe=True)\n\n        with open_csv(path_to_output_file, 'w') as out_ts:\n            out_ts.write(\"XYS {0} {1} \\\"{2}\\\"\\r\\n\".format(series_id,\n                                                          len(qout_df.index),\n                                                          series_name))\n            for index, pd_row in qout_df.iterrows():\n                date_str = index.strftime(\"%m/%d/%Y %I:%M:%S %p\")\n                out_ts.write(\"\\\"{0}\\\" {1:.5f}\\n\".format(date_str,\n                                                        pd_row[0]))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write_flows_to_gssha_time_series_ihg(self,\n                                             path_to_output_file,\n                                             connection_list_file,\n                                             date_search_start=None,\n                                             date_search_end=None,\n                                             daily=False,\n                                             filter_mode=\"mean\"):\n        # pylint: disable=line-too-long\n        \"\"\"\n        Write out RAPID output to GSSHA time series ihg file\n\n        .. note:: See: http://www.gsshawiki.com/Surface_Water_Routing:Introducing_Dischage/Constituent_Hydrographs\n\n        .. note:: GSSHA project card is CHAN_POINT_INPUT\n\n        Parameters\n        ----------\n        path_to_output_file: str\n            Path to the output xys file.\n        connection_list_file: str\n            CSV file with link_id, node_id, baseflow, and rapid_rivid header\n            and rows with data.\n        date_search_start: :obj:`datetime.datetime`, optional\n            This is a datetime object with the date of the minimum date\n            for starting.\n        date_search_end: :obj:`datetime.datetime`, optional\n            This is a datetime object with the date of the maximum date\n            for ending.\n        daily: bool, optional\n            If True and the file is CF-Compliant, write out daily flows.\n        filter_mode: str, optional\n            You can get the daily average \"mean\" or the maximum \"max\".\n            Defauls is \"mean\".\n\n\n        Example connection list file::\n\n            link_id, node_id, baseflow, rapid_rivid\n            599, 1, 0.0, 80968\n            603, 1, 0.0, 80967\n\n\n        Example writing entire time series to file:\n\n        .. code:: python\n\n            from RAPIDpy import RAPIDDataset\n\n            path_to_rapid_qout = '/path/to/Qout.nc'\n            connection_list_file = '/path/to/connection_list_file.csv'\n\n            with RAPIDDataset(path_to_rapid_qout) as qout_nc:\n                #for writing entire time series to file\n                qout_nc.write_flows_to_gssha_time_series_ihg(\n                    '/timeseries/Qout_3624735.ihg',\n                    connection_list_file)\n\n\n        Example writing entire time series as daily average to file:\n\n        .. code:: python\n\n            from RAPIDpy import RAPIDDataset\n\n            path_to_rapid_qout = '/path/to/Qout.nc'\n            connection_list_file = '/path/to/connection_list_file.csv'\n\n            with RAPIDDataset(path_to_rapid_qout) as qout_nc:\n                # if file is CF compliant, you can write out daily average\n                qout_nc.write_flows_to_gssha_time_series_ihg(\n                    '/timeseries/Qout_3624735.ihg',\n                    connection_list_file,\n                    daily=True)\n\n\n        Example writing subset of time series as daily maximum to file:\n\n        .. code:: python\n\n            from datetime import datetime\n            from RAPIDpy import RAPIDDataset\n\n            path_to_rapid_qout = '/path/to/Qout.nc'\n            connection_list_file = '/path/to/connection_list_file.csv'\n\n            with RAPIDDataset(path_to_rapid_qout) as qout_nc:\n                # if file is CF compliant, you can filter by\n                # date and get daily values\n                qout_nc.write_flows_to_gssha_time_series_ihg(\n                    '/timeseries/Qout_daily_date_filter.ihg',\n                    connection_list_file,\n                    date_search_start=datetime(2002, 8, 31),\n                    date_search_end=datetime(2002, 9, 15),\n                    daily=True,\n                    filter_mode=\"max\")\n        \"\"\"  # noqa\n        self.raise_time_valid()\n\n        # analyze and write\n        with open_csv(path_to_output_file, 'w') as out_ts:\n            # HEADER SECTION EXAMPLE:\n            # NUMPT 3\n            # POINT 1 599 0.0\n            # POINT 1 603 0.0\n            # POINT 1 605 0.0\n\n            connection_list = np.loadtxt(connection_list_file,\n                                         skiprows=1, ndmin=1,\n                                         delimiter=',',\n                                         usecols=(0, 1, 2, 3),\n                                         dtype={'names': ('link_id',\n                                                          'node_id',\n                                                          'baseflow',\n                                                          'rapid_rivid'),\n                                                'formats': ('i8', 'i8',\n                                                            'f4', 'i8')\n                                                },\n                                         )\n\n            out_ts.write(\"NUMPT {0}\\n\".format(connection_list.size))\n\n            river_idx_list = []\n            for connection in connection_list:\n                out_ts.write(\"POINT {0} {1} {2}\\n\"\n                             \"\".format(connection['node_id'],\n                                       connection['link_id'],\n                                       connection['baseflow'],\n                                       ),\n                             )\n                river_idx_list.append(\n                    self.get_river_index(connection['rapid_rivid'])\n                    )\n\n            # INFLOW SECTION EXAMPLE:\n            # NRPDS 54\n            # INPUT 2002 01 01 00 00 15.551210 12.765090 0.000000\n            # INPUT 2002 01 02 00 00 15.480830 12.765090 0.000000\n            # INPUT 2002 01 03 00 00 16.078910 12.765090 0.000000\n            # ...\n            qout_df = self.get_qout_index(\n                river_idx_list,\n                date_search_start=date_search_start,\n                date_search_end=date_search_end,\n                daily=daily,\n                filter_mode=filter_mode,\n                as_dataframe=True)\n\n            out_ts.write(\"NRPDS {0}\\n\".format(len(qout_df.index)))\n\n            for index, pd_row in qout_df.iterrows():\n                date_str = index.strftime(\"%Y %m %d %H %M\")\n                qout_str = \" \".join([\"{0:.5f}\".format(pd_row[column])\n                                     for column in qout_df])\n                out_ts.write(\"INPUT {0} {1}\\n\".format(date_str, qout_str))", "response": "Writes out flows to GSSHA time series ihg file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsearching for a file with pattern with case insensitive search", "response": "def case_insensitive_file_search(directory, pattern):\n    \"\"\"\n    Looks for file with pattern with case insensitive search\n    \"\"\"\n    try:\n        return os.path.join(\n            directory,\n            [filename for filename in os.listdir(directory)\n             if re.search(pattern, filename, re.IGNORECASE)][0])\n    except IndexError:\n        print(\"{0} not found\".format(pattern))\n        raise"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndivide list into n equal parts", "response": "def partition(lst, n):\n    \"\"\"\n        Divide list into n equal parts\n    \"\"\"\n    q, r = divmod(len(lst), n)\n    indices = [q*i + min(i, r) for i in xrange(n+1)]\n    return [lst[indices[i]:indices[i+1]] for i in xrange(n)], \\\n           [list(xrange(indices[i], indices[i+1])) for i in xrange(n)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_valid_directory_list(input_directory):\n    valid_input_directories = []\n    for directory in os.listdir(input_directory):\n        if os.path.isdir(os.path.join(input_directory, directory)):\n            valid_input_directories.append(directory)\n        else:\n            print(\"{0} not a directory. Skipping ...\".format(directory))\n    return valid_input_directories", "response": "Get a list of folders that are valid for the input_directory."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrunning the command line and prints out the output of the command line.", "response": "def _run_mpi_cmd(self, cmd):\n        \"\"\"\n        This runs the command you send in\n        \"\"\"\n        log(\"Number of Processes: {0}\".format(self.num_processors))\n        time_start = datetime.utcnow()\n\n        # Construct the taudem command line.\n        cmd = [self.mpiexec_path, '-n', str(self.num_processors)] + cmd\n        log(\"Command Line: {0}\".format(\" \".join(cmd)))\n        process = Popen(cmd, stdout=PIPE, stderr=PIPE, shell=False)\n        out, err = process.communicate()\n        if out:\n            log(\"OUTPUT:\")\n            for line in out.split(b'\\n'):\n                log(line)\n        if err:\n            log(err, severity=\"WARNING\")\n        log(\"Time to complete: {0}\".format(datetime.utcnow()-time_start))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _add_prj_file(original_gis_file, new_gis_file):\n        out_prj_file = \"{0}.prj\".format(os.path.splitext(new_gis_file)[0])\n        if original_gis_file.endswith(\".shp\"):\n            dataset = ogr.Open(original_gis_file)\n            layer = dataset.GetLayer()\n            spatial_ref = layer.GetSpatialRef()\n            spatial_ref.MorphToESRI()\n            spatial_ref_str = spatial_ref.ExportToWkt()\n        else:\n            dataset = gdal.Open(original_gis_file)\n            spatial_ref_str = dataset.GetProjection()\n\n        with open(out_prj_file, 'w') as prj_file:\n            prj_file.write(spatial_ref_str)", "response": "Adds projection file to the project file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nextracting a subset river network from the main river network based on the outlet IDs. Parameters ---------- network_file: str Path to the stream network shapefile. out_subset_network_file: str Path to the output subset stream network shapefile. outlet_ids: list List of integers reperesenting the outlet IDs to be included in the subset stream network. river_id_field: str Name of the river ID field in the stream network shapefile. next_down_id_field: str Name if the field with the river ID of the next downstream river segment in the stream network shapefile. river_magnitude_field: str Name of the river magnitude field in the stream network shapefile. safe_mode: bool, optional If True, it will kill the simulation early before over taxing your computer. If you are confident your computer can handle it, set it to False. Here is an example of how to use this: .. code:: python import os from RAPIDpy.gis.taudem import TauDEM output_directory = '/path/to/output/files' network_shp = os.path.join(output_directory, \"stream_reach_file.shp\") out_shp = os.path.join(output_directory, \"stream_reach_file_subset.shp\") TauDEM.extractSubNetwork( network_file=network_shp, out_subset_network_file=out_shp, outlet_ids=[60830], river_id_field=\"LINKNO\", next_down_id_field=\"DSLINKNO\", river_magnitude_field=\"Magnitude\", )", "response": "def extractSubNetwork(network_file,\n                          out_subset_network_file,\n                          outlet_ids,\n                          river_id_field,\n                          next_down_id_field,\n                          river_magnitude_field,\n                          safe_mode=True):\n        \"\"\"\n        Extracts a subset river network from the main river network based on\n        the outlet IDs.\n\n        Parameters\n        ----------\n        network_file: str\n            Path to the stream network shapefile.\n        out_subset_network_file: str\n            Path to the output subset stream network shapefile.\n        outlet_ids: list\n            List of integers reperesenting the outlet IDs to be included in\n            the subset stream network.\n        river_id_field: str\n            Name of the river ID field in the stream network shapefile.\n        next_down_id_field: str\n            Name if the field with the river ID of the next downstream\n            river segment in the stream network shapefile.\n        river_magnitude_field: str\n            Name of the river magnitude field in the stream network shapefile.\n        safe_mode: bool, optional\n            If True, it will kill the simulation early before over taxing\n            your computer. If you are confident your computer can handle it,\n            set it to False.\n\n\n        Here is an example of how to use this:\n\n        .. code:: python\n\n            import os\n            from RAPIDpy.gis.taudem import TauDEM\n\n\n            output_directory = '/path/to/output/files'\n            network_shp = os.path.join(output_directory,\n                                       \"stream_reach_file.shp\")\n            out_shp = os.path.join(output_directory,\n                                   \"stream_reach_file_subset.shp\")\n\n            TauDEM.extractSubNetwork(\n                network_file=network_shp,\n                out_subset_network_file=out_shp,\n                outlet_ids=[60830],\n                river_id_field=\"LINKNO\",\n                next_down_id_field=\"DSLINKNO\",\n                river_magnitude_field=\"Magnitude\",\n            )\n\n        \"\"\"\n        network_shapefile = ogr.Open(network_file)\n        network_layer = network_shapefile.GetLayer()\n        number_of_features = network_layer.GetFeatureCount()\n        network_layer_defn = network_layer.GetLayerDefn()\n        rivid_list = np.zeros(number_of_features, dtype=np.int32)\n        next_down_rivid_list = np.zeros(number_of_features, dtype=np.int32)\n        for feature_idx, drainage_line_feature in enumerate(network_layer):\n            rivid_list[feature_idx] = \\\n                drainage_line_feature.GetField(river_id_field)\n            next_down_rivid_list[feature_idx] = \\\n                drainage_line_feature.GetField(next_down_id_field)\n\n        def getSubNetworkIDList(outlet_river_id,\n                                _rivid_list,\n                                _next_down_rivid_list):\n            \"\"\"\n            Adds ids upstream of the outlet to a list\n            \"\"\"\n            sub_network_index_list = []\n            try:\n                for feature_ii in \\\n                        np.where(_next_down_rivid_list == outlet_river_id)[0]:\n                    sub_network_index_list.append(feature_ii)\n                    sub_network_index_list += \\\n                        getSubNetworkIDList(_rivid_list[feature_ii],\n                                            _rivid_list,\n                                            _next_down_rivid_list)\n            except IndexError:\n                pass\n            return sub_network_index_list\n\n        original_recursion_limit = getrecursionlimit()\n        try:\n            main_sub_network_index_list = []\n            for outlet_id in outlet_ids:\n                outlet_index = np.where(rivid_list == outlet_id)[0][0]\n                outlet_feature = network_layer.GetFeature(outlet_index)\n                outlet_magnitude = \\\n                    outlet_feature.GetField(river_magnitude_field)\n                if outlet_magnitude > original_recursion_limit:\n                    if not safe_mode:\n                        setrecursionlimit(outlet_magnitude)\n                    else:\n                        raise Exception(\"Current recursion limit {0} will not \"\n                                        \"allow extraction for stream magnitude\"\n                                        \" {1}. To override, set safe_mode to \"\n                                        \"False ...\"\n                                        .format(original_recursion_limit,\n                                                outlet_magnitude))\n                main_sub_network_index_list.append(outlet_index)\n                main_sub_network_index_list += \\\n                    getSubNetworkIDList(outlet_id,\n                                        rivid_list,\n                                        next_down_rivid_list)\n        except Exception:\n            setrecursionlimit(original_recursion_limit)\n            raise\n\n        setrecursionlimit(original_recursion_limit)\n\n        # Write out subset to new shapefile\n        shp_drv = ogr.GetDriverByName('ESRI Shapefile')\n        # Remove output shapefile if it already exists\n        if os.path.exists(out_subset_network_file):\n            shp_drv.DeleteDataSource(out_subset_network_file)\n\n        network_subset_shp = shp_drv.CreateDataSource(out_subset_network_file)\n        network_subset_layer = \\\n            network_subset_shp.CreateLayer('',\n                                           network_layer.GetSpatialRef(),\n                                           ogr.wkbLineString)\n        # Add input Layer Fields to the output Layer if it is the one we want\n        for iii in xrange(network_layer_defn.GetFieldCount()):\n            network_subset_layer.CreateField(\n                network_layer_defn.GetFieldDefn(iii))\n        network_subset_layer_defn = network_subset_layer.GetLayerDefn()\n\n        for feature_index in main_sub_network_index_list:\n            subset_feature = network_layer.GetFeature(feature_index)\n            # add to list\n            new_feat = ogr.Feature(network_subset_layer_defn)\n\n            # Add field values from input Layer\n            for iii in xrange(network_layer_defn.GetFieldCount()):\n                new_feat.SetField(\n                    network_subset_layer_defn.GetFieldDefn(iii).GetNameRef(),\n                    subset_feature.GetField(iii))\n\n            # Set geometry as centroid\n            geom = subset_feature.GetGeometryRef()\n            new_feat.SetGeometry(geom.Clone())\n            # Add new feature to output Layer\n            network_subset_layer.CreateFeature(new_feat)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef extractLargestSubNetwork(cls,\n                                 network_file,\n                                 out_subset_network_file,\n                                 river_id_field,\n                                 next_down_id_field,\n                                 river_magnitude_field,\n                                 safe_mode=True):\n        \"\"\"\n        Extracts the larges sub network from the watershed based on the\n        magnitude parameter.\n\n        Parameters\n        ----------\n        network_file: str\n            Path to the stream network shapefile.\n        out_subset_network_file: str\n            Path to the output subset stream network shapefile.\n        river_id_field: str\n            Name of the river ID field in the stream network shapefile.\n        next_down_id_field: str\n            Name of the field with the river ID of the next downstream river\n            segment in the stream network shapefile.\n        river_magnitude_field: str\n            Name of the river magnitude field in the stream network shapefile.\n        safe_mode: bool, optional\n            If True, it will kill the simulation early before over taxing\n            your computer. If you are confident your computer can handle it,\n            set it to False.\n\n\n        Here is an example of how to use this:\n\n        .. code:: python\n\n            import os\n            from RAPIDpy.gis.taudem import TauDEM\n\n            output_directory = '/path/to/output/files'\n            network_shp = os.path.join(output_directory,\n                                       \"stream_reach_file.shp\")\n            out_shp = os.path.join(output_directory,\n                                   \"stream_reach_file_subset.shp\")\n\n            TauDEM.extractLargestSubNetwork(\n                network_file=network_shp,\n                out_subset_network_file=out_shp,\n                river_id_field=\"LINKNO\",\n                next_down_id_field=\"DSLINKNO\",\n                river_magnitude_field=\"Magnitude\",\n            )\n        \"\"\"\n        network_shapefile = ogr.Open(network_file)\n        network_layer = network_shapefile.GetLayer()\n        number_of_features = network_layer.GetFeatureCount()\n        riv_magnuitude_list = np.zeros(number_of_features, dtype=np.int32)\n        for feature_idx, drainage_line_feature in enumerate(network_layer):\n            riv_magnuitude_list[feature_idx] =\\\n                drainage_line_feature.GetField(river_magnitude_field)\n\n        max_magnitude_feature = \\\n            network_layer.GetFeature(np.argmax(riv_magnuitude_list))\n        cls.extractSubNetwork(network_file,\n                              out_subset_network_file,\n                              [max_magnitude_feature.GetField(river_id_field)],\n                              river_id_field,\n                              next_down_id_field,\n                              river_magnitude_field,\n                              safe_mode)", "response": "This function extracts the larges sub network from the watershed network."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nextract catchment by using subset network file. Use this after using either :func:`~RAPIDpy.gis.taudem.TauDEM.extractSubNetwork()` or :func:`~RAPIDpy.gis.taudem.TauDEM.extractLargestSubNetwork()`. Parameters ---------- subset_network_file: str Path to the pre-subsetted stream network shapefile. subset_network_river_id_field: str The field name with the river ID in the stream network shapefile. watershed_file: str Path to the watershed shapefile. watershed_network_river_id_field: str Name of the field with the river ID in the watershed shapefile. out_watershed_subset_file: str The path to output the subset watershed shapefile. Here is an example of how to use this: .. code:: python import os from RAPIDpy.gis.taudem import TauDEM output_directory = '/path/to/output/files' network_shp = os.path.join(output_directory, \"stream_reach_file.shp\") water_shp = os.path.join(output_directory, \"watershed_shapefile.shp\") out_shp = os.path.join(output_directory, \"watershed_shapefile_subset.shp\") TauDEM.extractSubsetFromWatershed( subset_network_filenetwork_shp, subset_network_river_id_field=\"LINKNO\", watershed_file=water_shp, watershed_network_river_id_field=\"LINKNO\", out_watershed_subset_file=out_shp)", "response": "def extractSubsetFromWatershed(subset_network_file,\n                                   subset_network_river_id_field,\n                                   watershed_file,\n                                   watershed_network_river_id_field,\n                                   out_watershed_subset_file):\n        \"\"\"\n        Extract catchment by using subset network file.\n        Use this after using either\n        :func:`~RAPIDpy.gis.taudem.TauDEM.extractSubNetwork()`\n        or :func:`~RAPIDpy.gis.taudem.TauDEM.extractLargestSubNetwork()`.\n\n        Parameters\n        ----------\n        subset_network_file: str\n            Path to the pre-subsetted stream network shapefile.\n        subset_network_river_id_field: str\n            The field name with the river ID in the stream network shapefile.\n        watershed_file: str\n            Path to the watershed shapefile.\n        watershed_network_river_id_field: str\n            Name of the field with the river ID in the watershed shapefile.\n        out_watershed_subset_file: str\n            The path to output the subset watershed shapefile.\n\n\n        Here is an example of how to use this:\n\n        .. code:: python\n\n            import os\n            from RAPIDpy.gis.taudem import TauDEM\n\n            output_directory = '/path/to/output/files'\n            network_shp = os.path.join(output_directory,\n                                       \"stream_reach_file.shp\")\n            water_shp = os.path.join(output_directory,\n                                    \"watershed_shapefile.shp\")\n            out_shp = os.path.join(output_directory,\n                                   \"watershed_shapefile_subset.shp\")\n            TauDEM.extractSubsetFromWatershed(\n                subset_network_filenetwork_shp,\n                subset_network_river_id_field=\"LINKNO\",\n                watershed_file=water_shp,\n                watershed_network_river_id_field=\"LINKNO\",\n                out_watershed_subset_file=out_shp)\n\n        \"\"\"\n        subset_network_shapefile = ogr.Open(subset_network_file)\n        subset_network_layer = subset_network_shapefile.GetLayer()\n\n        ogr_watershed_shapefile = ogr.Open(watershed_file)\n        ogr_watershed_shapefile_lyr = ogr_watershed_shapefile.GetLayer()\n        ogr_watershed_shapefile_lyr_defn = \\\n            ogr_watershed_shapefile_lyr.GetLayerDefn()\n\n        number_of_features = ogr_watershed_shapefile_lyr.GetFeatureCount()\n        watershed_rivid_list = np.zeros(number_of_features, dtype=np.int32)\n        for feature_idx, watershed_feature in \\\n                enumerate(ogr_watershed_shapefile_lyr):\n            watershed_rivid_list[feature_idx] = \\\n                watershed_feature.GetField(watershed_network_river_id_field)\n\n        shp_drv = ogr.GetDriverByName('ESRI Shapefile')\n        # Remove output shapefile if it already exists\n        if os.path.exists(out_watershed_subset_file):\n            shp_drv.DeleteDataSource(out_watershed_subset_file)\n\n        subset_watershed_shapefile = \\\n            shp_drv.CreateDataSource(out_watershed_subset_file)\n        subset_watershed_layer = \\\n            subset_watershed_shapefile.CreateLayer(\n                '',\n                ogr_watershed_shapefile_lyr.GetSpatialRef(),\n                ogr.wkbPolygon)\n        # Add input Layer Fields to the output Layer if it is the one we want\n        for iii in xrange(ogr_watershed_shapefile_lyr_defn.GetFieldCount()):\n            subset_watershed_layer.CreateField(\n                ogr_watershed_shapefile_lyr_defn.GetFieldDefn(iii))\n        subset_watershed_layer_defn = subset_watershed_layer.GetLayerDefn()\n\n        for drainage_line_feature in subset_network_layer:\n            try:\n                watershed_feature_index = \\\n                    np.where(watershed_rivid_list ==\n                             drainage_line_feature.GetField(\n                                 subset_network_river_id_field))[0][0]\n            except IndexError:\n                log(\"{0} {1} not found ...\".format(\n                    subset_network_river_id_field,\n                    drainage_line_feature.GetField(\n                        subset_network_river_id_field)))\n                continue\n\n            subset_feature = \\\n                ogr_watershed_shapefile_lyr.GetFeature(watershed_feature_index)\n            # add to list\n            new_feat = ogr.Feature(subset_watershed_layer_defn)\n\n            # Add field values from input Layer\n            for iii in \\\n                    xrange(ogr_watershed_shapefile_lyr_defn.GetFieldCount()):\n                new_feat.SetField(\n                    subset_watershed_layer_defn.GetFieldDefn(iii).GetNameRef(),\n                    subset_feature.GetField(iii))\n\n            # Set geometry as centroid\n            geom = subset_feature.GetGeometryRef()\n            new_feat.SetGeometry(geom.Clone())\n            # Add new feature to output Layer\n            subset_watershed_layer.CreateFeature(new_feat)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts watershed raster to polygon and then dissolves features based on the LINKNO attribute.", "response": "def rasterToPolygon(raster_file, polygon_file):\n        \"\"\"\n        Converts watershed raster to polygon and then dissolves it.\n        It dissolves features based on the LINKNO attribute.\n        \"\"\"\n        log(\"Process: Raster to Polygon ...\")\n        time_start = datetime.utcnow()\n        temp_polygon_file = \\\n            \"{0}_temp.shp\".format(\n                os.path.splitext(os.path.basename(polygon_file))[0])\n\n        GDALGrid(raster_file).to_polygon(out_shapefile=temp_polygon_file,\n                                         fieldname=\"LINKNO\",\n                                         self_mask=True)\n\n        log(\"Time to convert to polygon: {0}\"\n            .format(datetime.utcnow()-time_start))\n\n        log(\"Dissolving ...\")\n        time_start_dissolve = datetime.utcnow()\n        ogr_polygin_shapefile = ogr.Open(temp_polygon_file)\n        ogr_polygon_shapefile_lyr = ogr_polygin_shapefile.GetLayer()\n        number_of_features = ogr_polygon_shapefile_lyr.GetFeatureCount()\n        polygon_rivid_list = np.zeros(number_of_features, dtype=np.int32)\n        for feature_idx, catchment_feature in \\\n                enumerate(ogr_polygon_shapefile_lyr):\n            polygon_rivid_list[feature_idx] = \\\n                catchment_feature.GetField('LINKNO')\n\n        shp_drv = ogr.GetDriverByName('ESRI Shapefile')\n        # Remove output shapefile if it already exists\n        if os.path.exists(polygon_file):\n            shp_drv.DeleteDataSource(polygon_file)\n\n        dissolve_shapefile = shp_drv.CreateDataSource(polygon_file)\n        dissolve_layer = \\\n            dissolve_shapefile.CreateLayer(\n                '',\n                ogr_polygon_shapefile_lyr.GetSpatialRef(),\n                ogr.wkbPolygon)\n        dissolve_layer.CreateField(ogr.FieldDefn('LINKNO', ogr.OFTInteger))\n        dissolve_layer_defn = dissolve_layer.GetLayerDefn()\n\n        for unique_rivid in np.unique(polygon_rivid_list):\n            # get indices where it is in the polygon\n            feature_indices = np.where(polygon_rivid_list == unique_rivid)[0]\n            new_feat = ogr.Feature(dissolve_layer_defn)\n            new_feat.SetField('LINKNO', int(unique_rivid))\n\n            if len(feature_indices) == 1:\n                # write feature to file\n                feature = \\\n                    ogr_polygon_shapefile_lyr.GetFeature(feature_indices[0])\n                new_feat.SetGeometry(feature.GetGeometryRef())\n            else:\n                # dissolve\n                dissolve_poly_list = []\n                for feature_index in feature_indices:\n                    feature = \\\n                        ogr_polygon_shapefile_lyr.GetFeature(feature_index)\n                    feat_geom = feature.GetGeometryRef()\n                    dissolve_poly_list.append(\n                        shapely_loads(feat_geom.ExportToWkb()))\n                dissolve_polygon = cascaded_union(dissolve_poly_list)\n                new_feat.SetGeometry(\n                    ogr.CreateGeometryFromWkb(dissolve_polygon.wkb))\n            dissolve_layer.CreateFeature(new_feat)\n        # clean up\n        shp_drv.DeleteDataSource(temp_polygon_file)\n        log(\"Time to dissolve: {0}\".format(datetime.utcnow() -\n                                           time_start_dissolve))\n        log(\"Total time to convert: {0}\".format(datetime.utcnow() -\n                                                time_start))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds length field in meters to the Kfac file.", "response": "def addLengthMeters(stream_network):\n        \"\"\"\n        Adds length field in meters to network\n        (The added field name will be 'LENGTH_M').\n\n        .. note:: This may be needed for generating the kfac file\n                  depending on the units of your raster. See: :doc:`gis_tools`.\n\n        Parameters\n        ----------\n        stream_network: str\n            Path to stream network file.\n\n\n        Here is an example of how to use this:\n\n        .. code:: python\n\n            import os\n            from RAPIDpy.gis.taudem import TauDEM\n\n            output_directory = '/path/to/output/files'\n            TauDEM.addLengthMeters(os.path.join(output_directory,\n                                                \"stream_reach_file.shp\"))\n\n        \"\"\"\n        network_shapefile = ogr.Open(stream_network, 1)\n        network_layer = network_shapefile.GetLayer()\n        network_layer_defn = network_layer.GetLayerDefn()\n\n        # make sure projection EPSG:4326\n        network_layer_proj = network_layer.GetSpatialRef()\n        geographic_proj = osr.SpatialReference()\n        geographic_proj.ImportFromEPSG(4326)\n        proj_transform = None\n        if network_layer_proj != geographic_proj:\n            proj_transform = osr.CoordinateTransformation(network_layer_proj,\n                                                          geographic_proj)\n\n        # check for field\n        create_field = True\n        for i in xrange(network_layer_defn.GetFieldCount()):\n            field_name = network_layer_defn.GetFieldDefn(i).GetName()\n            if field_name == 'LENGTH_M':\n                create_field = False\n                break\n\n        if create_field:\n            network_layer.CreateField(ogr.FieldDefn('LENGTH_M', ogr.OFTReal))\n\n        geo_manager = Geod(ellps=\"WGS84\")\n        for network_feature in network_layer:\n            feat_geom = network_feature.GetGeometryRef()\n            # make sure coordinates are geographic\n            if proj_transform:\n                feat_geom.Transform(proj_transform)\n\n            line = shapely_loads(feat_geom.ExportToWkb())\n            lon_list, lat_list = line.xy\n            dist = geo_manager.inv(lon_list[:-1], lat_list[:-1],\n                                   lon_list[1:], lat_list[1:])[2]\n            network_feature.SetField('LENGTH_M', sum(dist))\n            network_layer.SetFeature(network_feature)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pitRemove(self,\n                  elevation_grid,\n                  pit_filled_elevation_grid,\n                  input_depression_mask_grid=None,\n                  consider4way=False,\n                  ):\n        \"\"\"\n        Remove low spots from DEM.\n        \"\"\"\n        log(\"PROCESS: PitRemove\")\n        self.pit_filled_elevation_grid = pit_filled_elevation_grid\n\n        # Construct the taudem command line.\n        cmd = [os.path.join(self.taudem_exe_path, 'pitremove'),\n               '-z', elevation_grid,\n               '-fel', self.pit_filled_elevation_grid,\n               ]\n\n        if input_depression_mask_grid:\n            cmd += ['-depmask', input_depression_mask_grid]\n        if consider4way:\n            cmd += ['-4way']\n\n        self._run_mpi_cmd(cmd)\n\n        # create projection file\n        self._add_prj_file(elevation_grid,\n                           self.pit_filled_elevation_grid)", "response": "This function removes low spots from DEM."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef dinfFlowDirection(self,\n                          flow_dir_grid,\n                          slope_grid,\n                          pit_filled_elevation_grid=None):\n        \"\"\"\n        Calculates flow direction with Dinf method\n        \"\"\"\n        log(\"PROCESS: DinfFlowDirection\")\n        if pit_filled_elevation_grid:\n            self.pit_filled_elevation_grid = pit_filled_elevation_grid\n\n        # Construct the taudem command line.\n        cmd = [os.path.join(self.taudem_exe_path, 'dinfflowdir'),\n               '-fel', self.pit_filled_elevation_grid,\n               '-ang', flow_dir_grid,\n               '-slp', slope_grid,\n               ]\n\n        self._run_mpi_cmd(cmd)\n\n        # create projection files\n        self._add_prj_file(self.pit_filled_elevation_grid,\n                           flow_dir_grid)\n        self._add_prj_file(self.pit_filled_elevation_grid,\n                           slope_grid)", "response": "Calculates flow direction with Dinf method."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate contributing area with Dinf method.", "response": "def dinfContributingArea(self,\n                             contributing_area_grid,\n                             flow_dir_grid,\n                             outlet_shapefile=None,\n                             weight_grid=None,\n                             edge_contamination=False,\n                             ):\n        \"\"\"\n        Calculates contributing area with Dinf method.\n        \"\"\"\n        log(\"PROCESS: DinfContributingArea\")\n\n        # Construct the taudem command line.\n        cmd = [os.path.join(self.taudem_exe_path, 'areadinf'),\n               '-ang', flow_dir_grid,\n               '-sca', contributing_area_grid,\n               ]\n\n        if outlet_shapefile:\n            cmd += ['-o', outlet_shapefile]\n        if weight_grid:\n            cmd += ['-wg', weight_grid]\n        if not edge_contamination:\n            cmd = cmd + ['-nc']\n\n        self._run_mpi_cmd(cmd)\n\n        # create projection file\n        self._add_prj_file(flow_dir_grid,\n                           contributing_area_grid)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate the stream definition by threshold.", "response": "def streamDefByThreshold(self,\n                             stream_raster_grid,\n                             threshold,\n                             contributing_area_grid,\n                             mask_grid=None,\n                             ):\n        \"\"\"\n        Calculates the stream definition by threshold.\n        \"\"\"\n        log(\"PROCESS: StreamDefByThreshold\")\n        self.stream_raster_grid = stream_raster_grid\n\n        # Construct the taudem command line.\n        cmd = [os.path.join(self.taudem_exe_path, 'threshold'),\n               '-ssa', contributing_area_grid,\n               '-src', self.stream_raster_grid,\n               '-thresh', str(threshold),\n               ]\n\n        if mask_grid:\n            cmd += ['-mask', mask_grid]\n\n        self._run_mpi_cmd(cmd)\n\n        # create projection file\n        self._add_prj_file(contributing_area_grid,\n                           self.stream_raster_grid)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef streamReachAndWatershed(self,\n                                delineate,\n                                out_stream_order_grid,\n                                out_network_connectivity_tree,\n                                out_network_coordinates,\n                                out_stream_reach_file,\n                                out_watershed_grid,\n                                pit_filled_elevation_grid=None,\n                                flow_dir_grid=None,\n                                contributing_area_grid=None,\n                                stream_raster_grid=None,\n                                outlet_shapefile=None\n                                ):\n        \"\"\"\n        Creates vector network and shapefile from stream raster grid\n        \"\"\"\n        log(\"PROCESS: StreamReachAndWatershed\")\n        if pit_filled_elevation_grid:\n            self.pit_filled_elevation_grid = pit_filled_elevation_grid\n        if flow_dir_grid:\n            self.flow_dir_grid = flow_dir_grid\n        if contributing_area_grid:\n            self.contributing_area_grid = contributing_area_grid\n        if stream_raster_grid:\n            self.stream_raster_grid = stream_raster_grid\n\n        # Construct the taudem command line.\n        cmd = [os.path.join(self.taudem_exe_path, 'streamnet'),\n               '-fel', self.pit_filled_elevation_grid,\n               '-p', self.flow_dir_grid,\n               '-ad8', self.contributing_area_grid,\n               '-src', self.stream_raster_grid,\n               '-ord', out_stream_order_grid,\n               '-tree', out_network_connectivity_tree,\n               '-coord', out_network_coordinates,\n               '-net', out_stream_reach_file,\n               '-w', out_watershed_grid,\n               ]\n\n        if outlet_shapefile:\n            cmd += ['-o', outlet_shapefile]\n        if delineate:\n            cmd += ['-sw']\n\n        self._run_mpi_cmd(cmd)\n\n        # create projection file\n        self._add_prj_file(self.pit_filled_elevation_grid,\n                           out_stream_order_grid)\n        self._add_prj_file(self.pit_filled_elevation_grid,\n                           out_stream_reach_file)\n        self._add_prj_file(self.pit_filled_elevation_grid,\n                           out_watershed_grid)", "response": "This function creates vector network and shapefile from stream raster grid."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef demToStreamNetwork(self,\n                           output_directory,\n                           raw_elevation_dem=\"\",\n                           pit_filled_elevation_grid=\"\",\n                           flow_dir_grid_d8=\"\",\n                           contributing_area_grid_d8=\"\",\n                           flow_dir_grid_dinf=\"\",\n                           contributing_area_grid_dinf=\"\",\n                           use_dinf=False,\n                           threshold=1000,\n                           delineate=False):\n        \"\"\"\n        This function will run all of the TauDEM processes to generate\n        a stream network from an elevation raster.\n\n        .. note:: For information about the stream reach and watershed process:\n                  http://hydrology.usu.edu/taudem/taudem5/help53/StreamReachAndWatershed.html\n\n        .. note:: For information about the *threshold* parameter, see:\n                  http://hydrology.usu.edu/taudem/taudem5/help53/StreamDefinitionByThreshold.html\n\n        Parameters\n        ----------\n        output_directory: str\n            Path to output generated files to.\n        raw_elevation_dem: str, optional\n            Path to original elevation DEM file. Required if\n            *pit_filled_elevation_grid* is not used.\n        pit_filled_elevation_grid: str, optional\n            Path to pit filled elevation DEM file. Required if\n            *raw_elevation_dem* is not used.\n        flow_dir_grid_d8: str, optional\n            Path to flow direction grid generated using TauDEM's D8 method.\n        contributing_area_grid_d8: str, optional\n            Path to contributing area grid generated using TauDEM's D8 method.\n        flow_dir_grid_dinf: str, optional\n            Path to flow direction grid generated using TauDEM's\n            D-Infinity method (EXPERIMENTAL).\n        contributing_area_grid_dinf: str, optional\n            Path to contributing area grid generated using TauDEM's\n            D-Infinity method (EXPERIMENTAL).\n        use_dinf: bool, optional\n            Use the D-Infinity method to get stream definition (EXPERIMENTAL).\n        threshold: int, optional\n            The stream threshold or maximum number of upstream grid cells.\n            See above note.\n        delineate: bool, optional\n            If True, this will use the delineate option for theis method\n            using TauDEM. Default is False.\n\n\n        Here is an example of how to use this:\n\n        .. code:: python\n\n            from RAPIDpy.gis.taudem import TauDEM\n\n\n            elevation_dem = '/path/to/dem.tif'\n            output_directory = '/path/to/output/files'\n\n            td = TauDEM(\"/path/to/scripts/TauDEM\")\n            td.demToStreamNetwork(output_directory,\n                                  elevation_dem,\n                                  threshold=1000)\n\n        \"\"\"\n        time_start = datetime.utcnow()\n\n        # FILL PITS IF NEEDED\n        self.pit_filled_elevation_grid = pit_filled_elevation_grid\n        if not pit_filled_elevation_grid:\n            pit_filled_elevation_grid = \\\n                os.path.join(output_directory, 'pit_filled_elevation_grid.tif')\n            self.pitRemove(raw_elevation_dem,\n                           pit_filled_elevation_grid)\n\n        # GENERATE D8 RASTERS\n        self.flow_dir_grid = flow_dir_grid_d8\n        if not flow_dir_grid_d8:\n            flow_dir_grid_d8 = \\\n                os.path.join(output_directory, 'flow_dir_grid_d8.tif')\n            slope_grid_d8 = os.path.join(output_directory, 'slope_grid_d8.tif')\n            self.d8FlowDirection(flow_dir_grid_d8,\n                                 slope_grid_d8)\n\n        self.contributing_area_grid = contributing_area_grid_d8\n        if not contributing_area_grid_d8:\n            contributing_area_grid_d8 = \\\n                os.path.join(output_directory, 'contributing_area_grid_d8.tif')\n            self.d8ContributingArea(contributing_area_grid_d8)\n\n        stream_raster_grid = \\\n            os.path.join(output_directory, 'stream_raster_grid.tif')\n        if use_dinf:\n            log(\"USING DINF METHOD TO GET STREAM DEFINITION ...\")\n            if not flow_dir_grid_dinf:\n                flow_dir_grid_dinf = \\\n                    os.path.join(output_directory, 'flow_dir_grid_dinf.tif')\n                slope_grid_dinf = \\\n                    os.path.join(output_directory, 'slope_grid_dinf.tif')\n                self.dinfFlowDirection(flow_dir_grid_dinf,\n                                       slope_grid_dinf)\n            if not contributing_area_grid_dinf:\n                contributing_area_grid_dinf = \\\n                    os.path.join(output_directory,\n                                 'contributing_area_grid_dinf.tif')\n                self.dinfContributingArea(contributing_area_grid_dinf,\n                                          flow_dir_grid_dinf)\n\n            self.streamDefByThreshold(stream_raster_grid,\n                                      threshold,\n                                      contributing_area_grid_dinf)\n        else:\n            log(\"USING D8 METHOD TO GET STREAM DEFINITION ...\")\n            self.streamDefByThreshold(stream_raster_grid,\n                                      threshold,\n                                      contributing_area_grid_d8)\n\n        # GENERATE STREAM NETWORK\n        out_stream_order_grid = \\\n            os.path.join(output_directory, 'stream_order_grid.tif')\n        out_network_connectivity_tree = \\\n            os.path.join(output_directory, 'network_connectivity_tree.txt')\n        out_network_coordinates = \\\n            os.path.join(output_directory, 'network_coordinates.txt')\n        out_stream_reach_file = \\\n            os.path.join(output_directory, 'stream_reach_file.shp')\n        out_watershed_grid = \\\n            os.path.join(output_directory, 'watershed_grid.tif')\n        self.streamReachAndWatershed(delineate,\n                                     out_stream_order_grid,\n                                     out_network_connectivity_tree,\n                                     out_network_coordinates,\n                                     out_stream_reach_file,\n                                     out_watershed_grid)\n\n        # convert watersed grid to shapefile\n        out_watershed_shapefile = \\\n            os.path.join(output_directory, 'watershed_shapefile.shp')\n        self.rasterToPolygon(out_watershed_grid, out_watershed_shapefile)\n        log(\"Total time to complete: {0}\".format(datetime.utcnow() -\n                                                 time_start))", "response": "This function generates a stream network from an elevation raster."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing a region parameter to determine the rectangular portion of the full image.", "response": "def parse_region(region, image_width, image_height):\n    \"\"\"\n    Parse Region parameter to determine the rectangular portion of the full\n    image to be returned, informed by the actual image dimensions.\n    Returns (x, y, width, height):\n        - x,y are pixel offsets into the image from the upper left\n        - width, height are pixel dimensions for cropped image.\n    \"\"\"\n    if region == 'full':\n        # Return complete image, no cropping\n        x, y, width, height = 0, 0, image_width, image_height\n    elif region == 'square':\n        square_size = min(image_width, image_height)\n        # Generate x,y offsets to centre cropped image. This is not mandated\n        # by the spec but is recommended as a sensible default.\n        x = int((image_width - square_size) / 2)\n        y = int((image_height - square_size) / 2)\n        width = height = square_size\n    elif region.startswith('pct:'):\n        x_pct, y_pct, width_pct, height_pct = \\\n            parse_dimensions_string(region[4:], permit_floats=True)\n        x, y, width, height = map(int, (\n            x_pct / 100 * image_width,\n            y_pct / 100 * image_height,\n            width_pct / 100 * image_width,\n            height_pct / 100 * image_height,\n        ))\n    else:\n        x, y, width, height = parse_dimensions_string(region)\n    # If region extends beyond original's dimensions, crop extends only to\n    # image edge.\n    width = min(width, image_width - x)\n    height = min(height, image_height - y)\n    return x, y, width, height"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the canonical URL path for an image for the given region size rotation and format.", "response": "def make_canonical_path(\n        image_identifier, image_width, image_height,\n        region, size, rotation, quality, format_str\n):\n    \"\"\"\n    Return the canonical URL path for an image for the given region/size/\n    rotation/quality/format API tranformation settings.\n\n    See http://iiif.io/api/image/2.1/#canonical-uri-syntax\n    \"\"\"\n    original_aspect_ratio = float(image_width) / image_height\n\n    if (\n        region == 'full' or\n        # Use 'full' if 'square' is requested for an already square image\n        (region == 'square' and image_width == image_height) or\n        # Use 'full' if region exactly matches image dimensions\n        (region == (0, 0, image_width, image_height))\n    ):\n        canonical_region = 'full'\n    else:\n        # Use explicit x,y,width,height region settings\n        canonical_region = ','.join(map(str, region))\n\n    if size in ['full', 'max']:\n        canonical_size = 'full'\n    elif (region[2:] == size and (image_width, image_height) == size):\n        # Use 'full' if result image dimensions are unchanged from original\n        # and are also unchanged from the region operation's output\n        canonical_size = 'full'\n    elif float(size[0]) / size[1] == original_aspect_ratio:\n        # w, syntax for images scaled to maintain the aspect ratio\n        canonical_size = '%d,' % size[0]\n    else:\n        # Full with,height size if aspect ratio is changed\n        canonical_size = ','.join(map(str, size))\n\n    canonical_rotation = ''\n    if rotation[0]:\n        # Image is mirrored\n        canonical_rotation += '!'\n    canonical_rotation += '%d' % rotation[1]\n\n    canonical_quality = quality\n\n    canonical_format = format_str\n\n    return reverse(\n        'iiif_image_api',\n        args=[image_identifier, canonical_region, canonical_size,\n              canonical_rotation, canonical_quality, canonical_format]\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef build_iiif_file_storage_path(url_path, ik_image, iiif_storage):\n    storage_path = url_path[1:]  # Stip leading slash\n\n    # Strip redundant 'iiif-' prefix if present (re-added below)\n    if storage_path.startswith('iiif/'):\n        storage_path = storage_path[5:]\n\n    # Add Image's modified timestamp to storage path as a primitive\n    # cache-busting mechanism.\n    ik_image_ts = str(calendar.timegm(ik_image.date_modified.timetuple()))\n    splits = storage_path.split('/')\n    storage_path = '/'.join(\n        [splits[0]] +  # Image ID\n        [ik_image_ts] +  # Image instance modified timestamp\n        splits[1:]  # Remainder of storage path\n    )\n\n    # Replace '/' & ',' with '-' to keep separators of some kind in\n    # storage file name, otherwise the characters get purged and\n    # produce storage names with potentially ambiguous and clashing\n    # values e.g. /3/100,100,200,200/... => iiif3100100200200\n    storage_path = storage_path.replace('/', '-').replace(',', '-')\n\n    # Convert URL path format to a valid file name for a given storage engine\n    storage_path = iiif_storage.get_valid_name(storage_path)\n\n    # Add path prefix to storage path to avoid dumping image files\n    # into the location of a storage location that might be used for many\n    # purposes.\n    if iiif_storage.location != 'iiif':\n        storage_path = 'iiif/' + storage_path\n\n    return storage_path", "response": "Builds a file storage path for a given IIIF Image API URL path."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef derive_and_set_name_fields_and_slug(\n            self, set_name_sort=True, set_slug=True\n    ):\n        \"\"\"\n        Derive subordinate name_* field values from the `name_full` field\n        unless these fields are set in their own right.\n\n        This method is called during `save()`\n        \"\"\"\n        # name_full is the primary required name field. It must be set.\n        if is_empty(self.name_full):\n            if not is_empty(self.name_display):\n                self.name_full = self.name_display\n            else:\n                raise ValueError(\n                    u\"%s.name_full cannot be empty at save\" % type(self).__name__)\n        # if empty, `name_sort` == `name_full`\n        if set_name_sort and is_empty(self.name_sort):\n            self.name_sort = self.derive_sort_name()\n        # if empty, `slug` is set to slugified `name_full`\n        if set_slug and is_empty(self.slug):\n            self.slug = slugify(self.name_display or self.name_full)", "response": "Derive subordinate name_full field values from the name_full field."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the m2m relations connecting me to works", "response": "def get_roles(self):\n        \"\"\"Return the m2m relations connecting me to works\"\"\"\n        work_ids = self.get_works().values_list('id', flat=True)\n        return self.works.through.objects.filter(\n            creator=self.get_draft(),\n            work_id__in=work_ids,\n        ).select_related('role')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef derive_and_set_slug(self, set_name_sort=True, set_slug=True):\n        # `title` is the primary required name field. It must be set.\n        if is_empty(self.title):\n            raise ValueError(\n                u\"%s.title cannot be empty at save\" % type(self).__name__)\n        # if empty, `slug` is set to slugified `title`\n        if set_slug and is_empty(self.slug):\n            self.slug = slugify(self.title)", "response": "Derive slug from title unless it is set in its own right."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_roles(self):\n\n        creator_ids = self.get_creators().values_list('id', flat=True)\n\n        return self.creators.through.objects.filter(\n            work=self.get_draft(),\n            creator_id__in=creator_ids,\n        ).select_related('role')", "response": "Return the m2m relations connecting me to creators."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of tuples that are used to identify the event types that will appear in the right sidebar.", "response": "def lookups(self, request, model_admin):\n        \"\"\"\n        Returns a list of tuples. The first element in each\n        tuple is the coded value for the option that will\n        appear in the URL query. The second element is the\n        human-readable name for the option that will appear\n        in the right sidebar.\n        \"\"\"\n\n        types = models.EventType.objects.filter(is_public=True).order_by('slug')\n\n        types = [(t.id, t.swatch()) for t in types]\n\n        return types"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds a calendar URL.", "response": "def get_urls(self):\n        \"\"\"\n        Add a calendar URL.\n        \"\"\"\n        from django.conf.urls import patterns, url\n        urls = super(EventAdmin, self).get_urls()\n        my_urls = patterns(\n            '',\n            url(\n                r'^calendar/$',\n                self.admin_site.admin_view(self.calendar),\n                name='icekit_events_eventbase_calendar'\n            ),\n            url(\n                r'^calendar_data/$',\n                self.admin_site.admin_view(self.calendar_data),\n                name='icekit_events_eventbase_calendar_data'\n            ),\n        )\n        return my_urls + urls"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a calendar page to be loaded in an iframe.", "response": "def calendar(self, request):\n        \"\"\"\n        Return a calendar page to be loaded in an iframe.\n        \"\"\"\n        context = {\n            'is_popup': bool(int(request.GET.get('_popup', 0))),\n        }\n        return TemplateResponse(\n            request, 'admin/icekit_events/eventbase/calendar.html', context)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the calendar data for the current event.", "response": "def calendar_data(self, request):\n        \"\"\"\n        Return event data in JSON format for AJAX requests, or a calendar page\n        to be loaded in an iframe.\n        \"\"\"\n\n        # mutable copy\n        request.GET = request.GET.copy()\n\n        if 'timezone' in request.GET:\n            tz = djtz.get(request.GET.pop('timezone'))\n        else:\n            tz = get_current_timezone()\n\n        if 'start' in request.GET:\n            start_dt = self._parse_dt_from_request(request, 'start')\n            if start_dt:\n                start = djtz.localize(start_dt, tz)\n            else:\n                start = None\n        else:\n            start = None\n        if 'end' in request.GET:\n            end_dt = self._parse_dt_from_request(request, 'end')\n            if end_dt:\n                end = djtz.localize(end_dt, tz)\n            else:\n                end = None\n        else:\n            end = None\n\n        # filter the qs like the changelist filters\n        cl = ChangeList(request, self.model, self.list_display,\n                        self.list_display_links, self.list_filter,\n                        self.date_hierarchy, self.search_fields,\n                        self.list_select_related, self.list_per_page,\n                        self.list_max_show_all, self.list_editable, self)\n\n        filtered_event_ids = cl.get_queryset(request).values_list('id', flat=True)\n        all_occurrences = models.Occurrence.objects.filter(event__id__in=filtered_event_ids).overlapping(start, end)\n\n        data = []\n        for occurrence in all_occurrences.all():\n            data.append(self._calendar_json_for_occurrence(occurrence))\n        data = json.dumps(data, cls=DjangoJSONEncoder)\n        return HttpResponse(content=data, content_type='application/json')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _calendar_json_for_occurrence(self, occurrence):\n        # Slugify the plugin's verbose name for use as a class name.\n        if occurrence.is_all_day:\n            start = occurrence.start\n            # `end` is exclusive according to the doc in\n            # http://fullcalendar.io/docs/event_data/Event_Object/, so\n            # we need to add 1 day to ``end`` to have the end date\n            # included in the calendar.\n            end = occurrence.start + timedelta(days=1)\n        else:\n            start = djtz.localize(occurrence.start)\n            end = djtz.localize(occurrence.end)\n        if occurrence.is_cancelled and occurrence.cancel_reason:\n            title = u\"{0} [{1}]\".format(\n                occurrence.event.title, occurrence.cancel_reason)\n        else:\n            title = occurrence.event.title\n\n        if occurrence.event.primary_type:\n            color = occurrence.event.primary_type.color\n        else:\n            color = \"#cccccc\"\n        return {\n            'title': title,\n            'allDay': occurrence.is_all_day or occurrence.event.contained_events.exists(),\n            'start': start,\n            'end': end,\n            'url': reverse('admin:icekit_events_eventbase_change',\n                           args=[occurrence.event.pk]),\n            'className': self._calendar_classes_for_occurrence(occurrence),\n            'backgroundColor': color,\n        }", "response": "Return a JSON dict for a single Occurrence."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns css classes to be used in admin calendar JSON.", "response": "def _calendar_classes_for_occurrence(self, occurrence):\n        \"\"\"\n        Return css classes to be used in admin calendar JSON\n        \"\"\"\n        classes = [slugify(occurrence.event.polymorphic_ctype.name)]\n\n        # Add a class name for the type of event.\n        if occurrence.is_all_day:\n            classes.append('is-all-day')\n        if occurrence.is_protected_from_regeneration:\n            classes.append('is-user-modified')\n        if occurrence.is_cancelled:\n            classes.append('is-cancelled')\n\n        # if an event isn't published or does not have show_in_calendar ticked,\n        # indicate that it is hidden\n        if not occurrence.event.show_in_calendar:\n            classes.append('do-not-show-in-calendar')\n\n        # Prefix class names with \"fcc-\" (full calendar class).\n        classes = ['fcc-%s' % class_ for class_ in classes]\n\n        return classes"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_urls(self):\n        from django.conf.urls import patterns, url\n        urls = super(RecurrenceRuleAdmin, self).get_urls()\n        my_urls = patterns(\n            '',\n            url(\n                r'^preview/$',\n                self.admin_site.admin_view(self.preview),\n                name='icekit_events_recurrencerule_preview'\n            ),\n        )\n        return my_urls + urls", "response": "Add a preview URL."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef preview(self, request):\n        recurrence_rule = request.POST.get('recurrence_rule')\n        limit = int(request.POST.get('limit', 10))\n        try:\n            rruleset = rrule.rrulestr(\n                recurrence_rule, dtstart=djtz.now(), forceset=True)\n        except ValueError as e:\n            data = {\n                'error': six.text_type(e),\n            }\n        else:\n            data = {\n                'occurrences': rruleset[:limit]\n            }\n        return JsonResponse(data)", "response": "Return a list of occurrences in JSON format up to the configured limit."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse the start date for event occurrences which is one of the following.", "response": "def parse_start_date(self, request):\n        \"\"\"\n        Return start date for event occurrences, which is one of the following\n        in order of priority:\n         - `start_date` GET parameter value, if given and valid\n         - page's `default_start_date` if set\n         - today's date\n        \"\"\"\n        if request.GET.get('start_date'):\n            try:\n                return djtz.parse('%s 00:00' % request.GET.get('start_date'))\n            except ValueError:\n                pass\n        return self.default_start_date or djtz.midnight()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_end_date(self, request, start_date):\n        if request.GET.get('end_date'):\n            try:\n                return djtz.parse('%s 00:00' % request.GET.get('end_date'))\n            except ValueError:\n                pass\n        days_to_show = self.default_days_to_show or \\\n            appsettings.DEFAULT_DAYS_TO_SHOW\n        if 'days_to_show' in request.GET:\n            try:\n                days_to_show = int(request.GET.get('days_to_show'))\n            except ValueError:\n                pass\n        return start_date + timedelta(days=days_to_show)", "response": "Parse the end date of the event occurrences page."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_primary_types(self, request):\n        if request.GET.getlist('primary_types'):\n            return EventType.objects.get(\n                slug__in=request.GET.getlist('primary_types'))\n        return None", "response": "Parse primary types from GET parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses secondary types from the request.", "response": "def parse_secondary_types(self, request):\n        \"\"\"\n        Return secondary event types that occurrences must belong to, or `None`\n        if there is no constraint on secondary type.\n        \"\"\"\n        if request.GET.getlist('secondary_types'):\n            return EventType.objects.filter(\n                slug__in=request.GET.getlist('secondary_types'))\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of users that are staff and have a usable password.", "response": "def get_users(self, email):\n        \"\"\"\n        Make sure users are staff users.\n\n        Additionally to the other PasswordResetForm conditions ensure\n        that the user is a staff user before sending them a password\n        reset email.\n\n        :param email: Textual email address.\n        :return: List of users.\n        \"\"\"\n        # Django 1.8 supports this feature.\n        if hasattr(super(PasswordResetForm, self), 'get_users'):\n            return (\n                u for u in super(PasswordResetForm, self).get_users(email)\n                if u.is_staff and u.is_active\n            )\n\n        # Django Django < 1.8 support we can do this manually.\n        active_users = get_user_model()._default_manager.filter(email__iexact=email, is_active=True)\n        return (u for u in active_users if u.has_usable_password() and u.is_staff and u.is_active)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef send_email_notifications_for_workflow_state_change(\n    sender, instance, *args, **kwargs\n):\n    \"\"\"\n    Send email notifications for save events on ``WorkflowState`` based on\n    settings in this module's `appsettings`.\n    \"\"\"\n    # Short-circuit processing if we have no notification targets\n    if not appsettings.WORKFLOW_EMAIL_NOTIFICATION_TARGETS:\n        return\n\n    obj = instance.content_object\n\n    # Look up admin URL for object, getting the admin site domain from\n    # ``settings.SITE_URL` if available otherwise punt to the first available\n    # ``Site`` domain.\n    obj_ct = ContentType.objects.get_for_model(obj)\n    obj_app_name = '_'.join(obj_ct.natural_key())\n    admin_path = reverse(\n        'admin:%s_change' % obj_app_name, args=(obj.pk,))\n    admin_domain = getattr(\n        settings,\n        'SITE_URL',\n        Site.objects.all()[0].domain)\n    if '://' not in admin_domain:\n        # TODO Detect when we should use 'https' instead\n        admin_domain = 'http://' + admin_domain\n    admin_url = ''.join([admin_domain, admin_path])\n\n    # Send an email notification to each target configured in settings\n    for target_settings in appsettings.WORKFLOW_EMAIL_NOTIFICATION_TARGETS:\n        # Send email to specific address if configured...\n        if 'to' in target_settings:\n            to_addresses = target_settings['to']\n        # ... otherwise to the assigned user if any...\n        elif instance.assigned_to:\n            to_addresses = instance.assigned_to.email\n        # ... otherwise skip, since we have no TO addressee\n        else:\n            continue\n        if not isinstance(to_addresses, list):\n            to_addresses = [to_addresses]\n\n        from_address = target_settings.get(\n            'from', appsettings.WORKFLOW_EMAIL_NOTIFICATION_DEFAULT_FROM)\n\n        subject_template = target_settings.get(\n            'subject_template',\n            appsettings.WORKFLOW_EMAIL_NOTIFICATION_SUBJECT_TEMPLATE)\n\n        message_template = target_settings.get(\n            'message_template',\n            appsettings.WORKFLOW_EMAIL_NOTIFICATION_MESSAGE_TEMPLATE)\n\n        # Optionally map assigned user to some other identifier, such as\n        # a Slack username etc.\n        assigned_to_mapper = target_settings.get(\n            'map_assigned_to_fn', lambda x: x)\n\n        # Render subject and message templates\n        template_context = Context({\n            'state': instance,\n            'object': obj,\n            'admin_url': admin_url,\n            'assigned_to': assigned_to_mapper(instance.assigned_to),\n        })\n        subject = Template(subject_template).render(template_context)\n        message = Template(message_template).render(template_context)\n\n        send_mail(subject, message, from_address, to_addresses)", "response": "Send email notifications for save events on WorkflowState based on settings. WORKFLOW_EMAIL_NOTIFICATION_TARGETS."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_image_or_404(identifier, load_image=False):\n    try:\n        image_id = int(identifier)\n    except ValueError:\n        raise Http404()\n\n    ik_image = get_object_or_404(ICEkitImage, id=image_id)\n    if not load_image:\n        return ik_image, None\n\n    ####################################################################\n    # Image-loading incantation cribbed from easythumbnail's `pil_image`\n    image = Image.open(BytesIO(ik_image.image.read()))\n    # Fully load the image now to catch any problems with the image contents.\n    try:\n        # An \"Image file truncated\" exception can occur for some images that\n        # are still mostly valid -- we'll swallow the exception.\n        image.load()\n    except IOError:\n        pass\n    # Try a second time to catch any other potential exceptions.\n    image.load()\n    if True:  # Support EXIF orientation data\n        image = et_utils.exif_orientation(image)\n    ####################################################################\n\n    return ik_image, image", "response": "Return an image matching the given identifier."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef iiif_image_api_info(request, identifier_param):\n    # TODO Add support for 'application/ld+json' response when requested\n    accept_header = request.environ.get('HTTP_ACCEPT')\n    if accept_header == 'application/ld+json':\n        return HttpResponseNotImplemented(\n            \"JSON-LD response is not yet supported\")\n\n    ik_image, __ = _get_image_or_404(identifier_param)\n    info = {\n        \"@context\": \"http://iiif.io/api/image/2/context.json\",\n        \"@id\": request.get_full_path(),\n        \"@type\": \"iiif:Image\",\n        \"protocol\": \"http://iiif.io/api/image\",\n        \"width\": ik_image.width,\n        \"height\": ik_image.height,\n    }\n    # TODO Return more complete info.json response per spec\n\n    if ik_image.license:\n        info['license'] = [ik_image.license]\n\n    attribution_value = u' '.join([\n        u\"Credit: %s.\" % ik_image.credit if ik_image.credit else '',\n        u\"Provided by: %s.\" % ik_image.source if ik_image.source else '',\n    ]).strip()\n    if attribution_value:\n        info['attribution'] = [{\n            \"@value\": attribution_value,\n            \"@language\": \"en\",\n        }]\n\n    # TODO Send header \"Access-Control-Allow-Origin: *\" per spec?\n    return JsonResponse(info)", "response": "IIIF Image API 2. 1 Section 9. 1. 2. 1."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef iiif_image_api(request, identifier_param, region_param, size_param,\n                   rotation_param, quality_param, format_param):\n    \"\"\" Image repurposing endpoint for IIIF Image API 2.1 \"\"\"\n    ik_image, image = _get_image_or_404(identifier_param, load_image=True)\n\n    is_transparent = et_utils.is_transparent(image)\n    is_grayscale = image.mode in ('L', 'LA')\n\n    # Map format names used for IIIF URL path extension to proper name\n    format_mapping = {\n        'jpg': 'jpeg',\n        'tif': 'tiff',\n    }\n\n    try:\n        # Parse region\n        x, y, r_width, r_height = parse_region(\n            region_param, image.width, image.height)\n\n        # Parse size\n        s_width, s_height = parse_size(size_param, r_width, r_height)\n\n        # Parse rotation\n        is_mirrored, rotation_degrees = \\\n            parse_rotation(rotation_param, s_width, s_height)\n\n        # Parse quality\n        quality = parse_quality(quality_param)\n\n        # Parse format\n        # TODO Add support for unsupported formats (see `parse_format`)\n        image_format = os.path.splitext(ik_image.image.name)[1][1:].lower()\n        output_format = parse_format(format_param, image_format)\n        corrected_format = format_mapping.get(output_format, output_format)\n\n        # Redirect to canonical URL if appropriate, per\n        # http://iiif.io/api/image/2.1/#canonical-uri-syntax\n        canonical_path = make_canonical_path(\n            identifier_param, image.width, image.height,\n            (x, y, r_width, r_height),  # Region\n            (s_width, s_height),  # Size\n            (is_mirrored, rotation_degrees),  # Rotation\n            quality,\n            output_format\n        )\n        if request.path != canonical_path:\n            return HttpResponseRedirect(canonical_path)\n\n        # Determine storage file name for item\n        if iiif_storage:\n            storage_path = build_iiif_file_storage_path(\n                canonical_path, ik_image, iiif_storage)\n        else:\n            storage_path = None\n\n        # Load pre-generated image from storage if one exists and is up-to-date\n        # with the original image (per timestampt info embedded in the storage\n        # path)\n        # TODO The exists lookup is slow for S3 storage, cache metadata?\n        # TODO Detect when original image would be unchanged & use it directly?\n        if (\n            storage_path and\n            iiif_storage.exists(storage_path)\n        ):\n            if is_remote_storage(iiif_storage, storage_path):\n                return HttpResponseRedirect(iiif_storage.url(storage_path))\n            else:\n                return FileResponse(\n                    iiif_storage.open(storage_path),\n                    content_type='image/%s' % corrected_format,\n                )\n\n        ##################\n        # Generate image #\n        ##################\n\n        # Apply region\n        if x or y or r_width != image.width or r_height != image.height:\n            box = (x, y, x + r_width, y + r_height)\n            image = image.crop(box)\n\n        # Apply size\n        if s_width != r_width or s_height != r_height:\n            size = (s_width, s_height)\n            image = image.resize(size)\n\n        # TODO Apply rotation\n\n        # Apply quality\n        # Much of this is cribbed from easythumbnails' `colorspace` processor\n        # TODO Replace with glamkit-imagetools' sRGB colour space converter?\n        if quality in ('default', 'color') and not is_grayscale:\n            if is_transparent:\n                new_mode = 'RGBA'\n            else:\n                new_mode = 'RGB'\n        elif is_grayscale or quality == 'gray':\n            if is_transparent:\n                new_mode = 'LA'\n            else:\n                new_mode = 'L'\n        if new_mode != image.mode:\n            image = image.convert(new_mode)\n\n        # Apply format and \"save\"\n        result_image = BytesIO()\n        image.save(result_image, format=corrected_format)\n\n        # Save generated image to storage if possible\n        if storage_path:\n            iiif_storage.save(storage_path, result_image)\n\n        if iiif_storage and is_remote_storage(iiif_storage, storage_path):\n            return HttpResponseRedirect(iiif_storage.url(storage_path))\n        else:\n            result_image.seek(0)  # Reset image file in case it's just created\n            return FileResponse(\n                result_image.read(),\n                content_type='image/%s' % corrected_format,\n            )\n    # Handle error conditions per iiif.io/api/image/2.1/#server-responses\n    except ClientError, ex:\n        return HttpResponseBadRequest(ex.message)  # 400 response\n    except UnsupportedError, ex:\n        return HttpResponseNotImplemented(ex.message)", "response": "IIIF Image API 2. 1"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmakes sure the URL provided matches the instagram URL format.", "response": "def clean_url(self):\n        \"\"\"\n        Make sure the URL provided matches the instagram URL format.\n        \"\"\"\n        url = self.cleaned_data['url']\n\n        if url:\n            pattern = re.compile(r'https?://(www\\.)?instagr(\\.am|am\\.com)/p/\\S+')\n            if not pattern.match(url):\n                raise forms.ValidationError('Please provide a valid instagram link.')\n\n        return url"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds sample events to the internal log file.", "response": "def forwards(apps, schema_editor):\n    \"\"\"\n    Create sample events.\n    \"\"\"\n    starts = timeutils.round_datetime(\n        when=timezone.now(),\n        precision=timedelta(days=1),\n        rounding=timeutils.ROUND_DOWN)\n    ends = starts + appsettings.DEFAULT_ENDS_DELTA\n\n    recurrence_rules = dict(\n        RecurrenceRule.objects.values_list('description', 'recurrence_rule'))\n    daily = recurrence_rules['Daily']\n    weekdays = recurrence_rules['Daily, Weekdays']\n    weekends = recurrence_rules['Daily, Weekends']\n    weekly = recurrence_rules['Weekly']\n    monthly = recurrence_rules['Monthly']\n    yearly = recurrence_rules['Yearly']\n\n    daily_event = G(\n        EventBase,\n        title='Daily Event',\n        starts=starts + timedelta(hours=9),\n        ends=ends + timedelta(hours=9),\n        recurrence_rule=daily,\n    )\n\n    weekday_event = G(\n        EventBase,\n        title='Weekday Event',\n        starts=starts + timedelta(hours=11),\n        ends=ends + timedelta(hours=11),\n        recurrence_rule=weekdays,\n    )\n\n    weekend_event = G(\n        EventBase,\n        title='Weekend Event',\n        starts=starts + timedelta(hours=13),\n        ends=ends + timedelta(hours=13),\n        recurrence_rule=weekends,\n    )\n\n    weekly_event = G(\n        EventBase,\n        title='Weekly Event',\n        starts=starts + timedelta(hours=15),\n        ends=ends + timedelta(hours=15),\n        recurrence_rule=weekly,\n    )\n\n    monthly_event = G(\n        EventBase,\n        title='Monthly Event',\n        starts=starts + timedelta(hours=17),\n        ends=ends + timedelta(hours=17),\n        recurrence_rule=monthly,\n    )\n\n    yearly_event = G(\n        EventBase,\n        title='Yearly Event',\n        starts=starts + timedelta(hours=19),\n        ends=ends + timedelta(hours=19),\n        recurrence_rule=yearly,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef backwards(apps, schema_editor):\n    titles = [\n        'Daily Event',\n        'Weekday Event',\n        'Weekend Event',\n        'Weekly Event',\n        'Monthly Event',\n        'Yearly Event',\n    ]\n    samples = EventBase.objects.filter(title__in=titles)\n    samples.delete()", "response": "Delete sample events in order to be able to use the same event types."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dump_viewset(viewset_class, root_folder, folder_fn=lambda i: \".\", sample_size=None):\n    if os.path.exists(root_folder):\n        shutil.rmtree(root_folder)\n    os.makedirs(root_folder)\n\n    vs = viewset_class()\n    vs.request = rf.get('')\n\n    serializer_class = vs.get_serializer_class()\n    serializer = serializer_class(context={'request': vs.request, 'format': 'json', 'view': vs})\n\n    renderer = PrettyJSONRenderer()\n\n    bar = progressbar.ProgressBar()\n    for instance in bar(vs.get_queryset()[:sample_size]):\n        dct = serializer.to_representation(instance)\n        content = renderer.render(dct)\n        folder = os.path.join(root_folder, folder_fn(instance))\n        if not os.path.exists(folder):\n            os.makedirs(folder)\n        filename = \"%s.json\" % instance.slug\n\n        f = file(os.path.join(folder, filename), 'w')\n        f.write(content)\n        f.close()", "response": "Dump the contents of a rest - api viewset to a folder structure."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconcatenating all the json files in a folder to one big JSON file.", "response": "def concatenate_json(source_folder, destination_file):\n    \"\"\"\n    Concatenate all the json files in a folder to one big JSON file.\n    \"\"\"\n    matches = []\n    for root, dirnames, filenames in os.walk(source_folder):\n        for filename in fnmatch.filter(filenames, '*.json'):\n            matches.append(os.path.join(root, filename))\n\n    with open(destination_file, \"wb\") as f:\n        f.write(\"[\\n\")\n        for m in matches[:-1]:\n            f.write(open(m, \"rb\").read())\n            f.write(\",\\n\")\n        f.write(open(matches[-1], \"rb\").read())\n        f.write(\"\\n]\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef validate_unique_slug(self):\n        clashes_qs = type(self).objects.filter(slug=self.slug)\n        if self.pk:\n            clashes_qs = clashes_qs.exclude(pk=self.pk)\n        if isinstance(self, PublishingModel):\n            clashes_qs = clashes_qs.filter(\n                publishing_is_draft=self.publishing_is_draft)\n        if clashes_qs:\n            raise ValidationError(\n                \"Slug '%s' clashes with other items: %s\"\n                % (self.slug, clashes_qs))", "response": "Ensure that the slug is unique for this object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the absolute URL of the current object.", "response": "def get_absolute_url(self):\n        \"\"\"\n        The majority of the time, the URL is the parent's URL plus the slug.\n        If not, override this function.\n        \"\"\"\n        # Get the appropriate draft or published parent object for the current\n        # item, otherwise we risk mixing the draft parent URL with a published\n        # item's URL etc.\n        if self.is_draft:\n            visible_parent = self.parent.get_draft()\n        else:\n            # Fallback to draft URL for unpublished parent.\n            visible_parent = self.parent.get_published() or \\\n                self.parent.get_draft()\n\n        # Appending \"/\" to avoid a) django redirect and b) incorrect edit slug\n        # for the admin.\n        return urljoin(visible_parent.get_absolute_url(), self.slug + \"/\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_response(self, request, parent, *args, **kwargs):\n        context = {\n            'page': self,\n        }\n        try:\n            return TemplateResponse(\n                request,\n                self.get_layout_template_name(),\n                context\n            )\n        except AttributeError:\n            raise AttributeError(\"You need to define \"\n                 \"`get_layout_template_name()` on your `%s` model, \"\n                 \"or override `get_response()`\" % type(self).__name__)", "response": "Render this collected content to a response."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef forwards_migration(apps, schema_editor):\n    RecurrenceRule = apps.get_model('icekit_events', 'RecurrenceRule')\n    for rrule in RecurrenceRule.objects.all():\n        if ';\\n' in rrule.recurrence_rule:\n            parts = rrule.recurrence_rule.split(';\\n')\n            rrule.recurrence_rule = '\\n'.join(parts)\n            rrule.save()", "response": "Migrate malformed recurrence rules to new format."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_response_page(request, return_type, template_location, response_page_type):\n    try:\n        page = models.ResponsePage.objects.get(\n            is_active=True,\n            type=response_page_type,\n        )\n        template = loader.get_template(template_location)\n        content_type = None\n\n        body = template.render(\n            RequestContext(request, {'request_path': request.path, 'page': page, })\n        )\n        return return_type(body, content_type=content_type)\n    except models.ResponsePage.DoesNotExist:\n        return None", "response": "Helper function to get an appropriate response page if it exists."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef page_not_found(request, template_name='404.html'):\n    rendered_page = get_response_page(\n        request,\n        http.HttpResponseNotFound,\n        'icekit/response_pages/404.html',\n        abstract_models.RESPONSE_HTTP404\n    )\n    if rendered_page is None:\n        return defaults.page_not_found(request, template_name)\n    return rendered_page", "response": "Custom page not found handler."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef allowed_to_preview(user):\n    if (\n        user.is_authenticated and\n        user.is_active and\n        user.is_staff\n    ):\n        return True\n    return False", "response": "Returns True if the user is allowed to view the preview?"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_child_type_choices(self, request, action):\n        # Get choices from the super class to check permissions.\n        choices = super(ChildModelPluginPolymorphicParentModelAdmin, self) \\\n            .get_child_type_choices(request, action)\n        # Update label with verbose name from plugins.\n        plugins = self.child_model_plugin_class.get_plugins()\n        labels = {}\n        sort_priorities = {}\n\n        if plugins:\n            for plugin in plugins:\n                pk = plugin.content_type.pk\n                labels[pk] = capfirst(plugin.verbose_name)\n                sort_priorities[pk] = getattr(plugin, 'sort_priority', labels[pk])\n\n            choices = [(ctype, labels[ctype]) for ctype, _ in choices]\n            return sorted(choices,\n                cmp=lambda a, b: cmp(\n                    sort_priorities[a[0]],\n                    sort_priorities[b[0]]\n                )\n            )\n\n        return choices", "response": "Override choices with verbose name from plugins and sort."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget child models from registered plugins. Fallback to the child model admin and its base model.", "response": "def get_child_models(self):\n        \"\"\"\n        Get child models from registered plugins. Fallback to the child model\n        admin and its base model if no plugins are registered.\n        \"\"\"\n        child_models = []\n        for plugin in self.child_model_plugin_class.get_plugins():\n            child_models.append((plugin.model, plugin.model_admin))\n        if not child_models:\n            child_models.append((\n                self.child_model_admin.base_model,\n                self.child_model_admin,\n            ))\n        return child_models"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the separate AdminSite instance that django - polymorphic has a child model that is registered in the admin site.", "response": "def _get_child_admin_site(self, rel):\n        \"\"\"\n        Returns the separate AdminSite instance that django-polymorphic\n        maintains for child models.\n\n        This admin site needs to be passed to the widget so that it passes the\n        check of whether the field is pointing to a model that's registered\n        in the admin.\n\n        The hackiness of this implementation reflects the hackiness of the way\n        django-polymorphic does things.\n        \"\"\"\n        if rel.to not in self.admin_site._registry:\n            # Go through the objects the model inherits from and find one\n            # that's registered in the main admin and has a reference to the\n            # child admin site in it attributes.\n            for parent in rel.to.mro():\n                if parent in self.admin_site._registry \\\n                and hasattr(self.admin_site._registry[parent], '_child_admin_site'):\n                    return self.admin_site._registry[parent]._child_admin_site\n        return self.admin_site"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef formfield_for_foreignkey(self, db_field, request=None, **kwargs):\n        db = kwargs.get('using')\n        if db_field.name in self.raw_id_fields:\n            kwargs['widget'] = PolymorphicForeignKeyRawIdWidget(\n                db_field.rel,\n                admin_site=self._get_child_admin_site(db_field.rel),\n                using=db\n            )\n            if 'queryset' not in kwargs:\n                queryset = self.get_field_queryset(db, db_field, request)\n                if queryset is not None:\n                    kwargs['queryset'] = queryset\n            return db_field.formfield(**kwargs)\n        return super(PolymorphicAdminRawIdFix, self).formfield_for_foreignkey(\n            db_field, request=request, **kwargs)", "response": "Replicates the logic in ModelAdmin. forfield_for_foreignkey replacing the widget with the patched one"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef wikipedia_slugify(value, do_unidecode=False):\n    if do_unidecode:\n        value = unidecode(value)\n    value = value.strip()\n    return mark_safe(re.sub('[\\s/#\\?:@]+', '_', value))", "response": "Converts a string to ASCII via unidecode."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ensure_unique(qs, field_name, value, exclude_id=None):\n    orig = value\n    if not value:\n        value = \"None\"\n    for x in itertools.count(1):\n        if not qs.exclude(id=exclude_id).filter(**{field_name: value}).exists():\n            break\n        if orig:\n            value = '%s-%d' % (orig, x)\n        else:\n            value = '%d' % x\n\n    return value", "response": "Makes sure that value is unique on model. fieldname. And nonempty."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting \\ r \\ n and \\ r to \\ n chars. Strip any leading or trailing whitespace on each line. Remove blank lines.", "response": "def fix_line_breaks(s):\n    \"\"\"\n    Convert \\r\\n and \\r to \\n chars. Strip any leading or trailing whitespace\n    on each line. Remove blank lines.\n    \"\"\"\n    l = s.splitlines()\n    x = [i.strip() for i in l]\n    x = [i for i in x if i]  # remove blank lines\n    return \"\\n\".join(x)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update_json_analysis(analysis, j):\n    def _analyze_list(l, parent=\"\"):\n        for v in l:\n            if isinstance(v, (dict, CaseInsensitiveDict)):\n                _analyze_json(v, parent=parent)\n            elif isinstance(v, list):\n                _analyze_list(v, parent=parent+\"[]\")\n            else:\n                analysis[parent].add(v)\n\n\n    def _analyze_json(d, parent=\"\"):\n        for k, v in d.iteritems():\n            if parent:\n                path = \".\".join([parent, k])\n            else:\n                path = k\n            if isinstance(v, (dict, CaseInsensitiveDict)):\n                _analyze_json(v, parent=path)\n            elif isinstance(v, list):\n                _analyze_list(v, parent=path+\"[]\")\n            else:\n                analysis[path].add(v)\n\n    if isinstance(j, list):\n        _analyze_list(j)\n    if isinstance(j, (dict, CaseInsensitiveDict)):\n        _analyze_json(j)", "response": "Update the analysis dict with the values found in the json."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmonkey patch the original CTYPE method to override a new version of the same name.", "response": "def monkey_patch_override_method(klass):\n    \"\"\"\n    Override a class method with a new version of the same name. The original\n    method implementation is made available within the override method as\n    `_original_<METHOD_NAME>`.\n    \"\"\"\n    def perform_override(override_fn):\n        fn_name = override_fn.__name__\n        original_fn_name = '_original_' + fn_name\n        # Override class method, if it hasn't already been done\n        if not hasattr(klass, original_fn_name):\n            original_fn = getattr(klass, fn_name)\n            setattr(klass, original_fn_name, original_fn)\n            setattr(klass, fn_name, override_fn)\n    return perform_override"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef monkey_patch_override_instance_method(instance):\n    def perform_override(override_fn):\n        fn_name = override_fn.__name__\n        original_fn_name = '_original_' + fn_name\n        # Override instance method, if it hasn't already been done\n        if not hasattr(instance, original_fn_name):\n            original_fn = getattr(instance, fn_name)\n            setattr(instance, original_fn_name, original_fn)\n            bound_override_fn = override_fn.__get__(instance)\n            setattr(instance, fn_name, bound_override_fn)\n    return perform_override", "response": "Monkey patch the override method to work with the new version of the same name."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_missing_placeholders(self):\n        content_type = ContentType.objects.get_for_model(self)\n        result = False\n        if self.layout:\n            for data in self.layout.get_placeholder_data():\n                placeholder, created = Placeholder.objects.update_or_create(\n                    parent_type=content_type,\n                    parent_id=self.pk,\n                    slot=data.slot,\n                    defaults=dict(\n                        role=data.role,\n                        title=data.title,\n                    ))\n                result = result or created\n        return result", "response": "Add missing placeholders from templates. Return True if any missing placeholders were created."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_type_plural(self):\n        t = self.get_type()\n        if t:\n            if hasattr(t, 'get_plural'):\n                return t.get_plural()\n            if t == type(self)._meta.verbose_name:\n                return unicode(type(self)._meta.verbose_name_plural)\n            return u\"{0}s\".format(t)\n\n        return unicode(type(self)._meta.verbose_name_plural)", "response": "returns a string indicating the plural of the type name"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the OG Title of the object.", "response": "def get_og_title(self):\n        \"\"\"\n        return meta_title if exists otherwise fall back to title\n        \"\"\"\n        if hasattr(self, 'meta_title') and self.meta_title:\n            return self.meta_title\n        return self.get_title()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_og_image_url(self):\n        li = self.get_list_image()\n        if li:\n            from easy_thumbnails.files import get_thumbnailer\n            thumb_url = get_thumbnailer(li)['og_image'].url\n            # TODO: looks like this may fail if SITE_DOMAIN = \"acmi.lvh.me\"\n            return urljoin(settings.SITE_DOMAIN, thumb_url)", "response": "Returns the URL of the image to use in OG shares"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a serializable data set describing the map location", "response": "def get_map_data(self):\n        \"\"\"\n        Returns a serializable data set describing the map location\n        \"\"\"\n\n        return {\n            'containerSelector': '#' + self.get_map_element_id(),\n            'center': self.map_center_description,\n            'marker': self.map_marker_description or self.map_center_description,\n            'zoom': self.map_zoom,\n            'href': self.get_map_href(),\n            'key': getattr(settings, 'GOOGLE_MAPS_API_KEY', ''),\n            # Python's line-splitting is more cross-OS compatible, so we feed\n            # a pre-built array to the front-end\n            'description': [\n                line for line in self.map_description.splitlines() if line\n            ],\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_map_href(self):\n        if self.map_center_lat and self.map_center_long:\n            params = {\n                'll': '%s,%s' % (self.map_center_lat, self.map_center_long)\n            }\n        else:\n            params = {'q': self.map_center_description}\n\n        return self.GOOGLE_MAPS_HREF_ROOT + urllib.urlencode(params)", "response": "Returns a link to an external view of the map"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nrounds a datetime object to a time that matches the given precision.", "response": "def round_datetime(when=None, precision=60, rounding=ROUND_NEAREST):\n    \"\"\"\n    Round a datetime object to a time that matches the given precision.\n\n        when (datetime), default now\n            The datetime object to be rounded.\n\n        precision (int, timedelta, str), default 60\n            The number of seconds, weekday (MON, TUE, WED, etc.) or timedelta\n            object to which the datetime object should be rounded.\n\n        rounding (str), default ROUND_NEAREST\n            The rounding method to use (ROUND_DOWN, ROUND_NEAREST, ROUND_UP).\n\n    \"\"\"\n    when = when or djtz.now()\n    weekday = WEEKDAYS.get(precision, WEEKDAYS['MON'])\n    if precision in WEEKDAYS:\n        precision = int(timedelta(days=7).total_seconds())\n    elif isinstance(precision, timedelta):\n        precision = int(precision.total_seconds())\n    # Get delta between the beginning of time and the given datetime object.\n    # If precision is a weekday, the beginning of time must be that same day.\n    when_min = when.min + timedelta(days=weekday)\n    if djtz.is_aware(when):\n        # It doesn't seem to be possible to localise `datetime.min` without\n        # raising `OverflowError`, so create a timezone aware object manually.\n        when_min = datetime(tzinfo=when.tzinfo, *when_min.timetuple()[:3])\n    delta = when - when_min\n    remainder = int(delta.total_seconds()) % precision\n    # First round down and strip microseconds.\n    when -= timedelta(seconds=remainder, microseconds=when.microsecond)\n    # Then add precision to round up.\n    if rounding == ROUND_UP or (\n            rounding == ROUND_NEAREST and remainder >= precision / 2):\n        when += timedelta(seconds=precision)\n    return when"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a given datetime with hour minute seconds and microseconds zeroed and the timezone coerced to the given timezone.", "response": "def zero_datetime(dt, tz=None):\n    \"\"\"\n    Return the given datetime with hour/minutes/seconds/ms zeroed and the\n    timezone coerced to the given ``tz`` (or UTC if none is given).\n    \"\"\"\n    if tz is None:\n        tz = get_current_timezone()\n    return coerce_naive(dt).replace(hour=0, minute=0, second=0, microsecond=0)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef coerce_dt_awareness(date_or_datetime, tz=None, t=None):\n    if isinstance(date_or_datetime, datetime):\n        dt = date_or_datetime\n    else:\n        dt = datetime.combine(date_or_datetime, t or time.min)\n    is_project_tz_aware = settings.USE_TZ\n    if is_project_tz_aware:\n        return coerce_aware(dt, tz)\n    elif not is_project_tz_aware:\n        return coerce_naive(dt, tz)\n    # No changes necessary\n    return dt", "response": "Coerce the given datetime or date object into a timezone - aware or\n    timezone - naive datetime result depending on which is appropriate for\n    the project s settings."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a datetime formatted for use in iCal as a naive datetime value.", "response": "def format_naive_ical_dt(date_or_datetime):\n    \"\"\"\n    Return datetime formatted for use in iCal as a *naive* datetime value to\n    work more like people expect, e.g. creating a series of events starting\n    at 9am should not create some occurrences that start at 8am or 10am after\n    a daylight savings change.\n    \"\"\"\n    dt = coerce_dt_awareness(date_or_datetime)\n    if is_naive(dt):\n        return dt.strftime('%Y%m%dT%H%M%S')\n    else:\n        return dt.astimezone(get_current_timezone()).strftime('%Y%m%dT%H%M%S')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a given datetime with the same time - of - day as the given datetime with the same year month day and hour and minute as the given datetime.", "response": "def localize_preserving_time_of_day(dt):\n    \"\"\"\n    Return the given datetime with the same time-of-day (hours and minutes) as\n    it *seems* to have, even if it has been adjusted by applying `timedelta`\n    additions that traverse daylight savings dates and would therefore\n    otherwise trigger changes to its apparent time.\n    \"\"\"\n    # Remember the apparent time-of-day years, month, hours and minutes\n    year, month, day, hour, minute = \\\n        dt.year, dt.month, dt.day, dt.hour, dt.minute\n    # Ensure we have a localized datetime, which will trigger any hour/minute\n    # changes depending on daylight saving changes triggered by a timedelta\n    # operation\n    dt_localized = djtz.localize(dt)\n    # Forcibly reset the hour and minute time-of-day values to the apparent\n    # values we stored\n    return dt_localized.replace(\n        year=year, month=month, day=day, hour=hour, minute=minute)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_draft_secret_key():\n    # TODO: Per URL secret keys, so we can invalidate draft URLs for individual\n    #       pages. For example, on publish.\n    draft_secret_key, created = Text.objects.get_or_create(\n        name='DRAFT_SECRET_KEY',\n        defaults=dict(\n            value=get_random_string(50),\n        ))\n    return draft_secret_key.value", "response": "Returns the secret key used to generate draft mode HMACs."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the given URL with a draft mode HMAC in its querystring.", "response": "def get_draft_url(url):\n    \"\"\"\n    Return the given URL with a draft mode HMAC in its querystring.\n    \"\"\"\n    if verify_draft_url(url):\n        # Nothing to do. Already a valid draft URL.\n        return url\n    # Parse querystring and add draft mode HMAC.\n    url = urlparse.urlparse(url)\n    salt = get_random_string(5)\n    # QueryDict requires a bytestring as its first argument\n    query = QueryDict(force_bytes(url.query), mutable=True)\n    query['preview'] = '%s:%s' % (salt, get_draft_hmac(salt, url.path))\n    # Reconstruct URL.\n    parts = list(url)\n    parts[4] = query.urlencode(safe=':')\n    return urlparse.urlunparse(parts)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nverifying that the given URL has a draft mode HMAC in its querystring.", "response": "def verify_draft_url(url):\n    \"\"\"\n    Return ``True`` if the given URL has a valid draft mode HMAC in its\n    querystring.\n    \"\"\"\n    url = urlparse.urlparse(url)\n    # QueryDict requires a bytestring as its first argument\n    query = QueryDict(force_bytes(url.query))\n    # TODO Support legacy 'edit' param name for now\n    preview_hmac = query.get('preview') or query.get('edit')\n    if preview_hmac:\n        salt, hmac = preview_hmac.split(':')\n        return hmac == get_draft_hmac(salt, url.path)\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_visible_object_or_404(klass, *args, **kwargs):\n    qs = _get_queryset(klass)\n    # If class is publishable, find only *visible* objects\n    try:\n        qs = qs.visible()\n    except AttributeError:\n        pass  # Ignore error calling `visible()` on unpublishable class\n    return get_object_or_404(qs, *args, **kwargs)", "response": "Returns the object that is visible in the current context or raises a Http404 if the object is not available."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef contribute_to_class(model_class, name='slots', descriptor=None):\n    rel_obj = descriptor or PlaceholderDescriptor()\n    rel_obj.contribute_to_class(model_class, name)\n    setattr(model_class, name, rel_obj)\n    return True", "response": "Function that adds a description to a model class."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a PlaceholderAccess object for the given instance.", "response": "def create_placeholder_access_object(self, instance):\n        \"\"\"\n        Created objects with placeholder slots as properties.\n\n        Each placeholder created for an object will be added to a\n        `PlaceHolderAccess` object as a set property.\n        \"\"\"\n        related_model = self.related_model\n\n        def get_related_model_objects(name):\n            \"\"\"\n            Obtains the related model objects based upon the slot name.\n\n            :param name: The slot name in string form.\n            :returns; Related model contents if they exist or it will\n            raise a `DoesNotExist` exception.\n            \"\"\"\n            # Parent type, parent id and slot are set to be unique on the default\n            # related model and therefore treated as such here.\n            return related_model.objects.get(\n                parent_type=ContentType.objects.get_for_model(type(instance)),\n                parent_id=instance.id,\n                slot=name,\n            ).get_content_items()\n\n        class PlaceholderAccess(object):\n            def __getattribute__(self, name):\n                \"\"\"\n                Allow placeholder contents to be accessed via slot\n                name on the descriptor object.\n\n                For example if the slot was named `main` and you had\n                a descriptor named `slots` on an object named `page`\n                you would call it by `page.slots.main`.\n\n                If a slot name is used that does not exist an\n                `AttributeError` will be raised.\n\n                If you get this error for a slot that should exist, you may\n                need to run `manage.py add_missing_placeholders`.\n                \"\"\"\n                try:\n                    return get_related_model_objects(name)\n                except related_model.DoesNotExist:\n                    return super(PlaceholderAccess, self).__getattribute__(name)\n\n            def __getitem__(self, item):\n                \"\"\"\n                Allow placeholder contents to be accessed via slot\n                name one the descriptor object via a dictionary\n                lookup.\n\n                For example if the slot was named `main` and you had\n                a descriptor named `slots` on an object named `page`\n                you would call it by `page.slots['main']`.\n\n                If a slot name is used that does not exist a\n                `KeyError` will be raised.\n                \"\"\"\n                try:\n                    return get_related_model_objects(item)\n                except related_model.DoesNotExist:\n                    raise KeyError\n\n        return PlaceholderAccess()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_users_for_assigned_to():\n    User = get_user_model()\n    return User.objects.filter(is_active=True, is_staff=True)", "response": "Return a list of users who can be assigned to workflow states"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_obj_ct(self, obj):\n        if not hasattr(obj, '_wfct'):\n            # Use polymorpic content type if available\n            if hasattr(obj, 'polymorphic_ctype'):\n                obj._wfct = obj.polymorphic_ctype\n            else:\n                obj._wfct = ContentType.objects.get_for_model(obj)\n        return obj._wfct", "response": "Look up and return object s content type and cache for reuse"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns text description of workflow states assigned to object", "response": "def workflow_states_column(self, obj):\n        \"\"\" Return text description of workflow states assigned to object \"\"\"\n        workflow_states = models.WorkflowState.objects.filter(\n            content_type=self._get_obj_ct(obj),\n            object_id=obj.pk,\n        )\n        return ', '.join([unicode(wfs) for wfs in workflow_states])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef created_by_column(self, obj):\n        try:\n            first_addition_logentry = admin.models.LogEntry.objects.filter(\n                object_id=obj.pk,\n                content_type_id=self._get_obj_ct(obj).pk,\n                action_flag=admin.models.ADDITION,\n            ).get()\n            return first_addition_logentry.user\n        except admin.models.LogEntry.DoesNotExist:\n            return None", "response": "Return the user who first created an item in Django admin"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the user who last edited an item in Django admin where edited is True.", "response": "def last_edited_by_column(self, obj):\n        \"\"\"\n        Return user who last edited an item in Django admin, where \"edited\"\n        means either created (addition) or modified (change).\n        \"\"\"\n        latest_logentry = admin.models.LogEntry.objects.filter(\n            object_id=obj.pk,\n            content_type_id=self._get_obj_ct(obj).pk,\n            action_flag__in=[admin.models.ADDITION, admin.models.CHANGE],\n        ).first()\n        if latest_logentry:\n            return latest_logentry.user\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_content_instance(content_plugin_class, page, placeholder_name='main', **kwargs):\n    # Get the placeholders that are currently available for the slot.\n    placeholders = page.get_placeholder_by_slot(placeholder_name)\n\n    # If a placeholder exists for the placeholder_name use the first one provided otherwise create\n    # a new placeholder instance.\n    if placeholders.exists():\n        placeholder = placeholders[0]\n    else:\n        placeholder = page.create_placeholder(placeholder_name)\n\n    # Obtain the content type for the page instance class.\n    ct = ContentType.objects.get_for_model(type(page))\n\n    # Create the actual plugin instance.\n    try:\n        content_instance = content_plugin_class.objects.create(\n            parent_type=ct,\n            parent_id=page.id,\n            placeholder=placeholder,\n            **kwargs\n        )\n    except TypeError:\n        raise Exception(\n            'Could not create content item instance, ensure you '\n            'have all required field values for the Model.'\n        )\n    return content_instance", "response": "Creates a content instance from a content plugin class."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nexpand Python versions to all identifiers used on PyPI.", "response": "def expand_python_version(version):\n    \"\"\"\n    Expand Python versions to all identifiers used on PyPI.\n\n    >>> expand_python_version('3.5')\n    ['3.5', 'py3', 'py2.py3', 'cp35']\n    \"\"\"\n    if not re.match(r\"^\\d\\.\\d$\", version):\n        return [version]\n\n    major, minor = version.split(\".\")\n    patterns = [\n        \"{major}.{minor}\",\n        \"cp{major}{minor}\",\n        \"py{major}\",\n        \"py{major}.{minor}\",\n        \"py{major}{minor}\",\n        \"source\",\n        \"py2.py3\",\n    ]\n    return set(pattern.format(major=major, minor=minor) for pattern in patterns)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the hashes for a given package and version.", "response": "def get_package_hashes(\n    package,\n    version=None,\n    algorithm=DEFAULT_ALGORITHM,\n    python_versions=(),\n    verbose=False,\n    include_prereleases=False,\n    lookup_memory=None,\n    index_url=DEFAULT_INDEX_URL,\n):\n    \"\"\"\n    Gets the hashes for the given package.\n\n    >>> get_package_hashes('hashin')\n    {\n        'package': 'hashin',\n        'version': '0.10',\n        'hashes': [\n            {\n                'url': 'https://pypi.org/packages/[...]',\n                'hash': '45d1c5d2237a3b4f78b4198709fb2ecf[...]'\n            },\n            {\n                'url': 'https://pypi.org/packages/[...]',\n                'hash': '0d63bf4c115154781846ecf573049324[...]'\n            },\n            {\n                'url': 'https://pypi.org/packages/[...]',\n                'hash': 'c32e6d9fb09dc36ab9222c4606a1f43a[...]'\n            }\n        ]\n    }\n    \"\"\"\n    if lookup_memory is not None and package in lookup_memory:\n        data = lookup_memory[package]\n    else:\n        data = get_package_data(package, index_url, verbose)\n    if not version:\n        version = get_latest_version(data, include_prereleases)\n        assert version\n        if verbose:\n            _verbose(\"Latest version for {0} is {1}\".format(package, version))\n\n    # Independent of how you like to case type it, pick the correct\n    # name from the PyPI index.\n    package = data[\"info\"][\"name\"]\n\n    try:\n        releases = data[\"releases\"][version]\n    except KeyError:\n        raise PackageError(\"No data found for version {0}\".format(version))\n\n    if python_versions:\n        releases = filter_releases(releases, python_versions)\n\n    if not releases:\n        if python_versions:\n            raise PackageError(\n                \"No releases could be found for \"\n                \"{0} matching Python versions {1}\".format(version, python_versions)\n            )\n        else:\n            raise PackageError(\"No releases could be found for {0}\".format(version))\n\n    hashes = list(\n        get_releases_hashes(releases=releases, algorithm=algorithm, verbose=verbose)\n    )\n    return {\"package\": package, \"version\": version, \"hashes\": hashes}"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef formfield(self, **kwargs):\n        if self.plugin_class:\n            self._choices = self.plugin_class.get_all_choices(field=self)\n        return super(TemplateNameField, self).formfield(**kwargs)", "response": "Get choices from plugins if necessary."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef register_model_once(cls, ModelClass, **kwargs):\n        if cls._static_registry.get_for_model(ModelClass) is None:\n            logger.warn(\"Model is already registered with {0}: '{1}'\"\n                        .format(cls, ModelClass))\n        else:\n            cls.register_model.register(ModelClass, **kwargs)", "response": "Tweaked version of AnyUrlField. register_model that only registers the\n            given model."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _build_app_models(request, admin_apps, models_tuples, ensure_all_models=False):\n\n    app_models = []\n    for app_and_model, config in models_tuples:\n        if app_and_model:\n            app_label, model_name = app_and_model.split('.')\n\n            for app in admin_apps:\n                if app['app_label'] == app_label:\n                    for model in app['models']:\n                        if model['object_name'] == model_name:\n                            try:\n                                model['name'] = \\\n                                    config['name']\n                            except KeyError:\n                                pass\n\n                            # Get each of the polymorphic types to allow addition.\n                            model_class = apps.get_model(app_and_model)\n                            model_admin = admin.site._registry[model_class]\n\n                            if hasattr(model_admin, 'get_child_type_choices'):\n                                model['polymorphic_classes'] = \\\n                                    model_admin.get_child_type_choices(\n                                        request, 'add'\n                                    )\n\n                            # TODO: Fluent/polymorphic has a way of sorting child order which should be used instead\n                            if 'default_poly_child' in config.keys():\n                                ct = ContentType.objects.get_by_natural_key(\n                                    *config['default_poly_child'].lower().split('.')\n                                )\n                                model.default_poly_child = ct.id\n\n                            app_models.append(model)\n\n                    break\n\n    return app_models", "response": "Builds the list of models that can be added to the admin instance."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef filter_featured_apps(admin_apps, request):\n    featured_apps = []\n\n    # Build the featured apps list based upon settings.\n    for orig_app_spec in appsettings.DASHBOARD_FEATURED_APPS:\n        # make a copy that we can write to, to fix deprecations without\n        # changing settings\n        app_spec = orig_app_spec.copy()\n\n        if \"verbose_name\" in app_spec:\n            warnings.warn(\n                \"DASHBOARD_FEATURED_APPS[]['verbose_name'] = '%s' is deprecated. \"\n                \"Use 'name' instead)\" % app_spec['verbose_name'],\n                DeprecationWarning, stacklevel=2\n            )\n            app_spec['name'] = app_spec['verbose_name']\n\n        if hasattr(app_spec['models'], 'items'):\n            warnings.warn(\n                \"DASHBOARD_FEATURED_APPS[]['models'] for '%s' should now be a \"\n                \"list of tuples, not a dict.\" % app_spec['name'],\n                DeprecationWarning, stacklevel=2\n            )\n            app_spec['models'] = app_spec['models'].items()\n\n        # lookup the models from the names\n        app_spec['models'] = _build_app_models(\n            request, admin_apps, app_spec['models']\n        )\n\n        # Only add the panel if at least one model is listed.\n        if app_spec['models']:\n            featured_apps.append(app_spec)\n\n    return featured_apps", "response": "Given a list of apps return a set of pseudo - apps that are considered featured."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _remove_app_models(all_apps, models_to_remove):\n\n    filtered_apps = []\n\n    for app in all_apps:\n        models = [x for x in app['models'] if x not in models_to_remove]\n        if models:\n            app['models'] = models\n            filtered_apps.append(app)\n\n    return filtered_apps", "response": "Remove the models in models_to_remove from the apps in all_apps."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef filter_sorted_apps(admin_apps, request):\n\n    sorted_apps = []\n\n    for orig_app_spec in appsettings.DASHBOARD_SORTED_APPS:\n        # make a copy that we can write to, to fix deprecations without\n        # changing settings\n        app_spec = orig_app_spec.copy()\n        # lookup the models from the names\n        app_spec['models'] = _build_app_models(\n            request, admin_apps, app_spec['models'], ensure_all_models=True\n        )\n        # Only add the panel if at least one model is listed.\n        if app_spec['models']:\n            sorted_apps.append(app_spec)\n\n    used_models = []\n    for app in sorted_apps:\n        used_models += app['models']\n\n    sorted_apps += _remove_app_models(admin_apps, used_models)\n\n    return sorted_apps", "response": "Filter admin_apps to show the ones in DASHBOARD_SORTED_APPS first and remove them from the subsequent listings."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef partition_app_list(app_list, n):\n    num_rows = sum([1 + len(x['models']) for x in app_list]) # + 1 for app title\n    num_rows_per_partition = num_rows / n\n\n    result = [[] for i in range(n)] # start with n empty lists of lists\n    partition = 0\n    count = 0\n\n    for a in app_list:\n        # will the app fit in this column or overflow?\n        c = len(a['models']) + 1 # the +1 is for the app title\n        # if we're not on the last partition, and the models list fits\n        # more on the next partition than this one, start the next partition.\n        if (partition < n - 1) and (count + c/2.0 > num_rows_per_partition):\n            partition += 1\n            count = 0\n        result[partition].append(a)\n        count += c\n\n    return result", "response": "Partition apps into n partitions."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating queryset for layout field.", "response": "def formfield_for_foreignkey(self, db_field, *args, **kwargs):\n        \"\"\"\n        Update queryset for ``layout`` field.\n        \"\"\"\n        formfield = super(FluentLayoutsMixin, self).formfield_for_foreignkey(\n            db_field, *args, **kwargs)\n        if db_field.name == 'layout':\n            formfield.queryset = formfield.queryset.for_model(self.model)\n        return formfield"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting placeholder data from layout.", "response": "def get_placeholder_data(self, request, obj):\n        \"\"\"\n        Get placeholder data from layout.\n        \"\"\"\n        if not obj or not getattr(obj, 'layout', None):\n            data = [PlaceholderData(slot='main', role='m', title='Main')]\n        else:\n            data = obj.layout.get_placeholder_data()\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the source image field for the thumbnail.", "response": "def get_thumbnail_source(self, obj):\n        \"\"\"\n        Obtains the source image field for the thumbnail.\n\n        :param obj: An object with a thumbnail_field defined.\n        :return: Image field for thumbnail or None if not found.\n        \"\"\"\n        if hasattr(self, 'thumbnail_field') and self.thumbnail_field:\n            return resolve(obj, self.thumbnail_field)\n\n        # try get_list_image, from ListableMixin\n        if hasattr(obj, 'get_list_image'):\n            return resolve(obj, \"get_list_image\")\n\n        logger.warning('ThumbnailAdminMixin.thumbnail_field unspecified')\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef preview(self, obj, request=None):\n        source = self.get_thumbnail_source(obj)\n        if source:\n            try:\n                from easy_thumbnails.files import get_thumbnailer\n            except ImportError:\n                logger.warning(\n                    _(\n                        '`easy_thumbnails` is not installed and required for '\n                        'icekit.admin_tools.mixins.ThumbnailAdminMixin'\n                    )\n                )\n                return ''\n            try:\n                thumbnailer = get_thumbnailer(source)\n                thumbnail = thumbnailer.get_thumbnail(self.thumbnail_options)\n                return '<img class=\"thumbnail\" src=\"{0}\" />'.format(\n                    thumbnail.url)\n            except Exception as ex:\n                logger.warning(\n                    _(u'`easy_thumbnails` failed to generate a thumbnail image'\n                      u' for {0}'.format(source)))\n                if self.thumbnail_show_exceptions:\n                    return 'Thumbnail exception: {0}'.format(ex)\n        return ''", "response": "Generate the HTML to display for the image."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef one_instance(function=None, key='', timeout=DEFAULT_ONE_INSTANCE_TIMEOUT):\n    def _dec(run_func):\n        def _caller(*args, **kwargs):\n            ret_value = None\n            have_lock = False\n            # Use Redis AOF for persistent lock.\n            if REDIS_CLIENT.config_get('appendonly').values()[0] == 'no':\n                REDIS_CLIENT.config_set('appendonly', 'yes')\n            lock = REDIS_CLIENT.lock(key, timeout=timeout)\n            try:\n                logger.debug(\n                    '%s: trying to acquire lock (PID %d).'\n                    % (key, os.getpid()))\n                have_lock = lock.acquire(blocking=False)\n                if have_lock:\n                    logger.debug(\n                        '%s: acquired lock (PID %d).' % (key, os.getpid()))\n                    ret_value = run_func(*args, **kwargs)\n                else:\n                    logger.error(\n                        '%s: did not acquire lock (PID %d).'\n                        % (key, os.getpid()))\n            finally:\n                if have_lock:\n                    lock.release()\n                    logger.debug(\n                        '%s: released lock (PID %d).' % (key, os.getpid()))\n            return ret_value\n        return _caller\n    return _dec(function) if function is not None else _dec", "response": "Decorator to enforce only one Celery task execution at a time when multiple Celery task workers are available."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef index_queryset(self, using=None):\n        translation.activate(settings.LANGUAGE_CODE)\n        return self.get_model().objects.filter(status=UrlNode.PUBLISHED).select_related()", "response": "Index current language translation of published objects.\n\n        TODO: Find a way to index all translations of the given model, not just\n        the current site language's translation."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nincludes in context items to be visible on listing page", "response": "def get_context(self, request, page, **kwargs):\n        \"\"\" Include in context items to be visible on listing page \"\"\"\n        context = super(ListingPagePlugin, self).get_context(\n            request, page, **kwargs)\n        context['items_to_list'] = page.get_items_to_list(request)\n        return context"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_view_response(self, request, page, view_func, view_args, view_kwargs):\n        return view_func(request, page, *view_args, **view_kwargs)", "response": "Render the custom view that was exposed by the extra plugin URL patterns."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef has_publish_permission(self, request, obj=None):\n        # If auto-publishing is enabled, no user has \"permission\" to publish\n        # because it happens automatically\n        if is_automatic_publishing_enabled(self.model):\n            return False\n        user_obj = request.user\n        if not user_obj.is_active:\n            return False\n        if user_obj.is_superuser:\n            return True\n        # Normal user with `can_publish` permission can always publish\n        if user_obj.has_perm('%s.can_publish' % self.opts.app_label):\n            return True\n        # Normal user with `can_republish` permission can only publish if the\n        # item is already published.\n        if user_obj.has_perm('%s.can_republish' % self.opts.app_label) and \\\n                obj and getattr(obj, 'has_been_published', False):\n            return True\n        # User does not meet any publishing permisison requirements; reject!\n        return False", "response": "Determines if the user has permission to publish the item."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning True if the user has permission to preview a publishable item.", "response": "def has_preview_permission(self, request, obj=None):\n        \"\"\"\n        Return `True` if the user has permissions to preview a publishable\n        item.\n\n        NOTE: this method does not actually change who can or cannot preview\n        any particular item, just whether to show the preview link. The real\n        dcision is made by a combination of:\n\n        - `PublishingMiddleware` which chooses who can view draft content\n        - the view code for a particular item, which may or may not render\n          draft content for a specific user.\n\n        :param request: Django request object.\n        :param obj: The object the user would preview, if permitted.\n        :return: Boolean.\n        \"\"\"\n        # User who can publish always has preview permission.\n        if self.has_publish_permission(request, obj=obj):\n            return True\n        user_obj = request.user\n        if not user_obj.is_active:\n            return False\n        if user_obj.is_staff:\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef publishing_column(self, obj):\n        # TODO Hack to convert polymorphic objects to real instances\n        if hasattr(obj, 'get_real_instance'):\n            obj = obj.get_real_instance()\n\n        try:\n            object_url = obj.get_absolute_url()\n        except (NoReverseMatch, AttributeError):\n            object_url = ''\n\n        template_name = 'admin/publishing/_change_list_publishing_column.html'\n        t = loader.get_template(template_name)\n        c = Context({\n            'object': obj,\n            'object_url': object_url,\n            'has_publish_permission':\n                self.has_publish_permission(self.request, obj),\n            'has_preview_permission':\n                self.has_preview_permission(self.request, obj),\n        })\n        try:\n            if isinstance(obj, PublishingModel):\n                c['publish_url'] = reverse(\n                    self.publish_reverse(type(obj)), args=(obj.pk, ))\n                c['unpublish_url'] = reverse(\n                    self.unpublish_reverse(type(obj)), args=(obj.pk, ))\n        except NoReverseMatch:\n            pass\n        return t.render(c)", "response": "Render the publishing - related status icons and view links for display in the admin."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_first_available_template(self, template_name_list):\n        if isinstance(template_name_list, six.string_types):\n            return template_name_list\n        else:\n            # Take advantage of fluent_pages' internal implementation\n            return _select_template_name(template_name_list)", "response": "Given a list of template names find the first available template in the list."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_queryset(self, request):\n        # TODO Can we remove this hack?\n        self.request = request\n\n        # Obtain the full queryset defined on the registered model.\n        qs = self.model.objects\n\n        # Determine if a specific language should be used and filter by it if\n        # required.\n        try:\n            qs_language = self.get_queryset_language(request)\n            if qs_language:\n                qs = qs.language(qs_language)\n        except AttributeError:  # this isn't translatable\n            pass\n\n        # Use all draft object versions in the admin.\n        qs = self.publishing_admin_filter_for_drafts(qs)\n\n        # If ordering has been specified in the admin definition order by it.\n        ordering = getattr(self, 'ordering', None) or ()\n        if ordering:\n            qs = qs.order_by(*ordering)\n\n        # Return the filtered queryset.\n        return qs", "response": "Returns the full queryset for the administration list page."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef save_related(self, request, form, *args, **kwargs):\n        result = super(PublishingAdmin, self) \\\n            .save_related(request, form, *args, **kwargs)\n        # Send signal that draft has been saved and all relationships created\n        if form.instance:\n            publishing_signals.publishing_post_save_related.send(\n                sender=type(self), instance=form.instance)\n        return result", "response": "Override save_related to send the signal publishing_post_save_related when a draft copy is saved and all its relationships have also been created."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef render_change_form(self, request, context, add=False, change=False,\n                           form_url='', obj=None):\n        \"\"\"\n        Provides the context and rendering for the admin change form.\n\n        :param request: Django request object.\n        :param context: The context dictionary to be passed to the template.\n        :param add: Should the add form be rendered? Boolean input.\n        :param change: Should the change for be rendered? Boolean input.\n        :param form_url: The URL to use for the form submit action.\n        :param obj: An object the render change form is for.\n        \"\"\"\n        obj = context.get('original', None)\n        if obj:\n            context['object'] = obj\n            context['has_been_published'] = obj.has_been_published\n            context['is_dirty'] = obj.is_dirty\n            context['has_preview_permission'] = \\\n                self.has_preview_permission(request, obj)\n\n            if not self.has_publish_permission(request, obj):\n                context['has_publish_permission'] = False\n            else:\n                context['has_publish_permission'] = True\n\n                try:\n                    object_url = obj.get_absolute_url()\n                except (NoReverseMatch, AttributeError):\n                    object_url = ''\n\n                # If the user has publishing permission and if there are\n                # changes which are to be published show the publish button\n                # with relevant URL.\n                publish_btn = None\n                if obj.is_dirty:\n                    publish_btn = reverse(\n                        self.publish_reverse(type(obj)), args=(obj.pk, ))\n\n                # If the user has publishing permission, a draft object and\n                # a published object show the unpublish button with relevant\n                # URL.\n                unpublish_btn = None\n                if obj.is_draft and obj.publishing_linked:\n                    unpublish_btn = reverse(\n                        self.unpublish_reverse(type(obj)), args=(obj.pk, ))\n\n                # If the user has publishing permission, the object has draft\n                # changes and a published version show a revert button to\n                # change back to the published information.\n                revert_btn = None\n                if obj.is_dirty and obj.publishing_linked:\n                    revert_btn = reverse(self.revert_reverse, args=(obj.pk, ))\n\n                context.update({\n                    'object_url': object_url,\n                    'publish_btn': publish_btn,\n                    'unpublish_btn': unpublish_btn,\n                    'revert_btn': revert_btn,\n                })\n\n        # Make the original non-publishing template available to the publishing\n        # change form template, so it knows what base it extends.\n        # The `original_change_form_template` context variable does the same\n        # job for reversion's versioning change form template.\n        context.update({\n            'non_publishing_change_form_template':\n                self.non_publishing_change_form_template,\n            'original_change_form_template':\n                self.non_publishing_change_form_template,\n        })\n\n        # Hook to permit subclasses to modify context in change form\n        try:\n            self.update_context_for_render_change_form(context, obj)\n        except AttributeError:\n            pass\n\n        # Override this admin's change form template to point to the publishing\n        # admin page template, but only for long enough to render the change\n        # form.\n        if hasattr(type(self).change_form_template, '__get__') \\\n                and isinstance(self.change_form_template, (list, tuple)):\n            # Special handling for class that provide multiple templates via,\n            # a `change_form_template` get-only property instead of the usual\n            # plain class attribute.  In this case, find the best available\n            # template preferring publishing-specific templates and apply it\n            # via a context variable that should be respected by Fluent's base\n            # templates, which we are most likely using at this point.\n            opts = self.model._meta\n            app_label = opts.app_label\n            extra_change_form_templates = [\n                \"admin/%s/%s/publishing_change_form.html\" % (\n                    app_label, opts.model_name),\n                \"admin/%s/publishing_change_form.html\" % app_label,\n                \"admin/publishing/publishing_change_form.html\"\n            ]\n            context['default_change_form_template'] = \\\n                self.find_first_available_template(\n                    extra_change_form_templates + self.change_form_template)\n\n            # Directly overriding the `change_form_template` attr here is\n            # particularly messy since it can be a property getter, not\n            # a normal attr, and any normal way of overriding or updating the\n            # value will not work.\n            try:\n                self._change_form_template_getter = \\\n                    type(self).change_form_template.__get__\n                type(self).change_form_template = \\\n                    context['default_change_form_template']\n                has_change_form_attr_getter = True\n            except AttributeError:\n                # AttributeError presumably from `.__get__` hack above, in\n                # which case we don't need to worry about getters and can just\n                # do things the usual way\n                self.change_form_template = \\\n                    context['default_change_form_template']\n                has_change_form_attr_getter = False\n        else:\n            self.change_form_template = \\\n                \"admin/publishing/publishing_change_form.html\"\n            has_change_form_attr_getter = False\n\n        response = super(PublishingAdmin, self).render_change_form(\n            request, context,\n            add=add, change=change, form_url=form_url, obj=obj)\n\n        # Reset change form template\n        if has_change_form_attr_getter:\n            type(self).change_form_template = \\\n                self._change_form_template_getter\n        else:\n            self.change_form_template = \\\n                self.non_publishing_change_form_template\n\n        return response", "response": "Renders the admin change form for the given object."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nshows only DRAFT Fluent page items in admin.", "response": "def get_queryset(self, request):\n        \"\"\"\n        Show only DRAFT Fluent page items in admin.\n\n        NOTE: We rely on the `UrlNode.status` to recognise DRAFT versus\n        PUBLISHED objects, since there the top-level `UrlNode` model and\n        queryset don't know about ICEKit publishing.\n        \"\"\"\n        self.request = request\n\n        qs = super(PublishingFluentPagesParentAdminMixin, self) \\\n            .get_queryset(request)\n        qs = qs.filter(status=UrlNode.DRAFT)\n        return qs"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind and return an existing page matching the given titles hierarchy.", "response": "def find_existing_page(self, titles_hierarchy):\n        \"\"\"\n        Find and return existing page matching the given titles hierarchy\n        \"\"\"\n        # Step backwards through import doc's titles hierarchy with a parent\n        # count which is the offset from the current level to each ancestor\n        titles_filters = {'publishing_is_draft': True}\n        for parent_count, ancestor_title \\\n                in enumerate(titles_hierarchy[::-1]):\n            # Convert import doc's ancestor title and parent count into\n            # a filter query of the form:\n            #    translations__title=<entry (ancestor 0) title>\n            #    parent__translations__title=<ancestor 1 title>\n            #    parent__parent__translations__title=<ancestor 2 title>\n            #    parent__parent__parent__translations__title=<ancestor 3 title>\n            parent_path = '__'.join(['parent'] * parent_count)\n            filter_name = '%s%stranslations__title' % (\n                parent_path, parent_path and '__' or '')\n            titles_filters[filter_name] = ancestor_title\n        # Return a corresponding page if one exists\n        return self.page_model.objects \\\n            .filter(**titles_filters) \\\n            .first()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nresolve obj. attr to a value calling it as a function if necessary.", "response": "def resolve(obj, attr, fallback=None):\n    \"\"\"\n    Resolves obj.attr to a value, calling it as a function if necessary.\n    :param obj:\n    :param attr: a string name of a property or function\n    :param fallback: the value to return if none can be resolved\n    :return: the result of the attribute, or fallback if object/attr not found\n    \"\"\"\n    if obj is None:\n        return fallback\n    value = getattr(obj, attr, fallback)\n    if callable(value):\n        return value()\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef first_of(obj, *attrs):\n    for attr in attrs:\n        r = resolve(obj, attr)\n        if r:\n            return r", "response": "returns the first truthy attribute of obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the values from the sqs facet counts result.", "response": "def set_values_from_sqs_facet_counts(self, sqs_facet_counts):\n        \"\"\"\n        Use the sqs.facet_counts() result to set up values and counts.\n        \"\"\"\n        self._values = []\n\n        # inject an 'All results' value (if there are other values):\n        if sqs_facet_counts.has_key('fields'):\n            if not self.select_many and sqs_facet_counts['fields'][self.field_name]:\n                self._values += [\n                    FacetValue(\n                        facet=self,\n                        value=None,\n                        label=\"All results\",\n                        is_all_results=True,\n                    )\n                ]\n\n            # inject the values from the sqs\n            self._values += [\n                FacetValue(\n                    facet=self,\n                    value=value,\n                    label=value,\n                    count=count\n                )\n                for value, count in sqs_facet_counts['fields'][self.field_name]\n            ]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef apply_request_and_page_to_values(self, request, page=None):\n        value_is_set = False\n        for value in self._values:\n            value.apply_request_and_page(request, page)", "response": "Apply request and page to all the active values."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nnarrow the sqs query to include only the relevant ones.", "response": "def narrow_sqs(self, sqs):\n        \"\"\"\n        TODO: Currently this is an AND conjunction. It should vary depending\n        on the value of self.select_many.\n        \"\"\"\n        if self.select_many:\n            sq = None\n            for value in self.get_applicable_values():\n                q = SQ(**{self.field_name: sqs.query.clean(value.value)})\n                if sq:\n                    sq = sq | q\n                else:\n                    sq = q\n            if sq:\n                sqs = sqs.narrow(sq)\n        else:\n            for value in self.get_applicable_values():\n                sqs = sqs.narrow(\n                    u'%s:\"%s\"' % (self.field_name, sqs.query.clean(value.value))\n                )\n\n        return sqs"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns selected values that will affect the search result", "response": "def get_applicable_values(self):\n        \"\"\"Return selected values that will affect the search result\"\"\"\n        return [v for v in self._values if v.is_active and not v.is_all_results]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the label of the first value in the list", "response": "def get_value(self):\n        \"\"\"Returns the label of the first value\"\"\"\n        try:\n            return self.get_applicable_values()[0]\n        except IndexError:\n            if not self.select_many and self.get_values():\n                return self.get_values()[0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_default(self):\n        if not self.get_applicable_values():\n            return True\n\n        if self.get_value().is_default:\n            return True\n\n        return False", "response": "Return True if the active value is the default"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef admin_url(inst):\n    if inst:\n        t = Template(\"\"\"{% load admin_urls %}{% url opts|admin_urlname:'change' inst.pk %}\"\"\")\n        return t.render(Context({ 'inst': inst, 'opts': inst._meta}))\n    return \"\"", "response": "Returns the admin URL for the instance."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a complete admin link for the given instance.", "response": "def admin_link(inst, attr_string=\"target=\\\"_blank\\\"\", inner_html=\"\"):\n    \"\"\"\n    :param inst: An Model Instance\n    :param attr_string: A string of attributes to be added to the <a> Tag.\n    :param inner_html: Override html inside the <a> Tag.\n    :return: a complete admin link for the instance. Permissions aren't checked.\n    \"\"\"\n\n    # TODO: call with new window command, that adds new window icon and attr\n    if inst:\n        t = Template(\"\"\"{% load admin_urls %}<a title=\"Edit '{{ inst }}'\" href=\"{% url opts|admin_urlname:'change' inst.pk %}\" {{ attr_string|safe }}>{% if inner_html %}{{ inner_html|safe }}{% else %}Edit <em>{{ inst }}</em>{% endif %}</a>\"\"\")\n        return mark_safe(t.render(Context({ 'inst': inst, 'opts': inst._meta, 'attr_string': attr_string, 'inner_html': inner_html})))\n    return \"\""}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate the time modified before saving a publishable object.", "response": "def publishing_set_update_time(sender, instance, **kwargs):\n    \"\"\" Update the time modified before saving a publishable object. \"\"\"\n    if hasattr(instance, 'publishing_linked'):\n        # Hack to avoid updating `publishing_modified_at` field when a draft\n        # publishable item is saved as part of a `publish` operation. This\n        # ensures that the `publishing_published_at` timestamp is later than\n        # the `publishing_modified_at` timestamp when we publish, which is\n        # vital for us to correctly detect whether a draft is \"dirty\".\n        if getattr(instance, '_skip_update_publishing_modified_at', False):\n            # Reset flag, in case instance is re-used (e.g. in tests)\n            instance._skip_update_publishing_modified_at = False\n            return\n        instance.publishing_modified_at = timezone.now()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef handle_publishable_m2m_changed(\n        sender, instance, action, reverse, model, pk_set, **kwargs):\n    \"\"\"\n    Cache related published objects in `pre_clear` so they can be restored in\n    `post_clear`.\n    \"\"\"\n    # Do nothing if the target model is not publishable.\n    if not issubclass(model, PublishingModel):\n        return\n    # Get the right `ManyRelatedManager`. Iterate M2Ms and compare `sender`\n    # (the through model), in case there are multiple M2Ms to the same model.\n    if reverse:\n        for rel_obj in instance._meta.get_all_related_many_to_many_objects():\n            if rel_obj.field.rel.through == sender:\n                m2m = getattr(instance, rel_obj.get_accessor_name())\n                break\n    else:\n        for field in instance._meta.many_to_many:\n            if field.rel.through == sender:\n                m2m = getattr(instance, field.attname)\n                break\n    # Cache published PKs on the instance.\n    if action == 'pre_clear':\n        instance._published_m2m_cache = set(\n            m2m.filter(publishing_is_draft=False).values_list('pk', flat=True))\n    # Add published PKs from the cache.\n    if action == 'post_clear':\n        m2m.add(*instance._published_m2m_cache)\n        del instance._published_m2m_cache", "response": "Handle a change in the PublishingModel."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsynchronize MPTT tree structure field changes from draft to published copy.", "response": "def sync_mptt_tree_fields_from_draft_to_published_post_save(\n        sender, instance, **kwargs):\n    \"\"\"\n    Post save trigger to immediately sync MPTT tree structure field changes\n    made to draft copies to their corresponding published copy.\n    \"\"\"\n    mptt_opts = getattr(instance, '_mptt_meta', None)\n    published_copy = getattr(instance, 'publishing_linked', None)\n    if mptt_opts and published_copy:\n        sync_mptt_tree_fields_from_draft_to_published(instance)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sync_mptt_tree_fields_from_draft_to_published(\n        draft_copy, dry_run=False, force_update_cached_urls=False):\n    \"\"\"\n    Sync tree structure changes from a draft publishable object to its\n    published copy, and updates the published copy's Fluent cached URLs when\n    necessary. Or simulates doing this if ``dry_run`` is ``True``.\n\n    Syncs both actual structural changes (i.e. different parent) and MPTT's\n    fields which are a cached representation (and may or may not be correct).\n    \"\"\"\n    mptt_opts = getattr(draft_copy, '_mptt_meta', None)\n    published_copy = getattr(draft_copy, 'publishing_linked', None)\n    if not mptt_opts or not published_copy:\n        return {}\n    # Identify changed values and prepare dict of changes to apply to DB\n    parent_changed = draft_copy.parent != published_copy.parent\n    update_kwargs = {\n        mptt_opts.parent_attr: draft_copy._mpttfield('parent'),\n        mptt_opts.tree_id_attr: draft_copy._mpttfield('tree_id'),\n        mptt_opts.left_attr: draft_copy._mpttfield('left'),\n        mptt_opts.right_attr: draft_copy._mpttfield('right'),\n        mptt_opts.level_attr: draft_copy._mpttfield('level'),\n    }\n    # Strip out DB update entries for unchanged or invalid tree fields\n    update_kwargs = dict(\n        (field, value) for field, value in update_kwargs.items()\n        if getattr(draft_copy, field) != getattr(published_copy, field)\n        # Only parent may be None, never set tree_id/left/right/level to None\n        and not (field != 'parent' and value is None)\n    )\n\n    change_report = []\n    for field, new_value in update_kwargs.items():\n        old_value = getattr(published_copy, field)\n        change_report.append((draft_copy, field, old_value, new_value))\n\n    # Forcibly update MPTT field values via UPDATE commands instead of normal\n    # model attr changes, which MPTT ignores when you `save`\n    if update_kwargs and not dry_run:\n        type(published_copy).objects.filter(pk=published_copy.pk).update(\n            **update_kwargs)\n\n    # If real tree structure (not just MPTT fields) has changed we must\n    # regenerate the cached URLs for published copy translations.\n    if parent_changed or force_update_cached_urls:\n        # Make our local published obj aware of DB change made by `update`\n        published_copy.parent = draft_copy.parent\n        # Regenerate the cached URLs for published copy translations.\n        change_report += \\\n            update_fluent_cached_urls(published_copy, dry_run=dry_run)\n\n    return change_report", "response": "Syncs MPTT s tree structure fields from a draft publishable object to its published copy and updates Fluent cached URLs when necessary."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating the cached URLs for an item.", "response": "def update_fluent_cached_urls(item, dry_run=False):\n    \"\"\"\n    Regenerate the cached URLs for an item's translations. This is a fiddly\n    business: we use \"hidden\" methods instead of the public ones to avoid\n    unnecessary and unwanted slug changes to ensure uniqueness, the logic for\n    which doesn't work with our publishing.\n    \"\"\"\n    change_report = []\n    if hasattr(item, 'translations'):\n        for translation in item.translations.all():\n            old_url = translation._cached_url\n            item._update_cached_url(translation)\n            change_report.append(\n                (translation, '_cached_url', old_url, translation._cached_url))\n            if not dry_run:\n                translation.save()\n        if not dry_run:\n            item._expire_url_caches()\n        # Also process all the item's children, in case changes to this item\n        # affect the URL that should be cached for the children. We process\n        # only draft-or-published children, according to the item's status.\n        if item.is_draft:\n            children = [child for child in item.children.all()\n                        if child.is_draft]\n        else:\n            children = [child for child in item.get_draft().children.all()\n                        if child.is_published]\n        for child in children:\n            update_fluent_cached_urls(child, dry_run=dry_run)\n\n    return change_report"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_can_publish_and_can_republish_permissions(sender, **kwargs):\n    for model in sender.get_models():\n        if not issubclass(model, PublishingModel):\n            continue\n        content_type = ContentType.objects.get_for_model(model)\n        permission, created = Permission.objects.get_or_create(\n            content_type=content_type, codename='can_publish',\n            defaults=dict(name='Can Publish %s' % model.__name__))\n        permission, created = Permission.objects.get_or_create(\n            content_type=content_type, codename='can_republish',\n            defaults=dict(name='Can Republish %s' % model.__name__))", "response": "Add can_publish and ca_nrepublish permissions for each publishable\n            model in the system."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef maybe_automatically_publish_drafts_on_save(sender, instance, **kwargs):\n    # Skip processing if auto-publishing is not enabled\n    if not is_automatic_publishing_enabled(sender):\n        return\n    # Skip missing or unpublishable instances\n    if not instance or not hasattr(instance, 'publishing_linked'):\n        return\n    # Ignore saves of published copies\n    if instance.is_published:\n        return\n    # Ignore saves of already-published draft copies\n    if not instance.is_dirty:\n        return\n    # Immediately publish saved draft copy\n    instance.publish()", "response": "If automatic publishing is enabled immediately publish a draft copy after the instance has been saved."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_within_publication_dates(obj, timestamp=None):\n        if timestamp is None:\n            timestamp = timezone.now()\n        start_date_ok = not obj.publication_date \\\n            or obj.publication_date <= timestamp\n        end_date_ok = not obj.publication_end_date \\\n            or obj.publication_end_date > timestamp\n        return start_date_ok and end_date_ok", "response": "Return True if the given object is within any publication start and end dates."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef has_been_published(self):\n        if self.is_published:\n            return True\n        elif self.is_draft:\n            return self.publishing_linked_id is not None\n        raise ValueError(  # pragma: no cover\n            \"Publishable object %r is neither draft nor published\" % self)", "response": "Return True if the item is either published itself or a draft object that has an associated published copy."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_draft(self):\n        if self.is_draft:\n            return self\n        elif self.is_published:\n            draft = self.publishing_draft\n            # Previously the reverse relation could be `DraftItemBoobyTrapped`\n            # in some cases. This should be fixed by extra monkey-patching of\n            # the `publishing_draft` field in icekit.publishing.apps, but we\n            # will leave this extra sanity check here just in case.\n            if hasattr(draft, 'get_draft_payload'):\n                draft = draft.get_draft_payload()\n            return draft\n        raise ValueError(  # pragma: no cover\n            \"Publishable object %r is neither draft nor published\" % self)", "response": "Returns self if this object is a draft otherwise return the draft\n            copy of a published item."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn self is this object is published otherwise return None.", "response": "def get_published(self):\n        \"\"\"\n        Return self is this object is published, otherwise return the\n        published copy of a draft item. If this object is a draft with\n        no published copy it will return ``None``.\n        \"\"\"\n        if self.is_published:\n            return self\n        elif self.is_draft:\n            return self.publishing_linked\n        raise ValueError(  # pragma: no cover\n            \"Publishable object %r is neither draft nor published\" % self)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_published_or_draft(self):\n        if self.is_published:\n            return self\n        elif self.publishing_linked_id:\n            return self.publishing_linked\n        if is_draft_request_context():\n            return self.get_draft()\n        # There is no public version, and there is no privilege to view the\n        # draft version\n        return None", "response": "Returns the published item or the draft version if it exists."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\npublishes the object. The decorator `assert_draft` makes sure that you cannot publish a published object. :param self: The object to tbe published. :return: The published object.", "response": "def publish(self):\n        \"\"\"\n        Publishes the object.\n\n        The decorator `assert_draft` makes sure that you cannot publish\n        a published object.\n        :param self: The object to tbe published.\n        :return: The published object.\n        \"\"\"\n        if self.is_draft:\n            # If the object has previously been linked then patch the\n            # placeholder data and remove the previously linked object.\n            # Otherwise set the published date.\n            if self.publishing_linked:\n                self.patch_placeholders()\n                # Unlink draft and published copies then delete published.\n                # NOTE: This indirect dance is necessary to avoid\n                # triggering unwanted MPTT tree structure updates via\n                # `save`.\n                type(self.publishing_linked).objects \\\n                    .filter(pk=self.publishing_linked.pk) \\\n                    .delete()  # Instead of self.publishing_linked.delete()\n            else:\n                self.publishing_published_at = timezone.now()\n\n            # Create a new object copying all fields.\n            publish_obj = deepcopy(self)\n\n            # If any fields are defined not to copy set them to None.\n            for fld in self.publishing_publish_empty_fields + (\n                'urlnode_ptr_id', 'publishing_linked_id'\n            ):\n                setattr(publish_obj, fld, None)\n\n            # Set the state of publication to published on the object.\n            publish_obj.publishing_is_draft = False\n\n            # Update Fluent's publishing status field mechanism to correspond\n            # to our own notion of publication, to help use work together more\n            # easily with Fluent Pages.\n            if isinstance(self, UrlNode):\n                self.status = UrlNode.DRAFT\n                publish_obj.status = UrlNode.PUBLISHED\n\n            # Set the date the object should be published at.\n            publish_obj.publishing_published_at = self.publishing_published_at\n\n            # Perform per-model preparation before saving published copy\n            publish_obj.publishing_prepare_published_copy(self)\n\n            # Save the new published object as a separate instance to self.\n            publish_obj.save()\n            # Sanity-check that we successfully saved the published copy\n            if not publish_obj.pk:  # pragma: no cover\n                raise PublishingException(\"Failed to save published copy\")\n\n            # As it is a new object we need to clone each of the\n            # translatable fields, placeholders and required relations.\n            self.clone_parler_translations(publish_obj)\n            self.clone_fluent_placeholders_and_content_items(publish_obj)\n            self.clone_fluent_contentitems_m2m_relationships(publish_obj)\n\n            # Extra relationship-cloning smarts\n            publish_obj.publishing_clone_relations(self)\n\n            # Link the published object to the draft object.\n            self.publishing_linked = publish_obj\n\n            # Flag draft instance when it is being updated as part of a\n            # publish action, for use in `publishing_set_update_time`\n            self._skip_update_publishing_modified_at = True\n\n            # Signal the pre-save hook for publication, save then signal\n            # the post publish hook.\n            publishing_signals.publishing_publish_pre_save_draft.send(\n                sender=type(self), instance=self)\n\n            # Save the draft and its new relationship with the published copy\n            publishing_signals.publishing_publish_save_draft.send(\n                sender=type(self), instance=self)\n\n            publishing_signals.publishing_post_publish.send(\n                sender=type(self), instance=self)\n            return publish_obj"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef unpublish(self):\n        if self.is_draft and self.publishing_linked:\n            publishing_signals.publishing_pre_unpublish.send(\n                sender=type(self), instance=self)\n            # Unlink draft and published copies then delete published.\n            # NOTE: This indirect dance is necessary to avoid triggering\n            # unwanted MPTT tree structure updates via `delete`.\n            type(self.publishing_linked).objects \\\n                .filter(pk=self.publishing_linked.pk) \\\n                .delete()  # Instead of self.publishing_linked.delete()\n            # NOTE: We update and save the object *after* deleting the\n            # published version, in case the `save()` method does some\n            # validation that breaks when unlinked published objects exist.\n            self.publishing_linked = None\n            self.publishing_published_at = None\n\n            # Save the draft to remove its relationship with the published copy\n            publishing_signals.publishing_unpublish_save_draft.send(\n                sender=type(self), instance=self)\n\n            publishing_signals.publishing_post_unpublish.send(\n                sender=type(self), instance=self)", "response": "Un - publish the current object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef publishing_prepare_published_copy(self, draft_obj):\n        # We call super here, somewhat perversely, to ensure this method will\n        # be called on publishable subclasses if implemented there.\n        mysuper = super(PublishingModel, self)\n        if hasattr(mysuper, 'publishing_prepare_published_copy'):\n            mysuper.publishing_prepare_published_copy(draft_obj)", "response": "Prepare published copy of draft prior to saving it"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncloning forward and reverse M2Ms. This code is difficult to follow because the logic it applies is confusing, but here is a summary that might help: - when a draft object is published, the \"current\" and definitive relationships are cloned to the published copy. The definitive relationships are the draft-to-draft ones, as set in the admin. - a \"related draft\" is the draft object at the other side of a draft-to-draft M2M relationship - if a related draft also has a published copy, a draft-to- published relationship is added to that published copy. This makes our newly-published item also \"published\" from the reverse direction - if our draft object has a related published copy without a correponding related draft -- that is, a draft-to-published relation without a definitive draft-to-draft relation -- then we remove that relation as it is no longer \"current\". This makes our newly-published item \"unpublished\" from the reverse direction when an admin removes the underlying relationship. An example case: - We have Event \"E\" (unpublished) and Program \"P\" (published) - We add an M2M relationship from E to P. Until the relationship change is published it only affects drafts. Relationships are: E draft <-> P draft - We publish E, applying the relationship to published copies on both sides: E draft <-> P draft E published <-> P draft P published <-> E draft - We remove the M2M relationship between E and P (We could do this from either side: remove E from P; or, remove P from E). The draft-to-draft relation is removed but published copy relationships are not affected: E published <-> P draft P published <-> E draft - We publish P (or E) to apply the relationshp removal to published copies on both sides. No relationships remain. To handle M2M relationships in general we iterate over entries in the through-table relationship table to clone these entries, or remove them, as appropriate. By processing the M2M relationships in this way we can handle both kinds of M2M relationship: - standard M2M relationships with no explicit through table defined (these get an auto-generated through table) which are easier to handle because we can add/remove items with the relationship's queryset directly - M2M relationships with an explicit through table defined, which are more difficult to handle because we must use the through model's manager to add/remove relationships. See unit tests in ``TestPublishingOfM2MRelationships``.", "response": "def publishing_clone_relations(self, src_obj):\n        \"\"\"\n        Clone forward and reverse M2Ms.\n\n        This code is difficult to follow because the logic it applies is\n        confusing, but here is a summary that might help:\n\n            - when a draft object is published, the \"current\" and definitive\n              relationships are cloned to the published copy. The definitive\n              relationships are the draft-to-draft ones, as set in the admin.\n            - a \"related draft\" is the draft object at the other side of\n              a draft-to-draft M2M relationship\n            - if a related draft also has a published copy, a draft-to-\n              published relationship is added to that published copy. This\n              makes our newly-published item also \"published\" from the reverse\n              direction\n            - if our draft object has a related published copy without a\n              correponding related draft -- that is, a draft-to-published\n              relation without a definitive draft-to-draft relation -- then\n              we remove that relation as it is no longer \"current\". This\n              makes our newly-published item \"unpublished\" from the reverse\n              direction when an admin removes the underlying relationship.\n\n        An example case:\n\n            - We have Event \"E\" (unpublished) and Program \"P\" (published)\n            - We add an M2M relationship from E to P. Until the relationship\n              change is published it only affects drafts. Relationships are:\n                  E draft <-> P draft\n            - We publish E, applying the relationship to published copies on\n              both sides:\n                  E draft <-> P draft\n                  E published <-> P draft\n                  P published <-> E draft\n            - We remove the M2M relationship between E and P (We could do this\n              from either side: remove E from P; or, remove P from E). The\n              draft-to-draft relation is removed but published copy\n              relationships are not affected:\n                  E published <-> P draft\n                  P published <-> E draft\n            - We publish P (or E) to apply the relationshp removal to\n              published copies on both sides. No relationships remain.\n\n        To handle M2M relationships in general we iterate over entries in the\n        through-table relationship table to clone these entries, or remove\n        them, as appropriate. By processing the M2M relationships in this way\n        we can handle both kinds of M2M relationship:\n\n            - standard M2M relationships with no explicit through table defined\n              (these get an auto-generated through table) which are easier to\n              handle because we can add/remove items with the relationship's\n              queryset directly\n            - M2M relationships with an explicit through table defined, which\n              are more difficult to handle because we must use the through\n              model's manager to add/remove relationships.\n\n        See unit tests in ``TestPublishingOfM2MRelationships``.\n        \"\"\"\n\n        def clone_through_model_relationship(\n                manager, through_entry, dst_obj, rel_obj\n        ):\n            dst_obj_filter = build_filter_for_through_field(\n                manager, manager.source_field_name, dst_obj)\n            rel_obj_filter = build_filter_for_through_field(\n                manager, manager.target_field_name, rel_obj)\n            if manager.through.objects \\\n                    .filter(**dst_obj_filter) \\\n                    .filter(**rel_obj_filter) \\\n                    .exists():\n                return\n            through_entry.pk = None\n            setattr(through_entry, manager.source_field_name, dst_obj)\n            setattr(through_entry, manager.target_field_name, rel_obj)\n            through_entry.save()\n\n        def delete_through_model_relationship(manager, src_obj, dst_obj):\n            src_obj_filter = build_filter_for_through_field(\n                manager, manager.source_field_name, src_obj)\n            dst_obj_filter = build_filter_for_through_field(\n                manager, manager.target_field_name, dst_obj)\n            manager.through.objects \\\n                .filter(**src_obj_filter) \\\n                .filter(**dst_obj_filter) \\\n                .delete()\n\n        def build_filter_for_through_field(manager, field_name, obj):\n            # If the field is a `GenericForeignKey` we need to build\n            # a compatible filter dict against the field target's content type\n            # and PK...\n            field = getattr(manager.through, field_name)\n            if isinstance(field, GenericForeignKey):\n                field_filter = {\n                    getattr(field, 'fk_field'): obj.pk,\n                    getattr(field, 'ct_field'):\n                        ContentType.objects.get_for_model(obj)\n                }\n            # ...otherwise standard FK fields can be handled simply\n            else:\n                field_filter = {field_name: obj}\n            return field_filter\n\n        def clone(src_manager):\n            if (\n                not hasattr(src_manager, 'source_field_name') or\n                not hasattr(src_manager, 'target_field_name')\n            ):\n                raise PublishingException(\n                    \"Publishing requires many-to-many managers to have\"\n                    \" 'source_field_name' and 'target_field_name' attributes\"\n                    \" with the source and target field names that relate the\"\n                    \" through model to the ends of the M2M relationship.\"\n                    \" If a non-standard manager does not provide these\"\n                    \" attributes you must add them.\"\n                )\n\n            src_obj_source_field_filter = build_filter_for_through_field(\n                src_manager, src_manager.source_field_name, src_obj)\n            through_qs = src_manager.through.objects \\\n                .filter(**src_obj_source_field_filter)\n            published_rel_objs_maybe_obsolete = []\n            current_draft_rel_pks = set()\n            for through_entry in through_qs:\n                rel_obj = getattr(through_entry, src_manager.target_field_name)\n                # If the object referenced by the M2M is publishable we only\n                # clone the relationship if it is to a draft copy, not if it is\n                # to a published copy. If it is not a publishable object at\n                # all then we always clone the relationship (True by default).\n                if getattr(rel_obj, 'publishing_is_draft', True):\n                    clone_through_model_relationship(\n                        src_manager, through_entry, self, rel_obj)\n                    # If the related draft object also has a published copy,\n                    # we need to make sure the published copy also knows about\n                    # this newly-published draft.\n                    try:\n                        # Get published copy for related object, if any\n                        rel_obj_published = rel_obj.publishing_linked\n                    except AttributeError:\n                        pass  # Related item has no published copy\n                    else:\n                        if rel_obj_published:\n                            clone_through_model_relationship(\n                                src_manager, through_entry,\n                                src_obj, rel_obj_published)\n                    # Track IDs of related draft copies, so we can tell later\n                    # whether relationshps with published copies are obsolete\n                    current_draft_rel_pks.add(rel_obj.pk)\n                else:\n                    # Track related published copies, in case they have\n                    # become obsolete\n                    published_rel_objs_maybe_obsolete.append(rel_obj)\n            # If related published copies have no corresponding related\n            # draft after all the previous processing, the relationship is\n            # obsolete and must be removed.\n            for published_rel_obj in published_rel_objs_maybe_obsolete:\n                draft = published_rel_obj.get_draft()\n                if not draft or draft.pk not in current_draft_rel_pks:\n                    delete_through_model_relationship(\n                        src_manager, src_obj, published_rel_obj)\n\n        # Track the relationship through-tables we have processed to avoid\n        # processing the same relationships in both forward and reverse\n        # directions, which could otherwise happen in unusual cases like\n        # for SFMOMA event M2M inter-relationships which are explicitly\n        # defined both ways as a hack to expose form widgets.\n        seen_rel_through_tables = set()\n\n        # Forward.\n        for field in src_obj._meta.many_to_many:\n            src_manager = getattr(src_obj, field.name)\n            clone(src_manager)\n            seen_rel_through_tables.add(field.rel.through)\n\n        # Reverse.\n        for field in src_obj._meta.get_all_related_many_to_many_objects():\n            # Skip reverse relationship we have already seen\n            if field.field.rel.through in seen_rel_through_tables:\n                continue\n            field_accessor_name = field.get_accessor_name()\n            # M2M relationships with `self` don't have accessor names\n            if not field_accessor_name:\n                continue\n            src_manager = getattr(src_obj, field_accessor_name)\n            clone(src_manager)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef clone_parler_translations(self, dst_obj):\n        # Find all django-parler translation attributes on model\n        translation_attrs = []\n        if hasattr(self, '_parler_meta'):\n            for parler_meta in self._parler_meta:\n                translation_attrs.append(parler_meta.rel_name)\n        # Clone all django-parler translations via attributes\n        for translation_attr in translation_attrs:\n            # Clear any translations already cloned to published object\n            # before we get here, which seems to happen via deepcopy()\n            # sometimes.\n            setattr(dst_obj, translation_attr, [])\n            # Clone attribute's translations from source to destination\n            for translation in getattr(self, translation_attr).all():\n                translation.pk = None\n                translation.master = dst_obj\n                translation.save()", "response": "Clone each of the translations from an object and relate them to another object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef clone_fluent_placeholders_and_content_items(self, dst_obj):\n        if not self.has_placeholder_relationships():\n            return\n\n        for src_placeholder in Placeholder.objects.parent(self):\n            dst_placeholder = Placeholder.objects.create_for_object(\n                dst_obj,\n                slot=src_placeholder.slot,\n                role=src_placeholder.role,\n                title=src_placeholder.title\n            )\n\n            src_items = src_placeholder.get_content_items()\n            src_items.copy_to_placeholder(dst_placeholder)", "response": "Clones each Placeholder and its ContentItem s."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nclones the fluent content items M2M relationships on related ContentItems and return the new ones.", "response": "def clone_fluent_contentitems_m2m_relationships(self, dst_obj):\n        \"\"\"\n        Find all MTM relationships on related ContentItem's and ensure the\n        published M2M relationships directed back to the draft (src)\n        content items are maintained for the published (dst) page's content\n        items.\n        \"\"\"\n        if not hasattr(self, 'contentitem_set'):\n            return\n        # We must explicitly and reliably order both the src and dst content\n        # items here to ensure that we are processing the same logical item for\n        # the draft and published copies. The default `ContentItem` ordering of\n        # ('placeholder', 'sort_order') is not sufficient because it relies on\n        # the placeholder PK remaining static, whereas we clone placeholders to\n        # the published copy and may sometimes clone them with PKs in a\n        # different order.\n        reliable_ordering = [\n            # Group items by owning placeholder using slot name, not PK\n            'placeholder__slot',\n            # Order items correctly within the placeholder grouping\n            'sort_order'\n        ]\n        for src_ci, dst_ci in zip(\n            self.contentitem_set.order_by(*reliable_ordering),\n            dst_obj.contentitem_set.order_by(*reliable_ordering)\n        ):\n            for field, __ in src_ci._meta.get_m2m_with_model():\n                field_name = field.name\n                src_m2m = getattr(src_ci, field_name)\n                dst_m2m = getattr(dst_ci, field_name)\n\n                # It is safe to just `add` here, rather than match src and\n                # dst listing exactly (i.e. potentially delete or re-order\n                # items) because the destination content items are\n                # re-created on publish thus always have empty M2M rels.\n                dst_m2m.add(*src_m2m.all())"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef handle_noargs(self, **options):\n        is_dry_run = options.get('dry-run', False)\n        mptt_only = options.get('mptt-only', False)\n        slugs = {}\n        overrides = {}\n\n        # MODIFIED\n        # This was modified to filter draft objects only.\n        # ORIGINAL LINE -> `parents = dict(UrlNode.objects.values_list('id', 'parent_id'))`\n        parents = dict(\n            UrlNode.objects.filter(status=UrlNode.DRAFT).values_list('id', 'parent_id')\n        )\n        # END MODIFIED\n\n        self.stdout.write(\"Updated MPTT columns\")\n        if is_dry_run and mptt_only:\n            # Can't really do anything\n            return\n\n        if not is_dry_run:\n            # Fix MPTT first, that is the basis for walking through all nodes.\n\n            # MODIFIED\n            # Original line -> `UrlNode.objects.rebuild()`\n            # The `rebuild` function works on the manager. As we need to filter the queryset first\n            # it does not play nicely. The code for `rebuild` was brought in here and modified to\n            # work with the current context.\n\n            # Get opts from `UrlNode` rather than `self.model`.\n            opts = UrlNode._mptt_meta\n\n            # Add a queryset parameter will draft objects only.\n            qs = UrlNode.objects._mptt_filter(\n                qs=UrlNode.objects.filter(status=UrlNode.DRAFT),\n                parent=None\n            )\n            if opts.order_insertion_by:\n                qs = qs.order_by(*opts.order_insertion_by)\n            pks = qs.values_list('pk', flat=True)\n\n            # Obtain the `rebuild_helper` from `UrlNode.objects` rather than `self`.\n            rebuild_helper = UrlNode.objects._rebuild_helper\n            idx = 0\n            for pk in pks:\n                idx += 1\n                rebuild_helper(pk, 1, idx)\n            # END MODIFIED\n\n            self.stdout.write(\"Updated MPTT columns\")\n            if mptt_only:\n                return\n\n            self.stdout.write(\"Updating cached URLs\")\n            self.stdout.write(\"Page tree nodes:\\n\\n\")\n\n        col_style = u\"| {0:6} | {1:6} | {2:6} | {3}\"\n        header = col_style.format(\"Site\", \"Page\", \"Locale\", \"URL\")\n        sep = '-' * (len(header) + 40)\n        self.stdout.write(sep)\n        self.stdout.write(header)\n        self.stdout.write(sep)\n\n        # MODIFIED\n        # Modified to add the filter for draft objects only.\n        for translation in UrlNode_Translation.objects.filter(\n            master__status=UrlNode.DRAFT\n        ).select_related('master').order_by(\n            'master__parent_site__id', 'master__tree_id', 'master__lft', 'language_code'\n        ):\n            # END MODIFIED\n            slugs.setdefault(translation.language_code, {})[translation.master_id] = translation.slug\n            overrides.setdefault(translation.language_code, {})[translation.master_id] = translation.override_url\n\n            old_url = translation._cached_url\n            try:\n                new_url = self._construct_url(translation.language_code, translation.master_id, parents, slugs, overrides)\n            except KeyError:\n                if is_dry_run:\n                    # When the mptt tree is broken, some URLs can't be correctly generated yet.\n                    self.stderr.write(\"Failed to determine new URL for {0}, please run with --mptt-only first.\".format(old_url))\n                    return\n                raise\n\n            if old_url != new_url:\n                translation._cached_url = new_url\n                if not is_dry_run:\n                    translation.save()\n\n            if old_url != new_url:\n                self.stdout.write(smart_text(u\"{0}  {1} {2}\\n\".format(\n                    col_style.format(translation.master.parent_site_id, translation.master_id, translation.language_code, translation._cached_url),\n                    \"WILL CHANGE from\" if is_dry_run else \"UPDATED from\",\n                    old_url\n                )))\n            else:\n                self.stdout.write(smart_text(col_style.format(\n                    translation.master.parent_site_id, translation.master_id, translation.language_code, translation._cached_url\n                )))", "response": "This function is called when no arguments are passed to the function."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmake django_ct equal to the type of get_model to make polymorphic children show up in results.", "response": "def full_prepare(self, obj):\n        \"\"\"\n        Make django_ct equal to the type of get_model, to make polymorphic\n        children show up in results.\n        \"\"\"\n        prepared_data = super(AbstractLayoutIndex, self).full_prepare(obj)\n        prepared_data['django_ct'] = get_model_ct(self.get_model())\n        return prepared_data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of items nicely with a different string before the final item.", "response": "def grammatical_join(l, initial_joins=\", \", final_join=\" and \"):\n    \"\"\"\n    Display a list of items nicely, with a different string before the final\n    item. Useful for using lists in sentences.\n\n    >>> grammatical_join(['apples', 'pears', 'bananas'])\n    'apples, pears and bananas'\n\n    >>> grammatical_join(['apples', 'pears', 'bananas'], initial_joins=\";\", final_join=\"; or \")\n    'apples; pears; or bananas'\n\n    :param l: List of strings to join\n    :param initial_joins: the string to join the non-ultimate items with\n    :param final_join: the string to join the final item with\n    :return: items joined with commas except \" and \" before the final one.\n    \"\"\"\n    # http://stackoverflow.com/questions/19838976/grammatical-list-join-in-python\n    return initial_joins.join(l[:-2] + [final_join.join(l[-2:])])"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a string that grammatically concatenates the items in the list l.", "response": "def _grammatical_join_filter(l, arg=None):\n    \"\"\"\n    :param l: List of strings to join\n    :param arg: A pipe-separated list of final_join (\" and \") and\n    initial_join (\", \") strings. For example\n    :return: A string that grammatically concatenates the items in the list.\n    \"\"\"\n    if not arg:\n        arg = \" and |, \"\n    try:\n        final_join, initial_joins = arg.split(\"|\")\n    except ValueError:\n        final_join = arg\n        initial_joins = \", \"\n    return grammatical_join(l, initial_joins, final_join)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate the GET parameters of the current request.", "response": "def update_GET(parser, token):\n    \"\"\"\n    ``update_GET`` allows you to substitute parameters into the current request's\n    GET parameters. This is useful for updating search filters, page numbers,\n    without losing the current set.\n\n    For example, the template fragment::\n\n        <a href=\"?{% update_GET 'attr1' += value1 'attr2' -= value2\n        'attr3' = value3 %}\">foo</a>\n\n    -  adds ``value1`` to (the list of values in) ``'attr1'``,\n    -  removes ``value2`` from (the list of values in) ``'attr2'``,\n    -  sets ``attr3`` to ``value3``.\n    and returns a urlencoded GET string.\n\n    Allowed attributes are:\n\n    -  strings, in quotes\n    -  vars that resolve to strings\n\n    Allowed values are:\n\n    -  strings, in quotes\n    -  vars that resolve to strings\n    -  lists of strings\n    -  None (without quotes)\n\n    Note:\n\n    -  If a attribute is set to ``None`` or an empty list, the GET parameter is\n       removed.\n    -  If an attribute's value is an empty string, or ``[\"\"]`` or ``None``, the value\n       remains, but has a ``\"\"`` value.\n    -  If you try to ``=-`` a value from a list that doesn't contain that value,\n       nothing happens.\n    -  If you try to ``=-`` a value from a list where the value appears more\n       than once, only the first value is removed.\n    \"\"\"\n    try:\n        args = token.split_contents()[1:]\n        triples = list(_chunks(args, 3))\n        if triples and len(triples[-1]) != 3:\n            raise template.TemplateSyntaxError, \"%r tag requires arguments in groups of three (op, attr, value).\" % token.contents.split()[0]\n        ops = set([t[1] for t in triples])\n        if not ops <= set(['+=', '-=', '=']):\n            raise template.TemplateSyntaxError, \"The only allowed operators are '+=', '-=' and '='. You have used %s\" % \", \".join(ops)\n\n    except ValueError:\n        return UpdateGetNode()\n\n    return UpdateGetNode(triples)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef oembed(url, params=\"\"):\n    # Note: this method isn't currently very efficient - the data isn't\n    # cached or stored.\n    kwargs = dict(urlparse.parse_qsl(params))\n\n    try:\n        return mark_safe(get_oembed_data(\n            url,\n            **kwargs\n        )['html'])\n    except (KeyError, ProviderException):\n        if settings.DEBUG:\n            return \"No OEmbed data returned\"\n        return \"\"", "response": "Render an OEmbed - compatible link as an embedded item."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn an HTML link to the admin URL of an object.", "response": "def admin_link(obj):\n    \"\"\"\n    Returns a link to the admin URL of an object.\n\n    No permissions checking is involved, so use with caution to avoid exposing\n    the link to unauthorised users.\n\n    Example::\n\n        {{ foo_obj|admin_link }}\n\n    renders as::\n\n        <a href='/admin/foo/123'>Foo</a>\n\n    :param obj: A Django model instance.\n    :return: A safe string expressing an HTML link to the admin page for an\n    object.\n    \"\"\"\n    if hasattr(obj, 'get_admin_link'):\n        return mark_safe(obj.get_admin_link())\n    return mark_safe(admin_link_fn(obj))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the admin URL of the object.", "response": "def admin_url(obj):\n    \"\"\"\n    Returns the admin URL of the object.\n\n    No permissions checking is involved, so use with caution to avoid exposing\n    the link to unauthorised users.\n\n    Example::\n\n        {{ foo_obj|admin_url }}\n\n    renders as::\n\n        /admin/foo/123\n\n    :param obj: A Django model instance.\n    :return: the admin URL of the object\n    \"\"\"\n    if hasattr(obj, 'get_admin_url'):\n        return mark_safe(obj.get_admin_url())\n    return mark_safe(admin_url_fn(obj))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef deprecate_and_include(parser, token):\n    split_contents = token.split_contents()\n    current_template = split_contents[1]\n    new_template = split_contents[2]\n    if settings.DEBUG:\n        warnings.simplefilter('always', DeprecationWarning)\n    warnings.warn(\"The %s template is deprecated; Use %s instead.\" % (current_template, new_template), DeprecationWarning, stacklevel=2)\n    new_contents = [split_contents[0]] + split_contents[2:]\n    include_token = Token(token.token_type, \" \".join(new_contents))\n\n    return do_include(parser, include_token)", "response": "Deprecate the first argument and include the new template."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns True if shared content with the given slug name exists.", "response": "def sharedcontent_exists(slug):\n    \"\"\"\n    Return `True` if shared content with the given slug name exists.\n\n    This filter makes it possible to conditionally include shared content with\n    surrounding markup only when the shared content item actually exits, and\n    avoid outputting the surrounding markup when it doesn't.\n\n    Example usage:\n\n        {% load icekit_tags sharedcontent_tags %}\n\n        {% if \"shared-content-slug\"|sharedcontent_exists %}\n        <div class=\"surrounding-html\">\n            {% sharedcontent \"shared-content-slug\" %}\n        </div>\n        {% endif %}\n    \"\"\"\n    from django.contrib.sites.models import Site\n    from fluent_contents.plugins.sharedcontent.models import SharedContent\n    site = Site.objects.get_current()\n    return SharedContent.objects.parent_site(site).filter(slug=slug).exists()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef work(request, slug):\n    # If this is a preview make sure the user has appropriate permissions.\n\n    item = get_object_or_404(models.WorkBase.objects.visible(), slug=slug)\n    if not item:\n        raise Http404\n\n    context = RequestContext(request, {\n        'page': item,\n        'work': item,\n    })\n\n    template = 'gk_collections/work.html'\n    return TemplateResponse(request, template, context)", "response": "This function returns a listing page for the given work."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef creator(request, slug):\n    # If this is a preview make sure the user has appropriate permissions.\n\n    item = get_object_or_404(models.CreatorBase.objects.visible(), slug=slug)\n    if not item:\n        raise Http404\n\n    context = RequestContext(request, {\n        'page': item,\n        'creator': item,\n    })\n\n    template = 'gk_collections/creator.html'\n    return TemplateResponse(request, template, context)", "response": "This function generates a listing page for the given slug."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_content(self, obj):\n        serializer = ContentSerializer(\n            instance=obj.contentitem_set.all(),\n            many=True,\n            context=self.context,\n        )\n        return serializer.data", "response": "Obtain the QuerySet of content items."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndefault rendering for items in field where the usual rendering method raised an exception.", "response": "def render_field_error(self, obj_id, obj, exception, request):\n        \"\"\"\n        Default rendering for items in field where the the usual rendering\n        method raised an exception.\n        \"\"\"\n        if obj is None:\n            msg = 'No match for ID={0}'.format(obj_id)\n        else:\n            msg = unicode(exception)\n        return u'<p class=\"error\">{0}</p>'.format(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\noverriding this to customize the preview representation of all objects.", "response": "def render_field_previews(self, id_and_obj_list, admin, request, field_name):\n        \"\"\"\n        Override this to customise the preview representation of all objects.\n        \"\"\"\n        obj_preview_list = []\n        for obj_id, obj in id_and_obj_list:\n            try:\n                # Handle invalid IDs\n                if obj is None:\n                    obj_preview = self.render_field_error(\n                        obj_id, obj, None, request\n                    )\n                else:\n                    try:\n                        obj_preview = admin.preview(obj, request)\n                    except AttributeError:\n                        try:\n                            obj_preview = obj.preview(request)\n                        except AttributeError:\n                            try:\n                                obj_preview = getattr(self, 'preview_{0}'.format(\n                                    field_name))(obj, request)\n                            except AttributeError:\n                                # Fall back to default field rendering\n                                obj_preview = self.render_field_default(obj, request)\n\n                obj_link = admin_link(obj, inner_html=obj_preview)\n            except Exception as ex:\n                obj_link = self.render_field_error(obj_id, obj, ex, request)\n            obj_preview_list.append(obj_link)\n        li_html_list = [u'<li>{0}</li>'.format(preview)\n                        for preview in obj_preview_list]\n        if li_html_list:\n            return u'<ul>{0}</ul>'.format(u''.join(li_html_list))\n        else:\n            return ''"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_item(self):\n        \"If the item is publishable, get the visible version\"\n\n        if hasattr(self, 'get_draft'):\n            draft = self.get_draft()\n        else:\n            draft = self\n\n        if not hasattr(self, '_item_cache'):\n            try:\n                self._item_cache = draft.item.get_published_or_draft()\n            except AttributeError:\n                # not publishable\n                self._item_cache = draft.item\n        return self._item_cache", "response": "If the item is publishable get the visible version"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef render(self, request, instance, **kwargs):\n        if instance.get_item():\n            return super(LinkPlugin, self).render(request, instance, **kwargs)\n        return \"\"", "response": "Only render the plugin if the item is shown to the user"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts default field names for this sub - serializer into versions where the field name has the prefix removed.", "response": "def get_fields(self):\n        \"\"\"\n        Convert default field names for this sub-serializer into versions where\n        the field name has the prefix removed, but each field object knows the\n        real model field name by setting the field's `source` attribute.\n        \"\"\"\n        prefix = getattr(self.Meta, 'source_prefix', '')\n        fields = super(ModelSubSerializer, self).get_fields()\n        fields_without_prefix = OrderedDict()\n        for field_name, field in fields.items():\n            if field_name.startswith(prefix):\n                # Set real model field name as field's `source` unless the\n                # source is already explicitly set, in which case it is\n                # probably a method name not the direct field name\n                if not field.source:\n                    field.source = field_name\n                field_name = field_name[len(prefix):]\n            fields_without_prefix[field_name] = field\n        return fields_without_prefix"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\npopulating validated_data with nested sub - field data.", "response": "def _populate_validated_data_with_sub_field_data(self, validated_data):\n        \"\"\"\n        Move field data nested in `ModelSubSerializer` fields back into the\n        overall validated data dict.\n        \"\"\"\n        for fieldname, field in self.get_fields().items():\n            if isinstance(field, ModelSubSerializer):\n                field_data = validated_data.pop(fieldname, None)\n                if field_data:\n                    validated_data.update(field_data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _prepare_related_single_or_m2m_relations(self, validated_data):\n        many_to_many_relationships = {}\n        for fieldname, field in self.get_fields().items():\n            if (\n                # `ModelSubSerializer` is handled separately\n                isinstance(field, ModelSubSerializer) or\n                # Skip read-only fields, obviously\n                field.read_only or\n                # Only list or model serializers are supported\n                not (\n                    isinstance(field, serializers.ModelSerializer) or\n                    isinstance(field, serializers.ListSerializer)\n                )\n            ):\n                continue  # Skip field\n\n            is_list_field = isinstance(field, serializers.ListSerializer)\n\n            if is_list_field:\n                ModelClass = field.child.Meta.model\n                field_data_list = validated_data.pop(fieldname, [])\n            else:\n                ModelClass = field.Meta.model\n                field_data_list = validated_data.pop(fieldname, None)\n                field_data_list = field_data_list and [field_data_list] or []\n\n            # Skip field if no data was provided\n            if not field_data_list:\n                continue\n\n            related_instances = []\n            for field_data in field_data_list:\n                related_instance = \\\n                    self._get_or_update_or_create_related_instance(\n                        ModelClass, fieldname, field_data)\n                if related_instance:\n                    related_instances.append(related_instance)\n\n            # Add related model instance as validated data parameter for\n            # later create/update operation on parent instance.\n            if not is_list_field:\n                validated_data[fieldname] = related_instances[0]\n            # Many-to-many relationships must be handled after super's `create`\n            # or `update` method, not here. So we just return the list for now\n            else:\n                many_to_many_relationships[fieldname] = related_instances\n        return many_to_many_relationships", "response": "Handle writing to nested related model fields for both single and M2M relationships."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nhandle lookup, update, or creation of related instances based on the field data provided and the field's `writable_related_fields` settings as defined on the serializer's `Meta`. This method will: - fail immediately if the field does not have `writable_related_fields` settings defined, or if these settings are not valid `WritableRelatedFieldSettings` objects - look up an existing instance using the defined `lookup_field` if a value is provided for the lookup field: - if no lookup field value is provided, fail unless `can_create` is set on the field since we cannot find any existing instance - if no existing instance is found, fail unless `can_create` - find the matching existing instance - if there is an existing instance: - if other data is provided, fail unless `can_update` is set - update existing instance based on other data provided if `can_update is set` - return the existing instance - if there is not an existing instance and `can_create` is set: - create a new instance with provided data - return the new instance", "response": "def _get_or_update_or_create_related_instance(\n            self, ModelClass, fieldname, field_data\n    ):\n        \"\"\"\n        Handle lookup, update, or creation of related instances based on the\n        field data provided and the field's `writable_related_fields` settings\n        as defined on the serializer's `Meta`.\n\n        This method will:\n\n            - fail immediately if the field does not have\n              `writable_related_fields` settings defined, or if these settings\n              are not valid `WritableRelatedFieldSettings` objects\n\n            - look up an existing instance using the defined `lookup_field` if\n              a value is provided for the lookup field:\n\n              - if no lookup field value is provided, fail unless `can_create`\n                is set on the field since we cannot find any existing instance\n              - if no existing instance is found, fail unless `can_create`\n              - find the matching existing instance\n\n            - if there is an existing instance:\n\n              - if other data is provided, fail unless `can_update` is set\n              - update existing instance based on other data provided if\n                `can_update is set`\n              - return the existing instance\n\n            - if there is not an existing instance and `can_create` is set:\n\n              - create a new instance with provided data\n              - return the new instance\n        \"\"\"\n        writable_related_fields = getattr(\n            self.Meta, 'writable_related_fields', {})\n\n        # Get settings for writable related field\n        if fieldname not in writable_related_fields:\n            raise TypeError(\n                \"Cannot write related model field '%s' for %s on %s\"\n                \" without corresponding 'writable_related_fields' settings\"\n                \" in the Meta class\"\n                % (fieldname, ModelClass.__name__,\n                   self.Meta.model.__name__)\n            )\n        field_settings = writable_related_fields[fieldname]\n        if not isinstance(field_settings, WritableRelatedFieldSettings):\n            raise TypeError(\n                \"Settings for related model field '%s' in\"\n                \" '%s.Meta.writable_related_fields' must be of type\"\n                \" 'WritableRelatedFieldSettings': %s\"\n                % (fieldname, ModelClass.__name__, type(field_settings))\n            )\n\n        # Get settings for lookup field; may be a string or a strings list\n        lookup_fields = field_settings.lookup_field\n        if not isinstance(lookup_fields, (list, tuple)):\n            lookup_fields = [lookup_fields]\n\n        # We use the first of potentially multiple lookup field values for\n        # which we have been given field data.\n        lookup_value = None\n        for lookup_field in lookup_fields:\n            if lookup_field in field_data:\n                lookup_value = field_data.pop(lookup_field)\n                break\n\n        # Fail if we have no lookup value and we cannot create an instance\n        if lookup_value is None and not field_settings.can_create:\n            raise TypeError(\n                \"Cannot look up related model field '%s' for %s on %s\"\n                \" using the lookup field(s) %r because no value\"\n                \" was provided for the lookup field(s) in %s\"\n                % (fieldname, ModelClass.__name__,\n                    self.Meta.model.__name__, lookup_fields, field_data)\n            )\n\n        related_instance = None\n\n        # Fetch existing instance using lookup field\n        try:\n            related_instance = ModelClass.objects.get(\n                **{lookup_field: lookup_value})\n\n            # Update existing related instance with values provided in\n            # parent's create/update operation, if such updates are\n            # permitted. If updates are not permitted, raise an exception\n            # only if submitted values differ from existing ones.\n            is_updated = False\n            for name, value in field_data.items():\n                original_value = getattr(related_instance, name)\n                if value != original_value:\n                    if not field_settings.can_update:\n                        raise TypeError(\n                            u\"Cannot update instance for related model\"\n                            u\" field '%s' for %s on %s because\"\n                            u\" 'can_update' is not set for this field in\"\n                            u\" 'writable_related_fields' but submitted\"\n                            u\" value `%s=%s` does not match existing\"\n                            u\" instance value `%s`\"\n                            % (fieldname, ModelClass.__name__,\n                               self.Meta.model.__name__, name, value,\n                               original_value)\n                        )\n                    setattr(related_instance, name, value)\n                    is_updated = True\n            if is_updated:\n                related_instance.save()\n        except ModelClass.MultipleObjectsReturned, ex:\n            raise TypeError(\n                \"Cannot look up related model field '%s' for %s on %s\"\n                \" using '%s' as the lookup field because it returns\"\n                \" multiple results for value '%s': %s\"\n                % (fieldname, ModelClass.__name__,\n                   self.Meta.model.__name__, lookup_field, lookup_value, ex)\n            )\n        # If a related instance does not yet exist, optionally create one\n        except ModelClass.DoesNotExist:\n            if not field_settings.can_create:\n                raise TypeError(\n                    \"Cannot create instance for related model field '%s'\"\n                    \" for %s on %s because 'can_create' is not set for\"\n                    \" this field in 'writable_related_fields'\"\n                    % (fieldname, ModelClass.__name__,\n                       self.Meta.model.__name__)\n                )\n\n            field_data.update({lookup_field: lookup_value})\n            related_instance = ModelClass.objects.create(**field_data)\n\n        return related_instance"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _write_related_m2m_relations(self, obj, many_to_many_relationships):\n        for fieldname, related_objs in many_to_many_relationships.items():\n            # TODO On PATCH avoid clearing existing relationships not provided?\n            setattr(obj, fieldname, related_objs)", "response": "Write the related M2M relationships to the given object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of choices for default app and model and child model.", "response": "def get_choices(self):\n        \"\"\"\n        Return a list of choices for default app and app/model template names:\n\n            * ``{{ app }}/{{ model }}/layouts/{{ child_model }}.html``\n            * ``{{ app }}/{{ model }}/layouts/default.html``\n            * ``{{ app }}/layouts/default.html``\n\n        \"\"\"\n        choices = []\n        for related_object in self.field.model._meta.get_all_related_objects():\n            # Django 1.8 deprecated `get_all_related_objects()`. We're still\n            # using it for now with the documented work-around for\n            # compatibility with Django <=1.7.\n            meta = getattr(\n                related_object, 'related_model', related_object.model)._meta\n            # App default.\n            template_name = '%s/layouts/default.html' % meta.app_label\n            label = '%s: Default' % meta.app_config.verbose_name\n            choices.append((template_name, label))\n            # Model default.\n            template_name = '%s/%s/layouts/default.html' % (\n                meta.app_label,\n                meta.model_name,\n            )\n            label = '%s: %s' % (\n                meta.app_config.verbose_name,\n                meta.verbose_name.capitalize(),\n            )\n            choices.append((template_name, label))\n            # Polymorphic child model defaults.\n            for relation in meta.get_all_related_objects():\n                model = getattr(relation, 'related_model', relation.model)\n                if issubclass(model, meta.model):\n                    template_name = '%s/%s/layouts/%s.html' % (\n                        meta.app_label,\n                        meta.model_name,\n                        model._meta.model_name,\n                    )\n                    label = '%s: %s' % (\n                        meta.app_config.verbose_name,\n                        model._meta.verbose_name.capitalize(),\n                    )\n                    choices.append((template_name, label))\n        return choices"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_choices(self):\n        choices = []\n        for label_prefix, templates_dir, template_name_prefix in \\\n                appsettings.LAYOUT_TEMPLATES:\n            source_dir = os.path.join(templates_dir, template_name_prefix)\n            # Walk directories, appending a choice for each source file.\n            for local, dirs, files in os.walk(source_dir, followlinks=True):\n                for source_file in files:\n                    template_name = os.path.join(\n                        template_name_prefix, source_file)\n                    label = '%s: %s' % (label_prefix, source_file)\n                    choices.append((template_name, label))\n        return choices", "response": "Returns a list of choices for source files found in configured layout\n        template directories."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _process_results(self, raw_results, *args, **kwargs):\n        if 'aggregations' in raw_results:\n            for agg_fieldname, agg_info in raw_results['aggregations'].items():\n                agg_info['_type'] = 'terms'\n                for bucket_item in agg_info['buckets']:\n                    if 'doc_count' in bucket_item:\n                        bucket_item['term'] = bucket_item['key']\n                        bucket_item['count'] = bucket_item['doc_count']\n                agg_info['terms'] = agg_info['buckets']\n            raw_results['facets'] = raw_results['aggregations']\n        return super(ICEkitConfigurableElasticBackend, self) \\\n            ._process_results(raw_results, *args, **kwargs)", "response": "This method handles the actual search results from ElasticSearch 2 + and returns the raw_results as a dictionary of data that can be used to populate the internal structure of the results."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving HTML or XML character references and entities from a text string. :param text: The HTML (or XML) source text. :return: The plain text, as a Unicode string, if necessary.", "response": "def unescape(text):\n    \"\"\"\n    Removes HTML or XML character references and entities from a text string.\n\n    :param text: The HTML (or XML) source text.\n    :return: The plain text, as a Unicode string, if necessary.\n    \"\"\"\n    def fixup(m):\n        text = m.group(0)\n        if text[:2] == \"&#\":\n            # character reference\n            try:\n                if text[:3] == \"&#x\":\n                    return unichr(int(text[3:-1], 16))\n                else:\n                    return unichr(int(text[2:-1]))\n            except ValueError:\n                pass\n        else:\n            # named entity\n            try:\n                text = unichr(htmlentitydefs.name2codepoint[text[1:-1]])\n            except KeyError:\n                pass\n        return text # leave as is\n\n    return re.sub(\"&#?\\w+;\", fixup, text)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a Jinja2 Environment object with the given options.", "response": "def environment(**options):\n    \"\"\"\n    Add ``static`` and ``url`` functions to the ``environment`` context\n    processor and return as a Jinja2 ``Environment`` object.\n    \"\"\"\n    env = Environment(**options)\n    env.globals.update({\n        'static': staticfiles_storage.url,\n        'url': reverse,\n    })\n    env.globals.update(context_processors.environment())\n    return env"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_site(sender, **kwargs):\n    Site = apps.get_model('sites', 'Site')\n    domain = settings.SITE_DOMAIN\n    if settings.SITE_PORT:\n        domain += ':%s' % settings.SITE_PORT\n    Site.objects.update_or_create(\n        pk=settings.SITE_ID,\n        defaults=dict(\n            domain=domain,\n            name=settings.SITE_NAME))\n\n    # We set an explicit pk instead of relying on auto-incrementation,\n    # so we need to reset the database sequence.\n    sequence_sql = connection.ops.sequence_reset_sql(no_style(), [Site])\n    if sequence_sql:\n        cursor = connection.cursor()\n        for command in sequence_sql:\n            cursor.execute(command)", "response": "Update the site object with the current settings."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a version of format_specifier that renders a date assuming it has the same year as another date.", "response": "def _format_with_same_year(format_specifier):\n    \"\"\"\n    Return a version of `format_specifier` that renders a date\n    assuming it has the same year as another date. Usually this means ommitting\n    the year.\n    \n    This can be overridden by specifying a format that has `_SAME_YEAR` appended\n    to the name in the project's `formats` spec.\n    \"\"\"\n    # gotta use a janky way of resolving the format\n    test_format_specifier = format_specifier + \"_SAME_YEAR\"\n    test_format = get_format(test_format_specifier, use_l10n=True)\n    if test_format == test_format_specifier:\n        # this format string didn't resolve to anything and may be a raw format.\n        # Use a regex to remove year markers instead.\n        return re.sub(YEAR_RE, '', get_format(format_specifier))\n    else:\n        return test_format"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a version of format_specifier that renders a date assuming it has the same year and month as another date.", "response": "def _format_with_same_year_and_month(format_specifier):\n    \"\"\"\n    Return a version of `format_specifier` that renders a date\n    assuming it has the same year and month as another date. Usually this \n    means ommitting the year and month.\n\n    This can be overridden by specifying a format that has \n    `_SAME_YEAR_SAME_MONTH` appended to the name in the project's `formats` \n    spec.\n    \"\"\"\n    test_format_specifier = format_specifier + \"_SAME_YEAR_SAME_MONTH\"\n    test_format = get_format(test_format_specifier, use_l10n=True)\n    if test_format == test_format_specifier:\n        # this format string didn't resolve to anything and may be a raw format.\n        # Use a regex to remove year and month markers instead.\n        no_year = re.sub(YEAR_RE, '', get_format(format_specifier))\n        return re.sub(MONTH_RE, '', no_year)\n    else:\n        return test_format"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dates_range(event, format=\"\"):\n\n    # TODO: factor out a more general filter that accepts 1-2 dates and\n    # renders the range.\n\n    # resolve arguments\n    date_format = settings.DATE_FORMAT # Django's default\n    separator = \"&nbsp;&ndash; \"\n    no_dates_text = ''\n    from_text = \"from \"\n    arg_list = [arg.strip() for arg in format.split('|')]\n    if arg_list:\n        date_format = arg_list[0]\n        try:\n            separator = arg_list[1]\n            no_dates_text = arg_list[2]\n            from_text = arg_list[3]\n            ended_text = arg_list[4]\n        except IndexError:\n            pass\n\n    if event.human_dates:\n        return event.human_dates.strip()\n\n    # Get the dates from the occurrence\n    first, last = event.get_occurrences_range()\n    start, end = None, None\n    if first:\n        start = first.local_start\n    if last:\n        end = last.local_end\n\n    # figure out to what extent the dates differ\n    if start and end:\n        first_date_format = get_format(date_format, use_l10n=True)\n        if start.year == end.year:\n            # use a first_date_format without the year\n            first_date_format = _format_with_same_year(date_format)\n            if start.month == end.month:\n                # remove month spec from first_date_format\n                first_date_format = _format_with_same_year_and_month(date_format)\n                if start.day == end.day:\n                    # the two dates are equal, just return one date.\n                    return mark_safe(datefilter(start, date_format))\n\n        return mark_safe('%s%s%s' % (\n            datefilter(start, first_date_format),\n            separator,\n            datefilter(end, date_format)\n        ))\n\n    elif start and not end:\n        return '%s%s' % (from_text, datefilter(start, date_format))\n    elif not (start or end):\n        return no_dates_text\n    else:\n        raise AssertionError(\"Got a date range that has a last date but no first date\")", "response": "Returns a string describing the dates of the event in the specified format."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_ctypes(self):\n        ctypes = []\n        for related_object in self.model._meta.get_all_related_objects():\n            model = getattr(related_object, 'related_model', related_object.model)\n            ctypes.append(ContentType.objects.get_for_model(model).pk)\n            if model.__subclasses__():\n                for child in model.__subclasses__():\n                    ctypes.append(ContentType.objects.get_for_model(child).pk)\n        return ctypes", "response": "Returns all related objects for this model."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns placeholder data for the given layout s template.", "response": "def placeholder_data_view(self, request, id):\n        \"\"\"\n        Return placeholder data for the given layout's template.\n        \"\"\"\n        # See: `fluent_pages.pagetypes.fluentpage.admin.FluentPageAdmin`.\n        try:\n            layout = models.Layout.objects.get(pk=id)\n        except models.Layout.DoesNotExist:\n            json = {'success': False, 'error': 'Layout not found'}\n            status = 404\n        else:\n            placeholders = layout.get_placeholder_data()\n            status = 200\n\n            placeholders = [p.as_dict() for p in placeholders]\n\n            # inject placeholder help text, if any is set\n            for p in placeholders:\n                try:\n                    p['help_text'] = settings.FLUENT_CONTENTS_PLACEHOLDER_CONFIG.get(p['slot']).get('help_text')\n                except AttributeError:\n                    p['help_text'] = None\n\n            json = {\n                'id': layout.id,\n                'title': layout.title,\n                'placeholders': placeholders,\n            }\n\n        return JsonResponse(json, status=status)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndecompress the given recurrence rule into a tuple of primary key value and recurrence rule value.", "response": "def decompress(self, value):\n        \"\"\"\n        Return the primary key value for the ``Select`` widget if the given\n        recurrence rule exists in the queryset.\n        \"\"\"\n        if value:\n            try:\n                pk = self.queryset.get(recurrence_rule=value).pk\n            except self.queryset.model.DoesNotExist:\n                pk = None\n            return [pk, None, value]\n        return [None, None, None]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrendering the format_output. html template with the given rendered_widgets.", "response": "def format_output(self, rendered_widgets):\n        \"\"\"\n        Render the ``icekit_events/recurrence_rule_widget/format_output.html``\n        template with the following context:\n\n            preset\n                A choice field for preset recurrence rules.\n            natural\n                An input field for natural language recurrence rules.\n            rfc\n                A text field for RFC compliant recurrence rules.\n\n        The default template positions the ``preset`` field above the\n        ``natural`` and ``rfc`` fields.\n        \"\"\"\n        template = loader.get_template(\n            'icekit_events/recurrence_rule_widget/format_output.html')\n        preset, natural, rfc = rendered_widgets\n        context = Context({\n            'preset': preset,\n            'natural': natural,\n            'rfc': rfc,\n        })\n        return template.render(context)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef render(self, name, value, attrs=None):\n        rendered_widgets = super(RecurrenceRuleWidget, self).render(\n            name, value, attrs)\n        template = loader.get_template(\n            'icekit_events/recurrence_rule_widget/render.html')\n        recurrence_rules = json.dumps(dict(\n            self.queryset.values_list('pk', 'recurrence_rule')))\n        context = Context({\n            'rendered_widgets': rendered_widgets,\n            'id': attrs['id'],\n            'recurrence_rules': recurrence_rules,\n        })\n        return template.render(context)", "response": "Render the recurrence rule widget."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _set_queryset(self, queryset):\n        self.fields[0].queryset = self.widget.queryset = queryset\n        self.widget.choices = self.fields[0].choices", "response": "Set the queryset on the ModelChoiceField and choices on the widget."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef filter_content_types(self, content_type_qs):\n        valid_ct_ids = []\n        for ct in content_type_qs:\n            model = ct.model_class()\n            if model and issubclass(model, EventBase):\n                valid_ct_ids.append(ct.id)\n        return content_type_qs.filter(pk__in=valid_ct_ids)", "response": "Filter the content types selectable to only event subclasses"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a dictionary describing the page numbers of the current page.", "response": "def describe_page_numbers(current_page, total_count, per_page, page_numbers_at_ends=3, pages_numbers_around_current=3):\n    \"\"\"\n    Produces a description of how to display a paginated list's page numbers. Rather than just\n    spitting out a list of every page available, the page numbers returned will be trimmed\n    to display only the immediate numbers around the start, end, and the current page.\n\n    :param current_page: the current page number (page numbers should start at 1)\n    :param total_count: the total number of items that are being paginated\n    :param per_page: the number of items that are displayed per page\n    :param page_numbers_at_ends: the amount of page numbers to display at the beginning and end of the list\n    :param pages_numbers_around_current: the amount of page numbers to display around the currently selected page\n\n    :return: a dictionary describing the page numbers, relative to the current page\n    \"\"\"\n    if total_count:\n        page_count = int(math.ceil(1.0 * total_count / per_page))\n        if page_count < current_page:\n            raise PageNumberOutOfBounds\n        page_numbers = get_page_numbers(\n            current_page=current_page,\n            num_pages=page_count,\n            extremes=page_numbers_at_ends,\n            arounds=pages_numbers_around_current,\n        )\n        current_items_start = (current_page * per_page) - per_page + 1\n        current_items_end = (current_items_start + per_page) - 1\n        if current_items_end > total_count:\n            current_items_end = total_count\n    else:\n        page_count = 0\n        page_numbers = []\n        current_items_start = 0\n        current_items_end = 0\n\n    return {\n        'numbers': [num for num in page_numbers if not isinstance(num, six.string_types)],\n        'has_previous': 'previous' in page_numbers,\n        'has_next': 'next' in page_numbers,\n        'current_page': current_page,\n        'previous_page': current_page - 1,\n        'next_page': current_page + 1,\n        'total_count': total_count,\n        'page_count': page_count,\n        'per_page': per_page,\n        'current_items_start': current_items_start,\n        'current_items_end': current_items_end,\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmigrating origin_* fields to a new GeographicLocation.", "response": "def migrate_origin_locations(apps, _):\n    \"\"\"\n    Copy origin_* fields to a new GeographicLocation.\n    \"\"\"\n\n    Country = apps.get_model(\"glamkit_collections\", \"Country\")\n    GeographicLocation = apps.get_model(\"glamkit_collections\", \"GeographicLocation\")\n    WorkOrigin = apps.get_model(\"gk_collections_work_creator\", \"WorkOrigin\")\n    WorkBase = apps.get_model(\"gk_collections_work_creator\", \"WorkBase\")\n\n    countries = dict(((c.iso_country, c) for c in Country.objects.all()))\n\n    for w in WorkBase.objects.all():\n        if w.origin_country or \\\n            w.origin_state_province or \\\n            w.origin_city or \\\n            w.origin_neighborhood or \\\n            w.origin_colloquial:\n            loc, created = GeographicLocation.objects.get_or_create(\n                country=countries.get(w.origin_country, None),\n                state_province=w.origin_state_province,\n                city=w.origin_city,\n                neighborhood=w.origin_neighborhood,\n                colloquial_historical=w.origin_colloquial\n            )\n            WorkOrigin.objects.create(work=w, geographic_location=loc)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a StringIO containing the formatted statistics from _statsfile_.", "response": "def render_stats(stats, sort, format):\n    \"\"\"\n    Returns a StringIO containing the formatted statistics from _statsfile_.\n\n    _sort_ is a list of fields to sort by.\n    _format_ is the name of the method that pstats uses to format the data.\n    \"\"\"\n    output = StdoutWrapper()\n    if hasattr(stats, \"stream\"):\n        stats.stream = output.stream\n    stats.sort_stats(*sort)\n    getattr(stats, format)()\n    return output.stream"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrendering a list of SQL queries.", "response": "def render_queries(queries, sort):\n    \"\"\"\n    Returns a StringIO containing the formatted SQL queries.\n\n    _sort_ is a field to sort by.\n    \"\"\"\n    output = StringIO()\n    if sort == 'order':\n        print >>output, \"     time query\"\n        for query in queries:\n            print >>output, \" %8s %s\" % (query[\"time\"], query[\"sql\"])\n        return output\n    if sort == 'time':\n        def sorter(x, y):\n            return cmp(x[1][1], y[1][1])\n    elif sort == 'queries':\n        def sorter(x, y):\n            return cmp(x[1][0], y[1][0])\n    else:\n        raise RuntimeError(\"Unknown sort: %s\" % sort)\n    print >>output, \"  queries     time query\"\n    results = {}\n    for query in queries:\n        try:\n            result = results[query[\"sql\"]]\n            result[0] += 1\n            result[1] += Decimal(query[\"time\"])\n        except KeyError:\n            results[query[\"sql\"]] = [1, Decimal(query[\"time\"])]\n    results = sorted(results.iteritems(), cmp=sorter, reverse=True)\n    for result in results:\n        print >>output, \" %8d %8.3f %s\" % (\n            result[1][0], result[1][1], result[0]\n        )\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate a HttpResponse of functions for a profiling run.", "response": "def display_stats(request, stats, queries):\n    \"\"\"\n    Generate a HttpResponse of functions for a profiling run.\n\n    _stats_ should contain a pstats.Stats of a hotshot session.\n    _queries_ should contain a list of SQL queries.\n    \"\"\"\n    sort = [\n        request.REQUEST.get('sort_first', 'time'),\n        request.REQUEST.get('sort_second', 'calls')\n    ]\n    fmt = request.REQUEST.get('format', 'print_stats')\n    sort_first_buttons = RadioButtons('sort_first', sort[0], sort_categories)\n    sort_second_buttons = RadioButtons('sort_second', sort[1], sort_categories)\n    format_buttons = RadioButtons('format', fmt, (\n        ('print_stats', 'by function'),\n        ('print_callers', 'by callers'),\n        ('print_callees', 'by callees')\n    ))\n    output = render_stats(stats, sort, fmt)\n    output.reset()\n    output = [html.escape(unicode(line)) for line in output.readlines()]\n    response = HttpResponse(content_type='text/html; charset=utf-8')\n    response.content = (stats_template % {\n        'format_buttons': format_buttons,\n        'sort_first_buttons': sort_first_buttons,\n        'sort_second_buttons': sort_second_buttons,\n        'rawqueries' : b64encode(cPickle.dumps(queries)),\n        'rawstats': b64encode(pickle_stats(stats)),\n        'stats': \"\".join(output),\n        'url': request.path\n    })\n    return response"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef display_queries(request, stats, queries):\n    sort = request.REQUEST.get('sort_by', 'time')\n    sort_buttons = RadioButtons('sort_by', sort, (\n        ('order', 'by order'), ('time', 'time'), ('queries', 'query count')\n    ))\n    output = render_queries(queries, sort)\n    output.reset()\n    output = [html.escape(unicode(line)) for line in output.readlines()]\n    response = HttpResponse(mimetype='text/html; charset=utf-8')\n    response.content = (queries_template % {\n        'sort_buttons': sort_buttons,\n        'num_queries': len(queries),\n        'queries': \"\".join(output),\n        'rawqueries' : b64encode(cPickle.dumps(queries)),\n        'rawstats': b64encode(pickle_stats(stats)),\n        'url': request.path\n    })\n    return response", "response": "Generates a HttpResponse of SQL queries for a profiling run."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef process_request(self, request):\n        def unpickle(params):\n            stats = unpickle_stats(b64decode(params.get('stats', '')))\n            queries = cPickle.loads(b64decode(params.get('queries', '')))\n            return stats, queries\n\n        if request.method != 'GET' and \\\n           not (request.META.get(\n               'HTTP_CONTENT_TYPE', request.META.get('CONTENT_TYPE', '')\n           ) in ['multipart/form-data', 'application/x-www-form-urlencoded']):\n            return\n        if (request.REQUEST.get('profile', False) and\n            (settings.DEBUG == True or request.user.is_staff)):\n            request.statsfile = tempfile.NamedTemporaryFile()\n            params = request.REQUEST\n            if (params.get('show_stats', False)\n                and params.get('show_queries', '1') == '1'):\n                # Instantly re-sort the existing stats data\n                stats, queries = unpickle(params)\n                return display_stats(request, stats, queries)\n            elif (params.get('show_queries', False)\n                  and params.get('show_stats', '1') == '1'):\n                stats, queries = unpickle(params)\n                return display_queries(request, stats, queries)\n            else:\n                # We don't have previous data, so initialize the profiler\n                request.profiler = hotshot.Profile(request.statsfile.name)\n                reset_queries()", "response": "Process the request and return the list of SQL query log entries."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef process_view(self, request, view_func, view_args, view_kwargs):\n        profiler = getattr(request, 'profiler', None)\n        if profiler:\n            # Make sure profiler variables don't get passed into view_func\n            original_get = request.GET\n            request.GET = original_get.copy()\n            request.GET.pop('profile', None)\n            request.GET.pop('show_queries', None)\n            request.GET.pop('show_stats', None)\n            try:\n                return profiler.runcall(\n                    view_func, request, *view_args, **view_kwargs\n                )\n            finally:\n                request.GET = original_get", "response": "Run the profiler on the _view_func_."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfinishing profiling and render the results.", "response": "def process_response(self, request, response):\n        \"\"\"Finish profiling and render the results.\"\"\"\n        profiler = getattr(request, 'profiler', None)\n        if profiler:\n            profiler.close()\n            params = request.REQUEST\n            stats = hotshot.stats.load(request.statsfile.name)\n            queries = connection.queries\n            if (params.get('show_queries', False)\n                and params.get('show_stats', '1') == '1'):\n                response = display_queries(request, stats, queries)\n            else:\n                response = display_stats(request, stats, queries)\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef creators_grouped_by_role(self):\n        role = -1\n        creators = []\n        for wc in self:\n            if wc.role != role:\n                if creators:\n                    yield (role, creators)\n                role = wc.role\n                creators = []\n            creators.append(wc.creator)\n\n        if creators:\n            yield (role, creators)", "response": "A generator that yields 2 - tuples of role and list of creators where the role is not equal to the creator."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef images_grouped_by_type(self):\n        type = -1\n        images = []\n        for wc in self:\n            if wc.type != type:\n                if images:\n                    yield (type, images)\n                role = wc.role\n                creators = []\n                images.append(wc.image)\n\n        if images:\n            yield (type, images)", "response": "A generator yielding 2 - tuples of type and list of images where the type of image is different from the type of the image."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef items_to_extract(self, offset=0, length=None):\n        endoffset = length and offset + length\n        qs = self.origin_data()[offset:endoffset]\n        self.items_to_extract_length = qs.count()\n        return qs", "response": "Return an iterable of specific items to extract."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef slice_sequences(sequences, start, end, apply_slice=None):\n\n    if start < 0 or end < 0 or end <= start:\n        raise ValueError('Start and/or End out of range. Start: %s. End: %s' % (start, end))\n\n    items_to_take = end - start\n    items_passed = 0\n    collected_items = []\n\n    if apply_slice is None:\n        apply_slice = _apply_slice\n\n    for sequence, count in sequences:\n        offset_start = start - items_passed\n        offset_end = end - items_passed\n\n        if items_passed == start:\n            items = apply_slice(sequence, 0, items_to_take)\n        elif 0 < offset_start < count:\n            items = apply_slice(sequence, offset_start, offset_end)\n        elif offset_start < 0:\n            items = apply_slice(sequence, 0, offset_end)\n        else:\n            items = []\n\n        items = list(items)\n        collected_items += items\n        items_to_take -= len(items)\n        items_passed += count\n\n        if items_passed > end or items_to_take == 0:\n            break\n\n    return collected_items", "response": "Takes a list of nested sequences and returns a list of the items that are in the specified range."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_draft(request):\n        # Admin resource requested.\n        if PublishingMiddleware.is_admin_request(request):\n            return True\n        # API resource requested.\n        if PublishingMiddleware.is_api_request(request):\n            return True\n        # Draft-only view requested.\n        if PublishingMiddleware.is_draft_only_view(request):\n            return True\n        # Content reviewer made request.\n        if PublishingMiddleware.is_content_reviewer_user(request):\n            return True\n        # Draft mode requested.\n        if PublishingMiddleware.is_draft_request(request):\n            # User is staff.\n            if PublishingMiddleware.is_staff_user(request):\n                return True\n            # Request contains a valid draft mode HMAC in the querystring.\n            if verify_draft_url(request.get_full_path()):\n                return True\n        # Not draft mode.\n        return False", "response": "Returns True if the request is considered to be in draft mode."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef redirect_staff_to_draft_view_on_404(request, response):\n        if (response.status_code == 404\n                # No point redirecting if we already have a draft request\n                and not PublishingMiddleware.is_draft_request(request)\n                # Don't mess with admin requests at all\n                and not PublishingMiddleware.is_admin_request(request)\n                # Don't mess with API requests at all\n                and not PublishingMiddleware.is_api_request(request)\n                # Can user view draft content if we add the 'preview' param\n                and PublishingMiddleware.is_staff_user(request)):\n            # TODO Is there a sane way to check for draft version of resource\n            # at this URL path, without just redirecting the user to it?\n            return HttpResponseRedirect(get_draft_url(request.get_full_path()))\n        return response", "response": "Redirect staff to draft view on 404."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef quote(key, value):\n\n    if key in quoted_options and isinstance(value, string_types):\n        return \"'%s'\" % value\n\n    if key in quoted_bool_options and isinstance(value, bool):\n        return {True:'true',False:'false'}[value]\n\n    return value", "response": "Quote a value for the specified key."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks that all required settings are set.", "response": "def check_settings(required_settings):\n    \"\"\"\n    Checks all settings required by a module have been set.\n\n    If a setting is required and it could not be found a\n    NotImplementedError will be raised informing which settings are\n    missing.\n\n    :param required_settings: List of settings names (as strings) that\n    are anticipated to be in the settings module.\n    :return: None.\n    \"\"\"\n    defined_settings = [\n        setting if hasattr(settings, setting) else None for setting in required_settings\n    ]\n\n    if not all(defined_settings):\n        raise NotImplementedError(\n            'The following settings have not been set: %s' % ', '.join(\n                set(required_settings) - set(defined_settings)\n            )\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef with_upcoming_occurrences(self):\n        return self.filter(\n            Q(is_drop_in=False, occurrences__start__gte=djtz.now) |\n            Q(Q(is_drop_in=True) | Q(occurrences__is_all_day=True), occurrences__end__gt=djtz.now)\n        )", "response": "Return a new QuerySet with events having upcoming occurrences and all their children."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef with_upcoming_or_no_occurrences(self):\n        ids = set(self.with_upcoming_occurrences().values_list('id', flat=True)) | set(self.with_no_occurrences().values_list('id', flat=True))\n        return self.model.objects.filter(id__in=ids)", "response": "Return a QuerySet of all the related objects that are upcoming or no occurrences."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the event in order of minimum occurrence.", "response": "def order_by_first_occurrence(self):\n        \"\"\"\n        :return: The event in order of minimum occurrence. \n        \"\"\"\n        def _key(e):\n            try:\n                return e.occurrence_list[0].start\n            except IndexError: # no occurrences; put last\n                return localize(datetime.max - timedelta(days=365))\n\n        return sorted(list(self), key=_key)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef order_by_next_occurrence(self):\n\n        qs = self.prefetch_related('occurrences')\n        def _sort(x):\n            try:\n                # If there's an upcoming occurrence, use it.\n                return x.get_next_occurrence().start\n            except AttributeError:\n                try:\n                    # If there's any occurrence, use the first one, plus 1000 years.\n                    return x.get_first_occurrence().start + timedelta(days=1000*365)\n                except AttributeError:\n                    # If no occurence, use a localised datetime.max (minus a\n                    # few days to avoid overflow)\n                    return make_aware(datetime.max-timedelta(2))\n\n        sorted_qs = sorted(qs, key=_sort)\n\n        return sorted_qs", "response": "Returns a list of events in order of next occurrence."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a QuerySet of occurrences that overlap the given start and end datetimes.", "response": "def overlapping(self, start=None, end=None):\n        \"\"\"\n        :return: occurrences overlapping the given start and end datetimes,\n        inclusive.\n        Special logic is applied for all-day occurrences, for which the start\n        and end times are zeroed to find all occurrences that occur on a DATE\n        as opposed to within DATETIMEs.\n        \"\"\"\n        qs = self\n\n        if start:\n            dt_start=coerce_dt_awareness(start)\n            qs = qs.filter(\n                # Exclusive for datetime, inclusive for date.\n                Q(is_all_day=False, end__gt=dt_start) |\n                Q(is_all_day=True, end__gte=zero_datetime(dt_start))\n            )\n        if end:\n            dt_end=coerce_dt_awareness(end, t=time.max)\n            qs = qs.filter(\n                Q(is_all_day=False, start__lt=dt_end) |\n                Q(is_all_day=True, start__lt=zero_datetime(dt_end))\n            )\n\n        return qs"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a QuerySet of occurrences that start within the given start and end datetimes inclusive and drop - in occurrences that contain the given datetimes that are not part of the current date range.", "response": "def starts_within(self, start=None, end=None):\n        \"\"\"\n        :return:  normal occurrences that start within the given start and end\n        datetimes, inclusive, and drop-in occurrences that \n        \"\"\"\n\n        qs = self\n\n        if start:\n            dt_start=coerce_dt_awareness(start)\n            qs = qs.filter(\n                Q(is_all_day=False, start__gte=dt_start) |\n                Q(is_all_day=True, start__gte=zero_datetime(dt_start))\n            )\n        if end:\n            dt_end=coerce_dt_awareness(end, t=time.max)\n            qs = qs.filter(\n                # Exclusive for datetime, inclusive for date.\n                Q(is_all_day=False, start__lt=dt_end) |\n                Q(is_all_day=True, start__lte=zero_datetime(dt_end))\n            )\n\n        return qs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef available_within(self, start=None, end=None):\n        return self.filter(event__is_drop_in=False).starts_within(start, end) | \\\n               self.filter(event__is_drop_in=True).overlapping(start, end)", "response": "Return a QuerySet of available events that are in the given range."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef available_on_day(self, day):\n        if isinstance(day, datetime):\n            d = day.date()\n        else:\n            d = day\n        return self.starts_within(d, d)", "response": "Return events that are available on a given day."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the ids of occurrences that finish on the same day that they start or midnight the next day.", "response": "def _same_day_ids(self):\n        \"\"\"\n        :return: ids of occurrences that finish on the same day that they\n        start, or midnight the next day.\n        \"\"\"\n        # we can pre-filter to return only occurrences that are <=24h long,\n        # but until at least the `__date` can be used in F() statements\n        # we'll have to refine manually\n        qs = self.filter(end__lte=F('start') + timedelta(days=1))\n\n        # filter occurrences to those sharing the same end date, or\n        # midnight the next day (unless it's an all-day occurrence)\n        ids = [o.id for o in qs if (\n            (o.local_start.date() == o.local_end.date()) or\n            (\n                o.local_end.time() == time(0,0) and\n                o.local_end.date() == o.local_start.date() + timedelta(days=1) and\n                o.is_all_day == False\n            )\n        )]\n        return ids"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncleans up internal database connections and connections to all databases.", "response": "def cleanup(self):\n        \"\"\"\n        Performs clean-up after task is completed before it is executed again\n        in the next internal.\n        \"\"\"\n        # Closes connections to all databases to avoid the long running process\n        # from holding connections indefinitely.\n        for alias in db.connections.databases:\n            logger.info('Closing database connection: %s', alias)\n            db.connections[alias].close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_plugins(cls, *args, **kwargs):\n        return [plugin(*args, **kwargs) for plugin in cls.plugins]", "response": "Return a list of plugin instances and pass through arguments."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting all available choices for all plugins and return a list of tuples.", "response": "def get_all_choices(cls, *args, **kwargs):\n        \"\"\"\n        Validate (template), de-duplicate (by template), sort (by label) and\n        return a list of ``(template name, label)`` choices for all plugins.\n        \"\"\"\n        plugins = cls.get_plugins(*args, **kwargs)\n        all_choices = reduce(operator.add, [\n            plugin.choices for plugin in plugins\n        ])\n        choices = []\n        seen = set()\n        for template, label in all_choices:\n            # De-duplicate.\n            if template in seen:\n                continue\n            seen.add(template)\n            # Validate.\n            try:\n                validators.template_name(template)\n            except ValidationError:\n                continue\n            choices.append((template, label))\n        # Sort by label.\n        choices = sorted(choices, key=lambda a: (a[0], a[1]))\n        return choices"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the queryset used for generating facets before any facets are applied", "response": "def pre_facet_sqs(self):\n        \"\"\"\n        Return the queryset used for generating facets, before any facets\n        are applied\n        \"\"\"\n        sqs = SearchQuerySet()\n\n        if self.query:\n            sqs = sqs.filter(\n                SQ(content=AutoQuery(self.query)) |  # Search `text` document\n                SQ(get_title=AutoQuery(self.query)) | # boosted field\n                SQ(boosted_search_terms=AutoQuery(self.query)) # boosted field\n            )\n\n        return sqs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get(self, request, *args, **kwargs):\n\n        form_class = self.get_form_class()\n        form = self.get_form(form_class)\n\n        top_value = self.get_top_level_facet_value()\n        subfacets = SEARCH_SUBFACETS.get(top_value, [])\n        self.active_facets = [self.top_facet] + subfacets\n\n        if form.is_valid():\n            self.query = form.cleaned_data.get(self.search_field)\n        else:\n            self.query = \"\"\n\n        sqs = self.pre_facet_sqs()\n\n        for facet in self.active_facets:\n            sqs = facet.set_on_sqs(sqs)\n\n        facet_counts = sqs.facet_counts()\n\n        for facet in self.active_facets:\n            facet.set_values_from_sqs_facet_counts(facet_counts)\n            facet.apply_request_and_page_to_values(self.request, self.fluent_page)\n\n        for facet in self.active_facets:\n            sqs = facet.narrow_sqs(sqs)\n\n        context = self.get_context_data(**{\n            self.form_name: form,\n            'facets': self.active_facets,\n            'top_facet': self.top_facet,\n            'query': self.query,\n            'object_list': sqs,\n            'page': self.fluent_page,\n            'show_placeholders': self.show_placeholders()\n        })\n        return self.render_to_response(context)", "response": "Return a list of active facets and the current page."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef index(request):\n    warnings.warn(\n        \"icekit_events.views.index is deprecated and will disappear in a \"\n        \"future version. If you need this code, copy it into your project.\"\n        , DeprecationWarning\n    )\n\n    occurrences = models.Occurrence.objects.visible()\n    context = {\n        'occurrences': occurrences,\n    }\n    return TemplateResponse(request, 'icekit_events/index.html', context)", "response": "This function returns a listing page for the event Occurrences."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a response object that can be used to render a single event.", "response": "def event(request, slug):\n    \"\"\"\n    :param request: Django request object.\n    :param event_id: The `id` associated with the event.\n    :param is_preview: Should the listing page be generated as a preview? This\n                       will allow preview specific actions to be done in the\n                       template such as turning off tracking options or adding\n                       links to the admin.\n    :return: TemplateResponse\n    \"\"\"\n    # If this is a preview make sure the user has appropriate permissions.\n\n    event = get_object_or_404(models.EventBase.objects.visible(), slug=slug)\n\n    context = RequestContext(request, {\n        'event': event,\n        'page': event,\n    })\n    # TODO Not sure where the `Event.template` notion comes from, keeping it\n    # here for now for backwards compatibility\n    template = getattr(event, 'template',  'icekit_events/event.html')\n    return TemplateResponse(request, template, context)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_assigned_to_user(parser, token):\n    tokens = token.contents.split()\n    if len(tokens) < 4:\n        raise template.TemplateSyntaxError(\n            \"'get_assigned_to_user' statements require two arguments\")\n    if not tokens[1].isdigit():\n        raise template.TemplateSyntaxError(\n            \"First argument to 'get_assigned_to_user' must be an integer\")\n    if tokens[2] != 'as':\n        raise template.TemplateSyntaxError(\n            \"Second argument to 'get_assigned_to_user' must be 'as'\")\n    if len(tokens) > 4:\n        if tokens[4] != 'for_user':\n            raise template.TemplateSyntaxError(\n                \"Fourth argument to 'get_assigned_to_user' must be 'for_user'\")\n    return AssigedToUserNode(limit=tokens[1], varname=tokens[3], user=(tokens[5] if len(tokens) > 5 else None))", "response": "Return the assigned content of a user in the current context."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates initial recurrence rules.", "response": "def forwards(apps, schema_editor):\n    \"\"\"\n    Create initial recurrence rules.\n    \"\"\"\n    RecurrenceRule = apps.get_model('icekit_events', 'RecurrenceRule')\n    for description, recurrence_rule in RULES:\n        RecurrenceRule.objects.get_or_create(\n            description=description,\n            defaults=dict(recurrence_rule=recurrence_rule),\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndeleting initial recurrence rules.", "response": "def backwards(apps, schema_editor):\n    \"\"\"\n    Delete initial recurrence rules.\n    \"\"\"\n    RecurrenceRule = apps.get_model('icekit_events', 'RecurrenceRule')\n    descriptions = [d for d, rr in RULES]\n    RecurrenceRule.objects.filter(description__in=descriptions).delete()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the current environment as a dictionary.", "response": "def environment(request=None):\n    \"\"\"\n    Return ``COMPRESS_ENABLED``, ``SITE_NAME``, and any settings listed\n    in ``ICEKIT_CONTEXT_PROCESSOR_SETTINGS`` as context.\n    \"\"\"\n    context = {\n        'COMPRESS_ENABLED': settings.COMPRESS_ENABLED,\n        'SITE_NAME': settings.SITE_NAME,\n    }\n    for key in settings.ICEKIT_CONTEXT_PROCESSOR_SETTINGS:\n        context[key] = getattr(settings, key, None)\n    return context"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_proxy_ancestor_classes(klass):\n    proxy_ancestor_classes = set()\n    for superclass in klass.__bases__:\n        if hasattr(superclass, '_meta') and superclass._meta.proxy:\n            proxy_ancestor_classes.add(superclass)\n        proxy_ancestor_classes.update(\n            get_proxy_ancestor_classes(superclass))\n    return proxy_ancestor_classes", "response": "Returns a set containing all the proxy model classes that are ancestors of the given class."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\noverriding this method from CreatorBase to handle additional name related fields for Person creators.", "response": "def derive_and_set_name_fields_and_slug(\n        self, set_name_sort=True, set_slug=True\n    ):\n        \"\"\"\n        Override this method from `CreatorBase` to handle additional name\n        fields for Person creators.\n\n        This method is called during `save()`\n        \"\"\"\n        super(PersonCreator, self).derive_and_set_name_fields_and_slug(\n            set_name_sort=False, set_slug=False)\n        # Collect person name fields, but only if they are not empty\n        person_names = [\n            name for name in [self.name_family, self.name_given]\n            if not is_empty(name)\n        ]\n        # if empty, set `name_sort` = '{name_family}, {name_given}' if these\n        # person name values are available otherwise `name_full`\n        if set_name_sort and is_empty(self.name_sort):\n            if person_names:\n                self.name_sort = ', '.join(person_names)\n            else:\n                self.name_sort = self.name_full\n        # if empty, set `slug` to slugified '{name_family} {name_given}' if\n        # these person name values are available otherwise slugified\n        # `name_full`\n        if set_slug and is_empty(self.slug):\n            if person_names:\n                self.slug = slugify(' '.join(person_names))\n            else:\n                self.slug = slugify(self.name_full)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the lifespan of the current resource for the web.", "response": "def lifespan_for_web(self, join=\"&nbsp;&ndash; \"):\n        \"\"\"\n        Returns lifespan formatted for the web, for example:\n\n        1850 - 1922\n        1850, Cologne - 1922, Berlin\n        1968 -\n        n.d. - 1540s\n        \"\"\"\n        birth = \", \".join(filter(None, (\n            self.start_date_display,\n            self.start_place\n        )))\n        death = \", \".join(filter(None, (\n            self.end_date_display,\n            self.end_place\n        )))\n\n        if death and not birth:\n            if not death.startswith(\"died\"):\n                return mark_safe(\"n.d.&nbsp;&ndash;&nbsp;\" + death)\n\n        return mark_safe(join.join(filter(None, (birth, death))))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef for_model(self, model, **kwargs):\n        queryset = self.filter(\n            content_types=ContentType.objects.get_for_model(model),\n            **kwargs\n        )\n        return queryset", "response": "Return layouts that are allowed for the given model."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning all records with non - null latitude and longitude values with the specified distance_in_km annotation value.", "response": "def annotate_distance_in_km(self, latitude, longitude):\n        \"\"\"\n        Return all records with non-null latitude/longitude values with the\n        annotation value `distance_in_km` which is the distance between\n        the record and the given `latitude`/`longitude` location.\n        \"\"\"\n        # Great circle distance formula, taken on faith from StackOverflow link\n        # above, see also https://en.wikipedia.org/wiki/Great-circle_distance\n        # NOTE: We use psql-specific COALESCE() to choose marker lat/long\n        # values when available (non-NULL) otherwise center lat/long values\n        GCD = \"\"\"\n            6371\n            * ACOS(\n                COS(RADIANS(%s))\n                * COS(RADIANS(\n                    COALESCE(map_marker_lat, map_center_lat)\n                  ))\n                * COS(RADIANS(\n                    COALESCE(map_marker_long, map_center_long)\n                  )\n                - RADIANS(%s))\n                + SIN(RADIANS(%s))\n                * SIN(RADIANS(\n                    COALESCE(map_marker_lat, map_center_lat)\n                ))\n            )\n            \"\"\"\n        return self.get_queryset() \\\n            .exclude(map_center_lat=None) \\\n            .exclude(map_center_long=None) \\\n            .annotate(\n                distance_in_km=models.expressions.RawSQL(\n                    GCD,\n                    (latitude, longitude, latitude)\n                )\n            ) \\\n            .order_by('distance_in_km')"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a tuple containing the start and end times for an Event s Occurrences.", "response": "def get_occurrence_times_for_event(event):\n    \"\"\"\n    Return a tuple with two sets containing the (start, end) *naive* datetimes\n    of an Event's Occurrences, or the original start datetime if an\n    Occurrence's start was modified by a user.\n    \"\"\"\n    occurrences_starts = set()\n    occurrences_ends = set()\n    for o in event.occurrence_list:\n        occurrences_starts.add(\n            coerce_naive(o.original_start or o.start)\n        )\n        occurrences_ends.add(\n            coerce_naive(o.original_end or o.end)\n        )\n    return occurrences_starts, occurrences_ends"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nexchanging the results in a queryset of published items for the original QS.", "response": "def _exchange_for_published(qs):\n    \"\"\"\n    Exchange the results in a queryset of publishable items for the\n    corresponding published copies. This means the result QS has:\n        - already published items in original QS\n        - the published copies for drafts with published versions\n        - unpublished draft items in the original QS are removed.\n\n    Unfortunately we cannot just perform a normal queryset filter operation\n    in the DB because FK/M2M relationships assigned in the site admin are\n    *always* to draft objects. Instead we exchange draft items for\n    published copies.\n    \"\"\"\n    # TODO: Can this be lazily executed when a queryset is evaluated?\n    # Currently any `published()` queryset will obtain its list available\n    # published PKs when this function is called, which might not be\n    # immediately before the queryset is evaluated. It might be possible\n    # to pass a subquery instead of collecting PKs into a list to pass?\n    published_version_pks = []\n    draft_version_pks = []\n    is_exchange_required = False\n    # Use direct DB query if possible (be sure to prioritise the ordering of\n    # the draft items over the published items by checking the draft items\n    # first, since the draft item ordering may be explicitly set via admin)...\n    from .models import PublishingModel\n    if issubclass(qs.model, PublishingModel):\n        for pk, publishing_is_draft, publishing_linked_id in qs.values_list(\n                'pk', 'publishing_is_draft', 'publishing_linked_id'):\n            # If item is draft and if it has a linked published copy, exchange\n            # the draft to get the published copy instead...\n            if publishing_is_draft:\n                draft_version_pks.append(pk)\n                if publishing_linked_id:\n                    published_version_pks.append(publishing_linked_id)\n                    is_exchange_required = True\n            # ...otherwise if item is already the published copy, use it.\n            elif not publishing_is_draft:\n                published_version_pks.append(pk)\n    # ...otherwise we are forced to retrieve the real instances to check fields\n    # and we may be dealing with a UrlNode model without our own publishing\n    # fields so be defensive in our field lookups.\n    else:\n        for item in qs:\n            # If item is draft and if it has a linked published copy, exchange\n            # the draft to get the published copy instead...\n            if getattr(item, 'publishing_is_draft', None):\n                draft_version_pks.append(item.pk)\n                if getattr(item, 'publishing_linked_id', None):\n                    published_version_pks.append(item.publishing_linked_id)\n                    is_exchange_required = True\n            # ...otherwise if item is already the published copy, use it.\n            elif getattr(item, 'is_published', None):\n                published_version_pks.append(item.pk)\n    # Only perform exchange query and re-ordering if necessary\n    if not is_exchange_required:\n        # If no exchange is required, we must make sure any draft items we\n        # found are excluded from the queryset since we won't be filtering by\n        # published-item PKs\n        return qs.exclude(pk__in=draft_version_pks)\n    else:\n        # TODO: Salvage more attributes from the original queryset, such as\n        # `annotate()`, `distinct()`, `select_related()`, `values()`, etc.\n        qs = qs.model.objects.filter(pk__in=published_version_pks)\n        # Restore ordering from original queryset.\n        qs = _order_by_pks(qs, published_version_pks)\n        return qs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _order_by_pks(qs, pks):\n    pk_colname = '%s.%s' % (\n        qs.model._meta.db_table, qs.model._meta.pk.column)\n    clauses = ' '.join(\n        ['WHEN %s=%s THEN %s' % (pk_colname, pk, i)\n            for i, pk in enumerate(pks)]\n    )\n    ordering = 'CASE %s END' % clauses\n    return qs.extra(\n        select={'pk_ordering': ordering}, order_by=('pk_ordering',))", "response": "Adjusts the given queryset to order items according to the explicit ordering\n    of PKs provided."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\noverride default iterator to wrap returned items in a publishing sanity-checker \"booby trap\" to lazily raise an exception if DRAFT items are mistakenly returned and mis-used in a public context where only PUBLISHED items should be used. This booby trap is added when all of: - the publishing middleware is active, and therefore able to report accurately whether the request is in a drafts-permitted context - the publishing middleware tells us we are not in a drafts-permitted context, which means only published items should be used.", "response": "def _queryset_iterator(qs):\n    \"\"\"\n    Override default iterator to wrap returned items in a publishing\n    sanity-checker \"booby trap\" to lazily raise an exception if DRAFT\n    items are mistakenly returned and mis-used in a public context\n    where only PUBLISHED items should be used.\n\n    This booby trap is added when all of:\n\n    - the publishing middleware is active, and therefore able to report\n    accurately whether the request is in a drafts-permitted context\n    - the publishing middleware tells us we are not in\n    a drafts-permitted context, which means only published items\n    should be used.\n    \"\"\"\n    # Avoid double-processing draft items in our custom iterator when we\n    # are in a `PublishingQuerySet` that is also a subclass of the\n    # monkey-patched `UrlNodeQuerySet`\n    if issubclass(type(qs), UrlNodeQuerySet):\n        super_without_boobytrap_iterator = super(UrlNodeQuerySet, qs)\n    else:\n        super_without_boobytrap_iterator = super(PublishingQuerySet, qs)\n\n    if is_publishing_middleware_active() \\\n            and not is_draft_request_context():\n        for item in super_without_boobytrap_iterator.iterator():\n            if getattr(item, 'publishing_is_draft', False):\n                yield DraftItemBoobyTrap(item)\n            else:\n                yield item\n    else:\n        for item in super_without_boobytrap_iterator.iterator():\n            yield item"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef published(self, for_user=UNSET, force_exchange=False):\n        # Re-interpret call to `published` from within Fluent to our\n        # `visible` implementation, if the `for_user` kwarg is provided by the\n        # calling code with a value other than `UNSET`. We use the `UNSET` flag\n        # so we can distinguish calls that actually include `for_user`.\n        # NOTE: This could well come straight back to us if `visible` calls\n        # `published`, but the 'for_user' value will have been stripped.\n        # NOTE 2: The ``for_user`` keyword argument is a no-op for us, we\n        # support it solely for compatibility with Fluent code that expects it.\n        if for_user is not UNSET:\n            return self.visible()\n\n        queryset = self.all()\n        if force_exchange or self.exchange_on_published:\n            # Exclude any draft items without a published copy. We keep all\n            # published copy items, and draft items with a published copy, so\n            # result may contain both draft and published items.\n            queryset = queryset.exclude(\n                publishing_is_draft=True, publishing_linked=None)\n            # Return only published items, exchanging draft items for their\n            # published copies where necessary.\n            return queryset.exchange_for_published()\n        else:\n            # No draft-to-published exchange requested, use simple constraint\n            return queryset.filter(publishing_is_draft=False)", "response": "Returns a new queryset that contains published items."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef only(self, *args, **kwargs):\n        field_names = args\n        if 'publishing_is_draft' not in field_names:\n            field_names += ('publishing_is_draft',)\n        return super(PublishingQuerySet, self) \\\n            .only(*field_names, **kwargs)", "response": "Override default implementation to ensure that the item\n        is not published when only is invoked."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef published(self, for_user=UNSET, force_exchange=False):\n        if for_user is not UNSET:\n            return self.visible()\n\n        queryset = super(PublishingUrlNodeQuerySet, self).published(\n            for_user=for_user, force_exchange=force_exchange)\n\n        # Exclude by publication date on the published version of items, *not*\n        # the draft vesion, or we could get the wrong result.\n        # Exclude fields of published copy of draft items, not draft itself...\n        queryset = queryset.exclude(\n            Q(publishing_is_draft=True) & Q(\n                Q(publishing_linked__publication_date__gt=now())\n                | Q(publishing_linked__publication_end_date__lte=now())))\n        # ...and exclude fields directly on published items\n        queryset = queryset.exclude(\n            Q(publishing_is_draft=False) & Q(\n                Q(publication_date__gt=now())\n                | Q(publication_end_date__lte=now())))\n\n        return queryset", "response": "Returns a new QuerySet with published items."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a QuerySet of published items for this node.", "response": "def published(self, for_user=None, force_exchange=True):\n        \"\"\"\n        Customise `UrlNodeQuerySet.published()` to add filtering by publication\n        date constraints and exchange of draft items for published ones.\n        \"\"\"\n        qs = self._single_site()\n        # Avoid filtering to only published items when we are in a draft\n        # context and we know this method is triggered by Fluent (because\n        # the `for_user` is present) because we may actually want to find\n        # and return draft items to priveleged users in this situation.\n        if for_user and is_draft_request_context():\n            return qs\n\n        if for_user is not None and for_user.is_staff:\n            pass  # Don't filter by publication date for Staff\n        else:\n            qs = qs.filter(\n                    Q(publication_date__isnull=True) |\n                    Q(publication_date__lt=now())\n                ).filter(\n                    Q(publication_end_date__isnull=True) |\n                    Q(publication_end_date__gte=now())\n                )\n        if force_exchange:\n            return _exchange_for_published(qs)\n        else:\n            return qs.filter(status=UrlNode.PUBLISHED)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef redis_from_url(url):\n    # Makes sure that we only try to import redis when we need\n    # to use it\n    import redis\n\n    url = url or \"\"\n    parsed_url = urlparse(url)\n    if parsed_url.scheme != \"redis\":\n        return None\n\n    kwargs = {}\n    match = PASS_HOST_PORT.match(parsed_url.netloc)\n    if match.group('password') is not None:\n        kwargs['password'] = match.group('password')\n    if match.group('host') is not None:\n        kwargs['host'] = match.group('host')\n    if match.group('port') is not None:\n        kwargs['port'] = int(match.group('port'))\n\n    if len(parsed_url.path) > 1:\n        # Removes \"/\" from the beginning\n        kwargs['db'] = int(parsed_url.path[1:])\n\n    return redis.StrictRedis(**kwargs)", "response": "Converts a redis URL used by celery into a redis. Redis object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef process_model_scores(self, model_names, root_cache,\n                             include_features=False):\n        \"\"\"\n        Generates a score map for a set of models based on a `root_cache`.\n        This method performs no substantial IO, but may incur substantial CPU\n        usage.\n\n        :Parameters:\n            model_names : `set` ( `str` )\n                A set of models to score\n            root_cache : `dict` ( `str` --> `mixed` )\n                A cache of pre-computed root_dependencies for a specific\n                revision.  See `extract_root_dependency_caches()`\n            include_features : `bool`\n                If True, include a map of basic features used in scoring along\n                with the model score.  If False, just generate the scores.\n        \"\"\"\n        model_scores = {}\n\n        for model_name in model_names:\n            model_scores[model_name] = {}\n\n            # Mostly CPU\n            model_scores[model_name]['score'] = \\\n                self._process_score(model_name, dependency_cache=root_cache)\n\n            # Essentially free\n            if include_features:\n                base_feature_map = self._solve_base_feature_map(\n                    model_name, dependency_cache=root_cache)\n                model_scores[model_name]['features'] = base_feature_map\n\n        return model_scores", "response": "Generates a score map for a set of models based on a root_cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _solve_features(self, model_name, dependency_cache=None):\n        features = self[model_name].features\n        return list(self.extractor.solve(features, cache=dependency_cache))", "response": "Solve the vector of features for a given model using the dependency_cache."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsolve the base feature map for a given model name.", "response": "def _solve_base_feature_map(self, model_name, dependency_cache=None):\n        \"\"\"\n        Solves the leaf :class:`revscoring.Feature` from the dependency for\n        `model_name` using `dependency_cache`.  This will return a mapping\n        between the `str` name of the base features and the solved values.\n        \"\"\"\n        features = list(trim(self[model_name].features))\n        feature_values = self.extractor.solve(features, cache=dependency_cache)\n        return {str(f): v\n                for f, v in zip(features, feature_values)}"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _process_score(self, model_name, dependency_cache=None):\n        version = self[model_name].version\n\n        start = time.time()\n        feature_values = self._solve_features(model_name, dependency_cache)\n        logger.debug(\"Extracted features for {0}:{1}:{2} in {3} secs\"\n                     .format(self.name, model_name, version,\n                             round(time.time() - start, 3)))\n\n        start = time.time()\n        score = self[model_name].score(feature_values)\n        logger.debug(\"Scored features for {0}:{1}:{2} in {3} secs\"\n                     .format(self.name, model_name, version,\n                             round(time.time() - start, 3)))\n\n        return score", "response": "Generates a score for a given model name and version."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef extract_root_dependency_caches(\n            self, model_names, rev_ids, injection_caches=None):\n        \"\"\"\n        Extracts a mapping of root :class:`revscoring.Datasource`\n        capable of generating the features needed for a particular set of\n        models without additional IO.  This method implements all of the IO\n        heavy operations.  The roots dependency caches produced by calling\n        this method can then be passed to `process_model_scores()` for scoring.\n\n        :Parameters:\n            model_names : `list` ( `str` )\n                The names of a :class:`revscoring.ScorerModel` to\n                extract the roots dependencies for\n        \"\"\"\n        # Make a copy of injection_caches\n        _injection_caches = {}\n        for rev_id in rev_ids:\n            injection_cache = injection_caches.get(rev_id, {}) \\\n                              if injection_caches is not None else {}\n            _injection_caches[rev_id] = dict(injection_cache.items())\n\n        # Find our root datasources\n        root_datasources = \\\n            list(set(self._generate_root_datasources(model_names)))\n\n        start = time.time()\n        error_root_vals = self.extractor.extract(\n            rev_ids, root_datasources, caches=_injection_caches)\n\n        # Check each extraction for errors\n        root_caches = {}\n        errors = {}\n        for rev_id, (error, values) in zip(rev_ids, error_root_vals):\n            if error is not None:\n                errors[rev_id] = error\n                if rev_id in root_caches:\n                    del root_caches[rev_id]\n            else:\n                root_caches[rev_id] = dict(zip(root_datasources, values))\n        logger.debug(\"Extracted root datasources for {0}:{1}:{2} in {3} secs\"\n                     .format(self.name, set(model_names), rev_ids,\n                             round(time.time() - start, 3)))\n\n        # Note that root_caches should have been modified in place\n        return root_caches, errors", "response": "Extracts a mapping of roots dependency caches for a particular set of models and their corresponding roots."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads a whole set of ScoringContext s from a configuration file.", "response": "def map_from_config(cls, config, context_names,\n                        section_key=\"scoring_contexts\"):\n        \"\"\"\n        Loads a whole set of ScoringContext's from a configuration file\n        while maintaining a cache of model names.  This aids in better memory\n        management and allows model aliases to be implemented at the\n        configuration level.\n\n        :Returns:\n            A map of context_names and ScoringContext's where models are loaded\n            once and reused cross contexts.\n        \"\"\"\n        model_key_map = {}\n        context_map = {}\n\n        for context_name in context_names:\n            section = config[section_key][context_name]\n            model_map = {}\n            for model_name, key in section['scorer_models'].items():\n                if key in model_key_map:\n                    scorer_model = model_key_map[key]\n                else:\n                    scorer_model = Model.from_config(config, key)\n                    model_key_map[key] = scorer_model\n                model_map[model_name] = scorer_model\n\n            extractor = Extractor.from_config(config, section['extractor'])\n            context_map[context_name] = cls(\n                context_name, model_map=model_map, extractor=extractor)\n\n        return context_map"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload a new object from a configuration file.", "response": "def from_config(cls, config, name, section_key=\"scoring_contexts\"):\n        \"\"\"\n        Expects:\n\n            scoring_contexts:\n                enwiki:\n                    scorer_models:\n                        damaging: enwiki_damaging_2014\n                        good-faith: enwiki_good-faith_2014\n                    extractor: enwiki\n                ptwiki:\n                    scorer_models:\n                        damaging: ptwiki_damaging_2014\n                        good-faith: ptwiki_good-faith_2014\n                    extractor: ptwiki\n\n            extractors:\n                enwiki_api: ...\n                ptwiki_api: ...\n\n            scorer_models:\n                enwiki_damaging_2014: ...\n                enwiki_good-faith_2014: ...\n        \"\"\"\n        logger.info(\"Loading {0} '{1}' from config.\".format(cls.__name__, name))\n        section = config[section_key][name]\n\n        model_map = {}\n        for model_name, key in section['scorer_models'].items():\n            scorer_model = Model.from_config(config, key)\n            model_map[model_name] = scorer_model\n\n        extractor = Extractor.from_config(config, section['extractor'])\n\n        return cls(name, model_map=model_map, extractor=extractor)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef format(self, rev_id=None, model_name=None):\n        rev_ids = rev_id if rev_id is not None else set(self.rev_ids)\n        model_names = model_name if model_name is not None else set(self.model_names)\n        common = [self.context_name, rev_ids, model_names]\n\n        optional = []\n        if self.precache:\n            optional.append(\"precache\")\n        if self.include_features:\n            optional.append(\"features\")\n        if self.injection_caches:\n            optional.append(\"injection_caches={0}\".format(self.injection_caches))\n        if self.model_info:\n            optional.append(\"model_info=\" + json.dumps(self.model_info))\n        if self.ip:\n            optional.append(\"ip={0}\".format(self.ip))\n\n        return \"{0}({1})\".format(\":\".join(repr(v) for v in common),\n                                 \", \".join(optional))", "response": "Format the log entry for a specific revision ID and model name."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef build_score_request(scoring_system, request, context_name=None, rev_id=None,\n                        model_name=None):\n    \"\"\"\n    Build an :class:`ores.ScoreRequest` from information contained in a\n    request.\n\n    :Parameters:\n        scoring_system : :class:`ores.ScoringSystem`\n            A scoring system to build request with\n        request : :class:`flask.Request`\n            A web request to extract information from\n        context_name : `str`\n            The name of the context to perform scoring\n        rev_id : int\n            The revision ID to score.  Note that multiple IDs can be provided\n            in `request.args`\n        model_name = `str`\n            The name of the model to score.  Note that multiple models can be\n            provided in `request.args`\n    \"\"\"\n    rev_ids = parse_rev_ids(request, rev_id)\n    model_names = parse_model_names(request, model_name)\n    precache = 'precache' in request.args\n    include_features = 'features' in request.args\n    injection_caches = parse_injection(request, rev_id)\n    model_info = parse_model_info(request)\n\n    if context_name and context_name in scoring_system and not model_names:\n        model_names = scoring_system[context_name].keys()\n\n    # WMF specific solution\n    if request.headers.get('X-Client-IP') is None:\n        ip = request.remote_addr.strip()\n    else:\n        ip = request.headers['X-Client-IP'].strip()\n\n    return ScoreRequest(context_name, rev_ids, model_names,\n                        precache=precache,\n                        include_features=include_features,\n                        injection_caches=injection_caches,\n                        model_info=model_info,\n                        ip=ip)", "response": "Builds a ScoreRequest object from a flask. Request object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_injection(request, rev_id):\n    cache = {}\n\n    if 'inject' in request.values:\n        try:\n            cache = json.loads(request.values['inject'])\n        except json.JSONDecodeError as e:\n            raise CacheParsingError(e)\n\n    if rev_id is not None:\n        rev_cache = cache\n\n        try:\n            for k, v in request.values.items():\n                if k.startswith((\"feature.\", \"datasource.\")):\n                    rev_cache[k] = json.loads(v)\n        except json.JSONDecodeError as e:\n            raise CacheParsingError(e)\n\n        if len(rev_cache) > 0:\n            cache = {rev_id: rev_cache}\n    else:\n        cache = {int(rev_id): c for rev_id, c in cache.items()}\n\n    return cache or None", "response": "Parse values for features and datasources of interest."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef build_event_set(event):\n    event_set = set()\n    if re.match(r\"([^\\.]+.)?mediawiki\\.revision-create$\",\n                event['meta']['topic']):\n        event_set.add('edit')\n\n        user_groups = event.get('performer', {}).get('user_groups', [])\n        if 'bot' in user_groups:\n            event_set.add('bot_edit')\n        else:\n            event_set.add('nonbot_edit')\n\n        if not event.get('rev_parent_id'):\n            event_set.add('page_creation')\n            if 'bot' in user_groups:\n                event_set.add('bot_page_creation')\n            else:\n                event_set.add('nonbot_page_creation')\n\n    return event_set", "response": "Turn an EventStream event into a set of event types that ORES\n    uses internally."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef build_precache_map(config):\n    precache_map = {}\n    ss_name = config['ores']['scoring_system']\n    for context in config['scoring_systems'][ss_name]['scoring_contexts']:\n        precache_map[context] = {}\n        for model in config['scoring_contexts'][context].get('precache', []):\n            precached_config = \\\n                config['scoring_contexts'][context]['precache'][model]\n\n            events = precached_config['on']\n            if len(set(events) - AVAILABLE_EVENTS) > 0:\n                logger.warning(\"{0} events are not available\"\n                               .format(set(events) - AVAILABLE_EVENTS))\n            for event in precached_config['on']:\n                if event in precache_map[context]:\n                    precache_map[context][event].add(model)\n                else:\n                    precache_map[context][event] = {model}\n                logger.debug(\"Setting up precaching for {0} in {1} on {2}\"\n                             .format(model, context, event))\n\n    return precache_map", "response": "Builds a mapping of contexts and models from the configuration."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_config(cls, config, name, section_key=\"score_caches\"):\n        sentinel_logger.info(\"Loading RedisSentinel '{0}' from config.\".format(name))\n        section = config[section_key][name]\n\n        kwargs = {k: v for k, v in section.items() if k != \"class\"}\n\n        return cls.from_parameters(**kwargs)", "response": "Load a RedisSentinel object from a configuration dictionary."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef calculate_statistics(self):\n        \"Jam some data through to generate statistics\"\n        rev_ids = range(0, 100, 1)\n        feature_values = zip(rev_ids, [0] * 100)\n        scores = [self.score(f) for f in feature_values]\n        labels = [s['prediction'] for s in scores]\n        statistics = Classification(labels, threshold_ndigits=1, decision_key='probability')\n        score_labels = list(zip(scores, labels))\n        statistics.fit(score_labels)\n        return statistics", "response": "Jam some data through to generate statistics"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read_hector_input(csv_file):\n    df = pd.read_csv(csv_file, skiprows=3, index_col=0)\n    df.name = os.path.splitext(os.path.basename(csv_file))[0]\n    return df", "response": "Reads a Hector CSV file and returns it as a Pandas DataFrame."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write_hector_input(scenario, path=None):\n\n    # Output header format:\n    # ; Scenario name\n    # ; Generated with pyhector\n    # ;UNITS:   GtC/yr  GtC/yr [...]\n    # Date      ffi_emissions   luc_emissions [...]\n\n    out = \"\"\n    try:\n        name = \"; \" + scenario.name + \"\\n\"\n    except AttributeError:\n        name = \"; Hector Scenario\\n\"\n    out += name\n\n    out += \"; Written with pyhector\\n\"\n    unit_names = [units[source] for source in scenario.columns]\n    out += \";UNITS:,\" + \",\".join(unit_names) + \"\\n\"\n    out += scenario.to_csv()\n\n    if isinstance(path, str):\n        f = open(path, \"w\")\n    elif path is None:\n        return out\n    else:\n        f = path\n    f.write(out)\n    if hasattr(f, \"close\"):\n        f.close()\n    return None", "response": "Writes a DataFrame with emissions to a CSV emissions file as used in Hector."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread a Hector contraint CSV file and returns it as a Pandas Series", "response": "def read_hector_constraint(constraint_file):\n    \"\"\"\n    Reads a Hector contraint CSV file and returns it as a Pandas Series\n    \"\"\"\n    df = pd.read_csv(constraint_file, index_col=0, comment=\";\")\n    df = df[df.applymap(lambda x: isinstance(x, (int, float)))]\n    df.index = df.index.astype(int)\n    return df.iloc[:, 0]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread a Hector output stream CSV file and returns a wide DataFrame with Hector output data.", "response": "def read_hector_output(csv_file):\n    \"\"\"\n    Reads a Hector output stream CSV file and returns a wide DataFrame with\n    Hector output data.\n    \"\"\"\n    # Filter out spin-up values. In Hector 1.x RCP output streams years are\n    # given as end of simulation year.\n    # See https://github.com/JGCRI/hector/issues/177\n    start_year = 1746\n    output_stream = pd.read_csv(csv_file, skiprows=1)\n\n    wide = output_stream[output_stream.year >= start_year].pivot_table(\n        index=\"year\", columns=\"variable\", values=\"value\"\n    )\n\n    return wide"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run(scenario, config=None, base_config=None, outputs=None, return_config=False):\n    if outputs is None:\n        outputs = [\"temperature.Tgav\", \"simpleNbox.Ca\", \"forcing.Ftot\"]\n    if base_config is None:\n        parameters = deepcopy(_default_config)\n    else:\n        parameters = deepcopy(base_config)\n    if config:\n        for key, data in config.items():\n            for option, value in data.items():\n                parameters[key][option] = value\n    with Hector() as h:\n        h.config(parameters)\n        h.set_emissions(scenario)\n        if outputs == \"all\":\n            outputs = output.keys()\n        for name in outputs:\n            h.add_observable(\n                output[name][\"component\"],\n                output[name][\"variable\"],\n                output[name].get(\"needs_date\", False),\n            )\n        h.run()\n        results = {}\n        for name in outputs:\n            results[name] = h.get_observable(\n                output[name][\"component\"], output[name][\"variable\"]\n            )\n\n        # In Hector 1.x output value years are given as end of simulation\n        # year, e.g. 1745-12-31 = 1746.0.\n        # See https://github.com/JGCRI/hector/issues/177\n        start = int(parameters[\"core\"][\"startDate\"]) + 1\n        # End of range is non-inclusive in Python ranges.\n        end = int(parameters[\"core\"][\"endDate\"]) + 1\n        index = np.arange(start, end)\n        results = pd.DataFrame(results, index=index)\n    if return_config:\n        return results, parameters\n    return results", "response": "Runs a scenario through the Hector climate model."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_value(self, section, variable, value):\n        if isinstance(value, pd.Series):  # values with time as Series\n            values = list(zip(value.index, value))\n            for v in values:\n                self._set_timed_double(section, variable, v[0], v[1])\n        elif isinstance(value, list):  # values with time\n            for v in value:\n                if len(v) == 3:  # timed value with unit\n                    self._set_timed_double_unit(section, variable, v[0], v[1], v[2])\n                else:  # timed value without unit\n                    self._set_timed_double(section, variable, v[0], v[1])\n        elif isinstance(value, tuple):  # value with unit\n            self._set_double_unit(section, variable, value[0], value[1])\n        elif isinstance(value, str):  # value is string\n            self._set_string(section, variable, value)\n        else:  # value is only double\n            self._set_double(section, variable, value)", "response": "Set config input value directly."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef config(self, config):\n        for section, data in config.items():\n            for variable, value in data.items():\n                self.set_value(section, variable, value)", "response": "Set config values from config dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting emissions from Pandas DataFrame.", "response": "def set_emissions(self, scenario):\n        \"\"\"Set emissions from Pandas DataFrame.\"\"\"\n        for section in emissions:\n            for source in emissions[section]:\n                if source not in scenario.columns:\n                    continue\n                self._set_timed_array(\n                    section, source, list(scenario.index), list(scenario[source])\n                )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef format_v1_score_response(response, limit_to_model=None):\n    response_doc = defaultdict(dict)\n    for rev_id, rev_scores in response.scores.items():\n        for model_name, score in rev_scores.items():\n            response_doc[rev_id][model_name] = score\n\n    for rev_id, rev_errors in response.errors.items():\n        for model_name, error in rev_errors.items():\n            response_doc[rev_id][model_name] = util.format_error(error)\n\n    if limit_to_model is not None:\n        return util.jsonify({rev_id: model_scores[limit_to_model]\n                            for rev_id, model_scores in response_doc.items()})\n    else:\n        return util.jsonify(response_doc)", "response": "Formats the response to be returned by the v1 score API."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the score of a set of models and revision IDs.", "response": "def score(self, context, models, revids):\n        \"\"\"\n        Genetate scores for model applied to a sequence of revisions.\n\n        :Parameters:\n            context : str\n                The name of the context -- usually the database name of a wiki\n            models : `iterable`\n                The names of a models to apply\n            revids : `iterable`\n                A sequence of revision IDs to score.\n        \"\"\"\n        if isinstance(revids, int):\n            rev_ids = [revids]\n        else:\n            rev_ids = [int(rid) for rid in revids]\n\n        return self._score(context, models, rev_ids)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef format_v2_score_response(request, response):\n    return util.jsonify({\"scores\": {\n        response.context.name: {\n            model_name: format_v2_model(request, response, model_name)\n            for model_name in response.request.model_names}}})", "response": "Formats the response for v2 scores response."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nbuild a v2 context model map for the given score request.", "response": "def build_v2_context_model_map(score_request, scoring_system):\n    \"\"\"\n    {\n        \"scores\": {\n            \"<context>\": {\n                \"<model_name>\": {\n                    \"version\": \"<model_version>\",\n                    \"info\": <model_info>\n                }\n            },\n            \"<context>\": {\n                \"<model_name>\": {\n                    \"version\": \"<model_version>\",\n                    \"info\": <model_info>\n                }\n            }\n        }\n    \"\"\"\n    try:\n        context_models_doc = {}\n        for context_name, context in scoring_system.items():\n            context_models_doc[context_name] = {}\n            for model_name in context:\n                model_doc = {'version': context.model_version(model_name)}\n                if score_request.model_info:\n                    model_doc['info'] = context.format_model_info(\n                        model_name, score_request.model_info)\n                context_models_doc[context_name][model_name] = model_doc\n        return util.jsonify({'scores': context_models_doc})\n    except Exception:\n        return responses.unknown_error(traceback.format_exc())"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef format_v3_score_response(response):\n    context_doc = defaultdict(lambda: defaultdict(dict))\n    if len(response.scores) > 0 or len(response.errors) > 0:\n        for rev_id, rev_scores in response.scores.items():\n            for model_name, score in rev_scores.items():\n                context_doc['scores'][rev_id][model_name] = \\\n                    {'score': score}\n\n        for rev_id, rev_errors in response.errors.items():\n            for model_name, error in rev_errors.items():\n                context_doc['scores'][rev_id][model_name] = \\\n                    util.format_error(error)\n\n        for rev_id, rev_features in response.features.items():\n            for model_name, features in rev_features.items():\n                context_doc['scores'][rev_id][model_name]['features'] = \\\n                    features\n\n    if len(response.model_info) > 0:\n        context_doc['models'] = {\n            model_name: info_doc\n            for model_name, info_doc in response.model_info.items()}\n\n    return util.jsonify({response.context.name: context_doc})", "response": "Formats the V3 score response."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntakes an array like [ 1 5 0 0", "response": "def nice_indices(arr):\n    '''\n    takes an array like [1,1,5,5,5,999,1,1]\n    and maps to something like [0,0,1,1,1,2,0,0]\n    modifies original in place as well as returns a ref\n    '''\n    # surprisingly, this is slower for very small (and very large) inputs:\n    # u,f,i = np.unique(arr,return_index=True,return_inverse=True)\n    # arr[:] = np.arange(u.shape[0])[np.argsort(f)][i]\n    ids = collections.defaultdict(count().__next__)\n    for idx,x in enumerate(arr):\n        arr[idx] = ids[x]\n    return arr"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfinds the principal angle between two subspaces A and B.", "response": "def principal_angle(A,B):\n    \"\"\"\n    Find the principal angle between two subspaces\n    spanned by columns of A and B\n    \"\"\"\n    from numpy.linalg import qr, svd\n    qA, _ = qr(A)\n    qB, _ = qr(B)\n    U,S,V = svd(qA.T.dot(qB))\n    return np.arccos(min(S.min(), 1.0))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nintroduces a mask that allows for missing data", "response": "def resample(self, data, stats=None, mask=None, niter=None):\n        \"\"\"\n        Introduce a mask that allows for missing data\n        \"\"\"\n        stats = self._get_statistics(data, mask=mask) if stats is None else stats\n        stats = self._stats_ensure_array(stats)\n\n        niter = niter if niter else self.niter\n        for itr in range(niter):\n            self._resample_A(stats)\n            self._resample_sigma(stats)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating the degree of freedom parameter with Metropolis-Hastings. Assume a prior nu ~ Ga(alpha, beta) and use a proposal nu' ~ N(nu, prop_std^2). If proposals are negative, reject automatically due to likelihood.", "response": "def _resample_nu(self, tau, N_steps=100, prop_std=0.1, alpha=1, beta=1):\n        \"\"\"\n        Update the degree of freedom parameter with \n        Metropolis-Hastings. Assume a prior nu ~ Ga(alpha, beta) \n        and use a proposal nu' ~ N(nu, prop_std^2). If proposals\n        are negative, reject automatically due to likelihood.\n        \"\"\"\n        # Convert tau to a list of arrays\n        taus = [tau] if isinstance(tau, np.ndarray) else tau\n\n        N = 0\n        E_tau = 0\n        E_logtau = 0\n        for tau in taus:\n            bad = ~np.isfinite(tau)\n            N += np.sum(~bad)\n            E_tau += np.sum(tau[~bad])\n            E_logtau += np.sum(np.log(tau[~bad]))\n\n        if N > 0:\n            E_tau /= N\n            E_logtau /= N\n        \n        # Compute the log prior, likelihood, and posterior\n        lprior = lambda nu: (alpha - 1) * np.log(nu) - alpha * nu\n        ll = lambda nu: N * (nu/2 * np.log(nu/2)  - gammaln(nu/2) + (nu/2 - 1) * E_logtau - nu/2 * E_tau)\n        lp = lambda nu: ll(nu) + lprior(nu)\n\n        lp_curr = lp(self.nu)\n        for step in range(N_steps):\n            # Symmetric proposal\n            nu_new = self.nu + prop_std * np.random.randn()\n            if nu_new <1e-3:\n                # Reject if too small\n                continue\n\n            # Accept / reject based on likelihoods\n            lp_new = lp(nu_new)\n            if np.log(np.random.rand()) < lp_new - lp_curr:\n                self.nu = nu_new\n                lp_curr = lp_new"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsampling a truncated normal with the specified params.", "response": "def sample_truncated_gaussian(mu=0, sigma=1, lb=-np.Inf, ub=np.Inf):\n    \"\"\"\n    Sample a truncated normal with the specified params. This\n    is not the most stable way but it works as long as the\n    truncation region is not too far from the mean.\n    \"\"\"\n    # Broadcast arrays to be of the same shape\n    mu, sigma, lb, ub = np.broadcast_arrays(mu, sigma, lb, ub)\n    shp = mu.shape\n    if np.allclose(sigma, 0.0):\n        return mu\n\n    cdflb = normal_cdf(lb, mu, sigma)\n    cdfub = normal_cdf(ub, mu, sigma)\n\n    # Sample uniformly from the CDF\n    cdfsamples = cdflb + np.random.rand(*shp) * (cdfub-cdflb)\n\n    # Clip the CDF samples so that we can invert them\n    cdfsamples = np.clip(cdfsamples, 1e-15, 1-1e-15)\n    zs = -np.sqrt(2) * special.erfcinv(2 * cdfsamples)\n\n    # Transform the standard normal samples\n    xs = sigma * zs + mu\n    xs = np.clip(xs, lb, ub)\n\n    return xs"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsample log probability array along specified axis", "response": "def sample_discrete_from_log(p_log,return_lognorms=False,axis=0,dtype=np.int32):\n    'samples log probability array along specified axis'\n    lognorms = logsumexp(p_log,axis=axis)\n    cumvals = np.exp(p_log - np.expand_dims(lognorms,axis)).cumsum(axis)\n    thesize = np.array(p_log.shape)\n    thesize[axis] = 1\n    randvals = random(size=thesize) * \\\n            np.reshape(cumvals[[slice(None) if i is not axis else -1\n                for i in range(p_log.ndim)]],thesize)\n    samples = np.sum(randvals > cumvals,axis=axis,dtype=dtype)\n    if return_lognorms:\n        return samples, lognorms\n    else:\n        return samples"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef BIC(self,data=None):\n        '''\n        BIC on the passed data.\n        If passed data is None (default), calculates BIC on the model's assigned data.\n        '''\n        # NOTE: in principle this method computes the BIC only after finding the\n        # maximum likelihood parameters (or, of course, an EM fixed-point as an\n        # approximation!)\n        if data is None:\n            assert len(self.labels_list) > 0, \\\n                    \"If not passing in data, the class must already have it. Use the method add_data()\"\n            return -2*sum(self.log_likelihood(l.data) for l in self.labels_list) + \\\n                        self.num_parameters * np.log(sum(l.data.shape[0] for l in self.labels_list))\n        else:\n            return -2*self.log_likelihood(data) + self.num_parameters * np.log(data.shape[0])", "response": "Calculates the BIC on the passed data."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmerges a single sample into a new sample.", "response": "def merge_sample(sample, sample_subann,\n                 data_sources=None, derived_attributes=None):\n    \"\"\"\n    Use merge table (subannotation) data to augment/modify Sample.\n\n    :param Sample sample: sample to modify via merge table data\n    :param sample_subann: data with which to alter Sample\n    :param Mapping data_sources: collection of named paths to data locations,\n        optional\n    :param Iterable[str] derived_attributes: names of attributes for which\n        corresponding Sample attribute's value is data-derived, optional\n    :return Set[str]: names of columns/attributes that were merged\n    \"\"\"\n\n    merged_attrs = {}\n\n    if sample_subann is None:\n        _LOGGER.log(5, \"No data for sample merge, skipping\")\n        return merged_attrs\n\n    if SAMPLE_NAME_COLNAME not in sample_subann.columns:\n        raise KeyError(\n            \"Merge table requires a column named '{}'.\".\n                format(SAMPLE_NAME_COLNAME))\n\n    _LOGGER.debug(\"Merging Sample with data sources: {}\".\n                  format(data_sources))\n\n    # Hash derived columns for faster lookup in case of many samples/columns.\n    derived_attributes = set(derived_attributes or [])\n    _LOGGER.debug(\"Merging Sample with derived attributes: {}\".\n                  format(derived_attributes))\n\n    sample_name = getattr(sample, SAMPLE_NAME_COLNAME)\n    sample_indexer = sample_subann[SAMPLE_NAME_COLNAME] == sample_name\n    this_sample_rows = sample_subann[sample_indexer]\n    if len(this_sample_rows) == 0:\n        _LOGGER.debug(\"No merge rows for sample '%s', skipping\", sample.name)\n        return merged_attrs\n    _LOGGER.log(5, \"%d rows to merge\", len(this_sample_rows))\n    _LOGGER.log(5, \"Merge rows dict: {}\".format(this_sample_rows.to_dict()))\n\n    # For each row in the merge table of this sample:\n    # 1) populate any derived columns\n    # 2) derived columns --> space-delimited strings\n    # 3) update the sample values with the merge table\n    # Keep track of merged cols,\n    # so we don't re-derive them later.\n    merged_attrs = {key: \"\" for key in this_sample_rows.columns}\n    subsamples = []\n    _LOGGER.debug(this_sample_rows)\n    for subsample_row_id, row in this_sample_rows.iterrows():\n        try:\n            row['subsample_name']\n        except KeyError:\n            # default to a numeric count on subsamples if they aren't named\n            row['subsample_name'] = str(subsample_row_id)\n        subann_unit = Subsample(row)\n        subsamples.append(subann_unit)\n        _LOGGER.debug(subsamples)\n        rowdata = row.to_dict()\n\n        # Iterate over column names to avoid Python3 RuntimeError for\n        # during-iteration change of dictionary size.\n        for attr_name in this_sample_rows.columns:\n            if attr_name == SAMPLE_NAME_COLNAME or \\\n                            attr_name not in derived_attributes:\n                _LOGGER.log(5, \"Skipping merger of attribute '%s'\", attr_name)\n                continue\n\n            attr_value = rowdata[attr_name]\n\n            # Initialize key in parent dict.\n            col_key = attr_name + COL_KEY_SUFFIX\n            merged_attrs[col_key] = \"\"\n            rowdata[col_key] = attr_value\n            data_src_path = sample.locate_data_source(\n                data_sources, attr_name, source_key=rowdata[attr_name],\n                extra_vars=rowdata)  # 1)\n            rowdata[attr_name] = data_src_path\n\n        _LOGGER.log(5, \"Adding derived attributes\")\n\n        for attr in derived_attributes:\n\n            # Skip over any attributes that the sample lacks or that are\n            # covered by the data from the current (row's) data.\n            if not hasattr(sample, attr) or attr in rowdata:\n                _LOGGER.log(5, \"Skipping column: '%s'\", attr)\n                continue\n\n            # Map key to sample's value for the attribute given by column name.\n            col_key = attr + COL_KEY_SUFFIX\n            rowdata[col_key] = getattr(sample, attr)\n            # Map the col/attr name itself to the populated data source \n            # template string.\n            rowdata[attr] = sample.locate_data_source(\n                data_sources, attr, source_key=getattr(sample, attr),\n                extra_vars=rowdata)\n\n        # TODO: this (below) is where we could maintain grouped values\n        # TODO (cont.): as a collection and defer the true merger.\n\n        # Since we are now jamming multiple (merged) entries into a single\n        # attribute on a Sample, we have to join the individual items into a\n        # space-delimited string and then use that value as the Sample\n        # attribute. The intended use case for this sort of merge is for\n        # multiple data source paths associated with a single Sample, hence\n        # the choice of space-delimited string as the joined-/merged-entry\n        # format--it's what's most amenable to use in building up an argument\n        # string for a pipeline command.\n        for attname, attval in rowdata.items():\n            if attname == SAMPLE_NAME_COLNAME or not attval:\n                _LOGGER.log(5, \"Skipping KV: {}={}\".format(attname, attval))\n                continue\n            _LOGGER.log(5, \"merge: sample '%s'; '%s'='%s'\",\n                        str(sample.name), str(attname), str(attval))\n            if attname not in merged_attrs:\n                new_attval = str(attval).rstrip()\n            else:\n                new_attval = \"{} {}\".format(merged_attrs[attname],\n                                            str(attval)).strip()\n            merged_attrs[attname] = new_attval  # 2)\n            _LOGGER.log(5, \"Stored '%s' as value for '%s' in merged_attrs\",\n                        new_attval, attname)\n\n    # If present, remove sample name from the data with which to update sample.\n    merged_attrs.pop(SAMPLE_NAME_COLNAME, None)\n\n    _LOGGER.log(5, \"Updating Sample {}: {}\".format(sample.name, merged_attrs))\n    sample.update(merged_attrs)  # 3)\n    sample.merged_cols = merged_attrs\n    sample.merged = True\n    sample.subsamples = subsamples\n\n    return sample"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef debug_print(self):\n\n        ring = self._fetch_all()\n\n        print('Hash ring \"{key}\" replicas:'.format(key=self.key))\n\n        now = time.time()\n        n_replicas = len(ring)\n        if ring:\n            print('{:10} {:6} {:7} {}'.format('Start', 'Range', 'Delay', 'Node'))\n        else:\n            print('(no replicas)')\n\n        nodes = collections.defaultdict(list)\n\n        for n, (start, replica, heartbeat, expired) in enumerate(ring):\n            hostname, pid, rnd = replica.split(':')\n            node = ':'.join([hostname, pid])\n\n            abs_size = (ring[(n+1) % n_replicas][0] - ring[n][0]) % RING_SIZE\n            size = 100. / RING_SIZE * abs_size\n            delay = int(now - heartbeat)\n\n            nodes[node].append((hostname, pid, abs_size, delay, expired))\n\n            print('{start:10} {size:5.2f}% {delay:6}s {replica}{extra}'.format(\n                start=start,\n                replica=replica,\n                delay=delay,\n                size=size,\n                extra=' (EXPIRED)' if expired else ''\n            ))\n\n        print()\n        print('Hash ring \"{key}\" nodes:'.format(key=self.key))\n\n        if nodes:\n            print('{:8} {:8} {:7} {:20} {:5}'.format('Range', 'Replicas', 'Delay', 'Hostname', 'PID'))\n        else:\n            print('(no nodes)')\n\n        for k, v in nodes.items():\n            hostname, pid = v[0][0], v[0][1]\n            abs_size = sum(replica[2] for replica in v)\n            size = 100. / RING_SIZE * abs_size\n            delay = max(replica[3] for replica in v)\n            expired = any(replica[4] for replica in v)\n            count = len(v)\n            print('{size:5.2f}% {count:8} {delay:6}s {hostname:20} {pid:5}{extra}'.format(\n                start=start,\n                count=count,\n                hostname=hostname,\n                pid=pid,\n                delay=delay,\n                size=size,\n                extra=' (EXPIRED)' if expired else ''\n            ))", "response": "Prints the ring for debugging purposes."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef heartbeat(self):\n        pipeline = self.conn.pipeline()\n        now = time.time()\n        for replica in self.replicas:\n            pipeline.zadd(self.key, '{start}:{name}'.format(\n                start=replica[0],\n                name=replica[1]\n            ), now)\n        ret = pipeline.execute()\n\n        # Only notify the other nodes if we're not in the ring yet.\n        if any(ret):\n            self._notify()", "response": "Add the node in Redis and update the node in Redis."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving the node from the ring.", "response": "def remove(self):\n        \"\"\"\n        Call this to remove the node/replicas from the ring.\n        \"\"\"\n        pipeline = self.conn.pipeline()\n        for replica in self.replicas:\n            pipeline.zrem(self.key, '{start}:{name}'.format(\n                start=replica[0],\n                name=replica[1]\n            ))\n        pipeline.execute()\n        self._notify()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cleanup(self):\n        now = time.time()\n        expired = now - NODE_TIMEOUT\n        if self.conn.zremrangebyscore(self.key, 0, expired):\n            self._notify()", "response": "Removes expired nodes and replicas from the ring."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update(self):\n        ring = self._fetch()\n        n_replicas = len(ring)\n        replica_set = set([r[1] for r in self.replicas])\n        self.ranges = []\n        for n, (start, replica) in enumerate(ring):\n            if replica in replica_set:\n                end = ring[(n+1) % n_replicas][0] % RING_SIZE\n                if start < end:\n                    self.ranges.append((start, end))\n                elif end < start:\n                    self.ranges.append((start, RING_SIZE))\n                    self.ranges.append((0, end))\n                else:\n                    self.ranges.append((0, RING_SIZE))", "response": "Updates the current ranges with the updated ones."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a boolean indicating if this node is responsible for handling the given key.", "response": "def contains(self, key):\n        \"\"\"\n        Returns a boolean indicating if this node is responsible for handling\n        the given key.\n        \"\"\"\n        n = binascii.crc32(key.encode()) % RING_SIZE\n        for start, end in self.ranges:\n            if start <= n < end:\n                return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstarts the gevent - based application.", "response": "def gevent_start(self):\n        \"\"\"\n        Helper method to start the node for gevent-based applications.\n        \"\"\"\n        import gevent\n        import gevent.select\n        self._poller_greenlet = gevent.spawn(self.poll)\n        self._select = gevent.select.select\n        self.heartbeat()\n        self.update()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nstopping the gevent - based application.", "response": "def gevent_stop(self):\n        \"\"\"\n        Helper method to stop the node for gevent-based applications.\n        \"\"\"\n        import gevent\n        gevent.kill(self._poller_greenlet)\n        self.remove()\n        self._select = select.select"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsuggests that the given project contains what could be implied attributes", "response": "def suggest_implied_attributes(prj):\n    \"\"\"\n    If given project contains what could be implied attributes, suggest that.\n\n    :param Iterable prj: Intent is a Project, but this could be any iterable\n        of strings to check for suitability of declaration as implied attr\n    :return list[str]: (likely empty) list of warning messages about project\n        config keys that could be implied attributes\n    \"\"\"\n    def suggest(key):\n        return \"To declare {}, consider using {}\".format(\n            key, IMPLICATIONS_DECLARATION)\n    return [suggest(k) for k in prj if k in IDEALLY_IMPLIED]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef alpha_cased(text, lower=False):\n    text = \"\".join(filter(\n            lambda c: c.isalpha() or c == GENERIC_PROTOCOL_KEY, text))\n    return text.lower() if lower else text.upper()", "response": "Filter text to just letters with homogenized case."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck reads in BAM file for read type and lengths.", "response": "def check_bam(bam, o):\n    \"\"\"\n    Check reads in BAM file for read type and lengths.\n\n    :param str bam: BAM file path.\n    :param int o: Number of reads to look at for estimation.\n    \"\"\"\n    try:\n        p = sp.Popen(['samtools', 'view', bam], stdout=sp.PIPE)\n        # Count paired alignments\n        paired = 0\n        read_lengths = defaultdict(int)\n        while o > 0:  # Count down number of lines\n            line = p.stdout.readline().decode().split(\"\\t\")\n            flag = int(line[1])\n            read_lengths[len(line[9])] += 1\n            if 1 & flag:  # check decimal flag contains 1 (paired)\n                paired += 1\n            o -= 1\n        p.kill()\n    except OSError:\n        reason = \"Note (samtools not in path): For NGS inputs, \" \\\n                 \"pep needs samtools to auto-populate \" \\\n                 \"'read_length' and 'read_type' attributes; \" \\\n                 \"these attributes were not populated.\"\n        raise OSError(reason)\n\n    _LOGGER.debug(\"Read lengths: {}\".format(read_lengths))\n    _LOGGER.debug(\"paired: {}\".format(paired))\n    return read_lengths, paired"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nexpanding a filesystem path that may or may not contain user and env vars.", "response": "def expandpath(path):\n    \"\"\"\n    Expand a filesystem path that may or may not contain user/env vars.\n\n    :param str path: path to expand\n    :return str: expanded version of input path\n    \"\"\"\n    return os.path.expandvars(os.path.expanduser(path)).replace(\"//\", \"/\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget size of a file in gigabytes.", "response": "def get_file_size(filename):\n    \"\"\"\n    Get size of all files in gigabytes (Gb).\n\n    :param str | collections.Iterable[str] filename: A space-separated\n        string or list of space-separated strings of absolute file paths.\n    :return float: size of file(s), in gigabytes.\n    \"\"\"\n    if filename is None:\n        return float(0)\n    if type(filename) is list:\n        return float(sum([get_file_size(x) for x in filename]))\n    try:\n        total_bytes = sum([float(os.stat(f).st_size)\n                           for f in filename.split(\" \") if f is not ''])\n    except OSError:\n        # File not found\n        return 0.0\n    else:\n        return float(total_bytes) / (1024 ** 3)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of samples for the specified project.", "response": "def fetch_samples(proj, selector_attribute=None, selector_include=None, selector_exclude=None):\n    \"\"\"\n    Collect samples of particular protocol(s).\n\n    Protocols can't be both positively selected for and negatively\n    selected against. That is, it makes no sense and is not allowed to\n    specify both selector_include and selector_exclude protocols. On the other hand, if\n    neither is provided, all of the Project's Samples are returned.\n    If selector_include is specified, Samples without a protocol will be excluded,\n    but if selector_exclude is specified, protocol-less Samples will be included.\n\n    :param Project proj: the Project with Samples to fetch\n    :param Project str: the sample selector_attribute to select for\n    :param Iterable[str] | str selector_include: protocol(s) of interest;\n        if specified, a Sample must\n    :param Iterable[str] | str selector_exclude: protocol(s) to include\n    :return list[Sample]: Collection of this Project's samples with\n        protocol that either matches one of those in selector_include, or either\n        lacks a protocol or does not match one of those in selector_exclude\n    :raise TypeError: if both selector_include and selector_exclude protocols are\n        specified; TypeError since it's basically providing two arguments\n        when only one is accepted, so remain consistent with vanilla Python2\n    \"\"\"\n    if selector_attribute is None or (not selector_include and not selector_exclude):\n        # Simple; keep all samples.  In this case, this function simply\n        # offers a list rather than an iterator.\n        return list(proj.samples)\n\n    # At least one of the samples has to have the specified attribute\n    if proj.samples and not any([hasattr(i, selector_attribute) for i in proj.samples]):\n        raise AttributeError(\"The Project samples do not have the attribute '{attr}'\"\n                             .format(attr=selector_attribute))\n\n    # Intersection between selector_include and selector_exclude is nonsense user error.\n    if selector_include and selector_exclude:\n        raise TypeError(\"Specify only selector_include or selector_exclude parameter, \"\n                         \"not both.\")\n\n    # Ensure that we're working with sets.\n    def make_set(items):\n        if isinstance(items, str):\n            items = [items]\n        return items\n\n    # Use the attr check here rather than exception block in case the\n    # hypothetical AttributeError would occur; we want such\n    # an exception to arise, not to catch it as if the Sample lacks \"protocol\"\n    if not selector_include:\n        # Loose; keep all samples not in the selector_exclude.\n        def keep(s):\n            return not hasattr(s, selector_attribute) or \\\n                   getattr(s, selector_attribute) not in make_set(selector_exclude)\n    else:\n        # Strict; keep only samples in the selector_include.\n        def keep(s):\n            return hasattr(s, selector_attribute) and \\\n                   getattr(s, selector_attribute) in make_set(selector_include)\n\n    return list(filter(keep, proj.samples))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngrabbing Sample - independent data from the given Project.", "response": "def grab_project_data(prj):\n    \"\"\"\n    From the given Project, grab Sample-independent data.\n\n    There are some aspects of a Project of which it's beneficial for a Sample\n    to be aware, particularly for post-hoc analysis. Since Sample objects\n    within a Project are mutually independent, though, each doesn't need to\n    know about any of the others. A Project manages its, Sample instances,\n    so for each Sample knowledge of Project data is limited. This method\n    facilitates adoption of that conceptual model.\n\n    :param Project prj: Project from which to grab data\n    :return Mapping: Sample-independent data sections from given Project\n    \"\"\"\n    if not prj:\n        return {}\n    data = {}\n    for section in SAMPLE_INDEPENDENT_PROJECT_SECTIONS:\n        try:\n            data[section] = getattr(prj, section)\n        except AttributeError:\n            _LOGGER.debug(\"Project lacks section '%s', skipping\", section)\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nimports a module from a particular filesystem location.", "response": "def import_from_source(module_filepath):\n    \"\"\"\n    Import a module from a particular filesystem location.\n\n    :param str module_filepath: path to the file that constitutes the module\n        to import\n    :return module: module imported from the given location, named as indicated\n    :raises ValueError: if path provided does not point to an extant file\n    \"\"\"\n    import sys\n\n    if not os.path.exists(module_filepath):\n        raise ValueError(\"Path to alleged module file doesn't point to an \"\n                         \"extant file: '{}'\".format(module_filepath))\n\n    # Randomly generate module name.\n    fname_chars = string.ascii_letters + string.digits\n    name = \"\".join(random.choice(fname_chars) for _ in range(20))\n\n    # Import logic is version-dependent.\n    if sys.version_info >= (3, 5):\n        from importlib import util as _il_util\n        modspec = _il_util.spec_from_file_location(\n            name, module_filepath)\n        mod = _il_util.module_from_spec(modspec)\n        modspec.loader.exec_module(mod)\n    elif sys.version_info < (3, 3):\n        import imp\n        mod = imp.load_source(name, module_filepath)\n    else:\n        # 3.3 or 3.4\n        from importlib import machinery as _il_mach\n        loader = _il_mach.SourceFileLoader(name, module_filepath)\n        mod = loader.load_module()\n\n    return mod"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninfer delimiter used in a separated values file.", "response": "def infer_delimiter(filepath):\n    \"\"\"\n    From extension infer delimiter used in a separated values file.\n\n    :param str filepath: path to file about which to make inference\n    :return str | NoneType: extension if inference succeeded; else null\n    \"\"\"\n    ext = os.path.splitext(filepath)[1][1:].lower()\n    return {\"txt\": \"\\t\", \"tsv\": \"\\t\", \"csv\": \",\"}.get(ext)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_null_like(x):\n    return x in [None, \"\"] or \\\n        (coll_like(x) and isinstance(x, Sized) and 0 == len(x))", "response": "Determines whether an object is effectively null."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks determine filetype from extension.", "response": "def parse_ftype(input_file):\n    \"\"\"\n    Checks determine filetype from extension.\n\n    :param str input_file: String to check.\n    :return str: filetype (extension without dot prefix)\n    :raises TypeError: if file does not appear of a supported type\n    \"\"\"\n    if input_file.endswith(\".bam\"):\n        return \"bam\"\n    elif input_file.endswith(\".fastq\") or \\\n            input_file.endswith(\".fq\") or \\\n            input_file.endswith(\".fq.gz\") or \\\n            input_file.endswith(\".fastq.gz\"):\n        return \"fastq\"\n    else:\n        raise TypeError(\"Type of input file ends in neither '.bam' \"\n                        \"nor '.fastq' [file: '\" + input_file + \"']\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninterpret input argument as lines of text.", "response": "def parse_text_data(lines_or_path, delimiter=os.linesep):\n    \"\"\"\n    Interpret input argument as lines of data. This is intended to support\n    multiple input argument types to core model constructors.\n\n    :param str | collections.Iterable lines_or_path:\n    :param str delimiter: line separator used when parsing a raw string that's\n        not a file\n    :return collections.Iterable: lines of text data\n    :raises ValueError: if primary data argument is neither a string nor\n        another iterable\n    \"\"\"\n\n    if os.path.isfile(lines_or_path):\n        with open(lines_or_path, 'r') as f:\n            return f.readlines()\n    else:\n        _LOGGER.debug(\"Not a file: '{}'\".format(lines_or_path))\n\n    if isinstance(lines_or_path, str):\n        return lines_or_path.split(delimiter)\n    elif isinstance(lines_or_path, Iterable):\n        return lines_or_path\n    else:\n        raise ValueError(\"Unable to parse as data lines {} ({})\".\n                         format(lines_or_path, type(lines_or_path)))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sample_folder(prj, sample):\n    return os.path.join(prj.metadata.results_subdir,\n                        sample[\"sample_name\"])", "response": "Get the path to this Project s root folder for the given Sample."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_command_callable(command, name=\"\"):\n\n    # Use `command` to see if command is callable, store exit code\n    code = os.system(\n        \"command -v {0} >/dev/null 2>&1 || {{ exit 1; }}\".format(command))\n\n    if code != 0:\n        alias_value = \" ('{}') \".format(name) if name else \" \"\n        _LOGGER.debug(\"Command '{0}' is not callable: {1}\".\n                      format(alias_value, command))\n    return not bool(code)", "response": "Check if given command can be called."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _store_status(self, section, command, name):\n        succeeded = is_command_callable(command, name)\n        # Store status regardless of its value in the instance's largest DS.\n        self.section_to_status_by_command[section][command] = succeeded\n        if not succeeded:\n            # Only update the failure-specific structures conditionally.\n            self.failures_by_section[section].append(command)\n            self.failures.add(command)\n        return succeeded", "response": "Store the status of a new command in the instance s status_by_command dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef search(\n            self,\n            q=\"yellow flower\",\n            lang=\"en\",\n            video_type=\"all\",\n            category=\"\",\n            min_width=0,\n            min_height=0,\n            editors_choice=\"false\",\n            safesearch=\"false\",\n            order=\"popular\",\n            page=1,\n            per_page=20,\n            callback=\"\",\n            pretty=\"false\",\n    ):\n        \"\"\"returns videos API data in dict\n\n        Videos search\n\n        :param q :type str :desc A URL encoded search term. If omitted,\n        all images are returned. This value may not exceed 100 characters.\n        Example: \"yellow+flower\"\n        Default: \"yellow+flower\"\n\n        :param lang :type str :desc Language code of the language to be searched in.\n        Accepted values: cs, da, de, en, es, fr, id, it, hu, nl, no, pl, pt, ro, sk, fi,\n        sv, tr, vi, th, bg, ru, el, ja, ko, zh\n        Default: \"en\"\n        For more info, see https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes\n\n        :param video_type :type str :desc Filter results by video type.\n        Accepted values: \"all\", \"film\", \"animation\"\n        Default: \"all\"\n\n        :param category :type str :desc Filter results by category.\n        Accepted values: fashion, nature, backgrounds, science, education, people,\n        feelings, religion, health, places, animals, industry, food, computer, sports,\n        transportation, travel, buildings, business, music\n\n        :param min_width :type int :desc Minimum image width\n        Default: 0\n\n        :param min_height :type int :desc Minimum image height\n        Default: 0\n\n        :param editors_choice :type bool (python-pixabay use \"true\" and \"false\" string instead)\n        :desc Select images that have received\n        an Editor's Choice award.\n        Accepted values: \"true\", \"false\"\n        Default: \"false\"\n\n        :param safesearch :type bool (python-pixabay use \"true\" and \"false\" string instead)\n        :desc A flag indicating that only images suitable\n        for all ages should be returned.\n        Accepted values: \"true\", \"false\"\n        Default: \"false\"\n\n        :param order :type str :desc How the results should be ordered.\n        Accepted values: \"popular\", \"latest\"\n        Default: \"popular\"\n\n        :param page :type int :desc Returned search results are paginated.\n        Use this parameter to select the page number.\n        Default: 1\n\n        :param per_page :type int :desc Determine the number of results per page.\n        Accepted values: 3 - 200\n        Default: 20\n\n        :param callback :type str :desc JSONP callback function name\n\n        :param pretty :type bool (python-pixabay use \"true\" and \"false\" string instead)\n        :desc Indent JSON output. This option should not\n        be used in production.\n        Accepted values: \"true\", \"false\"\n        Default: \"false\"\n\n        Code Example\n        >>> from pixabay import Video\n        >>>\n        >>> video = Video(\"api_key\")\n        >>> video.search(q=\"apple\", page=1)\n        \"\"\"\n        payload = {\n            \"key\": self.api_key,\n            \"q\": q,\n            \"lang\": lang,\n            \"video_type\": video_type,\n            \"category\": category,\n            \"min_width\": min_width,\n            \"min_height\": min_height,\n            \"editors_choice\": editors_choice,\n            \"safesearch\": safesearch,\n            \"order\": order,\n            \"page\": page,\n            \"per_page\": per_page,\n            \"callback\": callback,\n            \"pretty\": pretty,\n        }\n\n        resp = get(self.root_url + \"videos/\", params=payload)\n        if resp.status_code == 200:\n            return resp.json()\n        else:\n            raise ValueError(resp.text)", "response": "Search for videos in a specific language."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngive a string return a quoted string as per RFC 3501 section 9. Implementation copied from ImapClient. py", "response": "def quoted(arg):\n    \"\"\" Given a string, return a quoted string as per RFC 3501, section 9.\n\n        Implementation copied from https://github.com/mjs/imapclient\n        (imapclient/imapclient.py), 3-clause BSD license\n    \"\"\"\n    if isinstance(arg, str):\n        arg = arg.replace('\\\\', '\\\\\\\\')\n        arg = arg.replace('\"', '\\\\\"')\n        q = '\"'\n    else:\n        arg = arg.replace(b'\\\\', b'\\\\\\\\')\n        arg = arg.replace(b'\"', b'\\\\\"')\n        q = b'\"'\n    return q + arg + q"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting integer to A - P string representation.", "response": "def int2ap(num):\n    \"\"\"Convert integer to A-P string representation.\"\"\"\n    val = ''\n    ap = 'ABCDEFGHIJKLMNOP'\n    num = int(abs(num))\n    while num:\n        num, mod = divmod(num, 16)\n        val += ap[mod:mod + 1]\n    return val"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert a date_time to IMAP4 INTERNALDATE representation.", "response": "def time2internaldate(date_time):\n    \"\"\"Convert date_time to IMAP4 INTERNALDATE representation.\n\n    Return string in form: '\"DD-Mmm-YYYY HH:MM:SS +HHMM\"'.  The\n    date_time argument can be a number (int or float) representing\n    seconds since epoch (as returned by time.time()), a 9-tuple\n    representing local time, an instance of time.struct_time (as\n    returned by time.localtime()), an aware datetime instance or a\n    double-quoted string.  In the last case, it is assumed to already\n    be in the correct format.\n    \"\"\"\n    if isinstance(date_time, (int, float)):\n        dt = datetime.fromtimestamp(date_time, timezone.utc).astimezone()\n    elif isinstance(date_time, tuple):\n        try:\n            gmtoff = date_time.tm_gmtoff\n        except AttributeError:\n            if time.daylight:\n                dst = date_time[8]\n                if dst == -1:\n                    dst = time.localtime(time.mktime(date_time))[8]\n                gmtoff = -(time.timezone, time.altzone)[dst]\n            else:\n                gmtoff = -time.timezone\n        delta = timedelta(seconds=gmtoff)\n        dt = datetime(*date_time[:6], tzinfo=timezone(delta))\n    elif isinstance(date_time, datetime):\n        if date_time.tzinfo is None:\n            raise ValueError(\"date_time must be aware\")\n        dt = date_time\n    elif isinstance(date_time, str) and (date_time[0],date_time[-1]) == ('\"','\"'):\n        return date_time        # Assume in correct format\n    else:\n        raise ValueError(\"date_time not of a known type\")\n    fmt = '\"%d-{}-%Y %H:%M:%S %z\"'.format(Months[dt.month])\n    return dt.strftime(fmt)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsends all delayed Celery tasks and stop queuing new ones for now.", "response": "def _send_tasks_and_stop_queuing(**kwargs):\n    \"\"\"Sends all delayed Celery tasks and stop queuing new ones for now.\"\"\"\n    log.info('Stopping queueing tasks and sending already queued ones.')\n    _stop_queuing_tasks()\n    task_queue = _get_task_queue()\n    while task_queue:\n        task, args, kwargs, extrakw = task_queue.pop(0)\n        task.original_apply_async(args=args, kwargs=kwargs, **extrakw)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nappending a task to the queue.", "response": "def _append_task(t):\n    \"\"\"Append a task to the queue.\n\n    Expected argument is a tuple of the following form:\n    (task class, args, kwargs, extra kwargs).\n\n    This doesn't append to queue if the argument is already in the queue.\n\n    \"\"\"\n    task_queue = _get_task_queue()\n    if t not in task_queue:\n        log.debug('Appended new task to the queue: %s.', t)\n        task_queue.append(t)\n    else:\n        log.debug('Did not append duplicate task to the queue: %s.', t)\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a Task instance from a tuple.", "response": "def create_from_tuple(cls, tube, the_tuple):\n        \"\"\"\n        Create task from tuple.\n\n        Returns `Task` instance.\n        \"\"\"\n        if the_tuple is None:\n            return\n\n        if not the_tuple.rowcount:\n            raise Queue.ZeroTupleException(\"Error creating task\")\n\n        row = the_tuple[0]\n\n        return cls(\n            tube,\n            task_id=row[0],\n            state=row[1],\n            data=row[2]\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate the state and data of the task from a tuple.", "response": "def update_from_tuple(self, the_tuple):\n        \"\"\"\n        Update task from tuple.\n        \"\"\"\n        if not the_tuple.rowcount:\n            raise Queue.ZeroTupleException(\"Error updating task\")\n\n        row = the_tuple[0]\n\n        if self.task_id != row[0]:\n            raise Queue.BadTupleException(\"Wrong task: id's are not match\")\n\n        self.state = row[1]\n        self.data = row[2]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrelease the task from the queue.", "response": "async def release(self, delay=None):\n        \"\"\"\n        Put the task back into the queue.\n\n        May contain a possible new `delay` before the task is executed again.\n\n        Returns `True` is task is released (task state is 'ready'\n        or 'delayed' if `delay` is set now).\n        \"\"\"\n        the_tuple = await self.queue.release(self.tube, self.task_id, delay=delay)\n\n        self.update_from_tuple(the_tuple)\n\n        if delay is None:\n            return bool(self.state == READY)\n        else:\n            return bool(self.state == DELAYED)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlook at a task without changing its state. Always returns True.", "response": "async def peek(self):\n        \"\"\"\n        Look at a task without changing its state.\n\n        Always returns `True`.\n        \"\"\"\n        the_tuple = await self.queue.peek(self.tube, self.task_id)\n\n        self.update_from_tuple(the_tuple)\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndeletes the task permanently. Returns True if the task is deleted permanently.", "response": "async def delete(self):\n        \"\"\"\n        Delete task (in any state) permanently.\n\n        Returns `True` is task is deleted.\n        \"\"\"\n        the_tuple = await self.queue.delete(self.tube, self.task_id)\n\n        self.update_from_tuple(the_tuple)\n\n        return bool(self.state == DONE)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns tarantool queue command name for current tube.", "response": "def cmd(self, cmd_name):\n        \"\"\"\n        Returns tarantool queue command name for current tube.\n        \"\"\"\n        return \"{0}.tube.{1}:{2}\".format(self.queue.lua_queue_name, self.name, cmd_name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def put(self, data, ttl=None, ttr=None, delay=None):\n        the_tuple = await self.queue.put(self, data, ttl=ttl, ttr=ttr, delay=delay)\n        if the_tuple.rowcount:\n            return Task.create_from_tuple(self, the_tuple)", "response": "Enqueue a task.\n\n        Returns a `Task` object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def take(self, timeout=None):\n        the_tuple = await self.queue.take(self, timeout=timeout)\n\n        if the_tuple.rowcount:\n            return Task.create_from_tuple(self, the_tuple)", "response": "Get a task from the queue for execution."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting a task from the queue for execution.", "response": "async def take(self, tube, timeout=None):\n        \"\"\"\n        Get a task from queue for execution.\n\n        Waits `timeout` seconds until a READY task appears in the queue.\n        If `timeout` is `None` - waits forever.\n\n        Returns tarantool tuple object.\n        \"\"\"\n        cmd = tube.cmd(\"take\")\n        args = ()\n\n        if timeout is not None:\n            args += (timeout,)\n\n        res = await self.tnt.call(cmd, args)\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def ack(self, tube, task_id):\n        cmd = tube.cmd(\"ack\")\n        args = (task_id,)\n\n        res = await self.tnt.call(cmd, args)\n        return res", "response": "Report task successful execution."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreleasing the task_id from the queue.", "response": "async def release(self, tube, task_id, delay=None):\n        \"\"\"\n        Put the task back into the queue.\n\n        Used in case of a consumer for any reason can not execute a task.\n        May contain a possible new `delay` before the task is executed again.\n\n        Returns tarantool tuple object.\n        \"\"\"\n        cmd = tube.cmd(\"release\")\n        args = (task_id,)\n        params = dict()\n        if delay is not None:\n            params[\"delay\"] = delay\n        if params:\n            args += (params,)\n\n        res = await self.tnt.call(cmd, args)\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndropping all in - progress tasks or workers from the given tube. Returns True on successful drop.", "response": "async def drop(self, tube):\n        \"\"\"\n        Drop entire query (if there are no in-progress tasks or workers).\n\n        Returns `True` on successful drop.\n        \"\"\"\n        cmd = tube.cmd(\"drop\")\n        args = ()\n\n        res = await self.tnt.call(cmd, args)\n\n        return bool(res.return_code == 0)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a Tube object with the given name.", "response": "def tube(self, name):\n        \"\"\"\n        Create tube object, if not created before.\n\n        Returns `Tube` object.\n        \"\"\"\n        tube = self.tubes.get(name)\n\n        if tube is None:\n            tube = Tube(self, name)\n            self.tubes[name] = tube\n\n        return tube"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef plot(x, y, rows=None, columns=None):\n    if not rows or not columns:\n        rows, columns = get_terminal_size()\n    # offset for caption\n    rows -= 4\n\n    # Scale points such that they fit on canvas\n    x_scaled = scale(x, columns)\n    y_scaled = scale(y, rows)\n\n    # Create empty canvas\n    canvas = [[' ' for _ in range(columns)] for _ in range(rows)]\n\n    # Add scaled points to canvas\n    for ix, iy in zip(x_scaled, y_scaled):\n        canvas[rows - iy - 1][ix] = '*'\n\n    # Print rows of canvas\n    for row in [''.join(row) for row in canvas]:\n        print(row)\n\n    # Print scale\n    print(''.join([\n        '\\nMin x: ', str(min(x)),\n        ' Max x: ',  str(max(x)),\n        ' Min y: ',  str(min(y)),\n        ' Max y: ',  str(max(y))\n    ]))", "response": "Plot the data on x y on x - and y - axis."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef scale(x, length):\n    s = float(length - 1) / \\\n        (max(x) - min(x)) if x and max(x) - min(x) != 0 else length\n    return [int((i - min(x)) * s) for i in x]", "response": "Scale points in x such that distance between max and min equals to length."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalls a command and returns the result.", "response": "def call(cmd, input=None, assert_zero_exit_status=True, warn_on_non_zero_exist_status=False, **kwargs):\n    \"\"\"\n    :rtype: SubprocessResult\n\n    Raises OSError if command was not found\n\n    Returns non-zero result in result.ret if subprocess terminated with non-zero exist status.\n    \"\"\"\n    if (not kwargs.get('shell')) and isinstance(cmd, basestring):\n        raise ValueError('cmd should be list or tuple, not a string: %r' % cmd)\n    result = SubprocessResult.call(cmd, input=input, **kwargs)\n    if assert_zero_exit_status and result.ret != 0:\n        raise SubprocessError(result)\n\n    if warn_on_non_zero_exist_status and result.ret != 0:\n        logger.warn('subprocess failed %r' % result)\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef format_event_time(date_time):\n    if not date_time:\n        # Return None if passed None\n        return date_time\n    date_time_type = type(date_time)\n    if date_time_type in (type(''), type(u'')):\n        # If passed a string, return the string.\n        return date_time\n    elif date_time_type == datetime.date:\n        # If passed a date, return an iso8601 formatted date string.\n        return date_time.strftime(ISO_8601_DATE_FORMAT)\n    elif date_time_type is dict:\n        if date_time.get('time'):\n            date_time['time'] = format_event_time(date_time['time'])\n        return date_time\n    elif date_time_type != datetime.datetime:\n        # If passed anything other than a datetime, date, string, dict, or None, raise an Exception.\n        error_message = 'Unsupported type: ``%s``.\\nSupported types: ``<datetime.datetime>``, ``<datetime.date>``, ``<dict>``, or ``<str>``.'\n        raise PyCronofyDateTimeError(\n            error_message % (repr(type(date_time))), date_time)\n    if date_time.tzinfo and date_time.tzinfo != pytz.utc:\n        date_time = date_time.astimezone(pytz.utc)\n    return date_time.strftime(ISO_8601_DATETIME_FORMAT)", "response": "Formats the given date time into a string."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nperforming a get for a json API endpoint.", "response": "def get(self, endpoint='', url='', params=None, use_api_key=False):\n        \"\"\"Perform a get for a json API endpoint.\n\n        :param string endpoint: Target endpoint. (Optional).\n        :param string url: Override the endpoint and provide the full url (eg for pagination). (Optional).\n        :param dict params: Provide parameters to pass to the request. (Optional).\n        :return: Response json.\n        :rtype: ``dict``\n        \"\"\"\n        return self._request('get', endpoint, url, params=params, use_api_key=use_api_key)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nperform a get for a json API endpoint.", "response": "def delete(self, endpoint='', url='', params=None, data=None):\n        \"\"\"Perform a get for a json API endpoint.\n\n        :param string endpoint: Target endpoint. (Optional).\n        :param string url: Override the endpoint and provide the full url (eg for pagination). (Optional).\n        :param dict params: Provide parameters to pass to the request. (Optional).\n        :param dict data: Data to pass to the request. (Optional).\n        :return: Response json.\n        :rtype: ``dict``\n        \"\"\"\n        return self._request('delete', endpoint, url, params=params, data=data)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nperform a post to an API endpoint.", "response": "def post(self, endpoint='', url='', data=None, use_api_key=False, omit_api_version=False):\n        \"\"\"Perform a post to an API endpoint.\n\n        :param string endpoint: Target endpoint. (Optional).\n        :param string url: Override the endpoint and provide the full url (eg for pagination). (Optional).\n        :param dict data: Data to pass to the post. (Optional).\n        :return: Response.\n        :rtype: ``Response``\n        \"\"\"\n        return self._request('post', endpoint, url, data=data, use_api_key=use_api_key, omit_api_version=omit_api_version)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _request(self, request_method, endpoint='', url='', data=None, params=None, use_api_key=False, omit_api_version=False):\n        if not data:\n            data = {}\n        if not params:\n            params = {}\n        if endpoint and omit_api_version and not url:\n            url = '%s/%s' % (self.base_url, endpoint)\n        if endpoint and not url:\n            url = '%s/%s/%s' % (self.base_url, settings.API_VERSION, endpoint)\n\n        if use_api_key:\n            headers = {\n                'Authorization': self.auth.get_api_key(),\n                'User-Agent': self.user_agent,\n            }\n        else:\n            headers = {\n                'Authorization': self.auth.get_authorization(),\n                'User-Agent': self.user_agent,\n            }\n\n        response = requests.__getattribute__(request_method)(\n            url=url,\n            hooks=settings.REQUEST_HOOK,\n            headers=headers,\n            json=data,\n            params=params\n        )\n        if ((response.status_code != 200) and (response.status_code != 202)):\n            try:\n                response.raise_for_status()\n            except requests.exceptions.HTTPError as e:\n                raise PyCronofyRequestError(\n                    request=e.request,\n                    response=e.response,\n                )\n        return response", "response": "Perform a http request to an API endpoint."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update(self, **kwargs):\n        for kw in kwargs:\n            setattr(self, kw, kwargs[kw])", "response": "Update the fields of the object with the values from the keyword arguments."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nchange the participation status for a calendar event", "response": "def change_participation_status(self, calendar_id, event_uid, status):\n        \"\"\"Changes the participation status for a calendar event\n\n        :param string calendar_id: The String Cronofy ID for the calendar to delete the event from.\n        :param string event_uid: A String uniquely identifying the event for your\n        application (note: this is NOT an ID generated by Cronofy).\n        :param string status: A String to set the participation status of the event to\n        :return: None\n        \"\"\"\n        data = {'status': status}\n\n        self.request_handler.post('calendars/%s/events/%s/participation_status' % (calendar_id, event_uid), data=data)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a new channel for receiving push notifications.", "response": "def create_notification_channel(self, callback_url, calendar_ids=()):\n        \"\"\"Create a new channel for receiving push notifications.\n\n        :param string callback_url: The url that will receive push notifications.\n        Must not be longer than 128 characters and should be HTTPS.\n        :param tuple calendar_ids: List of calendar ids to create notification channels for. (Optional. Default empty tuple)\n        :return: Channel id and channel callback\n        :rtype: ``dict``\n        \"\"\"\n        data = {'callback_url': callback_url}\n        if calendar_ids:\n            data['filters'] = {'calendar_ids': calendar_ids}\n\n        return self.request_handler.post('channels', data=data).json()['channel']"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndelete all events managed through Cronofy from the all of the user s calendars.", "response": "def delete_all_events(self, calendar_ids=()):\n        \"\"\"Deletes all events managed through Cronofy from the all of the user's calendars.\n\n        :param tuple calendar_ids: List of calendar ids to delete events for. (Optional. Default empty tuple)\n        \"\"\"\n        params = {'delete_all': True}\n        if calendar_ids:\n            params = {'calendar_ids[]': calendar_ids}\n\n        self.request_handler.delete(endpoint='events', params=params)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndelete an event from the specified calendar.", "response": "def delete_event(self, calendar_id, event_id):\n        \"\"\"Delete an event from the specified calendar.\n\n        :param string calendar_id: ID of calendar to delete from.\n        :param string event_id: ID of event to delete.\n        \"\"\"\n        self.request_handler.delete(endpoint='calendars/%s/events' % calendar_id, data={'event_id': event_id})"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef delete_external_event(self, calendar_id, event_uid):\n        self.request_handler.delete(endpoint='calendars/%s/events' % calendar_id, data={'event_uid': event_uid})", "response": "Delete an external event from the specified calendar."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrequest elevated permissions for a set of calendars.", "response": "def elevated_permissions(self, permissions, redirect_uri=None):\n        \"\"\"Requests elevated permissions for a set of calendars.\n\n        :param tuple permissions  - calendar permission dicts set each dict\n        must contain values for both `calendar_id` and `permission_level`\n        :param string redirect_uri - A uri to redirect the end user back to after they\n        have either granted or rejected the request for elevated permission.\n\n        In the case of normal accounts:\n        After making this call the end user will have to grant the extended\n        permissions to their calendar via rhe url returned from the response.\n\n        In the case of service accounts:\n        After making this call the exteneded permissions will be granted provided\n        the relevant scope has been granted to the account\n\n        :return: a extended permissions response.\n        :rtype: ``dict``\n        \"\"\"\n\n        body = {'permissions': permissions}\n\n        if redirect_uri:\n            body['redirect_uri'] = redirect_uri\n\n        return self.request_handler.post('permissions', data=body).json()['permissions_request']"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef upsert_smart_invite(self, smart_invite_id, recipient, event, callback_url=None, organizer=None):\n        event['start'] = format_event_time(event['start'])\n        event['end'] = format_event_time(event['end'])\n\n        body = {\n            'smart_invite_id': smart_invite_id,\n            'event': event\n        }\n        if type(recipient) == dict:\n            body['recipient'] = recipient\n        elif type(recipient) == list:\n            body['recipients'] = recipient\n\n        if callback_url:\n            body['callback_url'] = callback_url\n\n        if organizer:\n            body['organizer'] = organizer\n\n        return self.request_handler.post('smart_invites', data=body, use_api_key=True).json()", "response": "Creates or updates a Smart Invite for the given event."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the details for a smart invite.", "response": "def get_smart_invite(self, smart_invite_id, recipient_email):\n        \"\"\"Gets the details for a smart invite.\n\n        :param string smart_invite_id: - A String uniquely identifying the event for your\n                                        application (note: this is NOT an ID generated by Cronofy).\n        :param string recipient_email: - The email address for the recipient to get details for.\n        \"\"\"\n        params = {\n            'smart_invite_id': smart_invite_id,\n            'recipient_email': recipient_email\n        }\n\n        return self.request_handler.get('smart_invites', params=params, use_api_key=True).json()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate and Retrieves an authorization for an application calendar.", "response": "def application_calendar(self, application_calendar_id):\n        \"\"\"Creates and Retrieves authorization for an application calendar\n\n        :param string application_calendar_id: The Id for this application calendar\n        :return: Dictionary containing auth tokens, expiration info, and response status.\n        :rtype: ``dict``\n        \"\"\"\n        response = self.request_handler.post(\n            endpoint='application_calendar',\n            data={\n                'client_id': self.auth.client_id,\n                'client_secret': self.auth.client_secret,\n                'application_calendar_id': application_calendar_id,\n            })\n        data = response.json()\n        token_expiration = (datetime.datetime.utcnow() + datetime.timedelta(seconds=data['expires_in']))\n        self.auth.update(\n            token_expiration=token_expiration,\n            access_token=data['access_token'],\n            refresh_token=data['refresh_token'],\n        )\n        return {\n            'access_token': self.auth.access_token,\n            'refresh_token': self.auth.refresh_token,\n            'token_expiration': format_event_time(self.auth.token_expiration),\n            'sub': data.get('sub'),\n            'application_calendar_id': data.get('application_calendar_id')\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_authorization_expired(self):\n        if not self.auth.token_expiration:\n            return True\n        return (datetime.datetime.utcnow() > self.auth.token_expiration)", "response": "Checks if the authorization token has expired."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read_events(self,\n                    calendar_ids=(),\n                    from_date=None,\n                    to_date=None,\n                    last_modified=None,\n                    tzid=settings.DEFAULT_TIMEZONE_ID,\n                    only_managed=False,\n                    include_managed=True,\n                    include_deleted=False,\n                    include_moved=False,\n                    include_geo=False,\n                    localized_times=False,\n                    automatic_pagination=True):\n        \"\"\"Read events for linked account (optionally for the specified calendars).\n\n        :param tuple calendar_ids: Tuple or list of calendar ids to pass to cronofy. (Optional).\n        :param datetime.date from_date: Start datetime (or ISO8601 string) for query. (Optional).\n        :param datetime.date to_date: End datetime (or ISO8601 string) for query. (Optional).\n        :param datetime.datetime last_modified: Return items modified on or after last_modified. Datetime or ISO8601 string. (Optional).\n        :param string tzid: Timezone ID for query. (Optional, default settings.DEFAULT_TIMEZONE_ID). Should match tzinfo on datetime objects.\n        :param bool only_managed: Only include events created through the API. (Optional, default False)\n        :param bool include_managed: Include events created through the API. (Optional, default True)\n        :param bool include_deleted: Include deleted events. (Optional, default False)\n        :param bool include_moved: Include events that ever existed within the from_date/to_date time window. (Optional, default False)\n        :param bool include_geo: Include any geo location information for events when available (Optional, default False)\n        :param bool localized_times: Return time values for event start/end with localization information. This varies across providers. (Optional, default False).\n        :param bool automatic_pagination: Autonatically fetch next page when iterating through results (Optional, default True)\n        :return: Wrapped results (Containing first page of events).\n        :rtype: ``Pages``\n        \"\"\"\n        results = self.request_handler.get(endpoint='events', params={\n            'tzid': tzid,\n            'calendar_ids[]': calendar_ids,\n            'from': format_event_time(from_date),\n            'to': format_event_time(to_date),\n            'last_modified': format_event_time(last_modified),\n            'only_managed': only_managed,\n            'include_managed': include_managed,\n            'include_deleted': include_deleted,\n            'include_moved': include_moved,\n            'include_geo': include_geo,\n            'localized_times': localized_times,\n        }).json()\n\n        return Pages(self.request_handler, results, 'events', automatic_pagination)", "response": "Read events for the specified calendars."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read_free_busy(self,\n                       calendar_ids=(),\n                       from_date=None,\n                       to_date=None,\n                       last_modified=None,\n                       tzid=settings.DEFAULT_TIMEZONE_ID,\n                       include_managed=True,\n                       localized_times=False,\n                       automatic_pagination=True):\n        \"\"\"Read free/busy blocks for linked account (optionally for the specified calendars).\n\n        :param tuple calendar_ids: Tuple or list of calendar ids to pass to cronofy. (Optional).\n        :param datetime.date from_date: Start datetime (or ISO8601 string) for query. (Optional).\n        :param datetime.date to_date: End datetime (or ISO8601 string) for query. (Optional).\n        :param string tzid: Timezone ID for query. (Optional, default settings.DEFAULT_TIMEZONE_ID). Should match tzinfo on datetime objects.\n        :param bool include_managed: Include pages created through the API. (Optional, default True)\n        :param bool localized_times: Return time values for event start/end with localization information. This varies across providers. (Optional, default False).\n        :param bool automatic_pagination: Automatically fetch next page when iterating through results (Optional, default True)\n        :return: Wrapped results (Containing first page of free/busy blocks).\n        :rtype: ``Pages``\n        \"\"\"\n        results = self.request_handler.get(endpoint='free_busy', params={\n            'tzid': tzid,\n            'calendar_ids[]': calendar_ids,\n            'from': format_event_time(from_date),\n            'to': format_event_time(to_date),\n            'include_managed': include_managed,\n            'localized_times': localized_times,\n        }).json()\n\n        return Pages(self.request_handler, results, 'free_busy', automatic_pagination)", "response": "Reads free and busy blocks for the linked account."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef availability(self, participants=(), required_duration=(), available_periods=(), start_interval=None, buffer=()):\n        options = {}\n        options['participants'] = self.map_availability_participants(\n            participants)\n        options['required_duration'] = self.map_availability_required_duration(\n            required_duration)\n        options['buffer'] = self.map_availability_buffer(buffer)\n\n        if start_interval:\n            options['start_interval'] = self.map_availability_required_duration(start_interval)\n\n        self.translate_available_periods(available_periods)\n        options['available_periods'] = available_periods\n\n        return self.request_handler.post(endpoint='availability', data=options).json()['available_periods']", "response": "Performs an availability query."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nperforms an availability query for the sequenced sequence.", "response": "def sequenced_availability(self, sequence=(), available_periods=()):\n        \"\"\" Performs an availability query.\n        :param list sequence: An Array of dics representing sequences to find availability for\n                       each sequence can contain.\n                :sequence_id - A string identifying this step in the sequence.\n                :ordinal - An Integer defining the order of this step in the sequence.\n                :participants      - A dict stating who is required for the availability\n                                 call\n                :required_duration - A dict stating the length of time the event will\n                                     last for\n                :event - A dict describing the event\n                :available_periods - A dict stating the available periods for the step\n                :start_interval - An Interger representing the start interval minutes for the event.\n                :buffer - An Dict representing the buffer to apply to the request.\n        :param list available_periods - An Array of available time periods dicts, each must specify a start and end Time.\n\n        :rtype: ``list``\n        \"\"\"\n        options = {}\n        options['sequence'] = self.map_availability_sequence(sequence)\n\n        self.translate_available_periods(available_periods)\n        options['available_periods'] = available_periods\n\n        return self.request_handler.post(endpoint='sequenced_availability', data=options).json()['sequences']"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrefreshing the authorization tokens.", "response": "def refresh_authorization(self):\n        \"\"\"Refreshes the authorization tokens.\n\n        :return: Dictionary containing auth tokens, expiration info, and response status.\n        :rtype: ``dict``\n        \"\"\"\n        response = self.request_handler.post(\n            endpoint='oauth/token',\n            omit_api_version=True,\n            data={\n                'grant_type': 'refresh_token',\n                'client_id': self.auth.client_id,\n                'client_secret': self.auth.client_secret,\n                'refresh_token': self.auth.refresh_token,\n            }\n        )\n        data = response.json()\n        token_expiration = (datetime.datetime.utcnow() + datetime.timedelta(seconds=data['expires_in']))\n        self.auth.update(\n            token_expiration=token_expiration,\n            access_token=data['access_token'],\n            refresh_token=data['refresh_token'],\n        )\n        return {\n            'access_token': self.auth.access_token,\n            'refresh_token': self.auth.refresh_token,\n            'token_expiration': format_event_time(self.auth.token_expiration),\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninserting or updates an event for the specified calendar.", "response": "def upsert_event(self, calendar_id, event):\n        \"\"\"Inserts or updates an event for the specified calendar.\n\n        :param string calendar_id: ID of calendar to insert/update event into.\n        :param dict event: Dictionary of event data to send to cronofy.\n        \"\"\"\n        event['start'] = format_event_time(event['start'])\n        event['end'] = format_event_time(event['end'])\n        self.request_handler.post(\n            endpoint='calendars/%s/events' % calendar_id, data=event)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nattempting to authorize the email with impersonation from a service account.", "response": "def authorize_with_service_account(self, email, scope, callback_url, state=None):\n        \"\"\" Attempts to authorize the email with impersonation from a service account\n\n        :param string email: the email address to impersonate\n        :param string callback_url: URL to callback with the OAuth code.\n        :param string scope: The scope of the privileges you want the eventual access_token to grant.\n        :return: nothing\n        \"\"\"\n        params = {\n            'email': email,\n            'scope': scope,\n            'callback_url': callback_url\n        }\n\n        if state is not None:\n            params['state'] = state\n\n        self.request_handler.post(\n            endpoint=\"service_account_authorizations\", data=params)\n        None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef real_time_scheduling(self, availability, oauth, event, target_calendars=()):\n        args = {\n            'oauth': oauth,\n            'event': event,\n            'target_calendars': target_calendars\n        }\n\n        if availability:\n            options = {}\n            options['participants'] = self.map_availability_participants(availability.get('participants', None))\n            options['required_duration'] = self.map_availability_required_duration(availability.get('required_duration', None))\n            options['start_interval'] = self.map_availability_required_duration(availability.get('start_interval', None))\n            options['buffer'] = self.map_availability_buffer(availability.get('buffer', None))\n\n            self.translate_available_periods(availability['available_periods'])\n            options['available_periods'] = availability['available_periods']\n            args['availability'] = options\n\n        return self.request_handler.post(endpoint='real_time_scheduling', data=args, use_api_key=True).json()", "response": "Generates a real time scheduling link to start the OAuth process with the event."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates a real time sequencing link to start the OAuth process with the given event and the given OAuth flow.", "response": "def real_time_sequencing(self, availability, oauth, event, target_calendars=()):\n        \"\"\"Generates an real time sequencing link to start the OAuth process with\n        an event to be automatically upserted\n\n        :param dict availability:  - A dict describing the availability details for the event:\n\n            :sequence: An Array of dics representing sequences to find availability for\n                       each sequence can contain.\n                :sequence_id - A string identifying this step in the sequence.\n                :ordinal - An Integer defining the order of this step in the sequence.\n                :participants      - A dict stating who is required for the availability\n                                 call\n                :required_duration - A dict stating the length of time the event will\n                                     last for\n                :event - A dict describing the event\n                :available_periods - A dict stating the available periods for the step\n            :available_periods - A dict stating the available periods for the sequence\n        :param dict oauth:   - A dict describing the OAuth flow required:\n            :scope             - A String representing the scopes to ask for\n                                 within the OAuth flow\n            :redirect_uri      - A String containing a url to redirect the\n                                 user to after completing the OAuth flow.\n            :scope             - A String representing additional state to\n                                 be passed within the OAuth flow.\n        :param dict event:     - A dict describing the event\n        :param list target_calendars: - An list of dics stating into which calendars\n                                        to insert the created event\n        See http://www.cronofy.com/developers/api#upsert-event for reference.\n        \"\"\"\n        args = {\n            'oauth': oauth,\n            'event': event,\n            'target_calendars': target_calendars\n        }\n\n        if availability:\n            options = {}\n            options['sequence'] = self.map_availability_sequence(availability.get('sequence', None))\n\n            if availability.get('available_periods', None):\n                self.translate_available_periods(availability['available_periods'])\n                options['available_periods'] = availability['available_periods']\n\n        args['availability'] = options\n\n        return self.request_handler.post(endpoint='real_time_sequencing', data=args, use_api_key=True).json()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate a URL to send the user to the OAuth 2. 0 authorization link.", "response": "def user_auth_link(self, redirect_uri, scope='', state='', avoid_linking=False):\n        \"\"\"Generates a URL to send the user for OAuth 2.0\n\n        :param string redirect_uri: URL to redirect the user to after auth.\n        :param string scope: The scope of the privileges you want the eventual access_token to grant.\n        :param string state: A value that will be returned to you unaltered along with the user's authorization request decision.\n        (The OAuth 2.0 RFC recommends using this to prevent cross-site request forgery.)\n        :param bool avoid_linking: Avoid linking calendar accounts together under one set of credentials. (Optional, default: false).\n        :return: authorization link\n        :rtype: ``string``\n        \"\"\"\n        if not scope:\n            scope = ' '.join(settings.DEFAULT_OAUTH_SCOPE)\n\n        self.auth.update(redirect_uri=redirect_uri)\n\n        url = '%s/oauth/authorize' % self.app_base_url\n        params = {\n            'response_type': 'code',\n            'client_id': self.auth.client_id,\n            'redirect_uri': redirect_uri,\n            'scope': scope,\n            'state': state,\n            'avoid_linking': avoid_linking,\n        }\n        urlencoded_params = urlencode(params)\n        return \"{url}?{params}\".format(url=url, params=urlencoded_params)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nvalidate authentication and values passed to the specified method. Raises a PyCronofyValidationError on error.", "response": "def validate(self, method, *args, **kwargs):\n        \"\"\"Validate authentication and values passed to the specified method.\n        Raises a PyCronofyValidationError on error.\n\n        :param string method: Method name to check.\n        :param *args: Arguments for \"Method\".\n        :param **kwargs: Keyword arguments for \"Method\".\n        \"\"\"\n        validate(method, self.auth, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef all(self):\n        results = self.data[self.data_type]\n        while self.current < self.total:\n            self.fetch_next_page()\n            results.extend(self.data[self.data_type])\n        return results", "response": "Return all results as a list by automatically fetching all pages."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nretrieve the next page of data and refreshes Pages instance.", "response": "def fetch_next_page(self):\n        \"\"\"Retrieves the next page of data and refreshes Pages instance.\"\"\"\n        result = self.request_handler.get(url=self.next_page_url).json()\n        self.__init__(self.request_handler, result,\n                      self.data_type, self.automatic_pagination)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if required fields exist in the object instance.", "response": "def check_exists_in_object(method, obj, required_fields):\n    \"\"\"Checks if required fields have a value in the object instance.\n    Throws an exception with the missing fields.\n\n    :param object obj: Object to check.\n    :param typle required_fields: Required fields that must have a value.\n    \"\"\"\n    missing = []\n    for field in required_fields:\n        if getattr(obj, field) is None:\n            missing.append(field)\n    if missing:\n        raise PyCronofyValidationError('Method: %s. Missing auth field(s): %s' % (method, missing),\n                                       method,\n                                       missing\n                                       )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_datetime(method, dictionary, fields, label=None):\n    improperly_formatted = []\n    values = []\n    for field in fields:\n        if field in dictionary and dictionary[field] is not None:\n            if type(dictionary[field]) not in (datetime.datetime, datetime.date) and not ISO_8601_REGEX.match(dictionary[field]):\n                improperly_formatted.append(field)\n                values.append(dictionary[field])\n    if improperly_formatted:\n        error_label = ' for \"%s\"' % label if label else ''\n        raise PyCronofyValidationError(\n            'Method: %s. Improperly formatted datetime/date field(s)%s: %s\\n%s' % (\n                method, error_label, improperly_formatted, values),\n            method,\n            improperly_formatted,\n            values\n        )", "response": "Checks if the specified fields are formatted correctly if they have a value."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if required fields have a value in the object instance.", "response": "def check_exists_in_dictionary(method, dictionary, required_fields, label=None):\n    \"\"\"Checks if required fields have a value in the object instance.\n    Throws an exception with the missing fields.\n\n    :param dict dictionary: Dictionary to check.\n    :param typle required_fields: Required fields that must have a value.\n    :param string label: Dictionary name.\n    \"\"\"\n    missing = []\n    for field in required_fields:\n        if field not in dictionary or dictionary[field] is None:\n            missing.append(field)\n    if missing:\n        error_label = ' for \"%s\"' % label if label else ''\n        raise PyCronofyValidationError('Method: %s. Missing required field(s)%s: %s' % (method, error_label, missing),\n                                       method,\n                                       missing\n                                       )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef validate(method, auth, *args, **kwargs):\n    if method not in METHOD_RULES:\n        raise PyCronofyValidationError('Method \"%s\" not found.' % method, method)\n\n    m = METHOD_RULES[method]\n    arguments = {}\n    number_of_args = len(args)\n    for i, key in enumerate(m['args']):\n        if i < number_of_args:\n            arguments[key] = args[i]\n        elif key in kwargs:\n            arguments[key] = kwargs[key]\n        else:\n            arguments[key] = None\n    check_exists_in_object(method, auth, m['auth'])\n    if 'required' in m:\n        check_exists_in_dictionary(method, arguments, m['required'])\n    if 'datetime' in m:\n        check_datetime(method, arguments, m['datetime'])\n    if 'dicts' in m:\n        for d in m['dicts']:\n            check_exists_in_dictionary(method, arguments[d], m['dicts'][d], d)\n    if 'dicts_datetime' in m:\n        for d in m['dicts_datetime']:\n            check_datetime(method, arguments[d], m['dicts_datetime'][d], d)", "response": "Validate a method based on the METHOD_RULES."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef release(part='patch'):\n\n    # Dry run 'bumpversion' to find out what the new version number\n    # would be. Useful side effect: exits if the working directory is not\n    # clean.\n\n    bumpver = subprocess.check_output(\n        ['bumpversion', part, '--dry-run', '--verbose'],\n        stderr=subprocess.STDOUT)\n    m = re.search(r'New version will be \\'(\\d+\\.\\d+\\.\\d+)\\'', bumpver.decode('utf-8'))\n    version = m.groups(0)[0]\n\n    # Really run bumpver to set the new release and tag\n    bv_args = ['bumpversion', part]\n\n    bv_args += ['--new-version', version]\n\n    subprocess.check_output(bv_args)", "response": "Automated software release workflow"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fetch(self, endpoint=None, page=1, key=None, filter='default', **kwargs):\n        if not endpoint:\n            raise ValueError('No end point provided.')\n\n        self._endpoint = endpoint\n\n        params = {\n            \"pagesize\": self.page_size,\n            \"page\": page,\n            \"filter\": filter\n        }\n\n        if self.key:\n            params['key'] = self.key\n        if self.access_token:\n            params['access_token'] = self.access_token\n\n        # This block will replace {ids} placeholds in end points\n        # converting .fetch('badges/{ids}', ids=[222, 1306, 99999]) to\n        #   badges/222;1306;99999\n        for k, value in list(kwargs.items()):\n            if \"{\" + k + \"}\" in endpoint:\n                endpoint = endpoint.replace(\"{\"+k+\"}\", ';'.join(requests.compat.quote_plus(str(x)) for x in value))\n                kwargs.pop(k, None)\n\n        date_time_keys = ['fromdate', 'todate', 'since', 'min', 'max']\n        for k in date_time_keys:\n            if k in kwargs:\n                if isinstance(kwargs[k], datetime.datetime):\n                    kwargs[k] = int(calendar.timegm(kwargs[k].utctimetuple()))\n\n        # This block will see if there there are ids remaining\n        # This would occur if the developer passed `badges` instead of `badges/{ids}` to `fetch`\n        # If this is the case, then convert to a string and assume this goes at the end of the endpoint\n\n        if 'ids' in kwargs:\n            ids = ';'.join(str(x) for x in kwargs['ids'])\n            kwargs.pop('ids', None)\n            endpoint += \"/{}\".format(ids)\n\n        params.update(kwargs)\n        if self._api_key:\n            params['site'] = self._api_key\n\n        data = []\n        run_cnt = 1\n        backoff = 0\n        total = 0\n        while run_cnt <= self.max_pages:\n            run_cnt += 1\n\n            base_url = \"{}{}/\".format(self._base_url, endpoint)\n\n            try:\n                response = requests.get(base_url, params=params, proxies=self.proxy)\n            except requests.exceptions.ConnectionError as e:\n                raise StackAPIError(self._previous_call, str(e), str(e), str(e))\n\n            self._previous_call = response.url\n            try:\n                response.encoding = 'utf-8-sig'\n                response = response.json()\n            except ValueError as e:\n                raise StackAPIError(self._previous_call, str(e), str(e), str(e))\n\n            try:\n                error = response[\"error_id\"]\n                code = response[\"error_name\"]\n                message = response[\"error_message\"]\n                raise StackAPIError(self._previous_call, error, code, message)\n            except KeyError:\n                pass  # This means there is no error\n\n            if key:\n                data.append(response[key])\n            else:\n                data.append(response)\n\n            if len(data) < 1:\n                break\n\n            backoff = 0\n            total = 0\n            page = 1\n            if 'backoff' in response:\n                backoff = int(response['backoff'])\n                sleep(backoff+1)        # Sleep an extra second to ensure no timing issues\n            if 'total' in response:\n                total = response['total']\n            if 'has_more' in response and response['has_more'] and run_cnt <= self.max_pages:\n                params[\"page\"] += 1\n            else:\n                break\n\n        r = []\n        for d in data:\n            if 'items' in d:\n                r.extend(d['items'])\n        result = {'backoff': backoff,\n                  'has_more': False if 'has_more' not in data[-1] else data[-1]['has_more'],\n                  'page': params['page'],\n                  'quota_max': -1 if 'quota_max' not in data[-1] else data[-1]['quota_max'],\n                  'quota_remaining': -1 if 'quota_remaining' not in data[-1] else data[-1]['quota_max'],\n                  'total': total,\n                  'items': list(chain(r))}\n\n        return result", "response": "This method returns the results of an API call to the specified endpoint and returns the results as a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef send_data(self, endpoint=None, page=1, key=None, filter='default', **kwargs):\n        if not endpoint:\n            raise ValueError('No end point provided.')\n\n        self._endpoint = endpoint\n\n        params = {\n            \"pagesize\": self.page_size,\n            \"page\": page,\n            \"filter\": filter\n        }\n\n        if self.key:\n            params['key'] = self.key\n        if self.access_token:\n            params['access_token'] = self.access_token\n\n        if 'ids' in kwargs:\n            ids = ';'.join(str(x) for x in kwargs['ids'])\n            kwargs.pop('ids', None)\n        else:\n            ids = None\n\n        params.update(kwargs)\n        if self._api_key:\n            params['site'] = self._api_key\n\n        data = []\n\n        base_url = \"{}{}/\".format(self._base_url, endpoint)\n        response = requests.post(base_url, data=params, proxies=self.proxy)\n        self._previous_call = response.url\n        response = response.json()\n\n        try:\n            error = response[\"error_id\"]\n            code = response[\"error_name\"]\n            message = response[\"error_message\"]\n            raise StackAPIError(self._previous_call, error, code, message)\n        except KeyError:\n            pass  # This means there is no error\n\n        data.append(response)\n        r = []\n        for d in data:\n            r.extend(d['items'])\n        result = {'has_more': data[-1]['has_more'],\n                  'page': params['page'],\n                  'quota_max': data[-1]['quota_max'],\n                  'quota_remaining': data[-1]['quota_remaining'],\n                  'items': list(chain(r))}\n\n        return result", "response": "Sends data to the API."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconnecting a callback to a specific sender and senders and key.", "response": "def connect(self, callback, sender=None, senders=None, key=None, keys=None, weak=True):\n        '''\n        *This method is a coroutine.*\n\n        Connect a callback. By default, the callback will be called any time\n        :meth:`asyncio_dispatch.Signal.send` is called. You may restrict this by adding ``senders``\n        and/or ``keys`` which will cause the connected callback to only be called when\n        :meth:`asyncio_dispatch.Signal.send` is called with a matching `sender` and/or ``key``.\n\n        The difference between a ``sender`` and a ``key`` is that a ``sender`` refers to a specific\n        object's location in memory and uses :func:`builtins.id` for comparison while a ``key``\n        uses :func:`builtins.hash` for comparison. Thus two strings of the same value will always\n        be equal when used as a ``key`` but may be seen as different objects when used as a\n        ``sender``.\n\n\n        :param callback: A callable to be called when the signal is sent\n        :param Object sender: Any python object. Connects the callback to a single ``sender``.\n        :param list senders: An iterable of ``sender`` objects. Connects the callback against\n            multiple ``senders``.\n        :param str key: Any object that can be used as a key in a python dictionary. Connects the\n            callback against a single ``key``.\n        :param list keys: An iterable of ``key`` objects. Connects the callback against multiple\n            ``keys``.\n        :param weak: If ``True``, the callback will be stored as a weakreference. If a long-lived\n            reference is required, use ``False``.\n        '''\n        weak_callback = yield from self._get_ref(callback, weak)\n\n        # dispatch\n        if (sender is None) and (senders is None) and (key is None) and (keys is None):\n            # subscribe always activate the callback when the signal is sent\n            with (yield from self._lock_all):\n                self._all.add(weak_callback)\n        else:\n            if sender is not None:\n                yield from self._add_sender(sender, weak_callback)\n\n            if senders is not None:\n                for sender in senders:\n                    yield from self._add_sender(sender, weak_callback)\n\n            if key is not None:\n                yield from self._add_key(key, weak_callback)\n\n            if keys is not None:\n                for key in keys:\n                    yield from self._add_key(key, weak_callback)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef disconnect(self, callback=None, sender=None, senders=None, key=None, keys=None, weak=True):\n        '''\n        *This method is a coroutine.*\n\n        Disconnects the callback from the signal. If no arguments are\n        supplied for ``sender``, ``senders``, ``key``, or ``keys`` -- the callback is\n        completely disconnected. Otherwise, only the supplied ``senders`` and ``keys``\n        are disconnected for the callback.\n\n        .. Note::\n\n            the argument ``weak`` must be the same as when the callback was\n            connected to the signal.\n        '''\n        weak_callback = yield from self._get_ref(callback, weak)\n\n        if (sender is None) and (senders is None) and (key is None) and (keys is None):\n            # removing from _all signals\n            # need a lock because we are changing the size of the dict\n            if weak_callback in self._all:\n                with (yield from self._lock_all):\n                    self._all.remove(weak_callback)\n\n            with (yield from self._lock_by_senders):\n                sender_keys = list(self._by_senders.keys())\n            for sender in sender_keys:\n                yield from self._disconnect_from_sender(weak_callback, sender, is_id=True)\n\n            with (yield from self._lock_by_keys):\n                key_keys = list(self._by_keys.keys())\n            for key in key_keys:\n                yield from self._disconnect_from_key(weak_callback, key)\n\n        else:\n            # only disconnect from specific senders/keys\n            if sender is not None:\n                yield from self._disconnect_from_sender(weak_callback, sender)\n\n            if senders is not None:\n                for sender in senders:\n                    yield from self._disconnect_from_sender(weak_callback, sender)\n\n            if key is not None:\n                yield from self._disconnect_from_key(weak_callback, key)\n\n            if keys is not None:\n                for key in keys:\n                    yield from self._disconnect_from_key(weak_callback, key)", "response": "Disconnects the callback from the signal."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsend a signal to one or more senders and optionally a key.", "response": "def send(self, sender=None, senders=None, key=None, keys=None, **kwargs):\n        '''\n        *This method is a coroutine.*\n\n        Schedules connected callbacks for execution. You may optionally supply ``senders`` and/or\n        ``keys`` to include callbacks that were connected with one or more matching\n        ``senders`` and/or ``keys``.\n\n        Each call back will receive the following keyword arguments when called:\n            :signal: the signal that scheduled the execution of the callback\n            :senders: a :class:`set` of ``senders``\n            :keys: a :class:`set` of ``keys``\n            :\\*\\*kwargs: the additional kwargs supplied when the signal was created\n\n        :param kwargs: keyword pairs to send to the callbacks.\n            these override the defaults set when the\n            signal was initiated. You can only include\n            keywords set when the signal was created.\n\n        :Returns: the number of callbacks that received the signal\n        '''\n\n        default_kwargs = self._default_kwargs.copy()\n        for keyword in kwargs:\n            if keyword not in default_kwargs:\n                raise ValueError('You can not add new kwargs to an existing signal.')\n\n        default_kwargs.update(kwargs)\n\n        if senders is not None:\n            senders = set(senders)\n        else:\n            senders = set()\n        if sender is not None:\n            senders.add(sender)\n\n        if keys is not None:\n            keys = set(keys)\n        else:\n            keys = set()\n        if key is not None:\n            keys.add(key)\n\n        live_callbacks = set()\n\n        # collect callbacks connected to all send calls\n        with (yield from self._lock_all):\n            all_callbacks = yield from self._get_callbacks(self._all)\n\n        live_callbacks = live_callbacks | all_callbacks\n\n        # collect sender filtered callbacks\n        sender_callbacks = set()\n        for sender in senders:\n            id_ = yield from self._make_id(sender)\n            if id_ in self._by_senders:\n                sender_lock = self._get_lock(self._locks_senders, id_)\n                with (yield from sender_lock):\n                    new_sender_callbacks = yield from self._get_callbacks(self._by_senders[id_])\n\n                    if not new_sender_callbacks:\n                        with (yield from self._lock_by_senders):\n                            # Do some pruning\n                            del(self._by_senders[id_])\n                            del(self._locks_senders[id_])\n                    else:\n                        sender_callbacks = sender_callbacks | new_sender_callbacks\n\n        live_callbacks = live_callbacks | sender_callbacks\n\n        # collect key filtered callbacks\n        key_callbacks = set()\n        for key in keys:\n            if key in self._by_keys:\n                key_lock = self._get_lock(self._locks_keys, key)\n                with (yield from key_lock):\n                    new_key_callbacks = yield from self._get_callbacks(self._by_keys[key])\n\n                    if not new_key_callbacks:\n                        # Do some pruning\n                        with (yield from self._lock_by_keys):\n                            del(self._by_keys[key])\n                            del(self._locks_keys[key])\n                    else:\n                        key_callbacks = key_callbacks | new_key_callbacks\n\n        live_callbacks = live_callbacks | key_callbacks\n\n        # schedule all collected callbacks\n\n        for callback in live_callbacks:\n            yield from self._call_callback(callback,\n                                           senders,\n                                           keys,\n                                           **default_kwargs)\n\n        return len(live_callbacks)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make_thumbnail_name(image_name, extension):\n    file_name, _ = os.path.splitext(image_name)\n    return file_name + '.' + clean_extension(extension)", "response": "Return name of the downloaded thumbnail based on the extension."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the contents of the thumbnail of the given file.", "response": "def get_thumbnail_of_file(image_name, width):\n    \"\"\"Return the file contents of the thumbnail of the given file.\"\"\"\n    hdr = {'User-Agent': 'Python urllib2'}\n    url = make_thumb_url(image_name, width)\n    req = urllib2.Request(url, headers=hdr)\n    try:\n        logging.debug(\"Retrieving %s\", url)\n        opened = urllib2.urlopen(req)\n        extension = opened.headers.subtype\n        return opened.read(), make_thumbnail_name(image_name, extension)\n    except urllib2.HTTPError, e:\n        message = e.fp.read()\n        raise get_exception_based_on_api_message(message, image_name)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the exception based on the given API error message.", "response": "def get_exception_based_on_api_message(message, image_name=\"\"):\n    \"\"\"Return the exception matching the given API error message.\"\"\"\n    msg_bigger_than_source = re.compile('Image was not scaled, is the requested width bigger than the source?')\n    msg_does_not_exist = re.compile('The source file .* does not exist')\n    msg_does_not_exist_bis = re.compile('<div class=\"error\"><p>Value not found')\n    if re.search(msg_bigger_than_source, message):\n        msg = \"File %s requested at a width bigger than source\" % image_name\n        return RequestedWidthBiggerThanSourceException(msg)\n    elif re.search(msg_does_not_exist, message):\n        msg = \"File %s does not exist\" % image_name\n        return FileDoesNotExistException(msg)\n    elif re.search(msg_does_not_exist_bis, message):\n        msg = \"File %s does not exist\" % image_name\n        return FileDoesNotExistException(msg)\n    else:\n        return DownloadException(message)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef download_file(image_name, output_path, width=DEFAULT_WIDTH):\n    image_name = clean_up_filename(image_name)\n    logging.info(\"Downloading %s with width %s\", image_name, width)\n    try:\n        contents, output_file_name = get_thumbnail_of_file(image_name, width)\n    except RequestedWidthBiggerThanSourceException:\n        logging.warning(\"Requested width is bigger than source - downloading full size\")\n        contents, output_file_name = get_full_size_file(image_name)\n    output_file_path = os.path.join(output_path, output_file_name)\n    try:\n        with open(output_file_path, 'wb') as f:\n            logging.debug(\"Writing as %s\", output_file_path)\n            f.write(contents)\n        return output_file_path\n    except IOError, e:\n        msg = 'Could not write file %s on disk to %s: %s' % \\\n              (image_name, output_path, e.message)\n        logging.error(msg)\n        raise CouldNotWriteFileOnDiskException(msg)\n    except Exception, e:\n        logging.critical(e.message)\n        msg = 'An unexpected error occured when downloading %s to %s: %s' % \\\n              (image_name, output_path, e.message)\n        raise DownloadException(msg)", "response": "Download a given Wikimedia Commons file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nyielding the file names of a category by querying the MediaWiki API.", "response": "def get_category_files_from_api(category_name):\n    \"\"\"Yield the file names of a category by querying the MediaWiki API.\"\"\"\n    import mwclient\n    site = mwclient.Site('commons.wikimedia.org')\n    category = site.Categories[category_name]\n    return (x.page_title.encode('utf-8')\n            for x in category.members(namespace=6))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndownloads files of a given category.", "response": "def download_from_category(category_name, output_path, width):\n    \"\"\"Download files of a given category.\"\"\"\n    file_names = get_category_files_from_api(category_name)\n    files_to_download = izip_longest(file_names, [], fillvalue=width)\n    download_files_if_not_in_manifest(files_to_download, output_path)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nyielding the file names and widths by parsing a text file handler.", "response": "def get_files_from_textfile(textfile_handler):\n    \"\"\"Yield the file names and widths by parsing a text file handler.\"\"\"\n    for line in textfile_handler:\n        line = line.rstrip()\n        try:\n            (image_name, width) = line.rsplit(',', 1)\n            width = int(width)\n        except ValueError:\n            image_name = line\n            width = None\n        yield (image_name, width)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef download_from_files(files, output_path, width):\n    files_to_download = get_files_from_arguments(files, width)\n    download_files_if_not_in_manifest(files_to_download, output_path)", "response": "Download files from a given list of files."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef read_local_manifest(output_path):\n    local_manifest_path = get_local_manifest_path(output_path)\n    try:\n        with open(local_manifest_path, 'r') as f:\n            manifest = dict(get_files_from_textfile(f))\n            logging.debug('Retrieving %s elements from manifest', len(manifest))\n            return manifest\n    except IOError:\n        logging.debug('No local manifest at %s', local_manifest_path)\n        return {}", "response": "Return the contents of the local manifest as a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwrites the given file in manifest.", "response": "def write_file_to_manifest(file_name, width, manifest_fh):\n    \"\"\"Write the given file in manifest.\"\"\"\n    manifest_fh.write(\"%s,%s\\n\" % (file_name, str(width)))\n    logging.debug(\"Wrote file %s to manifest\", file_name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndownloads the given files to the given path unless in manifest.", "response": "def download_files_if_not_in_manifest(files_iterator, output_path):\n    \"\"\"Download the given files to the given path, unless in manifest.\"\"\"\n    local_manifest = read_local_manifest(output_path)\n    with open(get_local_manifest_path(output_path), 'a') as manifest_fh:\n        for (file_name, width) in files_iterator:\n            if is_file_in_manifest(file_name, width, local_manifest):\n                logging.info('Skipping file %s', file_name)\n                continue\n            try:\n                download_file(file_name, output_path, width=width)\n                write_file_to_manifest(file_name, width, manifest_fh)\n            except DownloadException, e:\n                logging.error(\"Could not download %s: %s\", file_name, e.message)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nallowing you to use pyyaml to load as OrderedDict.", "response": "def ordered_load(self, stream, Loader=yaml.Loader, object_pairs_hook=OrderedDict):\n        \"\"\"Allows you to use `pyyaml` to load as OrderedDict.\n\n        Taken from https://stackoverflow.com/a/21912744/1927102\n        \"\"\"\n        class OrderedLoader(Loader):\n            pass\n\n        def construct_mapping(loader, node):\n            loader.flatten_mapping(node)\n            return object_pairs_hook(loader.construct_pairs(node))\n        OrderedLoader.add_constructor(\n            yaml.resolver.BaseResolver.DEFAULT_MAPPING_TAG,\n            construct_mapping)\n        try:\n            try:\n                result = yaml.load(stream, OrderedLoader)\n            except yaml.scanner.ScannerError:\n                if type(stream) == str:\n                    result = json.loads(stream, object_pairs_hook=object_pairs_hook)\n                else:\n                    stream.seek(0)\n                    result = json.load(stream, object_pairs_hook=object_pairs_hook)\n        except Exception as e:\n            self.error(e)\n            result = {}\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef send(self, send_string, newline=None):\n        self.current_send_string = send_string\n        newline = newline if newline is not None else self.newline\n\n        self.channel.send(send_string + newline)", "response": "Saves and sends the send string provided."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef take_control(self):\n\n        if has_termios:\n            # Get attributes of the shell you were in before going to the\n            # new one\n            original_tty = termios.tcgetattr(sys.stdin)\n            try:\n                tty.setraw(sys.stdin.fileno())\n                tty.setcbreak(sys.stdin.fileno())\n\n                # We must set the timeout to 0 so that we can bypass times when\n                # there is no available text to receive\n                self.channel.settimeout(0)\n\n                # Loop forever until the user exits (i.e. read buffer is empty)\n                while True:\n                    select_read, select_write, select_exception = (\n                        select.select([self.channel, sys.stdin], [], [])\n                    )\n                    # Read any output from the terminal and print it to the\n                    # screen.  With timeout set to 0, we just can ignore times\n                    # when there's nothing to receive.\n                    if self.channel in select_read:\n                        try:\n                            buffer = self.channel.recv(self.buffer_size)\n                            if len(buffer) == 0:\n                                break\n                            sys.stdout.write(buffer.decode(self.encoding))\n                            sys.stdout.flush()\n                        except socket.timeout:\n                            pass\n                    # Send any keyboard input to the terminal one byte at a\n                    # time\n                    if sys.stdin in select_read:\n                        buffer = sys.stdin.read(1)\n                        if len(buffer) == 0:\n                            break\n                        self.channel.send(buffer)\n            finally:\n                # Restore the attributes of the shell you were in\n                termios.tcsetattr(sys.stdin, termios.TCSADRAIN, original_tty)\n        else:\n            def writeall(sock):\n                while True:\n                    buffer = sock.recv(self.buffer_size)\n                    if len(buffer) == 0:\n                        break\n                    sys.stdout.write(buffer.decode(self.encoding))\n                    sys.stdout.flush()\n\n            writer = threading.Thread(target=writeall, args=(self.channel,))\n            writer.start()\n\n            try:\n                while True:\n                    buffer = sys.stdin.read(1)\n                    if len(buffer) == 0:\n                        break\n                    self.channel.send(buffer)\n            # User has hit Ctrl+Z or F6\n            except EOFError:\n                pass", "response": "This function is used to take control of the user. It is used to take control of the user."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nuses the stored refresh token to get a new access token and expiration time.", "response": "def _get_access_from_refresh(self) -> Tuple[str, float]:\n        \"\"\"Uses the stored refresh token to get a new access token.\n\n        This method assumes that the refresh token exists.\n\n        Args:\n            None\n\n        Returns:\n            new access token and expiration time (from now)\n        \"\"\"\n        headers = self._get_authorization_headers()\n        data = {\n            'grant_type': 'refresh_token',\n            'refresh_token': self.refresh_token\n        }\n        r = self.session.post(self.TOKEN_URL, headers=headers, data=data)\n        response_data = r.json()\n        return (response_data['access_token'], response_data['expires_in'])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_authorization_headers(self) -> dict:\n        auth = base64.encodestring((self.client_id + ':' + self.client_secret).encode('latin-1')).decode('latin-1')\n        auth = auth.replace('\\n', '').replace(' ', '')\n        auth = 'Basic {}'.format(auth)\n        headers = {'Authorization': auth}\n        return headers", "response": "Constructs and returns the Authorization header for the client app."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nattempting to get a new access token using the refresh token. If the access token is expired and the refresh token is not yet set to the current time.", "response": "def _try_refresh_access_token(self) -> None:\n        \"\"\"Attempts to get a new access token using the refresh token, if needed.\n\n        If the access token is expired and this instance has a stored refresh token,\n        then the refresh token is in the API call to get a new access token. If\n        successful, this instance is modified in-place with that new access token.\n\n        Args:\n            None\n\n        Returns:\n            None\n        \"\"\"\n        if self.refresh_token:\n            if not self.access_token or self._is_access_token_expired():\n                self.access_token, self.access_expiration = self._get_access_from_refresh()\n                self.access_expiration = time.time() + self.access_expiration"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nauthenticates using the code from the EVE SSO.", "response": "def authenticate(self, code: str) -> 'Preston':\n        \"\"\"Authenticates using the code from the EVE SSO.\n\n        A new Preston object is returned; this object is not modified.\n\n        The intended usage is:\n\n            auth = preston.authenticate('some_code_here')\n\n        Args:\n            code: SSO code\n\n        Returns:\n            new Preston, authenticated\n        \"\"\"\n        headers = self._get_authorization_headers()\n        data = {\n            'grant_type': 'authorization_code',\n            'code': code\n        }\n        r = self.session.post(self.TOKEN_URL, headers=headers, data=data)\n        if not r.status_code == 200:\n            raise Exception(f'Could not authenticate, got repsonse code {r.status_code}')\n        new_kwargs = dict(self._kwargs)\n        response_data = r.json()\n        new_kwargs['access_token'] = response_data['access_token']\n        new_kwargs['access_expiration'] = time.time() + float(response_data['expires_in'])\n        new_kwargs['refresh_token'] = response_data['refresh_token']\n        return Preston(**new_kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfetch the OpenAPI spec from the server.", "response": "def _get_spec(self) -> dict:\n        \"\"\"Fetches the OpenAPI spec from the server.\n\n        If the spec has already been fetched, the cached version is returned instead.\n\n        ArgS:\n            None\n\n        Returns:\n            OpenAPI spec data\n        \"\"\"\n        if self.spec:\n            return self.spec\n        self.spec = requests.get(self.SPEC_URL.format(self.version)).json()\n        return self.spec"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsearching the spec for a path matching the operation id.", "response": "def _get_path_for_op_id(self, id: str) -> Optional[str]:\n        \"\"\"Searches the spec for a path matching the operation id.\n\n        Args:\n            id: operation id\n\n        Returns:\n            path to the endpoint, or None if not found\n        \"\"\"\n        for path_key, path_value in self._get_spec()['paths'].items():\n            for method in self.METHODS:\n                if method in path_value:\n                    if self.OPERATION_ID_KEY in path_value[method]:\n                        if path_value[method][self.OPERATION_ID_KEY] == id:\n                            return path_key\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _insert_vars(self, path: str, data: dict) -> str:\n        data = data.copy()\n        while True:\n            match = re.search(self.VAR_REPLACE_REGEX, path)\n            if not match:\n                return path\n            replace_from = match.group(0)\n            replace_with = str(data.get(match.group(1)))\n            path = path.replace(replace_from, replace_with)", "response": "Inserts variables into the ESI URL path."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef whoami(self) -> dict:\n        if not self.access_token:\n            return {}\n        self._try_refresh_access_token()\n        return self.session.get(self.WHOAMI_URL).json()", "response": "Returns the basic information about the authenticated character."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nquerying the ESI by an endpoint URL.", "response": "def get_path(self, path: str, data: dict) -> Tuple[dict, dict]:\n        \"\"\"Queries the ESI by an endpoint URL.\n\n        This method is not marked \"private\" as it _can_ be used\n        by consuming code, but it's probably easier to call the\n        `get_op` method instead.\n\n        Args:\n            path: raw ESI URL path\n            data: data to insert into the URL\n\n        Returns:\n            ESI data\n        \"\"\"\n        path = self._insert_vars(path, data)\n        path = self.BASE_URL + path\n        data = self.cache.check(path)\n        if data:\n            return data\n        self._try_refresh_access_token()\n        r = self.session.get(path)\n        self.cache.set(r)\n        return r.json()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_op(self, id: str, **kwargs: str) -> dict:\n        path = self._get_path_for_op_id(id)\n        return self.get_path(path, kwargs)", "response": "Queries the ESI by looking up an operation id."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef post_path(self, path: str, path_data: Union[dict, None], post_data: Any) -> dict:\n        path = self._insert_vars(path, path_data or {})\n        path = self.BASE_URL + path\n        self._try_refresh_access_token()\n        return self.session.post(path, json=post_data).json()", "response": "Modifies the ESI by an endpoint URL."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef post_op(self, id: str, path_data: Union[dict, None], post_data: Any) -> dict:\n        path = self._get_path_for_op_id(id)\n        return self.post_path(path, path_data, post_data)", "response": "Modifies the ESI by looking up an operation id and sending the data to the ESI."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_expiration(self, headers: dict) -> int:\n        expiration_str = headers.get('expires')\n        if not expiration_str:\n            return 0\n        expiration = datetime.strptime(expiration_str, '%a, %d %b %Y %H:%M:%S %Z')\n        delta = (expiration - datetime.utcnow()).total_seconds()\n        return math.ceil(abs(delta))", "response": "Gets the expiration time of the data from the response headers."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set(self, response: 'requests.Response') -> None:\n        self.data[response.url] = SavedEndpoint(\n            response.json(),\n            self._get_expiration(response.headers)\n        )", "response": "Adds a response to the cache."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _check_expiration(self, url: str, data: 'SavedEndpoint') -> 'SavedEndpoint':\n        if data.expires_after < time.time():\n            del self.data[url]\n            data = None\n        return data", "response": "Checks the expiration time for data for a url."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if data for a url has expired.", "response": "def check(self, url: str) -> Optional[dict]:\n        \"\"\"Check if data for a url has expired.\n\n        Data is not fetched again if it has expired.\n\n        Args:\n            url: url to check expiration on\n\n        Returns:\n            value of the data, possibly None\n        \"\"\"\n        data = self.data.get(url)\n        if data:\n            data = self._check_expiration(url, data)\n        return data.data if data else None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef deriv(self, n=1):\n        if n==1:\n            if self.order > 0:\n                return PowerSeries(self.c[1:]*range(1,len(self.c)))\n            else:\n                return PowerSeries([0. * self.c[0]])\n        elif n>1:\n            return self.deriv().deriv(n-1)\n        elif n==0:\n            return self", "response": "Compute the n th derivative of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef integ(self,n=1,x0=None):\n        if n==1:\n            if self.order<0:\n                ans = PowerSeries([0])\n            else:\n                ans = PowerSeries([0.*self.c[0]] +\n                    [x/(i+1.) for i,x in enumerate(self.c)])\n            if x0 is not None:\n                return PowerSeries([ans.c[0]-ans(x0)]+list(ans.c[1:]))\n            else:\n                return ans\n        elif n>1:\n            return self.integ().integ(n=n-1, x0=x0)\n        elif n==0:\n            return self", "response": "Return the n indefinite integral of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute y from y and dy from x and dy = deriv ( x y", "response": "def rk5_stepper(x, h, y , deriv, errors=False):\n    \"\"\" Compute y(x+h) from y and dy/dx=deriv(x,y).\n\n    Uses a one-step 5th-order Runge-Kutta algorithm.\n\n    Returns x+h, y(x+h) if errors is False; otherwise\n    returns x+h, y(x+h), yerr where yerr is an error\n    estimate.\n\n    Adapted from Numerical Recipes.\n    \"\"\"\n    k1 = h * deriv(x,y)\n    k2 = h * deriv(x+0.2*h, y+0.2*k1)\n    k3 = h * deriv(x+0.3*h, y+0.075*k1+0.225*k2)\n    k4 = h * deriv(x+0.6*h, y+0.3*k1-0.9*k2+1.2*k3)\n    k5 = h * deriv(x+h, y-.2037037037037037037037037*k1\n             +2.5*k2-2.592592592592592592592593*k3\n             +1.296296296296296296296296*k4)\n    k6 = h * deriv(x+0.875*h, y+.2949580439814814814814815e-1*k1\n             +.341796875*k2+.4159432870370370370370370e-1*k3\n             +.4003454137731481481481481*k4+.61767578125e-1*k5)\n    yn = y + (.9788359788359788359788361e-1*k1\n             +.4025764895330112721417070*k3\n             +.2104377104377104377104378*k4\n             +.2891022021456804065499718*k6)\n    xn = x+h\n    if errors:\n        yerr = (-.429377480158730158730159e-2*k1\n                +.186685860938578329882678e-1*k3\n                -.341550268308080808080807e-1*k4\n                -.1932198660714285714285714e-1*k5\n                +.391022021456804065499718e-1*k6)\n        return xn,yn,yerr\n    else:\n        return xn,yn"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef integral(fcn, interval, fcnshape=None, tol=1e-8, hmin=None):\n    if fcnshape is None:\n        fx0 = fcn(interval[0])\n        if hasattr(fx0, 'keys'):\n            fx0 = gvar.BufferDict(fx0)\n            fcnshape = None\n        else:\n            fcnshape = numpy.shape(fx0)\n    if fcnshape is None:\n        def deriv(x, y, fcn=fcn):\n            return gvar.BufferDict(fcn(x)).buf\n        y0 = fx0.buf * 0.0\n    else:\n        def deriv(x, y, fcn=fcn):\n            return fcn(x)\n        y0 = 0.0 if fcnshape == () else numpy.zeros(fcnshape, float)\n    odeint = Integrator(deriv=deriv, tol=tol, hmin=hmin)\n    ans = odeint(y0, interval=interval)\n    return ans if fcnshape is not None else gvar.BufferDict(fx0, buf=ans)", "response": "Calculates the integral of a function f over a given interval."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a solution function y ( x ) such that y = y0.", "response": "def solution(self, x0, y0):\n        \"\"\" Create a solution function ``y(x)`` such that ``y(x0) = y0``.\n\n        A list of solution values ``[y(x0), y(x1) ...]`` is returned if the\n        function is called with a list ``[x0, x1 ...]`` of ``x`` values.\n        \"\"\"\n        def soln(x):\n            if numpy.size(x) > 1:\n                x = [soln.x] + list(x)\n                ans = self(soln.y, interval=x)\n                soln.x = x[-1]\n                soln.y = ans[-1]\n                return ans\n            else:\n                soln.y = self(soln.y, interval=(soln.x, x))\n                soln.x = x\n                return soln.y\n        soln.x = x0\n        soln.y = y0\n        return soln"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef search(fcn, x0, incr=0, fac=1.1, maxit=100, analyzer=None):\n    # custom return class for result\n    class ans(tuple):\n        def __new__(cls, interval, nit, fcnval):\n            return tuple.__new__(cls, interval)\n        def __init__(self, interval, nit, fcnval):\n            self.interval = interval\n            self.nit = nit \n            self.fcnval = fcnval\n    x = x0\n    f = fcn(x)\n    for nit in range(maxit):\n        xo, fo = x, f\n        x = xo * fac + incr\n        f = fcn(x)\n        if analyzer!=None:\n            analyzer(x, f)\n        if f*fo<=0:\n            if numpy.fabs(fo)<=numpy.fabs(f):\n                return ans((xo, x), nit=nit + 1, fcnval=(fo, f))\n            else:\n                return ans((x, xo), nit=nit+1, fcnval=(f, fo))\n    raise RuntimeError(\"unable to bracket root\")", "response": "Search for and bracket root of one - dimensional function whose root is sought x0 and incr = 0. 1 and maxit = 100."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind root ``x`` of one-dimensional function ``fcn`` on an interval. This method finds a root ``x`` of ``fcn(x)=0`` inside an ``interval=(a,b)`` that brackets the root, with ``fcn(a) * fcn(b) <= 0``. This method is a pure Python adaptation of an algorithm from Richard Brent's book \"Algorithms for Minimization without Derivatives\" (1973). Being pure Python it works with :class:`gvar.GVar`-valued functions and variables. Example: The following code finds a root of ``sin(x)`` in the interval ``1 <= x <= 4``, using 7 iterative refinements of the initial interval:: >>> import math >>> import gvar as gv >>> root = gv.root.refine(math.sin, (1, 4)) >>> print(root) 3.14159265359 >>> print(root.nit) 7 Args: fcn: One-dimensional function whose zero/root is sought. interval: Tuple ``(a,b)`` specifying an interval containing the root, with ``fcn(a) * fcn(b) <= 0``. The search for a root is confined to this interval. rtol (float, optional): Relative tolerance for the root. The default value is ``None``, which sets ``rtol`` equal to machine precision (``sys.float_info.epsilon``). A larger value usually leads to less precision but is faster. maxit (int, optional): Maximum number of iterations used to find a root with the given tolerance. A warning is issued if the algorithm does not converge in time. (Default value is 1000.) analyzer: Optional function ``f(x, fcn(x))`` that is called for each point ``x`` examined by the algorithm. This can be used, for example, to monitor convergence while debugging. Default is ``None``. Returns: The root, which is either a ``float`` or a :class:`gvar.GVar` but with extra attributes that provide additional information about the root: - **nit** --- Number of iterations used to find the root. - **interval** --- Smallest interval ``(b,c)`` found containing the root, where ``b`` is the root returned by the method. - **fcnval** --- Value of ``fcn(x)`` at the root. Raises: ValueError: If ``fcn(a) * fcn(b) > 0`` for initial interval ``(a,b)``. UserWarning: If the algorithm fails to converge after ``maxit`` iterations.", "response": "def refine(fcn, interval, rtol=None, maxit=1000, analyzer=None):\n    \"\"\" Find root ``x`` of one-dimensional function ``fcn`` on an interval.\n\n    This method finds a root ``x`` of ``fcn(x)=0`` inside an ``interval=(a,b)``\n    that brackets the root, with ``fcn(a) * fcn(b) <= 0``.\n\n    This method is a pure Python adaptation of an algorithm\n    from Richard Brent's book \"Algorithms for Minimization \n    without Derivatives\" (1973). Being pure Python it works with\n    :class:`gvar.GVar`-valued functions and variables.\n\n    Example:\n        The following code finds a root of ``sin(x)`` in the interval\n        ``1 <= x <= 4``, using 7 iterative refinements of the initial\n        interval::\n\n            >>> import math\n            >>> import gvar as gv\n            >>> root = gv.root.refine(math.sin, (1, 4))\n            >>> print(root)\n            3.14159265359\n            >>> print(root.nit)\n            7\n\n    Args:\n        fcn: One-dimensional function whose zero/root is sought.\n        interval: Tuple ``(a,b)`` specifying an interval containing\n            the root, with ``fcn(a) * fcn(b) <= 0``. The search \n            for a root is confined to this interval.\n        rtol (float, optional): Relative tolerance for the root. The default \n            value is ``None``, which sets ``rtol`` equal to machine\n            precision (``sys.float_info.epsilon``). A larger value \n            usually leads to less precision but is faster.\n        maxit (int, optional): Maximum number of iterations used to find \n            a root with the given tolerance. A warning is \n            issued if the algorithm does not converge in time.\n            (Default value is 1000.)\n        analyzer: Optional function ``f(x, fcn(x))`` that is called\n            for each point ``x`` examined by the algorithm. This can \n            be used, for example, to monitor convergence while \n            debugging. Default is ``None``.\n\n    Returns:\n        The root, which is either a ``float`` or \n        a :class:`gvar.GVar` but with extra attributes that\n        provide additional information about the root: \n\n        - **nit** --- Number of iterations used to find the root.\n\n        - **interval** --- Smallest interval ``(b,c)`` found containing\n          the root, where ``b`` is the root returned by the method.\n\n        - **fcnval** --- Value of ``fcn(x)`` at the root.\n\n    Raises:\n        ValueError: If ``fcn(a) * fcn(b) > 0`` for initial \n            interval ``(a,b)``.\n        UserWarning: If the algorithm fails to converge \n            after ``maxit`` iterations.\n    \"\"\"\n    # Create custom class for answer\n    def ans(root, interval, nit, fcnval):\n        if isinstance(root, gvar.GVar):\n            class _ans(gvar.GVar):\n                def __init__(self, root):\n                    super(_ans, self).__init__(*root.internaldata)\n        else:\n            class _ans(float):\n                def __new__(cls, root):\n                    return float.__new__(cls, root)\n        root = _ans(root)\n        root.interval = interval\n        root.nit = nit \n        root.fcnval = fcnval\n        return root\n\n    # Throughout the routine: root is in interval (b,c), \n    # b is the best value, and a is the previous value of b.\n    if rtol is None:\n        rtol = sys.float_info.epsilon\n    if rtol < 0:\n        raise ValueError('negative rtol: {}'.format(rtol))\n    a, b = interval \n    fa, fb = fcn(a), fcn(b)  \n    if (fa > 0) == (fb > 0):\n        raise ValueError(\"fcn(a)*fcn(b) is not negative for (a,b)=interval\")\n    if analyzer is not None:\n        analyzer(a, fa)\n        analyzer(b, fb)        \n    # put a,b,c into canonical order by swapping as necessary.\n    if numpy.fabs(fa) < numpy.fabs(fb):\n        a, b = b, a \n        fa, fb = fb, fa\n    c, fc = a, fa\n    d = b - a \n    e = d\n    for nit in range(maxit):\n        tol = 2 * sys.float_info.epsilon * numpy.fabs(b) + rtol \n        m = 0.5 * (c - b) \n        if numpy.fabs(m) < tol or fb == 0:\n            # found root\n            # refine.fcnval = fb\n            # refine.nit = nit\n            # refine.interval = (b, c)\n            return ans(b, fcnval=fb, nit=nit, interval=(b, c))\n        # check for bisection\n        if numpy.fabs(e) < tol or numpy.fabs(fa) <= numpy.fabs(fb):\n            d = m \n            e = m\n        else:\n            s = fb / fa \n            if a == c:\n                # linear interpolation\n                p = 2 * m * s\n                q = 1. - s\n            else:\n                # inverse quadratic interpolation\n                q = fa / fc \n                r = fb / fc \n                p = s * (2. * m * q * (q-r) - (b - a) * (r - 1.))\n                q = (q - 1.) * (r - 1.) * (s - 1.)\n            if p > 0:\n                q = -q \n            else:\n                p = -p\n            s = e\n            e = d\n            if 2 * p < 3 * m * q - numpy.fabs(tol * q) and p < numpy.fabs(0.5 * s * q):\n                d = p / q \n            else:\n                d = m \n                e = m\n        a, fa = b, fb\n        if numpy.fabs(d) > tol:\n            b = b + d \n        elif m > 0:\n            b = b + tol\n        else:\n            b = b - tol \n        fb = fcn(b)\n        if analyzer is not None:\n            analyzer(b, fb)\n        # reorder a,b,c if necessary\n        if (fb > 0) == (fc > 0):\n            c, fc = a, fa\n            d = b - a \n            e = d\n        if numpy.fabs(fc) < numpy.fabs(fb):\n            a, b = b, c\n            c = a\n            fa, fb = fb, fc\n            fc = fa \n    warnings.warn(\n        \"failed to converge in maxit={} iterations\".format(maxit)\n        ) \n    # refine.fcnval = fb\n    # refine.nit = nit\n    # refine.interval = (b, c)            \n    return ans(b, fcnval=fb, nit=nit, interval=(b, c))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nappending version number to gvar. py", "response": "def run(self):\n        \"\"\" Append version number to gvar/__init__.py \"\"\"\n        with open('src/gvar/__init__.py', 'a') as gvfile:\n            gvfile.write(\"\\n__version__ = '%s'\\n\" % GVAR_VERSION)\n        _build_py.run(self)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef eigh(a, eigvec=True, rcond=None):\n    a = numpy.asarray(a)\n    if a.dtype != object:\n        val, vec = numpy.linalg.eigh(a)\n        return (val, vec) if eigvec else val\n    amean = gvar.mean(a)\n    if amean.ndim != 2 or amean.shape[0] != amean.shape[1]:\n        raise ValueError('bad matrix shape: ' + str(a.shape))\n    if rcond is None:\n        rcond = numpy.finfo(float).eps * max(a.shape)\n    da = a - amean\n    val0, vec0 = numpy.linalg.eigh(amean)\n    val = val0 + [\n        vec0[:, i].conjugate().dot(da.dot(vec0[:, i])) for i in range(vec0.shape[1])\n        ]\n    if eigvec == True:\n        if vec0.dtype == complex:\n            raise ValueError('cannot evaluate eigenvectors when a is complex')\n        vec = numpy.array(vec0, dtype=object)\n        for i in range(len(val)):\n            for j in range(len(val)):\n                dval = val0[i] - val0[j]\n                if abs(dval) < rcond * abs(val0[j] + val0[i]) or dval == 0.0:\n                    continue\n                vec[:, i] += vec0[:, j] * (\n                    vec0[:, j].dot(da.dot(vec0[:, i])) / dval\n                    )\n        return val, vec\n    else:\n        return val", "response": "Computes the eigenvalues and eigenvectors of a symmetric matrix."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef svd(a, compute_uv=True, rcond=None):\n    a = numpy.asarray(a)\n    if a.dtype != object:\n        return numpy.linalg.svd(a, compute_uv=compute_uv)\n    amean = gvar.mean(a)\n    if amean.ndim != 2:\n        raise ValueError(\n            'matrix must have dimension 2: actual shape = ' + str(a.shape)\n            )\n    if rcond is None:\n        rcond = numpy.finfo(float).eps * max(a.shape)\n    da = a - amean\n    u0,s0,v0T = numpy.linalg.svd(amean, compute_uv=True, full_matrices=True)\n    k = min(a.shape)\n    s = s0 + [\n        u0[:, i].dot(da.dot(v0T[i, :])) for i in range(k)\n        ]\n    if compute_uv:\n        u = numpy.array(u0, dtype=object)\n        vT = numpy.array(v0T, dtype=object)\n        # u first\n        daaT = da.dot(a.T) + a.dot(da.T)\n        s02 = numpy.zeros(daaT.shape[0], float)\n        s02[:len(s0)] = s0 ** 2\n        for j in range(s02.shape[0]):\n            for i in range(k):\n                if i == j:\n                    continue\n                ds2 = s02[i]  - s02[j]\n                if abs(ds2) < rcond * abs(s02[i] + s02[j]) or ds2 == 0:\n                    continue\n                u[:, i] +=  u0[:, j]  * u0[:, j].dot(daaT.dot(u0[:, i])) / ds2\n        # v next\n        daTa = da.T.dot(a) + a.T.dot(da)\n        s02 = numpy.zeros(daTa.shape[0], float)\n        s02[:len(s0)] = s0 ** 2\n        for j in range(s02.shape[0]):\n            for i in range(k):\n                if i == j:\n                    continue\n                ds2 = s02[i]  - s02[j]\n                if abs(ds2) < rcond * abs(s02[i] + s02[j]) or ds2 == 0:\n                    continue\n                vT[i, :] +=  v0T[j, :]  * v0T[j, :].dot(daTa.dot(v0T[i, :])) / ds2\n        return u[:,:k], s, vT[:k, :]\n    else:\n        return s", "response": "SVD decomposition of a two - dimensional matrix containing numbers\n            and or GVar \\ s \\ n."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef lstsq(a, b, rcond=None, weighted=False, extrainfo=False):\n    a = numpy.asarray(a)\n    b = numpy.asarray(b)\n    if a.ndim != 2:\n        raise ValueError(\n            'a must have dimension 2: actual shape = ' + str(a.shape)\n            )\n    if a.shape[0] != b.shape[0]:\n        raise ValueError(\n            'a and b shapes mismatched: {} vs {}'.format(a.shape, b.shape)\n            )\n    if rcond is None:\n        rcond = numpy.finfo(float).eps * max(a.shape)\n    if weighted:\n        try:\n            cov = gvar.evalcov(b)\n        except ValueError:\n            raise ValueError('b does not have a covariance matrix')\n        try:\n            icov = numpy.linalg.inv(cov)\n        except numpy.linalg.LinAlgError:\n            raise ValueError(\"b's covariance matrix cannot be inverted\")\n        ata = a.T.dot(icov.dot(a))\n        atb = a.T.dot(icov.dot(b))\n    else:\n        ata = a.T.dot(a)\n        atb = a.T.dot(b)\n    val, vec = gvar.linalg.eigh(ata)\n    maxval = numpy.max(gvar.mean(val))  # N.B. val > 0 required\n    ans = 0\n    for i in range(len(val)):\n        if gvar.mean(val[i]) < rcond * maxval:\n            continue\n        ans += vec[:, i] * vec[:, i].dot(atb) / val[i]\n    if not extrainfo:\n        return ans\n    val = val[val >= rcond * maxval] ** 0.5\n    d = a.dot(ans) - b\n    residual = d.dot(icov.dot(d)) if weighted else d.dot(d)\n    k = len(val)\n    return ans, residual, k, val", "response": "Return the Least - Squares solution x to a and b."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninversing of matrix a.", "response": "def inv(a):\n    \"\"\" Inverse of matrix ``a``.\n\n    Args:\n        a: Two-dimensional, square matrix/array of numbers\n            and/or :class:`gvar.GVar`\\s.\n\n    Returns:\n        The inverse of matrix ``a``.\n\n    Raises:\n        ValueError: If matrix is not square and two-dimensional.\n    \"\"\"\n    amean = gvar.mean(a)\n    if amean.ndim != 2 or amean.shape[0] != amean.shape[1]:\n        raise ValueError('bad matrix shape: ' + str(a.shape))\n    da = a - amean\n    ainv = numpy.linalg.inv(amean)\n    return ainv - ainv.dot(da.dot(ainv))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsolving the identity matrix a and b.", "response": "def solve(a, b):\n    \"\"\" Find ``x`` such that ``a @ x = b`` for matrix ``a``.\n\n    Args:\n        a: Two-dimensional, square matrix/array of numbers\n            and/or :class:`gvar.GVar`\\s.\n        b: One-dimensional vector/array of numbers and/or\n            :class:`gvar.GVar`\\s, or an array of such vectors.\n            Requires ``b.shape[0] == a.shape[1]``.\n\n    Returns:\n        The solution ``x`` of ``a.dot(x) = b``, which is equivalent\n        to ``inv(a).dot(b)``.\n\n    Raises:\n        ValueError: If ``a`` is not square and two-dimensional.\n        ValueError: If shape of ``b`` does not match that of ``a``\n            (that is ``b.shape[0] != a.shape[1]``).\n    \"\"\"\n    amean = gvar.mean(a)\n    if amean.ndim != 2 or amean.shape[0] != amean.shape[1]:\n        raise ValueError('bad matrix shape: ' + str(a.shape))\n    bmean = gvar.mean(b)\n    if bmean.shape[0] != a.shape[1]:\n        raise ValueError(\n            'Mismatch between shapes of a and b: {} {}'.format(a.shape, b.shape)\n            )\n    # xmean = numpy.linalg.solve(amean, bmean)\n    ainv = numpy.linalg.inv(amean)\n    xmean = ainv.dot(bmean)\n    return xmean + ainv.dot(b-bmean - (a-amean).dot(xmean))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfinding oscillation period of y.", "response": "def find_period(y, Tapprox):\n    \"\"\" Find oscillation period of y(t).\n\n    Parameter Tapprox is the approximate period. The code finds the time\n    between 0.7 * Tapprox and 1.3 * Tapprox where y(t)[1] = d/dt theta(t)\n    vanishes. This is the period.\n    \"\"\"\n    def dtheta_dt(t):\n        \"\"\" vanishes when dtheta/dt = 0 \"\"\"\n        return y(t)[1]\n    return  gv.root.refine(dtheta_dt, (0.7 * Tapprox, 1.3 * Tapprox))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates pendulum for the pendulum.", "response": "def make_pendulum(theta_max, l):\n    \"\"\" Create ode solution y(t) = [theta(t), d/dt theta(t)].\n\n    Initial conditions are y(0) = [theta_max, 0]. Parameter l is the\n    length of the pendulum.\n    \"\"\"\n    g_l = 9.8 / l\n    def deriv(t, y):\n        \"\"\" Calculate [d/d theta(t), d/dt d/dt theta(t)]. \"\"\"\n        theta, dtheta_dt = y\n        return np.array([dtheta_dt, - g_l * gv.sin(theta)])\n    y0 = np.array([theta_max, 0.0])\n    return gv.ode.Integrator(deriv=deriv).solution(0.0, y0)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ranseed(seed=None):\n    if seed is None:\n        seed = numpy.random.randint(1, int(2e9), size=3)\n    try:\n        seed = tuple(seed)\n    except TypeError:\n        pass\n    numpy.random.seed(seed)\n    ranseed.seed = seed\n    return seed", "response": "Returns a random number generator with seed."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef switch_gvar(cov=None):\n    global gvar\n    _GVAR_LIST.append(gvar)\n    gvar = GVarFactory(cov)\n    return gvar", "response": "Switch gvar to new GVarFactory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the chi^2 of g1 - g2.", "response": "def chi2(g1, g2=None, svdcut=1e-12, nocorr=False):\n    \"\"\" Compute chi**2 of ``g1-g2``.\n\n    ``chi**2`` is a measure of whether the multi-dimensional\n    Gaussian distributions ``g1`` and ``g2`` (dictionaries or arrays)\n    agree with each other --- that is, do their means agree\n    within errors for corresponding elements. The probability is high\n    if ``chi2(g1,g2)/chi2.dof`` is of order 1 or smaller.\n\n    Usually ``g1`` and ``g2`` are dictionaries with the same keys,\n    where ``g1[k]`` and ``g2[k]`` are |GVar|\\s or arrays of\n    |GVar|\\s having the same shape. Alternatively ``g1`` and ``g2``\n    can be |GVar|\\s, or arrays of |GVar|\\s having the same shape.\n\n    One of ``g1`` or ``g2`` can contain numbers instead of |GVar|\\s,\n    in which case ``chi**2`` is a measure of the likelihood that\n    the numbers came from the distribution specified by the other\n    argument.\n\n    One or the other of ``g1`` or ``g2`` can be missing keys, or missing\n    elements from arrays. Only the parts of ``g1`` and ``g2`` that\n    overlap are used. Also setting ``g2=None`` is equivalent to replacing its\n    elements by zeros.\n\n    ``chi**2`` is computed from the inverse of the covariance matrix\n    of ``g1-g2``. The matrix inversion can be sensitive to roundoff\n    errors. In such cases, SVD cuts can be applied by setting\n    parameters ``svdcut``; see the documentation\n    for :func:`gvar.svd`, which is used to apply the cut.\n\n    The return value is the ``chi**2``. Extra attributes attached to this\n    value give additional information:\n\n    - **dof** --- Number of degrees of freedom (that is, the number of variables\n      compared).\n\n    - **Q** --- The probability that the ``chi**2`` could have been larger,\n      by chance, even if ``g1`` and ``g2`` agree. Values smaller than 0.1\n      or so suggest that they do not agree. Also called the *p-value*.\n    \"\"\"\n    # customized class for answer\n    class ans(float):\n        def __new__(cls, chi2, dof, Q):\n            return float.__new__(cls, chi2)\n        def __init__(self, chi2, dof, Q):\n            self.dof = dof\n            self.Q = Q\n            self.chi2 = chi2\n\n    # leaving nocorr (turn off correlations) undocumented because I\n    #   suspect I will remove it\n    if g2 is None:\n        diff = BufferDict(g1).buf if hasattr(g1, 'keys') else numpy.asarray(g1).flatten()\n    elif hasattr(g1, 'keys') and hasattr(g2, 'keys'):\n        # g1 and g2 are dictionaries\n        g1 = BufferDict(g1)\n        g2 = BufferDict(g2)\n        diff = BufferDict()\n        keys = set(g1.keys())\n        keys = keys.intersection(g2.keys())\n        for k in keys:\n            g1k = g1[k]\n            g2k = g2[k]\n            shape = tuple(\n                [min(s1,s2) for s1, s2 in zip(numpy.shape(g1k), numpy.shape(g2k))]\n                )\n            diff[k] = numpy.zeros(shape, object)\n            if len(shape) == 0:\n                diff[k] = g1k - g2k\n            else:\n                for i in numpy.ndindex(shape):\n                    diff[k][i] = g1k[i] - g2k[i]\n        diff = diff.buf\n    elif not hasattr(g1, 'keys') and not hasattr(g2, 'keys'):\n        # g1 and g2 are arrays or scalars\n        g1 = numpy.asarray(g1)\n        g2 = numpy.asarray(g2)\n        shape = tuple(\n            [min(s1,s2) for s1, s2 in zip(numpy.shape(g1), numpy.shape(g2))]\n            )\n        diff = numpy.zeros(shape, object)\n        if len(shape) == 0:\n            diff = numpy.array(g1 - g2)\n        else:\n            for i in numpy.ndindex(shape):\n                diff[i] = g1[i] - g2[i]\n        diff = diff.flatten()\n    else:\n        # g1 and g2 are something else\n        raise ValueError(\n            'cannot compute chi**2 for types ' + str(type(g1)) + ' ' +\n            str(type(g2))\n            )\n    dof = diff.size\n    if dof == 0:\n        return ans(0.0, 0, 0)\n    if nocorr:\n        # ignore correlations\n        chi2 = numpy.sum(mean(diff) ** 2 / var(diff))\n        dof = len(diff)\n    else:\n        diffmod, i_wgts = svd(diff, svdcut=svdcut, wgts=-1)\n        diffmean = mean(diffmod)\n        i, wgts = i_wgts[0]\n        chi2 = 0.0\n        if len(i) > 0:\n            chi2 += numpy.sum((diffmean[i] * wgts) ** 2)\n        for i, wgts in i_wgts[1:]:\n            chi2 += numpy.sum(wgts.dot(diffmean[i]) ** 2)\n        dof = sum(len(wgts) for i, wgts in i_wgts)\n    Q = gammaQ(dof/2., chi2/2.)\n    return ans(chi2, dof=dof, Q=Q)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef equivalent(g1, g2, rtol=1e-10, atol=1e-10):\n    atol = abs(atol)\n    rtol = abs(rtol)\n    if hasattr(g1, 'keys') and hasattr(g2, 'keys'):\n        # g1 and g2 are dictionaries\n        g1 = BufferDict(g1)\n        g2 = BufferDict(g2)\n        diff = BufferDict()\n        summ = BufferDict()\n        keys = set(g1.keys())\n        keys = keys.intersection(g2.keys())\n        for k in keys:\n            g1k = g1[k]\n            g2k = g2[k]\n            shape = tuple(\n                [min(s1,s2) for s1, s2 in zip(numpy.shape(g1k), numpy.shape(g2k))]\n                )\n            diff[k] = numpy.zeros(shape, object)\n            summ[k] = numpy.zeros(shape, object)\n            if len(shape) == 0:\n                diff[k] = g1k - g2k\n                summ[k] = g1k + g2k\n            else:\n                for i in numpy.ndindex(shape):\n                    diff[k][i] = g1k[i] - g2k[i]\n                    summ[k][i] = g1k[i] + g2k[i]\n        diff = diff.buf\n        summ = summ.buf\n    elif not hasattr(g1, 'keys') and not hasattr(g2, 'keys'):\n        # g1 and g2 are arrays or scalars\n        g1 = numpy.asarray(g1)\n        g2 = numpy.asarray(g2)\n        shape = tuple(\n            [min(s1,s2) for s1, s2 in zip(numpy.shape(g1), numpy.shape(g2))]\n            )\n        diff = numpy.zeros(shape, object)\n        summ = numpy.zeros(shape, object)\n        if len(shape) == 0:\n            diff = numpy.array(g1 - g2)\n            summ = numpy.array(g1 + g2)\n        else:\n            for i in numpy.ndindex(shape):\n                diff[i] = g1[i] - g2[i]\n                summ[i] = g1[i] + g2[i]\n        diff = diff.flatten()\n        summ = summ.flatten()\n    else:\n        # g1 and g2 are something else\n        raise ValueError(\n            'cannot compare types ' + str(type(g1)) + ' ' +\n            str(type(g2))\n            )\n    if diff.size == 0:\n        return True\n\n    avgg = summ / 2.\n    # check means\n    dmean = mean(diff)\n    amean = mean(avgg)\n    if not numpy.all(numpy.abs(dmean) < (numpy.abs(amean) * rtol + atol)):\n        return False\n\n    # check derivatives\n    for ai, di in zip(avgg, diff):\n        # focus on large derivatives to avoid comparing noise to noise\n        ai_der = numpy.abs(ai.der)\n        di_der = numpy.abs(di.der)\n        idx = (ai_der > rtol * max(ai_der))\n        if not numpy.all(di_der[idx] < ai_der[idx] * rtol + atol):\n            return False\n    return True", "response": "Determines whether two sets of GVar objects are equivalent."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning string containing chi**2 / downto and Q from a base - level object.", "response": "def fmt_chi2(f):\n    \"\"\" Return string containing ``chi**2/dof``, ``dof`` and ``Q`` from ``f``.\n\n    Assumes ``f`` has attributes ``chi2``, ``dof`` and ``Q``. The\n    logarithm of the Bayes factor will also be printed if ``f`` has\n    attribute ``logGBF``.\n    \"\"\"\n    if hasattr(f, 'logGBF'):\n        fmt = \"chi2/dof = %.2g [%d]    Q = %.2g    log(GBF) = %.5g\"\n        chi2_dof = f.chi2 / f.dof if f.dof != 0 else 0\n        return fmt % (chi2_dof, f.dof, f.Q, f.logGBF)\n    else:\n        fmt = \"chi2/dof = %.2g [%d]    Q = %.2g\"\n        chi2_dof = f.chi2 / f.dof if f.dof != 0 else 0\n        return fmt % (chi2_dof, f.dof, f.Q)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\napply SVD cuts to collection of GVar or dictionary of GVar.", "response": "def svd(g, svdcut=1e-12, wgts=False, add_svdnoise=False):\n    \"\"\" Apply SVD cuts to collection of |GVar|\\s in ``g``.\n\n    Standard usage is, for example, ::\n\n        svdcut = ...\n        gmod = svd(g, svdcut=svdcut)\n\n    where ``g`` is an array of |GVar|\\s or a dictionary containing |GVar|\\s\n    and/or arrays of |GVar|\\s. When ``svdcut>0``, ``gmod`` is\n    a copy of ``g`` whose |GVar|\\s have been modified to make\n    their correlation matrix less singular than that of the\n    original ``g``: each eigenvalue ``eig`` of the correlation matrix is\n    replaced by ``max(eig, svdcut * max_eig)`` where ``max_eig`` is\n    the largest eigenvalue. This SVD cut, which is applied separately\n    to each block-diagonal sub-matrix of the correlation matrix,\n    increases the variance of the eigenmodes with eigenvalues smaller\n    than ``svdcut * max_eig``.\n\n    The modification of ``g``'s covariance matrix is implemented by adding\n    (to ``g``) a set of |GVar|\\s with zero means::\n\n        gmod = g + gmod.svdcorrection\n\n    where ``gmod.svdcorrection`` is an array/dictionary\n    containing the |GVar|\\s. If\n    parameter ``add_svdnoise=True``,\n    noise is included in ``gmod.svdcorrection``,\n    ::\n\n        gmod.svdcorrection += gv.sample(gmod.svdcorrection),\n\n    before it is added to ``g``. The noise can be useful for testing fits\n    and other applications.\n\n    When ``svdcut`` is negative, eigenmodes of the correlation matrix\n    whose eigenvalues are smaller than ``|svdcut| * max_eig`` are dropped\n    from the new matrix and the corresponding components of ``g`` are\n    zeroed out (that is, replaced by 0(0)) in ``gmod``.\n\n    There is an additional parameter ``wgts`` in :func:`gvar.svd` whose\n    default value is ``False``. Setting ``wgts=1`` or ``wgts=-1`` instead\n    causes :func:`gvar.svd` to return a tuple ``(gmod, i_wgts)`` where\n    ``gmod``  is the modified copy of ``g``, and ``i_wgts`` contains a\n    spectral  decomposition of the covariance matrix corresponding to\n    the  modified correlation matrix if ``wgts=1``, or a decomposition of its\n    inverse if ``wgts=-1``. The first entry ``i, wgts = i_wgts[0]``  specifies\n    the diagonal part of the matrix: ``i`` is a list of the indices in\n    ``gmod.flat`` corresponding to diagonal elements, and ``wgts ** 2``\n    gives the corresponding matrix elements. The second and subsequent\n    entries, ``i, wgts = i_wgts[n]`` for ``n > 0``, each correspond\n    to block-diagonal sub-matrices, where ``i`` is the list of\n    indices corresponding to the block, and ``wgts[j]`` are eigenvectors of\n    the sub-matrix rescaled so that ::\n\n        numpy.sum(numpy.outer(wi, wi) for wi in wgts[j]\n\n    is the sub-matrix (``wgts=1``) or its inverse (``wgts=-1``).\n\n    To compute the inverse of the covariance matrix from ``i_wgts``,\n    for example, one could use code like::\n\n        gmod, i_wgts = svd(g, svdcut=svdcut, wgts=-1)\n\n        inv_cov = numpy.zeros((n, n), float)\n        i, wgts = i_wgts[0]                       # 1x1 sub-matrices\n        if len(i) > 0:\n            inv_cov[i, i] = numpy.array(wgts) ** 2\n        for i, wgts in i_wgts[1:]:                # nxn sub-matrices (n>1)\n            for w in wgts:\n                inv_cov[i[:, None], i] += numpy.outer(w, w)\n\n    This sets ``inv_cov`` equal to the inverse of the covariance matrix of\n    the ``gmod``\\s. Similarly, we can  compute the expectation value,\n    ``u.dot(inv_cov.dot(v))``, between two vectors (:mod:`numpy` arrays)\n    using::\n\n        result = 0.0\n        i, wgts = i_wgts[0]                       # 1x1 sub-matrices\n        if len(i) > 0:\n            result += numpy.sum((u[i] * wgts) * (v[i] * wgts))\n        for i, wgts in i_wgts[1:]:                # nxn sub-matrices (n>1)\n            result += numpy.sum(wgts.dot(u[i]) * wgts.dot(v[i]))\n\n    where ``result`` is the desired expectation value.\n\n    Args:\n\n        g: An array of |GVar|\\s or a dicitionary whose values are\n            |GVar|\\s and/or arrays of |GVar|\\s.\n        svdcut (None or float): If positive, replace eigenvalues ``eig``\n            of the correlation matrix with ``max(eig, svdcut * max_eig)``\n            where ``max_eig`` is the largest eigenvalue; if negative, discard\n            eigenmodes with eigenvalues smaller than ``|svdcut| * max_eig``.\n            Note ``|svdcut| < 1``. Default is 1e-12.\n        wgts: Setting ``wgts=1`` causes :func:`gvar.svd` to compute\n            and return a spectral decomposition of the covariance matrix of\n            the modified |GVar|\\s, ``gmod``. Setting ``wgts=-1`` results in\n            a decomposition of the inverse of the covariance matrix. The\n            default value is ``False``, in which case only ``gmod`` is returned.\n        add_svdnoise: If ``True``, noise is added to the SVD correction (see\n            above).\n\n    Returns:\n        A copy ``gmod`` of ``g`` whose correlation matrix is modified by\n        SVD cuts. If ``wgts`` is not ``False``,\n        a tuple ``(g, i_wgts)`` is returned where ``i_wgts``\n        contains a spectral decomposition of ``gmod``'s\n        covariance matrix or its inverse.\n\n    Data from the SVD analysis is stored in ``gmod``:\n\n    .. attribute:: gmod.svdcut\n\n        SVD cut used to create ``gmod``.\n\n    .. attribute:: gmod.dof\n\n        Number of independent degrees of freedom left after the\n        SVD cut. This is the same as the number initially unless\n        ``svdcut < 0`` in which case it may be smaller.\n\n    .. attribute:: gmod.nmod\n\n        Number of modes whose eignevalue was modified by the\n        SVD cut.\n\n    .. attribute:: gmod.nblocks\n\n        A dictionary where ``gmod.nblocks[s]`` contains the number of\n        block-diagonal ``s``-by-``s`` sub-matrices in the correlation\n        matrix.\n\n    .. attribute:: gmod.eigen_range\n\n        Ratio of the smallest to largest eigenvalue before SVD cuts are\n        applied (but after rescaling).\n\n    .. attribute:: gmod.logdet\n\n        Logarithm of the determinant of the covariance matrix after SVD\n        cuts are applied (excluding any omitted modes when\n        ``svdcut < 0`` and any diagonal zero modes).\n\n    .. attribute:: gmod.svdcorrection\n\n        Array or dictionary containing the SVD corrections added to ``g``\n        to create ``gmod``: ``gmod = g + gmod.svdcorrection``.\n    \"\"\"\n    # replace g by a copy of g\n    if hasattr(g,'keys'):\n        is_dict = True\n        g = BufferDict(g)\n    else:\n        is_dict = False\n        class svdarray(numpy.ndarray):\n            def __new__(cls, inputarray):\n                obj = numpy.array(g).view(cls)\n                return obj\n        g = svdarray(g)\n    idx_bcov = evalcov_blocks(g.flat)\n    g.logdet = 0.0\n    svdcorrection = numpy.zeros(len(g.flat), object)\n    svdcorrection[:] = gvar(0, 0)\n    g.eigen_range = 1.\n    g.nmod = 0\n    if wgts is not False:\n        i_wgts = [([], [])] # 1st entry for all 1x1 blocks\n    lost_modes = 0\n    g.nblocks = {}\n    for idx, block_cov in idx_bcov:\n        g.nblocks[len(idx)] = g.nblocks.get(len(idx), 0) + 1\n        if len(idx) == 1:\n            i = idx[0]\n            if block_cov[0, 0] == 0:\n                g.logdet = numpy.inf\n            else:\n                g.logdet += numpy.log(block_cov[0, 0])\n            if wgts is not False:\n                i_wgts[0][0].append(i)\n                i_wgts[0][1].append(block_cov[0, 0] ** (wgts * 0.5))\n        else:\n            s = SVD(block_cov, svdcut=svdcut, rescale=True, compute_delta=True)\n            if s.D is not None:\n                g.logdet -= 2 * sum(numpy.log(di) for di in s.D)\n            g.logdet += sum(numpy.log(vali) for vali in s.val)\n            g.nmod += s.nmod\n            if s.delta is not None:\n                if add_svdnoise:\n                    for vali, valorigi, veci in zip(s.val, s.valorig, s.vec):\n                        if vali > valorigi:\n                            # add next(raniter(s.delta)) to s.delta in svdcorrection\n                            s.delta += (veci / s.D) * (\n                                numpy.random.normal(0.0, (vali - valorigi) ** 0.5)\n                                )\n                svdcorrection[idx] = s.delta\n                g.flat[idx] += s.delta\n            elif svdcut is not None and svdcut < 0:\n                newg = numpy.zeros(len(idx), object)\n                for veci in s.vec:\n                    veci_D = veci / s.D\n                    newg += veci_D * (veci.dot(s.D * g.flat[idx]))\n                lost_modes += len(idx) - len(s.vec)\n                g.flat[idx] = newg\n            if wgts is not False:\n                i_wgts.append(\n                    (idx, [w for w in s.decomp(wgts)[::-1]])\n                    )\n            if s.eigen_range < g.eigen_range:\n                g.eigen_range = s.eigen_range\n    g.nmod += lost_modes\n    g.dof = len(g.flat) - lost_modes\n    g.svdcut = svdcut\n\n    # repackage svdcorrection\n    if is_dict:\n        g.svdcorrection = BufferDict(g, buf=svdcorrection)\n    else:\n        g.svdcorrection = svdcorrection.reshape(g.shape)\n\n    ##### for legacy code (don't use)\n    svd.dof = g.dof\n    svd.nmod = g.nmod\n    svd.eigen_range = g.eigen_range\n    svd.logdet = g.logdet\n    svd.correction = g.svdcorrection.flat[:]\n    svd.nblocks = g.nblocks\n    ##### end of legacy code\n\n    # repack into numpy arrays\n    if wgts is not False:\n        tmp = []\n        for iw, wgts in i_wgts:\n            tmp.append(\n                (numpy.array(iw, numpy.intp), numpy.array(wgts, numpy.double))\n                )\n        i_wgts = tmp\n        return (g, i_wgts)\n    else:\n        return g"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef tabulate(g, ncol=1, headers=True, offset='', ndecimal=None):\n    entries = []\n    if hasattr(g, 'keys'):\n        if headers is True:\n            headers = ('key/index', 'value')\n        g = BufferDict(g)\n        for k in g:\n            if g[k].shape == ():\n                entries.append((\n                    str(k), fmt(g[k], sep=' ', ndecimal=ndecimal)\n                    ))\n            else:\n                prefix = str(k) + ' '\n                gk = g[k]\n                for idx in numpy.ndindex(gk.shape):\n                    str_idx = str(idx)[1:-1]\n                    str_idx = ''.join(str_idx.split(' '))\n                    if str_idx[-1] == ',':\n                        str_idx = str_idx[:-1]\n                    entries.append((\n                        prefix + str_idx,\n                        fmt(gk[idx], sep=' ', ndecimal=ndecimal),\n                        ))\n                    if prefix != '':\n                        prefix = ''\n    else:\n        if headers is True:\n            headers = ('index', 'value')\n        g = numpy.asarray(g)\n        if g.shape == ():\n            return fmt(g)\n        for idx in numpy.ndindex(g.shape):\n            str_idx = str(idx)[1:-1]\n            str_idx = ''.join(str_idx.split(' '))\n            if str_idx[-1] == ',':\n                str_idx = str_idx[:-1]\n            entries.append((\n                str_idx, fmt(g[idx], sep=' ', ndecimal=ndecimal)\n                ))\n    w0 = max(len(ei[0]) for ei in entries)\n    w1 = max(len(ei[1]) for ei in entries)\n    linefmt = '  {e0:>{w0}}    {e1:>{w1}}'\n    table = ncol * [[]]\n    # nl = length of long columns; ns = lenght of short columns\n    nl = len(entries) // ncol\n    if nl * ncol < len(entries):\n        nl += 1\n    ns = len(entries) - (ncol - 1) * nl\n    ne = (ncol - 1) * [nl] + [ns]\n    iter_entries = iter(entries)\n    if headers is not False and len(headers) != 2:\n        raise ValueError('headers must be True, False or a 2-tuple')\n    for col in range(ncol):\n        if headers is not False:\n            e0, e1 = headers\n            w0 = max(len(e0), w0)\n            w1 = max(len(e1), w1)\n            table[col] = [linefmt.format(e0=e0, w0=w0, e1=e1, w1=w1)]\n            table[col].append(len(table[col][0]) * '-')\n        else:\n            table[col] = []\n        for ii in range(ne[col]):\n            e0, e1 = next(iter_entries)\n            table[col].append(linefmt.format(e0=e0, w0=w0, e1=e1, w1=w1))\n    mtable = []\n    if headers is not False:\n        ns += 2\n        nl += 2\n    for i in range(ns):\n        mtable.append('  '.join([tabcol[i] for tabcol in table]))\n    for i in range(ns, nl):\n        mtable.append('  '.join([tabcol[i] for tabcol in table[:-1]]))\n    return offset + ('\\n' + offset).join(mtable)", "response": "Tabulate contents of an array or dictionary of |GVar| or arrays of |GVar| or arrays of |GVar| or arrays of |GVar| or arrays of |GVar| or arrays of |GVar| or arrays of |GVar| or arrays of |GVar|."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef erf(x):\n    try:\n        return math.erf(x)\n    except TypeError:\n        pass\n    if isinstance(x, GVar):\n        f = math.erf(x.mean)\n        dfdx = 2. * math.exp(- x.mean ** 2) / math.sqrt(math.pi)\n        return gvar_function(x, f, dfdx)\n    else:\n        x = numpy.asarray(x)\n        ans = numpy.empty(x.shape, x.dtype)\n        for i in range(x.size):\n            try:\n                ans.flat[i] = erf(x.flat[i])\n            except TypeError:\n                xi = x.flat[i]\n                f = math.erf(xi.mean)\n                dfdx = 2. * math.exp(- xi.mean ** 2) / math.sqrt(math.pi)\n                ans.flat[i] = gvar_function(xi, f, dfdx)\n        return ans", "response": "Error function.\n    Works for floats GVar|\\s and numpy arrays."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make_fake_data(g, fac=1.0):\n    if hasattr(g, 'keys'):\n        if not isinstance(g, BufferDict):\n            g = BufferDict(g)\n        return BufferDict(g, buf=make_fake_data(g.buf, fac))\n    else:\n        g_shape = numpy.shape(g)\n        g_flat = numpy.array(g).flat\n        zero = numpy.zeros(len(g_flat), float)\n        dg = (2. ** -0.5) * gvar(zero, evalcov(g_flat))\n        dg *= fac\n        noise = gvar(zero, sdev(dg))\n        g_flat = mean(g_flat) + dg + noise + next(raniter(dg + noise))\n        return g_flat[0] if g_shape == () else g_flat.reshape(g_shape)", "response": "Makes fake data based on g."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmapping parameters p to vector in x - space.", "response": "def p2x(self, p):\n        \"\"\" Map parameters ``p`` to vector in x-space.\n\n        x-space is a vector space of dimension ``p.size``. Its axes are\n        in the directions specified by the eigenvectors of ``p``'s covariance\n        matrix, and distance along an axis is in units of the standard\n        deviation in that direction.\n        \"\"\"\n        if hasattr(p, 'keys'):\n            dp = BufferDict(p, keys=self.g.keys())._buf[:self.meanflat.size] - self.meanflat\n        else:\n            dp = numpy.asarray(p).reshape(-1) - self.meanflat\n        return self.vec_isig.dot(dp)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef count(self, data):\n        if isinstance(data, float) or isinstance(data, int):\n            hist = numpy.zeros(self.nbin + 2, float)\n            if data > self.bins[-1]:\n                hist[-1] = 1.\n            elif data < self.bins[0]:\n                hist[0] = 1.\n            elif data == self.bins[-1]:\n                if self.nbin > 1:\n                    hist[-2] = 1.\n            else:\n                hist[numpy.searchsorted(self.bins, data, side='right')] = 1.\n            return hist\n        if numpy.ndim(data) != 1:\n            data = numpy.reshape(data, -1)\n        else:\n            data = numpy.asarray(data)\n        middle = numpy.histogram(data, self.bins)[0]\n        below = numpy.sum(data < self.bins[0])\n        above = numpy.sum(data > self.bins[-1])\n        return numpy.array([below] + middle.tolist() + [above], float)", "response": "Compute the number of elements in each bin of the cluster."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nanalyzing count data from PDFHistogram. count.", "response": "def analyze(self, count):\n        \"\"\" Analyze count data from :meth:`PDFHistogram.count`.\n\n        Turns an array of counts (see :meth:`PDFHistogram.count`) into a\n        histogram of probabilities, and  estimates the mean, standard\n        deviation, and other statistical characteristics of the corresponding\n        probability distribution.\n\n        Args:\n            count (array): Array of length ``nbin+2`` containing histogram\n                data where ``count[0]`` is the count for values that are\n                below the range of the histogram, ``count[-1]`` is the count\n                for values above the range, and ``count[i]`` is the count\n                for the ``i``-th bin where ``i=1...nbin``.\n\n        Returns a named tuple containing the following information (in order):\n\n            *bins*: Array of bin edges for histogram (length ``nbin+1``)\n\n            *prob*: Array of probabilities for each bin.\n\n            *stats*: Statistical data about histogram. See :class:`PDFStatistics`.\n\n            *norm*: Convert counts into probabilities by dividing by ``norm``.\n        \"\"\"\n        if numpy.ndim(count) != 1:\n            raise ValueError('count must have dimension 1')\n        if len(count) == len(self.midpoints) + 2:\n            norm = numpy.sum(count)\n            data = numpy.asarray(count[1:-1]) / norm\n        elif len(count) != len(self.midpoints):\n            raise ValueError(\n                'wrong data length: %s != %s'\n                    % (len(count), len(self.midpoints))\n                )\n        else:\n            data = count\n            norm = 1.\n        mid = self.midpoints\n        stats = PDFStatistics(histogram=(self.bins, count))\n        return PDFHistogram.Histogram(self.bins, data, stats, norm)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the Gaussian probability density function at x for GVar |g|.", "response": "def gaussian_pdf(x, g):\n        \"\"\" Gaussian probability density function at ``x`` for |GVar| ``g``. \"\"\"\n        return (\n            numpy.exp(-(x - g.mean) ** 2 / 2. /g.var) /\n            numpy.sqrt(g.var * 2 * numpy.pi)\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a plot of the count of the current object.", "response": "def make_plot(\n        self, count, plot=None, show=False, plottype='probability',\n        bar=dict(alpha=0.15, color='b', linewidth=1.0, edgecolor='b'),\n        errorbar=dict(fmt='b.'),\n        gaussian=dict(ls='--', c='r')\n        ):\n        \"\"\" Convert histogram counts in array ``count`` into a plot.\n\n        Args:\n            count (array): Array of histogram counts (see\n                :meth:`PDFHistogram.count`).\n            plot (plotter): :mod:`matplotlib` plotting window. If ``None``\n                uses the default window. Default is ``None``.\n            show (boolean): Displayes plot if ``True``; otherwise returns\n                the plot. Default is ``False``.\n            plottype (str): The probabilities in each bin are plotted if\n                ``plottype='probability'`` (default). The average probability\n                density is plot if ``plottype='density'``. The\n                cumulative probability is plotted if ``plottype=cumulative``.\n            bar (dictionary): Additional plotting arguments for the bar graph\n                showing the histogram. This part of the plot is omitted\n                if ``bar=None``.\n            errorbar (dictionary): Additional plotting arguments for the\n                errorbar graph, showing error bars on the histogram. This\n                part of the plot is omitted if ``errorbar=None``.\n            gaussian (dictionary): Additional plotting arguments for the\n                plot of the Gaussian probability for the |GVar| (``g``)\n                specified in the initialization. This part of the plot\n                is omitted if ``gaussian=None`` or if no ``g`` was\n                specified.\n        \"\"\"\n        if numpy.ndim(count) != 1:\n            raise ValueError('count must have dimension 1')\n        if plot is None:\n            import matplotlib.pyplot as plot\n        if len(count) == len(self.midpoints) + 2:\n            norm = numpy.sum(count)\n            data = numpy.asarray(count[1:-1]) / norm\n        elif len(count) != len(self.midpoints):\n            raise ValueError(\n                'wrong data length: %s != %s'\n                    % (len(count), len(self.midpoints))\n                )\n        else:\n            data = numpy.asarray(count)\n        if plottype == 'cumulative':\n            data = numpy.cumsum(data)\n            data = numpy.array([0.] + data.tolist())\n            data_sdev = sdev(data)\n            if not numpy.all(data_sdev == 0.0):\n                data_mean = mean(data)\n                plot.errorbar(self.bins, data_mean, data_sdev, **errorbar)\n            if bar is not None:\n                plot.fill_between(self.bins, 0, data_mean, **bar)\n                # mean, +- 1 sigma lines\n                plot.plot([self.bins[0], self.bins[-1]], [0.5, 0.5], 'k:')\n                plot.plot([self.bins[0], self.bins[-1]], [0.158655254, 0.158655254], 'k:')\n                plot.plot([self.bins[0], self.bins[-1]], [0.841344746, 0.841344746], 'k:')\n        else:\n            if plottype == 'density':\n                data = data / self.widths\n            if errorbar is not None:\n                data_sdev = sdev(data)\n                if not numpy.all(data_sdev == 0.0):\n                    data_mean = mean(data)\n                    plot.errorbar(self.midpoints, data_mean, data_sdev, **errorbar)\n            if bar is not None:\n                plot.bar(self.bins[:-1], mean(data), width=self.widths, align='edge', **bar)\n        if gaussian is not None and self.g is not None:\n            # spline goes through the errorbar points for gaussian stats\n            if plottype == 'cumulative':\n                x = numpy.array(self.bins.tolist() + self.midpoints.tolist())\n                x.sort()\n                dx = (x - self.g.mean) / self.g.sdev\n                y = (erf(dx / 2**0.5) + 1) / 2.\n                yspline = cspline.CSpline(x, y)\n                plot.ylabel('cumulative probability')\n                plot.ylim(0, 1.0)\n            elif plottype in ['density', 'probability']:\n                x = self.bins\n                dx = (x - self.g.mean) / self.g.sdev\n                y = (erf(dx / 2**0.5) + 1) / 2.\n                x = self.midpoints\n                y = (y[1:] - y[:-1])\n                if plottype == 'density':\n                    y /= self.widths\n                    plot.ylabel('probability density')\n                else:\n                    plot.ylabel('probability')\n                yspline = cspline.CSpline(x, y)\n            else:\n                raise ValueError('unknown plottype: ' + str(plottype))\n            if len(x) < 100:\n                ny = int(100. / len(x) + 0.5) * len(x)\n            else:\n                ny = len(x)\n            xplot = numpy.linspace(x[0], x[-1], ny)\n            plot.plot(xplot, yspline(xplot), **gaussian)\n        if show:\n            plot.show()\n        return plot"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsolve a and b using the Tridiagonal Matrix Algorithm.", "response": "def tri_diag_solve(a, b, c, d):\n    \"\"\" Solve a[i] * x[i-1] + b[i] * x[i] + c[i] * x[i+1] = d[i] for x[i]\n\n    Adapted from: http://en.wikipedia.org/wiki/Tridiagonal_matrix_algorithm.\n    \"\"\"\n    n = len(a)\n    a, b, c, d = map(numpy.array, (a, b, c, d))\n    c[0] /= b[0]\n    d[0] /= b[0]\n    for i in range(1, n):\n        m = b[i] - a[i] * c[i - 1]\n        c[i] /= m\n        d[i] = (d[i] - a[i] * d[i - 1]) / m\n    x = d\n    for i in range(n - 2, -1, -1):\n        x[i] -= c[i] * x[i + 1]\n    return x"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _dspace(\n       irez,\n       d2201,  d2211,  d3210,   d3222,  d4410,\n       d4422,  d5220,  d5232,   d5421,  d5433,\n       dedt,   del1,   del2,    del3,   didt,\n       dmdt,   dnodt,  domdt,   argpo,  argpdot,\n       t,      tc,     gsto,    xfact,  xlamo,\n       no,\n       atime, em,    argpm,  inclm, xli,\n       mm,    xni,   nodem,  nm,\n       ):\n\n     fasx2 = 0.13130908;\n     fasx4 = 2.8843198;\n     fasx6 = 0.37448087;\n     g22   = 5.7686396;\n     g32   = 0.95240898;\n     g44   = 1.8014998;\n     g52   = 1.0508330;\n     g54   = 4.4108898;\n     rptim = 4.37526908801129966e-3; # equates to 7.29211514668855e-5 rad/sec\n     stepp =    720.0;\n     stepn =   -720.0;\n     step2 = 259200.0;\n\n     #  ----------- calculate deep space resonance effects -----------\n     dndt   = 0.0;\n     theta  = (gsto + tc * rptim) % twopi\n     em     = em + dedt * t;\n\n     inclm  = inclm + didt * t;\n     argpm  = argpm + domdt * t;\n     nodem  = nodem + dnodt * t;\n     mm     = mm + dmdt * t;\n\n     \"\"\"\n     //   sgp4fix for negative inclinations\n     //   the following if statement should be commented out\n     //  if (inclm < 0.0)\n     // {\n     //    inclm = -inclm;\n     //    argpm = argpm - pi;\n     //    nodem = nodem + pi;\n     //  }\n\n     /* - update resonances : numerical (euler-maclaurin) integration - */\n     /* ------------------------- epoch restart ----------------------  */\n     //   sgp4fix for propagator problems\n     //   the following integration works for negative time steps and periods\n     //   the specific changes are unknown because the original code was so convoluted\n\n     // sgp4fix take out atime = 0.0 and fix for faster operation\n     \"\"\"\n     ft    = 0.0;\n     if irez != 0:\n\n         #  sgp4fix streamline check\n         if atime == 0.0 or t * atime <= 0.0 or fabs(t) < fabs(atime):\n\n             atime  = 0.0;\n             xni    = no;\n             xli    = xlamo;\n\n         # sgp4fix move check outside loop\n         if t > 0.0:\n               delt = stepp;\n         else:\n               delt = stepn;\n\n         iretn = 381; # added for do loop\n         iret  =   0; # added for loop\n         while iretn == 381:\n\n             #  ------------------- dot terms calculated -------------\n             #  ----------- near - synchronous resonance terms -------\n             if irez != 2:\n\n                 xndt  = del1 * sin(xli - fasx2) + del2 * sin(2.0 * (xli - fasx4)) + \\\n                         del3 * sin(3.0 * (xli - fasx6));\n                 xldot = xni + xfact;\n                 xnddt = del1 * cos(xli - fasx2) + \\\n                         2.0 * del2 * cos(2.0 * (xli - fasx4)) + \\\n                         3.0 * del3 * cos(3.0 * (xli - fasx6));\n                 xnddt = xnddt * xldot;\n\n             else:\n\n                 # --------- near - half-day resonance terms --------\n                 xomi  = argpo + argpdot * atime;\n                 x2omi = xomi + xomi;\n                 x2li  = xli + xli;\n                 xndt  = (d2201 * sin(x2omi + xli - g22) + d2211 * sin(xli - g22) +\n                       d3210 * sin(xomi + xli - g32)  + d3222 * sin(-xomi + xli - g32)+\n                       d4410 * sin(x2omi + x2li - g44)+ d4422 * sin(x2li - g44) +\n                       d5220 * sin(xomi + xli - g52)  + d5232 * sin(-xomi + xli - g52)+\n                       d5421 * sin(xomi + x2li - g54) + d5433 * sin(-xomi + x2li - g54));\n                 xldot = xni + xfact;\n                 xnddt = (d2201 * cos(x2omi + xli - g22) + d2211 * cos(xli - g22) +\n                       d3210 * cos(xomi + xli - g32) + d3222 * cos(-xomi + xli - g32) +\n                       d5220 * cos(xomi + xli - g52) + d5232 * cos(-xomi + xli - g52) +\n                       2.0 * (d4410 * cos(x2omi + x2li - g44) +\n                       d4422 * cos(x2li - g44) + d5421 * cos(xomi + x2li - g54) +\n                       d5433 * cos(-xomi + x2li - g54)));\n                 xnddt = xnddt * xldot;\n\n             #  ----------------------- integrator -------------------\n             #  sgp4fix move end checks to end of routine\n             if fabs(t - atime) >= stepp:\n                 iret  = 0;\n                 iretn = 381;\n\n             else:\n                 ft    = t - atime;\n                 iretn = 0;\n\n             if iretn == 381:\n\n                 xli   = xli + xldot * delt + xndt * step2;\n                 xni   = xni + xndt * delt + xnddt * step2;\n                 atime = atime + delt;\n\n         nm = xni + xndt * ft + xnddt * ft * ft * 0.5;\n         xl = xli + xldot * ft + xndt * ft * ft * 0.5;\n         if irez != 1:\n             mm   = xl - 2.0 * nodem + 2.0 * theta;\n             dndt = nm - no;\n\n         else:\n             mm   = xl - nodem - argpm + theta;\n             dndt = nm - no;\n\n         nm = no + dndt;\n\n     return (\n       atime, em,    argpm,  inclm, xli,\n       mm,    xni,   nodem,  dndt,  nm,\n       )", "response": "Private function that calculates the deep space resonance effects of a given object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates sgp4 from sgp4", "response": "def sgp4(satrec, tsince, whichconst=None):\n\n     mrt = 0.0\n     if whichconst is None:\n          whichconst = satrec.whichconst\n\n     \"\"\"\n     /* ------------------ set mathematical constants --------------- */\n     // sgp4fix divisor for divide by zero check on inclination\n     // the old check used 1.0 + cos(pi-1.0e-9), but then compared it to\n     // 1.5 e-12, so the threshold was changed to 1.5e-12 for consistency\n     \"\"\"\n     temp4 =   1.5e-12;\n     twopi = 2.0 * pi;\n     x2o3  = 2.0 / 3.0;\n     #  sgp4fix identify constants and allow alternate values\n     tumin, mu, radiusearthkm, xke, j2, j3, j4, j3oj2 = whichconst\n     vkmpersec     = radiusearthkm * xke/60.0;\n\n     #  --------------------- clear sgp4 error flag -----------------\n     satrec.t     = tsince;\n     satrec.error = 0;\n     satrec.error_message = None\n\n     #  ------- update for secular gravity and atmospheric drag -----\n     xmdf    = satrec.mo + satrec.mdot * satrec.t;\n     argpdf  = satrec.argpo + satrec.argpdot * satrec.t;\n     nodedf  = satrec.nodeo + satrec.nodedot * satrec.t;\n     argpm   = argpdf;\n     mm      = xmdf;\n     t2      = satrec.t * satrec.t;\n     nodem   = nodedf + satrec.nodecf * t2;\n     tempa   = 1.0 - satrec.cc1 * satrec.t;\n     tempe   = satrec.bstar * satrec.cc4 * satrec.t;\n     templ   = satrec.t2cof * t2;\n\n     if satrec.isimp != 1:\n\n         delomg = satrec.omgcof * satrec.t;\n         #  sgp4fix use mutliply for speed instead of pow\n         delmtemp =  1.0 + satrec.eta * cos(xmdf);\n         delm   = satrec.xmcof * \\\n                  (delmtemp * delmtemp * delmtemp -\n                  satrec.delmo);\n         temp   = delomg + delm;\n         mm     = xmdf + temp;\n         argpm  = argpdf - temp;\n         t3     = t2 * satrec.t;\n         t4     = t3 * satrec.t;\n         tempa  = tempa - satrec.d2 * t2 - satrec.d3 * t3 - \\\n                          satrec.d4 * t4;\n         tempe  = tempe + satrec.bstar * satrec.cc5 * (sin(mm) -\n                          satrec.sinmao);\n         templ  = templ + satrec.t3cof * t3 + t4 * (satrec.t4cof +\n                          satrec.t * satrec.t5cof);\n\n     nm    = satrec.no;\n     em    = satrec.ecco;\n     inclm = satrec.inclo;\n     if satrec.method == 'd':\n\n         tc = satrec.t;\n         (\n             atime, em,    argpm,  inclm, xli,\n             mm,    xni,   nodem,  dndt,  nm,\n         ) = _dspace(\n               satrec.irez,\n               satrec.d2201, satrec.d2211, satrec.d3210,\n               satrec.d3222, satrec.d4410, satrec.d4422,\n               satrec.d5220, satrec.d5232, satrec.d5421,\n               satrec.d5433, satrec.dedt,  satrec.del1,\n               satrec.del2,  satrec.del3,  satrec.didt,\n               satrec.dmdt,  satrec.dnodt, satrec.domdt,\n               satrec.argpo, satrec.argpdot, satrec.t, tc,\n               satrec.gsto, satrec.xfact, satrec.xlamo,\n               satrec.no, satrec.atime,\n               em, argpm, inclm, satrec.xli, mm, satrec.xni,\n               nodem, nm\n             );\n\n     if nm <= 0.0:\n\n         satrec.error_message = ('mean motion {0:f} is less than zero'\n                                 .format(nm))\n         satrec.error = 2;\n         #  sgp4fix add return\n         return false, false;\n\n     am = pow((xke / nm),x2o3) * tempa * tempa;\n     nm = xke / pow(am, 1.5);\n     em = em - tempe;\n\n     #  fix tolerance for error recognition\n     #  sgp4fix am is fixed from the previous nm check\n     if em >= 1.0 or em < -0.001:  # || (am < 0.95)\n\n         satrec.error_message = ('mean eccentricity {0:f} not within'\n                                 ' range 0.0 <= e < 1.0'.format(em))\n         satrec.error = 1;\n         #  sgp4fix to return if there is an error in eccentricity\n         return false, false;\n\n     #  sgp4fix fix tolerance to avoid a divide by zero\n     if em < 1.0e-6:\n         em  = 1.0e-6;\n     mm     = mm + satrec.no * templ;\n     xlm    = mm + argpm + nodem;\n     emsq   = em * em;\n     temp   = 1.0 - emsq;\n\n     nodem  = nodem % twopi if nodem >= 0.0 else -(-nodem % twopi)\n     argpm  = argpm % twopi\n     xlm    = xlm % twopi\n     mm     = (xlm - argpm - nodem) % twopi\n\n     #  ----------------- compute extra mean quantities -------------\n     sinim = sin(inclm);\n     cosim = cos(inclm);\n\n     #  -------------------- add lunar-solar periodics --------------\n     ep     = em;\n     xincp  = inclm;\n     argpp  = argpm;\n     nodep  = nodem;\n     mp     = mm;\n     sinip  = sinim;\n     cosip  = cosim;\n     if satrec.method == 'd':\n\n         ep, xincp, nodep, argpp, mp = _dpper(\n               satrec, satrec.inclo,\n               'n', ep, xincp, nodep, argpp, mp, satrec.afspc_mode\n             );\n         if xincp < 0.0:\n\n             xincp  = -xincp;\n             nodep = nodep + pi;\n             argpp  = argpp - pi;\n\n         if ep < 0.0 or ep > 1.0:\n\n             satrec.error_message = ('perturbed eccentricity {0:f} not within'\n                                     ' range 0.0 <= e <= 1.0'.format(ep))\n             satrec.error = 3;\n             #  sgp4fix add return\n             return false, false;\n\n     #  -------------------- long period periodics ------------------\n     if satrec.method == 'd':\n\n         sinip =  sin(xincp);\n         cosip =  cos(xincp);\n         satrec.aycof = -0.5*j3oj2*sinip;\n         #  sgp4fix for divide by zero for xincp = 180 deg\n         if fabs(cosip+1.0) > 1.5e-12:\n             satrec.xlcof = -0.25 * j3oj2 * sinip * (3.0 + 5.0 * cosip) / (1.0 + cosip);\n         else:\n             satrec.xlcof = -0.25 * j3oj2 * sinip * (3.0 + 5.0 * cosip) / temp4;\n\n     axnl = ep * cos(argpp);\n     temp = 1.0 / (am * (1.0 - ep * ep));\n     aynl = ep* sin(argpp) + temp * satrec.aycof;\n     xl   = mp + argpp + nodep + temp * satrec.xlcof * axnl;\n\n     #  --------------------- solve kepler's equation ---------------\n     u    = (xl - nodep) % twopi\n     eo1  = u;\n     tem5 = 9999.9;\n     ktr = 1;\n     #    sgp4fix for kepler iteration\n     #    the following iteration needs better limits on corrections\n     while fabs(tem5) >= 1.0e-12 and ktr <= 10:\n\n         sineo1 = sin(eo1);\n         coseo1 = cos(eo1);\n         tem5   = 1.0 - coseo1 * axnl - sineo1 * aynl;\n         tem5   = (u - aynl * coseo1 + axnl * sineo1 - eo1) / tem5;\n         if fabs(tem5) >= 0.95:\n             tem5 = 0.95 if tem5 > 0.0 else -0.95;\n         eo1    = eo1 + tem5;\n         ktr = ktr + 1;\n\n     #  ------------- short period preliminary quantities -----------\n     ecose = axnl*coseo1 + aynl*sineo1;\n     esine = axnl*sineo1 - aynl*coseo1;\n     el2   = axnl*axnl + aynl*aynl;\n     pl    = am*(1.0-el2);\n     if pl < 0.0:\n\n         satrec.error_message = ('semilatus rectum {0:f} is less than zero'\n                                 .format(pl))\n         satrec.error = 4;\n         #  sgp4fix add return\n         return false, false;\n\n     else:\n\n         rl     = am * (1.0 - ecose);\n         rdotl  = sqrt(am) * esine/rl;\n         rvdotl = sqrt(pl) / rl;\n         betal  = sqrt(1.0 - el2);\n         temp   = esine / (1.0 + betal);\n         sinu   = am / rl * (sineo1 - aynl - axnl * temp);\n         cosu   = am / rl * (coseo1 - axnl + aynl * temp);\n         su     = atan2(sinu, cosu);\n         sin2u  = (cosu + cosu) * sinu;\n         cos2u  = 1.0 - 2.0 * sinu * sinu;\n         temp   = 1.0 / pl;\n         temp1  = 0.5 * j2 * temp;\n         temp2  = temp1 * temp;\n\n         #  -------------- update for short period periodics ------------\n         if satrec.method == 'd':\n\n             cosisq                 = cosip * cosip;\n             satrec.con41  = 3.0*cosisq - 1.0;\n             satrec.x1mth2 = 1.0 - cosisq;\n             satrec.x7thm1 = 7.0*cosisq - 1.0;\n\n         mrt   = rl * (1.0 - 1.5 * temp2 * betal * satrec.con41) + \\\n                 0.5 * temp1 * satrec.x1mth2 * cos2u;\n         su    = su - 0.25 * temp2 * satrec.x7thm1 * sin2u;\n         xnode = nodep + 1.5 * temp2 * cosip * sin2u;\n         xinc  = xincp + 1.5 * temp2 * cosip * sinip * cos2u;\n         mvt   = rdotl - nm * temp1 * satrec.x1mth2 * sin2u / xke;\n         rvdot = rvdotl + nm * temp1 * (satrec.x1mth2 * cos2u +\n                 1.5 * satrec.con41) / xke;\n\n         #  --------------------- orientation vectors -------------------\n         sinsu =  sin(su);\n         cossu =  cos(su);\n         snod  =  sin(xnode);\n         cnod  =  cos(xnode);\n         sini  =  sin(xinc);\n         cosi  =  cos(xinc);\n         xmx   = -snod * cosi;\n         xmy   =  cnod * cosi;\n         ux    =  xmx * sinsu + cnod * cossu;\n         uy    =  xmy * sinsu + snod * cossu;\n         uz    =  sini * sinsu;\n         vx    =  xmx * cossu - cnod * sinsu;\n         vy    =  xmy * cossu - snod * sinsu;\n         vz    =  sini * cossu;\n\n         #  --------- position and velocity (in km and km/sec) ----------\n         _mr = mrt * radiusearthkm\n         r = (_mr * ux, _mr * uy, _mr * uz)\n         v = ((mvt * ux + rvdot * vx) * vkmpersec,\n              (mvt * uy + rvdot * vy) * vkmpersec,\n              (mvt * uz + rvdot * vz) * vkmpersec)\n\n     #  sgp4fix for decaying satellites\n     if mrt < 1.0:\n\n         satrec.error_message = ('mrt {0:f} is less than 1.0 indicating'\n                                 ' the satellite has decayed'.format(mrt))\n         satrec.error = 6;\n         return false, false;\n\n     return r, v;"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a Satellite imported from two lines of TLE data.", "response": "def twoline2rv(longstr1, longstr2, whichconst, afspc_mode=False):\n    \"\"\"Return a Satellite imported from two lines of TLE data.\n\n    Provide the two TLE lines as strings `longstr1` and `longstr2`,\n    and select which standard set of gravitational constants you want\n    by providing `gravity_constants`:\n\n    `sgp4.earth_gravity.wgs72` - Standard WGS 72 model\n    `sgp4.earth_gravity.wgs84` - More recent WGS 84 model\n    `sgp4.earth_gravity.wgs72old` - Legacy support for old SGP4 behavior\n\n    Normally, computations are made using various recent improvements\n    to the algorithm.  If you want to turn some of these off and go\n    back into \"afspc\" mode, then set `afspc_mode` to `True`.\n\n    \"\"\"\n\n    deg2rad  =   pi / 180.0;         #    0.0174532925199433\n    xpdotp   =  1440.0 / (2.0 *pi);  #  229.1831180523293\n\n    tumin = whichconst.tumin\n\n    satrec = Satellite()\n    satrec.error = 0;\n    satrec.whichconst = whichconst  # Python extension: remembers its consts\n\n    line = longstr1.rstrip()\n    # try/except is not well supported by Numba\n    if (len(line) >= 64 and\n        line.startswith('1 ') and\n        line[8] == ' ' and\n        line[23] == '.' and\n        line[32] == ' ' and\n        line[34] == '.' and\n        line[43] == ' ' and\n        line[52] == ' ' and\n        line[61] == ' ' and\n        line[63] == ' '):\n\n        _saved_satnum = satrec.satnum = int(line[2:7])\n        # classification = line[7] or 'U'\n        # intldesg = line[9:17]\n        two_digit_year = int(line[18:20])\n        satrec.epochdays = float(line[20:32])\n        satrec.ndot = float(line[33:43])\n        satrec.nddot = float(line[44] + '.' + line[45:50])\n        nexp = int(line[50:52])\n        satrec.bstar = float(line[53] + '.' + line[54:59])\n        ibexp = int(line[59:61])\n        # numb = int(line[62])\n        # elnum = int(line[64:68])\n    else:\n        raise ValueError(error_message.format(1, LINE1, line))\n\n    line = longstr2.rstrip()\n    if (len(line) >= 69 and\n        line.startswith('2 ') and\n        line[7] == ' ' and\n        line[11] == '.' and\n        line[16] == ' ' and\n        line[20] == '.' and\n        line[25] == ' ' and\n        line[33] == ' ' and\n        line[37] == '.' and\n        line[42] == ' ' and\n        line[46] == '.' and\n        line[51] == ' '):\n\n        satrec.satnum = int(line[2:7])\n        if _saved_satnum != satrec.satnum:\n            raise ValueError('Object numbers in lines 1 and 2 do not match')\n\n        satrec.inclo = float(line[8:16])\n        satrec.nodeo = float(line[17:25])\n        satrec.ecco = float('0.' + line[26:33].replace(' ', '0'))\n        satrec.argpo = float(line[34:42])\n        satrec.mo = float(line[43:51])\n        satrec.no = float(line[52:63])\n        #revnum = line[63:68]\n    #except (AssertionError, IndexError, ValueError):\n    else:\n        raise ValueError(error_message.format(2, LINE2, line))\n\n    #  ---- find no, ndot, nddot ----\n    satrec.no   = satrec.no / xpdotp; #   rad/min\n    satrec.nddot= satrec.nddot * pow(10.0, nexp);\n    satrec.bstar= satrec.bstar * pow(10.0, ibexp);\n\n    #  ---- convert to sgp4 units ----\n    satrec.a    = pow( satrec.no*tumin , (-2.0/3.0) );\n    satrec.ndot = satrec.ndot  / (xpdotp*1440.0);  #   ? * minperday\n    satrec.nddot= satrec.nddot / (xpdotp*1440.0*1440);\n\n    #  ---- find standard orbital elements ----\n    satrec.inclo = satrec.inclo  * deg2rad;\n    satrec.nodeo = satrec.nodeo  * deg2rad;\n    satrec.argpo = satrec.argpo  * deg2rad;\n    satrec.mo    = satrec.mo     * deg2rad;\n\n    satrec.alta = satrec.a*(1.0 + satrec.ecco) - 1.0;\n    satrec.altp = satrec.a*(1.0 - satrec.ecco) - 1.0;\n\n    \"\"\"\n    // ----------------------------------------------------------------\n    // find sgp4epoch time of element set\n    // remember that sgp4 uses units of days from 0 jan 1950 (sgp4epoch)\n    // and minutes from the epoch (time)\n    // ----------------------------------------------------------------\n\n    // ---------------- temp fix for years from 1957-2056 -------------------\n    // --------- correct fix will occur when year is 4-digit in tle ---------\n    \"\"\"\n    if two_digit_year < 57:\n        year = two_digit_year + 2000;\n    else:\n        year = two_digit_year + 1900;\n\n    mon,day,hr,minute,sec = days2mdhms(year, satrec.epochdays);\n    sec_whole, sec_fraction = divmod(sec, 1.0)\n\n    satrec.epochyr = year\n    satrec.jdsatepoch = jday(year,mon,day,hr,minute,sec);\n    satrec.epoch = datetime(year, mon, day, hr, minute, int(sec_whole),\n                            int(sec_fraction * 1000000.0 // 1.0))\n\n    #  ---------------- initialize the orbit at sgp4epoch -------------------\n    sgp4init(whichconst, afspc_mode, satrec.satnum, satrec.jdsatepoch-2433281.5, satrec.bstar,\n             satrec.ecco, satrec.argpo, satrec.inclo, satrec.mo, satrec.no,\n             satrec.nodeo, satrec)\n\n    return satrec"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef verify_checksum(*lines):\n    for line in lines:\n        checksum = line[68:69]\n        if not checksum.isdigit():\n            continue\n        checksum = int(checksum)\n        computed = compute_checksum(line)\n        if checksum != computed:\n            complaint = ('TLE line gives its checksum as {}'\n                         ' but in fact tallies to {}:\\n{}')\n            raise ValueError(complaint.format(checksum, computed, line))", "response": "Verify the checksum of one or more TLE lines."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef compute_checksum(line):\n    return sum((int(c) if c.isdigit() else c == '-') for c in line[0:68]) % 10", "response": "Compute the TLE checksum for the given line."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef propagate(self, year, month=1, day=1, hour=0, minute=0, second=0.0):\n\n        j = jday(year, month, day, hour, minute, second)\n        m = (j - self.jdsatepoch) * minutes_per_day\n        r, v = sgp4(self, m)\n        return r, v", "response": "Return a position and velocity vector for a given date and time."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsigning a Signature node by calling the sign method and then calculate the signature.", "response": "def sign(self, node):\n        \"\"\"\n        Signs a Signature node\n        :param node: Signature node \n        :type node: lxml.etree.Element\n        :return: None\n        \"\"\"\n        signed_info = node.find('ds:SignedInfo', namespaces=constants.NS_MAP)\n        signature_method = signed_info.find('ds:SignatureMethod',\n                                            namespaces=constants.NS_MAP).get(\n            'Algorithm')\n        key_info = node.find('ds:KeyInfo', namespaces=constants.NS_MAP)\n        if key_info is not None:\n            self.fill_key_info(key_info, signature_method)\n        self.fill_signed_info(signed_info)\n        self.calculate_signature(node)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fill_key_info(self, key_info, signature_method):\n        x509_data = key_info.find('ds:X509Data', namespaces=constants.NS_MAP)\n        if x509_data is not None:\n            self.fill_x509_data(x509_data)\n        key_name = key_info.find('ds:KeyName', namespaces=constants.NS_MAP)\n        if key_name is not None and self.key_name is not None:\n            key_name.text = self.key_name\n        key_value = key_info.find('ds:KeyValue', namespaces=constants.NS_MAP)\n        if key_value is not None:\n            key_value.text = '\\n'\n            signature = constants.TransformUsageSignatureMethod[\n                signature_method\n            ]\n            key = self.public_key\n            if self.public_key is None:\n                key = self.private_key.public_key()\n            if not isinstance(\n                    key, signature['method'].public_key_class\n            ):\n                raise Exception('Key not compatible with signature method')\n            signature['method'].key_value(key_value, key)", "response": "Fills the KeyInfo node with the information from the KeyInfo node."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fill_x509_data(self, x509_data):\n        x509_issuer_serial = x509_data.find(\n            'ds:X509IssuerSerial', namespaces=constants.NS_MAP\n        )\n        if x509_issuer_serial is not None:\n            self.fill_x509_issuer_name(x509_issuer_serial)\n\n        x509_crl = x509_data.find('ds:X509CRL', namespaces=constants.NS_MAP)\n        if x509_crl is not None and self.crl is not None:\n            x509_data.text = base64.b64encode(\n                self.crl.public_bytes(serialization.Encoding.DER)\n            )\n        x509_subject = x509_data.find(\n            'ds:X509SubjectName', namespaces=constants.NS_MAP\n        )\n        if x509_subject is not None:\n            x509_subject.text = get_rdns_name(self.x509.subject.rdns)\n        x509_ski = x509_data.find('ds:X509SKI', namespaces=constants.NS_MAP)\n        if x509_ski is not None:\n            x509_ski.text = base64.b64encode(\n                self.x509.extensions.get_extension_for_oid(\n                    ExtensionOID.SUBJECT_KEY_IDENTIFIER\n                ).value.digest)\n        x509_certificate = x509_data.find(\n            'ds:X509Certificate', namespaces=constants.NS_MAP\n        )\n        if x509_certificate is not None:\n            s = base64.b64encode(\n                self.x509.public_bytes(encoding=serialization.Encoding.DER)\n            )\n            x509_certificate.text = b64_print(s)", "response": "Fills the X509 data node with the data from the XML file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fill_x509_issuer_name(self, x509_issuer_serial):\n        x509_issuer_name = x509_issuer_serial.find(\n            'ds:X509IssuerName', namespaces=constants.NS_MAP\n        )\n        if x509_issuer_name is not None:\n            x509_issuer_name.text = get_rdns_name(self.x509.issuer.rdns)\n        x509_issuer_number = x509_issuer_serial.find(\n            'ds:X509SerialNumber', namespaces=constants.NS_MAP\n        )\n        if x509_issuer_number is not None:\n            x509_issuer_number.text = str(self.x509.serial_number)", "response": "Fills the X509IssuerName node with the name of the issuer."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fill_signed_info(self, signed_info):\n        for reference in signed_info.findall(\n                'ds:Reference', namespaces=constants.NS_MAP\n        ):\n            self.calculate_reference(reference, True)", "response": "Fills the node s attributes with the information from the SignedInfo element."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef verify(self, node):\n        # Added XSD Validation\n        with open(path.join(\n                path.dirname(__file__), \"data/xmldsig-core-schema.xsd\"\n        ), \"rb\") as file:\n            schema = etree.XMLSchema(etree.fromstring(file.read()))\n        schema.assertValid(node)\n        # Validates reference value\n        signed_info = node.find('ds:SignedInfo', namespaces=constants.NS_MAP)\n        for reference in signed_info.findall(\n                'ds:Reference', namespaces=constants.NS_MAP\n        ):\n            if not self.calculate_reference(reference, False):\n                raise Exception(\n                    'Reference with URI:\"' +\n                    reference.get(\"URI\", '') +\n                    '\" failed'\n                )\n        # Validates signature value\n        self.calculate_signature(node, False)", "response": "Verifies a signature node."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntransform a node following the transform especification :param transform: Transform node :type transform: lxml.etree.Element :param node: Element to transform :type node: str :return: Transformed node in a String", "response": "def transform(self, transform, node):\n        \"\"\"\n        Transforms a node following the transform especification\n        :param transform: Transform node\n        :type transform: lxml.etree.Element\n        :param node: Element to transform\n        :type node: str\n        :return: Transformed node in a String\n        \"\"\"\n        method = transform.get('Algorithm')\n        if method not in constants.TransformUsageDSigTransform:\n            raise Exception('Method not allowed')\n        # C14N methods are allowed\n        if method in constants.TransformUsageC14NMethod:\n            return self.canonicalization(method, etree.fromstring(node))\n        # Enveloped method removes the Signature Node from the element\n        if method == constants.TransformEnveloped:\n            tree = transform.getroottree()\n            root = etree.fromstring(node)\n            signature = root.find(\n                tree.getelementpath(\n                    transform.getparent().getparent().getparent().getparent()\n                )\n            )\n            root.remove(signature)\n            return self.canonicalization(\n                    constants.TransformInclC14N, root)\n        if method == constants.TransformBase64:\n            try:\n                root = etree.fromstring(node)\n                return base64.b64decode(root.text)\n            except Exception:\n                return base64.b64decode(node)\n\n        raise Exception('Method not found')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the digest of an object from a method name", "response": "def digest(self, method, node):\n        \"\"\"\n        Returns the digest of an object from a method name\n        :param method: hash method\n        :type method: str\n        :param node: Object to hash\n        :type node: str\n        :return: hash result\n        \"\"\"\n        if method not in constants.TransformUsageDigestMethod:\n            raise Exception('Method not allowed')\n        lib = hashlib.new(constants.TransformUsageDigestMethod[method])\n        lib.update(node)\n        return base64.b64encode(lib.digest())"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_uri(self, uri, reference):\n        if uri == \"\":\n            return self.canonicalization(\n                constants.TransformInclC14N, reference.getroottree()\n            )\n        if uri.startswith(\"#\"):\n            query = \"//*[@*[local-name() = '{}' ]=$uri]\"\n            node = reference.getroottree()\n            results = self.check_uri_attr(node, query, uri, constants.ID_ATTR)\n            if len(results) == 0:\n                results = self.check_uri_attr(node, query, uri, 'ID')\n            if len(results) == 0:\n                results = self.check_uri_attr(node, query, uri, 'Id')\n            if len(results) == 0:\n                results = self.check_uri_attr(node, query, uri, 'id')\n            if len(results) > 1:\n                raise Exception(\n                    \"Ambiguous reference URI {} resolved to {} nodes\".format(\n                        uri, len(results)))\n            elif len(results) == 1:\n                return self.canonicalization(\n                    constants.TransformInclC14N, results[0]\n                )\n        raise Exception('URI \"' + uri + '\" cannot be readed')", "response": "This method returns the node of the specified URI in a String\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef calculate_reference(self, reference, sign=True):\n        node = self.get_uri(reference.get('URI', ''), reference)\n        transforms = reference.find(\n            'ds:Transforms', namespaces=constants.NS_MAP\n        )\n        if transforms is not None:\n            for transform in transforms.findall(\n                    'ds:Transform', namespaces=constants.NS_MAP\n            ):\n                node = self.transform(transform, node)\n        digest_value = self.digest(\n            reference.find(\n                'ds:DigestMethod', namespaces=constants.NS_MAP\n            ).get('Algorithm'),\n            node\n        )\n        if not sign:\n            return digest_value.decode() == reference.find(\n                'ds:DigestValue', namespaces=constants.NS_MAP\n            ).text\n\n        reference.find(\n            'ds:DigestValue', namespaces=constants.NS_MAP\n        ).text = digest_value", "response": "Calculates or verifies the digest of the reference node."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates or verifies the signature of the node.", "response": "def calculate_signature(self, node, sign=True):\n        \"\"\"\n        Calculate or verifies the signature\n        :param node: Signature node\n        :type node: lxml.etree.Element\n        :param sign: It checks if it must calculate or verify\n        :type sign: bool\n        :return: None\n        \"\"\"\n        signed_info_xml = node.find('ds:SignedInfo',\n                                    namespaces=constants.NS_MAP)\n        canonicalization_method = signed_info_xml.find(\n            'ds:CanonicalizationMethod', namespaces=constants.NS_MAP\n        ).get('Algorithm')\n        signature_method = signed_info_xml.find(\n            'ds:SignatureMethod', namespaces=constants.NS_MAP\n        ).get('Algorithm')\n        if signature_method not in constants.TransformUsageSignatureMethod:\n            raise Exception('Method ' + signature_method + ' not accepted')\n        signature = constants.TransformUsageSignatureMethod[signature_method]\n        signed_info = self.canonicalization(\n            canonicalization_method, signed_info_xml\n        )\n        if not sign:\n            signature_value = node.find('ds:SignatureValue',\n                                        namespaces=constants.NS_MAP).text\n            public_key = signature['method'].get_public_key(node, self)\n            signature['method'].verify(\n                signature_value,\n                signed_info,\n                public_key,\n                signature['digest']\n            )\n        else:\n            node.find(\n                'ds:SignatureValue', namespaces=constants.NS_MAP\n            ).text = b64_print(base64.b64encode(\n                signature['method'].sign(\n                    signed_info,\n                    self.private_key,\n                    signature['digest']\n                )\n            ))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the public key from the key_info element. If the public key is not defined in X509Certificate node return the public key of the key.", "response": "def get_public_key(key_info, ctx):\n        \"\"\"\n        Get the public key if its defined in X509Certificate node. Otherwise,\n        take self.public_key element\n        :param sign: Signature node\n        :type sign: lxml.etree.Element\n        :return: Public key to use\n        \"\"\"\n        x509_certificate = key_info.find(\n            'ds:KeyInfo/ds:X509Data/ds:X509Certificate',\n            namespaces={'ds': ns.DSigNs}\n        )\n        if x509_certificate is not None:\n            return load_der_x509_certificate(\n                base64.b64decode(x509_certificate.text),\n                default_backend()\n            ).public_key()\n        if ctx.public_key is not None:\n            return ctx.public_key\n        if isinstance(ctx.private_key, (str, bytes)):\n            return ctx.private_key\n        return ctx.private_key.public_key()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef b64_print(s):\n    if USING_PYTHON2:\n        string = str(s)\n    else:\n        string = str(s, 'utf8')\n    return '\\n'.join(\n        string[pos:pos + b64_intro] for pos in range(0, len(string), b64_intro)\n    )", "response": "Print a string with spaces at every b64_intro characters"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a node with the given name and parent.", "response": "def create_node(name, parent=None, ns='', tail=False, text=False):\n    \"\"\"\n    Creates a new node\n    :param name: Node name\n    :param parent: Node parent\n    :param ns: Namespace to use\n    :param tail: Tail to add\n    :param text: Text of the node\n    :return: New node\n    \"\"\"\n    node = etree.Element(etree.QName(ns, name))\n    if parent is not None:\n        parent.append(node)\n    if tail:\n        node.tail = tail\n    if text:\n        node.text = text\n    return node"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the rdns String name", "response": "def get_rdns_name(rdns):\n    \"\"\"\n    Gets the rdns String name\n    :param rdns: RDNS object\n    :type rdns: cryptography.x509.RelativeDistinguishedName\n    :return: RDNS name\n    \"\"\"\n    name = ''\n    for rdn in rdns:\n        for attr in rdn._attributes:\n            if len(name) > 0:\n                name = name + ','\n            if attr.oid in OID_NAMES:\n                name = name + OID_NAMES[attr.oid]\n            else:\n                name = name + attr.oid._name\n            name = name + '=' + attr.value\n    return name"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_public_key(key_info, ctx):\n        key = key_info.find(\n            'ds:KeyInfo/ds:KeyValue/ds:RSAKeyValue', namespaces=NS_MAP\n        )\n        if key is not None:\n            n = os2ip(b64decode(key.find(\n                'ds:Modulus', namespaces=NS_MAP).text))\n            e = os2ip(b64decode(key.find(\n                'ds:Exponent', namespaces=NS_MAP).text))\n            return rsa.RSAPublicNumbers(e, n).public_key(default_backend())\n        return super(RSAAlgorithm, RSAAlgorithm).get_public_key(key_info, ctx)", "response": "Get the public key if it is defined in X509Certificate node otherwise take self. public_key element."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the name of the sensor.", "response": "def name(self):\n        \"\"\"Return the name of the sensor.\"\"\"\n        with self._bt_interface.connect(self._mac) as connection:\n            name = connection.read_handle(_HANDLE_READ_NAME)  # pylint: disable=no-member\n\n        if not name:\n            raise BluetoothBackendException(\"Could not read NAME using handle %s\"\n                                            \" from Mi Temp sensor %s\" % (hex(_HANDLE_READ_NAME), self._mac))\n        return ''.join(chr(n) for n in name)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfilling the cache with new data from the sensor.", "response": "def fill_cache(self):\n        \"\"\"Fill the cache with new data from the sensor.\"\"\"\n        _LOGGER.debug('Filling cache with new sensor data.')\n        try:\n            self.firmware_version()\n        except BluetoothBackendException:\n            # If a sensor doesn't work, wait 5 minutes before retrying\n            self._last_read = datetime.now() - self._cache_timeout + \\\n                timedelta(seconds=300)\n            raise\n\n        with self._bt_interface.connect(self._mac) as connection:\n            try:\n                connection.wait_for_notification(_HANDLE_READ_WRITE_SENSOR_DATA, self, 10)  # pylint: disable=no-member\n                # If a sensor doesn't work, wait 5 minutes before retrying\n            except BluetoothBackendException:\n                self._last_read = datetime.now() - self._cache_timeout + \\\n                    timedelta(seconds=300)\n                return"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the firmware version.", "response": "def firmware_version(self):\n        \"\"\"Return the firmware version.\"\"\"\n        if (self._firmware_version is None) or \\\n                (datetime.now() - timedelta(hours=24) > self._fw_last_read):\n            self._fw_last_read = datetime.now()\n            with self._bt_interface.connect(self._mac) as connection:\n                res_firmware = connection.read_handle(_HANDLE_READ_FIRMWARE_VERSION)  # pylint: disable=no-member\n                _LOGGER.debug('Received result for handle %s: %s',\n                              _HANDLE_READ_FIRMWARE_VERSION, res_firmware)\n                res_battery = connection.read_handle(_HANDLE_READ_BATTERY_LEVEL)  # pylint: disable=no-member\n                _LOGGER.debug('Received result for handle %s: %d',\n                              _HANDLE_READ_BATTERY_LEVEL, res_battery)\n\n            if res_firmware is None:\n                self._firmware_version = None\n            else:\n                self._firmware_version = res_firmware.decode(\"utf-8\")\n\n            if res_battery is None:\n                self.battery = 0\n            else:\n                self.battery = int(ord(res_battery))\n        return self._firmware_version"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _check_data(self):\n        if not self.cache_available():\n            return\n\n        parsed = self._parse_data()\n        _LOGGER.debug('Received new data from sensor: Temp=%.1f, Humidity=%.1f',\n                      parsed[MI_TEMPERATURE], parsed[MI_HUMIDITY])\n\n        if parsed[MI_HUMIDITY] > 100:  # humidity over 100 procent\n            self.clear_cache()\n            return\n\n        if parsed[MI_TEMPERATURE] == 0:  # humidity over 100 procent\n            self.clear_cache()\n            return", "response": "Check if the data in the cache is valid."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _parse_data(self):\n        data = self._cache\n\n        res = dict()\n        res[MI_HUMIDITY] = float(data[9:13])\n        res[MI_TEMPERATURE] = float(data[2:6])\n        return res", "response": "Parses the byte array returned by the sensor and returns a dictionary of the values"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef handleNotification(self, handle, raw_data):  # pylint: disable=unused-argument,invalid-name\n        if raw_data is None:\n            return\n        data = raw_data.decode(\"utf-8\").strip(' \\n\\t')\n        self._cache = data\n        self._check_data()\n        if self.cache_available():\n            self._last_read = datetime.now()\n        else:\n            # If a sensor doesn't work, wait 5 minutes before retrying\n            self._last_read = datetime.now() - self._cache_timeout + \\\n                timedelta(seconds=300)", "response": "Called by the bluepy backend when a notification is received from the device."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck for valid MAC adresses.", "response": "def valid_mitemp_mac(mac, pat=re.compile(r\"4C:65:A8:[0-9A-F]{2}:[0-9A-F]{2}:[0-9A-F]{2}\")):\n    \"\"\"Check for valid mac adresses.\"\"\"\n    if not pat.match(mac.upper()):\n        raise argparse.ArgumentTypeError('The MAC address \"{}\" seems to be in the wrong format'.format(mac))\n    return mac"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef poll(args):\n    backend = _get_backend(args)\n    poller = MiTempBtPoller(args.mac, backend)\n    print(\"Getting data from Mi Temperature and Humidity Sensor\")\n    print(\"FW: {}\".format(poller.firmware_version()))\n    print(\"Name: {}\".format(poller.name()))\n    print(\"Battery: {}\".format(poller.parameter_value(MI_BATTERY)))\n    print(\"Temperature: {}\".format(poller.parameter_value(MI_TEMPERATURE)))\n    print(\"Humidity: {}\".format(poller.parameter_value(MI_HUMIDITY)))", "response": "Poll data from the sensor."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nretrieve the collection ids according to the given 3 letters acronym", "response": "def collection(self, code):\n        \"\"\"\n        Retrieve the collection ids according to the given 3 letters acronym\n        \"\"\"\n        url = urljoin(self.ARTICLEMETA_URL, self.COLLECTION_ENDPOINT)\n\n        params = {'code': code}\n\n        result = self._do_request(url, params=params)\n\n        if not result:\n            return None\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_issue(self, data):\n\n        issue = self.dispatcher(\n            'add_issue',\n            data,\n            self._admintoken\n        )\n\n        return json.loads(issue)", "response": "This method adds a new issue to the ArticleMeta."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_document(self, data):\n\n        document = self.dispatcher(\n            'add_article',\n            data,\n            self._admintoken\n        )\n\n        return json.loads(document)", "response": "This method adds a new article to the article meta."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve the collection ids according to the given 3 letters acronym code", "response": "def collection(self, code):\n        \"\"\"\n        Retrieve the collection ids according to the given 3 letters acronym\n        \"\"\"\n        result = None\n        result = self.dispatcher(\n            'get_collection',\n            code=code\n        )\n\n        if not result:\n            logger.info('Collection not found for: %s', code)\n            return None\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrotate PDF by increments of 90 degrees.", "response": "def rotate(file_name, rotate, suffix='rotated', tempdir=None):\n    \"\"\"Rotate PDF by increments of 90 degrees.\"\"\"\n    # Set output file name\n    if tempdir:\n        outfn = NamedTemporaryFile(suffix='.pdf', dir=tempdir, delete=False).name\n    elif suffix:\n        outfn = os.path.join(os.path.dirname(file_name), add_suffix(file_name, suffix))\n    else:\n        outfn = NamedTemporaryFile(suffix='.pdf').name\n\n    with open(file_name, 'rb') as pdf_in:\n        pdf_writer = PdfFileWriter()\n        pdf_reader = PdfFileReader(pdf_in)\n        for pagenum in range(pdf_reader.numPages):\n            page = pdf_reader.getPage(pagenum)\n            page.rotateClockwise(rotate)\n            pdf_writer.addPage(page)\n\n        with open(outfn, 'wb') as pdf_out:\n            pdf_writer.write(pdf_out)\n    return outfn"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rotate(file_name, rotate, suffix='rotated', tempdir=None):\n    # Set output file name\n    if tempdir:\n        outfn = NamedTemporaryFile(suffix='.pdf', dir=tempdir, delete=False).name\n    elif suffix:\n        outfn = os.path.join(os.path.dirname(file_name), add_suffix(file_name, suffix))\n    else:\n        outfn = NamedTemporaryFile(suffix='.pdf').name\n\n    trailer = PdfReader(file_name)\n    pages = trailer.pages\n\n    ranges = [[1, len(pages)]]\n\n    for onerange in ranges:\n        onerange = (onerange + onerange[-1:])[:2]\n        for pagenum in range(onerange[0] - 1, onerange[1]):\n            pages[pagenum].Rotate = (int(pages[pagenum].inheritable.Rotate or 0) + rotate) % 360\n\n    outdata = PdfWriter(outfn)\n    outdata.trailer = trailer\n    outdata.write()\n    return outfn", "response": "Rotate PDF by increments of 90 degrees."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef watermark_process():\n    # Redirect to watermark page that contains form\n    if not request.method == 'POST':\n        abort(403)\n\n    # Check if the post request has the file part\n    if 'pdf' not in request.files:\n        abort(403)\n\n    # Retrieve PDF file and parameters\n    file = request.files['pdf']\n\n    # If user does not select file, browser also submit an empty part without filename\n    if file.filename == '':\n        abort(403)\n\n    # Check if the file is an allowed file type\n    if not allowed_file(file.filename):\n        abort(403)\n\n    params = {\n        'address': request.form['address'],\n        'town': request.form['town'],\n        'state': request.form['state'],\n    }\n\n    # Save file to uploads folder\n    filename = secure_filename(file.filename)\n    file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)\n\n    # Make uploads directory if it does not exist\n    if not os.path.exists(app.config['UPLOAD_FOLDER']):\n        os.mkdir(app.config['UPLOAD_FOLDER'])\n\n    file.save(file_path)\n\n    # Create new watermarked file and return file path\n    watermarked = apply_watermark(file_path, params)\n    return send_from_directory(app.config['UPLOAD_FOLDER'], os.path.basename(watermarked))", "response": "Apply a watermark to a PDF file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef slicer(document, first_page=None, last_page=None, suffix='sliced', tempdir=None):\n    # Set output file name\n    if tempdir:\n        with NamedTemporaryFile(suffix='.pdf', dir=tempdir, delete=False) as temp:\n            output = temp.name\n    elif suffix:\n        output = os.path.join(os.path.dirname(document), add_suffix(document, suffix))\n    else:\n        with NamedTemporaryFile(suffix='.pdf') as temp:\n            output = temp.name\n\n    # Reindex page selections for simple user input\n    first_page = first_page - 1 if not None else None\n\n    # Validate page range by comparing selection to number of pages in PDF document\n    pages = Info(document).pages\n    invalid = 'Number of pages: ' + str(pages) + ' ----> Page Range Input: ' + str(first_page) + '-' + str(last_page)\n    assert first_page <= last_page <= pages, invalid\n\n    pdf = PdfFileReader(document)\n    writer = PdfFileWriter()\n\n    pages = list(range(pdf.getNumPages()))[first_page:last_page]\n    for page in pages:\n        writer.addPage(pdf.getPage(page))\n\n    with open(output, 'wb') as out:\n        writer.write(out)\n    return output", "response": "Slice a PDF document to remove pages."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads PDF and decrypt if encrypted.", "response": "def _reader(path, password, prompt):\n        \"\"\"Read PDF and decrypt if encrypted.\"\"\"\n        pdf = PdfFileReader(path) if not isinstance(path, PdfFileReader) else path\n        # Check that PDF is encrypted\n        if pdf.isEncrypted:\n            # Check that password is none\n            if not password:\n                pdf.decrypt('')\n                # Try and decrypt PDF using no password, prompt for password\n                if pdf.isEncrypted and prompt:\n                    print('No password has been given for encrypted PDF ', path)\n                    password = input('Enter Password: ')\n                else:\n                    return False\n            pdf.decrypt(password)\n        return pdf"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nretrieve contents of each page of PDF", "response": "def resources(self):\n        \"\"\"Retrieve contents of each page of PDF\"\"\"\n        return [self.pdf.getPage(i) for i in range(self.pdf.getNumPages())]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef security(self):\n        return {k: v for i in self.pdf.resolvedObjects.items() for k, v in i[1].items()}", "response": "Print security object information for a pdf document"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets width and height of a PDF", "response": "def dimensions(self):\n        \"\"\"Get width and height of a PDF\"\"\"\n        size = self.pdf.getPage(0).mediaBox\n        return {'w': float(size[2]), 'h': float(size[3])}"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a HTTP request and return the response.", "response": "def _make_request(self, url, params=None, opener=None):\n        \"\"\"\n        Configure a HTTP request, fire it off and return the response.\n        \"\"\"\n        # Create the request object\n        args = [i for i in [url, params] if i]\n        request = urllib.request.Request(*args)\n        # If the client has credentials, include them as a header\n        if self.username and self.password:\n            credentials = '%s:%s' % (self.username, self.password)\n            encoded_credentials = base64.encodestring(\n                credentials.encode(\"utf-8\")\n            ).decode(\"utf-8\").replace(\"\\n\", \"\")\n            header = 'Basic %s' % encoded_credentials\n            request.add_header('Authorization', header)\n        # If the request provides a custom opener, like the upload request,\n        # which relies on a multipart request, it is applied here.\n        if opener:\n            opener = urllib.request.build_opener(opener)\n            request_method = opener.open\n        else:\n            request_method = urllib.request.urlopen\n        # Make the request\n        try:\n            response = request_method(request)\n        except Exception:\n            e = sys.exc_info()[1]\n            if getattr(e, 'code', None) == 404:\n                raise DoesNotExistError(\"The resource you've requested does \\\nnot exist or is unavailable without the proper credentials.\")\n            elif getattr(e, 'code', None) == 401:\n                raise CredentialsFailedError(\"The resource you've requested \\\nrequires proper credentials.\")\n            else:\n                raise e\n        # Read the response and return it\n        return response.read()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef put(self, method, params):\n        # Prepare the params, first by adding a custom command to\n        # simulate a PUT request even though we are actually POSTing.\n        # This is something DocumentCloud expects.\n        params['_method'] = 'put'\n        # Some special case handling of the document_ids list, if it exists\n        if params.get(\"document_ids\", None):\n            # Pull the document_ids out of the params\n            document_ids = params.get(\"document_ids\")\n            del params['document_ids']\n            params = urllib.parse.urlencode(params, doseq=True)\n            # These need to be specially formatted in the style documentcloud\n            # expects arrays. The example they provide is:\n            # ?document_ids[]=28-boumediene&document_ids[]=\\\n            # 207-academy&document_ids[]=30-insider-trading\n            params += \"\".join([\n                '&document_ids[]=%s' % id for id in document_ids\n            ])\n        # More special case handler of key/value data tags, if they exist\n        elif params.get(\"data\", None):\n            # Pull them out of the dict\n            data = params.get(\"data\")\n            del params['data']\n            params = urllib.parse.urlencode(params, doseq=True)\n            # Format them in the style documentcloud expects\n            # ?data['foo']=bar&data['tit']=tat\n            params += \"\".join([\n                '&data[%s]=%s' % (\n                    urllib.parse.quote_plus(key.encode(\"utf-8\")),\n                    urllib.parse.quote_plus(value.encode(\"utf-8\"))\n                ) for key, value in\n                data.items()\n            ])\n        else:\n            # Otherwise, we can just use the vanilla urllib prep method\n            params = urllib.parse.urlencode(params, doseq=True)\n\n        # Make the request\n        self._make_request(\n            self.BASE_URI + method,\n            params.encode(\"utf-8\"),\n        )", "response": "Send a PUT request to the DocumentCloud and return the ID of the new object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves one page of search results from the DocumentCloud API.", "response": "def _get_search_page(\n        self,\n        query,\n        page,\n        per_page=1000,\n        mentions=3,\n        data=False,\n    ):\n        \"\"\"\n        Retrieve one page of search results from the DocumentCloud API.\n        \"\"\"\n        if mentions > 10:\n            raise ValueError(\"You cannot search for more than 10 mentions\")\n        params = {\n            'q': query,\n            'page': page,\n            'per_page': per_page,\n            'mentions': mentions,\n        }\n        if data:\n            params['data'] = 'true'\n        response = self.fetch('search.json', params)\n        return response.get(\"documents\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef search(self, query, page=None, per_page=1000, mentions=3, data=False):\n        # If the user provides a page, search it and stop there\n        if page:\n            document_list = self._get_search_page(\n                query,\n                page=page,\n                per_page=per_page,\n                mentions=mentions,\n                data=data,\n            )\n        # If the user doesn't provide a page keep looping until you have\n        # everything\n        else:\n            page = 1\n            document_list = []\n            # Loop through all the search pages and fetch everything\n            while True:\n                results = self._get_search_page(\n                    query,\n                    page=page,\n                    per_page=per_page,\n                    mentions=mentions,\n                    data=data,\n                )\n                if results:\n                    document_list += results\n                    page += 1\n                else:\n                    break\n        # Convert the JSON objects from the API into Python objects\n        obj_list = []\n        for doc in document_list:\n            doc['_connection'] = self._connection\n            obj = Document(doc)\n            obj_list.append(obj)\n        # Pass it back out\n        return obj_list", "response": "Search for all objects that make a search query."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nretrieve a particular document using it s unique identifier.", "response": "def get(self, id):\n        \"\"\"\n        Retrieve a particular document using it's unique identifier.\n\n        Example usage:\n\n            >> documentcloud.documents.get('71072-oir-final-report')\n        \"\"\"\n        data = self.fetch('documents/%s.json' % id).get(\"document\")\n        data['_connection'] = self._connection\n        return Document(data)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef upload(\n        self, pdf, title=None, source=None, description=None,\n        related_article=None, published_url=None, access='private',\n        project=None, data=None, secure=False, force_ocr=False\n    ):\n        \"\"\"\n        Upload a PDF or other image file to DocumentCloud.\n\n        You can submit either a pdf opened as a file object or a path\n        to a pdf file.\n\n        Example usage:\n\n            # From a file path\n            >> documentcloud.documents.upload(\n            >>  \"/home/ben/sample.pdf\",\n            >>  \"sample title\"\n            >>)\n\n            # From a file object\n            >> pdf = open(path, 'rb')\n            >> documentcloud.documents.upload(pdf, \"sample title\")\n\n        Returns the document that's created as a Document object.\n\n        Based on code developed by Mitchell Kotler and\n        refined by Christopher Groskopf.\n        \"\"\"\n        # Required pdf parameter\n        if hasattr(pdf, 'read'):\n            try:\n                size = os.fstat(pdf.fileno()).st_size\n            except Exception:\n                size = 0\n            params = {'file': pdf}\n            opener = MultipartPostHandler\n        elif self.is_url(pdf):\n            size = 0\n            params = {'file': pdf}\n            opener = PostHandler  # URL uploads don't need MultiPart\n        else:\n            size = os.path.getsize(pdf)\n            params = {'file': open(pdf, 'rb')}\n            opener = MultipartPostHandler\n        # Enforce file size limit of the DocumentCloud API\n        if size >= 399999999:\n            raise ValueError(\"The pdf you have submitted is over the \\\nDocumentCloud API's 400MB file size limit. Split it into smaller pieces \\\nand try again.\")\n        # Optional parameters\n        if title:\n            params['title'] = title\n        else:\n            # Set it to the file name\n            if hasattr(pdf, 'read'):\n                params['title'] = pdf.name.split(os.sep)[-1].split(\".\")[0]\n            else:\n                params['title'] = pdf.split(os.sep)[-1].split(\".\")[0]\n        if source:\n            params['source'] = source\n        if description:\n            params['description'] = description\n        if related_article:\n            params['related_article'] = related_article\n        if published_url:\n            params['published_url'] = published_url\n        if access:\n            params['access'] = access\n        if project:\n            params['project'] = project\n        if data:\n            for key, value in list(data.items()):\n                is_valid_data_keyword(key)\n                params['data[%s]' % key] = value\n        if secure:\n            params['secure'] = 'true'\n        if force_ocr:\n            params['force_ocr'] = 'true'\n        # Make the request\n        response = self._make_request(\n            self.BASE_URI + 'upload.json',\n            params,\n            opener=opener\n        )\n        # Pull the id from the response\n        response_id = json.loads(response.decode(\"utf-8\"))['id'].split(\"-\")[0]\n        # Get the document and return it\n        return self.get(response_id)", "response": "Uploads a PDF or other image file to DocumentCloud."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nuploads all the PDFs in the provided directory and returns a list of the documents created during the upload.", "response": "def upload_directory(\n        self, path, source=None, description=None,\n        related_article=None, published_url=None, access='private',\n        project=None, data=None, secure=False, force_ocr=False\n    ):\n        \"\"\"\n        Uploads all the PDFs in the provided directory.\n\n        Example usage:\n\n            >> documentcloud.documents.upload_directory(\"/home/ben/pdfs/\")\n\n        Returns a list of the documents created during the upload.\n\n        Based on code developed by Mitchell Kotler and refined\n        by Christopher Groskopf.\n        \"\"\"\n        # Loop through the path and get all the files\n        path_list = []\n        for (dirpath, dirname, filenames) in os.walk(path):\n            path_list.extend([\n                os.path.join(dirpath, i) for i in filenames\n                if i.lower().endswith(\".pdf\")\n            ])\n        # Upload all the pdfs\n        obj_list = []\n        for pdf_path in path_list:\n            obj = self.upload(\n                pdf_path, source=source, description=description,\n                related_article=related_article, published_url=published_url,\n                access=access, project=project, data=data, secure=secure,\n                force_ocr=force_ocr\n            )\n            obj_list.append(obj)\n        # Pass back the list of documents\n        return obj_list"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nretrieve all your projects. Requires authentication.", "response": "def all(self):\n        \"\"\"\n        Retrieve all your projects. Requires authentication.\n\n        Example usage:\n\n            >> documentcloud.projects.all()\n        \"\"\"\n        project_list = self.fetch('projects.json').get(\"projects\")\n        obj_list = []\n        for proj in project_list:\n            proj['_connection'] = self._connection\n            proj = Project(proj)\n            obj_list.append(proj)\n        return obj_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nretrieves a particular project using its unique identifier or its title.", "response": "def get(self, id=None, title=None):\n        \"\"\"\n        Retrieve a particular project using its unique identifier or\n        it's title.\n\n        But not both.\n\n        Example usage:\n\n            >> documentcloud.projects.get('arizona-shootings')\n        \"\"\"\n        # Make sure the kwargs are kosher\n        if id and title:\n            raise ValueError(\"You can only retrieve a Project by id or \\\n                title, not by both\")\n        elif not id and not title:\n            raise ValueError(\"You must provide an id or a title to \\\n                make a request.\")\n        # Pull the hits\n        if id:\n            hit_list = [i for i in self.all() if str(i.id) == str(id)]\n        elif title:\n            hit_list = [\n                i for i in self.all() if\n                i.title.lower().strip() == title.lower().strip()\n            ]\n        # Throw an error if there's more than one hit.\n        if len(hit_list) > 1:\n            raise DuplicateObjectError(\"There is more than one project that \\\n                matches your request.\")\n        # Try to pull the first hit\n        try:\n            return hit_list[0]\n        except IndexError:\n            # If it's not there, you know to throw this error.\n            raise DoesNotExistError(\"The resource you've requested does not \\\n                exist or is unavailable without the proper credentials.\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a new project.", "response": "def create(self, title, description=None, document_ids=None):\n        \"\"\"\n        Creates a new project.\n\n        Returns its unique identifer in documentcloud\n\n        Example usage:\n\n            >> documentcloud.projects.create(\"The Ruben Salazar Files\")\n\n        \"\"\"\n        params = {\n            'title': title,\n        }\n        if description:\n            params['description'] = description\n        params = urllib.parse.urlencode(params, doseq=True)\n        if document_ids:\n            # These need to be specially formatted in the style documentcloud\n            # expects arrays. The example they provide is:\n            # ?document_ids[]=28-boumediene&document_ids[]=207-academy\\\n            # &document_ids[]=30-insider-trading\n            params += \"\".join([\n                '&document_ids[]=%s' % id for id in document_ids\n            ])\n        response = self._make_request(\n            self.BASE_URI + \"projects.json\",\n            params.encode(\"utf-8\")\n        )\n        new_id = json.loads(response.decode(\"utf-8\"))['project']['id']\n        # If it doesn't exist, that suggests the project already exists\n        if not new_id:\n            raise DuplicateObjectError(\"The Project title you tried to create \\\n                already exists\")\n        # Fetch the actual project object from the API and return that.\n        return self.get(new_id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_or_create_by_title(self, title):\n        try:\n            obj = self.get_by_title(title)\n            created = False\n        except DoesNotExistError:\n            obj = self.create(title=title)\n            created = True\n        return obj, created", "response": "Fetch a title and create it if it doesn t exist."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_location(self):\n        image_string = self.__dict__['location']['image']\n        image_ints = list(map(int, image_string.split(\",\")))\n        return Location(*image_ints)", "response": "Return the location as a good\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef put(self):\n        params = dict(\n            title=self.title or '',\n            source=self.source or '',\n            description=self.description or '',\n            related_article=self.resources.related_article or '',\n            published_url=self.resources.published_url or '',\n            access=self.access,\n            data=self.data,\n        )\n        self._connection.put('documents/%s.json' % self.id, params)", "response": "Save the object to DocumentCloud."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _lazy_load(self):\n        obj = self._connection.documents.get(id=self.id)\n        self.__dict__['contributor'] = obj.contributor\n        self.__dict__['contributor_organization'] = \\\n            obj.contributor_organization\n        self.__dict__['data'] = obj.data\n        self.__dict__['annotations'] = obj.__dict__['annotations']\n        self.__dict__['sections'] = obj.__dict__['sections']", "response": "Lazy load the metadata for this object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_data(self, data):\n        # Make sure a dict got passed it\n        if not isinstance(data, type({})):\n            raise TypeError(\"This attribute must be a dictionary.\")\n        # Set the attribute\n        self.__dict__['data'] = DocumentDataDict(data)", "response": "Update the data attribute making sure it s a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_data(self):\n        try:\n            return DocumentDataDict(self.__dict__['data'])\n        except KeyError:\n            self._lazy_load()\n            return DocumentDataDict(self.__dict__['data'])", "response": "Fetch the data field from the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfetches the annotations field.", "response": "def get_annotations(self):\n        \"\"\"\n        Fetch the annotations field if it does not exist.\n        \"\"\"\n        try:\n            obj_list = self.__dict__['annotations']\n            return [Annotation(i) for i in obj_list]\n        except KeyError:\n            self._lazy_load()\n            obj_list = self.__dict__['annotations']\n            return [Annotation(i) for i in obj_list]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfetching the sections field if it does not exist.", "response": "def get_sections(self):\n        \"\"\"\n        Fetch the sections field if it does not exist.\n        \"\"\"\n        try:\n            obj_list = self.__dict__['sections']\n            return [Section(i) for i in obj_list]\n        except KeyError:\n            self._lazy_load()\n            obj_list = self.__dict__['sections']\n            return [Section(i) for i in obj_list]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_entities(self):\n        try:\n            return self.__dict__['entities']\n        except KeyError:\n            entities = self._connection.fetch(\n                \"documents/%s/entities.json\" % self.id\n            ).get(\"entities\")\n            obj_list = []\n            for type, entity_list in list(entities.items()):\n                for entity in entity_list:\n                    entity['type'] = type\n                    obj = Entity(entity)\n                    obj_list.append(obj)\n            self.__dict__['entities'] = obj_list\n            return self.__dict__['entities']", "response": "Fetch the entities extracted from this document by OpenCalais."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the full text URL for a particular page in the document.", "response": "def get_page_text_url(self, page):\n        \"\"\"\n        Returns the URL for the full text of a particular page in the document.\n        \"\"\"\n        template = self.resources.page.get('text')\n        url = template.replace(\"{page}\", str(page))\n        return url"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndownloads and returns the full text of a particular page in the document.", "response": "def get_page_text(self, page):\n        \"\"\"\n        Downloads and returns the full text of a particular page\n        in the document.\n        \"\"\"\n        url = self.get_page_text_url(page)\n        return self._get_url(url)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the URL for the small sized image of a single page.", "response": "def get_small_image_url(self, page=1):\n        \"\"\"\n        Returns the URL for the small sized image of a single page.\n\n        The page kwarg specifies which page to return. One is the default.\n        \"\"\"\n        template = self.resources.page.get('image')\n        return template.replace(\n            \"{page}\",\n            str(page)\n        ).replace(\"{size}\", \"small\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_thumbnail_image_url(self, page=1):\n        template = self.resources.page.get('image')\n        return template.replace(\n            \"{page}\",\n            str(page)\n        ).replace(\"{size}\", \"thumbnail\")", "response": "Returns the URL for the thumbnail sized image of a single page."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the URL for the normal sized image of a single page.", "response": "def get_normal_image_url(self, page=1):\n        \"\"\"\n        Returns the URL for the \"normal\" sized image of a single page.\n\n        The page kwarg specifies which page to return. One is the default.\n        \"\"\"\n        template = self.resources.page.get('image')\n        return template.replace(\n            \"{page}\",\n            str(page)\n        ).replace(\"{size}\", \"normal\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the URL for the large sized image of a single page.", "response": "def get_large_image_url(self, page=1):\n        \"\"\"\n        Returns the URL for the large sized image of a single page.\n\n        The page kwarg specifies which page to return. One is the default.\n        \"\"\"\n        template = self.resources.page.get('image')\n        return template.replace(\"{page}\", str(page)).replace(\"{size}\", \"large\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_small_image(self, page=1):\n        url = self.get_small_image_url(page=page)\n        return self._get_url(url)", "response": "Downloads and returns the small sized image of a single page."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_thumbnail_image(self, page=1):\n        url = self.get_thumbnail_image_url(page=page)\n        return self._get_url(url)", "response": "Downloads and returns the thumbnail sized image of a single page."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndownload and returns the normal sized image of a single page.", "response": "def get_normal_image(self, page=1):\n        \"\"\"\n        Downloads and returns the normal sized image of a single page.\n\n        The page kwarg specifies which page to return. One is the default.\n        \"\"\"\n        url = self.get_normal_image_url(page=page)\n        return self._get_url(url)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndownloads and returns the large sized image of a single page.", "response": "def get_large_image(self, page=1):\n        \"\"\"\n        Downloads and returns the large sized image of a single page.\n\n        The page kwarg specifies which page to return. One is the default.\n        \"\"\"\n        url = self.get_large_image_url(page=page)\n        return self._get_url(url)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef put(self):\n        params = dict(\n            title=self.title or '',\n            description=self.description or '',\n            document_ids=[str(i.id) for i in self.document_list]\n        )\n        self._connection.put('projects/%s.json' % self.id, params)", "response": "Save the object to DocumentCloud s org\n            field."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_document_list(self):\n        try:\n            return self.__dict__['document_list']\n        except KeyError:\n            obj_list = DocumentSet([\n                self._connection.documents.get(i) for i in self.document_ids\n            ])\n            self.__dict__['document_list'] = obj_list\n            return obj_list", "response": "Retrieves all documents included in this project."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving a particular document from this project.", "response": "def get_document(self, id):\n        \"\"\"\n        Retrieves a particular document from this project.\n        \"\"\"\n        obj_list = self.document_list\n        matches = [i for i in obj_list if str(i.id) == str(id)]\n        if not matches:\n            raise DoesNotExistError(\"The resource you've requested does not \\\nexist or is unavailable without the proper credentials.\")\n        return matches[0]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef draw(self, text1=None, text2=None, copyright=True, image=IMAGE_DEFAULT, rotate=30, opacity=0.08, compress=0,\n             flatten=False, add=False):\n        \"\"\"\n        Draw watermark PDF file.\n\n        Create watermark using either a reportlabs canvas or a PIL image.\n\n        :param text1: str\n            Text line 1\n        :param text2: str\n            Text line 2\n        :param copyright: bool\n            Draw copyright and year to canvas\n        :param image: str\n            Logo image to be used as base watermark\n        :param rotate: int\n            Degrees to rotate canvas by\n        :param opacity: float\n            Watermark opacity\n        :param compress: bool\n            Compress watermark contents  (not entire PDF)\n        :param flatten: bool\n            Draw watermark with multiple layers or a single flattened layer\n        :param add: bool\n            Add watermark to original document\n        :return: str\n            Watermark PDF file full path\n        \"\"\"\n        im_path = os.path.join(IMAGE_DIRECTORY, image)\n        if os.path.isfile(im_path):\n            image = im_path\n\n        # Add to receipt\n        if self.use_receipt:\n            self.receipt.add('Text1', text1)\n            self.receipt.add('Text2', text2)\n            self.receipt.add('Image', os.path.basename(image))\n            self.receipt.add('WM Opacity', str(int(opacity * 100)) + '%')\n            self.receipt.add('WM Compression', compress)\n            self.receipt.add('WM Flattening', flatten)\n\n        co = CanvasConstructor(text1, text2, copyright, image, rotate, opacity, tempdir=self.tempdir)\n        objects, rotate = co.img() if flatten else co.canvas()  # Run img constructor method if flatten is True\n\n        # Draw watermark to file\n        self.watermark = WatermarkDraw(objects, rotate=rotate, compress=compress, tempdir=self.tempdir,\n                                       pagesize=Info(self.document_og).size, pagescale=True).write()\n\n        if not add:\n            return self.watermark\n        else:\n            self.add()\n            return self.cleanup()", "response": "Draw watermark PDF file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add(self, document=None, watermark=None, underneath=False, output=None, suffix='watermarked', method='pdfrw'):\n        if self.use_receipt:\n            self.receipt.add('WM Placement', 'Overlay')\n        if not watermark:\n            watermark = self.watermark\n        if not document:\n            document = self.document\n        self.document = str(WatermarkAdd(document, watermark, output=output, underneath=underneath,\n                                         tempdir=self.tempdir, suffix=suffix, method=method))\n        if self.use_receipt:\n            self.receipt.add('Watermarked PDF', os.path.basename(self.document))\n        if self.open_file:\n            open_window(self.document)\n        return self.document", "response": "Adds a watermark file to an existing PDF document."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nencrypting a PDF document with user password and owner password and restrict permissions.", "response": "def encrypt(self, user_pw='', owner_pw=None, encrypt_128=True, allow_printing=True, allow_commenting=False,\n                document=None):\n        \"\"\"\n        Encrypt a PDF document to add passwords and restrict permissions.\n\n        Add a user password that must be entered to view document and a owner password that must be entered to alter\n        permissions and security settings.  Encryption keys are 128 bit when encrypt_128 is True and 40 bit when\n        False.  By default permissions are restricted to print only, when set to false all permissions are allowed.\n        TODO: Add additional permission parameters\n\n        :param user_pw: str\n            User password required to open and view PDF document\n        :param owner_pw: str\n            Owner password required to alter security settings and permissions\n        :param encrypt_128: bool\n            Encrypt PDF document using 128 bit keys\n        :param allow_printing: bool\n            Restrict permissions to print only\n        :return: str\n            Encrypted PDF full path\n        \"\"\"\n        document = self.document if document is None else document\n        if self.use_receipt:\n            self.receipt.add('User pw', user_pw)\n            self.receipt.add('Owner pw', owner_pw)\n            if encrypt_128:\n                self.receipt.add('Encryption key size', '128')\n            else:\n                self.receipt.add('Encryption key size', '40')\n            if allow_printing:\n                self.receipt.add('Permissions', 'Allow printing')\n            else:\n                self.receipt.add('Permissions', 'Allow ALL')\n        p = str(Encrypt(document, user_pw, owner_pw, output=add_suffix(self.document_og, 'secured'),\n                        bit128=encrypt_128, allow_printing=allow_printing, allow_commenting=allow_commenting,\n                        progress_bar_enabled=self.progress_bar_enabled, progress_bar=self.progress_bar))\n        if self.use_receipt:\n            self.receipt.add('Secured PDF', os.path.basename(p))\n        return p"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef retry(ExceptionToCheck, tries=3, delay=2, backoff=2):\n    def deco_retry(f):\n        def f_retry(*args, **kwargs):\n            mtries, mdelay = tries, delay\n            try_one_last_time = True\n            while mtries > 1:\n                try:\n                    return f(*args, **kwargs)\n                    try_one_last_time = False\n                    break\n                except ExceptionToCheck:\n                    six.print_(\"Retrying in %s seconds\" % str(mdelay))\n                    time.sleep(mdelay)\n                    mtries -= 1\n                    mdelay *= backoff\n            if try_one_last_time:\n                return f(*args, **kwargs)\n            return\n        return f_retry  # true decorator\n    return deco_retry", "response": "Decorator to retry a function in a node s hierarchy."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndetermining with width in pixels of string.", "response": "def text_width(string, font_name, font_size):\n    \"\"\"Determine with width in pixels of string.\"\"\"\n    return stringWidth(string, fontName=font_name, fontSize=font_size)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef center_str(txt, font_name, font_size, offset=0):\n    return -(text_width(txt, font_name, font_size) / 2.0) + offset", "response": "Center a string on the x axis of a reportslab canvas"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsplits string in half to return two strings", "response": "def split_str(string):\n    \"\"\"Split string in half to return two strings\"\"\"\n    split = string.split(' ')\n    return ' '.join(split[:len(split) // 2]), ' '.join(split[len(split) // 2:])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndraw image object to reportlabs canvas.", "response": "def _draw_image(self, ci):\n        \"\"\"\n        Draw image object to reportlabs canvas.\n\n        :param ci: CanvasImage object\n        \"\"\"\n        img = img_adjust(ci.image, ci.opacity, tempdir=self.dir)\n        self.can.drawImage(img, x=ci.x, y=ci.y, width=ci.w, height=ci.h, mask=ci.mask,\n                           preserveAspectRatio=ci.preserve_aspect_ratio, anchorAtXY=True)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _draw_string(self, cs):\n        # 1. Font name\n        if self.can._fontname != cs.font:\n            self.can.setFont(cs.font, cs.size)\n\n        # 2. Font size\n        elif self.can._fontsize != cs.size:\n            self.can.setFontSize(cs.size)\n\n        # 3. Font file color\n        self.can.setFillColor(cs.color, cs.opacity)\n\n        # 4. X and Y positions\n        # X and Y are both centered\n        if cs.y_centered and cs.x_centered:\n            # Check if text_width is greater than the canvas page width\n            if text_width(cs.string, cs.font, cs.size) > self.can._pagesize[0]:\n                str1, str2 = split_str(cs.string)\n                self.can.drawString(x=center_str(str1, cs.font, cs.size, offset=0), y=cs.size, text=str1)\n                self.can.drawString(x=center_str(str2, cs.font, cs.size, offset=0), y=-cs.size, text=str2)\n                return\n            else:\n                x = center_str(cs.string, cs.font, cs.size, offset=0)\n                y = 0\n\n        # Y is centered and X is not\n        elif cs.y_centered and not cs.x_centered:\n            x = cs.x\n            y = 0\n\n        # X is centered and Y is not\n        elif cs.x_centered and not cs.y_centered:\n            x = center_str(cs.string, cs.font, cs.size, offset=0)\n            y = cs.y\n        else:\n            x = cs.x\n            y = cs.y\n        self.can.drawString(x=x, y=y, text=cs.string)\n        return", "response": "Draw string object to reportlabs canvas."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_pdf_list(self, input_pdfs):\n        if isinstance(input_pdfs, list):\n            return [pdf for pdf in input_pdfs if self.validate(pdf)]\n        elif os.path.isdir(input_pdfs):\n            return [os.path.join(input_pdfs, pdf) for pdf in os.listdir(input_pdfs) if self.validate(pdf)]", "response": "Generate list of PDFs or a directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef merge(self, pdf_files, output):\n        if self.method is 'pypdf3':\n            return self.pypdf3(pdf_files, output)\n        else:\n            return self.pdfrw(pdf_files, output)", "response": "Merge list of PDF files to a single PDF file."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds a suffix to a file name seperated by an underscore and returns the file path.", "response": "def add_suffix(file_path, suffix='modified', sep='_', ext=None):\n    \"\"\"Adds suffix to a file name seperated by an underscore and returns file path.\"\"\"\n    return _add_suffix(file_path, suffix, sep, ext)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_destination(source, suffix, filename=False, ext=None):\n    source_dirname = os.path.dirname(source)\n\n    # Do not create nested temp folders (/temp/temp)\n    if not source_dirname.endswith('temp'):\n        directory = os.path.join(source_dirname, 'temp')  # directory\n    else:\n        directory = source_dirname\n\n    # Create temp dir if it does not exist\n    if not os.path.isdir(directory):\n        os.mkdir(directory)\n\n    # Parse source filename\n    if filename:\n        src_file_name = filename\n    else:\n        src_file_name = Path(source).stem  # file name\n    if ext:\n        src_file_ext = ext\n    else:\n        src_file_ext = Path(source).suffix  # file extension\n\n    # Concatenate new filename\n    dst_path = src_file_name + '_' + suffix + src_file_ext\n    full_path = os.path.join(directory, dst_path)  # new full path\n\n    if not os.path.exists(full_path):\n        return full_path\n    else:\n        # If file exists, increment number until filename is unique\n        number = 1\n        while True:\n            dst_path = src_file_name + '_' + suffix + '_' + str(number) + src_file_ext\n            if not os.path.exists(dst_path):\n                break\n            number = number + 1\n        full_path = os.path.join(directory, dst_path)  # new full path\n        return full_path", "response": "Create new pdf filename for temp files"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getsize(o_file):\n    startpos = o_file.tell()\n    o_file.seek(0)\n    o_file.seek(0, SEEK_END)\n    size = o_file.tell()\n    o_file.seek(startpos)\n    return size", "response": "get the size of the file"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef dimensions(path):\n    pdf = PdfFileReader(path)\n    size = pdf.getPage(0).mediaBox\n    return {'w': float(size[2]), 'h': float(size[3])}", "response": "Get width and height of a PDF"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupscale a PDF to a large size.", "response": "def upscale(file_name, scale=1.5, margin_x=0, margin_y=0, suffix='scaled', tempdir=None):\n    \"\"\"Upscale a PDF to a large size.\"\"\"\n    # Set output file name\n    if tempdir:\n        output = NamedTemporaryFile(suffix='.pdf', dir=tempdir, delete=False).name\n    elif suffix:\n        output = os.path.join(os.path.dirname(file_name), add_suffix(file_name, suffix))\n    else:\n        output = NamedTemporaryFile(suffix='.pdf').name\n\n    reader = PdfFileReader(file_name)\n    writer = PdfFileWriter()\n    dims = dimensions(file_name)\n    target_w = dims['w'] * scale\n    target_h = dims['h'] * scale\n\n    # Number of pages in input document\n    page_count = reader.getNumPages()\n\n    for page_number in range(page_count):\n        wtrmrk = reader.getPage(page_number)\n\n        page = PageObject.createBlankPage(width=target_w, height=target_h)\n        page.mergeScaledTranslatedPage(wtrmrk, scale, margin_x, margin_y)\n        writer.addPage(page)\n\n    with open(output, \"wb\") as outputStream:\n        writer.write(outputStream)\n\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwraps function for PDF2IMG class", "response": "def pdf2img(file_name, output=None, tempdir=None, ext='png', progress_bar=None):\n    \"\"\"Wrapper function for PDF2IMG class\"\"\"\n    return PDF2IMG(file_name=file_name, output=output, tempdir=tempdir, ext=ext, progress_bar=progress_bar).save()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_page_data(self, pno, zoom=0):\n        dlist = self.dlist_tab[pno]  # get display list\n        if not dlist:  # create if not yet there\n            self.dlist_tab[pno] = self.doc[pno].getDisplayList()\n            dlist = self.dlist_tab[pno]\n        r = dlist.rect  # page rectangle\n        mp = r.tl + (r.br - r.tl) * 0.5  # rect middle point\n        mt = r.tl + (r.tr - r.tl) * 0.5  # middle of top edge\n        ml = r.tl + (r.bl - r.tl) * 0.5  # middle of left edge\n        mr = r.tr + (r.br - r.tr) * 0.5  # middle of right egde\n        mb = r.bl + (r.br - r.bl) * 0.5  # middle of bottom edge\n        mat = fitz.Matrix(2, 2)  # zoom matrix\n        if zoom == 1:  # top-left quadrant\n            clip = fitz.Rect(r.tl, mp)\n        elif zoom == 4:  # bot-right quadrant\n            clip = fitz.Rect(mp, r.br)\n        elif zoom == 2:  # top-right\n            clip = fitz.Rect(mt, mr)\n        elif zoom == 3:  # bot-left\n            clip = fitz.Rect(ml, mb)\n        if zoom == 0:  # total page\n            pix = dlist.getPixmap(alpha=False)\n        else:\n            pix = dlist.getPixmap(alpha=False, matrix=mat, clip=clip)\n        return pix.getPNGData()", "response": "Return a PNG image for a document page number."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nextracts text from a PDF file", "response": "def text_extract(path, password=None):\n    \"\"\"Extract text from a PDF file\"\"\"\n    pdf = Info(path, password).pdf\n\n    return [pdf.getPage(i).extractText() for i in range(pdf.getNumPages())]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nopen path in finder or explorer window", "response": "def open_window(path):\n    \"\"\"Open path in finder or explorer window\"\"\"\n    if 'pathlib' in modules:\n        try:\n            call([\"open\", \"-R\", str(Path(str(path)))])\n        except FileNotFoundError:\n            Popen(r'explorer /select,' + str(Path(str(path))))\n    else:\n        print('pathlib module must be installed to execute open_window function')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupscale a PDF to a large size.", "response": "def upscale(file_name, scale=1.5, margin_x=0, margin_y=0, suffix='scaled', tempdir=None):\n    \"\"\"Upscale a PDF to a large size.\"\"\"\n    def adjust(page):\n        info = PageMerge().add(page)\n        x1, y1, x2, y2 = info.xobj_box\n        viewrect = (margin_x, margin_y, x2 - x1 - 2 * margin_x, y2 - y1 - 2 * margin_y)\n        page = PageMerge().add(page, viewrect=viewrect)\n        page[0].scale(scale)\n        return page.render()\n\n    # Set output file name\n    if tempdir:\n        output = NamedTemporaryFile(suffix='.pdf', dir=tempdir, delete=False).name\n    elif suffix:\n        output = os.path.join(os.path.dirname(file_name), add_suffix(file_name, suffix))\n    else:\n        output = NamedTemporaryFile(suffix='.pdf').name\n\n    reader = PdfReader(file_name)\n    writer = PdfWriter(output)\n    for i in list(range(0, len(reader.pages))):\n        writer.addpage(adjust(reader.pages[i]))\n    writer.trailer.Info = IndirectPdfDict(reader.Info or {})\n    writer.write()\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_suffix(file_path, suffix):\n    split = os.path.basename(file_path).rsplit('.', 1)\n    ext = split[1]\n    name = split[0]\n    out = str(name + '_' + suffix + '.' + ext)\n    return os.path.join(os.path.dirname(file_path), out)", "response": "Adds a suffix to a file name seperated by an underscore and returns file path."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nencrypt a PDF file and restrict permissions to print only.", "response": "def secure(pdf, user_pw, owner_pw, restrict_permission=True, pdftk=get_pdftk_path(), output=None):\n    \"\"\"\n    Encrypt a PDF file and restrict permissions to print only.\n\n    Utilizes pdftk command line tool.\n\n    :param pdf: Path to PDF file\n    :param user_pw: Password to open and view\n    :param owner_pw: Password to transform permissions\n    :param restrict_permission: Restrict permissions to print only\n    :param pdftk: Path to pdftk binary\n    :param output: Output path\n    :return: Output path\n    \"\"\"\n    if pdftk:\n        # Check that PDF file is encrypted\n        with open(pdf, 'rb') as f:\n            reader = PdfFileReader(f)\n            if reader.isEncrypted:\n                print('PDF is already encrypted')\n                return pdf\n\n        # Create output filename if not already set\n        if not output:\n            output = add_suffix(pdf, 'secured')\n\n        # Replace spaces within paths with backslashes followed by a space\n        pdf_en = pdf.replace(' ', '\\ ')\n        output_en = output.replace(' ', '\\ ')\n\n        # Concatenate bash command\n        command = pdftk + ' ' + pdf_en + ' output ' + output_en + ' owner_pw ' + owner_pw + ' user_pw ' + user_pw\n\n        # Append string to command if printing is allowed\n        if restrict_permission:\n            command += ' allow printing'\n\n        # Execute command\n        os.system(command)\n        print('Secured PDF saved to...', output)\n        return output\n    else:\n        print('Unable to locate pdftk binary')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add(self, document, watermark):\n        # 5a. Create output PDF file name\n        output_filename = self.output_filename\n\n        def pypdf3():\n            \"\"\"Much slower than PyPDF3 method.\"\"\"\n            # 5b. Get our files ready\n            document_reader = PdfFileReader(document)\n            output_file = PdfFileWriter()\n\n            # Number of pages in input document\n            page_count = document_reader.getNumPages()\n\n            # Watermark objects\n            watermark_reader = PdfFileReader(watermark)\n            wtrmrk_page = watermark_reader.getPage(0)\n            wtrmrk_width = (wtrmrk_page.mediaBox.getWidth() / 2) + 0\n            wtrmrk_height = (wtrmrk_page.mediaBox.getHeight() / 2) + 80\n            wtrmrk_rotate = -int(Info(watermark_reader).rotate) if Info(watermark_reader).rotate is not None else 0\n\n            # 5c. Go through all the input file pages to add a watermark to them\n            for page_number in range(page_count):\n                # Merge the watermark with the page\n                if not self.underneath:\n                    input_page = document_reader.getPage(page_number)\n                    if wtrmrk_rotate is not 0:\n                        input_page.mergeRotatedTranslatedPage(wtrmrk_page, wtrmrk_rotate, wtrmrk_width, wtrmrk_height)\n                    else:\n                        wtrmrk_width = 0\n                        wtrmrk_height = 0\n                        input_page.mergeTranslatedPage(wtrmrk_page, wtrmrk_width, wtrmrk_height)\n                else:\n                    size = Info(document_reader).dimensions\n                    input_page = PageObject().createBlankPage(document_reader, size['w'], size['h'])\n                    if wtrmrk_rotate is not 0:\n                        input_page.mergeRotatedTranslatedPage(wtrmrk_page, wtrmrk_rotate, wtrmrk_width, wtrmrk_height)\n                    else:\n                        wtrmrk_width = 0\n                        wtrmrk_height = 0\n                        input_page.mergeTranslatedPage(wtrmrk_page, wtrmrk_width, wtrmrk_height)\n                    input_page.mergePage(document_reader.getPage(page_number))\n\n                # Add page from input file to output document\n                output_file.addPage(input_page)\n\n            # 5d. finally, write \"output\" to PDF\n            with open(output_filename, \"wb\") as outputStream:\n                output_file.write(outputStream)\n            return output_filename\n\n        def pdfrw():\n            \"\"\"Faster than PyPDF3 method by as much as 15x.\"\"\"\n            # TODO: Fix issue where watermark is improperly placed on large pagesize PDFs\n            # print(Info(document).size)\n            # print(Info(watermark).size)\n            # print('\\n')\n\n            # Open both the source files\n            wmark_trailer = PdfReader(watermark)\n            trailer = PdfReader(document)\n\n            # Handle different sized pages in same document with\n            # a memoization cache, so we don't create more watermark\n            # objects than we need to (typically only one per document).\n\n            wmark_page = wmark_trailer.pages[0]\n            wmark_cache = {}\n\n            # Process every page\n            for pagenum, page in enumerate(trailer.pages, 1):\n\n                # Get the media box of the page, and see\n                # if we have a matching watermark in the cache\n                mbox = tuple(float(x) for x in page.MediaBox)\n                odd = pagenum & 1\n                key = mbox, odd\n                wmark = wmark_cache.get(key)\n                if wmark is None:\n\n                    # Create and cache a new watermark object.\n                    wmark = wmark_cache[key] = PageMerge().add(wmark_page)[0]\n\n                    # The math is more complete than it probably needs to be,\n                    # because the origin of all pages is almost always (0, 0).\n                    # Nonetheless, we illustrate all the values and their names.\n\n                    page_x, page_y, page_x1, page_y1 = mbox\n                    page_w = page_x1 - page_x\n                    page_h = page_y1 - page_y  # For illustration, not used\n\n                    # Scale the watermark if it is too wide for the page\n                    # (Could do the same for height instead if needed)\n                    if wmark.w > page_w:\n                        wmark.scale(1.0 * page_w / wmark.w)\n\n                    # Always put watermark at the top of the page\n                    # (but see horizontal positioning for other ideas)\n                    wmark.y += page_y1 - wmark.h\n\n                    # For odd pages, put it at the left of the page,\n                    # and for even pages, put it on the right of the page.\n                    if odd:\n                        wmark.x = page_x\n                    else:\n                        wmark.x += page_x1 - wmark.w\n\n                    # Optimize the case where the watermark is same width\n                    # as page.\n                    if page_w == wmark.w:\n                        wmark_cache[mbox, not odd] = wmark\n\n                # Add the watermark to the page\n                PageMerge(page).add(wmark, prepend=self.underneath).render()\n\n            # Write out the destination file\n            PdfWriter(output_filename, trailer=trailer).write()\n\n        if self.method is 'pypdf3':\n            return pypdf3()\n        else:\n            return pdfrw()", "response": "Add watermark to the PDF by merging original PDF and watermark file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _add_suffix(file_path, suffix, sep='_', ext=None):\n    # Split file_path on last '.' to separate file_name and file_extension\n    split = os.path.basename(file_path).rsplit('.', 1)\n\n    # Use original file_extension if None is given\n    ext = split[1] if not ext else ext.strip('.')\n\n    # Rebuild new file_path with suffix\n    return os.path.join(os.path.dirname(file_path), split[0] + sep + suffix + '.' + ext)", "response": "Adds a suffix to a file name seperated by an underscore and returns file path."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef img_opacity(image, opacity):\n    # Validate parameters\n    assert 0 <= opacity <= 1, 'Opacity must be a float between 0 and 1'\n    assert os.path.isfile(image), 'Image is not a file'\n\n    # Open image in RGBA mode if not already in RGBA\n    im = Image.open(image)\n    if im.mode != 'RGBA':\n        im = im.convert('RGBA')\n    else:\n        im = im.copy()\n\n    # Adjust opacity\n    alpha = im.split()[3]\n    alpha = ImageEnhance.Brightness(alpha).enhance(opacity)\n    im.putalpha(alpha)\n\n    # Save modified image file\n    dst = _add_suffix(image, str(str(int(opacity * 100)) + '%'), ext='.png')\n    im.save(dst)\n    return dst", "response": "Reduce the opacity of a PNG image."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _image_loop(self):\n        if self.progress_bar and 'tqdm' in self.progress_bar.lower():\n            return tqdm(self.imgs, desc='Saving PNGs as flat PDFs', total=len(self.imgs), unit='PDFs')\n        else:\n            return self.imgs", "response": "Retrieve an iterable of images either with a progress bar or without a progress bar."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef convert(self, image, output=None):\n        return self._convert(image, image.replace(Path(image).suffix, '.pdf') if not output else output)", "response": "Convert an image to a PDF."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef overlay_pdfs(top_pdf, bottom_pdf, destination):\n    drawing = PdfFileReader(top_pdf)  # Create new PDF object\n    template = PdfFileReader(bottom_pdf)  # read your existing PDF\n\n    # add the \"watermark\" (which is the new pdf) on the existing page\n    page = template.getPage(0)\n    page.mergePage(drawing.getPage(0))\n    output = PdfFileWriter()  # Create new PDF file\n    output.addPage(page)\n\n    # finally, write \"output\" to a real file\n    with open(destination, \"wb\") as outputStream:\n        output.write(outputStream)", "response": "Overlays the PDFs top_pdf and bottom_pdf into a single file containing the new PDFs."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwrites a PDF object to a file", "response": "def write_pdf(pdf_obj, destination):\n    \"\"\"\n    Write PDF object to file\n    :param pdf_obj: PDF object to be written to file\n    :param destination: Desintation path\n    \"\"\"\n    reader = PdfFileReader(pdf_obj)  # Create new PDF object\n    writer = PdfFileWriter()\n\n    page_count = reader.getNumPages()\n\n    # add the \"watermark\" (which is the new pdf) on the existing page\n    for page_number in range(page_count):\n        page = reader.getPage(page_number)\n        writer.addPage(page)\n\n    # finally, write \"output\" to a real file\n    with open(destination, \"wb\") as outputStream:\n        writer.write(outputStream)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds an image to the GUI img library.", "response": "def add(image_path, file_name=None):\n    \"\"\"Add an image to the GUI img library.\"\"\"\n    if file_name is not None:\n        dst_path = os.path.join(IMG_DIR, str(Path(file_name).stem + Path(image_path).suffix))\n    else:\n        dst_path = IMG_DIR\n\n    if os.path.isfile(image_path):\n        shutil.copy2(image_path, dst_path)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef remove(image):\n    path = os.path.join(IMG_DIR, image)\n    if os.path.isfile(path):\n        os.remove(path)", "response": "Remove an image to the GUI img library."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rotate(file_name, rotation, suffix='rotated', tempdir=None, method='pypdf3'):\n    return str(Rotate(file_name, rotation, suffix, tempdir, method))", "response": "Rotate PDF by increments of 90 degrees."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a new blueprint access request.", "response": "def pluginkit_beforerequest():\n    \"\"\"blueprint access auth\"\"\"\n    authMethod = current_app.config.get(\"PLUGINKIT_AUTHMETHOD\")\n    authResult = dict(msg=None, code=1, method=authMethod)\n\n    def authTipmsg(authResult, code=403):\n        \"\"\"make response message\"\"\"\n        return \"%s Authentication failed [%s]: %s [%s]\" % (code, authResult[\"method\"], authResult[\"msg\"], authResult[\"code\"])\n\n    if authMethod == \"BOOL\":\n        \"\"\"Boolean Auth\"\"\"\n        PLUGINKIT_AUTHFIELD = current_app.config.get(\"PLUGINKIT_AUTHFIELD\")\n        if PLUGINKIT_AUTHFIELD:\n            if PLUGINKIT_AUTHFIELD is True:\n                authResult.update(code=0)\n        else:\n            if hasattr(g, \"signin\") and g.signin is True:\n                authResult.update(code=0)\n        authResult.update(code=10000, msg=\"Invalid authentication field\")\n\n    elif authMethod == \"BASIC\":\n        \"\"\"HTTP Basic Auth\"\"\"\n\n        #: the realm parameter is reserved for defining protection spaces and\n        #: it's used by the authentication schemes to indicate a scope of protection.\n        #:\n        #: .. versionadded:: 1.2.0\n        authRealm = current_app.config.get(\"PLUGINKIT_AUTHREALM\") or \"Flask-PluginKit Login Required\"\n\n        #: User and password configuration, format {user:pass, user:pass},\n        #: if format error, all authentication failure by default.\n        #:\n        #: .. versionadded:: 1.2.0\n        authUsers = current_app.config.get(\"PLUGINKIT_AUTHUSERS\")\n\n        def verify_auth(username, password):\n            \"\"\"Check the user and password\"\"\"\n            if isinstance(authUsers, dict) and username in authUsers:\n                return password == authUsers[username]\n            return False\n\n        def not_authenticated():\n            \"\"\"Sends a 401 response that enables basic auth\"\"\"\n            return Response(authTipmsg(authResult, 401), 401,\n                            {'WWW-Authenticate': 'Basic realm=\"%s\"' % authRealm})\n\n        #: Intercepts authentication and denies access if it fails\n        auth = request.authorization\n        if not auth or not verify_auth(auth.username, auth.password):\n            authResult.update(code=10001, msg=\"Invalid username or password\")\n            return not_authenticated()\n        else:\n            authResult.update(code=0)\n\n    else:\n        authResult.update(code=0, msg=\"No authentication required\", method=None)\n\n    #: return response if code != 0\n    if authResult[\"code\"] != 0:\n        return make_response(authTipmsg(authResult)), 403\n\n    #: get all plugins based flask-pluginkit\n    if hasattr(current_app, \"extensions\") and \"pluginkit\" in current_app.extensions:\n        g.plugin_manager = current_app.extensions[\"pluginkit\"]\n        g.plugins = g.plugin_manager.get_all_plugins"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef push_dcp(event, callback, position='right'):\n    ctx = stack.top\n    ctx.app.extensions.get('pluginkit').push_dcp(event, callback, position)", "response": "Push a callable for the dcp event."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nscan local plugin directories and third - party plugin packages and load them into self. plugins.", "response": "def __scanPlugins(self):\n        \"\"\"Scanning local plugin directories and third-party plugin packages.\n\n        :returns: No return, but self.__plugins will be updated\n\n        :raises:\u00a0PluginError:\u00a0raises\u00a0an\u00a0exception, maybe CSSLoadError, VersionError, based PluginError\n        \"\"\"\n        self.logger.info(\"Initialization Plugins Start, local plugins path: %s, third party plugins: %s\" % (self.plugins_abspath, self.plugin_packages))\n\n        #: Load third-party plugins\n        if self.plugin_packages and isinstance(self.plugin_packages, (list, tuple)):\n            for package_name in self.plugin_packages:\n                try:\n                    plugin = __import__(package_name)\n                except ImportError as e:\n                    raise PluginError(\"ImportError for %s, detail is %s\" %(package_name, e))\n                else:\n                    plugin_abspath = os.path.dirname(os.path.abspath(plugin.__file__))\n                    self.__loadPlugin(plugin, plugin_abspath, package_name)\n\n        #: Load local plug-in directory\n        if os.path.isdir(self.plugins_abspath) and os.path.isfile(os.path.join(self.plugins_abspath, \"__init__.py\")):\n            for package_name in os.listdir(self.plugins_abspath):\n                package_abspath = os.path.join(self.plugins_abspath, package_name)\n                if os.path.isdir(package_abspath) and os.path.isfile(os.path.join(package_abspath, \"__init__.py\")):\n                    self.logger.info(\"find plugin package: %s\" % package_name)\n                    #: Dynamic load module (plugins.package): you can query custom information and get the plugin's class definition through getPluginClass\n                    plugin = __import__(\"{0}.{1}\".format(self.plugins_folder, package_name), fromlist=[self.plugins_folder, ])\n                    self.__loadPlugin(plugin, package_abspath, package_name)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads a plugin from a python package with the given package path.", "response": "def __loadPlugin(self, plugin, package_abspath, package_name=None):\n        \"\"\"Load plugin, plugin is a python package with `__import__`.\n\n        :param plugin: Dynamically loaded plugin modules\n\n        :param package_abspath: The package absolute directory\n\n        :param package_name: Plugin package names (directories under plugins), such as PluginDemo\n        \"\"\"\n        #: Detection plugin information\n        if hasattr(plugin, \"__plugin_name__\") and \\\n                hasattr(plugin, \"__version__\") and \\\n                hasattr(plugin, \"__description__\") and \\\n                hasattr(plugin, \"__author__\") and \\\n                hasattr(plugin, \"getPluginClass\"):\n            try:\n                #: Get plugin information\n                pluginInfo = self.__getPluginInfo(plugin, package_abspath, package_name)\n                #: Get the plugin main class and instantiate it\n                p = plugin.getPluginClass()\n                i = p()\n            except Exception as e:\n                raise e\n            #: Subsequent methods are not executed when not enabled\n            if pluginInfo[\"plugin_state\"] != \"enabled\":\n                self.__plugins.append(pluginInfo)\n                return\n            #: Update plugin information\n            pluginInfo.update(plugin_instance=i)\n            #: Run the *run* method of the plugin main class\n            if hasattr(i, \"run\"):\n                #: Run once only when loading\n                i.run()\n            #: Register the template extension point\n            if hasattr(i, \"register_tep\"):\n                \"\"\" Template extension point requirements:\n                format:\n                    plugin_tep = {tep:dict(HTMLFile=str, HTMLString=str), tep...}\n\n                returns:\n                    return {tep_1: HTMLFile(str), tep_2: HTMLString(str)}\n\n                explanation:\n                    1. This must be dict, where key means that tep is the extension point identifier,\n                        and each extension point can contain only one template type,\n                        either HTML or string, which requires STR type, and other types trigger exceptions\n                    2. HTML template type suffix for `html` or `htm` as template file (the other as pure HTML code),\n                        to be real, will adopt a `render_template` way rendering,\n                        using template type can be specified when rendering and introduced to additional data\n                \"\"\"\n                tep = i.register_tep()\n                if isinstance(tep, dict):\n                    newTep = dict()\n                    for event, tpl in tep.items():\n                        if isinstance(tpl, string_types):\n                            if os.path.splitext(tpl)[-1] in (\".html\", \".htm\"):\n                                if os.path.isfile(os.path.join(pluginInfo[\"plugin_tpl_path\"], tpl.split(\"@\")[-1] if \"@\" in tpl and self.stpl is True else tpl)):\n                                    newTep[event] = dict(HTMLFile=tpl)\n                                else:\n                                    raise jinja2.TemplateNotFound(\"TEP Template File Not Found: %s\" % tpl)\n                            else:\n                                newTep[event] = dict(HTMLString=jinja2.Markup(tpl))\n                        else:\n                            raise PluginError(\"Invalid TEP Format\")\n                    pluginInfo.update(plugin_tep=newTep)\n                    self.logger.info(\"Register TEP Success\")\n                else:\n                    raise PluginError(\"Register TEP Failed, not a dict\")\n            #: Register context extension points\n            if hasattr(i, \"register_hep\"):\n                cep = i.register_hep()\n                if isinstance(cep, dict):\n                    pluginInfo.update(plugin_hep=cep)\n                    self.logger.info(\"Register HEP Success\")\n                else:\n                    raise PluginError(\"Register HEP Failed, not a dict\")\n            #: Register the blueprint extension point\n            if hasattr(i, \"register_bep\"):\n                bep = i.register_bep()\n                if isinstance(bep, dict):\n                    pluginInfo.update(plugin_bep=bep)\n                    self.logger.info(\"Register BEP Success\")\n                else:\n                    raise PluginError(\"Register BEP Failed, not a dict\")\n            #: Register the css extension point\n            if hasattr(i, \"register_yep\"):\n                \"\"\"Register the plugin's cascading style sheet (CSS) file, requirements:\n                format:\n                    plugin_yep = {yep: css, yep: [css, ...]}\n\n                returns:\n                    return {yep: [css, ...], yep: [css, ...], ...}\n\n                explanation:\n                    1. This must be dict, and a key or an extension is the name of the page that CSS suggests to be introduced,\n                        and each extension can be a single CSS or a set of CSS\n                    2. CSS suffix must be `.css`, and real\n                \"\"\"\n                yep = i.register_yep()\n                if isinstance(yep, dict):\n                    newYep = dict()\n                    for event, css in yep.items():\n                        if isinstance(css, string_types):\n                            if os.path.isfile(os.path.join(pluginInfo[\"plugin_ats_path\"], css)) and css.endswith(\".css\"):\n                                newYep[event] = [self.static_url_path + \"/\" + css]\n                            else:\n                                raise CSSLoadError(\"YEP CSS File Is Invalid: %s\" % css)\n                        elif isinstance(css, (list, tuple)):\n                            newCss = []\n                            for ac in css:\n                                if isinstance(ac, string_types) and os.path.isfile(os.path.join(pluginInfo[\"plugin_ats_path\"], ac)) and ac.endswith(\".css\"):\n                                    newCss.append(self.static_url_path + \"/\" + ac)\n                                else:\n                                    raise CSSLoadError(\"YEP CSS File Is Invalid: %s\" % ac)\n                            newYep[event] = newCss\n                        else:\n                            raise PluginError(\"Invalid YEP Format\")\n                    pluginInfo.update(plugin_yep=newYep)\n                    self.logger.info(\"Register YEP Success\")\n                else:\n                    raise PluginError(\"Register YEP Failed, not a dict\")\n            #: Register the dfp with a func\n            if hasattr(i, \"register_dfp\"):\n                dfp = i.register_dfp()\n                if isinstance(dfp, dict):\n                    for cuin,func in dfp.items():\n                        self.push_func(cuin,func)\n                else:\n                    raise PluginError(\"Register DFP Failed, not a dict\")\n            #: TODO: Register signal extension points `sep`\n            #: Add to the global plugin\n            if hasattr(i, \"run\") or hasattr(i, \"register_tep\") or hasattr(i, \"register_hep\") or hasattr(i, \"register_bep\") or hasattr(i, \"register_yep\") or hasattr(i, \"register_dfp\"):\n                self.__plugins.append(pluginInfo)\n            else:\n                self.logger.error(\"The current package does not have the `run` or `register_tep` or `register_hep` or `register_bep` or `register_yep` method\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the plugin info.", "response": "def __getPluginInfo(self, plugin, package_abspath, package_name):\n        \"\"\" Organize plugin information.\n\n        :returns: dict: plugin info\n        \"\"\"\n        if not isValidSemver(plugin.__version__):\n            raise VersionError(\"The plugin version does not conform to the standard named %s\" % package_name)\n\n        try:\n            url = plugin.__url__\n        except AttributeError:\n            url = None\n\n        try:\n            license = plugin.__license__\n        except AttributeError:\n            license = None\n\n        try:\n            license_file = plugin.__license_file__\n        except AttributeError:\n            license_file = None\n\n        try:\n            readme_file = plugin.__readme_file__\n        except AttributeError:\n            readme_file = None\n\n        try:\n            plugin_state = plugin.__state__\n        except AttributeError:\n            plugin_state = \"enabled\"\n        # \u63d2\u4ef6\u72b6\u6001\u9996\u5148\u8bfb\u53d6`__state`\u72b6\u6001\u503c\uff0c\u4f18\u5148\u7ea7\u4f4e\u4e8e\u72b6\u6001\u6587\u4ef6\uff0cENABLED\u6587\u4ef6\u4f18\u5148\u7ea7\u4f4e\u4e8eDISABLED\u6587\u4ef6\n        if os.path.isfile(os.path.join(package_abspath, \"ENABLED\")):\n            plugin_state = \"enabled\"\n        if os.path.isfile(os.path.join(package_abspath, \"DISABLED\")):\n            plugin_state = \"disabled\"\n\n        return {\n            \"plugin_name\": plugin.__plugin_name__,\n            \"plugin_package_name\": package_name,\n            \"plugin_package_abspath\": package_abspath,\n            \"plugin_description\": plugin.__description__,\n            \"plugin_version\": plugin.__version__,\n            \"plugin_author\": plugin.__author__,\n            \"plugin_url\": url,\n            \"plugin_license\": license,\n            \"plugin_license_file\": license_file,\n            \"plugin_readme_file\": readme_file,\n            \"plugin_state\": plugin_state,\n            \"plugin_tpl_path\": os.path.join(package_abspath, \"templates\"),\n            \"plugin_ats_path\": os.path.join(package_abspath, \"static\"),\n            \"plugin_tep\": {},\n            \"plugin_hep\": {},\n            \"plugin_bep\": {},\n            \"plugin_yep\": {}\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef disable_plugin(self, plugin_name):\n        plugin = self.get_plugin_info(plugin_name)\n        ENABLED = os.path.join(plugin[\"plugin_package_abspath\"], \"ENABLED\")\n        DISABLED = os.path.join(plugin[\"plugin_package_abspath\"], \"DISABLED\")\n        if os.path.isfile(ENABLED):\n            os.remove(ENABLED)\n        self.__touch_file(DISABLED)", "response": "Disable a plugin that is created a DISABLED empty file and restart the application to take effect"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_all_tep(self):\n        teps = {}\n        for p in self.get_enabled_plugins:\n            for e, v in p[\"plugin_tep\"].items():\n                tep = teps.get(e, dict())\n                tepHF = tep.get(\"HTMLFile\", [])\n                tepHS = tep.get(\"HTMLString\", [])\n                tepHF += [s for f, s in v.items() if f == \"HTMLFile\"]\n                tepHS += [s for f, s in v.items() if f == \"HTMLString\"]\n                teps[e] = dict(HTMLFile=tepHF, HTMLString=tepHS)\n        return teps", "response": "Return all tep entries in the template extension point."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets all the available hook extensions.", "response": "def get_all_hep(self):\n        \"\"\"Hook extension point.\n\n        * before_request_hook, Before request (intercept requests are allowed)\n\n        * before_request_top_hook, Before top request (Put it first)\n\n        * after_request_hook, After request (no exception before return)\n\n        * teardown_request_hook, After request (before return, with or without exception)\n        \"\"\"\n        return dict(\n            before_request_hook=[plugin[\"plugin_hep\"][\"before_request_hook\"] for plugin in self.get_enabled_plugins if plugin[\"plugin_hep\"].get(\"before_request_hook\")],\n            before_request_top_hook=[plugin[\"plugin_hep\"][\"before_request_top_hook\"] for plugin in self.get_enabled_plugins if plugin[\"plugin_hep\"].get(\"before_request_top_hook\")],\n            after_request_hook=[plugin[\"plugin_hep\"][\"after_request_hook\"] for plugin in self.get_enabled_plugins if plugin[\"plugin_hep\"].get(\"after_request_hook\")],\n            teardown_request_hook=[plugin[\"plugin_hep\"][\"teardown_request_hook\"] for plugin in self.get_enabled_plugins if plugin[\"plugin_hep\"].get(\"teardown_request_hook\")],\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncascading style sheet extension points.", "response": "def get_all_yep(self):\n        \"\"\"Cascading style sheet (CSS) extension points.\n\n        :returns: dict: {yep: [css...], ...}\n        \"\"\"\n        yeps = {}\n        for p in self.get_enabled_plugins:\n            for e, v in p[\"plugin_yep\"].items():\n                yep = yeps.get(e, []) + v\n                yeps[e] = yep\n        return yeps"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nemits a event with tep and gets the template extension point data.", "response": "def emit_tep(self, tep, typ=\"all\", **context):\n        \"\"\"Emit a event(with tep) and gets the template extension point data(html code).\n\n        Please use this function in the template. The extension point needs to be defined by itself.\n\n        Suppose you have an extension point named `tep`, only need to enable custom extension points in the template, for examples::\n\n            #: It can render HTML code and files for an template extension point,\n            #: or even pass in extra data at render time\n            {{ emit_tep('tep', data=data) }}\n\n\n        :param tep: str: Template extension point name, which is unique, a tep parsing result is list, within which can be HTML code and files\n\n        :param typ: str: Render type, default value = \"all\"\n                        all - render all, is default;\n                        fil - render HTML file;\n                        cod - render HTML code\n\n        :param context: dict: Keyword parameter, additional data passed to the template\n\n        :returns: html code with :class:`~jinja2.Markup`.\n        \"\"\"\n        e = self.get_all_tep.get(tep) or dict(HTMLFile=[], HTMLString=[])\n        #: Disposable template sequence\n        if self.stpl is True:\n            e[\"HTMLFile\"] = map(lambda tpl: tpl.split('@')[-1], sorted(e['HTMLFile'], key=lambda x: x.split('@')[0], reverse=self.stpl_reverse))\n            e[\"HTMLString\"] = map(lambda tpl: tpl.split('@')[-1], sorted(e['HTMLString'], key=lambda x: x.split('@')[0], reverse=self.stpl_reverse))\n        typ = \"all\" if not typ in (\"fil\", \"cod\") else typ\n        mtf = jinja2.Markup(\"\".join([render_template(i, **context) for i in e[\"HTMLFile\"]]))\n        mtc = jinja2.Markup(\"\".join(e[\"HTMLString\"]))\n        if typ == \"fil\":\n            return mtf\n        elif typ == \"cod\":\n            return mtc\n        else:\n            return mtf + mtc"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef emit_yep(self, yep):\n        e = self.get_all_yep.get(yep) or []\n        tpl = ''\n        for css in e:\n            tpl += '<link rel=\"stylesheet\" type=\"text/css\" href=\"%s\" />' % css\n        return jinja2.Markup(tpl)", "response": "Returns the css extension point data for the given yep."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef storage(self, sf=None, args=None):\n        from .utils import BaseStorage, LocalStorage, RedisStorage\n        if sf and isinstance(sf, BaseStorage):\n            return sf(args) if args else sf()\n        if self.s3 == \"local\":\n            return LocalStorage()\n        elif self.s3 == \"redis\":\n            return RedisStorage(self.s3_redis)", "response": "Returns a new instance of the common storage interface with the given name."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconnect a dcp push a function.", "response": "def push_dcp(self, event, callback, position=\"right\"):\n        \"\"\"Connect a dcp, push a function.\n\n        :param event: str,unicode: A unique identifier name for dcp.\n\n        :param callback: callable: Corresponding to the event to perform a function.\n\n        :param position: The position of the insertion function, right(default) and left.\n\n        :raises: DCPError,NotCallableError:\u00a0raises\u00a0an\u00a0exception\n\n        .. versionadded:: 2.1.0\n        \"\"\"\n        if event and isinstance(event, string_types) and callable(callback) and position in (\"left\", \"right\"):\n            if event in self._dcp_funcs:\n                if position == \"right\":\n                    self._dcp_funcs[event].append(callback)\n                else:\n                    self._dcp_funcs[event].appendleft(callback)\n            else:\n                self._dcp_funcs[event] = deque([callback])\n        else:\n            if not callable(callback):\n                raise NotCallableError(\"The event %s cannot be called back\" % event)\n            raise DCPError(\"Invalid parameter\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nemit a event and gets the dynamic join point data.", "response": "def emit_dcp(self, event, **context):\n        \"\"\"Emit a event(with dcp) and gets the dynamic join point data(html code).\n\n        :param event: str,unicode: A unique identifier name for dcp.\n\n        :param context: dict: Keyword parameter, additional data passed to the template\n\n        :returns: html code with :class:`~jinja2.Markup`.\n\n        .. versionadded:: 2.1.0\n        \"\"\"\n        if event and isinstance(event, string_types) and event in self._dcp_funcs:\n            results = []\n            for f in self._dcp_funcs[event]:\n                rv = f(**context)\n                if rv is not None:\n                    results.append(rv)\n            del self._dcp_funcs[event]\n            return jinja2.Markup(TemplateEventResult(results))\n        else:\n            return jinja2.Markup()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\npushes a function to the dfp.", "response": "def push_func(self, cuin, callback):\n        \"\"\"Push a function for dfp.\n\n        :param cuin: str,unicode: Callback Unique Identifier Name.\n\n        :param callback: callable: Corresponding to the cuin to perform a function.\n\n        :raises: DFPError,NotCallableError:\u00a0raises\u00a0an\u00a0exception\n\n        .. versionadded:: 2.3.0\n        \"\"\"\n        if cuin and isinstance(cuin, string_types) and callable(callback):\n            if cuin in self._dfp_funcs:\n                raise DFPError(\"The cuin already exists\")\n            else:\n                self._dfp_funcs[cuin] = callback\n        else:\n            if not callable(callback):\n                raise NotCallableError(\"The cuin %s cannot be called back\" % cuin)\n            raise DFPError(\"Invalid parameter\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nemit a cuin with dfp and gets the func result.", "response": "def emit_func(self, cuin, *args, **kwargs):\n        \"\"\"Emit a cuin(with dfp) and gets the func result.\n\n        :param cuin: str,unicode: Callback Unique Identifier Name.\n\n        :param args: list: Variable length parameter for cuin.\n\n        :param kwargs: dict: Keyword parameter for cuin.\n\n        :returns: The result of calling the function.\n\n        .. versionadded:: 2.3.0\n        \"\"\"\n        if cuin and isinstance(cuin, string_types):\n            if cuin in self._dfp_funcs:\n                f = self._dfp_funcs[cuin]\n                return f(*args, **kwargs)\n            else:\n                raise DFPError(\"The cuin does not exist\")\n        else:\n            raise DFPError(\"Invalid parameter\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\npost - process options.", "response": "def finalize_options(self):\n        \"\"\"Post-process options.\"\"\"\n        if self.test:\n            print(\"V%s will publish to the test.pypi.org\" % version)\n        elif self.release:\n            print(\"V%s will publish to the pypi.org\" % version)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the timestamp after the given timestamp.", "response": "def timestamp_after_timestamp(self, timestamp=None, seconds=0, minutes=0, hours=0, days=0):\n        \"\"\" \u7ed9\u5b9a\u65f6\u95f4\u6233,\u8ba1\u7b97\u8be5\u65f6\u95f4\u6233\u4e4b\u540e\u591a\u5c11\u79d2\u3001\u5206\u949f\u3001\u5c0f\u65f6\u3001\u5929\u7684\u65f6\u95f4\u6233(\u672c\u5730\u65f6\u95f4) \"\"\"\n        #1. \u9ed8\u8ba4\u65f6\u95f4\u6233\u4e3a\u5f53\u524d\u65f6\u95f4\n        timestamp = self.get_current_timestamp() if timestamp is None else timestamp\n        #2. \u5148\u8f6c\u6362\u4e3adatetime\n        d1 = datetime.datetime.fromtimestamp(timestamp)\n        #3. \u6839\u636e\u76f8\u5173\u65f6\u95f4\u5f97\u5230datetime\u5bf9\u8c61\u5e76\u76f8\u52a0\u7ed9\u5b9a\u65f6\u95f4\u6233\u7684\u65f6\u95f4\n        d2 = d1 + datetime.timedelta(seconds=int(seconds), minutes=int(minutes), hours=int(hours), days=int(days))\n        #4. \u8fd4\u56de\u67d0\u65f6\u95f4\u540e\u7684\u65f6\u95f4\u6233\n        return int(time.mktime(d2.timetuple()))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a datetime. datetime object from a timestamp.", "response": "def timestamp_datetime(self, timestamp):\n        \"\"\" \u5c06uninx\u65f6\u95f4\u6233\u8f6c\u6362\u4e3a\u53ef\u8bfb\u6027\u7684\u65f6\u95f4 \"\"\"\n        format = '%Y-%m-%d %H:%M:%S'\n        # timestamp\u4e3a\u4f20\u5165\u7684\u503c\u4e3a\u65f6\u95f4\u6233(10\u4f4d\u6574\u6570)\uff0c\u5982\uff1a1332888820\n        timestamp = time.localtime(timestamp)\n        ## \u7ecf\u8fc7localtime\u8f6c\u6362\u540e\u53d8\u6210\n        ## time.struct_time(tm_year=2012, tm_mon=3, tm_mday=28, tm_hour=6, tm_min=53, tm_sec=40, tm_wday=2, tm_yday=88, tm_isdst=0)\n        # \u6700\u540e\u518d\u7ecf\u8fc7strftime\u51fd\u6570\u8f6c\u6362\u4e3a\u6b63\u5e38\u65e5\u671f\u683c\u5f0f\u3002\n        return time.strftime(format, timestamp)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef signatureJWT(self, message):\n        return hmac.new(\n            key=self.secretkey,\n            msg=message,\n            digestmod=hashlib.sha256\n        ).hexdigest()", "response": "Python generate HMAC - SHA - 256 from string"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a JWT for the current user.", "response": "def createJWT(self, payload={}, expiredSeconds=3600):\n        \"\"\" \u751f\u6210token: https://tools.ietf.org/html/rfc7519\n        @param payload dict: \u81ea\u5b9a\u4e49\u516c\u6709\u6216\u79c1\u6709\u8f7d\u8377, \u5b58\u653e\u6709\u6548\u4fe1\u606f\u7684\u5730\u65b9;\n        @param expiredSeconds int: Token\u8fc7\u671f\u65f6\u95f4,\u5355\u4f4d\u79d2,\u7b7e\u53d1\u65f6\u95f4\u662f\u672c\u5730\u5f53\u524d\u65f6\u95f4\u6233,\u6b64\u53c2\u6570\u6307\u5b9a\u7b7e\u53d1\u65f6\u95f4\u4e4b\u540e\u591a\u5c11\u79d2\u8fc7\u671f;\n        \"\"\"\n        #1. check params\n        if isinstance(payload, dict):\n            for i in self._payloadkey:\n                if i in payload:\n                    raise KeyError(\"Standard key exists in payload\")\n        else:\n            raise TypeError(\"payload is not a dict\")\n\n        #2. predefined data\n        payload.update(self._payload)\n        payload.update(\n            exp=self.timestamp_after_timestamp(self.get_current_timestamp(), seconds=expiredSeconds),\n            iat=self.get_current_timestamp()\n        )\n\n        #3. base64 urlsafe encode\n        #\u5934\u90e8\u7f16\u7801\n        first_part  = base64.urlsafe_b64encode(json.dumps(self._header, sort_keys=True, separators=(',', ':')))\n        #\u8f7d\u8377\u6d88\u606f\u4f53\u7f16\u7801\n        second_part = base64.urlsafe_b64encode(json.dumps(payload, sort_keys=True, separators=(',', ':')))\n        #\u7b7e\u540d\u4ee5\u4e0a\u4e24\u90e8\u5206: \u628aheader\u3001playload\u7684base64url\u7f16\u7801\u52a0\u5bc6\u540e\u518d\u6b21base64\u7f16\u7801\n        third_part  = base64.urlsafe_b64encode(self.signatureJWT(\"{0}.{1}\".format(first_part, second_part)))\n\n        #4. returns the available token\n        token = first_part + '.' + second_part + '.' + third_part\n        logging.info(\"Generating token ok\")\n        return token"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef analysisJWT(self, token):\n        _header, _payload, _signature = token.split(\".\")\n        data = {\n            \"header\": json.loads(base64.urlsafe_b64decode(str(_header))),\n            \"payload\": json.loads(base64.urlsafe_b64decode(str(_payload))),\n            \"signature\": base64.urlsafe_b64decode(str(_signature))\n        }\n        logging.debug(\"analysis token: {0}\".format(data))\n        return data", "response": "analysisJWT - returns a dictionary of the data that can be used to create a new object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if the UrlAddr is in a valid format for example :: http://ip : port", "response": "def __isValidUrl(self, addr):\r\n        \"\"\"Check if the UrlAddr is in a valid format, for example::\r\n\r\n            http://ip:port\r\n            https://abc.com\r\n        \"\"\"\r\n        regex = re.compile(\r\n            r'^(?:http)s?://'  #: http:// or https://\r\n            r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+(?:[A-Z]{2,6}\\.?|[A-Z0-9-]{2,}\\.?)|'  #: domain...\r\n            r'localhost|'  #: localhost...\r\n            r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})'  #: ...or ip\r\n            r'(?::\\d+)?'  #: optional port\r\n            r'(?:/?|[/?]\\S+)$', re.IGNORECASE)\r\n        if addr and isinstance(addr, string_types):\r\n            if regex.match(addr):\r\n                return True\r\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __isValidTGZ(self, suffix):\r\n        if suffix and isinstance(suffix, string_types):\r\n            if suffix.endswith(\".tar.gz\") or suffix.endswith(\".tgz\"):\r\n                return True\r\n        return False", "response": "To determine whether the suffix. tar. gz or. tgz format is valid"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __isValidZIP(self, suffix):\r\n        if suffix and isinstance(suffix, string_types):\r\n            if suffix.endswith(\".zip\"):\r\n                return True\r\n        return False", "response": "Determine if the suffix is. zip format"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef __isValidFilename(self, filename):\r\n        if filename and isinstance(filename, string_types):\r\n            if re.match(r'^[\\w\\d\\_\\-\\.]+$', filename, re.I):\r\n                if self.__isValidTGZ(filename) or self.__isValidZIP(filename):\r\n                    return True\r\n        return False", "response": "Determine whether the filename is valid"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the filename from different scenarios in the filename and scene value see remote_download in 1 2 3 4", "response": "def __getFilename(self, data, scene=1):\r\n        \"\"\"To get the data from different scenarios in the filename, scene value see `remote_download` in 1, 2, 3, 4\"\"\"\r\n        try:\r\n            filename = None\r\n            if scene == 1:\r\n                plugin_filename = [i for i in parse_qs(urlsplit(data).query).get(\"plugin_filename\") or [] if i]\r\n                if plugin_filename and len(plugin_filename) == 1:\r\n                    filename = plugin_filename[0]\r\n            elif scene == 2:\r\n                filename = basename(urlsplit(data).path)\r\n            elif scene == 3:\r\n                if PY2:\r\n                    cd = data.headers.getheader(\"Content-Disposition\", \"\")\r\n                else:\r\n                    cd = data.getheader(\"Content-Disposition\", \"\")\r\n                filename = parse_header(cd)[-1].get(\"filename\")\r\n            elif scene == 4:\r\n                if PY2:\r\n                    cd = data.info().subtype\r\n                else:\r\n                    cd = data.info().get_content_subtype()\r\n                mt = {'zip': 'zip', 'x-compressed-tar': 'tar.gz', 'x-gzip': 'tar.gz'}\r\n                subtype = mt.get(cd)\r\n                if subtype:\r\n                    filename = \".\" + subtype\r\n        except Exception as e:\r\n            self.logger.warning(e)\r\n        else:\r\n            if self.__isValidFilename(filename):\r\n                return filename"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the filename suffix", "response": "def __getFilenameSuffix(self, filename):\r\n        \"\"\"Gets the filename suffix\"\"\"\r\n        if filename and isinstance(filename, string_types):\r\n            if self.__isValidTGZ(filename):\r\n                return \".tar.gz\"\r\n            elif filename.endswith(\".zip\"):\r\n                return \".zip\""}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef __unpack_tgz(self, filename):\r\n        if isinstance(filename, string_types) and self.__isValidTGZ(filename) and tarfile.is_tarfile(filename):\r\n            with tarfile.open(filename, mode='r:gz') as t:\r\n                for name in t.getnames():\r\n                    t.extract(name, self.plugin_abspath)\r\n        else:\r\n            raise TarError(\"Invalid Plugin Compressed File\")", "response": "Unpack the tar. gz tgz compressed file format"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef __unpack_zip(self, filename):\r\n        if isinstance(filename, string_types) and self.__isValidZIP(filename) and zipfile.is_zipfile(filename):\r\n            with zipfile.ZipFile(filename) as z:\r\n                for name in z.namelist():\r\n                    z.extract(name, self.plugin_abspath)\r\n        else:\r\n            raise ZipError(\"Invalid Plugin Compressed File\")", "response": "Unpack the zip compressed file format"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndownload the remote plugin package.", "response": "def _remote_download(self, url):\r\n        \"\"\"To download the remote plugin package,\r\n        there are four methods of setting filename according to priority,\r\n        each of which stops setting when a qualified filename is obtained,\r\n        and an exception is triggered when a qualified valid filename is ultimately unavailable.\r\n\r\n        1. Add url `plugin_filename` query parameters\r\n        2. The file name is resolved in the url, eg: http://xx.xx.com/plugin-v0.0.1.tar.gz\r\n        3. Parse the Content-Disposition in the return header\r\n        4. Parse the Content-Type in the return header\r\n        \"\"\"\r\n        #: Try to set filename in advance based on the previous two steps\r\n        if self.__isValidUrl(url):\r\n            filename = self.__getFilename(url, scene=1)\r\n            if not filename:\r\n                filename = self.__getFilename(url, scene=2)\r\n            #: fix UnboundLocalError\r\n            f = None\r\n            try:\r\n                f = urllib2.urlopen(url, timeout=10)\r\n            except (AttributeError, ValueError, urllib2.URLError):\r\n                raise InstallError(\"Open URL Error\")\r\n            else:\r\n                if not filename:\r\n                    filename = self.__getFilename(f, scene=3)\r\n                    if not filename:\r\n                        filename = self.__getFilename(f, scene=4)\r\n                if filename and self.__isValidFilename(filename):\r\n                    suffix = self.__getFilenameSuffix(filename)\r\n                    with NamedTemporaryFile(mode='w+b', prefix='fpk-', suffix=suffix, delete=False) as fp:\r\n                        fp.write(f.read())\r\n                        filename = fp.name\r\n                    try:\r\n                        self.__unpack_tgz(filename) if self.__isValidTGZ(suffix) else self.__unpack_zip(filename)\r\n                    finally:\r\n                        os.remove(filename)\r\n                else:\r\n                    raise InstallError(\"Invalid Filename\")\r\n            finally:\r\n                if f is not None:\r\n                    f.close()\r\n        else:\r\n            raise InstallError(\"Invalid URL\")"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _local_upload(self, filepath, remove=False):\r\n        if os.path.isfile(filepath):\r\n            filename = os.path.basename(os.path.abspath(filepath))\r\n            if filename and self.__isValidFilename(filename):\r\n                suffix = self.__getFilenameSuffix(filename)\r\n                try:\r\n                    self.__unpack_tgz(os.path.abspath(filepath)) if self.__isValidTGZ(suffix) else self.__unpack_zip(os.path.abspath(filepath))\r\n                finally:\r\n                    if remove is True:\r\n                        os.remove(filepath)\r\n            else:\r\n                raise InstallError(\"Invalid Filename\")\r\n        else:\r\n            raise InstallError(\"Invalid Filepath\")", "response": "Local plugin package processing"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds a plugin to the list of available source code packages.", "response": "def addPlugin(self, method=\"remote\", **kwargs):\r\n        \"\"\"Add a plugin, support only for `.tar.gz` or `.zip` compression packages.\r\n\r\n        :param method:\r\n            `remote`, download and unpack a remote plugin package;\r\n            `local`, unzip a local plugin package.\r\n\r\n        :param url: str: for method is remote, plugin can be downloaded from the address.\r\n\r\n        :param filepath: str: for method is local, plugin local absolute path\r\n\r\n        :param remove: Boolean: for method is local, remove the plugin source code package, default is False.\r\n\r\n        :returns: dict: add the result of the plugin, like dict(msg=str, code=int), code=0 is successful.\r\n        \"\"\"\r\n        res = dict(code=1, msg=None)\r\n        try:\r\n            if method == \"remote\":\r\n                self._remote_download(kwargs[\"url\"])\r\n            elif method == \"local\":\r\n                self._local_upload(kwargs[\"filepath\"], kwargs.get(\"remove\", False))\r\n            else:\r\n                res.update(msg=\"Invalid method\")\r\n        except Exception as e:\r\n            res.update(msg=str(e))\r\n        else:\r\n            res.update(code=0)\r\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef removePlugin(self, package):\r\n        res = dict(code=1, msg=None)\r\n        if package and isinstance(package, string_types):\r\n            path = os.path.join(self.plugin_abspath, package)\r\n            if os.path.isdir(path):\r\n                try:\r\n                    shutil.rmtree(path)\r\n                except Exception as e:\r\n                    res.update(msg=str(e))\r\n                else:\r\n                    res.update(code=0)\r\n            else:\r\n                res.update(msg=\"No Such Package\")\r\n        else:\r\n            res.update(msg=\"Invalid Package Format\")\r\n        return res", "response": "Remove a plugin from the list of available plugins."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef post(self):\n        #1.\n        username = request.form.get(\"username\")\n        password = request.form.get(\"password\")\n        exipres  = 7200\n        #expire time(seconds)\n        if username and password:\n            _authRes = self._getAuthentication(username, password)\n        else:\n            return {\"msg\": \"invalid username or password\"}\n        #2.\n        if _authRes:\n            _payload = self._getUserData(username)\n            #3.\n            try:\n                token = self.jwt.createJWT(_payload, expiredSeconds=exipres)\n            except JWTException:\n            \treturn {\"msg\": \"Failed to request token\"}\n            else:\n                return {\"token\": token}\n        else:\n            return {\"msg\": \"Authentication failed\"}", "response": "POST request to redis"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef isValidSemver(version):\n    if version and isinstance(version, string_types):\n        try:\n            semver.parse(version)\n        except (TypeError,ValueError):\n            return False\n        else:\n            return True\n    return False", "response": "Returns True if the version number is a valid semantic version number."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef open(self, flag=\"c\"):\n        return shelve.open(os.path.join(gettempdir(), self.index), flag=flag, protocol=2)", "response": "Open a new file handle for reading the cache entries."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset persistent data with shelve.", "response": "def set(self, key, value):\n        \"\"\"Set persistent data with shelve.\n\n        :param key: string: Index key\n\n        :param value: All supported data types in python\n\n        :raises:\n\n        :returns:\n        \"\"\"\n        db = self.open()\n        try:\n            db[key] = value\n        finally:\n            db.close()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nflattening an irregular sequence.", "response": "def flatten_list(items, seqtypes=(list, tuple), in_place=True):\n    \"\"\"Flatten an irregular sequence.\n\n    Works generally but may be slower than it could\n    be if you can make assumptions about your list.\n\n    `Source`__\n\n    __ https://stackoverflow.com/a/10824086\n\n    Parameters\n    ----------\n    items : iterable\n        The irregular sequence to flatten.\n    seqtypes : iterable of types (optional)\n        Types to flatten. Default is (list, tuple).\n    in_place : boolean (optional)\n        Toggle in_place flattening. Default is True.\n\n    Returns\n    -------\n    list\n        Flattened list.\n\n    Examples\n    --------\n    >>> l = [[[1, 2, 3], [4, 5]], 6]\n    >>> wt.kit.flatten_list(l)\n    [1, 2, 3, 4, 5, 6]\n    \"\"\"\n    if not in_place:\n        items = items[:]\n    for i, _ in enumerate(items):\n        while i < len(items) and isinstance(items[i], seqtypes):\n            items[i : i + 1] = items[i]\n    return items"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef intersperse(lis, value):\n    out = [value] * (len(lis) * 2 - 1)\n    out[0::2] = lis\n    return out", "response": "Put value between each existing item in list lis and value."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind the index of an item given either the item or index as an argument.", "response": "def get_index(lis, argument):\n    \"\"\"Find the index of an item, given either the item or index as an argument.\n\n    Particularly useful as a wrapper for arguments like channel or axis.\n\n    Parameters\n    ----------\n    lis : list\n        List to parse.\n    argument : int or object\n        Argument.\n\n    Returns\n    -------\n    int\n        Index of chosen object.\n    \"\"\"\n    # get channel\n    if isinstance(argument, int):\n        if -len(lis) <= argument < len(lis):\n            return argument\n        else:\n            raise IndexError(\"index {0} incompatible with length {1}\".format(argument, len(lis)))\n    else:\n        return lis.index(argument)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _title(fig, title, subtitle=\"\", *, margin=1, fontsize=20, subfontsize=18):\n    fig.suptitle(title, fontsize=fontsize)\n    height = fig.get_figheight()  # inches\n    distance = margin / 2.  # distance from top of plot, in inches\n    ratio = 1 - distance / height\n    fig.text(0.5, ratio, subtitle, fontsize=subfontsize, ha=\"center\", va=\"top\")", "response": "Add a title to a figure."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a sideplot to an axis.", "response": "def add_sideplot(\n    ax,\n    along,\n    pad=0.,\n    *,\n    grid=True,\n    zero_line=True,\n    arrs_to_bin=None,\n    normalize_bin=True,\n    ymin=0,\n    ymax=1.1,\n    height=0.75,\n    c=\"C0\"\n):\n    \"\"\"Add a sideplot to an axis. Sideplots share their corresponding axis.\n\n    Parameters\n    ----------\n    ax : matplotlib AxesSubplot object\n        The axis to add a sideplot along.\n    along : {'x', 'y'}\n        The dimension to add a sideplot along.\n    pad : number (optional)\n        Distance between axis and sideplot. Default is 0.\n    grid : bool (optional)\n        Toggle for plotting grid on sideplot. Default is True.\n    zero_line : bool (optional)\n        Toggle for plotting black line at zero signal. Default is True.\n    arrs_to_bin : list [xi, yi, zi] (optional)\n        Bins are plotted if arrays are supplied. Default is None.\n    normalize_bin : bool (optional)\n        Normalize bin by max value. Default is True.\n    ymin : number (optional)\n        Bin minimum extent. Default is 0.\n    ymax : number (optional)\n        Bin maximum extent. Default is 1.1\n    c : string (optional)\n        Line color. Default is C0.\n\n    Returns\n    -------\n    axCorr\n        AxesSubplot object\n    \"\"\"\n    # divider should only be created once\n    if hasattr(ax, \"WrightTools_sideplot_divider\"):\n        divider = ax.WrightTools_sideplot_divider\n    else:\n        divider = make_axes_locatable(ax)\n        setattr(ax, \"WrightTools_sideplot_divider\", divider)\n    # create sideplot axis\n    if along == \"x\":\n        axCorr = divider.append_axes(\"top\", height, pad=pad, sharex=ax)\n    elif along == \"y\":\n        axCorr = divider.append_axes(\"right\", height, pad=pad, sharey=ax)\n    axCorr.autoscale(False)\n    axCorr.set_adjustable(\"box\")\n    # bin\n    if arrs_to_bin is not None:\n        xi, yi, zi = arrs_to_bin\n        if along == \"x\":\n            b = np.nansum(zi, axis=0) * len(yi)\n            if normalize_bin:\n                b /= np.nanmax(b)\n            axCorr.plot(xi, b, c=c, lw=2)\n        elif along == \"y\":\n            b = np.nansum(zi, axis=1) * len(xi)\n            if normalize_bin:\n                b /= np.nanmax(b)\n            axCorr.plot(b, yi, c=c, lw=2)\n    # beautify\n    if along == \"x\":\n        axCorr.set_ylim(ymin, ymax)\n        axCorr.tick_params(axis=\"x\", which=\"both\", length=0)\n    elif along == \"y\":\n        axCorr.set_xlim(ymin, ymax)\n        axCorr.tick_params(axis=\"y\", which=\"both\", length=0)\n    plt.grid(grid)\n    if zero_line:\n        if along == \"x\":\n            plt.axhline(0, c=\"k\", lw=1)\n        elif along == \"y\":\n            plt.axvline(0, c=\"k\", lw=1)\n    plt.setp(axCorr.get_xticklabels(), visible=False)\n    plt.setp(axCorr.get_yticklabels(), visible=False)\n    return axCorr"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef corner_text(\n    text,\n    distance=0.075,\n    *,\n    ax=None,\n    corner=\"UL\",\n    factor=200,\n    bbox=True,\n    fontsize=18,\n    background_alpha=1,\n    edgecolor=None\n):\n    \"\"\"Place some text in the corner of the figure.\n\n    Parameters\n    ----------\n    text : str\n        The text to use.\n    distance : number (optional)\n        Distance from the corner. Default is 0.05.\n    ax : axis (optional)\n        The axis object to label. If None, uses current axis. Default is None.\n    corner : {'UL', 'LL', 'UR', 'LR'} (optional)\n        The corner to label. Upper left, Lower left etc. Default is UL.\n    factor : number (optional)\n        Scaling factor. Default is 200.\n    bbox : boolean (optional)\n        Toggle bounding box. Default is True.\n    fontsize : number (optional)\n        Text fontsize. If None, uses the matplotlib default. Default is 18.\n    background_alpha : number (optional)\n        Opacity of background bounding box. Default is 1.\n    edgecolor : string (optional)\n        Frame edgecolor. Default is None (inherits from legend.edgecolor\n        rcparam).\n\n    Returns\n    -------\n    text\n        The matplotlib text object.\n    \"\"\"\n    # get axis\n    if ax is None:\n        ax = plt.gca()\n    [h_scaled, v_scaled], [va, ha] = get_scaled_bounds(\n        ax, corner, distance=distance, factor=factor\n    )\n    # get edgecolor\n    if edgecolor is None:\n        edgecolor = matplotlib.rcParams[\"legend.edgecolor\"]\n    # apply text\n    props = dict(boxstyle=\"square\", facecolor=\"white\", alpha=background_alpha, edgecolor=edgecolor)\n    args = [v_scaled, h_scaled, text]\n    kwargs = {}\n    kwargs[\"fontsize\"] = fontsize\n    kwargs[\"verticalalignment\"] = va\n    kwargs[\"horizontalalignment\"] = ha\n    if bbox:\n        kwargs[\"bbox\"] = props\n    else:\n        kwargs[\"path_effects\"] = [PathEffects.withStroke(linewidth=3, foreground=\"w\")]\n    kwargs[\"transform\"] = ax.transAxes\n    if \"zlabel\" in ax.properties().keys():  # axis is 3D projection\n        out = ax.text2D(*args, **kwargs)\n    else:\n        out = ax.text(*args, **kwargs)\n    return out", "response": "Place some text in the corner of the figure."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a matplotlib figure that can be used to plot the base figure of the base figure.", "response": "def create_figure(\n    *,\n    width=\"single\",\n    nrows=1,\n    cols=[1],\n    margin=1.,\n    hspace=0.25,\n    wspace=0.25,\n    cbar_width=0.25,\n    aspects=[],\n    default_aspect=1\n):\n    \"\"\"Re-parameterization of matplotlib figure creation tools, exposing convenient variables.\n\n    Figures are defined primarily by their width. Height is defined by the\n    aspect ratios of the subplots contained within. hspace, wspace, and\n    cbar_width are defined in inches, making it easier to make consistent\n    plots. Margins are enforced to be equal around the entire plot, starting\n    from the edges of the subplots.\n\n    Parameters\n    ----------\n    width : {'single', 'double', 'dissertation'} or float (optional)\n        The total width of the generated figure. Can be given in inches\n        directly, or can be specified using keys. Default is 'single' (6.5\n        inches).\n    nrows : int (optional)\n        The number of subplot rows in the figure. Default is 1.\n    cols : list (optional)\n        A list of numbers, defining the number and width-ratios of the\n        figure columns. May also contain the special string 'cbar', defining\n        a column as a colorbar-containing column. Default is [1].\n    margin : float (optional)\n        Margin in inches. Margin is applied evenly around the figure, starting\n        from the subplot boundaries (so that ticks and labels appear in the\n        margin). Default is 1.\n    hspace : float (optional)\n        The 'height space' (space seperating two subplots vertically), in\n        inches. Default is 0.25.\n    wspace : float (optional)\n        The 'width space' (space seperating two subplots horizontally), in\n        inches. Default is 0.25.\n    cbar_width : float (optional)\n        The width of the colorbar in inches. Default is 0.25.\n    aspects : list of lists (optional)\n        Define the aspect ratio of individual subplots. List of lists, each\n        sub-ist having the format [[row, col], aspect]. The figure will expand\n        vertically to acomidate the defined aspect ratio. Aspects are V/H so\n        aspects larger than 1 will be taller than wide and vice-versa for\n        aspects smaller than 1. You may only define the aspect for one subplot\n        in each row. If no aspect is defined for a particular row, the leftmost\n        subplot will have an aspect of ``default_aspect``. Default is given by\n        default_aspect kwarg.\n    default_aspect : number (optional)\n        Default aspect of left-most column, if no aspect is defined for a\n        given row.\n\n    Returns\n    -------\n    tuple\n        (WrightTools.artists.Figure, WrightTools.artists.GridSpec). GridSpec\n        contains SubplotSpec objects that can have axes placed into them.\n        The SubplotSpec objects can be accessed through indexing: [row, col].\n        Slicing works, for example ``cax = plt.subplot(gs[:, -1])``. See\n        matplotlib gridspec__ documentation for more information.\n\n        __ http://matplotlib.org/users/gridspec.html#gridspec-and-subplotspec\n\n\n    Notes\n    -----\n    To ensure the margins work as expected, save the fig with\n    the same margins (``pad_inches``) as specified in this function. Common\n    savefig call:\n    ``plt.savefig(plt.savefig(output_path, dpi=300, transparent=True,\n    pad_inches=1))``\n\n    See Also\n    --------\n    wt.artists.plot_margins\n        Plot lines to visualize the figure edges, margins, and centers. For\n        debug and design purposes.\n    wt.artists.subplots_adjust\n        Enforce margins for figure generated elsewhere.\n\n    \"\"\"\n    # get width\n    if width == \"double\":\n        figure_width = 14.\n    elif width == \"single\":\n        figure_width = 6.5\n    elif width == \"dissertation\":\n        figure_width = 13.\n    else:\n        figure_width = float(width)\n    # check if aspect constraints are valid\n    rows_in_aspects = [item[0][0] for item in aspects]\n    if not len(rows_in_aspects) == len(set(rows_in_aspects)):\n        raise Exception(\"can only specify aspect for one plot in each row\")\n    # get width avalible to subplots (not including colorbars)\n    total_subplot_width = figure_width - 2 * margin\n    total_subplot_width -= (len(cols) - 1) * wspace  # whitespace in width\n    total_subplot_width -= cols.count(\"cbar\") * cbar_width  # colorbar width\n    # converters\n\n    def in_to_mpl(inches, total, n):\n        return (inches * n) / (total - inches * n + inches)\n\n    def mpl_to_in(mpl, total, n):\n        return (total / (n + mpl * (n - 1))) * mpl\n\n    # calculate column widths, width_ratio\n    subplot_ratios = np.array([i for i in cols if not i == \"cbar\"], dtype=np.float)\n    subplot_ratios /= sum(subplot_ratios)\n    subplot_widths = total_subplot_width * subplot_ratios\n    width_ratios = []\n    cols_idxs = []\n    i = 0\n    for col in cols:\n        if not col == \"cbar\":\n            width_ratios.append(subplot_widths[i])\n            cols_idxs.append(i)\n            i += 1\n        else:\n            width_ratios.append(cbar_width)\n            cols_idxs.append(np.nan)\n    # calculate figure height, height_ratios, figure height\n    subplot_heights = []\n    for row_index in range(nrows):\n        if row_index in rows_in_aspects:\n            aspect = aspects[rows_in_aspects.index(row_index)][1]\n            col_index = aspects[rows_in_aspects.index(row_index)][0][1]\n            height = subplot_widths[col_index] * aspect\n            subplot_heights.append(height)\n        else:\n            # make the leftmost (zero indexed) plot square as default\n            subplot_heights.append(subplot_widths[0] * default_aspect)\n    height_ratios = subplot_heights\n    figure_height = sum(subplot_heights)\n    figure_height += (nrows - 1) * hspace\n    figure_height += 2 * margin\n    # make figure\n    fig = plt.figure(figsize=[figure_width, figure_height], FigureClass=Figure)\n    # get hspace, wspace in relative units\n    hspace = in_to_mpl(hspace, figure_height - 2 * margin, nrows)\n    wspace = in_to_mpl(wspace, figure_width - 2 * margin, len(cols))\n    # make gridpsec\n    gs = GridSpec(\n        nrows,\n        len(cols),\n        hspace=hspace,\n        wspace=wspace,\n        width_ratios=width_ratios,\n        height_ratios=height_ratios,\n    )\n    # finish\n    subplots_adjust(fig, inches=margin)\n    return fig, gs"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef diagonal_line(xi=None, yi=None, *, ax=None, c=None, ls=None, lw=None, zorder=3):\n    if ax is None:\n        ax = plt.gca()\n    # parse xi, yi\n    if xi is None:\n        xi = ax.get_xlim()\n    if yi is None:\n        yi = ax.get_ylim()\n    # parse style\n    if c is None:\n        c = matplotlib.rcParams[\"grid.color\"]\n    if ls is None:\n        ls = matplotlib.rcParams[\"grid.linestyle\"]\n    if lw is None:\n        lw = matplotlib.rcParams[\"grid.linewidth\"]\n    # get axis\n    if ax is None:\n        ax = plt.gca()\n    # make plot\n    diag_min = max(min(xi), min(yi))\n    diag_max = min(max(xi), max(yi))\n    line = ax.plot([diag_min, diag_max], [diag_min, diag_max], c=c, ls=ls, lw=lw, zorder=zorder)\n    return line", "response": "Plot a diagonal line."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_scaled_bounds(ax, position, *, distance=0.1, factor=200):\n    # get bounds\n    x0, y0, width, height = ax.bbox.bounds\n    width_scaled = width / factor\n    height_scaled = height / factor\n    # get scaled postions\n    if position == \"UL\":\n        v_scaled = distance / width_scaled\n        h_scaled = 1 - (distance / height_scaled)\n        va = \"top\"\n        ha = \"left\"\n    elif position == \"LL\":\n        v_scaled = distance / width_scaled\n        h_scaled = distance / height_scaled\n        va = \"bottom\"\n        ha = \"left\"\n    elif position == \"UR\":\n        v_scaled = 1 - (distance / width_scaled)\n        h_scaled = 1 - (distance / height_scaled)\n        va = \"top\"\n        ha = \"right\"\n    elif position == \"LR\":\n        v_scaled = 1 - (distance / width_scaled)\n        h_scaled = distance / height_scaled\n        va = \"bottom\"\n        ha = \"right\"\n    else:\n        print(\"corner not recognized\")\n        v_scaled = h_scaled = 1.\n        va = \"center\"\n        ha = \"center\"\n    return [h_scaled, v_scaled], [va, ha]", "response": "Get the scaled bounds of a node."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npreparing a set of arrays for plotting using matplotlib. pcolor.", "response": "def pcolor_helper(xi, yi, zi=None):\n    \"\"\"Prepare a set of arrays for plotting using `pcolor`.\n\n    The return values are suitable for feeding directly into ``matplotlib.pcolor``\n    such that the pixels are properly centered.\n\n    Parameters\n    ----------\n    xi : 1D or 2D array-like\n        Array of X-coordinates.\n    yi : 1D or 2D array-like\n        Array of Y-coordinates.\n    zi : 2D array (optional, deprecated)\n        If zi is not None, it is returned unchanged in the output.\n\n    Returns\n    -------\n    X : 2D ndarray\n        X dimension for pcolor\n    Y : 2D ndarray\n        Y dimension for pcolor\n    zi : 2D ndarray\n        if zi parameter is not None, returns zi parameter unchanged\n    \"\"\"\n    xi = xi.copy()\n    yi = yi.copy()\n    if xi.ndim == 1:\n        xi.shape = (xi.size, 1)\n    if yi.ndim == 1:\n        yi.shape = (1, yi.size)\n    shape = wt_kit.joint_shape(xi, yi)\n\n    # full\n    def full(arr):\n        for i in range(arr.ndim):\n            if arr.shape[i] == 1:\n                arr = np.repeat(arr, shape[i], axis=i)\n        return arr\n\n    xi = full(xi)\n    yi = full(yi)\n    # pad\n    x = np.arange(shape[1])\n    y = np.arange(shape[0])\n    f_xi = interp2d(x, y, xi)\n    f_yi = interp2d(x, y, yi)\n    x_new = np.arange(-1, shape[1] + 1)\n    y_new = np.arange(-1, shape[0] + 1)\n    xi = f_xi(x_new, y_new)\n    yi = f_yi(x_new, y_new)\n    # fill\n    X = np.empty([s - 1 for s in xi.shape])\n    Y = np.empty([s - 1 for s in yi.shape])\n    for orig, out in [[xi, X], [yi, Y]]:\n        for idx in np.ndindex(out.shape):\n            ul = orig[idx[0] + 1, idx[1] + 0]\n            ur = orig[idx[0] + 1, idx[1] + 1]\n            ll = orig[idx[0] + 0, idx[1] + 0]\n            lr = orig[idx[0] + 0, idx[1] + 1]\n            out[idx] = np.mean([ul, ur, ll, lr])\n    if zi is not None:\n        warnings.warn(\n            \"zi argument is not used in pcolor_helper and is not required\",\n            wt_exceptions.VisibleDeprecationWarning,\n        )\n        return X, Y, zi.copy()\n    else:\n        return X, Y"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nplots a colorbar on a matplotlib axis.", "response": "def plot_colorbar(\n    cax=None,\n    cmap=\"default\",\n    ticks=None,\n    clim=None,\n    vlim=None,\n    label=None,\n    tick_fontsize=14,\n    label_fontsize=18,\n    decimals=None,\n    orientation=\"vertical\",\n    ticklocation=\"auto\",\n    extend=\"neither\",\n    extendfrac=None,\n    extendrect=False,\n):\n    \"\"\"Easily add a colormap to an axis.\n\n    Parameters\n    ----------\n    cax : matplotlib axis (optional)\n        The axis to plot the colorbar on. Finds the current axis if none is\n        given.\n    cmap : string or LinearSegmentedColormap (optional)\n        The colormap to fill the colorbar with. Strings map as keys to the\n        WrightTools colormaps dictionary. Default is `default`.\n    ticks : 1D array-like (optional)\n        Ticks. Default is None.\n    clim : two element list (optional)\n        The true limits of the colorbar, in the same units as ticks. If None,\n        streaches the colorbar over the limits of ticks. Default is None.\n    vlim : two element list-like (optional)\n        The limits of the displayed colorbar, in the same units as ticks. If\n        None, displays over clim. Default is None.\n    label : str (optional)\n        Label. Default is None.\n    tick_fontsize : number (optional)\n        Fontsize. Default is 14.\n    label_fontsize : number (optional)\n        Label fontsize. Default is 18.\n    decimals : integer (optional)\n        Number of decimals to appear in tick labels. Default is None (best guess).\n    orientation : {'vertical', 'horizontal'} (optional)\n        Colorbar orientation. Default is vertical.\n    ticklocation : {'auto', 'left', 'right', 'top', 'bottom'} (optional)\n        Tick location. Default is auto.\n    extend : {\u2018neither\u2019, \u2018both\u2019, \u2018min\u2019, \u2018max\u2019} (optional)\n        If not \u2018neither\u2019, make pointed end(s) for out-of- range values.\n        These are set for a given colormap using the colormap set_under and set_over methods.\n    extendfrac :\t{None, \u2018auto\u2019, length, lengths} (optional)\n        If set to None, both the minimum and maximum triangular colorbar extensions\n        have a length of 5% of the interior colorbar length (this is the default setting).\n        If set to \u2018auto\u2019, makes the triangular colorbar extensions the same lengths\n        as the interior boxes\n        (when spacing is set to \u2018uniform\u2019) or the same lengths as the respective adjacent\n        interior boxes (when spacing is set to \u2018proportional\u2019).\n        If a scalar, indicates the length of both the minimum and maximum triangular\n        colorbar extensions as a fraction of the interior colorbar length.\n        A two-element sequence of fractions may also be given, indicating the lengths\n        of the minimum and maximum colorbar extensions respectively as a fraction\n        of the interior colorbar length.\n    extendrect :\tbool (optional)\n        If False the minimum and maximum colorbar extensions will be triangular (the default).\n        If True the extensions will be rectangular.\n\n    Returns\n    -------\n    matplotlib.colorbar.ColorbarBase object\n        The created colorbar.\n    \"\"\"\n    # parse cax\n    if cax is None:\n        cax = plt.gca()\n    # parse cmap\n    if isinstance(cmap, str):\n        cmap = colormaps[cmap]\n    # parse ticks\n    if ticks is None:\n        ticks = np.linspace(0, 1, 11)\n    # parse clim\n    if clim is None:\n        clim = [min(ticks), max(ticks)]\n    # parse clim\n    if vlim is None:\n        vlim = clim\n    if max(vlim) == min(vlim):\n        vlim[-1] += 1e-1\n    # parse format\n    if isinstance(decimals, int):\n        format = \"%.{0}f\".format(decimals)\n    else:\n        magnitude = int(np.log10(max(vlim) - min(vlim)) - 0.99)\n        if 1 > magnitude > -3:\n            format = \"%.{0}f\".format(-magnitude + 1)\n        elif magnitude in (1, 2, 3):\n            format = \"%i\"\n        else:\n            # scientific notation\n            def fmt(x, _):\n                return \"%.1f\" % (x / float(10 ** magnitude))\n\n            format = matplotlib.ticker.FuncFormatter(fmt)\n            magnitude_label = r\"  $\\mathsf{\\times 10^{%d}}$\" % magnitude\n            if label is None:\n                label = magnitude_label\n            else:\n                label = \" \".join([label, magnitude_label])\n    # make cbar\n    norm = matplotlib.colors.Normalize(vmin=vlim[0], vmax=vlim[1])\n    cbar = matplotlib.colorbar.ColorbarBase(\n        ax=cax,\n        cmap=cmap,\n        norm=norm,\n        ticks=ticks,\n        orientation=orientation,\n        ticklocation=ticklocation,\n        format=format,\n        extend=extend,\n        extendfrac=extendfrac,\n        extendrect=extendrect,\n    )\n    # coerce properties\n    cbar.set_clim(clim[0], clim[1])\n    cbar.ax.tick_params(labelsize=tick_fontsize)\n    if label:\n        cbar.set_label(label, fontsize=label_fontsize)\n    # finish\n    return cbar"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nplots the margins of the current figure.", "response": "def plot_margins(*, fig=None, inches=1., centers=True, edges=True):\n    \"\"\"Add lines onto a figure indicating the margins, centers, and edges.\n\n    Useful for ensuring your figure design scripts work as intended, and for laying\n    out figures.\n\n    Parameters\n    ----------\n    fig : matplotlib.figure.Figure object (optional)\n        The figure to plot onto. If None, gets current figure. Default is None.\n    inches : float (optional)\n        The size of the figure margin, in inches. Default is 1.\n    centers : bool (optional)\n        Toggle for plotting lines indicating the figure center. Default is\n        True.\n    edges : bool (optional)\n        Toggle for plotting lines indicating the figure edges. Default is True.\n    \"\"\"\n    if fig is None:\n        fig = plt.gcf()\n    size = fig.get_size_inches()  # [H, V]\n    trans_vert = inches / size[0]\n    left = matplotlib.lines.Line2D(\n        [trans_vert, trans_vert], [0, 1], transform=fig.transFigure, figure=fig\n    )\n    right = matplotlib.lines.Line2D(\n        [1 - trans_vert, 1 - trans_vert], [0, 1], transform=fig.transFigure, figure=fig\n    )\n    trans_horz = inches / size[1]\n    bottom = matplotlib.lines.Line2D(\n        [0, 1], [trans_horz, trans_horz], transform=fig.transFigure, figure=fig\n    )\n    top = matplotlib.lines.Line2D(\n        [0, 1], [1 - trans_horz, 1 - trans_horz], transform=fig.transFigure, figure=fig\n    )\n    fig.lines.extend([left, right, bottom, top])\n    if centers:\n        vert = matplotlib.lines.Line2D(\n            [0.5, 0.5], [0, 1], transform=fig.transFigure, figure=fig, c=\"r\"\n        )\n        horiz = matplotlib.lines.Line2D(\n            [0, 1], [0.5, 0.5], transform=fig.transFigure, figure=fig, c=\"r\"\n        )\n        fig.lines.extend([vert, horiz])\n    if edges:\n        left = matplotlib.lines.Line2D(\n            [0, 0], [0, 1], transform=fig.transFigure, figure=fig, c=\"k\"\n        )\n        right = matplotlib.lines.Line2D(\n            [1, 1], [0, 1], transform=fig.transFigure, figure=fig, c=\"k\"\n        )\n        bottom = matplotlib.lines.Line2D(\n            [0, 1], [0, 0], transform=fig.transFigure, figure=fig, c=\"k\"\n        )\n        top = matplotlib.lines.Line2D([0, 1], [1, 1], transform=fig.transFigure, figure=fig, c=\"k\")\n        fig.lines.extend([left, right, bottom, top])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef plot_gridlines(ax=None, c=\"grey\", lw=1, diagonal=False, zorder=2, makegrid=True):\n    # get ax\n    if ax is None:\n        ax = plt.gca()\n    ax.grid()\n    # get dashes\n    ls = \":\"\n    dashes = (lw / 2, lw)\n    # grid\n    # ax.grid(True)\n    lines = ax.xaxis.get_gridlines() + ax.yaxis.get_gridlines()\n    for line in lines.copy():\n        line.set_linestyle(\":\")\n        line.set_color(c)\n        line.set_linewidth(lw)\n        line.set_zorder(zorder)\n        line.set_dashes(dashes)\n        ax.add_line(line)\n    # diagonal\n    if diagonal:\n        min_xi, max_xi = ax.get_xlim()\n        min_yi, max_yi = ax.get_ylim()\n        diag_min = max(min_xi, min_yi)\n        diag_max = min(max_xi, max_yi)\n        ax.plot(\n            [diag_min, diag_max],\n            [diag_min, diag_max],\n            c=c,\n            ls=ls,\n            lw=lw,\n            zorder=zorder,\n            dashes=dashes,\n        )\n\n        # Plot resets xlim and ylim sometimes for unknown reasons.\n        # This is here to ensure that the xlim and ylim are unchanged\n        # after adding a diagonal, whose limits are calculated so\n        # as to not change the xlim and ylim.\n        #           -- KFS 2017-09-26\n        ax.set_ylim(min_yi, max_yi)\n        ax.set_xlim(min_xi, max_xi)", "response": "Plot dotted gridlines onto an axis."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsave a figure. Parameters ---------- path : str Path to save figure at. fig : matplotlib.figure.Figure object (optional) The figure to plot onto. If None, gets current figure. Default is None. close : bool (optional) Toggle closing of figure after saving. Default is True. Returns ------- str The full path where the figure was saved.", "response": "def savefig(path, fig=None, close=True, *, dpi=300):\n    \"\"\"Save a figure.\n\n    Parameters\n    ----------\n    path : str\n        Path to save figure at.\n    fig : matplotlib.figure.Figure object (optional)\n        The figure to plot onto. If None, gets current figure. Default is None.\n    close : bool (optional)\n        Toggle closing of figure after saving. Default is True.\n\n    Returns\n    -------\n    str\n        The full path where the figure was saved.\n    \"\"\"\n    # get fig\n    if fig is None:\n        fig = plt.gcf()\n    # get full path\n    path = os.path.abspath(path)\n    # save\n    plt.savefig(path, dpi=dpi, transparent=False, pad_inches=1, facecolor=\"none\")\n    # close\n    if close:\n        plt.close(fig)\n    # finish\n    return path"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_ax_labels(ax=None, xlabel=None, ylabel=None, xticks=None, yticks=None, label_fontsize=18):\n    # get ax\n    if ax is None:\n        ax = plt.gca()\n    # x\n    if xlabel is not None:\n        ax.set_xlabel(xlabel, fontsize=label_fontsize)\n    if xticks is not None:\n        if isinstance(xticks, bool):\n            plt.setp(ax.get_xticklabels(), visible=xticks)\n            if not xticks:\n                ax.tick_params(axis=\"x\", which=\"both\", length=0)\n        else:\n            ax.set_xticks(xticks)\n    # y\n    if ylabel is not None:\n        ax.set_ylabel(ylabel, fontsize=label_fontsize)\n    if yticks is not None:\n        if isinstance(yticks, bool):\n            plt.setp(ax.get_yticklabels(), visible=yticks)\n            if not yticks:\n                ax.tick_params(axis=\"y\", which=\"both\", length=0)\n        else:\n            ax.set_yticks(yticks)", "response": "Set all axis labels easily."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets all axis labels of a base figure simultaniously.", "response": "def set_fig_labels(\n    fig=None,\n    xlabel=None,\n    ylabel=None,\n    xticks=None,\n    yticks=None,\n    title=None,\n    row=-1,\n    col=0,\n    label_fontsize=18,\n    title_fontsize=20,\n):\n    \"\"\"Set all axis labels of a figure simultaniously.\n\n    Only plots ticks and labels for edge axes.\n\n    Parameters\n    ----------\n    fig : matplotlib.figure.Figure object (optional)\n        Figure to set labels of. If None, uses current figure. Default is None.\n    xlabel : None or string (optional)\n        x axis label. Default is None.\n    ylabel : None or string (optional)\n        y axis label. Default is None.\n    xticks : None or False or list of numbers (optional)\n        xticks. If False, ticks are hidden. Default is None.\n    yticks : None or False or list of numbers (optional)\n        yticks. If False, ticks are hidden. Default is None.\n    title : None or string (optional)\n        Title of figure. Default is None.\n    row : integer or slice (optional)\n        Row to label. Default is -1. If slice, step is ignored.\n    col : integer or slice (optional)\n        col to label. Default is 0. If slice, step is ignored.\n    label_fontsize : number (optional)\n        Fontsize of label. Default is 18.\n    title_fontsize : number (optional)\n        Fontsize of title. Default is 20.\n\n    See Also\n    --------\n    set_ax_labels\n    \"\"\"\n    # get fig\n    if fig is None:\n        fig = plt.gcf()\n    # interpret row\n    numRows = fig.axes[0].numRows\n    if isinstance(row, int):\n        row %= numRows\n        row = slice(0, row)\n    row_start, row_stop, _ = row.indices(numRows)\n    # interpret col\n    numCols = fig.axes[0].numCols\n    if isinstance(col, int):\n        col %= numCols\n        col = slice(col, -1)\n    col_start, col_stop, _ = col.indices(numCols)\n    # axes\n    for ax in fig.axes:\n        if ax.is_sideplot:\n            continue\n        if row_start <= ax.rowNum <= row_stop and col_start <= ax.colNum <= col_stop:\n            if ax.colNum == col_start:\n                set_ax_labels(ax=ax, ylabel=ylabel, yticks=yticks, label_fontsize=label_fontsize)\n            else:\n                set_ax_labels(ax=ax, ylabel=\"\", yticks=False)\n            if ax.rowNum == row_stop:\n                set_ax_labels(ax=ax, xlabel=xlabel, xticks=xticks, label_fontsize=label_fontsize)\n            else:\n                set_ax_labels(ax=ax, xlabel=\"\", xticks=False)\n    # title\n    if title is not None:\n        fig.suptitle(title, fontsize=title_fontsize)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef subplots_adjust(fig=None, inches=1):\n    if fig is None:\n        fig = plt.gcf()\n    size = fig.get_size_inches()\n    vert = inches / size[0]\n    horz = inches / size[1]\n    fig.subplots_adjust(vert, horz, 1 - vert, 1 - horz)", "response": "Adjusts the margins of the figure to be equal around the figure."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nstitch a series of images into an animation.", "response": "def stitch_to_animation(images, outpath=None, *, duration=0.5, palettesize=256, verbose=True):\n    \"\"\"Stitch a series of images into an animation.\n\n    Currently supports animated gifs, other formats coming as needed.\n\n    Parameters\n    ----------\n    images : list of strings\n        Filepaths to the images to stitch together, in order of apperence.\n    outpath : string (optional)\n        Path of output, including extension. If None, bases output path on path\n        of first path in `images`. Default is None.\n    duration : number or list of numbers (optional)\n        Duration of (each) frame in seconds. Default is 0.5.\n    palettesize : int (optional)\n        The number of colors in the resulting animation. Input is rounded to\n        the nearest power of 2. Default is 1024.\n    verbose : bool (optional)\n        Toggle talkback. Default is True.\n    \"\"\"\n    # parse filename\n    if outpath is None:\n        outpath = os.path.splitext(images[0])[0] + \".gif\"\n    # write\n    t = wt_kit.Timer(verbose=False)\n    with t, imageio.get_writer(\n        outpath, mode=\"I\", duration=duration, palettesize=palettesize\n    ) as writer:\n        for p in images:\n            image = imageio.imread(p)\n            writer.append_data(image)\n    # finish\n    if verbose:\n        interval = np.round(t.interval, 2)\n        print(\"gif generated in {0} seconds - saved at {1}\".format(interval, outpath))\n    return outpath"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a data object from a single PyCMDS output file.", "response": "def from_PyCMDS(filepath, name=None, parent=None, verbose=True) -> Data:\n    \"\"\"Create a data object from a single PyCMDS output file.\n\n    Parameters\n    ----------\n    filepath : path-like\n        Path to the .data file\n        Can be either a local or remote file (http/ftp).\n        Can be compressed with gz/bz2, decompression based on file name.\n    name : str or None (optional)\n        The name to be applied to the new data object. If None, name is read\n        from file.\n    parent : WrightTools.Collection (optional)\n        Collection to place new data object within. Default is None.\n    verbose : bool (optional)\n        Toggle talkback. Default is True.\n\n    Returns\n    -------\n    data\n        A Data instance.\n    \"\"\"\n    filestr = os.fspath(filepath)\n    filepath = pathlib.Path(filepath)\n\n    # header\n    ds = np.DataSource(None)\n    file_ = ds.open(filestr, \"rt\")\n    headers = tidy_headers.read(file_)\n    file_.seek(0)\n    # name\n    if name is None:  # name not given in method arguments\n        data_name = headers[\"data name\"]\n    else:\n        data_name = name\n    if data_name == \"\":  # name not given in PyCMDS\n        data_name = headers[\"data origin\"]\n    # create data object\n    kwargs = {\n        \"name\": data_name,\n        \"kind\": \"PyCMDS\",\n        \"source\": filestr,\n        \"created\": headers[\"file created\"],\n    }\n    if parent is not None:\n        data = parent.create_data(**kwargs)\n    else:\n        data = Data(**kwargs)\n    # array\n    arr = np.genfromtxt(file_).T\n    file_.close()\n    # get axes and scanned variables\n    axes = []\n    for name, identity, units in zip(\n        headers[\"axis names\"], headers[\"axis identities\"], headers[\"axis units\"]\n    ):\n        # points and centers\n        points = np.array(headers[name + \" points\"])\n        if name + \" centers\" in headers.keys():\n            centers = headers[name + \" centers\"]\n        else:\n            centers = None\n        # create\n        axis = {\n            \"points\": points,\n            \"units\": units,\n            \"name\": name,\n            \"identity\": identity,\n            \"centers\": centers,\n        }\n        axes.append(axis)\n    shape = tuple([a[\"points\"].size for a in axes])\n    for i, ax in enumerate(axes):\n        sh = [1] * len(shape)\n        sh[i] = len(ax[\"points\"])\n        data.create_variable(\n            name=ax[\"name\"] + \"_points\", values=np.array(ax[\"points\"]).reshape(sh)\n        )\n        if ax[\"centers\"] is not None:\n            sh = list(shape)\n            sh[i] = 1\n            data.create_variable(\n                name=ax[\"name\"] + \"_centers\", values=np.array(ax[\"centers\"]).reshape(sh)\n            )\n    # get assorted remaining things\n    # variables and channels\n    try:\n        signed = iter(headers[\"channel signed\"])\n    except KeyError:\n        signed = itertools.repeat(False)\n    for index, kind, name in zip(range(len(arr)), headers[\"kind\"], headers[\"name\"]):\n        values = np.full(np.prod(shape), np.nan)\n        values[: len(arr[index])] = arr[index]\n        values.shape = shape\n        if name == \"time\":\n            data.create_variable(name=\"labtime\", values=values)\n        if kind == \"hardware\":\n            # sadly, recorded tolerances are not reliable\n            # so a bit of hard-coded hacking is needed\n            # if this ends up being too fragile, we might have to use the points arrays\n            # ---Blaise 2018-01-09\n            units = headers[\"units\"][index]\n            label = headers[\"label\"][index]\n            if (\n                \"w\" in name\n                and name.startswith(tuple(data.variable_names))\n                and name not in headers[\"axis names\"]\n            ):\n                inherited_shape = data[name.split(\"_\")[0]].shape\n                for i, s in enumerate(inherited_shape):\n                    if s == 1:\n                        values = np.mean(values, axis=i)\n                        values = np.expand_dims(values, i)\n            else:\n                tolerance = headers[\"tolerance\"][index]\n                units = headers[\"units\"][index]\n                for i in range(len(shape)):\n                    if tolerance is None:\n                        break\n                    if \"d\" in name:\n                        # This is a hack because delay is particularly\n                        # unreliable in tolerance. And 3 fs vs 3 ps is a huge\n                        # difference... KFS 2019-2-27\n                        if units == \"fs\":\n                            tolerance = 3.\n                        else:\n                            tolerance = 0.1\n                    if \"zero\" in name:\n                        tolerance = 1e-10\n                    try:\n                        assert i == headers[\"axis names\"].index(name)\n                        tolerance = 0\n                    except (ValueError, AssertionError):\n                        if (\n                            name in headers[\"axis names\"]\n                            and \"%s_centers\" % name not in data.variable_names\n                        ):\n                            tolerance = np.inf\n                    mean = np.nanmean(values, axis=i)\n                    mean = np.expand_dims(mean, i)\n                    values, meanexp = wt_kit.share_nans(values, mean)\n                    if np.allclose(meanexp, values, atol=tolerance, equal_nan=True):\n                        values = mean\n            if name in headers[\"axis names\"]:\n                points = np.array(headers[name + \" points\"])\n                pointsshape = [1] * values.ndim\n                for i, ax in enumerate(axes):\n                    if ax[\"name\"] == name:\n                        pointsshape[i] = len(points)\n                        break\n                points.shape = pointsshape\n                points = wt_units.converter(points, headers[\"axis units\"][i], units)\n                for i in range(points.ndim):\n                    if points.shape[i] == 1:\n                        points = np.repeat(points, values.shape[i], axis=i)\n                if points.size <= values.size:\n                    values[np.isnan(values)] = points[np.isnan(values)]\n            data.create_variable(name, values=values, units=units, label=label)\n        if kind == \"channel\":\n            data.create_channel(name=name, values=values, shape=values.shape, signed=next(signed))\n    # axes\n    for a in axes:\n        expression = a[\"identity\"]\n        if expression.startswith(\"D\"):\n            expression = expression[1:]\n        expression.replace(\"=D\", \"=\")\n        a[\"expression\"] = expression\n    data.transform(*[a[\"expression\"] for a in axes])\n    for a, u in zip(data.axes, headers[\"axis units\"]):\n        if u is not None:\n            a.convert(u)\n    if (\n        headers[\"system name\"] == \"fs\"\n        and int(headers[\"PyCMDS version\"].split(\".\")[0]) == 0\n        and int(headers[\"PyCMDS version\"].split(\".\")[1]) < 10\n    ):\n        # in versions of PyCMDS up to (and including) 0.9.0\n        # there was an incorrect hard-coded conversion factor between mm and fs\n        # this ONLY applied to Newport MFA stages\n        # we apply this correction knowing that Newport MFAs were only used on the \"fs\" system\n        # and knowing that the Newport MFAs were always assigned as \"d1\", \"d2\" and \"d3\"\n        # ---Blaise 2019-04-09\n        for delay in (\"d1\", \"d2\", \"d3\", \"d1_points\", \"d2_points\", \"d3_points\"):\n            if delay not in data.variable_names:\n                continue\n            data[delay][:] *= 6000.671281903963041 / 6671.281903963041\n            if verbose:\n                print(f\"Correction factor applied to {delay}\")\n    # return\n    if verbose:\n        print(\"data created at {0}\".format(data.fullpath))\n        print(\"  axes: {0}\".format(data.axis_names))\n        print(\"  shape: {0}\".format(data.shape))\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef leastsqfitter(p0, datax, datay, function, verbose=False, cov_verbose=False):\n    timer = Timer(verbose=False)\n    with timer:\n        # define error function\n        def errfunc(p, x, y):\n            return y - function(p, x)\n\n        # run optimization\n        pfit_leastsq, pcov, infodict, errmsg, success = scipy_optimize.leastsq(\n            errfunc, p0, args=(datax, datay), full_output=1, epsfcn=0.0001\n        )\n        # calculate covarience matrix\n        # original idea https://stackoverflow.com/a/21844726\n        if (len(datay) > len(p0)) and pcov is not None:\n            s_sq = (errfunc(pfit_leastsq, datax, datay) ** 2).sum() / (len(datay) - len(p0))\n            pcov = pcov * s_sq\n            if cov_verbose:\n                print(pcov)\n        else:\n            pcov = np.array(np.inf)\n        # calculate and write errors\n        error = []\n        for i in range(len(pfit_leastsq)):\n            try:\n                error.append(np.absolute(pcov[i][i]) ** 0.5)\n            except IndexError:\n                error.append(0.00)\n        perr_leastsq = np.array(error)\n    # exit\n    if verbose:\n        print(\"fit params:       \", pfit_leastsq)\n        print(\"fit params error: \", perr_leastsq)\n        print(\"fitting done in %f seconds\" % timer.interval)\n    return pfit_leastsq, perr_leastsq", "response": "Conveniently call scipy. optmize. leastsq."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the parent object of this instance.", "response": "def parent(self):\n        \"\"\"Parent.\"\"\"\n        try:\n            assert self._parent is not None\n        except (AssertionError, AttributeError):\n            key = posixpath.dirname(self.fullpath)\n            if key.endswith(\"::\"):\n                key += posixpath.sep\n            self._parent = Group._instances[key]\n        finally:\n            return self._parent"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncloses the file that contains the Group.", "response": "def close(self):\n        \"\"\"Close the file that contains the Group.\n\n        All groups which are in the file will be closed and removed from the\n        _instances dictionaries.\n        Tempfiles, if they exist, will be removed\n        \"\"\"\n        from .collection import Collection\n        from .data._data import Channel, Data, Variable\n\n        path = os.path.abspath(self.filepath) + \"::\"\n        for key in list(Group._instances.keys()):\n            if key.startswith(path):\n                Group._instances.pop(key, None)\n\n        if self.fid.valid > 0:\n            # for some reason, the following file operations sometimes fail\n            # this stops execution of the method, meaning that the tempfile is never removed\n            # the following try case ensures that the tempfile code is always executed\n            # ---Blaise 2018-01-08\n            try:\n                self.file.flush()\n                try:\n                    # Obtaining the file descriptor must be done prior to closing\n                    fd = self.fid.get_vfd_handle()\n                except (NotImplementedError, ValueError):\n                    # only available with certain h5py drivers\n                    # not needed if not available\n                    pass\n\n                self.file.close()\n                try:\n                    if fd:\n                        os.close(fd)\n                except OSError:\n                    # File already closed, e.g.\n                    pass\n            except SystemError as e:\n                warnings.warn(\"SystemError: {0}\".format(e))\n            finally:\n                if hasattr(self, \"_tmpfile\"):\n                    os.close(self._tmpfile[0])\n                    os.remove(self._tmpfile[1])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef copy(self, parent=None, name=None, verbose=True):\n        if name is None:\n            name = self.natural_name\n        if parent is None:\n            from ._open import open as wt_open  # circular import\n\n            new = Group()  # root of new tempfile\n            # attrs\n            new.attrs.update(self.attrs)\n            new.natural_name = name\n            # children\n            for k, v in self.items():\n                super().copy(v, new, name=v.natural_name)\n            new.flush()\n            p = new.filepath\n            new = wt_open(p)\n        else:\n            # copy\n            self.file.copy(self.name, parent, name=name)\n            new = parent[name]\n        # finish\n        if verbose:\n            print(\"{0} copied to {1}\".format(self.fullpath, new.fullpath))\n        return new", "response": "Create a copy of this object under parent."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsave as root of a new file.", "response": "def save(self, filepath=None, overwrite=False, verbose=True):\n        \"\"\"Save as root of a new file.\n\n        Parameters\n        ----------\n        filepath : Path-like object (optional)\n            Filepath to write. If None, file is created using natural_name.\n        overwrite : boolean (optional)\n            Toggle overwrite behavior. Default is False.\n        verbose : boolean (optional)\n            Toggle talkback. Default is True\n\n        Returns\n        -------\n        str\n            Written filepath.\n        \"\"\"\n        if filepath is None:\n            filepath = pathlib.Path(\".\") / self.natural_name\n        else:\n            filepath = pathlib.Path(filepath)\n        filepath = filepath.with_suffix(\".wt5\")\n        filepath = filepath.absolute().expanduser()\n        if filepath.exists():\n            if overwrite:\n                filepath.unlink()\n            else:\n                raise wt_exceptions.FileExistsError(filepath)\n\n        # copy to new file\n        h5py.File(filepath)\n        new = Group(filepath=filepath, edit_local=True)\n        # attrs\n        for k, v in self.attrs.items():\n            new.attrs[k] = v\n        # children\n        for k, v in self.items():\n            super().copy(v, new, name=v.natural_name)\n        # finish\n        new.flush()\n        new.close()\n        del new\n        if verbose:\n            print(\"file saved at\", filepath)\n        return str(filepath)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_JASCO(filepath, name=None, parent=None, verbose=True) -> Data:\n    # parse filepath\n    filestr = os.fspath(filepath)\n    filepath = pathlib.Path(filepath)\n\n    if not \".txt\" in filepath.suffixes:\n        wt_exceptions.WrongFileTypeWarning.warn(filepath, \".txt\")\n    # parse name\n    if not name:\n        name = filepath.name.split(\".\")[0]\n    # create data\n    kwargs = {\"name\": name, \"kind\": \"JASCO\", \"source\": filestr}\n    if parent is None:\n        data = Data(**kwargs)\n    else:\n        data = parent.create_data(**kwargs)\n    # array\n    ds = np.DataSource(None)\n    f = ds.open(filestr, \"rt\")\n    arr = np.genfromtxt(f, skip_header=18).T\n    f.close()\n\n    # chew through all scans\n    data.create_variable(name=\"energy\", values=arr[0], units=\"nm\")\n    data.create_channel(name=\"signal\", values=arr[1])\n    data.transform(\"energy\")\n    # finish\n    if verbose:\n        print(\"data created at {0}\".format(data.fullpath))\n        print(\"  range: {0} to {1} (nm)\".format(data.energy[0], data.energy[-1]))\n        print(\"  size: {0}\".format(data.size))\n    return data", "response": "Create a data object from JASCO UV - Vis spectrometers."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef make_cubehelix(name=\"WrightTools\", gamma=0.5, s=0.25, r=-1, h=1.3, reverse=False, darkest=0.7):\n    rr = .213 / .30\n    rg = .715 / .99\n    rb = .072 / .11\n\n    def get_color_function(p0, p1):\n        def color(x):\n            # Calculate amplitude and angle of deviation from the black to\n            # white diagonal in the plane of constant perceived intensity.\n            xg = darkest * x ** gamma\n            lum = 1 - xg  # starts at 1\n            if reverse:\n                lum = lum[::-1]\n            a = lum.copy()\n            a[lum < 0.5] = h * lum[lum < 0.5] / 2.\n            a[lum >= 0.5] = h * (1 - lum[lum >= 0.5]) / 2.\n            phi = 2 * np.pi * (s / 3 + r * x)\n            out = lum + a * (p0 * np.cos(phi) + p1 * np.sin(phi))\n            return out\n\n        return color\n\n    rgb_dict = {\n        \"red\": get_color_function(-0.14861 * rr, 1.78277 * rr),\n        \"green\": get_color_function(-0.29227 * rg, -0.90649 * rg),\n        \"blue\": get_color_function(1.97294 * rb, 0.0),\n    }\n    cmap = matplotlib.colors.LinearSegmentedColormap(name, rgb_dict)\n    return cmap", "response": "Define a cubehelix for the given color factor."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef make_colormap(seq, name=\"CustomMap\", plot=False):\n    seq = [(None,) * 3, 0.0] + list(seq) + [1.0, (None,) * 3]\n    cdict = {\"red\": [], \"green\": [], \"blue\": []}\n    for i, item in enumerate(seq):\n        if isinstance(item, float):\n            r1, g1, b1 = seq[i - 1]\n            r2, g2, b2 = seq[i + 1]\n            cdict[\"red\"].append([item, r1, r2])\n            cdict[\"green\"].append([item, g1, g2])\n            cdict[\"blue\"].append([item, b1, b2])\n    cmap = mplcolors.LinearSegmentedColormap(name, cdict)\n    if plot:\n        plot_colormap_components(cmap)\n    return cmap", "response": "Generates a LinearSegmentedColormap for the given sequence of tuples."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef nm_to_rgb(nm):\n    w = int(nm)\n    # color ---------------------------------------------------------------------------------------\n    if w >= 380 and w < 440:\n        R = -(w - 440.) / (440. - 350.)\n        G = 0.0\n        B = 1.0\n    elif w >= 440 and w < 490:\n        R = 0.0\n        G = (w - 440.) / (490. - 440.)\n        B = 1.0\n    elif w >= 490 and w < 510:\n        R = 0.0\n        G = 1.0\n        B = -(w - 510.) / (510. - 490.)\n    elif w >= 510 and w < 580:\n        R = (w - 510.) / (580. - 510.)\n        G = 1.0\n        B = 0.0\n    elif w >= 580 and w < 645:\n        R = 1.0\n        G = -(w - 645.) / (645. - 580.)\n        B = 0.0\n    elif w >= 645 and w <= 780:\n        R = 1.0\n        G = 0.0\n        B = 0.0\n    else:\n        R = 0.0\n        G = 0.0\n        B = 0.0\n    # intensity correction ------------------------------------------------------------------------\n    if w >= 380 and w < 420:\n        SSS = 0.3 + 0.7 * (w - 350) / (420 - 350)\n    elif w >= 420 and w <= 700:\n        SSS = 1.0\n    elif w > 700 and w <= 780:\n        SSS = 0.3 + 0.7 * (780 - w) / (780 - 700)\n    else:\n        SSS = 0.0\n    SSS *= 255\n    return [float(int(SSS * R) / 256.), float(int(SSS * G) / 256.), float(int(SSS * B) / 256.)]", "response": "Convert a wavelength to corresponding RGB values [ 0. 0 - 1. 0"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef plot_colormap_components(cmap):\n    from ._helpers import set_ax_labels  # recursive import protection\n\n    plt.figure(figsize=[8, 4])\n    gs = grd.GridSpec(3, 1, height_ratios=[1, 10, 1], hspace=0.05)\n    # colorbar\n    ax = plt.subplot(gs[0])\n    gradient = np.linspace(0, 1, 256)\n    gradient = np.vstack((gradient, gradient))\n    ax.imshow(gradient, aspect=\"auto\", cmap=cmap, vmin=0., vmax=1.)\n    ax.set_title(cmap.name, fontsize=20)\n    ax.set_axis_off()\n    # components\n    ax = plt.subplot(gs[1])\n    x = np.arange(cmap.N)\n    colors = cmap(x)\n    r = colors[:, 0]\n    g = colors[:, 1]\n    b = colors[:, 2]\n    RGB_weight = [0.299, 0.587, 0.114]\n    k = np.sqrt(np.dot(colors[:, :3] ** 2, RGB_weight))\n    r.clip(0, 1, out=r)\n    g.clip(0, 1, out=g)\n    b.clip(0, 1, out=b)\n    xi = np.linspace(0, 1, x.size)\n    plt.plot(xi, r, \"r\", linewidth=5, alpha=0.6)\n    plt.plot(xi, g, \"g\", linewidth=5, alpha=0.6)\n    plt.plot(xi, b, \"b\", linewidth=5, alpha=0.6)\n    plt.plot(xi, k, \"k\", linewidth=5, alpha=0.6)\n    ax.set_xlim(0, 1)\n    ax.set_ylim(-0.1, 1.1)\n    set_ax_labels(ax=ax, xlabel=None, xticks=False, ylabel=\"intensity\")\n    # grayified colorbar\n    cmap = grayify_cmap(cmap)\n    ax = plt.subplot(gs[2])\n    gradient = np.linspace(0, 1, 256)\n    gradient = np.vstack((gradient, gradient))\n    ax.imshow(gradient, aspect=\"auto\", cmap=cmap, vmin=0., vmax=1.)\n    ax.set_axis_off()", "response": "Plot the components of a given colormap."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef grayify_cmap(cmap):\n    cmap = plt.cm.get_cmap(cmap)\n    colors = cmap(np.arange(cmap.N))\n    # convert RGBA to perceived greyscale luminance\n    # cf. http://alienryderflex.com/hsp.html\n    RGB_weight = [0.299, 0.587, 0.114]\n    luminance = np.sqrt(np.dot(colors[:, :3] ** 2, RGB_weight))\n    colors[:, :3] = luminance[:, np.newaxis]\n    return mplcolors.LinearSegmentedColormap.from_list(cmap.name + \"_grayscale\", colors, cmap.N)", "response": "Return a grayscale version of the colormap."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_color_cycle(n, cmap=\"rainbow\", rotations=3):\n    cmap = colormaps[cmap]\n    if np.mod(n, rotations) == 0:\n        per = np.floor_divide(n, rotations)\n    else:\n        per = np.floor_divide(n, rotations) + 1\n    vals = list(np.linspace(0, 1, per))\n    vals = vals * rotations\n    vals = vals[:n]\n    out = cmap(vals)\n    return out", "response": "Returns a list of RGBA colors following a colormap."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlabeling the six time - orderings in a three - pulse experiment.", "response": "def label_sectors(\n    *,\n    labels=[\"I\", \"II\", \"IV\", \"VI\", \"V\", \"III\"],\n    ax=None,\n    lw=2,\n    lc=\"k\",\n    cs=None,\n    c_zlevel=2,\n    c_alpha=0.5,\n    fontsize=40\n):\n    \"\"\"Label the six time-orderings in a three-pulse experiment.\n\n    Parameters\n    ----------\n    labels : list of strings\n        Labels to place within sectors, starting in the upper left and\n        proceeding clockwise. Default is ['I', 'II', 'IV', 'VI', 'V', 'III'].\n    ax : matplotlib axis object (optional)\n        Axis to label. If None, uses current axis. Default is None.\n    cs : list of matplotlib colors (optional)\n        Color to label sectors. If None, sectors are not colored. Default is\n        None.\n    c_zlevel : number (optional)\n        Matplotlib zlevel of color. Default is 2.\n    c_alpha : number between 0 and 1.\n        Transparency of color. Default is 0.5\n    \"\"\"\n    if ax is None:\n        ax = plt.gca()\n    # label\n    factors = [\n        [0.25, 0.75],\n        [2 / 3, 5 / 6],\n        [5 / 6, 2 / 3],\n        [0.75, 0.25],\n        [1 / 3, 1 / 6],\n        [1 / 6, 1 / 3],\n    ]\n    transform = ax.transAxes\n    for label, factor in zip(labels, factors):\n        ax.text(\n            *factor + [label], fontsize=fontsize, va=\"center\", ha=\"center\", transform=transform\n        )\n    # lines\n    if lw > 0:\n        ax.axhline(0, c=lc, lw=lw)\n        ax.axvline(0, c=lc, lw=lw)\n        ax.plot([0, 1], [0, 1], c=lc, lw=lw, transform=transform)\n    # colors\n    if cs is None:\n        cs = [\"none\"] * 6\n    xbound = ax.get_xbound()\n    ybound = ax.get_ybound()\n    factors = []\n    factors.append([[xbound[0], 0], [0, 0], [ybound[1], ybound[1]]])\n    factors.append([[0, xbound[1]], [0, ybound[1]], [ybound[1], ybound[1]]])\n    factors.append([[0, xbound[1]], [0, 0], [0, ybound[1]]])\n    factors.append([[0, xbound[1]], [ybound[0], ybound[0]], [0, 0]])\n    factors.append([[xbound[0], 0], [ybound[0], ybound[0]], [ybound[0], 0]])\n    factors.append([[xbound[0], 0], [ybound[0], 0], [0, 0]])\n    for color, factor in zip(cs, factors):\n        poly = ax.fill_between(*factor, facecolor=color, edgecolor=\"none\", alpha=c_alpha)\n        poly.set_zorder(c_zlevel)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates data object from KENT file.", "response": "def from_KENT(\n    filepaths,\n    name=None,\n    ignore=[\"wm\"],\n    delay_tolerance=0.1,\n    frequency_tolerance=0.5,\n    parent=None,\n    verbose=True,\n) -> Data:\n    \"\"\"Create data object from KENT file(s).\n\n    Parameters\n    ----------\n    filepaths : path-like or list of path-like\n        Filepath(s).\n        Can be either a local or remote file (http/ftp).\n        Can be compressed with gz/bz2, decompression based on file name.\n    name : string (optional)\n        Unique dataset identifier. If None (default), autogenerated.\n    ignore : list of strings (optional)\n        Columns to ignore. Default is ['wm'].\n    delay_tolerance : float (optional)\n        Tolerance below-which to ignore delay changes (in picoseconds).\n        Default is 0.1.\n    frequency_tolerance : float (optional)\n        Tolerance below-which to ignore frequency changes (in wavenumbers).\n        Default is 0.5.\n    parent : WrightTools.Collection (optional)\n        Collection to place new data object within. Default is None.\n    verbose : bool (optional)\n        Toggle talkback. Default is True.\n\n    Returns\n    -------\n    WrightTools.Data\n        Data from KENT.\n    \"\"\"\n    # define columns ------------------------------------------------------------------------------\n    # axes\n    axes = collections.OrderedDict()\n    axes[\"w1\"] = {\"units\": \"wn\", \"idx\": 0, \"label\": \"1\"}\n    axes[\"w2\"] = {\"units\": \"wn\", \"idx\": 1, \"label\": \"2\"}\n    axes[\"wm\"] = {\"units\": \"wn\", \"idx\": 2, \"label\": \"m\"}\n    axes[\"d1\"] = {\"units\": \"ps\", \"idx\": 3, \"label\": \"1\"}\n    axes[\"d2\"] = {\"units\": \"ps\", \"idx\": 4, \"label\": \"2\"}\n    for key in axes.keys():\n        if \"w\" in key:\n            axes[key][\"tolerance\"] = frequency_tolerance\n        elif \"d\" in key:\n            axes[key][\"tolerance\"] = delay_tolerance\n    # channels\n    channels = collections.OrderedDict()\n    channels[\"signal\"] = {\"idx\": 5}\n    channels[\"OPA1\"] = {\"idx\": 6}\n    channels[\"OPA2\"] = {\"idx\": 7}\n    # do we have a list of files or just one file? ------------------------------------------------\n    if not isinstance(filepaths, list):\n        filepaths = [filepaths]\n    filestrs = [os.fspath(f) for f in filepaths]\n    filepaths = [pathlib.Path(f) for f in filepaths]\n    # import full array ---------------------------------------------------------------------------\n    ds = np.DataSource(None)\n    arr = []\n    for f in filestrs:\n        ff = ds.open(f, \"rt\")\n        arr.append(np.genfromtxt(ff).T)\n        ff.close()\n    arr = np.concatenate(arr, axis=1)\n    # recognize dimensionality of data ------------------------------------------------------------\n    axes_discover = axes.copy()\n    for key in ignore:\n        if key in axes_discover:\n            axes_discover.pop(key)  # remove dimensions that mess up discovery\n    scanned = wt_kit.discover_dimensions(arr, axes_discover)\n    # create data object --------------------------------------------------------------------------\n    if name is None:\n        name = wt_kit.string2identifier(filepaths[0].name)\n    kwargs = {\"name\": name, \"kind\": \"KENT\", \"source\": filestrs}\n    if parent is not None:\n        data = parent.create_data(**kwargs)\n    else:\n        data = Data(**kwargs)\n    # grid and fill data --------------------------------------------------------------------------\n    # variables\n    ndim = len(scanned)\n    for i, key in enumerate(scanned.keys()):\n        for name in key.split(\"=\"):\n            shape = [1] * ndim\n            a = scanned[key]\n            shape[i] = a.size\n            a.shape = tuple(shape)\n            units = axes[name][\"units\"]\n            label = axes[name][\"label\"]\n            data.create_variable(name=name, values=a, units=units, label=label)\n    for key, dic in axes.items():\n        if key not in data.variable_names:\n            c = np.mean(arr[dic[\"idx\"]])\n            if not np.isnan(c):\n                shape = [1] * ndim\n                a = np.array([c])\n                a.shape = tuple(shape)\n                units = dic[\"units\"]\n                label = dic[\"label\"]\n                data.create_variable(name=key, values=a, units=units, label=label)\n    # channels\n    if len(scanned) == 1:  # 1D data\n        for key in channels.keys():\n            channel = channels[key]\n            zi = arr[channel[\"idx\"]]\n            data.create_channel(name=key, values=zi)\n    else:  # all other dimensionalities\n        # channels\n        points = tuple(arr[axes[key.split(\"=\")[0]][\"idx\"]] for key in scanned.keys())\n        xi = tuple(np.meshgrid(*scanned.values(), indexing=\"ij\"))\n        for key in channels.keys():\n            channel = channels[key]\n            zi = arr[channel[\"idx\"]]\n            fill_value = min(zi)\n            grid_i = griddata(points, zi, xi, method=\"linear\", fill_value=fill_value)\n            data.create_channel(name=key, values=grid_i)\n    # axes\n    data.transform(*scanned.keys())\n    # return --------------------------------------------------------------------------------------\n    if verbose:\n        print(\"data created at {0}\".format(data.fullpath))\n        print(\"  axes: {0}\".format(data.axis_names))\n        print(\"  shape: {0}\".format(data.shape))\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fluence(\n    power_mW,\n    color,\n    beam_radius,\n    reprate_Hz,\n    pulse_width,\n    color_units=\"wn\",\n    beam_radius_units=\"mm\",\n    pulse_width_units=\"fs_t\",\n    area_type=\"even\",\n) -> tuple:\n    \"\"\"Calculate the fluence of a beam.\n\n    Parameters\n    ----------\n    power_mW : number\n        Time integrated power of beam.\n    color : number\n        Color of beam in units.\n    beam_radius : number\n        Radius of beam in units.\n    reprate_Hz : number\n        Laser repetition rate in inverse seconds (Hz).\n    pulse_width : number\n        Pulsewidth of laser in units\n    color_units : string (optional)\n        Valid wt.units color unit identifier. Default is wn.\n    beam_radius_units : string (optional)\n        Valid wt.units distance unit identifier. Default is mm.\n    pulse_width_units : number\n        Valid wt.units time unit identifier. Default is fs.\n    area_type : string (optional)\n        Type of calculation to accomplish for Gaussian area.\n        even specfies a flat-top calculation\n        average specifies a Gaussian average within the FWHM\n        Default is even.\n\n    Returns\n    -------\n    tuple\n        Fluence in uj/cm2, photons/cm2, and peak intensity in GW/cm2\n\n    \"\"\"\n    # calculate beam area\n    if area_type == \"even\":\n        radius_cm = wt_units.converter(beam_radius, beam_radius_units, \"cm\")\n        area_cm2 = np.pi * radius_cm ** 2  # cm^2\n    elif area_type == \"average\":\n        radius_cm = wt_units.converter(beam_radius, beam_radius_units, \"cm\")\n        area_cm2 = np.pi * radius_cm ** 2  # cm^2\n        area_cm2 /= 0.7213  # weight by average intensity felt by oscillator inside of FWHM\n    else:\n        raise NotImplementedError\n    # calculate fluence in uj/cm^2\n    ujcm2 = power_mW / reprate_Hz  # mJ\n    ujcm2 *= 1e3  # uJ\n    ujcm2 /= area_cm2  # uJ/cm^2\n    # calculate fluence in photons/cm^2\n    energy = wt_units.converter(color, color_units, \"eV\")  # eV\n    photonscm2 = ujcm2 * 1e-6  # J/cm2\n    photonscm2 /= 1.60218e-19  # eV/cm2\n    photonscm2 /= energy  # photons/cm2\n    # calculate peak intensity in GW/cm^2\n    pulse_width_s = wt_units.converter(pulse_width, pulse_width_units, \"s_t\")  # seconds\n    GWcm2 = ujcm2 / 1e6  # J/cm2\n    GWcm2 /= pulse_width_s  # W/cm2\n    GWcm2 /= 1e9\n    # finish\n    return ujcm2, photonscm2, GWcm2", "response": "Calculate the fluence of a beam."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef mono_resolution(\n    grooves_per_mm, slit_width, focal_length, output_color, output_units=\"wn\"\n) -> float:\n    \"\"\"Calculate the resolution of a monochromator.\n\n    Parameters\n    ----------\n    grooves_per_mm : number\n        Grooves per millimeter.\n    slit_width : number\n        Slit width in microns.\n    focal_length : number\n        Focal length in mm.\n    output_color : number\n        Output color in nm.\n    output_units : string (optional)\n        Output units. Default is wn.\n\n    Returns\n    -------\n    float\n        Resolution.\n    \"\"\"\n    d_lambda = 1e6 * slit_width / (grooves_per_mm * focal_length)  # nm\n    upper = output_color + d_lambda / 2  # nm\n    lower = output_color - d_lambda / 2  # nm\n    return abs(\n        wt_units.converter(upper, \"nm\", output_units)\n        - wt_units.converter(lower, \"nm\", output_units)\n    )", "response": "Calculate the resolution of a monochromator."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef nm_width(center, width, units=\"wn\") -> float:\n    red = wt_units.converter(center - width / 2., units, \"nm\")\n    blue = wt_units.converter(center + width / 2., units, \"nm\")\n    return red - blue", "response": "Given a center and width in energy units get back a width in nm."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes the symmetric square root of x.", "response": "def symmetric_sqrt(x, out=None):\n    \"\"\"Compute the 'symmetric' square root: sign(x) * sqrt(abs(x)).\n\n    Parameters\n    ----------\n    x : array_like or number\n        Input array.\n    out : ndarray, None, or tuple of ndarray and None (optional)\n        A location into which the result is stored. If provided, it must\n        have a shape that the inputs broadcast to. If not provided or None,\n        a freshly-allocated array is returned. A tuple (possible only as a\n        keyword argument) must have length equal to the number of outputs.\n\n    Returns\n    -------\n    np.ndarray\n        Symmetric square root of arr.\n    \"\"\"\n    factor = np.sign(x)\n    out = np.sqrt(np.abs(x), out=out)\n    return out * factor"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd an arrow to the WMEL diagram.", "response": "def add_arrow(\n        self,\n        index,\n        between,\n        kind,\n        label=\"\",\n        head_length=0.1,\n        head_aspect=2,\n        font_size=14,\n        color=\"k\",\n    ):\n        \"\"\"Add an arrow to the WMEL diagram.\n\n        Parameters\n        ----------\n        index : integer\n            The interaction, or start and stop interaction for the arrow.\n        between : 2-element iterable of integers\n            The inital and final state of the arrow\n        kind : {'ket', 'bra'}\n            The kind of interaction.\n        label : string (optional)\n            Interaction label. Default is empty string.\n        head_length: number (optional)\n            size of arrow head\n        font_size : number (optional)\n            Label font size. Default is 14.\n        color : matplotlib color (optional)\n            Arrow color. Default is black.\n\n        Returns\n        -------\n        [line,arrow_head,text]\n        \"\"\"\n        if hasattr(index, \"index\"):\n            x_pos = list(index)\n        else:\n            x_pos = [index] * 2\n        x_pos = [np.linspace(0, 1, self.interactions)[i] for i in x_pos]\n\n        # calculate arrow length\n        arrow_length = self.energies[between[1]] - self.energies[between[0]]\n        arrow_end = self.energies[between[1]]\n        if arrow_length > 0:\n            direction = 1\n            y_poss = [self.energies[between[0]], self.energies[between[1]] - head_length]\n        elif arrow_length < 0:\n            direction = -1\n            y_poss = [self.energies[between[0]], self.energies[between[1]] + head_length]\n        else:\n            raise ValueError(\"between invalid!\")\n\n        length = abs(y_poss[0] - y_poss[1])\n        if kind == \"ket\":\n            line = self.ax.plot(x_pos, y_poss, linestyle=\"-\", color=color, linewidth=2, zorder=9)\n        elif kind == \"bra\":\n            line = self.ax.plot(x_pos, y_poss, linestyle=\"--\", color=color, linewidth=2, zorder=9)\n        elif kind == \"out\":\n            yi = np.linspace(y_poss[0], y_poss[1], 100)\n            xi = (\n                np.sin((yi - y_poss[0]) * int((1 / length) * 20) * 2 * np.pi * length) / 40\n                + x_pos[0]\n            )\n            line = self.ax.plot(\n                xi, yi, linestyle=\"-\", color=color, linewidth=2, solid_capstyle=\"butt\", zorder=9\n            )\n        else:\n            raise ValueError(\"kind is not 'ket', 'out', or 'bra'.\")\n        # add arrow head\n        arrow_head = self.ax.arrow(\n            x_pos[1],\n            arrow_end - head_length * direction,\n            0,\n            0.0001 * direction,\n            head_width=head_length * head_aspect,\n            head_length=head_length,\n            fc=color,\n            ec=color,\n            linestyle=\"solid\",\n            linewidth=0,\n            zorder=10,\n        )\n        # add text\n        text = self.ax.text(\n            np.mean(x_pos), -0.15, label, fontsize=font_size, horizontalalignment=\"center\"\n        )\n        return line, arrow_head, text"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlabels the rows of the log file.", "response": "def label_rows(self, labels, font_size=15, text_buffer=1.5):\n        \"\"\"Label rows.\n\n        Parameters\n        ----------\n        labels : list of strings\n            Labels.\n        font_size : number (optional)\n            Font size. Default is 15.\n        text_buffer : number\n            Buffer around text. Default is 1.5.\n        \"\"\"\n        for i in range(len(self.subplots)):\n            plot = self.subplots[i][-1]\n            plot.text(\n                text_buffer,\n                0.5,\n                labels[i],\n                fontsize=font_size,\n                verticalalignment=\"center\",\n                horizontalalignment=\"center\",\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nclears the region of the region of the region of the region.", "response": "def clear_diagram(self, diagram):\n        \"\"\"Clear diagram.\n\n        Parameters\n        ----------\n        diagram : [column, row]\n            Diagram to clear.\n        \"\"\"\n        plot = self.subplots[diagram[1]][diagram[0]]\n        plot.cla()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_arrow(\n        self, diagram, number, between, kind, label=\"\", head_length=0.075, font_size=7, color=\"k\"\n    ):\n        \"\"\"Add arrow.\n\n        Parameters\n        ----------\n        diagram : [column, row]\n            Diagram position.\n        number : integer\n            Arrow position.\n        between : [start, stop]\n            Arrow span.\n        kind : {'ket', 'bra', 'out'}\n            Arrow style.\n        label : string (optional)\n            Arrow label. Default is ''.\n        head_length : number (optional)\n            Arrow head length. Default 0.075.\n        font_size : number (optional)\n            Font size. Default is 7.\n        color : matplotlib color\n            Arrow color. Default is 'k'.\n\n        Returns\n        -------\n        list\n            [line, arrow_head, text]\n        \"\"\"\n        column, row = diagram\n        x_pos = self.x_pos[number]\n        # calculate arrow length\n        arrow_length = self.energies[between[1]] - self.energies[between[0]]\n        arrow_end = self.energies[between[1]]\n        if arrow_length > 0:\n            direction = 1\n            y_poss = [self.energies[between[0]], self.energies[between[1]] - head_length]\n        elif arrow_length < 0:\n            direction = -1\n            y_poss = [self.energies[between[0]], self.energies[between[1]] + head_length]\n        else:\n            raise ValueError(\"Variable between invalid\")\n        subplot = self.subplots[row][column]\n        # add line\n        length = abs(y_poss[0] - y_poss[1])\n        if kind == \"ket\":\n            line = subplot.plot([x_pos, x_pos], y_poss, linestyle=\"-\", color=color, linewidth=2)\n        elif kind == \"bra\":\n            line = subplot.plot([x_pos, x_pos], y_poss, linestyle=\"--\", color=color, linewidth=2)\n        elif kind == \"out\":\n            yi = np.linspace(y_poss[0], y_poss[1], 100)\n            xi = (\n                np.sin((yi - y_poss[0]) * int((1 / length) * 20) * 2 * np.pi * length) / 40 + x_pos\n            )\n            line = subplot.plot(\n                xi, yi, linestyle=\"-\", color=color, linewidth=2, solid_capstyle=\"butt\"\n            )\n        # add arrow head\n        arrow_head = subplot.arrow(\n            self.x_pos[number],\n            arrow_end - head_length * direction,\n            0,\n            0.0001 * direction,\n            head_width=head_length * 2,\n            head_length=head_length,\n            fc=color,\n            ec=color,\n            linestyle=\"solid\",\n            linewidth=0,\n        )\n        # add text\n        text = subplot.text(\n            self.x_pos[number], -0.1, label, fontsize=font_size, horizontalalignment=\"center\"\n        )\n        return line, arrow_head, text", "response": "Adds an arrow to the current object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef plot(self, save_path=None, close=False, bbox_inches=\"tight\", pad_inches=1):\n        # final manipulations\n        for plot in self.subplots.flatten():\n            # set limits\n            plot.set_xlim(-0.1, 1.1)\n            plot.set_ylim(-0.1, 1.1)\n            # remove guff\n            plot.axis(\"off\")\n        # save\n        if save_path:\n            plt.savefig(\n                save_path,\n                transparent=True,\n                dpi=300,\n                bbox_inches=bbox_inches,\n                pad_inches=pad_inches,\n            )\n        # close\n        if close:\n            plt.close()", "response": "Plot the current state of the object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_collection(self, name=\"collection\", position=None, **kwargs):\n        if name in self.item_names:\n            wt_exceptions.ObjectExistsWarning.warn(name)\n            return self[name]\n        collection = Collection(\n            filepath=self.filepath, parent=self.name, name=name, edit_local=True, **kwargs\n        )\n        if position is not None:\n            self.attrs[\"item_names\"] = np.insert(\n                self.attrs[\"item_names\"][:-1], position, collection.natural_name.encode()\n            )\n        setattr(self, name, collection)\n        return collection", "response": "Create a new child colleciton."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_data(self, name=\"data\", position=None, **kwargs):\n        if name in self.item_names:\n            wt_exceptions.ObjectExistsWarning.warn(name)\n            return self[name]\n\n        if name == \"\":\n            data = None\n            natural_name = \"\".encode()\n        else:\n            data = wt_data.Data(\n                filepath=self.filepath, parent=self.name, name=name, edit_local=True, **kwargs\n            )\n            natural_name = data.natural_name.encode()\n        if position is not None:\n            self.attrs[\"item_names\"] = np.insert(\n                self.attrs[\"item_names\"][:-1], position, natural_name\n            )\n        setattr(self, name, data)\n        return data", "response": "Create a new child data object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nensures contents are written to file.", "response": "def flush(self):\n        \"\"\"Ensure contents are written to file.\"\"\"\n        for name in self.item_names:\n            item = self[name]\n            item.flush()\n        self.file.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef quick1D(\n    data,\n    axis=0,\n    at={},\n    channel=0,\n    *,\n    local=False,\n    autosave=False,\n    save_directory=None,\n    fname=None,\n    verbose=True\n):\n    \"\"\"Quickly plot 1D slice(s) of data.\n\n    Parameters\n    ----------\n    data : WrightTools.Data object\n        Data to plot.\n    axis : string or integer (optional)\n        Expression or index of axis. Default is 0.\n    at : dictionary (optional)\n        Dictionary of parameters in non-plotted dimension(s). If not\n        provided, plots will be made at each coordinate.\n    channel : string or integer (optional)\n        Name or index of channel to plot. Default is 0.\n    local : boolean (optional)\n        Toggle plotting locally. Default is False.\n    autosave : boolean (optional)\n         Toggle autosave. Default is False.\n    save_directory : string (optional)\n         Location to save image(s). Default is None (auto-generated).\n    fname : string (optional)\n         File name. If None, data name is used. Default is None.\n    verbose : boolean (optional)\n        Toggle talkback. Default is True.\n\n    Returns\n    -------\n    list of strings\n        List of saved image files (if any).\n    \"\"\"\n    # channel index\n    channel_index = wt_kit.get_index(data.channel_names, channel)\n    shape = data.channels[channel_index].shape\n    collapse = [i for i in range(len(shape)) if shape[i] == 1]\n    at = at.copy()\n    at.update({c: 0 for c in collapse})\n    # prepare data\n    chopped = data.chop(axis, at=at, verbose=False)\n    # prepare figure\n    fig = None\n    if len(chopped) > 10:\n        if not autosave:\n            print(\"more than 10 images will be generated: forcing autosave\")\n            autosave = True\n    # prepare output folders\n    if autosave:\n        if save_directory:\n            pass\n        else:\n            if len(chopped) == 1:\n                save_directory = os.getcwd()\n                if fname:\n                    pass\n                else:\n                    fname = data.natural_name\n            else:\n                folder_name = \"quick1D \" + wt_kit.TimeStamp().path\n                os.mkdir(folder_name)\n                save_directory = folder_name\n    # determine ymin and ymax for global axis scale\n    data_channel = data.channels[channel_index]\n    ymin, ymax = data_channel.min(), data_channel.max()\n    dynamic_range = ymax - ymin\n    ymin -= dynamic_range * 0.05\n    ymax += dynamic_range * 0.05\n    if np.sign(ymin) != np.sign(data_channel.min()):\n        ymin = 0\n    if np.sign(ymax) != np.sign(data_channel.max()):\n        ymax = 0\n    # chew through image generation\n    out = []\n    for i, d in enumerate(chopped.values()):\n        # unpack data -----------------------------------------------------------------------------\n        axis = d.axes[0]\n        xi = axis.full\n        channel = d.channels[channel_index]\n        zi = channel[:]\n        # create figure ---------------------------------------------------------------------------\n        aspects = [[[0, 0], 0.5]]\n        fig, gs = create_figure(width=\"single\", nrows=1, cols=[1], aspects=aspects)\n        ax = plt.subplot(gs[0, 0])\n        # plot ------------------------------------------------------------------------------------\n        plt.plot(xi, zi, lw=2)\n        plt.scatter(xi, zi, color=\"grey\", alpha=0.5, edgecolor=\"none\")\n        # decoration ------------------------------------------------------------------------------\n        plt.grid()\n        # limits\n        if local:\n            pass\n        else:\n            plt.ylim(ymin, ymax)\n        # label axes\n        ax.set_xlabel(axis.label, fontsize=18)\n        ax.set_ylabel(channel.natural_name, fontsize=18)\n        plt.xticks(rotation=45)\n        plt.axvline(0, lw=2, c=\"k\")\n        plt.xlim(xi.min(), xi.max())\n        # add constants to title\n        ls = []\n        for constant in d.constants:\n            ls.append(constant.label)\n        title = \", \".join(ls)\n        _title(fig, data.natural_name, subtitle=title)\n        # variable marker lines\n        for constant in d.constants:\n            if constant.units is not None:\n                if axis.units_kind == constant.units_kind:\n                    constant.convert(axis.units)\n                    plt.axvline(constant.value, color=\"k\", linewidth=4, alpha=0.25)\n        # save ------------------------------------------------------------------------------------\n        if autosave:\n            if fname:\n                file_name = fname + \" \" + str(i).zfill(3)\n            else:\n                file_name = str(i).zfill(3)\n            fpath = os.path.join(save_directory, file_name + \".png\")\n            savefig(fpath, fig=fig)\n            plt.close()\n            if verbose:\n                print(\"image saved at\", fpath)\n            out.append(fpath)\n    chopped.close()\n    return out", "response": "Quickly plot 1D slice of data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef closest_pair(arr, give=\"indicies\"):\n    idxs = [idx for idx in np.ndindex(arr.shape)]\n    outs = []\n    min_dist = arr.max() - arr.min()\n    for idxa in idxs:\n        for idxb in idxs:\n            if idxa == idxb:\n                continue\n            dist = abs(arr[idxa] - arr[idxb])\n            if dist == min_dist:\n                if not [idxb, idxa] in outs:\n                    outs.append([idxa, idxb])\n            elif dist < min_dist:\n                min_dist = dist\n                outs = [[idxa, idxb]]\n    if give == \"indicies\":\n        return outs\n    elif give == \"distance\":\n        return min_dist\n    else:\n        raise KeyError(\"give not recognized in closest_pair\")", "response": "Find the pair of indices corresponding to the closest elements in an array."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef diff(xi, yi, order=1) -> np.ndarray:\n    yi = np.array(yi).copy()\n    flip = False\n    if xi[-1] < xi[0]:\n        xi = np.flipud(xi.copy())\n        yi = np.flipud(yi)\n        flip = True\n    midpoints = (xi[1:] + xi[:-1]) / 2\n    for _ in range(order):\n        d = np.diff(yi)\n        d /= np.diff(xi)\n        yi = np.interp(xi, midpoints, d)\n    if flip:\n        yi = np.flipud(yi)\n    return yi", "response": "Take the numerical derivative of a 1D array."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntakes the 1D FFT of an N - dimensional array and return sensible shifted arrays.", "response": "def fft(xi, yi, axis=0) -> tuple:\n    \"\"\"Take the 1D FFT of an N-dimensional array and return \"sensible\" properly shifted arrays.\n\n    Parameters\n    ----------\n    xi : numpy.ndarray\n        1D array over which the points to be FFT'ed are defined\n    yi : numpy.ndarray\n        ND array with values to FFT\n    axis : int\n        axis of yi to perform FFT over\n\n    Returns\n    -------\n    xi : 1D numpy.ndarray\n        1D array. Conjugate to input xi. Example: if input xi is in the time\n        domain, output xi is in frequency domain.\n    yi : ND numpy.ndarray\n        FFT. Has the same shape as the input array (yi).\n    \"\"\"\n    # xi must be 1D\n    if xi.ndim != 1:\n        raise wt_exceptions.DimensionalityError(1, xi.ndim)\n    # xi must be evenly spaced\n    spacing = np.diff(xi)\n    if not np.allclose(spacing, spacing.mean()):\n        raise RuntimeError(\"WrightTools.kit.fft: argument xi must be evenly spaced\")\n    # fft\n    yi = np.fft.fft(yi, axis=axis)\n    d = (xi.max() - xi.min()) / (xi.size - 1)\n    xi = np.fft.fftfreq(xi.size, d=d)\n    # shift\n    xi = np.fft.fftshift(xi)\n    yi = np.fft.fftshift(yi, axes=axis)\n    return xi, yi"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngive a set of arrays return the joint shape.", "response": "def joint_shape(*args) -> tuple:\n    \"\"\"Given a set of arrays, return the joint shape.\n\n    Parameters\n    ----------\n    args : array-likes\n\n    Returns\n    -------\n    tuple of int\n        Joint shape.\n    \"\"\"\n    if len(args) == 0:\n        return ()\n    shape = []\n    shapes = [a.shape for a in args]\n    ndim = args[0].ndim\n    for i in range(ndim):\n        shape.append(max([s[i] for s in shapes]))\n    return tuple(shape)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef orthogonal(*args) -> bool:\n    for i, arg in enumerate(args):\n        if hasattr(arg, \"shape\"):\n            args[i] = arg.shape\n    for s in zip(*args):\n        if np.product(s) != max(s):\n            return False\n    return True", "response": "Determine if a set of arrays are orthogonal."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef remove_nans_1D(*args) -> tuple:\n    vals = np.isnan(args[0])\n    for a in args:\n        vals |= np.isnan(a)\n    return tuple(np.array(a)[~vals] for a in args)", "response": "Remove nans in a set of 1D arrays."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntake a list of nD arrays and return a new list of nD arrays with nan indicies syncronized.", "response": "def share_nans(*arrs) -> tuple:\n    \"\"\"Take a list of nD arrays and return a new list of nD arrays.\n\n    The new list is in the same order as the old list.\n    If one indexed element in an old array is nan then every element for that\n    index in all new arrays in the list is then nan.\n\n    Parameters\n    ----------\n    *arrs : nD arrays.\n\n    Returns\n    -------\n    list\n        List of nD arrays in same order as given, with nan indicies syncronized.\n    \"\"\"\n    nans = np.zeros(joint_shape(*arrs))\n    for arr in arrs:\n        nans *= arr\n    return tuple([a + nans for a in arrs])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef smooth_1D(arr, n=10, smooth_type=\"flat\") -> np.ndarray:\n\n    # check array input\n    if arr.ndim != 1:\n        raise wt_exceptions.DimensionalityError(1, arr.ndim)\n    if arr.size < n:\n        message = \"Input array size must be larger than window size.\"\n        raise wt_exceptions.ValueError(message)\n    if n < 3:\n        return arr\n    # construct window array\n    if smooth_type == \"flat\":\n        w = np.ones(n, dtype=arr.dtype)\n    elif smooth_type == \"hanning\":\n        w = np.hanning(n)\n    elif smooth_type == \"hamming\":\n        w = np.hamming(n)\n    elif smooth_type == \"bartlett\":\n        w = np.bartlett(n)\n    elif smooth_type == \"blackman\":\n        w = np.blackman(n)\n    else:\n        message = \"Given smooth_type, {0}, not available.\".format(str(smooth_type))\n        raise wt_exceptions.ValueError(message)\n    # convolve reflected array with window function\n    out = np.convolve(w / w.sum(), arr, mode=\"same\")\n    return out", "response": "Smooth 1D array using a window function."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef unique(arr, tolerance=1e-6) -> np.ndarray:\n    arr = sorted(arr.flatten())\n    unique = []\n    while len(arr) > 0:\n        current = arr[0]\n        lis = [xi for xi in arr if np.abs(current - xi) < tolerance]\n        arr = [xi for xi in arr if not np.abs(lis[0] - xi) < tolerance]\n        xi_lis_average = sum(lis) / len(lis)\n        unique.append(xi_lis_average)\n    return np.array(unique)", "response": "Return unique elements in 1D array within tolerance."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef valid_index(index, shape) -> tuple:\n    # append slices to index\n    index = list(index)\n    while len(index) < len(shape):\n        index.append(slice(None))\n    # fill out, in reverse\n    out = []\n    for i, s in zip(index[::-1], shape[::-1]):\n        if s == 1:\n            if isinstance(i, slice):\n                out.append(slice(None))\n            else:\n                out.append(0)\n        else:\n            out.append(i)\n    return tuple(out[::-1])", "response": "Get a valid index for a broadcastable shape."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef mask_reduce(mask):\n    mask = mask.copy()\n    for i in range(len(mask.shape)):\n        a = mask.copy()\n        j = list(range(len(mask.shape)))\n        j.remove(i)\n        j = tuple(j)\n        a = a.max(axis=j, keepdims=True)\n        idx = [slice(None)] * len(mask.shape)\n        a = a.flatten()\n        idx[i] = [k for k in range(len(a)) if a[k]]\n        mask = mask[tuple(idx)]\n    return mask", "response": "Reduce a boolean mask removing all false slices in any dimension."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef enforce_mask_shape(mask, shape):\n    red = tuple([i for i in range(len(shape)) if shape[i] == 1])\n    return mask.max(axis=red, keepdims=True)", "response": "Reduce a boolean mask to fit a given shape."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd dataset from files in a directory.", "response": "def _from_directory(self, dirname, prefix=\"\"):\n        \"\"\"Add dataset from files in a directory.\n\n        Parameters\n        ----------\n        dirname : string\n            Directory name.\n        prefix : string\n            Prefix.\n        \"\"\"\n        ps = [os.path.join(here, dirname, p) for p in os.listdir(os.path.join(here, dirname))]\n        n = prefix + wt_kit.string2identifier(os.path.basename(dirname))\n        setattr(self, n, ps)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef open(filepath, edit_local=False):\n    filepath = os.fspath(filepath)\n    ds = np.DataSource(None)\n    if edit_local is False:\n        tf = tempfile.mkstemp(prefix=\"\", suffix=\".wt5\")\n        with _open(tf[1], \"w+b\") as tff:\n            with ds.open(str(filepath), \"rb\") as f:\n                tff.write(f.read())\n        filepath = tf[1]\n    f = h5py.File(filepath)\n    class_name = f[\"/\"].attrs[\"class\"]\n    name = f[\"/\"].attrs[\"name\"]\n    if class_name == \"Data\":\n        obj = wt_data.Data(filepath=str(filepath), name=name, edit_local=True)\n    elif class_name == \"Collection\":\n        obj = wt_collection.Collection(filepath=str(filepath), name=name, edit_local=True)\n    else:\n        obj = wt_group.Group(filepath=str(filepath), name=name, edit_local=True)\n\n    if edit_local is False:\n        setattr(obj, \"_tmpfile\", tf)\n        weakref.finalize(obj, obj.close)\n    return obj", "response": "Open any wt5 file returning the top - level object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef close():\n    while len(wt_group.Group._instances) > 0:\n        wt_group.Group._instances.popitem()[1].close()", "response": "Close all open wt5 files."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nturning a string into a valid python identifier.", "response": "def string2identifier(s):\n    \"\"\"Turn a string into a valid python identifier.\n\n    Currently only allows ASCII letters and underscore. Illegal characters\n    are replaced with underscore. This is slightly more opinionated than\n    python 3 itself, and may be refactored in future (see PEP 3131).\n\n    Parameters\n    ----------\n    s : string\n        string to convert\n\n    Returns\n    -------\n    str\n        valid python identifier.\n    \"\"\"\n    # https://docs.python.org/3/reference/lexical_analysis.html#identifiers\n    # https://www.python.org/dev/peps/pep-3131/\n    if len(s) == 0:\n        return \"_\"\n    if s[0] not in string.ascii_letters:\n        s = \"_\" + s\n    valids = string.ascii_letters + string.digits + \"_\"\n    out = \"\"\n    for i, char in enumerate(s):\n        if char in valids:\n            out += char\n        else:\n            out += \"_\"\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts from one unit to another.", "response": "def converter(val, current_unit, destination_unit):\n    \"\"\"Convert from one unit to another.\n\n    Parameters\n    ----------\n    val : number\n        Number to convert.\n    current_unit : string\n        Current unit.\n    destination_unit : string\n        Destination unit.\n\n    Returns\n    -------\n    number\n        Converted value.\n    \"\"\"\n    x = val\n    for dic in dicts.values():\n        if current_unit in dic.keys() and destination_unit in dic.keys():\n            try:\n                native = eval(dic[current_unit][0])\n            except ZeroDivisionError:\n                native = np.inf\n            x = native  # noqa: F841\n            try:\n                out = eval(dic[destination_unit][1])\n            except ZeroDivisionError:\n                out = np.inf\n            return out\n    # if all dictionaries fail\n    if current_unit is None and destination_unit is None:\n        pass\n    else:\n        warnings.warn(\n            \"conversion {0} to {1} not valid: returning input\".format(\n                current_unit, destination_unit\n            )\n        )\n    return val"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets default LaTeX formatted symbol.", "response": "def get_symbol(units) -> str:\n    \"\"\"Get default symbol type.\n\n    Parameters\n    ----------\n    units_str : string\n        Units.\n\n    Returns\n    -------\n    string\n        LaTeX formatted symbol.\n    \"\"\"\n    if kind(units) == \"energy\":\n        d = {}\n        d[\"nm\"] = r\"\\lambda\"\n        d[\"wn\"] = r\"\\bar\\nu\"\n        d[\"eV\"] = r\"\\hslash\\omega\"\n        d[\"Hz\"] = r\"f\"\n        d[\"THz\"] = r\"f\"\n        d[\"GHz\"] = r\"f\"\n        return d.get(units, \"E\")\n    elif kind(units) == \"delay\":\n        return r\"\\tau\"\n    elif kind(units) == \"fluence\":\n        return r\"\\mathcal{F}\"\n    elif kind(units) == \"pulse_width\":\n        return r\"\\sigma\"\n    elif kind(units) == \"temperature\":\n        return r\"T\"\n    else:\n        return kind(units)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfinds the kind of the given units.", "response": "def kind(units):\n    \"\"\"Find the kind of given units.\n\n    Parameters\n    ----------\n    units : string\n        The units of interest\n\n    Returns\n    -------\n    string\n        The kind of the given units. If no match is found, returns None.\n    \"\"\"\n    for k, v in dicts.items():\n        if units in v.keys():\n            return k"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef latex_defs_to_katex_macros(defs):\n    r'''Converts LaTeX \\def statements to KaTeX macros.\n\n    This is a helper function that can be used in conf.py to translate your\n    already specified LaTeX definitions.\n\n    https://github.com/Khan/KaTeX#rendering-options, e.g.\n    `\\def \\e #1{\\mathrm{e}^{#1}}` => `\"\\\\e:\" \"\\\\mathrm{e}^{#1}\"`'\n\n    Example\n    -------\n    import sphinxcontrib.katex as katex\n    # Get your LaTeX defs into `latex_defs` and then do\n    latex_macros = katex.import_macros_from_latex(latex_defs)\n    katex_options = 'macros: {' + latex_macros + '}'\n    '''\n    # Remove empty lines\n    defs = defs.strip()\n    tmp = []\n    for line in defs.splitlines():\n        # Remove spaces from every line\n        line = line.strip()\n        # Remove \"\\def\" at the beginning of line\n        line = re.sub(r'^\\\\def[ ]?', '', line)\n        # Remove optional #1 parameter before {} command brackets\n        line = re.sub(r'(#[0-9])+', '', line, 1)\n        # Remove outer {} command brackets with \"\"\n        line = re.sub(r'( {)|(}$)', '\"', line)\n        # Add \"\": to the new command\n        line = re.sub(r'(^\\\\[A-Za-z]+)', r'\"\\1\":', line, 1)\n        # Add , at end of line\n        line = re.sub(r'$', ',', line, 1)\n        # Duplicate all \\\n        line = re.sub(r'\\\\', r'\\\\\\\\', line)\n        tmp.append(line)\n    macros = '\\n'.join(tmp)\n    return macros", "response": "Converts LaTeX definitions to KaTeX macros."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a string that can be used to set the default delimiters for the KaTeX auto - rendering options of the KaTeX.", "response": "def katex_rendering_delimiters(app):\n    \"\"\"Delimiters for rendering KaTeX math.\n\n    If no delimiters are specified in katex_options, add the\n    katex_inline and katex_display delimiters. See also\n    https://khan.github.io/KaTeX/docs/autorender.html\n    \"\"\"\n    # Return if we have user defined rendering delimiters\n    if 'delimiters' in app.config.katex_options:\n        return ''\n    katex_inline = [d.replace('\\\\', '\\\\\\\\') for d in app.config.katex_inline]\n    katex_display = [d.replace('\\\\', '\\\\\\\\') for d in app.config.katex_display]\n    katex_delimiters = {'inline': katex_inline, 'display': katex_display}\n    # Set chosen delimiters for the auto-rendering options of KaTeX\n    delimiters = r'''delimiters: [\n        {{ left: \"{inline[0]}\", right: \"{inline[1]}\", display: false }},\n        {{ left: \"{display[0]}\", right: \"{display[1]}\", display: true }}\n        ]'''.format(**katex_delimiters)\n    return delimiters"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nstrip katex_options from enclosing {} and append,", "response": "def katex_rendering_options(app):\n    \"\"\"Strip katex_options from enclosing {} and append ,\"\"\"\n    options = trim(app.config.katex_options)\n    # Remove surrounding {}\n    if options.startswith('{') and options.endswith('}'):\n        options = trim(options[1:-1])\n    # If options is not empty, ensure it ends with ','\n    if options and not options.endswith(','):\n        options += ','\n    return options"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef label(self) -> str:\n        label = self.expression.replace(\"_\", \"\\\\;\")\n        if self.units_kind:\n            symbol = wt_units.get_symbol(self.units)\n            for v in self.variables:\n                vl = \"%s_{%s}\" % (symbol, v.label)\n                vl = vl.replace(\"_{}\", \"\")  # label can be empty, no empty subscripts\n                label = label.replace(v.natural_name, vl)\n            units_dictionary = getattr(wt_units, self.units_kind)\n            label += r\"\\,\"\n            label += r\"\\left(\"\n            label += units_dictionary[self.units][2]\n            label += r\"\\right)\"\n        label = r\"$\\mathsf{%s}$\" % label\n        return label", "response": "A latex formatted label representing axis expression."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef natural_name(self) -> str:\n        name = self.expression.strip()\n        for op in operators:\n            name = name.replace(op, operator_to_identifier[op])\n        return wt_kit.string2identifier(name)", "response": "Valid python identifier representation of the expession."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of all variables that are defined in the expression.", "response": "def variables(self) -> list:\n        \"\"\"Variables.\"\"\"\n        try:\n            assert self._variables is not None\n        except (AssertionError, AttributeError):\n            pattern = \"|\".join(map(re.escape, operators))\n            keys = re.split(pattern, self.expression)\n            indices = []\n            for key in keys:\n                if key in self.parent.variable_names:\n                    indices.append(self.parent.variable_names.index(key))\n            self._variables = [self.parent.variables[i] for i in indices]\n        finally:\n            return self._variables"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\naxing expression evaluated and masked with NaN shared from data channels.", "response": "def masked(self) -> np.ndarray:\n        \"\"\"Axis expression evaluated, and masked with NaN shared from data channels.\"\"\"\n        arr = self[:]\n        arr.shape = self.shape\n        arr = wt_kit.share_nans(arr, *self.parent.channels)[0]\n        return np.nanmean(\n            arr, keepdims=True, axis=tuple(i for i in range(self.ndim) if self.shape[i] == 1)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef convert(self, destination_units, *, convert_variables=False):\n        if self.units is None and (destination_units is None or destination_units == \"None\"):\n            return\n        if not wt_units.is_valid_conversion(self.units, destination_units):\n            valid = wt_units.get_valid_conversions(self.units)\n            raise wt_exceptions.UnitsError(valid, destination_units)\n        if convert_variables:\n            for v in self.variables:\n                v.convert(destination_units)\n        self.units = destination_units", "response": "Convert axis to destination_units."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_section(self, section):\n        self.config.read(self.filepath)\n        self.config.add_section(section)\n        with open(self.filepath, \"w\") as f:\n            self.config.write(f)", "response": "Add section to the configuration file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dictionary(self) -> dict:\n        self.config.read(self.filepath)\n        return self.config._sections", "response": "Get a python dictionary of contents."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef has_option(self, section, option) -> bool:\n        self.config.read(self.filepath)\n        return self.config.has_option(section, option)", "response": "Test if file has option."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef has_section(self, section) -> bool:\n        self.config.read(self.filepath)\n        return self.config.has_section(section)", "response": "Test if file has section."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef read(self, section, option):\n        self.config.read(self.filepath)\n        raw = self.config.get(section, option)\n        out = tidy_headers._parse_item.string2item(raw, sep=\", \")\n        return out", "response": "Read from file and return a dict."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrite to file. Parameters ---------- section : string Section. option : string Option. value : string Value.", "response": "def write(self, section, option, value):\n        \"\"\"Write to file.\n\n        Parameters\n        ----------\n        section : string\n            Section.\n        option : string\n            Option.\n        value : string\n            Value.\n        \"\"\"\n        self.config.read(self.filepath)\n        string = tidy_headers._parse_item.item2string(value, sep=\", \")\n        self.config.set(section, option, string)\n        with open(self.filepath, \"w\") as f:\n            self.config.write(f)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef timestamp_from_RFC3339(RFC3339):\n    dt = dateutil.parser.parse(RFC3339)\n    if hasattr(dt.tzinfo, \"_offset\"):\n        timezone = dt.tzinfo._offset.total_seconds()\n    else:\n        timezone = \"utc\"\n    timestamp = TimeStamp(at=dt.timestamp(), timezone=timezone)\n    return timestamp", "response": "Generate a Timestamp object from a RFC3339 formatted string."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the RFC 3339 - compliant date string for the current date and time.", "response": "def RFC3339(self):\n        \"\"\"RFC3339.\n\n        `Link to RFC3339.`__\n\n        __ https://www.ietf.org/rfc/rfc3339.txt\n        \"\"\"\n        # get timezone offset\n        delta_sec = time.timezone\n        m, s = divmod(delta_sec, 60)\n        h, m = divmod(m, 60)\n        # timestamp\n        format_string = \"%Y-%m-%dT%H:%M:%S.%f\"\n        out = self.datetime.strftime(format_string)\n        # timezone\n        if delta_sec == 0.:\n            out += \"Z\"\n        else:\n            if delta_sec > 0:\n                sign = \"+\"\n            elif delta_sec < 0:\n                sign = \"-\"\n\n            def as_string(num):\n                return str(np.abs(int(num))).zfill(2)\n\n            out += sign + as_string(h) + \":\" + as_string(m)\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef path(self):\n        out = self.datetime.strftime(\"%Y-%m-%d\")\n        out += \" \"\n        ssm = (\n            self.datetime - self.datetime.replace(hour=0, minute=0, second=0, microsecond=0)\n        ).total_seconds()\n        out += str(int(ssm)).zfill(5)\n        return out", "response": "Timestamp for placing into filepaths."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef zoom2D(xi, yi, zi, xi_zoom=3., yi_zoom=3., order=3, mode=\"nearest\", cval=0.):\n    xi = ndimage.interpolation.zoom(xi, xi_zoom, order=order, mode=\"nearest\")\n    yi = ndimage.interpolation.zoom(yi, yi_zoom, order=order, mode=\"nearest\")\n    zi = ndimage.interpolation.zoom(zi, (xi_zoom, yi_zoom), order=order, mode=mode, cval=cval)\n    return xi, yi, zi", "response": "Zoom a 2D array with axes."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef apply_rcparams(kind=\"fast\"):\n    if kind == \"default\":\n        matplotlib.rcdefaults()\n    elif kind == \"fast\":\n        matplotlib.rcParams[\"text.usetex\"] = False\n        matplotlib.rcParams[\"mathtext.fontset\"] = \"cm\"\n        matplotlib.rcParams[\"font.family\"] = \"sans-serif\"\n        matplotlib.rcParams[\"font.size\"] = 14\n        matplotlib.rcParams[\"legend.edgecolor\"] = \"grey\"\n        matplotlib.rcParams[\"contour.negative_linestyle\"] = \"solid\"\n    elif kind == \"publication\":\n        matplotlib.rcParams[\"text.usetex\"] = True\n        preamble = \"\\\\usepackage[cm]{sfmath}\\\\usepackage{amssymb}\"\n        matplotlib.rcParams[\"text.latex.preamble\"] = preamble\n        matplotlib.rcParams[\"mathtext.fontset\"] = \"cm\"\n        matplotlib.rcParams[\"font.family\"] = \"sans-serif\"\n        matplotlib.rcParams[\"font.serif\"] = \"cm\"\n        matplotlib.rcParams[\"font.sans-serif\"] = \"cm\"\n        matplotlib.rcParams[\"font.size\"] = 14\n        matplotlib.rcParams[\"legend.edgecolor\"] = \"grey\"\n        matplotlib.rcParams[\"contour.negative_linestyle\"] = \"solid\"", "response": "Quickly apply rcparams for given purposes."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\napplying x and y labels to axes.", "response": "def _apply_labels(\n        self, autolabel=\"none\", xlabel=None, ylabel=None, data=None, channel_index=0\n    ):\n        \"\"\"Apply x and y labels to axes.\n\n        Parameters\n        ----------\n        autolabel : {'none', 'both', 'x', 'y'} (optional)\n            Label(s) to apply from data. Default is none.\n        xlabel : string (optional)\n            x label. Default is None.\n        ylabel : string (optional)\n            y label. Default is None.\n        data : WrightTools.data.Data object (optional)\n            data to read labels from. Default is None.\n        channel_index : integer (optional)\n            Channel index. Default is 0.\n        \"\"\"\n        # read from data\n        if autolabel in [\"xy\", \"both\", \"x\"] and not xlabel:\n            xlabel = data.axes[0].label\n        if autolabel in [\"xy\", \"both\", \"y\"] and not ylabel:\n            if data.ndim == 1:\n                ylabel = data.channels[channel_index].label\n            elif data.ndim == 2:\n                ylabel = data.axes[1].label\n        # apply\n        if xlabel:\n            if isinstance(xlabel, bool):\n                xlabel = data.axes[0].label\n            self.set_xlabel(xlabel, fontsize=18)\n        if ylabel:\n            if isinstance(ylabel, bool):\n                ylabel = data.axes[1].label\n            self.set_ylabel(ylabel, fontsize=18)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_sideplot(self, along, pad=0, height=0.75, ymin=0, ymax=1.1):\n        # divider should only be created once\n        if hasattr(self, \"divider\"):\n            divider = self.divider\n        else:\n            divider = make_axes_locatable(self)\n            setattr(self, \"divider\", divider)\n        # create\n        if along == \"x\":\n            ax = self.sidex = divider.append_axes(\"top\", height, pad=pad, sharex=self)\n        elif along == \"y\":\n            ax = self.sidey = divider.append_axes(\"right\", height, pad=pad, sharey=self)\n            ax.transposed = True\n        # beautify\n        if along == \"x\":\n            ax.set_ylim(ymin, ymax)\n        elif along == \"y\":\n            ax.set_xlim(ymin, ymax)\n        ax.autoscale(enable=False)\n        ax.set_adjustable(\"box\")\n        ax.is_sideplot = True\n        plt.setp(ax.get_xticklabels(), visible=False)\n        plt.setp(ax.get_yticklabels(), visible=False)\n        ax.tick_params(axis=\"both\", which=\"both\", length=0)\n        return ax", "response": "Add a sideplot to the current object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef contour(self, *args, **kwargs):\n        args, kwargs = self._parse_plot_args(*args, **kwargs, plot_type=\"contour\")\n        return super().contour(*args, **kwargs)", "response": "Plot contours. A contour plot can be used to plot a 3D or higher Data object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef contourf(self, *args, **kwargs):\n        args, kwargs = self._parse_plot_args(*args, **kwargs, plot_type=\"contourf\")\n        # Overloading contourf in an attempt to fix aliasing problems when saving vector graphics\n        # see https://stackoverflow.com/questions/15822159\n        # also see https://stackoverflow.com/a/32911283\n        # set_edgecolor('face') does indeed remove all of the aliasing problems\n        # unfortunately, it also seems to distort the plot in a subtle but important way\n        # it shifts the entire colorbar down w.r.t. the data (by one contour? not clear)\n        # so for now, I am trying to fix the problem by adding contour just below contourf\n        # this does not perfectly get rid of the aliasing, but it doesn't distort the data\n        # which is more important\n        # I anticipate that this method will be tinkered with in the future\n        # so I've left the things I have tried and abandoned as comments---good luck!\n        # ---Blaise 2017-07-30\n        kwargs[\"antialiased\"] = False\n        kwargs[\"extend\"] = \"both\"\n        contours = super().contourf(*args, **kwargs)\n        # fill lines\n        zorder = contours.collections[0].zorder - 0.1\n        levels = (contours.levels[1:] + contours.levels[:-1]) / 2\n        matplotlib.axes.Axes.contour(\n            self, *args[:3], levels=levels, cmap=contours.cmap, zorder=zorder\n        )\n        # decoration\n        self.set_facecolor([0.75] * 3)\n        # PathCollection modifications\n        for c in contours.collections:\n            pass\n            # c.set_rasterized(True)\n            # c.set_edgecolor('face')\n        return contours", "response": "Plot contours.\n\n        If a 3D or higher Data object is passed, a lower dimensional\n        channel can be plotted, provided the ``squeeze`` of the channel\n        has ``ndim==2`` and the first two axes do not span dimensions\n        other than those spanned by that channel.\n\n        Parameters\n        ----------\n        data : 2D WrightTools.data.Data object\n            Data to plot.\n        channel : int or string (optional)\n            Channel index or name. Default is 0.\n        dynamic_range : boolean (optional)\n            Force plotting of all contours, overloading for major extent. Only applies to signed\n            data. Default is False.\n        autolabel : {'none', 'both', 'x', 'y'}  (optional)\n            Parameterize application of labels directly from data object. Default is none.\n        xlabel : string (optional)\n            xlabel. Default is None.\n        ylabel : string (optional)\n            ylabel. Default is None.\n        **kwargs\n            matplotlib.axes.Axes.contourf__ optional keyword arguments.\n\n            __ https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.contourf.html\n\n        Returns\n        -------\n        matplotlib.contour.QuadContourSet"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef legend(self, *args, **kwargs):\n        if \"fancybox\" not in kwargs.keys():\n            kwargs[\"fancybox\"] = False\n        if \"framealpha\" not in kwargs.keys():\n            kwargs[\"framealpha\"] = 1.\n        return super().legend(*args, **kwargs)", "response": "Add a legend.\n\n        Parameters\n        ----------\n        *args\n            matplotlib legend args.\n        *kwargs\n            matplotlib legend kwargs.\n\n        Returns\n        -------\n        legend"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pcolor(self, *args, **kwargs):\n        args, kwargs = self._parse_plot_args(*args, **kwargs, plot_type=\"pcolor\")\n        return super().pcolor(*args, **kwargs)", "response": "Create a pseudocolor plot of a 2D array."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a pseudocolor plot of a 2D array.", "response": "def pcolormesh(self, *args, **kwargs):\n        \"\"\"Create a pseudocolor plot of a 2-D array.\n\n        If a 3D or higher Data object is passed, a lower dimensional\n        channel can be plotted, provided the ``squeeze`` of the channel\n        has ``ndim==2`` and the first two axes do not span dimensions\n        other than those spanned by that channel.\n\n        Uses pcolor_helper to ensure that color boundaries are drawn\n        bisecting point positions, when possible.\n        Quicker than pcolor\n\n        Parameters\n        ----------\n        data : 2D WrightTools.data.Data object\n            Data to plot.\n        channel : int or string (optional)\n            Channel index or name. Default is 0.\n        dynamic_range : boolean (optional)\n            Force plotting of all contours, overloading for major extent. Only applies to signed\n            data. Default is False.\n        autolabel : {'none', 'both', 'x', 'y'}  (optional)\n            Parameterize application of labels directly from data object. Default is none.\n        xlabel : string (optional)\n            xlabel. Default is None.\n        ylabel : string (optional)\n            ylabel. Default is None.\n        **kwargs\n            matplotlib.axes.Axes.pcolormesh__ optional keyword arguments.\n\n            __ https://matplotlib.org/api/_as_gen/matplotlib.pyplot.pcolormesh.html\n\n        Returns\n        -------\n        matplotlib.collections.QuadMesh\n        \"\"\"\n        args, kwargs = self._parse_plot_args(*args, **kwargs, plot_type=\"pcolormesh\")\n        return super().pcolormesh(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nplotting lines and markers of a signed or lower - dimensional channel.", "response": "def plot(self, *args, **kwargs):\n        \"\"\"Plot lines and/or markers.\n\n        If a 2D or higher Data object is passed, a lower dimensional\n        channel can be plotted, provided the ``squeeze`` of the channel\n        has ``ndim==1`` and the first axis does not span dimensions\n        other than that spanned by the channel.\n\n        Parameters\n        ----------\n        data : 1D WrightTools.data.Data object\n            Data to plot.\n        channel : int or string (optional)\n            Channel index or name. Default is 0.\n        dynamic_range : boolean (optional)\n            Force plotting of all contours, overloading for major extent. Only applies to signed\n            data. Default is False.\n        autolabel : {'none', 'both', 'x', 'y'}  (optional)\n            Parameterize application of labels directly from data object. Default is none.\n        xlabel : string (optional)\n            xlabel. Default is None.\n        ylabel : string (optional)\n            ylabel. Default is None.\n        **kwargs\n            matplotlib.axes.Axes.plot__ optional keyword arguments.\n\n            __ https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.plot.html\n\n        Returns\n        -------\n        list\n            list of matplotlib.lines.line2D objects\n        \"\"\"\n        args = list(args)  # offer pop, append etc\n        # unpack data object, if given\n        if isinstance(args[0], Data):\n            data = args.pop(0)\n            channel = kwargs.pop(\"channel\", 0)\n            channel_index = wt_kit.get_index(data.channel_names, channel)\n            squeeze = np.array(data.channels[channel_index].shape) == 1\n            xa = data.axes[0]\n            for sq, xs in zip(squeeze, xa.shape):\n                if sq and xs != 1:\n                    raise wt_exceptions.ValueError(\"Cannot squeeze axis to fit channel\")\n            squeeze = tuple([0 if i else slice(None) for i in squeeze])\n            zi = data.channels[channel_index].points\n            xi = xa[squeeze]\n            if not zi.ndim == 1:\n                raise wt_exceptions.DimensionalityError(1, data.ndim)\n            args = [xi, zi] + args\n        else:\n            data = None\n            channel_index = 0\n        # labels\n        self._apply_labels(\n            autolabel=kwargs.pop(\"autolabel\", False),\n            xlabel=kwargs.pop(\"xlabel\", None),\n            ylabel=kwargs.pop(\"ylabel\", None),\n            data=data,\n            channel_index=channel_index,\n        )\n        # call parent\n        return super().plot(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a subplot to the figure.", "response": "def add_subplot(self, *args, **kwargs):\n        \"\"\"Add a subplot to the figure.\n\n        Parameters\n        ----------\n        *args\n        **kwargs\n\n        Returns\n        -------\n        WrightTools.artists.Axes object\n        \"\"\"\n        kwargs.setdefault(\"projection\", \"wright\")\n        return super().add_subplot(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef warn(filepath, expected):\n        filepath = pathlib.Path(filepath)\n        message = \"file {0} has type {1} (expected {2})\".format(\n            filepath, filepath.suffix, expected\n        )\n        warnings.warn(message, WrongFileTypeWarning)", "response": "Raise warning.\n            Given filepath."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a tuple of all datasets in this dataset set.", "response": "def datasets(self) -> tuple:\n        \"\"\"Datasets.\"\"\"\n        return tuple(v for _, v in self.items() if isinstance(v, h5py.Dataset))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef kind(self):\n        if \"kind\" not in self.attrs.keys():\n            self.attrs[\"kind\"] = \"None\"\n        value = self.attrs[\"kind\"]\n        return value if not value == \"None\" else None", "response": "Kind of the assessment."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ndim(self) -> int:\n        try:\n            assert self._ndim is not None\n        except (AssertionError, AttributeError):\n            if len(self.variables) == 0:\n                self._ndim = 0\n            else:\n                self._ndim = self.variables[0].ndim\n        finally:\n            return self._ndim", "response": "Get number of dimensions."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef shape(self) -> tuple:\n        try:\n            assert self._shape is not None\n        except (AssertionError, AttributeError):\n            self._shape = wt_kit.joint_shape(*self.variables)\n        finally:\n            return self._shape", "response": "Returns the shape of the object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the source attribute of the object.", "response": "def source(self):\n        \"\"\"Source.\"\"\"\n        if \"source\" not in self.attrs.keys():\n            self.attrs[\"source\"] = \"None\"\n        value = self.attrs[\"source\"]\n        return value if not value == \"None\" else None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef variables(self) -> tuple:\n        try:\n            assert self._variables is not None\n        except (AssertionError, AttributeError):\n            self._variables = [self[n] for n in self.variable_names]\n        finally:\n            return tuple(self._variables)", "response": "Returns a tuple of all variables in the current object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nbring a specific channel to the zero - indexed position in channels.", "response": "def bring_to_front(self, channel):\n        \"\"\"Bring a specific channel to the zero-indexed position in channels.\n\n        All other channels get pushed back but remain in order.\n\n        Parameters\n        ----------\n        channel : int or str\n            Channel index or name.\n        \"\"\"\n        channel_index = wt_kit.get_index(self.channel_names, channel)\n        new = list(self.channel_names)\n        new.insert(0, new.pop(channel_index))\n        self.channel_names = new"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef chop(self, *args, at={}, parent=None, verbose=True) -> wt_collection.Collection:\n        from ._axis import operators, operator_to_identifier\n\n        # parse args\n        args = list(args)\n        for i, arg in enumerate(args):\n            if isinstance(arg, int):\n                args[i] = self._axes[arg].natural_name\n            elif isinstance(arg, str):\n                # same normalization that occurs in the natural_name @property\n                arg = arg.strip()\n                for op in operators:\n                    arg = arg.replace(op, operator_to_identifier[op])\n                args[i] = wt_kit.string2identifier(arg)\n\n        # normalize the at keys to the natural name\n        for k in [ak for ak in at.keys() if type(ak) == str]:\n            for op in operators:\n                if op in k:\n                    nk = k.replace(op, operator_to_identifier[op])\n                    at[nk] = at[k]\n                    at.pop(k)\n                    k = nk\n\n        # get output collection\n        out = wt_collection.Collection(name=\"chop\", parent=parent)\n        # get output shape\n        kept = args + [ak for ak in at.keys() if type(ak) == str]\n        kept_axes = [self._axes[self.axis_names.index(a)] for a in kept]\n        removed_axes = [a for a in self._axes if a not in kept_axes]\n        removed_shape = wt_kit.joint_shape(*removed_axes)\n        if removed_shape == ():\n            removed_shape = (1,) * self.ndim\n        removed_shape = list(removed_shape)\n        for i in at.keys():\n            if type(i) == int:\n                removed_shape[i] = 1\n        removed_shape = tuple(removed_shape)\n        # iterate\n        i = 0\n        for idx in np.ndindex(removed_shape):\n            idx = np.array(idx, dtype=object)\n            idx[np.array(removed_shape) == 1] = slice(None)\n            for axis, point in at.items():\n                if type(axis) == int:\n                    idx[axis] = point\n                    continue\n                point, units = point\n                destination_units = self._axes[self.axis_names.index(axis)].units\n                point = wt_units.converter(point, units, destination_units)\n                axis_index = self.axis_names.index(axis)\n                axis = self._axes[axis_index]\n                idx_index = np.array(axis.shape) > 1\n                if np.sum(idx_index) > 1:\n                    raise wt_exceptions.MultidimensionalAxisError(\"chop\", axis.natural_name)\n                idx_index = list(idx_index).index(True)\n                idx[idx_index] = np.argmin(np.abs(axis[tuple(idx)] - point))\n            data = out.create_data(name=\"chop%03i\" % i)\n            for v in self.variables:\n                kwargs = {}\n                kwargs[\"name\"] = v.natural_name\n                kwargs[\"values\"] = v[idx]\n                kwargs[\"units\"] = v.units\n                kwargs[\"label\"] = v.label\n                kwargs.update(v.attrs)\n                data.create_variable(**kwargs)\n            for c in self.channels:\n                kwargs = {}\n                kwargs[\"name\"] = c.natural_name\n                kwargs[\"values\"] = c[idx]\n                kwargs[\"units\"] = c.units\n                kwargs[\"label\"] = c.label\n                kwargs[\"signed\"] = c.signed\n                kwargs.update(c.attrs)\n                data.create_channel(**kwargs)\n            new_axes = [a.expression for a in kept_axes if a.expression not in at.keys()]\n            new_axis_units = [a.units for a in kept_axes if a.expression not in at.keys()]\n            data.transform(*new_axes)\n            for const in self.constant_expressions:\n                data.create_constant(const, verbose=False)\n            for ax in self.axis_expressions:\n                if ax not in new_axes:\n                    data.create_constant(ax, verbose=False)\n            for j, units in enumerate(new_axis_units):\n                data.axes[j].convert(units)\n            i += 1\n        out.flush()\n        # return\n        if verbose:\n            print(\"chopped data into %d piece(s)\" % len(out), \"in\", new_axes)\n        return out", "response": "Returns a new WrightTools Collection instance with the same dimensionality as the original dataset."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the gradient along one axis.", "response": "def gradient(self, axis, *, channel=0):\n        \"\"\"\n        Compute the gradient along one axis.\n\n        New channels have names ``<channel name>_<axis name>_gradient``.\n\n        Parameters\n        ----------\n        axis : int or str\n            The axis to differentiate along.\n            If given as an integer, the axis in the underlying array is used,\n            and unitary spacing is assumed.\n            If given as a string, the axis must exist, and be a 1D array-aligned axis.\n            (i.e. have a shape with a single value which is not ``1``)\n            The axis to collapse along is inferred from the shape of the axis.\n        channel : int or str\n            The channel to differentiate.\n            Default is the first channel.\n        \"\"\"\n        # get axis index --------------------------------------------------------------------------\n        if isinstance(axis, int):\n            axis_index = axis\n        elif isinstance(axis, str):\n            index = self.axis_names.index(axis)\n            axes = [i for i in range(self.ndim) if self.axes[index].shape[i] > 1]\n            if len(axes) > 1:\n                raise wt_exceptions.MultidimensionalAxisError(axis, \"collapse\")\n            elif len(axes) == 0:\n                raise wt_exceptions.ValueError(\n                    \"Axis '{}' is a single point, cannot compute gradient\".format(axis)\n                )\n            axis_index = axes[0]\n        else:\n            raise wt_exceptions.TypeError(\"axis: expected {int, str}, got %s\" % type(axis))\n\n        channel_index = wt_kit.get_index(self.channel_names, channel)\n        channel = self.channel_names[channel_index]\n\n        if self[channel].shape[axis_index] == 1:\n            raise wt_exceptions.ValueError(\n                \"Channel '{}' has a single point along Axis '{}', cannot compute gradient\".format(\n                    channel, axis\n                )\n            )\n        rtype = np.result_type(self[channel].dtype, float)\n        new = self.create_channel(\n            \"{}_{}_gradient\".format(channel, axis),\n            values=np.empty(self[channel].shape, dtype=rtype),\n        )\n\n        channel = self[channel]\n        if axis == axis_index:\n            new[:] = np.gradient(channel[:], axis=axis_index)\n        else:\n            new[:] = np.gradient(channel[:], self[axis].points, axis=axis_index)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef moment(self, axis, channel=0, moment=1, *, resultant=None):\n        # get axis index --------------------------------------------------------------------------\n        axis_index = None\n        if resultant is not None:\n            for i, (s, r) in enumerate(zip(self.shape, resultant)):\n                if s != r and r == 1 and axis_index is None:\n                    axis_index = i\n                elif s == r:\n                    continue\n                else:\n                    raise wt_exceptions.ValueError(\n                        f\"Invalid resultant shape '{resultant}' for shape {self.shape}. \"\n                        + \"Consider using `wt.kit.joint_shape` to join non-collapsed axes.\"\n                    )\n\n        index = wt_kit.get_index(self.axis_names, axis)\n        if axis_index is None:\n            axes = [i for i in range(self.ndim) if self.axes[index].shape[i] > 1]\n            if len(axes) > 1:\n                raise wt_exceptions.MultidimensionalAxisError(axis, \"moment\")\n            elif len(axes) == 0:\n                raise wt_exceptions.ValueError(\n                    \"Axis {} is a single point, cannot compute moment\".format(axis)\n                )\n            axis_index = axes[0]\n\n        warnings.warn(\"moment\", category=wt_exceptions.EntireDatasetInMemoryWarning)\n\n        channel_index = wt_kit.get_index(self.channel_names, channel)\n        channel = self.channel_names[channel_index]\n\n        if self[channel].shape[axis_index] == 1:\n            raise wt_exceptions.ValueError(\n                \"Channel '{}' has a single point along Axis '{}', cannot compute moment\".format(\n                    channel, axis\n                )\n            )\n\n        new_shape = list(self[channel].shape)\n        new_shape[axis_index] = 1\n\n        channel = self[channel]\n        axis_inp = axis\n        axis = self.axes[index]\n        x = axis[:]\n        if np.any(np.isnan(x)):\n            raise wt_exceptions.ValueError(\"Axis '{}' includes NaN\".format(axis_inp))\n        y = np.nan_to_num(channel[:])\n\n        try:\n            moments = tuple(moment)\n        except TypeError:\n            moments = (moment,)\n\n        multiplier = 1\n        if 0 in moments:\n            # May be possible to optimize, probably doesn't need the sum\n            # only matters for integral, all others normalize by integral\n            multiplier = np.sign(\n                np.sum(np.diff(x, axis=axis_index), axis=axis_index, keepdims=True)\n            )\n\n        for moment in moments:\n            about = 0\n            norm = 1\n            if moment > 0:\n                norm = np.trapz(y, x, axis=axis_index)\n                norm = np.array(norm)\n                norm.shape = new_shape\n            if moment > 1:\n                about = np.trapz(x * y, x, axis=axis_index)\n                about = np.array(about)\n                about.shape = new_shape\n                about /= norm\n            if moment > 2:\n                sigma = np.trapz((x - about) ** 2 * y, x, axis=axis_index)\n                sigma = np.array(sigma)\n                sigma.shape = new_shape\n                sigma /= norm\n                sigma **= 0.5\n                norm *= sigma ** moment\n\n            values = np.trapz((x - about) ** moment * y, x, axis=axis_index)\n            values = np.array(values)\n            values.shape = new_shape\n            values /= norm\n            if moment == 0:\n                values *= multiplier\n            self.create_channel(\n                \"{}_{}_{}_{}\".format(channel.natural_name, axis_inp, \"moment\", moment),\n                values=values,\n            )", "response": "Takes the nth moment the dataset along one axis adding lower rank channels."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncollapsing the dataset along a particular axis.", "response": "def collapse(self, axis, method=\"sum\"):\n        \"\"\"Collapse the dataset along one axis, adding lower rank channels.\n\n        New channels have names ``<channel name>_<axis name>_<method>``.\n\n        Parameters\n        ----------\n        axis : int or str\n            The axis to collapse along.\n            If given as an integer, the axis in the underlying array is used.\n            If given as a string, the axis must exist, and be a 1D array-aligned axis.\n            (i.e. have a shape with a single value which is not ``1``)\n            The axis to collapse along is inferred from the shape of the axis.\n        method : {'average', 'sum', 'max', 'min'} (optional)\n            The method of collapsing the given axis. Method may also be list\n            of methods corresponding to the channels of the object. Default\n            is sum. NaNs are ignored.\n            Can also be a list, allowing for different treatment for varied channels.\n            In this case, None indicates that no change to that channel should occur.\n\n        See Also\n        --------\n        chop\n            Divide the dataset into its lower-dimensionality components.\n        split\n            Split the dataset while maintaining its dimensionality.\n        moment\n            Take the moment along a particular axis\n        \"\"\"\n        if method in (\"int\", \"integrate\"):\n            warnings.warn(\n                \"integrate method of collapse is deprecated, use moment(moment=0) instead\",\n                wt_exceptions.VisibleDeprecationWarning,\n            )\n            for channel in self.channel_names:\n                try:\n                    self.moment(axis, channel, moment=0)\n                    self.rename_channels(\n                        **{self.channel_names[-1]: f\"{channel}_{axis}_{method}\"}, verbose=False\n                    )\n                except wt_exceptions.ValueError:\n                    pass  # may have some channels which fail, do so silently\n            return\n        # get axis index --------------------------------------------------------------------------\n        if isinstance(axis, int):\n            axis_index = axis\n        elif isinstance(axis, str):\n            index = self.axis_names.index(axis)\n            axes = [i for i in range(self.ndim) if self.axes[index].shape[i] > 1]\n            if len(axes) > 1:\n                raise wt_exceptions.MultidimensionalAxisError(axis, \"collapse\")\n            elif len(axes) == 0:\n                raise wt_exceptions.ValueError(\n                    \"Axis {} is a single point, cannot collapse\".format(axis)\n                )\n            axis_index = axes[0]\n        else:\n            raise wt_exceptions.TypeError(\"axis: expected {int, str}, got %s\" % type(axis))\n\n        new_shape = list(self.shape)\n        new_shape[axis_index] = 1\n        func = {\n            \"sum\": np.nansum,\n            \"max\": np.nanmax,\n            \"maximum\": np.nanmax,\n            \"min\": np.nanmin,\n            \"minimum\": np.nanmin,\n            \"ave\": np.nanmean,\n            \"average\": np.nanmean,\n            \"mean\": np.nanmean,\n        }\n\n        # methods ---------------------------------------------------------------------------------\n        if isinstance(method, str):\n            methods = [method for _ in self.channels]\n        if isinstance(method, list):\n            if len(method) == len(self.channels):\n                methods = method\n            else:\n                raise wt_exceptions.ValueError(\n                    \"method argument must have same number of elements as there are channels\"\n                )\n            for m in methods:\n                if m not in func.keys():\n                    raise wt_exceptions.ValueError(\"method '{}' not recognized\".format(m))\n\n        warnings.warn(\"collapse\", category=wt_exceptions.EntireDatasetInMemoryWarning)\n\n        # collapse --------------------------------------------------------------------------------\n        for method, channel in zip(methods, self.channel_names):\n            if method is None:\n                continue\n\n            if self[channel].shape[axis_index] == 1:\n                continue  # Cannot collapse any further, don't clutter data object\n\n            new_shape = list(self[channel].shape)\n            new_shape[axis_index] = 1\n            rtype = self[channel].dtype\n            if method in [\"ave\", \"average\", \"mean\"]:\n                rtype = np.result_type(self[channel].dtype, float)\n\n            new = self.create_channel(\n                \"{}_{}_{}\".format(channel, axis, method),\n                values=np.empty(new_shape, dtype=rtype),\n                units=self[channel].units,\n            )\n\n            new[:] = func[method](self[channel], axis=axis_index, keepdims=True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef convert(self, destination_units, *, convert_variables=False, verbose=True):\n        # get kind of units\n        units_kind = wt_units.kind(destination_units)\n        # apply to all compatible axes\n        for axis in self.axes:\n            if axis.units_kind == units_kind:\n                orig = axis.units\n                axis.convert(destination_units, convert_variables=convert_variables)\n                if verbose:\n                    print(\n                        \"axis {} converted from {} to {}\".format(\n                            axis.expression, orig, destination_units\n                        )\n                    )\n        # apply to all compatible constants\n        for constant in self.constants:\n            if constant.units_kind == units_kind:\n                orig = constant.units\n                constant.convert(destination_units, convert_variables=convert_variables)\n                if verbose:\n                    print(\n                        \"constant {} converted from {} to {}\".format(\n                            constant.expression, orig, destination_units\n                        )\n                    )\n        if convert_variables:\n            for var in self.variables:\n                if wt_units.kind(var.units) == units_kind:\n                    orig = var.units\n                    var.convert(destination_units)\n                    if verbose:\n                        print(\n                            \"variable {} converted from {} to {}\".format(\n                                var.natural_name, orig, destination_units\n                            )\n                        )\n        self._on_axes_updated()\n        self._on_constants_updated()", "response": "Convert all compatable axes and constants to given units."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_channel(\n        self, name, values=None, *, shape=None, units=None, dtype=None, **kwargs\n    ) -> Channel:\n        \"\"\"Append a new channel.\n\n        Parameters\n        ----------\n        name : string\n            Unique name for this channel.\n        values : array (optional)\n            Array. If None, an empty array equaling the data shape is\n            created. Default is None.\n        shape : tuple of int\n            Shape to use. Must broadcast with the full shape.\n            Only used if `values` is None.\n            Default is the full shape of self.\n        units : string (optional)\n            Channel units. Default is None.\n        dtype : numpy.dtype (optional)\n            dtype to use for dataset, default is np.float64.\n            Only used if `values` is None.\n        kwargs : dict\n            Additional keyword arguments passed to Channel instantiation.\n\n        Returns\n        -------\n        Channel\n            Created channel.\n        \"\"\"\n        if name in self.channel_names:\n            warnings.warn(name, wt_exceptions.ObjectExistsWarning)\n            return self[name]\n        elif name in self.variable_names:\n            raise wt_exceptions.NameNotUniqueError(name)\n\n        require_kwargs = {\"chunks\": True}\n        if values is None:\n            if shape is None:\n                require_kwargs[\"shape\"] = self.shape\n            else:\n                require_kwargs[\"shape\"] = shape\n            if dtype is None:\n                require_kwargs[\"dtype\"] = np.dtype(np.float64)\n            else:\n                require_kwargs[\"dtype\"] = dtype\n            if require_kwargs[\"dtype\"].kind in \"fcmM\":\n                require_kwargs[\"fillvalue\"] = np.nan\n            else:\n                require_kwargs[\"fillvalue\"] = 0\n        else:\n            require_kwargs[\"data\"] = values\n            require_kwargs[\"shape\"] = values.shape\n            require_kwargs[\"dtype\"] = values.dtype\n        if np.prod(require_kwargs[\"shape\"]) == 1:\n            require_kwargs[\"chunks\"] = None\n        # create dataset\n        dataset_id = self.require_dataset(name=name, **require_kwargs).id\n        channel = Channel(self, dataset_id, units=units, **kwargs)\n        # finish\n        self.attrs[\"channel_names\"] = np.append(self.attrs[\"channel_names\"], name.encode())\n        return channel", "response": "Creates a new channel with the specified parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_variable(\n        self, name, values=None, *, shape=None, units=None, dtype=None, **kwargs\n    ) -> Variable:\n        \"\"\"Add new child variable.\n\n        Parameters\n        ----------\n        name : string\n            Unique identifier.\n        values : array-like (optional)\n            Array to populate variable with. If None, an variable will be filled with NaN.\n            Default is None.\n        shape : tuple of int\n            Shape to use. must broadcast with the full shape.\n            Only used if `values` is None.\n            Default is the full shape of self.\n        units : string (optional)\n            Variable units. Default is None.\n        dtype : numpy.dtype (optional)\n            dtype to use for dataset, default is np.float64.\n            Only used if `values` is None.\n        kwargs\n            Additional kwargs to variable instantiation.\n\n        Returns\n        -------\n        WrightTools Variable\n            New child variable.\n        \"\"\"\n        if name in self.variable_names:\n            warnings.warn(name, wt_exceptions.ObjectExistsWarning)\n            return self[name]\n        elif name in self.channel_names:\n            raise wt_exceptions.NameNotUniqueError(name)\n        if values is None:\n            if shape is None:\n                shape = self.shape\n            if dtype is None:\n                dtype = np.dtype(np.float64)\n            if dtype.kind in \"fcmM\":\n                fillvalue = np.nan\n            else:\n                fillvalue = 0\n        else:\n            shape = values.shape\n            dtype = values.dtype\n            fillvalue = None\n        # create dataset\n        id = self.require_dataset(\n            name=name, data=values, shape=shape, dtype=dtype, fillvalue=fillvalue\n        ).id\n        variable = Variable(self, id, units=units, **kwargs)\n        # finish\n        self._variables = None\n        self.attrs[\"variable_names\"] = np.append(self.attrs[\"variable_names\"], name.encode())\n        return variable", "response": "Create a new child variable."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the coordinates in units of the minimum in a channel.", "response": "def get_nadir(self, channel=0) -> tuple:\n        \"\"\"Get the coordinates, in units, of the minimum in a channel.\n\n        Parameters\n        ----------\n        channel : int or str (optional)\n            Channel. Default is 0.\n\n        Returns\n        -------\n        generator of numbers\n            Coordinates in units for each axis.\n        \"\"\"\n        # get channel\n        if isinstance(channel, int):\n            channel_index = channel\n        elif isinstance(channel, str):\n            channel_index = self.channel_names.index(channel)\n        else:\n            raise TypeError(\"channel: expected {int, str}, got %s\" % type(channel))\n        channel = self.channels[channel_index]\n        # get indicies\n        idx = channel.argmin()\n        # finish\n        return tuple(a[idx] for a in self._axes)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_zenith(self, channel=0) -> tuple:\n        # get channel\n        if isinstance(channel, int):\n            channel_index = channel\n        elif isinstance(channel, str):\n            channel_index = self.channel_names.index(channel)\n        else:\n            raise TypeError(\"channel: expected {int, str}, got %s\" % type(channel))\n        channel = self.channels[channel_index]\n        # get indicies\n        idx = channel.argmax()\n        # finish\n        return tuple(a[idx] for a in self._axes)", "response": "Get the coordinates in units of the maximum in a channel."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef heal(self, channel=0, method=\"linear\", fill_value=np.nan, verbose=True):\n        warnings.warn(\"heal\", category=wt_exceptions.EntireDatasetInMemoryWarning)\n        timer = wt_kit.Timer(verbose=False)\n        with timer:\n            # channel\n            if isinstance(channel, int):\n                channel_index = channel\n            elif isinstance(channel, str):\n                channel_index = self.channel_names.index(channel)\n            else:\n                raise TypeError(\"channel: expected {int, str}, got %s\" % type(channel))\n            channel = self.channels[channel_index]\n            values = self.channels[channel_index][:]\n            points = [axis[:] for axis in self._axes]\n            xi = tuple(np.meshgrid(*points, indexing=\"ij\"))\n            # 'undo' gridding\n            arr = np.zeros((len(self._axes) + 1, values.size))\n            for i in range(len(self._axes)):\n                arr[i] = xi[i].flatten()\n            arr[-1] = values.flatten()\n            # remove nans\n            arr = arr[:, ~np.isnan(arr).any(axis=0)]\n            # grid data wants tuples\n            tup = tuple([arr[i] for i in range(len(arr) - 1)])\n            # grid data\n            out = griddata(tup, arr[-1], xi, method=method, fill_value=fill_value)\n            self.channels[channel_index][:] = out\n        # print\n        if verbose:\n            print(\n                \"channel {0} healed in {1} seconds\".format(\n                    channel.name, np.around(timer.interval, decimals=3)\n                )\n            )", "response": "This method is used to remove nans from a channel using interpolation."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsubtracts the average value of npts at the edge of a given axis.", "response": "def level(self, channel, axis, npts, *, verbose=True):\n        \"\"\"Subtract the average value of npts at the edge of a given axis.\n\n        Parameters\n        ----------\n        channel : int or str\n            Channel to level.\n        axis : int\n            Axis to level along.\n        npts : int\n            Number of points to average for each slice. Positive numbers\n            take points at leading indicies and negative numbers take points\n            at trailing indicies.\n        verbose : bool (optional)\n            Toggle talkback. Default is True.\n        \"\"\"\n        warnings.warn(\"level\", category=wt_exceptions.EntireDatasetInMemoryWarning)\n        channel_index = wt_kit.get_index(self.channel_names, channel)\n        channel = self.channels[channel_index]\n        # verify npts not zero\n        npts = int(npts)\n        if npts == 0:\n            raise wt_exceptions.ValueError(\"npts must not be zero\")\n        # get subtrahend\n        ss = [slice(None)] * self.ndim\n        if npts > 0:\n            ss[axis] = slice(0, npts, None)\n        else:\n            ss[axis] = slice(npts, None, None)\n        subtrahend = np.nanmean(channel[ss], axis=axis)\n        if self.ndim > 1:\n            subtrahend = np.expand_dims(subtrahend, axis=axis)\n        # level\n        channel -= subtrahend\n        # finish\n        channel._null = 0\n        if verbose:\n            print(\"channel {0} leveled along axis {1}\".format(channel.natural_name, axis))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef map_variable(\n        self, variable, points, input_units=\"same\", *, name=None, parent=None, verbose=True\n    ) -> \"Data\":\n        \"\"\"Map points of an axis to new points using linear interpolation.\n\n        Out-of-bounds points are written nan.\n\n        Parameters\n        ----------\n        variable : string\n            The variable to map onto.\n        points : array-like or int\n            If array, the new points. If int, new points will have the same\n            limits, with int defining the number of evenly spaced points\n            between.\n        input_units : str (optional)\n            The units of the new points. Default is same, which assumes\n            the new points have the same units as the axis.\n        name : string (optional)\n            The name of the new data object. If None, generated from\n            natural_name. Default is None.\n        parent : WrightTools.Collection (optional)\n            Parent of new data object. If None, data is made at root of a\n            new temporary file.\n        verbose : bool (optional)\n            Toggle talkback. Default is True.\n\n        Returns\n        -------\n        WrightTools.Data\n            New data object.\n        \"\"\"\n        # get variable index\n        variable_index = wt_kit.get_index(self.variable_names, variable)\n        variable = self.variables[variable_index]\n        # get points\n        if isinstance(points, int):\n            points = np.linspace(variable.min(), variable.max(), points)\n        points = np.array(points)\n        # points dimensionality\n        if points.ndim < variable.ndim:\n            for i, d in enumerate(variable.shape):\n                if d == 1:\n                    points = np.expand_dims(points, axis=i)\n        # convert points\n        if input_units == \"same\":\n            pass\n        else:\n            points = wt_units.converter(points, input_units, variable.units)\n        # construct new data object\n        special = [\"name\", \"axes\", \"constants\", \"channel_names\", \"variable_names\"]\n        kwargs = {k: v for k, v in self.attrs.items() if k not in special}\n        if name is None:\n            name = \"{0}_{1}_mapped\".format(self.natural_name, variable.natural_name)\n        kwargs[\"name\"] = name\n        kwargs[\"parent\"] = parent\n        out = Data(**kwargs)\n        # mapped variable\n        values = points\n        out.create_variable(values=values, **variable.attrs)\n        # orthogonal variables\n        for v in self.variables:\n            if wt_kit.orthogonal(v.shape, variable.shape):\n                out.create_variable(values=v[:], **v.attrs)\n        out.transform(*self.axis_expressions)\n        # interpolate\n        if self.ndim == 1:\n\n            def interpolate(dataset, points):\n                function = scipy.interpolate.interp1d(variable[:], dataset[:], bounds_error=False)\n                return function(points)\n\n        else:\n            pts = np.array([a.full.flatten() for a in self.axes]).T\n            out_pts = np.array([a.full.flatten() for a in out.axes]).T\n\n            def interpolate(dataset, points):\n                values = dataset.full.flatten()\n                function = scipy.interpolate.LinearNDInterpolator(pts, values, rescale=True)\n                new = function(out_pts)\n                new.shape = out.shape\n                return new\n\n        for v in self.variables:\n            if v.natural_name not in out.variable_names:\n                out.create_variable(values=interpolate(v, points), **v.attrs)\n        out.variable_names = self.variable_names  # enforce old order\n        out._variables = None  # force regeneration of variables @property\n        for channel in self.channels:\n            out.create_channel(values=interpolate(channel, points), **channel.attrs)\n        # finish\n        if verbose:\n            print(\"data mapped from {0} to {1}\".format(self.shape, out.shape))\n        return out", "response": "Maps a variable onto a new set of points."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\noffsets one or more valid logical entries in the current object.", "response": "def offset(\n        self,\n        points,\n        offsets,\n        along,\n        offset_axis,\n        units=\"same\",\n        offset_units=\"same\",\n        mode=\"valid\",\n        method=\"linear\",\n        verbose=True,\n    ):\n        \"\"\"Offset one axis based on another axis' values.\n\n        Useful for correcting instrumental artifacts such as zerotune.\n\n        Parameters\n        ----------\n        points : 1D array-like\n            Points.\n        offsets : 1D array-like\n            Offsets.\n        along : str or int\n            Axis that points array lies along.\n        offset_axis : str or int\n            Axis to offset using offsets.\n        units : str (optional)\n            Units of points array.\n        offset_units : str (optional)\n            Units of offsets aray.\n        mode : {'valid', 'full', 'old'} (optional)\n            Define how far the new axis will extend. Points outside of valid\n            interpolation range will be written nan.\n        method : {'linear', 'nearest', 'cubic'} (optional)\n            The interpolation method. Note that cubic interpolation is only\n            possible for 1D and 2D data. See `griddata`__ for more information.\n            Default is linear.\n        verbose : bool (optional)\n            Toggle talkback. Default is True.\n\n\n        __ http://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.griddata.html\n\n            >>> points  # an array of w1 points\n            >>> offsets  # an array of d1 corrections\n            >>> data.offset(points, offsets, 'w1', 'd1')\n\n        \"\"\"\n        raise NotImplementedError\n        # axis ------------------------------------------------------------------------------------\n        if isinstance(along, int):\n            axis_index = along\n        elif isinstance(along, str):\n            axis_index = self.axis_names.index(along)\n        else:\n            raise TypeError(\"along: expected {int, str}, got %s\" % type(along))\n        axis = self._axes[axis_index]\n        # values & points -------------------------------------------------------------------------\n        # get values, points, units\n        if units == \"same\":\n            input_units = axis.units\n        else:\n            input_units = units\n        # check offsets is 1D or 0D\n        if len(offsets.shape) == 1:\n            pass\n        else:\n            raise RuntimeError(\"values must be 1D or 0D in offset!\")\n        # check if units is compatible, convert\n        dictionary = getattr(wt_units, axis.units_kind)\n        if input_units in dictionary.keys():\n            pass\n        else:\n            raise RuntimeError(\"units incompatible in offset!\")\n        points = wt_units.converter(points, input_units, axis.units)\n        # create correction array\n        function = interp1d(points, offsets, bounds_error=False)\n        corrections = function(axis[:])\n        # remove nans\n        finite_indicies = np.where(np.isfinite(corrections))[0]\n        left_pad_width = finite_indicies[0]\n        right_pad_width = len(corrections) - finite_indicies[-1] - 1\n        corrections = np.pad(\n            corrections[np.isfinite(corrections)],\n            (int(left_pad_width), int(right_pad_width)),\n            mode=\"edge\",\n        )\n        # do correction ---------------------------------------------------------------------------\n        # transpose so axis is last\n        transpose_order = np.arange(len(self._axes))\n        transpose_order[axis_index] = len(self._axes) - 1\n        transpose_order[-1] = axis_index\n        self.transpose(transpose_order, verbose=False)\n        # get offset axis index\n        if isinstance(offset_axis, int):\n            offset_axis_index = offset_axis\n        elif isinstance(offset_axis, str):\n            offset_axis_index = self.axis_names.index(offset_axis)\n        else:\n            raise TypeError(\"offset_axis: expected {int, str}, got %s\" % type(offset_axis))\n        # new points\n        new_points = [a[:] for a in self._axes]\n        old_offset_axis_points = self._axes[offset_axis_index][:]\n        spacing = abs(\n            (old_offset_axis_points.max() - old_offset_axis_points.min())\n            / float(len(old_offset_axis_points))\n        )\n        if mode == \"old\":\n            new_offset_axis_points = old_offset_axis_points\n        elif mode == \"valid\":\n            _max = old_offset_axis_points.max() + corrections.min()\n            _min = old_offset_axis_points.min() + corrections.max()\n            n = int(abs(np.ceil((_max - _min) / spacing)))\n            new_offset_axis_points = np.linspace(_min, _max, n)\n        elif mode == \"full\":\n            _max = old_offset_axis_points.max() + corrections.max()\n            _min = old_offset_axis_points.min() + corrections.min()\n            n = np.ceil((_max - _min) / spacing)\n            new_offset_axis_points = np.linspace(_min, _max, n)\n        new_points[offset_axis_index] = new_offset_axis_points\n        new_xi = tuple(np.meshgrid(*new_points, indexing=\"ij\"))\n        xi = tuple(np.meshgrid(*[a[:] for a in self._axes], indexing=\"ij\"))\n        for channel in self.channels:\n            # 'undo' gridding\n            arr = np.zeros((len(self._axes) + 1, channel[:].size))\n            for i in range(len(self._axes)):\n                arr[i] = xi[i].flatten()\n            arr[-1] = channel[:].flatten()\n            # do corrections\n            corrections = list(corrections)\n            corrections = corrections * int((len(arr[0]) / len(corrections)))\n            arr[offset_axis_index] += corrections\n            # grid data\n            tup = tuple([arr[i] for i in range(len(arr) - 1)])\n            # note that rescale is crucial in this operation\n            out = griddata(tup, arr[-1], new_xi, method=method, fill_value=np.nan, rescale=True)\n            channel[:] = out\n        self._axes[offset_axis_index][:] = new_offset_axis_points\n        # transpose out\n        self.transpose(transpose_order, verbose=False)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef print_tree(self, *, verbose=True):\n        print(\"{0} ({1})\".format(self.natural_name, self.filepath))\n        self._print_branch(\"\", depth=0, verbose=verbose)", "response": "Print a ascii - formatted tree representation of the data contents."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef prune(self, keep_channels=True, *, verbose=True):\n        for v in self.variables:\n            for var in wt_kit.flatten_list([ax.variables for ax in self._axes + self._constants]):\n                if v == var:\n                    break\n            else:\n                self.remove_variable(v.natural_name, implied=False, verbose=verbose)\n        if keep_channels is not True:\n            try:\n                if isinstance(keep_channels, str):\n                    raise TypeError\n                indexes = tuple(keep_channels)\n            except TypeError:\n                indexes = (keep_channels,)\n\n            for i, ch in enumerate(self.channels):\n                if i not in indexes and not ch.natural_name in indexes:\n                    self.remove_channel(ch.natural_name, verbose=verbose)", "response": "Removes unused variables and channels from the Data object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef remove_channel(self, channel, *, verbose=True):\n        channel_index = wt_kit.get_index(self.channel_names, channel)\n        new = list(self.channel_names)\n        name = new.pop(channel_index)\n        del self[name]\n        self.channel_names = new\n        if verbose:\n            print(\"channel {0} removed\".format(name))", "response": "Removes a channel from the data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nremove a variable from the data.", "response": "def remove_variable(self, variable, *, implied=True, verbose=True):\n        \"\"\"Remove variable from data.\n\n        Parameters\n        ----------\n        variable : int or str\n            Variable index or name to remove.\n        implied : boolean (optional)\n            Toggle deletion of other variables that start with the same\n            name. Default is True.\n        verbose : boolean (optional)\n            Toggle talkback. Default is True.\n        \"\"\"\n        if isinstance(variable, int):\n            variable = self.variable_names[variable]\n        # find all of the implied variables\n        removed = []\n        if implied:\n            for n in self.variable_names:\n                if n.startswith(variable):\n                    removed.append(n)\n        else:\n            removed = [variable]\n        # check that axes will not be ruined\n        for n in removed:\n            for a in self._axes:\n                if n in [v.natural_name for v in a.variables]:\n                    message = \"{0} is contained in axis {1}\".format(n, a.expression)\n                    raise RuntimeError(message)\n            for c in self._constants:\n                if n in [v.natural_name for v in c.variables]:\n                    warnings.warn(\n                        \"Variable being removed used in a constant\",\n                        wt_exceptions.WrightToolsWarning,\n                    )\n\n        # do removal\n        for n in removed:\n            variable_index = wt_kit.get_index(self.variable_names, n)\n            new = list(self.variable_names)\n            name = new.pop(variable_index)\n            del self[name]\n            self.variable_names = new\n            self._variables = None\n        # finish\n        if verbose:\n            print(\"{0} variable(s) removed:\".format(len(removed)))\n            for n in removed:\n                print(\"  {0}\".format(n))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrename a set of channels.", "response": "def rename_channels(self, *, verbose=True, **kwargs):\n        \"\"\"Rename a set of channels.\n\n        Parameters\n        ----------\n        kwargs\n            Keyword arguments of the form current:'new'.\n        verbose : boolean (optional)\n            Toggle talkback. Default is True\n        \"\"\"\n        # ensure that items will remain unique\n        changed = kwargs.keys()\n        for k, v in kwargs.items():\n            if v not in changed and v in self.keys():\n                raise wt_exceptions.NameNotUniqueError(v)\n        # compile references to items that are changing\n        new = {}\n        for k, v in kwargs.items():\n            obj = self[k]\n            index = self.channel_names.index(k)\n            # rename\n            new[v] = obj, index\n            Group._instances.pop(obj.fullpath, None)\n            obj.natural_name = str(v)\n            # remove old references\n            del self[k]\n        # apply new references\n        names = list(self.channel_names)\n        for v, value in new.items():\n            obj, index = value\n            self[v] = obj\n            names[index] = v\n        self.channel_names = names\n        # finish\n        if verbose:\n            print(\"{0} channel(s) renamed:\".format(len(kwargs)))\n            for k, v in kwargs.items():\n                print(\"  {0} --> {1}\".format(k, v))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rename_variables(self, *, implied=True, verbose=True, **kwargs):\n        # find all of the implied variables\n        kwargs = collections.OrderedDict(kwargs)\n        if implied:\n            new = collections.OrderedDict()\n            for k, v in kwargs.items():\n                for n in self.variable_names:\n                    if n.startswith(k):\n                        new[n] = n.replace(k, v, 1)\n            kwargs = new\n        # ensure that items will remain unique\n        changed = kwargs.keys()\n        for k, v in kwargs.items():\n            if v not in changed and v in self.keys():\n                raise wt_exceptions.NameNotUniqueError(v)\n        # compile references to items that are changing\n        new = {}\n        for k, v in kwargs.items():\n            obj = self[k]\n            index = self.variable_names.index(k)\n            # rename\n            new[v] = obj, index\n            Group._instances.pop(obj.fullpath, None)\n            obj.natural_name = str(v)\n            # remove old references\n            del self[k]\n        # apply new references\n        names = list(self.variable_names)\n        for v, value in new.items():\n            obj, index = value\n            self[v] = obj\n            names[index] = v\n        self.variable_names = names\n        units = self.units\n        new = list(self.axis_expressions)\n        for i, v in enumerate(kwargs.keys()):\n            for j, n in enumerate(new):\n                new[j] = n.replace(v, \"{%i}\" % i)\n        for i, n in enumerate(new):\n            new[i] = n.format(*kwargs.values())\n        self.transform(*new)\n        for a, u in zip(self._axes, units):\n            a.convert(u)\n        units = self.constant_units\n        new = list(self.constant_expressions)\n        for i, v in enumerate(kwargs.keys()):\n            for j, n in enumerate(new):\n                new[j] = n.replace(v, \"{%i}\" % i)\n        for i, n in enumerate(new):\n            new[i] = n.format(*kwargs.values())\n        self.set_constants(*new)\n        for c, u in zip(self._constants, units):\n            c.convert(u)\n        # finish\n        if verbose:\n            print(\"{0} variable(s) renamed:\".format(len(kwargs)))\n            for k, v in kwargs.items():\n                print(\"  {0} --> {1}\".format(k, v))", "response": "Rename a set of variables."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nshares not - a - numbers between all channels.", "response": "def share_nans(self):\n        \"\"\"Share not-a-numbers between all channels.\n\n        If any channel is nan at a given index, all channels will be nan\n        at that index after this operation.\n\n        Uses the share_nans method found in wt.kit.\n        \"\"\"\n\n        def f(_, s, channels):\n            outs = wt_kit.share_nans(*[c[s] for c in channels])\n            for c, o in zip(channels, outs):\n                c[s] = o\n\n        self.channels[0].chunkwise(f, self.channels)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsmooth a channel using an n - dimenional kaiser window.", "response": "def smooth(self, factors, channel=None, verbose=True) -> \"Data\":\n        \"\"\"Smooth a channel using an n-dimenional kaiser window.\n\n        Note, all arrays are loaded into memory.\n\n        For more info see `Kaiser_window`__ wikipedia entry.\n\n        __ https://en.wikipedia.org/wiki/Kaiser_window\n\n        Parameters\n        ----------\n        factors : int or list of int\n            The smoothing factor. You may provide a list of smoothing factors\n            for each axis.\n        channel : int or str or None (optional)\n            The channel to smooth. If None, all channels will be smoothed.\n            Default is None.\n        verbose : bool (optional)\n            Toggle talkback. Default is True.\n        \"\"\"\n        warnings.warn(\"smooth\", category=wt_exceptions.EntireDatasetInMemoryWarning)\n        # get factors -----------------------------------------------------------------------------\n\n        if isinstance(factors, list):\n            pass\n        else:\n            dummy = np.zeros(len(self._axes))\n            dummy[::] = factors\n            factors = list(dummy)\n        # get channels ----------------------------------------------------------------------------\n        if channel is None:\n            channels = self.channels\n        else:\n            if isinstance(channel, int):\n                channel_index = channel\n            elif isinstance(channel, str):\n                channel_index = self.channel_names.index(channel)\n            else:\n                raise TypeError(\"channel: expected {int, str}, got %s\" % type(channel))\n            channels = [self.channels[channel_index]]\n        # smooth ----------------------------------------------------------------------------------\n        for channel in channels:\n            values = channel[:]\n            for axis_index in range(len(factors)):\n                factor = factors[axis_index]\n                # transpose so the axis of interest is last\n                transpose_order = range(len(values.shape))\n                # replace axis_index with zero\n                transpose_order = [\n                    len(values.shape) - 1 if i == axis_index else i for i in transpose_order\n                ]\n                transpose_order[len(values.shape) - 1] = axis_index\n                values = values.transpose(transpose_order)\n                # get kaiser window\n                beta = 5.0\n                w = np.kaiser(2 * factor + 1, beta)\n                # for all slices...\n                for index in np.ndindex(values[..., 0].shape):\n                    current_slice = values[index]\n                    temp_slice = np.pad(current_slice, int(factor), mode=str(\"edge\"))\n                    values[index] = np.convolve(temp_slice, w / w.sum(), mode=str(\"valid\"))\n                # transpose out\n                values = values.transpose(transpose_order)\n            # return array to channel object\n            channel[:] = values\n        if verbose:\n            print(\"smoothed data\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsplit the data object along a given expression in units.", "response": "def split(\n        self, expression, positions, *, units=None, parent=None, verbose=True\n    ) -> wt_collection.Collection:\n        \"\"\"\n        Split the data object along a given expression, in units.\n\n        Parameters\n        ----------\n        expression : int or str\n            The expression to split along. If given as an integer, the axis at that index\n            is used.\n        positions : number-type or 1D array-type\n            The position(s) to split at, in units.\n        units : str (optional)\n            The units of the given positions. Default is same, which assumes\n            input units are identical to first variable units.\n        parent : WrightTools.Collection (optional)\n            The parent collection in which to place the 'split' collection.\n            Default is a new Collection.\n        verbose : bool (optional)\n            Toggle talkback. Default is True.\n\n        Returns\n        -------\n        WrightTools.collection.Collection\n            A Collection of data objects.\n            The order of the objects is such that the axis points retain their original order.\n\n        See Also\n        --------\n        chop\n            Divide the dataset into its lower-dimensionality components.\n        collapse\n            Collapse the dataset along one axis.\n        \"\"\"\n        # axis ------------------------------------------------------------------------------------\n        old_expr = self.axis_expressions\n        old_units = self.units\n        out = wt_collection.Collection(name=\"split\", parent=parent)\n        if isinstance(expression, int):\n            if units is None:\n                units = self._axes[expression].units\n            expression = self._axes[expression].expression\n        elif isinstance(expression, str):\n            pass\n        else:\n            raise TypeError(\"expression: expected {int, str}, got %s\" % type(expression))\n\n        self.transform(expression)\n        if units:\n            self.convert(units)\n\n        try:\n            positions = [-np.inf] + sorted(list(positions)) + [np.inf]\n        except TypeError:\n            positions = [-np.inf, positions, np.inf]\n\n        values = self._axes[0].full\n        masks = [(values >= lo) & (values < hi) for lo, hi in wt_kit.pairwise(positions)]\n        omasks = []\n        cuts = []\n        for mask in masks:\n            try:\n                omasks.append(wt_kit.mask_reduce(mask))\n                cuts.append([i == 1 for i in omasks[-1].shape])\n                # Ensure at least one axis is kept\n                if np.all(cuts[-1]):\n                    cuts[-1][0] = False\n            except ValueError:\n                omasks.append(None)\n                cuts.append(None)\n        for i in range(len(positions) - 1):\n            out.create_data(\"split%03i\" % i)\n\n        for var in self.variables:\n            for i, (imask, omask, cut) in enumerate(zip(masks, omasks, cuts)):\n                if omask is None:\n                    # Zero length split\n                    continue\n                omask = wt_kit.enforce_mask_shape(omask, var.shape)\n                omask.shape = tuple([s for s, c in zip(omask.shape, cut) if not c])\n                out_arr = np.full(omask.shape, np.nan)\n                imask = wt_kit.enforce_mask_shape(imask, var.shape)\n                out_arr[omask] = var[:][imask]\n                out[i].create_variable(values=out_arr, **var.attrs)\n\n        for ch in self.channels:\n            for i, (imask, omask, cut) in enumerate(zip(masks, omasks, cuts)):\n                if omask is None:\n                    # Zero length split\n                    continue\n                omask = wt_kit.enforce_mask_shape(omask, ch.shape)\n                omask.shape = tuple([s for s, c in zip(omask.shape, cut) if not c])\n                out_arr = np.full(omask.shape, np.nan)\n                imask = wt_kit.enforce_mask_shape(imask, ch.shape)\n                out_arr[omask] = ch[:][imask]\n                out[i].create_channel(values=out_arr, **ch.attrs)\n\n        if verbose:\n            for d in out.values():\n                try:\n                    d.transform(expression)\n                except IndexError:\n                    continue\n\n            print(\"split data into {0} pieces along <{1}>:\".format(len(positions) - 1, expression))\n            for i, (lo, hi) in enumerate(wt_kit.pairwise(positions)):\n                new_data = out[i]\n                if new_data.shape == ():\n                    print(\"  {0} : None\".format(i))\n                else:\n                    new_axis = new_data.axes[0]\n                    print(\n                        \"  {0} : {1:0.2f} to {2:0.2f} {3} {4}\".format(\n                            i, lo, hi, new_axis.units, new_axis.shape\n                        )\n                    )\n\n        for d in out.values():\n            try:\n                d.transform(*old_expr)\n                keep = []\n                keep_units = []\n                for ax in d.axes:\n                    if ax.size > 1:\n                        keep.append(ax.expression)\n                        keep_units.append(ax.units)\n                    else:\n                        d.create_constant(ax.expression, verbose=False)\n                d.transform(*keep)\n                for ax, u in zip(d.axes, keep_units):\n                    ax.convert(u)\n            except IndexError:\n                continue\n            tempax = Axis(d, expression)\n            if all(\n                np.all(\n                    np.sum(~np.isnan(tempax.masked), axis=tuple(set(range(tempax.ndim)) - {j}))\n                    <= 1\n                )\n                for j in range(tempax.ndim)\n            ):\n                d.create_constant(expression, verbose=False)\n        self.transform(*old_expr)\n        for ax, u in zip(self.axes, old_units):\n            ax.convert(u)\n\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef transform(self, *axes, verbose=True):\n        # TODO: ensure that transform does not break data\n        # create\n        new = []\n        newt = \"newt\" in self.axis_expressions\n        current = {a.expression: a for a in self._axes}\n        for expression in axes:\n            axis = current.get(expression, Axis(self, expression))\n            new.append(axis)\n        self._axes = new\n        # units\n        for a in self._axes:\n            if a.units is None:\n                a.convert(a.variables[0].units)\n        # finish\n        self.flush()\n        self._on_axes_updated()\n        nownewt = \"newt\" in self.axis_expressions\n        if verbose and nownewt and not newt:\n            print(\"Look she turned me into a newt\")\n        elif verbose and newt and not nownewt:\n            print(\"I got better\")", "response": "Transform the data.\n\n        Parameters\n        ----------\n        axes : strings\n            Expressions for the new set of axes.\n        verbose : boolean (optional)\n            Toggle talkback. Default is True\n\n        See Also\n        --------\n        set_constants\n            Similar method except for constants"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the constants associated with the data.", "response": "def set_constants(self, *constants, verbose=True):\n        \"\"\"Set the constants associated with the data.\n\n        Parameters\n        ----------\n        constants : str\n            Expressions for the new set of constants.\n        verbose : boolean (optional)\n            Toggle talkback. Default is True\n\n        See Also\n        --------\n        transform\n            Similar method except for axes.\n        create_constant\n            Add an individual constant.\n        remove_constant\n            Remove an individual constant.\n        \"\"\"\n        # create\n        new = []\n        current = {c.expression: c for c in self._constants}\n        for expression in constants:\n            constant = current.get(expression, Constant(self, expression))\n            new.append(constant)\n        self._constants = new\n        # units\n        for c in self._constants:\n            if c.units is None:\n                c.convert(c.variables[0].units)\n        # finish\n        self.flush()\n        self._on_constants_updated()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a new constant in the list.", "response": "def create_constant(self, expression, *, verbose=True):\n        \"\"\"Append a constant to the stored list.\n\n        Parameters\n        ----------\n        expression : str\n            Expression for the new constant.\n        verbose : boolean (optional)\n            Toggle talkback. Default is True\n            \n        See Also\n        --------\n        set_constants\n            Remove and replace all constants.\n        remove_constant\n            Remove an individual constant.\n        \"\"\"\n        if expression in self.constant_expressions:\n            wt_exceptions.ObjectExistsWarning.warn(expression)\n            return self.constants[self.constant_expressions.index(expression)]\n        constant = Constant(self, expression)\n        if constant.units is None:\n            constant.convert(constant.variables[0].units)\n        self._constants.append(constant)\n        self.flush()\n        self._on_constants_updated()\n        if verbose:\n            print(\"Constant '{}' added\".format(constant.expression))\n        return constant"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef remove_constant(self, constant, *, verbose=True):\n        if isinstance(constant, (str, int)):\n            constant_index = wt_kit.get_index(self.constant_expressions, constant)\n        elif isinstance(constant, Constant):\n            constant_index = wt_kit.get_index(self.constants, constant)\n        constant = self._constants[constant_index]\n        self._constants.pop(constant_index)\n        self.flush()\n        self._on_constants_updated()\n        if verbose:\n            print(\"Constant '{}' removed\".format(constant.expression))", "response": "Removes a constant from the list."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef zoom(self, factor, order=1, verbose=True):\n        raise NotImplementedError\n        import scipy.ndimage\n\n        # axes\n        for axis in self._axes:\n            axis[:] = scipy.ndimage.interpolation.zoom(axis[:], factor, order=order)\n        # channels\n        for channel in self.channels:\n            channel[:] = scipy.ndimage.interpolation.zoom(channel[:], factor, order=order)\n        # return\n        if verbose:\n            print(\"data zoomed to new shape:\", self.shape)", "response": "Zoom the data array using spline interpolation of the requested order."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the full path matching a name.", "response": "def get_path_matching(name):\n    \"\"\"Get path matching a name.\n\n    Parameters\n    ----------\n    name : string\n        Name to search for.\n\n    Returns\n    -------\n    string\n        Full filepath.\n    \"\"\"\n    # first try looking in the user folder\n    p = os.path.join(os.path.expanduser(\"~\"), name)\n    # then try expanding upwards from cwd\n    if not os.path.isdir(p):\n        p = None\n        drive, folders = os.path.splitdrive(os.getcwd())\n        folders = folders.split(os.sep)\n        folders.insert(0, os.sep)\n        if name in folders:\n            p = os.path.join(drive, *folders[: folders.index(name) + 1])\n    # TODO: something more robust to catch the rest of the cases?\n    return p"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of all files matching the specified inputs.", "response": "def glob_handler(extension, folder=None, identifier=None):\n    \"\"\"Return a list of all files matching specified inputs.\n\n    Parameters\n    ----------\n    extension : string\n        File extension.\n    folder : string (optional)\n        Folder to search within. Default is None (current working\n        directory).\n    identifier : string\n        Unique identifier. Default is None.\n\n    Returns\n    -------\n    list of strings\n        Full path of matching files.\n    \"\"\"\n    filepaths = []\n    if folder:\n        # comment out [ and ]...\n        folder = folder.replace(\"[\", \"?\")\n        folder = folder.replace(\"]\", \"*\")\n        folder = folder.replace(\"?\", \"[[]\")\n        folder = folder.replace(\"*\", \"[]]\")\n        glob_str = os.path.join(folder, \"*\" + extension)\n    else:\n        glob_str = \"*\" + extension + \"*\"\n    for filepath in glob.glob(glob_str):\n        if identifier:\n            if identifier in filepath:\n                filepaths.append(filepath)\n        else:\n            filepaths.append(filepath)\n    return filepaths"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef minor_extent(self) -> complex:\n        return min((self.max() - self.null, self.null - self.min()))", "response": "Maximum deviation from null."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef normalize(self, mag=1.):\n\n        def f(dataset, s, null, mag):\n            dataset[s] -= null\n            dataset[s] /= mag\n\n        if self.signed:\n            mag = self.mag() / mag\n        else:\n            mag = self.max() / mag\n        self.chunkwise(f, null=self.null, mag=mag)\n        self._null = 0", "response": "Normalize a Channel set null to 0 and the mag to given value."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove outliers from the dataset by comparing each point to its neighborhood mean and standard deviation.", "response": "def trim(self, neighborhood, method=\"ztest\", factor=3, replace=\"nan\", verbose=True):\n        \"\"\"Remove outliers from the dataset.\n\n        Identifies outliers by comparing each point to its\n        neighbors using a statistical test.\n\n        Parameters\n        ----------\n        neighborhood : list of integers\n            Size of the neighborhood in each dimension. Length of the list must\n            be equal to the dimensionality of the channel.\n        method : {'ztest'} (optional)\n            Statistical test used to detect outliers. Default is ztest.\n\n            ztest\n                Compare point deviation from neighborhood mean to neighborhood\n                standard deviation.\n\n        factor : number (optional)\n            Tolerance factor.  Default is 3.\n        replace : {'nan', 'mean', 'exclusive_mean', number} (optional)\n            Behavior of outlier replacement. Default is nan.\n\n            nan\n                Outliers are replaced by numpy nans.\n\n            mean\n                Outliers are replaced by the mean of its neighborhood, including itself.\n\n            exclusive_mean\n                Outilers are replaced by the mean of its neighborhood, not including itself.\n\n            number\n                Array becomes given number.\n\n        Returns\n        -------\n        list of tuples\n            Indicies of trimmed outliers.\n\n        See Also\n        --------\n        clip\n            Remove pixels outside of a certain range.\n        \"\"\"\n        warnings.warn(\"trim\", category=wt_exceptions.EntireDatasetInMemoryWarning)\n        outliers = []\n        means = []\n        ex_means = []\n        # find outliers\n        for idx in np.ndindex(self.shape):\n            slices = []\n            for i, di, size in zip(idx, neighborhood, self.shape):\n                start = max(0, i - di)\n                stop = min(size, i + di + 1)\n                slices.append(slice(start, stop, 1))\n            neighbors = self[slices]\n            mean = np.nanmean(neighbors)\n            sum_ = np.nansum(neighbors)\n            limit = np.nanstd(neighbors) * factor\n            if np.abs(self[idx] - mean) > limit:\n                outliers.append(idx)\n                means.append(mean)\n                # Note, \"- 1\" is to exclude the point itself, which is not nan, in order\n                # to enter this if block, as `np.abs(nan - mean)` is nan, which would\n                # evaluate to False\n                ex_means.append((sum_ - self[idx]) / (np.sum(~np.isnan(neighbors)) - 1))\n\n        # replace outliers\n        i = tuple(zip(*outliers))\n\n        if len(i) == 0:\n            if verbose:\n                print(\"No outliers found\")\n            return []\n\n        replace = {\"nan\": np.nan, \"mean\": means, \"exclusive_mean\": ex_means}.get(replace, replace)\n\n        # This may someday be available in h5py directly, but seems that day is not yet.\n        # This is annoying because it is the only reason we hold the whole set in memory.\n        # KFS 2019-03-21\n        arr = self[:]\n        arr[i] = replace\n        self[:] = arr\n\n        # finish\n        if verbose:\n            print(\"%i outliers removed\" % len(outliers))\n        return outliers"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_spcm(filepath, name=None, *, delimiter=\",\", parent=None, verbose=True) -> Data:\n    filestr = os.fspath(filepath)\n    filepath = pathlib.Path(filepath)\n\n    # check filepath\n    if not \".asc\" in filepath.suffixes:\n        wt_exceptions.WrongFileTypeWarning.warn(filepath, \".asc\")\n    # parse name\n    if not name:\n        name = filepath.name.split(\".\")[0]\n    # create headers dictionary\n    headers = collections.OrderedDict()\n    header_lines = 0\n    ds = np.DataSource(None)\n    f = ds.open(filestr, \"rt\")\n    while True:\n        line = f.readline().strip()\n        header_lines += 1\n        if len(line) == 0:\n            break\n        else:\n            key, value = line.split(\":\", 1)\n            if key.strip() == \"Revision\":\n                headers[\"resolution\"] = int(value.strip(\" bits ADC\"))\n            else:\n                headers[key.strip()] = value.strip()\n    line = f.readline().strip()\n    while \"_BEGIN\" in line:\n        header_lines += 1\n        section = line.split(\"_BEGIN\")[0]\n        while True:\n            line = f.readline().strip()\n            header_lines += 1\n            if section + \"_END\" in line:\n                break\n            if section == \"SYS_PARA\":\n                use_type = {\n                    \"B\": lambda b: int(b) == 1,\n                    \"C\": str,  # e.g. #SP [SP_OVERFL,C,N]\n                    \"F\": float,\n                    \"I\": int,\n                    \"L\": int,  # e.g. #DI [DI_MAXCNT,L,128]\n                    \"S\": str,\n                    \"U\": int,  # unsigned int?\n                }\n                item = line[line.find(\"[\") + 1 : line.find(\"]\")].split(\",\")\n                key = item[0]\n                value = use_type[item[1]](item[2])\n                headers[key] = value\n            else:\n                splitted = line.split()\n                value = splitted[-1][1:-1].split(\",\")\n                key = \" \".join(splitted[:-1])\n                headers[key] = value\n        line = f.readline().strip()\n        if \"END\" in line:\n            header_lines += 1\n            break\n    if \"Date\" in headers.keys() and \"Time\" in headers.keys():\n        # NOTE:  reports created in local time, no-way to calculate absolute time\n        created = \" \".join([headers[\"Date\"], headers[\"Time\"]])\n        created = time.strptime(created, \"%Y-%m-%d %H:%M:%S\")\n        created = timestamp.TimeStamp(time.mktime(created)).RFC3339\n        headers[\"created\"] = created\n\n    # initialize data object\n    kwargs = {\"name\": name, \"kind\": \"spcm\", \"source\": filestr, **headers}\n    if parent:\n        data = parent.create_data(**kwargs)\n    else:\n        data = Data(**kwargs)\n    # import data\n    f.seek(0)\n    arr = np.genfromtxt(\n        f, skip_header=(header_lines + 1), skip_footer=1, delimiter=delimiter, unpack=True\n    )\n    f.close()\n    # construct data\n    data.create_variable(name=\"time\", values=arr[0], units=\"ns\")\n    data.create_channel(name=\"counts\", values=arr[1])\n    data.transform(\"time\")\n    # finish\n    if verbose:\n        print(\"data created at {0}\".format(data.fullpath))\n        print(\"  kind: {0}\".format(data.kind))\n        print(\"  range: {0} to {1} (ns)\".format(data.time[0], data.time[-1]))\n        print(\"  size: {0}\".format(data.size))\n        if \"SP_COL_T\" in data.attrs.keys():\n            print(\"  collection time:  {0} sec\".format(data.attrs[\"SP_COL_T\"]))\n    return data", "response": "Create a new object from a Becker & Hickl spcm file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a data object from Andor Solis software.", "response": "def from_Solis(filepath, name=None, parent=None, verbose=True) -> Data:\n    \"\"\"Create a data object from Andor Solis software (ascii exports).\n\n    Parameters\n    ----------\n    filepath : path-like\n        Path to .txt file.\n        Can be either a local or remote file (http/ftp).\n        Can be compressed with gz/bz2, decompression based on file name.\n    name : string (optional)\n        Name to give to the created data object. If None, filename is used.\n        Default is None.\n    parent : WrightTools.Collection (optional)\n        Collection to place new data object within. Default is None.\n    verbose : boolean (optional)\n        Toggle talkback. Default is True.\n\n    Returns\n    -------\n    data\n        New data object.\n    \"\"\"\n    # parse filepath\n    filestr = os.fspath(filepath)\n    filepath = pathlib.Path(filepath)\n\n    if not \".asc\" in filepath.suffixes:\n        wt_exceptions.WrongFileTypeWarning.warn(filepath, \".asc\")\n    # parse name\n    if not name:\n        name = filepath.name.split(\".\")[0]\n    # create data\n    ds = np.DataSource(None)\n    f = ds.open(filestr, \"rt\")\n    axis0 = []\n    arr = []\n    attrs = {}\n    while True:\n        line = f.readline().strip()[:-1]\n        if len(line) == 0:\n            break\n        else:\n            line = line.split(\",\")\n            line = [float(x) for x in line]\n            axis0.append(line.pop(0))\n            arr.append(line)\n\n    i = 0\n    while i < 3:\n        line = f.readline().strip()\n        if len(line) == 0:\n            i += 1\n        else:\n            try:\n                key, val = line.split(\":\", 1)\n            except ValueError:\n                pass\n            else:\n                attrs[key.strip()] = val.strip()\n\n    f.close()\n    created = attrs[\"Date and Time\"]  # is this UTC?\n    created = time.strptime(created, \"%a %b %d %H:%M:%S %Y\")\n    created = timestamp.TimeStamp(time.mktime(created)).RFC3339\n\n    kwargs = {\"name\": name, \"kind\": \"Solis\", \"source\": filestr, \"created\": created}\n    if parent is None:\n        data = Data(**kwargs)\n    else:\n        data = parent.create_data(**kwargs)\n    arr = np.array(arr)\n    arr /= float(attrs[\"Exposure Time (secs)\"])\n    # signal has units of Hz because time normalized\n    arr = data.create_channel(name=\"signal\", values=arr, signed=False, units=\"Hz\")\n    axis0 = np.array(axis0)\n    if float(attrs[\"Grating Groove Density (l/mm)\"]) == 0:\n        xname = \"xindex\"\n        xunits = None\n    else:\n        xname = \"wm\"\n        xunits = \"nm\"\n    data.create_variable(name=xname, values=axis0[:, None], units=xunits)\n    data.create_variable(name=\"yindex\", values=np.arange(arr.shape[1])[None, :], units=None)\n    data.transform(data.variables[0].natural_name, \"yindex\")\n\n    for key, val in attrs.items():\n        data.attrs[key] = val\n\n    # finish\n    if verbose:\n        print(\"data created at {0}\".format(data.fullpath))\n        print(\"  axes: {0}\".format(data.axis_names))\n        print(\"  shape: {0}\".format(data.shape))\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the units of the object.", "response": "def units(self):\n        \"\"\"Units.\"\"\"\n        if \"units\" in self.attrs.keys():\n            # This try-except here for compatibility with v1.0.0 of WT5 format\n            try:\n                self.attrs[\"units\"] = self.attrs[\"units\"].decode()\n            except AttributeError:\n                pass  # already a string, not bytes\n            return self.attrs[\"units\"]\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nindexes of the maximum ignorning nans.", "response": "def argmax(self):\n        \"\"\"Index of the maximum, ignorning nans.\"\"\"\n        if \"argmax\" not in self.attrs.keys():\n\n            def f(dataset, s):\n                arr = dataset[s]\n                try:\n                    amin = np.nanargmax(arr)\n                except ValueError:\n                    amin = 0\n                idx = np.unravel_index(amin, arr.shape)\n                val = arr[idx]\n                return (tuple(i + (ss.start if ss.start else 0) for i, ss in zip(idx, s)), val)\n\n            chunk_res = self.chunkwise(f)\n            idxs = [i[0] for i in chunk_res.values()]\n            vals = [i[1] for i in chunk_res.values()]\n            self.attrs[\"argmax\"] = idxs[np.nanargmax(vals)]\n        return tuple(self.attrs[\"argmax\"])"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nexecutes a function for each chunk in the dataset.", "response": "def chunkwise(self, func, *args, **kwargs):\n        \"\"\"Execute a function for each chunk in the dataset.\n\n        Order of excecution is not guaranteed.\n\n        Parameters\n        ----------\n        func : function\n            Function to execute. First two arguments must be dataset,\n            slices.\n        args (optional)\n            Additional (unchanging) arguments passed to func.\n        kwargs (optional)\n            Additional (unchanging) keyword arguments passed to func.\n\n        Returns\n        -------\n        collections OrderedDict\n            Dictionary of index: function output. Index is to lowest corner\n            of each chunk.\n        \"\"\"\n        out = collections.OrderedDict()\n        for s in self.slices():\n            key = tuple(sss.start for sss in s)\n            out[key] = func(self, s, *args, **kwargs)\n        self._clear_array_attributes_cache()\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nclipping values outside of a defined range.", "response": "def clip(self, min=None, max=None, replace=np.nan):\n        \"\"\"Clip values outside of a defined range.\n\n        Parameters\n        ----------\n        min : number (optional)\n            New channel minimum. Default is None.\n        max : number (optional)\n            New channel maximum. Default is None.\n        replace : number or 'value' (optional)\n            Replace behavior. Default is nan.\n        \"\"\"\n        if max is None:\n            max = self.max()\n        if min is None:\n            min = self.min()\n\n        def f(dataset, s, min, max, replace):\n            if hasattr(min, \"shape\"):\n                min = min[wt_kit.valid_index(s, min.shape)]\n            if hasattr(max, \"shape\"):\n                max = max[wt_kit.valid_index(s, max.shape)]\n            if hasattr(replace, \"shape\"):\n                replace = replace[wt_kit.valid_index(s, replace.shape)]\n            arr = dataset[s]\n            if replace == \"value\":\n                dataset[s] = np.clip(arr, min, max)\n            else:\n                arr[arr < min] = replace\n                arr[arr > max] = replace\n                dataset[s] = arr\n\n        self.chunkwise(f, min=min, max=max, replace=replace)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts units. Parameters ---------- destination_units : string (optional) Units to convert into.", "response": "def convert(self, destination_units):\n        \"\"\"Convert units.\n\n        Parameters\n        ----------\n        destination_units : string (optional)\n            Units to convert into.\n        \"\"\"\n        if not wt_units.is_valid_conversion(self.units, destination_units):\n            kind = wt_units.kind(self.units)\n            valid = list(wt_units.dicts[kind].keys())\n            raise wt_exceptions.UnitsError(valid, destination_units)\n        if self.units is None:\n            return\n\n        def f(dataset, s, destination_units):\n            dataset[s] = wt_units.converter(dataset[s], dataset.units, destination_units)\n\n        self.chunkwise(f, destination_units=destination_units)\n        self.units = destination_units"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntaking the log of the entire dataset.", "response": "def log(self, base=np.e, floor=None):\n        \"\"\"Take the log of the entire dataset.\n\n        Parameters\n        ----------\n        base : number (optional)\n            Base of log. Default is e.\n        floor : number (optional)\n            Clip values below floor after log. Default is None.\n        \"\"\"\n\n        def f(dataset, s, base, floor):\n            arr = dataset[s]\n            arr = np.log(arr)\n            if base != np.e:\n                arr /= np.log(base)\n            if floor is not None:\n                arr[arr < floor] = floor\n            dataset[s] = arr\n\n        self.chunkwise(f, base=base, floor=floor)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntakes the log base 10 of the entire dataset.", "response": "def log10(self, floor=None):\n        \"\"\"Take the log base 10 of the entire dataset.\n\n        Parameters\n        ----------\n        floor : number (optional)\n            Clip values below floor after log. Default is None.\n        \"\"\"\n\n        def f(dataset, s, floor):\n            arr = dataset[s]\n            arr = np.log10(arr)\n            if floor is not None:\n                arr[arr < floor] = floor\n            dataset[s] = arr\n\n        self.chunkwise(f, floor=floor)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef slices(self):\n        if self.chunks is None:\n            yield tuple(slice(None, s) for s in self.shape)\n        else:\n            ceilings = tuple(-(-s // c) for s, c in zip(self.shape, self.chunks))\n            for idx in np.ndindex(ceilings):  # could also use itertools.product\n                out = []\n                for i, c, s in zip(idx, self.chunks, self.shape):\n                    start = i * c\n                    stop = min(start + c, s + 1)\n                    out.append(slice(start, stop, 1))\n                yield tuple(out)", "response": "Returns a generator yielding tuple of slice objects."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef join(\n    datas, *, atol=None, rtol=None, name=\"join\", parent=None, method=\"first\", verbose=True\n) -> Data:\n    \"\"\"Join a list of data objects together.\n\n    Joined datas must have the same transformation applied.\n    This transformation will define the array order for the joined dataset.\n    All axes in the applied transformation must be a single variable,\n    the result will have sorted numbers.\n\n    Join does not perform any interpolation.\n    For that, look to ``Data.map_variable`` or ``Data.heal``\n\n    Parameters\n    ----------\n    datas : list of data or WrightTools.Collection\n        The list or collection of data objects to join together.\n    atol : numeric or list of numeric\n        The absolute tolerance to use (in ``np.isclose``) to consider points overlapped.\n        If given as a single number, applies to all axes.\n        If given as a list, must have same length as the data transformation.\n        ``None`` in the list invokes default behavior.\n        Default is 10% of the minimum spacing between consecutive points in any\n        input data file.\n    rtol : numeric or list of numeric\n        The relative tolerance to use (in ``np.isclose``) to consider points overlapped.\n        If given as a single number, applies to all axes.\n        If given as a list, must have same length as the data transformation.\n        ``None`` in the list invokes default behavior.\n        Default is ``4 * np.finfo(dtype).resolution`` for floating point types,\n        ``0`` for integer types.\n    name : str (optional)\n        The name for the data object which is created. Default is 'join'.\n    parent : WrightTools.Collection (optional)\n        The location to place the joined data object. Default is new temp file at root.\n    method : {'first', 'last', 'min', 'max', 'sum', 'mean'}\n        Mode to use for merged points in the joined space.\n        Default is 'first'.\n    verbose : bool (optional)\n        Toggle talkback. Default is True.\n\n    Returns\n    -------\n    WrightTools.data.Data\n        A new Data instance.\n    \"\"\"\n    warnings.warn(\"join\", category=wt_exceptions.EntireDatasetInMemoryWarning)\n    if isinstance(datas, Collection):\n        datas = datas.values()\n    datas = list(datas)\n    if not isinstance(atol, collections.abc.Iterable):\n        atol = [atol] * len(datas[0].axes)\n    if not isinstance(rtol, collections.abc.Iterable):\n        rtol = [rtol] * len(datas[0].axes)\n    # check if variables are valid\n    axis_expressions = datas[0].axis_expressions\n    variable_names = set(datas[0].variable_names)\n    channel_names = set(datas[0].channel_names)\n    for d in datas[1:]:\n        if d.axis_expressions != axis_expressions:\n            raise wt_exceptions.ValueError(\"Joined data must have same axis_expressions\")\n        variable_names &= set(d.variable_names)\n        channel_names &= set(d.channel_names)\n    variable_names = list(variable_names)\n    channel_names = list(channel_names)\n    variable_units = []\n    channel_units = []\n    for v in variable_names:\n        variable_units.append(datas[0][v].units)\n    for c in channel_names:\n        channel_units.append(datas[0][c].units)\n    axis_variable_names = []\n    axis_variable_units = []\n    for a in datas[0].axes:\n        if len(a.variables) > 1:\n            raise wt_exceptions.ValueError(\"Applied transform must have single variable axes\")\n        for v in a.variables:\n            axis_variable_names.append(v.natural_name)\n            axis_variable_units.append(v.units)\n\n    vs = collections.OrderedDict()\n    for n, units, atol_, rtol_ in zip(axis_variable_names, axis_variable_units, atol, rtol):\n        dtype = np.result_type(*[d[n].dtype for d in datas])\n        if atol_ is None:\n            try:\n                # 10% of the minimum spacing between consecutive points in any singular input data\n                atol_ = min(np.min(np.abs(np.diff(d[n][:]))) for d in datas if d[n].size > 1) * 0.1\n            except ValueError:\n                atol_ = 0\n        if rtol_ is None:\n            # Ignore floating point precision rounding, if dtype is floting\n            rtol_ = 4 * np.finfo(dtype).resolution if dtype.kind in \"fcmM\" else 0\n        values = np.concatenate([d[n][:].flat for d in datas])\n        values = np.sort(values)\n        filtered = []\n        i = 0\n        # Filter out consecutive values that are \"close\"\n        while i < len(values):\n            sum_ = values[i]\n            count = 1\n            i += 1\n            if i < len(values):\n                while np.isclose(values[i - 1], values[i], atol=atol_, rtol=rtol_):\n                    sum_ += values[i]\n                    count += 1\n                    i += 1\n                    if i >= len(values):\n                        break\n            filtered.append(sum_ / count)\n        vs[n] = {\"values\": np.array(filtered), \"units\": units}\n    # TODO: the following should become a new from method\n\n    def from_dict(d, parent=None):\n        ndim = len(d)\n        i = 0\n        out = Data(name=name, parent=parent)\n        for k, v in d.items():\n            values = v[\"values\"]\n            shape = [1] * ndim\n            shape[i] = values.size\n            values.shape = tuple(shape)\n            # **attrs passes the name and units as well\n            out.create_variable(values=values, **datas[0][k].attrs)\n            i += 1\n        out.transform(*list(d.keys()))\n        return out\n\n    def get_shape(out, datas, item_name):\n        shape = [1] * out.ndim\n        for i, s in enumerate(out.shape):\n            idx = [np.argmax(d[out.axes[i].expression].shape) for d in datas]\n            if any(d[item_name].shape[j] != 1 for d, j in zip(datas, idx)) or all(\n                d[out.axes[i].expression].size == 1 for d in datas\n            ):\n                shape[i] = s\n        return shape\n\n    out = from_dict(vs, parent=parent)\n    count = {}\n    for channel_name in channel_names:\n        # **attrs passes the name and units as well\n        out.create_channel(\n            shape=get_shape(out, datas, channel_name),\n            **datas[0][channel_name].attrs,\n            dtype=datas[0][channel_name].dtype\n        )\n        count[channel_name] = np.zeros_like(out[channel_name], dtype=int)\n    for variable_name in variable_names:\n        if variable_name not in vs.keys():\n            # **attrs passes the name and units as well\n            out.create_variable(\n                shape=get_shape(out, datas, variable_name),\n                **datas[0][variable_name].attrs,\n                dtype=datas[0][variable_name].dtype\n            )\n            count[variable_name] = np.zeros_like(out[variable_name], dtype=int)\n\n    def combine(data, out, item_name, new_idx, transpose, slice_):\n        old = data[item_name]\n        new = out[item_name]\n        vals = np.empty_like(new)\n        # Default fill value based on whether dtype is floating or not\n        if vals.dtype.kind in \"fcmM\":\n            vals[:] = np.nan\n        else:\n            vals[:] = 0\n        # Use advanced indexing to populate vals, a temporary array with same shape as out\n        valid_index = tuple(wt_kit.valid_index(new_idx, new.shape))\n        vals[valid_index] = old[:].transpose(transpose)[slice_]\n\n        # Overlap methods are accomplished by adding the existing array with the one added\n        # for this particular data. Thus locations which should be set, but conflict by\n        # the method chosen are set to 0. Handling for floating point vs. integer types may vary.\n        # For floating types, nan indicates invalid, and must be explicitly allowed to add in.\n        if method == \"first\":\n            # Set any locations which have already been populated\n            vals[~np.isnan(new[:])] = 0\n            if not vals.dtype.kind in \"fcmM\":\n                vals[count[item_name] > 0] = 0\n        elif method == \"last\":\n            # Reset points which are to be overwritten\n            new[~np.isnan(vals)] = 0\n            if not vals.dtype.kind in \"fcmM\":\n                new[valid_index] = 0\n        elif method == \"min\":\n            rep_new = new > vals\n            rep_vals = vals > new\n            new[rep_new] = 0\n            vals[rep_vals] = 0\n        elif method == \"max\":\n            rep_new = new < vals\n            rep_vals = vals < new\n            new[rep_new] = 0\n            vals[rep_vals] = 0\n        # Ensure that previously NaN points which have values are written\n        new[np.isnan(new) & ~np.isnan(vals)] = 0\n        # Ensure that new data does not overwrite any previous data with nan\n        vals[np.isnan(vals)] = 0\n        # Track how many times each point is set (for mean)\n        count[item_name][valid_index] += 1\n        new[:] += vals\n\n    for data in datas:\n        new_idx = []\n        transpose = []\n        slice_ = []\n        for variable_name in vs.keys():\n            # p is at most 1-D by precondition to join\n            p = data[variable_name].points\n            # If p not scalar, append the proper transposition to interop with out\n            # And do not add new axis\n            if np.ndim(p) > 0:\n                transpose.append(np.argmax(data[variable_name].shape))\n                slice_.append(slice(None))\n            # If p is scalar, a new axis must be added, no transpose needed\n            else:\n                slice_.append(np.newaxis)\n            # Triple subscripting needed because newaxis only applys to numpy array\n            # New axis added so that subtracting p will broadcast\n            arr = out[variable_name][:][..., np.newaxis]\n            i = np.argmin(np.abs(arr - p), axis=np.argmax(arr.shape))\n            # Reshape i, to match with the output shape\n            sh = [1] * i.ndim\n            sh[np.argmax(arr.shape)] = i.size\n            i.shape = sh\n            new_idx.append(i)\n        slice_ = tuple(slice_)\n        for variable_name in out.variable_names:\n            if variable_name not in vs.keys():\n                combine(data, out, variable_name, new_idx, transpose, slice_)\n        for channel_name in channel_names:\n            combine(data, out, channel_name, new_idx, transpose, slice_)\n\n    if method == \"mean\":\n        for name, c in count.items():\n            if out[name].dtype.kind in \"fcmM\":\n                out[name][:] /= c\n            else:\n                out[name][:] //= c\n    out.transform(*axis_expressions)\n    if verbose:\n        print(len(datas), \"datas joined to create new data:\")\n        print(\"  axes:\")\n        for axis in out.axes:\n            points = axis[:]\n            print(\n                \"    {0} : {1} points from {2} to {3} {4}\".format(\n                    axis.expression, points.size, np.min(points), np.max(points), axis.units\n                )\n            )\n        print(\"  channels:\")\n        for channel in out.channels:\n            percent_nan = np.around(\n                100. * (np.isnan(channel[:]).sum() / float(channel.size)), decimals=2\n            )\n            print(\n                \"    {0} : {1} to {2} ({3}% NaN)\".format(\n                    channel.name, channel.min(), channel.max(), percent_nan\n                )\n            )\n    return out", "response": "Join a list of data objects together."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef label(self) -> str:\n        label = self.expression.replace(\"_\", \"\\\\;\")\n        if self.units_kind:\n            symbol = wt_units.get_symbol(self.units)\n            for v in self.variables:\n                vl = \"%s_{%s}\" % (symbol, v.label)\n                vl = vl.replace(\"_{}\", \"\")  # label can be empty, no empty subscripts\n                label = label.replace(v.natural_name, vl)\n                val = (\n                    round(self.value, self.round_spec)\n                    if self.round_spec is not None\n                    else self.value\n                )\n        label += r\"\\,=\\,{}\".format(format(val, self.format_spec))\n        if self.units_kind:\n            units_dictionary = getattr(wt_units, self.units_kind)\n            label += r\"\\,\"\n            label += units_dictionary[self.units][2]\n        label = r\"$\\mathsf{%s}$\" % label\n        return label", "response": "A latex formatted label representing constant expression and united value."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a WrightTools Collection from a directory of source files.", "response": "def from_directory(filepath, from_methods, *, name=None, parent=None, verbose=True):\n    \"\"\"Create a WrightTools Collection from a directory of source files.\n\n    Parameters\n    ----------\n    filepath: path-like\n        Path to the directory on the file system\n    from_methods: dict<str, callable>\n        Dictionary which maps patterns (using Unix-like glob wildcard patterns)\n        to functions which take a filepath, plus the keyword arguments\n        ['name', 'parent', and 'verbose'].\n        (e.g. most from_<kind> methods within WrightTools)\n        The value can be `None` which results in that item being ignored.\n        The *first* matching pattern encountered will be used.\n        Therefore, if multiple patterns will match the same file, use and `OrderedDict`.\n        Patterns are matched on the file name level, not using the full path.\n\n    Keyword Arguments\n    -----------------\n    name: str\n        Name to use for the root data object. Default is the directory name.\n    parent: Collection\n        Parent collection to insert the directory structure into. Default is a new\n        collection in temp file.\n    verbose: bool\n        Print information as objects are created. Passed to the functions.\n\n    Examples\n    --------\n    >>> from_dict = {'*.data':wt.data.from_PyCMDS,\n    ...              '*.csv':wt.collections.from_Cary,\n    ...              'unused':None,\n    ...             }\n    >>> col = wt.collection.from_directory('path/to/folder', from_dict)\n    \"\"\"\n    filepath = pathlib.Path(filepath).resolve()\n    if name is None:\n        name = filepath.name\n\n    if verbose:\n        print(\"Creating Collection:\", name)\n\n    root = Collection(name=name, parent=parent)\n\n    q = queue.Queue()\n\n    for i in filepath.iterdir():\n        q.put((filepath, i.name, root))\n\n    while not q.empty():\n        path, fname, parent = q.get()\n        for pattern, func in from_methods.items():\n            if fnmatch.fnmatch(fname, pattern):\n                if func is not None:\n                    func(\n                        path / fname,\n                        name=os.path.splitext(fname)[0],\n                        parent=parent,\n                        verbose=verbose,\n                    )\n                break\n        else:\n            if (path / fname).is_dir():\n                if verbose:\n                    print(\"Creating Collection at\", pathlib.PurePosixPath(parent.name) / fname)\n                col = parent.create_collection(name=fname)\n                for i in (path / fname).iterdir():\n                    q.put((path / fname, i.name, col))\n    return root"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_Cary(filepath, name=None, parent=None, verbose=True):\n    # check filepath\n    filestr = os.fspath(filepath)\n    filepath = pathlib.Path(filepath)\n\n    if \".csv\" not in filepath.suffixes:\n        wt_exceptions.WrongFileTypeWarning.warn(filepath, \"csv\")\n    if name is None:\n        name = \"cary\"\n    # import array\n    lines = []\n    ds = np.DataSource(None)\n    with ds.open(filestr, \"rt\", encoding=\"iso-8859-1\") as f:\n        header = f.readline()\n        columns = f.readline()\n        while True:\n            line = f.readline()\n            if line == \"\\n\" or line == \"\" or line == \"\\r\\n\":\n                break\n            else:\n                # Note, it is necessary to call this twice, as a single call will\n                # result in something like ',,,,' > ',nan,,nan,'.\n                line = line.replace(\",,\", \",nan,\")\n                line = line.replace(\",,\", \",nan,\")\n                # Ensure that the first column has nan, if necessary\n                if line[0] == \",\":\n                    line = \"nan\" + line\n                clean = line[:-2]  # lines end with ',/n'\n                lines.append(np.fromstring(clean, sep=\",\"))\n    lines = [line for line in lines if len(line) > 0]\n    header = header.split(\",\")\n    columns = columns.split(\",\")\n    arr = np.array(lines).T\n    duplicate = len(header) // 2 == len(set(header) - {\"\"})\n    # chew through all scans\n    datas = Collection(name=name, parent=parent, edit_local=parent is not None)\n    units_dict = {\"\u00b0c\": \"deg_C\", \"\u00b0f\": \"deg_F\"}\n    for i in range(0, len(header) - 1, 2):\n        r = re.compile(r\"[ \\t\\(\\)]+\")\n        spl = r.split(columns[i])\n        ax = spl[0].lower() if len(spl) > 0 else None\n        units = spl[1].lower() if len(spl) > 1 else None\n        units = units_dict.get(units, units)\n        if duplicate:\n            name = \"{}_{:03d}\".format(header[i], i // 2)\n        else:\n            name = header[i]\n        dat = datas.create_data(name, kind=\"Cary\", source=filestr)\n        dat.create_variable(ax, arr[i][~np.isnan(arr[i])], units=units)\n        dat.create_channel(\n            columns[i + 1].lower(), arr[i + 1][~np.isnan(arr[i + 1])], label=columns[i + 1].lower()\n        )\n        dat.transform(ax)\n    # finish\n    if verbose:\n        print(\"{0} data objects successfully created from Cary file:\".format(len(datas)))\n        for i, data in enumerate(datas):\n            print(\"  {0}: {1}\".format(i, data))\n    return datas", "response": "Create a collection object from a Cary 50 UV VIS absorbance file."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndiscovers the dimensions of a flattened multidimensional array.", "response": "def discover_dimensions(arr, cols) -> collections.OrderedDict:\n    \"\"\"Discover the dimensions of a flattened multidimensional array.\n\n    Parameters\n    ----------\n    arr : 2D numpy ndarray\n        Array in [col, value].\n    cols : dictionary\n        Dictionary with column names as keys, and idx, tolerance and units\n        as values.\n\n    Returns\n    -------\n    dictionary\n        expression: points\n    \"\"\"\n    # import values -------------------------------------------------------------------------------\n    di = [cols[key][\"idx\"] for key in cols.keys()]\n    dt = [cols[key][\"tolerance\"] for key in cols.keys()]\n    du = [cols[key][\"units\"] for key in cols.keys()]\n    dk = [key for key in cols.keys()]\n    dims = list(zip(di, dt, du, dk))\n    # remove nan dimensions and bad dimensions ----------------------------------------------------\n    to_pop = []\n    for i in range(len(dims)):\n        if np.all(np.isnan(arr[dims[i][0]])):\n            to_pop.append(i)\n    to_pop.reverse()\n    for i in to_pop:\n        dims.pop(i)\n    # which dimensions are equal ------------------------------------------------------------------\n    # find\n    d_equal = np.zeros((len(dims), len(dims)), dtype=bool)\n    d_equal[:, :] = True\n    for i in range(len(dims)):  # test\n        for j in range(len(dims)):  # against\n            for k in range(len(arr[0])):\n                upper_bound = arr[dims[i][0], k] + dims[i][1]\n                lower_bound = arr[dims[i][0], k] - dims[i][1]\n                test_point = arr[dims[j][0], k]\n                if upper_bound > test_point > lower_bound:\n                    pass\n                else:\n                    d_equal[i, j] = False\n                    break\n    # condense\n    dims_unaccounted = list(range(len(dims)))\n    dims_condensed = []\n    while dims_unaccounted:\n        dim_current = dims_unaccounted[0]\n        index = dims[dim_current][0]\n        tolerance = [dims[dim_current][1]]\n        units = dims[dim_current][2]\n        key = [dims[dim_current][3]]\n        dims_unaccounted.pop(0)\n        indicies = list(range(len(dims_unaccounted)))\n        indicies.reverse()\n        for i in indicies:\n            dim_check = dims_unaccounted[i]\n            if d_equal[dim_check, dim_current]:\n                tolerance.append(dims[dim_check][1])\n                key.append(dims[dim_check][3])\n                dims_unaccounted.pop(i)\n        tolerance = max(tolerance)\n        dims_condensed.append([index, tolerance, units, key])\n    dims = dims_condensed\n    # which dimensions are scanned ----------------------------------------------------------------\n    # find\n    scanned = []\n    constant_list = []\n    for dim in dims:\n        name = dim[3]\n        index = dim[0]\n        vals = arr[index]\n        tolerance = dim[1]\n        if vals.max() - vals.min() > tolerance:\n            scanned.append([name, index, tolerance, None])\n        else:\n            constant_list.append([name, index, tolerance, arr[index, 0]])\n    # order scanned dimensions (..., zi, yi, xi)\n    first_change_indicies = []\n    for axis in scanned:\n        first_point = arr[axis[1], 0]\n        for i in range(len(arr[0])):\n            upper_bound = arr[axis[1], i] + axis[2]\n            lower_bound = arr[axis[1], i] - axis[2]\n            if upper_bound > first_point > lower_bound:\n                pass\n            else:\n                first_change_indicies.append(i)\n                break\n    scanned_ordered = [scanned[i] for i in np.argsort(first_change_indicies)]\n    scanned_ordered.reverse()\n    # shape ---------------------------------------------------------------------------------------\n    out = collections.OrderedDict()\n    for a in scanned_ordered:\n        key = a[0][0]\n        axis = cols[key]\n        # generate lists from data\n        lis = sorted(arr[axis[\"idx\"]])\n        tol = axis[\"tolerance\"]\n        # values are binned according to their averages now, so min and max\n        #  are better represented\n        xstd = []\n        xs = []\n        # check to see if unique values are sufficiently unique\n        # deplete to list of values by finding points that are within\n        #  tolerance\n        while len(lis) > 0:\n            # find all the xi's that are like this one and group them\n            # after grouping, remove from the list\n            set_val = lis[0]\n            xi_lis = [xi for xi in lis if np.abs(set_val - xi) < tol]\n            # the complement of xi_lis is what remains of xlis, then\n            lis = [xi for xi in lis if not np.abs(xi_lis[0] - xi) < tol]\n            xi_lis_average = sum(xi_lis) / len(xi_lis)\n            xs.append(xi_lis_average)\n            xstdi = sum(np.abs(xi_lis - xi_lis_average)) / len(xi_lis)\n            xstd.append(xstdi)\n        tol = sum(xstd) / len(xstd)\n        tol = max(tol, 1e-4)\n        if axis[\"units\"] == \"nm\":\n            min_wn = 1e7 / max(xs) + tol\n            max_wn = 1e7 / min(xs) - tol\n            points = np.linspace(min_wn, max_wn, num=len(xs))\n            points = wt_units.converter(points, \"wn\", \"nm\")\n        else:\n            points = np.linspace(min(xs) + tol, max(xs) - tol, num=len(xs))\n        key = \"=\".join(a[0])\n        out[key] = points\n    # warn if data doesn't seem like the right shape ----------------------------------------------\n    length = len(arr[0])\n    size = 1\n    for a in out.values():\n        size *= a.size\n    if not size == length:\n        message = \"array length ({0}) inconsistent with data size ({1})\".format(length, size)\n        warnings.warn(message)\n    # return --------------------------------------------------------------------------------------\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn True if combination is valid and False otherwise.", "response": "def is_valid_combination(values, names):\n\n    dictionary = dict(zip(names, values))\n\n    \"\"\"\n    Should return True if combination is valid and False otherwise.\n\n    Dictionary that is passed here can be incomplete.\n    To prevent search for unnecessary items filtering function\n    is executed with found subset of data to validate it.\n    \"\"\"\n\n    rules = [\n        # Brand Y does not support Windows 98\n        # Brand X does not work with XP\n        # Contractors are billed in 30 min increments\n        lambda d: \"98\" == d[\"os\"] and \"Brand Y\" == d[\"brand\"],\n        lambda d: \"XP\" == d[\"os\"] and \"Brand X\" == d[\"brand\"],\n        lambda d: \"Contr.\" == d[\"employee\"] and d[\"increment\"] < 30,\n    ]\n\n    for rule in rules:\n        try:\n            if rule(dictionary):\n                return False\n        except KeyError:\n            pass\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck if a combination of items is valid.", "response": "def is_valid_combination(row):\n    \"\"\"\n    This is a filtering function. Filtering functions should return True\n    if combination is valid and False otherwise.\n\n    Test row that is passed here can be incomplete.\n    To prevent search for unnecessary items filtering function\n    is executed with found subset of data to validate it.\n    \"\"\"\n\n    n = len(row)\n\n    if n > 1:\n        # Brand Y does not support Windows 98\n        if \"98\" == row[1] and \"Brand Y\" == row[0]:\n            return False\n\n        # Brand X does not work with XP\n        if \"XP\" == row[1] and \"Brand X\" == row[0]:\n            return False\n\n    if n > 4:\n        # Contractors are billed in 30 min increments\n        if \"Contr.\" == row[3] and row[4] < 30:\n            return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_station(name, latlonalt, parent_frame=WGS84, orientation='N', mask=None):\n\n    if isinstance(orientation, str):\n        orient = {'N': np.pi, 'S': 0., 'E': np.pi / 2., 'W': 3 * np.pi / 2.}\n        heading = orient[orientation]\n    else:\n        heading = orientation\n\n    latlonalt = list(latlonalt)\n    latlonalt[:2] = np.radians(latlonalt[:2])\n    coordinates = TopocentricFrame._geodetic_to_cartesian(*latlonalt)\n\n    mtd = '_to_%s' % parent_frame.__name__\n    dct = {\n        mtd: TopocentricFrame._to_parent_frame,\n        'latlonalt': latlonalt,\n        'coordinates': coordinates,\n        'parent_frame': parent_frame,\n        'heading': heading,\n        'orientation': orientation,\n        'mask': np.array(mask) if mask else None,\n    }\n    cls = _MetaFrame(name, (TopocentricFrame,), dct)\n    cls + parent_frame\n\n    return cls", "response": "Create a ground station instance for the given station name latitude longitude and orientation."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef visibility(cls, orb, **kwargs):\n\n        from ..orbits.listeners import stations_listeners, Listener\n\n        listeners = kwargs.setdefault('listeners', [])\n        events = kwargs.pop('events', None)\n        event_classes = tuple()\n\n        if events:\n            # Handling of the listeners passed in the 'events' kwarg\n            # and merging them with the `listeners` kwarg\n            if isinstance(events, Listener):\n                listeners.append(events)\n            elif isinstance(events, (list, tuple)):\n                listeners.extend(events)\n\n            sta_list = stations_listeners(cls)\n            listeners.extend(sta_list)\n\n            # Only the events present in the `event_classes` list will be yielded\n            # outside of visibility. This list was created in order to force\n            # the yield of AOS and LOS.\n\n            event_classes = tuple(listener.event for listener in sta_list)\n\n        for point in orb.iter(**kwargs):\n            point.frame = cls\n            point.form = 'spherical'\n\n            # Not very clean !\n            if point.phi < 0 and not isinstance(point.event, event_classes):\n                continue\n\n            yield point", "response": "Compute the visibility of a station in a topocentric frame."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _to_parent_frame(self, *args, **kwargs):\n        lat, lon, _ = self.latlonalt\n        m = rot3(-lon) @ rot2(lat - np.pi / 2.) @ rot3(self.heading)\n        offset = np.zeros(6)\n        offset[:3] = self.coordinates\n        return self._convert(m, m), offset", "response": "Conversion from Topocentric Frame to parent frame"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts from geodetic coordinates to cartesian with respect to an ellipsoid", "response": "def _geodetic_to_cartesian(cls, lat, lon, alt):\n        \"\"\"Conversion from latitude, longitude and altitude coordinates to\n        cartesian with respect to an ellipsoid\n\n        Args:\n            lat (float): Latitude in radians\n            lon (float): Longitude in radians\n            alt (float): Altitude to sea level in meters\n\n        Return:\n            numpy.array: 3D element (in meters)\n        \"\"\"\n        C = Earth.r / np.sqrt(1 - (Earth.e * np.sin(lat)) ** 2)\n        S = Earth.r * (1 - Earth.e ** 2) / np.sqrt(1 - (Earth.e * np.sin(lat)) ** 2)\n        r_d = (C + alt) * np.cos(lat)\n        r_k = (S + alt) * np.sin(lat)\n\n        norm = np.sqrt(r_d ** 2 + r_k ** 2)\n        return norm * np.array([\n            np.cos(lat) * np.cos(lon),\n            np.cos(lat) * np.sin(lon),\n            np.sin(lat)\n        ])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_mask(cls, azim):\n\n        if cls.mask is None:\n            raise ValueError(\"No mask defined for the station {}\".format(cls.name))\n\n        azim %= 2 * np.pi\n\n        if azim in cls.mask[0, :]:\n            return cls.mask[1, np.where(azim == cls.mask[0, :])[0][0]]\n\n        for next_i, mask_azim in enumerate(cls.mask[0, :]):\n            if mask_azim > azim:\n                break\n        else:\n            next_i = 0\n\n        x0, y0 = cls.mask[:, next_i - 1]\n        x1, y1 = cls.mask[:, next_i]\n\n        if next_i - 1 == -1:\n            x0 = 0\n\n        return y0 + (y1 - y0) * (azim - x0) / (x1 - x0)", "response": "Linear interpolation between two points of the mask"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_body(name):\n\n    try:\n\n        body, propag = _bodies[name.lower()]\n        # attach a propagator to the object\n        body.propagate = propag.propagate\n    except KeyError as e:\n        raise UnknownBodyError(e.args[0])\n\n    return body", "response": "Retrieve a given object s body orbits and parameters"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef propagate(cls, date):\n\n        date = date.change_scale('TDB')\n        t_tdb = date.julian_century\n\n        def cos(angle):\n            \"\"\"cosine in degrees\"\"\"\n            return np.cos(np.radians(angle))\n\n        def sin(angle):\n            \"\"\"sine in degrees\"\"\"\n            return np.sin(np.radians(angle))\n\n        lambda_el = 218.32 + 481267.8813 * t_tdb + 6.29 * sin(134.9 + 477198.85 * t_tdb) \\\n            - 1.27 * sin(259.2 - 413335.38 * t_tdb) + 0.66 * sin(235.7 + 890534.23 * t_tdb) \\\n            + 0.21 * sin(269.9 + 954397.7 * t_tdb) - 0.19 * sin(357.5 + 35999.05 * t_tdb) \\\n            - 0.11 * sin(186.6 + 966404.05 * t_tdb)\n\n        phi_el = 5.13 * sin(93.3 + 483202.03 * t_tdb) + 0.28 * sin(228.2 + 960400.87 * t_tdb) \\\n            - 0.28 * sin(318.3 + 6003.18 * t_tdb) - 0.17 * sin(217.6 - 407332.2 * t_tdb)\n\n        p = 0.9508 + 0.0518 * cos(134.9 + 477198.85 * t_tdb) + 0.0095 * cos(259.2 - 413335.38 * t_tdb) \\\n            + 0.0078 * cos(235.7 + 890534.23 * t_tdb) + 0.0028 * cos(269.9 + 954397.70 * t_tdb)\n\n        e_bar = 23.439291 - 0.0130042 * t_tdb - 1.64e-7 * t_tdb ** 2 + 5.04e-7 * t_tdb ** 3\n\n        r_moon = Earth.r / sin(p)\n\n        state_vector = r_moon * np.array([\n            cos(phi_el) * cos(lambda_el),\n            cos(e_bar) * cos(phi_el) * sin(lambda_el) - sin(e_bar) * sin(phi_el),\n            sin(e_bar) * cos(phi_el) * sin(lambda_el) + cos(e_bar) * sin(phi_el),\n            0, 0, 0\n        ])\n\n        return Orbit(date, state_vector, 'cartesian', 'EME2000', cls())", "response": "Compute the position of the Moon at a given date."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef propagate(cls, date):\n\n        date = date.change_scale('UT1')\n        t_ut1 = date.julian_century\n\n        lambda_M = 280.460 + 36000.771 * t_ut1\n        M = np.radians(357.5291092 + 35999.05034 * t_ut1)\n        lambda_el = np.radians(lambda_M + 1.914666471 * np.sin(M) + 0.019994643 * np.sin(2 * M))\n\n        r = 1.000140612 - 0.016708617 * np.cos(M) - 0.000139589 * np.cos(2 * M)\n        eps = np.radians(23.439291 - 0.0130042 * t_ut1)\n\n        pv = r * np.array([\n            np.cos(lambda_el),\n            np.cos(eps) * np.sin(lambda_el),\n            np.sin(eps) * np.sin(lambda_el),\n            0,\n            0,\n            0\n        ]) * AU\n\n        return Orbit(date, pv, 'cartesian', 'MOD', cls())", "response": "Compute the position of the sun at a given date."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\niterate over the orbits between two dates.", "response": "def iter(self, **kwargs):\n        \"\"\"Compute a range of orbits between two dates\n\n        Keyword Arguments:\n            dates (list of :py:class:`~beyond.dates.date.Date`): Dates from which iterate over\n            start (Date or None): Date of the first point\n            stop (Date, timedelta or None): Date of the last point\n            step (timedelta or None): Step to use during the computation. Use the same step as\n                `self` if `None`\n            listeners (list of:py:class:`~beyond.orbits.listeners.Listener`):\n        Yield:\n            :py:class:`Orbit`:\n\n        There is two ways to use the iter() method.\n\n        If *dates* is defined, it should be an iterable of dates. This could be\n        a generator as per :py:meth:`Date.range <beyond.dates.date.Date.range>`, or a list.\n\n        .. code-block:: python\n\n            # Create two successive ranges of dates, with different steps\n            dates = list(Date.range(Date(2019, 3, 23), Date(2019, 3, 24), timedelta(minutes=3)))\n            dates.extend(Date.range(Date(2019, 3, 24), Date(2019, 3, 25), timedelta(minutes=10), inclusive=True))\n            propag.iter(dates=dates)\n\n        The alternative, is the use of *start*, *stop* and *step* keyword arguments\n        which work exactly as :code:`Date.range(start, stop, step, inclusive=True)`\n\n        If one of *start*, *stop* or *step* arguments is set to ``None`` it will keep\n        the same property as the generating ephemeris.\n\n        .. code-block:: python\n\n            propag.iter(stop=stop)  # If the iterator has a default step (e.g. numerical propagators)\n            propag.iter(stop=stop, step=step)\n            propag.iter(start=start, stop=stop, step=step)\n        \"\"\"\n\n        if 'dates' not in kwargs:\n            start = kwargs.setdefault('start', self.orbit.date)\n            stop = kwargs.get('stop')\n            step = kwargs.setdefault('step', getattr(self, 'step', None))\n\n            if 'stop' is None:\n                raise ValueError(\"The end of the propagation should be defined\")\n\n            start = self.orbit.date if start is None else start\n            step = self.step if step is None else step\n\n            if isinstance(kwargs['stop'], timedelta):\n                kwargs['stop'] = start + kwargs['stop']\n            if start > kwargs['stop'] and step.total_seconds() > 0:\n                kwargs['step'] = -step\n\n        listeners = kwargs.pop('listeners', [])\n\n        for orb in self._iter(**kwargs):\n            for listen_orb in self.listen(orb, listeners):\n                yield listen_orb\n            yield orb"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nretrieve the orbit of a solar system object.", "response": "def get_orbit(name, date):\n    \"\"\"Retrieve the orbit of a solar system object\n\n    Args:\n        name (str): The name of the body desired. For exact nomenclature, see\n            :py:func:`available_planets`\n        date (Date): Date at which the state vector will be extracted\n    Return:\n        Orbit: Orbit of the desired object, in the reference frame in which it is declared in\n            the .bsp file\n    \"\"\"\n\n    # On-demand Propagator and Frame generation\n\n    if name not in [x.name for x in Bsp().top.list]:\n        raise UnknownBodyError(name)\n\n    for a, b in Bsp().top.steps(name):\n        if b.name not in _propagator_cache:\n\n            # Creation of the specific propagator class\n            propagator = type(\n                \"%sBspPropagator\" % b.name,\n                (GenericBspPropagator,),\n                {'src': a, 'dst': b}\n            )\n\n            # Retrieve informations for the central body. If unavailable, create a virtual body with\n            # dummy values\n            center = Pck()[b.full_name.title()]\n\n            # Register the Orbit as a frame\n            propagator.propagate(date).as_frame(b.name, center=center)\n            _propagator_cache[b.name] = propagator\n\n    if Bsp().top not in _propagator_cache:\n        _propagator_cache[Bsp().top.name] = EarthPropagator()\n\n    return _propagator_cache[name].propagate(date)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_frames(until=None):\n\n    now = Date.now()\n\n    if until:\n        get_orbit(until, now)\n    else:\n        for body in list_bodies():\n            get_orbit(body.name, now)", "response": "Create all the frames available in the JPL files."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_body(name):\n\n    body = Pck()[name]\n    body.propagate = lambda date: get_orbit(name, date)\n    return body", "response": "Retrieve the Body structure of a JPL. bsp file object"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef open(self):\n        segments = []\n\n        files = config.get('env', 'jpl', fallback=[])\n\n        if not files:\n            raise JplConfigError(\"No JPL file defined\")\n\n        # Extraction of segments from each .bsp file\n        for filepath in files:\n\n            filepath = Path(filepath)\n\n            if filepath.suffix.lower() != \".bsp\":\n                continue\n\n            segments.extend(SPK.open(str(filepath)).segments)\n\n        if not segments:\n            raise JplError(\"No segment loaded\")\n\n        # list of available segments\n        self.segments = dict(((s.center, s.target), s) for s in segments)\n\n        # This variable will contain the Target of reference from which\n        # all relations between frames are linked\n        targets = {}\n\n        for center_id, target_id in self.segments.keys():\n\n            center_name = target_names.get(center_id, 'Unknown')\n            target_name = target_names.get(target_id, 'Unknown')\n\n            # Retrieval of the Target object representing the center if it exists\n            # or creation of said object if it doesn't.\n            center = targets.setdefault(center_id, Target(center_name, center_id))\n            target = targets.setdefault(target_id, Target(target_name, target_id))\n\n            # Link between the Target objects (see Node2)\n            center + target\n\n        # We take the Earth target and make it the top of the structure.\n        # That way, it is easy to link it to the already declared earth-centered reference frames\n        # from the `frames.frame` module.\n        self.top = targets[399]", "response": "Open the files and load the segments."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving the position and velocity of a target with respect to a center", "response": "def get(self, center, target, date):\n        \"\"\"Retrieve the position and velocity of a target with respect to a center\n\n        Args:\n            center (Target):\n            target (Target):\n            date (Date):\n        Return:\n            numpy.array: length-6 array position and velocity (in m and m/s) of the\n                target, with respect to the center\n        \"\"\"\n\n        if (center.index, target.index) in self.segments:\n            pos, vel = self.segments[center.index, target.index].compute_and_differentiate(date.jd)\n            sign = 1\n        else:\n            # When we wish to get a segment that is not available in the files (such as\n            # EarthBarycenter with respect to the Moon, for example), we take the segment\n            # representing the inverse vector if available and reverse it\n            pos, vel = self.segments[target.index, center.index].compute_and_differentiate(date.jd)\n            sign = -1\n\n        # In some cases, the pos vector contains both position and velocity\n        if len(pos) == 3:\n            # The velocity is given in km/days, so we convert to km/s\n            # see: https://github.com/brandon-rhodes/python-jplephem/issues/19 for clarifications\n            pv = np.concatenate((pos, vel / S_PER_DAY))\n        elif len(pos) == 6:\n            pv = np.array(pos)\n        else:\n            raise JplError(\"Unknown state vector format\")\n\n        return sign * pv * 1000"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_propagator(name):\n\n    from .sgp4 import Sgp4\n    from .sgp4beta import Sgp4Beta\n\n    scope = locals().copy()\n    scope.update(globals())\n\n    if name not in scope:\n        raise UnknownPropagatorError(name)\n\n    return scope[name]", "response": "Retrieve a named propagator"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef stations_listeners(stations):\n    stations = stations if isinstance(stations, (list, tuple)) else [stations]\n\n    listeners = []\n    for sta in stations:\n\n        listeners.append(StationSignalListener(sta))\n        listeners.append(StationMaxListener(sta))\n        if sta.mask is not None:\n            listeners.append(StationMaskListener(sta))\n\n    return listeners", "response": "Function for creating listeners for a list of stations"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts from cartesian to keplerian.", "response": "def _cartesian_to_keplerian(cls, coord, center):\n        \"\"\"Conversion from cartesian (position and velocity) to keplerian\n\n        The keplerian form is\n\n            * a : semi-major axis\n            * e : eccentricity\n            * i : inclination\n            * \u03a9 : right-ascension of ascending node\n            * \u03c9 : Argument of perigee\n            * \u03bd : True anomaly\n        \"\"\"\n\n        r, v = coord[:3], coord[3:]\n        h = np.cross(r, v)                      # angular momentum vector\n        h_norm = np.linalg.norm(h)\n        r_norm = np.linalg.norm(r)\n        v_norm = np.linalg.norm(v)\n\n        K = v_norm ** 2 / 2 - center.\u00b5 / r_norm      # specific energy\n        a = - center.\u00b5 / (2 * K)                     # semi-major axis\n        e = sqrt(1 - h_norm ** 2 / (a * center.\u00b5))   # eccentricity\n        p = a * (1 - e ** 2)\n        i = arccos(h[2] / h_norm)               # inclination\n        \u03a9 = arctan2(h[0], -h[1]) % (2 * np.pi)  # right ascension of the ascending node\n\n        \u03c9_\u03bd = arctan2(r[2] / sin(i), r[0] * cos(\u03a9) + r[1] * sin(\u03a9))\n        \u03bd = arctan2(sqrt(p / center.\u00b5) * np.dot(v, r), p - r_norm) % (2 * np.pi)\n        \u03c9 = (\u03c9_\u03bd - \u03bd) % (2 * np.pi)             # argument of the perigee\n\n        return np.array([a, e, i, \u03a9, \u03c9, \u03bd], dtype=float)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _keplerian_to_cartesian(cls, coord, center):\n\n        a, e, i, \u03a9, \u03c9, \u03bd = coord\n\n        p = a * (1 - e ** 2)\n        r = p / (1 + e * cos(\u03bd))\n        h = sqrt(center.\u00b5 * p)\n        x = r * (cos(\u03a9) * cos(\u03c9 + \u03bd) - sin(\u03a9) * sin(\u03c9 + \u03bd) * cos(i))\n        y = r * (sin(\u03a9) * cos(\u03c9 + \u03bd) + cos(\u03a9) * sin(\u03c9 + \u03bd) * cos(i))\n        z = r * sin(i) * sin(\u03c9 + \u03bd)\n        vx = x * h * e / (r * p) * sin(\u03bd) - h / r * (cos(\u03a9) * sin(\u03c9 + \u03bd) + sin(\u03a9) * cos(\u03c9 + \u03bd) * cos(i))\n        vy = y * h * e / (r * p) * sin(\u03bd) - h / r * (sin(\u03a9) * sin(\u03c9 + \u03bd) - cos(\u03a9) * cos(\u03c9 + \u03bd) * cos(i))\n        vz = z * h * e / (r * p) * sin(\u03bd) + h / r * sin(i) * cos(\u03c9 + \u03bd)\n\n        return np.array([x, y, z, vx, vy, vz], dtype=float)", "response": "Conversion from Keplerian to Cartesian coordinates"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _keplerian_to_keplerian_mean(cls, coord, center):\n\n        a, e, i, \u03a9, \u03c9, \u03bd = coord\n        if e < 1:\n            # Elliptic case\n            cos_E = (e + cos(\u03bd)) / (1 + e * cos(\u03bd))\n            sin_E = (sin(\u03bd) * sqrt(1 - e ** 2)) / (1 + e * cos(\u03bd))\n            E = arctan2(sin_E, cos_E) % (2 * np.pi)\n            M = E - e * sin(E)  # Mean anomaly\n        else:\n            # Hyperbolic case\n            H = arccosh((e + cos(\u03bd)) / (1 + e * cos(\u03bd)))\n            M = e * sinh(H) - H\n\n        return np.array([a, e, i, \u03a9, \u03c9, M], dtype=float)", "response": "Conversion from Keplerian to Keplerian Mean\n           "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _keplerian_mean_to_keplerian(cls, coord, center):\n        a, e, i, \u03a9, \u03c9, M = coord\n        E = cls._m_to_e(e, M)\n        cos_\u03bd = (cos(E) - e) / (1 - e * cos(E))\n        sin_\u03bd = (sin(E) * sqrt(1 - e**2)) / (1 - e * cos(E))\n\n        \u03bd = arctan2(sin_\u03bd, cos_\u03bd) % (np.pi * 2)\n\n        return np.array([a, e, i, \u03a9, \u03c9, \u03bd], dtype=float)", "response": "Conversion from Mean Keplerian to True Keplerian"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _m_to_e(cls, e, M):\n\n        k1 = 3 * np.pi + 2\n        k2 = np.pi - 1\n        k3 = 6 * np.pi - 1\n        A = 3 * k2 ** 2 / k1\n        B = k3 ** 2 / (6 * k1)\n\n        m1 = float(M)\n        if abs(m1) < 1 / 6:\n            E = m1 + e * (6 * m1) ** (1 / 3) - m1\n        elif m1 < 0:\n            w = np.pi + m1\n            E = m1 + e * (A * w / (B - w) - np.pi - m1)\n        else:\n            w = np.pi - m1\n            E = m1 + e * (np.pi - A * w / (B - w) - m1)\n\n        e1 = 1 - e\n        risk_disabler = (e1 + E ** 2 / 6) >= 0.1\n\n        for i in range(2):\n            fdd = e * sin(E)\n            fddd = e * cos(E)\n\n            if risk_disabler:\n                f = (E - fdd) - m1\n                fd = 1 - fddd\n            else:\n                f = cls._e_e_sin_e(e, E) - m1\n                s = sin(E / 2)\n                fd = e1 + 2 * e * s ** 2\n            dee = f * fd / (0.5 * f * fdd - fd ** 2)\n\n            w = fd + 0.5 * dee * (fdd + dee * fddd / 3)\n            fd += dee * (fdd + 0.5 * dee * fddd)\n            E -= (f - dee * (fd - w)) / fd\n\n        E += M - m1\n\n        return E", "response": "Conversion from Mean Anomaly to Eccentric anomaly"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting from the TLE standard format to the Mean Keplerian format.", "response": "def _tle_to_keplerian_mean(cls, coord, center):\n        \"\"\"Conversion from the TLE standard format to the Mean Keplerian\n\n        see :py:class:`Tle` for more information.\n        \"\"\"\n        i, \u03a9, e, \u03c9, M, n = coord\n        a = (center.\u00b5 / n ** 2) ** (1 / 3)\n\n        return np.array([a, e, i, \u03a9, \u03c9, M], dtype=float)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _cartesian_to_spherical(cls, coord, center):\n        x, y, z, vx, vy, vz = coord\n        r = np.linalg.norm(coord[:3])\n        phi = arcsin(z / r)\n        theta = arctan2(y, x)\n\n        r_dot = (x * vx + y * vy + z * vz) / r\n        phi_dot = (vz * (x ** 2 + y ** 2) - z * (x * vx + y * vy)) / (r ** 2 * sqrt(x ** 2 + y ** 2))\n        theta_dot = (x * vy - y * vx) / (x ** 2 + y ** 2)\n\n        return np.array([r, theta, phi, r_dot, theta_dot, phi_dot], dtype=float)", "response": "Convert from cartesian to spherical"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _spherical_to_cartesian(cls, coord, center):\n        r, theta, phi, r_dot, theta_dot, phi_dot = coord\n        x = r * cos(phi) * cos(theta)\n        y = r * cos(phi) * sin(theta)\n        z = r * sin(phi)\n\n        vx = r_dot * x / r - y * theta_dot - z * phi_dot * cos(theta)\n        vy = r_dot * y / r + x * theta_dot - z * phi_dot * sin(theta)\n        vz = r_dot * z / r + r * phi_dot * cos(phi)\n\n        return np.array([x, y, z, vx, vy, vz], dtype=float)", "response": "Spherical to cartesian conversion"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ninterpolates data at a given date.", "response": "def interpolate(self, date, method=None, order=None):\n        \"\"\"Interpolate data at a given date\n\n        Args:\n            date (Date):\n            method (str): Method of interpolation to use\n            order (int): In case of ``LAGRANGE`` method is used\n        Return:\n            Orbit:\n        \"\"\"\n\n        if not self.start <= date <= self.stop:\n            raise ValueError(\"Date '%s' not in range\" % date)\n\n        prev_idx = 0\n        ephem = self\n\n        # Binary search of the orbit step just before the desired date\n        while True:\n            idx = len(ephem)\n            if idx == 1:\n                break\n            k = idx // 2\n\n            if date > ephem[k].date:\n                prev_idx += k\n                ephem = ephem[k:]\n            else:\n                ephem = ephem[:k]\n\n        method = method if method is not None else self.method\n        order = order if order is not None else self.order\n\n        if method == self.LINEAR:\n\n            y0 = self[prev_idx]\n            y1 = self[prev_idx + 1]\n\n            result = y0[:] + (y1[:] - y0[:]) * (date.mjd - y0.date.mjd) / (y1.date.mjd - y0.date.mjd)\n\n        elif method == self.LAGRANGE:\n\n            stop = prev_idx + 1 + order // 2 + order % 2\n            start = prev_idx - order // 2 + 1\n            if stop >= len(self):\n                start -= stop - len(self)\n            elif start < 0:\n                stop -= start\n                start = 0\n\n            # selection of the subset of data, of length 'order' around the desired value\n            subset = self[start:stop]\n            date_subset = np.array([x.date.mjd for x in subset])\n\n            result = np.zeros(6)\n\n            # Everything is on wikipedia\n            #        k\n            # L(x) = \u03a3 y_j * l_j(x)\n            #        j=0\n            #\n            # l_j(x) = \u03a0 (x - x_m) / (x_j - x_m)\n            #     0 <= m <= k\n            #        m != j\n            for j in range(order):\n                # This mask is here to enforce the m != j in the lagrange polynomials\n                mask = date_subset != date_subset[j]\n                l_j = (date.mjd - date_subset[mask]) / (date_subset[j] - date_subset[mask])\n                result = result + l_j.prod() * subset[j]\n\n        else:\n            raise ValueError(\"Unkown interpolation method\", method)\n\n        orb = ephem[0]\n\n        return orb.__class__(date, result, orb.form, orb.frame, orb.propagator)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate an Ephem object which is a subset of this one", "response": "def ephem(self, *args, **kwargs):\n        \"\"\"Create an Ephem object which is a subset of this one\n\n        Take the same keyword arguments as :py:meth:`ephemeris`\n\n        Return:\n            Ephem:\n        \"\"\"\n\n        return self.__class__(self.ephemeris(*args, **kwargs))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef copy(self, *, form=None, frame=None):  # pragma: no cover\n        new = self.ephem()\n        if frame:\n            new.frame = frame\n        if form:\n            new.form = form\n\n        return new", "response": "Create a deep copy of the ephemeris and allow frame and form changing\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef loads(text):\n    if text.startswith(\"CCSDS_OEM_VERS\"):\n        func = _read_oem\n    elif text.startswith(\"CCSDS_OPM_VERS\"):\n        func = _read_opm\n    else:\n        raise ValueError(\"Unknown CCSDS type\")\n    return func(text)", "response": "Read CCSDS from a string and provide the beyond class corresponding ;\n    Orbit or list of Orbit if it s an OPM Ephem if it s an OEM."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a string CCSDS representation of the object", "response": "def dumps(data, **kwargs):\n    \"\"\"Create a string CCSDS representation of the object\n\n    Same arguments and behaviour as :py:func:`dump`\n    \"\"\"\n    if isinstance(data, Ephem) or (isinstance(data, Iterable) and all(isinstance(x, Ephem) for x in data)):\n        content = _dump_oem(data, **kwargs)\n    elif isinstance(data, Orbit):\n        content = _dump_opm(data, **kwargs)\n    else:\n        raise TypeError(\"Unknown object type\")\n\n    return content"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _float(value):\n    if \"[\" in value:\n        # There is a unit field\n        value, sep, unit = value.partition(\"[\")\n        unit = sep + unit\n\n        # As defined in the CCSDS Orbital Data Message Blue Book, the unit should\n        # be the same as defined in table 3-3 which are for km and km/s for position and\n        # velocity respectively. Thus, there should be no other conversion to make\n        if unit in (\"[km]\", \"[km/s]\"):\n            multiplier = 1000\n        elif unit == \"[s]\":\n            multiplier = 1\n        else:\n            raise ValueError(\"Unknown unit for this field\", unit)\n    else:\n        # if no unit is provided, the default is km, and km/s\n        multiplier = 1000\n\n    return float(value) * multiplier", "response": "Convert a value to a float in the state vector field."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _read_oem(string):\n\n    ephems = []\n    required = ('REF_FRAME', 'CENTER_NAME', 'TIME_SYSTEM', 'OBJECT_ID', 'OBJECT_NAME')\n\n    mode = None\n    for line in string.splitlines():\n\n        if not line or line.startswith(\"COMMENT\"):  # pragma: no cover\n            continue\n        elif line.startswith(\"META_START\"):\n            mode = \"meta\"\n            ephem = {'orbits': []}\n            ephems.append(ephem)\n        elif line.startswith(\"META_STOP\"):\n            mode = \"data\"\n\n            # Check for required fields\n            for k in required:\n                if k not in ephem:\n                    raise ValueError(\"Missing field '{}'\".format(k))\n\n            # Conversion to be compliant with beyond.env.jpl dynamic reference\n            # frames naming convention.\n            if ephem['CENTER_NAME'].lower() != \"earth\":\n                ephem['REF_FRAME'] = ephem['CENTER_NAME'].title().replace(\" \", \"\")\n        elif mode == \"meta\":\n            key, _, value = line.partition(\"=\")\n            ephem[key.strip()] = value.strip()\n        elif mode == \"data\":\n            date, *state_vector = line.split()\n            date = Date.strptime(date, \"%Y-%m-%dT%H:%M:%S.%f\", scale=ephem['TIME_SYSTEM'])\n\n            # Conversion from km to m, from km/s to m/s\n            # and discard acceleration if present\n            state_vector = np.array([float(x) for x in state_vector[:6]]) * 1000\n\n            ephem['orbits'].append(Orbit(date, state_vector, 'cartesian', ephem['REF_FRAME'], None))\n\n    for i, ephem_dict in enumerate(ephems):\n        if not ephem_dict['orbits']:\n            raise ValueError(\"Empty ephemeris\")\n\n        # In case there is no recommendation for interpolation\n        # default to a Lagrange 8th order\n        method = ephem_dict.get('INTERPOLATION', 'Lagrange').lower()\n        order = int(ephem_dict.get('INTERPOLATION_DEGREE', 7)) + 1\n        ephem = Ephem(ephem_dict['orbits'], method=method, order=order)\n\n        ephem.name = ephem_dict['OBJECT_NAME']\n        ephem.cospar_id = ephem_dict['OBJECT_ID']\n        ephems[i] = ephem\n\n    if len(ephems) == 1:\n        return ephems[0]\n\n    return ephems", "response": "Reads the OEM file and returns a list of Ephem objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _read_opm(string):\n\n    maneuvers = []\n\n    data = {}\n    comments = {}\n    for i, line in enumerate(string.splitlines()):\n        if not line:\n            continue\n        if line.startswith(\"COMMENT\"):\n            comments[i] = line.split(\"COMMENT\")[-1].strip()\n            continue\n\n        key, _, value = line.partition(\"=\")\n\n        key = key.strip()\n        value = value.strip()\n\n        if key.startswith('MAN_'):\n            if key == \"MAN_EPOCH_IGNITION\":\n                maneuvers.append({})\n                man_idx = len(maneuvers) - 1\n                if i - 1 in comments:\n                    maneuvers[man_idx][\"comment\"] = comments[i - 1]\n            maneuvers[man_idx][key] = value\n        else:\n            data[key] = value\n\n    try:\n        name = data['OBJECT_NAME']\n        cospar_id = data['OBJECT_ID']\n        scale = data['TIME_SYSTEM']\n        frame = data['REF_FRAME']\n        date = Date.strptime(data['EPOCH'], \"%Y-%m-%dT%H:%M:%S.%f\", scale=scale)\n        vx = _float(data['X_DOT'])\n        vy = _float(data['Y_DOT'])\n        vz = _float(data['Z_DOT'])\n        x = _float(data['X'])\n        y = _float(data['Y'])\n        z = _float(data['Z'])\n    except KeyError as e:\n        raise ValueError('Missing mandatory parameter')\n\n    orb = Orbit(date, [x, y, z, vx, vy, vz], 'cartesian', frame, None)\n    orb.name = name\n    orb.cospar_id = cospar_id\n\n    for raw_man in maneuvers:\n\n        man = {}\n        man['date'] = Date.strptime(raw_man['MAN_EPOCH_IGNITION'], \"%Y-%m-%dT%H:%M:%S.%f\", scale=scale)\n        man['duration'] = timedelta(seconds=_float(raw_man['MAN_DURATION']))\n        man['frame'] = raw_man['MAN_REF_FRAME'] if raw_man['MAN_REF_FRAME'] != frame else None\n        man['delta_mass'] = raw_man['MAN_DELTA_MASS']\n        man['comment'] = raw_man.get('comment')\n\n        for i in range(1, 4):\n            man.setdefault('dv', []).append(_float(raw_man['MAN_DV_{}'.format(i)]))\n\n        if man['duration'].total_seconds() == 0:\n            orb.maneuvers.append(Maneuver(man['date'], man['dv'], frame=man['frame'], comment=man['comment']))\n\n    if 'CX_X' in data:\n\n        frame = data.get('COV_REF_FRAME', orb.cov.PARENT_FRAME)\n        if frame in ('RSW', 'RTN'):\n            frame = \"QSW\"\n\n        values = [\n            [data['CX_X'],     data['CY_X'],     data['CZ_X'],     data['CX_DOT_X'],     data['CY_DOT_X'],     data['CZ_DOT_X']],\n            [data['CY_X'],     data['CY_Y'],     data['CZ_Y'],     data['CX_DOT_Y'],     data['CY_DOT_Y'],     data['CZ_DOT_Y']],\n            [data['CZ_X'],     data['CZ_Y'],     data['CZ_Z'],     data['CX_DOT_Z'],     data['CY_DOT_Z'],     data['CZ_DOT_Z']],\n            [data['CX_DOT_X'], data['CX_DOT_Y'], data['CX_DOT_Z'], data['CX_DOT_X_DOT'], data['CY_DOT_X_DOT'], data['CZ_DOT_X_DOT']],\n            [data['CY_DOT_X'], data['CY_DOT_Y'], data['CY_DOT_Z'], data['CY_DOT_X_DOT'], data['CY_DOT_Y_DOT'], data['CZ_DOT_Y_DOT']],\n            [data['CZ_DOT_X'], data['CZ_DOT_Y'], data['CZ_DOT_Z'], data['CZ_DOT_X_DOT'], data['CZ_DOT_Y_DOT'], data['CZ_DOT_Z_DOT']]\n        ]\n\n        orb.cov = np.array(values).astype(np.float) * 1e6\n        orb.cov._frame = frame\n\n    return orb", "response": "Reads an OPM string into a Orbit object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _tab(element):\n\n    elements = {\n        'x': 'tab5.2a.txt',\n        'y': 'tab5.2b.txt',\n        's': 'tab5.2d.txt'\n    }\n\n    if element.lower() not in elements.keys():\n        raise ValueError('Unknown element \\'%s\\'' % element)\n\n    filepath = Path(__file__).parent / \"data\" / elements[element.lower()]\n\n    total = []\n    with filepath.open() as fhd:\n\n        for line in fhd.read().splitlines():\n\n            line = line.strip()\n\n            if line.startswith(\"#\") or not line.strip():\n                continue\n\n            if line.startswith('j = '):\n                result = []\n                total.append(result)\n                continue\n\n            # The first field is only an index\n            fields = line.split()[1:]\n            fields[:2] = [float(x) for x in fields[:2]]\n            fields[2:] = [int(x) for x in fields[2:]]\n            result.append(fields)\n\n    return total", "response": "Extract and caching of IAU2000 nutation coefficients from the tab file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _earth_orientation(date):\n\n    ttt = date.change_scale('TT').julian_century\n    # a_a = 0.12\n    # a_c = 0.26\n    # s_prime = -0.0015 * (a_c ** 2 / 1.2 + a_a ** 2) * ttt\n    s_prime = - 0.000047 * ttt\n\n    return date.eop.x / 3600., date.eop.y / 3600., s_prime / 3600", "response": "Earth orientation parameters in degrees\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef earth_orientation(date):\n\n    x_p, y_p, s_prime = np.deg2rad(_earth_orientation(date))\n    return rot3(-s_prime) @ rot2(x_p) @ rot1(y_p)", "response": "Earth orientation as a rotating matrix"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _xysxy2(date):\n\n    planets = _planets(date)\n    x_tab, y_tab, s_tab = _tab('X'), _tab('Y'), _tab('s')\n\n    ttt = date.change_scale('TT').julian_century\n\n    # Units: micro-arcsecond\n    X = -16616.99 + 2004191742.88 * ttt - 427219.05 * ttt ** 2 - 198620.54 * ttt ** 3\\\n        - 46.05 * ttt ** 4 + 5.98 * ttt ** 5\n\n    Y = -6950.78 - 25381.99 * ttt - 22407250.99 * ttt ** 2 + 1842.28 * ttt ** 3\\\n        + 1113.06 * ttt ** 4 + 0.99 * ttt ** 5\n\n    s_xy2 = 94.0 + 3808.65 * ttt - 122.68 * ttt ** 2 - 72574.11 * ttt ** 3\\\n        + 27.98 * ttt ** 4 + 15.62 * ttt ** 5\n\n    for j in range(5):\n\n        _x, _y, _s = 0, 0, 0\n        for i in range(len(x_tab[j])):\n            Axs, Axc, *p_coefs = x_tab[j][i]\n            ax_p = np.dot(p_coefs, planets)\n            _x += Axs * np.sin(ax_p) + Axc * np.cos(ax_p)\n\n        for i in range(len(y_tab[j])):\n            Ays, Ayc, *p_coefs = y_tab[j][i]\n            ay_p = np.dot(p_coefs, planets)\n            _y += Ays * np.sin(ay_p) + Ayc * np.cos(ay_p)\n\n        for i in range(len(s_tab[j])):\n            Ass, Asc, *p_coefs = s_tab[j][i]\n            as_p = np.dot(p_coefs, planets)\n            _s += Ass * np.sin(as_p) + Asc * np.cos(as_p)\n\n        X += _x * ttt ** j\n        Y += _y * ttt ** j\n        s_xy2 += _s * ttt ** j\n\n    # Conversion to arcsecond\n    return X * 1e-6, Y * 1e-6, s_xy2 * 1e-6", "response": "This function is a function that takes the formulas of Axs Axc s + XY2 and returns the x y s + XY2 in the arcsecond\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets The X Y and s coordinates of the current log entry.", "response": "def _xys(date):\n    \"\"\"Get The X, Y and s coordinates\n\n    Args:\n        date (Date):\n    Return:\n        3-tuple of float: Values of X, Y and s, in radians\n    \"\"\"\n\n    X, Y, s_xy2 = _xysxy2(date)\n\n    # convert milli-arcsecond to arcsecond\n    dX, dY = date.eop.dx / 1000., date.eop.dy / 1000.\n\n    # Convert arcsecond to degrees then to radians\n    X = np.radians((X + dX) / 3600.)\n    Y = np.radians((Y + dY) / 3600.)\n    s = np.radians(s_xy2 / 3600.) - (X * Y / 2)\n\n    return X, Y, s"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef precesion_nutation(date):\n\n    X, Y, s = _xys(date)\n\n    d = np.arctan(np.sqrt((X**2 + Y**2) / (1 - X ** 2 - Y ** 2)))\n    a = 1 / (1 + np.cos(d))\n\n    return np.array([\n        [1 - a * X ** 2, -a * X * Y, X],\n        [-a * X * Y, 1 - a * Y ** 2, Y],\n        [-X, -Y, 1 - a * (X**2 + Y**2)]\n    ]) @ rot3(s)", "response": "Precession and nutation joint rotation matrix for the IAU2010 model."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef orbit(self, orbit):\n\n        self._orbit = orbit\n        tle = Tle.from_orbit(orbit)\n        lines = tle.text.splitlines()\n\n        if len(lines) == 3:\n            _, line1, line2 = lines\n        else:\n            line1, line2 = lines\n\n        self.tle = twoline2rv(line1, line2, wgs72)", "response": "Initialize the propagator\n            object"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef propagate(self, date):\n\n        if type(date) is timedelta:\n            date = self.orbit.date + date\n\n        # Convert the date to a tuple usable by the sgp4 library\n        _date = [float(x) for x in \"{:%Y %m %d %H %M %S.%f}\".format(date).split()]\n        p, v = self.tle.propagate(*_date)\n\n        # Convert from km to meters\n        result = [x * 1000 for x in p + v]\n\n        return self.orbit.__class__(\n            date,\n            result,\n            'cartesian',\n            'TEME',\n            self.__class__(),\n            **self.orbit.complements\n        )", "response": "Propagate the initialized orbit to a date."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_qsw(orbit):\n\n    pos, vel = _split(orbit)\n\n    q = pos / norm(pos)\n    w = np.cross(pos, vel) / (norm(pos) * norm(vel))\n    s = np.cross(w, q)\n\n    return np.array([q, s, w])", "response": "In the QSW Local Orbital Reference Frame x z is oriented along the position vector z along the angular momentum and y complete the frame."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes state of orbit at a given date past or future.", "response": "def propagate(self, date):\n        \"\"\"Compute state of orbit at a given date, past or future\n\n        Args:\n            date (Date)\n        Return:\n            Orbit:\n        \"\"\"\n\n        i0, \u03a90, e0, \u03c90, M0, n0 = self.tle\n        n0 *= 60  # conversion to min\u207b\u00b9\n        if isinstance(date, Date):\n            t0 = self.tle.date.datetime\n            tdiff = (date.datetime - t0).total_seconds() / 60.\n        elif isinstance(date, timedelta):\n            tdiff = date.total_seconds() / 60.\n            date = self.tle.date + date\n        else:\n            raise TypeError(\"Unhandled type for 'date': %s\" % type(date))\n\n        bstar = self.tle.complements['bstar']\n        \u00b5 = self.gravity.\u00b5_e\n        r_e = self.gravity.r_e\n        k_e = self.gravity.k_e\n\n        # retrieve initialized variables\n        _i = self._init\n        n0 = _i.n0\n\n        Mdf = M0 + _i.Mdot * n0 * tdiff\n        \u03c9df = \u03c90 + _i.\u03c9dot * n0 * tdiff\n        \u03a9df = \u03a90 + _i.\u03a9dot * n0 * tdiff\n\n        delta_\u03c9 = bstar * _i.C3 * cos(\u03c90) * tdiff\n        delta_M = 0.\n        if e0 > 1e-4:\n            delta_M = - 2 / 3 * (_i.q0 - _i.s) ** 4 * bstar * _i.\u03be ** 4 / (e0 * _i.\u03b7) * ((1 + _i.\u03b7 * cos(Mdf)) ** 3 - (1 + _i.\u03b7 * cos(M0)) ** 3)\n\n        Mp = (Mdf + delta_\u03c9 + delta_M) % (2 * np.pi)\n        \u03c9 = \u03c9df - delta_\u03c9 - delta_M\n        \u03a9 = \u03a9df - 21 * n0 * _i.k2 * _i.\u03b8 / (2 * _i.a0 ** 2 * _i.\u03b2_0 ** 2) * _i.C1 * tdiff ** 2\n        e = e0 - bstar * _i.C4 * tdiff - bstar * _i.C5 * (sin(Mp) - sin(M0))\n\n        if e < 1e-6:\n            e = 1e-6\n\n        a = _i.a0 * (1 - _i.C1 * tdiff - _i.D2 * tdiff ** 2 - _i.D3 * tdiff ** 3 - _i.D4 * tdiff ** 4) ** 2\n\n        L = Mp + \u03c9 + \u03a9 + n0 * (3 / 2 * _i.C1 * tdiff ** 2 + (_i.D2 + 2 * _i.C1 ** 2) * tdiff ** 3 + 1 / 4 * (3 * _i.D3 + 12 * _i.C1 * _i.D2 + 10 * _i.C1 ** 3) * tdiff ** 4 + 1 / 5 * (3 * _i.D4 + 12 * _i.C1 * _i.D3 + 6 * _i.D2 ** 2 + 30 * _i.C1 ** 2 * _i.D2 + 15 * _i.C1 ** 4) * tdiff ** 5)\n\n        \u03b2 = sqrt(1 - e ** 2)\n        n = \u00b5 / (a ** (3 / 2))\n\n        # Long-period terms\n        axN = e * cos(\u03c9)\n        ayNL = _i.A30 * sin(i0) / (4 * _i.k2 * a * \u03b2 ** 2)\n        tmp = (1 + _i.\u03b8) if (1 + _i.\u03b8) > 1.5e-12 else 1.5e-12\n        L_L = ayNL / 2 * axN * ((3 + 5 * _i.\u03b8) / tmp)\n\n        L_T = L + L_L\n        ayN = e * sin(\u03c9) + ayNL\n\n        # Resolving of kepler equation\n        U = (L_T - \u03a9) % (2 * np.pi)\n        Ep\u03c9 = U\n        for xxx in range(10):\n            delta_Ep\u03c9 = (U - ayN * cos(Ep\u03c9) + axN * sin(Ep\u03c9) - Ep\u03c9) / (1 - ayN * sin(Ep\u03c9) - axN * cos(Ep\u03c9))\n            if abs(delta_Ep\u03c9) < 1e-12:\n                break\n            Ep\u03c9 = Ep\u03c9 + delta_Ep\u03c9\n\n        # Short-period terms\n        ecosE = axN * cos(Ep\u03c9) + ayN * sin(Ep\u03c9)\n        esinE = axN * sin(Ep\u03c9) - ayN * cos(Ep\u03c9)\n        e_L = sqrt(axN ** 2 + ayN ** 2)\n        p_L = a * (1 - e_L ** 2)\n        r = a * (1 - ecosE)\n        rdot = sqrt(a) / r * esinE\n        rfdot = sqrt(p_L) / r\n\n        cosu = a / r * (cos(Ep\u03c9) - axN + ayN * esinE / (1 + sqrt(1 - e_L ** 2)))\n        sinu = a / r * (sin(Ep\u03c9) - ayN - axN * esinE / (1 + sqrt(1 - e_L ** 2)))\n        u = arctan2(sinu, cosu)\n\n        Delta_r = _i.k2 / (2 * p_L) * (1 - _i.\u03b8 ** 2) * cos(2 * u)\n        Delta_u = - _i.k2 / (4 * p_L ** 2) * (7 * _i.\u03b8 ** 2 - 1) * sin(2 * u)\n        Delta_\u03a9 = 3 * _i.k2 * _i.\u03b8 / (2 * p_L ** 2) * sin(2 * u)\n        Delta_i = 3 * _i.k2 * _i.\u03b8 / (2 * p_L ** 2) * sin(i0) * cos(2 * u)\n        Delta_rdot = - n * _i.k2 * (1 - _i.\u03b8 ** 2) * sin(2 * u) / (p_L * \u00b5)\n        Delta_rfdot = _i.k2 * n * ((1 - _i.\u03b8 ** 2) * cos(2 * u) - 3 / 2 * (1 - 3 * _i.\u03b8 ** 2)) / (p_L * \u00b5)\n\n        rk = r * (1 - 3 / 2 * _i.k2 * sqrt(1 - e_L ** 2) / (p_L ** 2) * (3 * _i.\u03b8 ** 2 - 1)) + Delta_r\n        uk = u + Delta_u\n        \u03a9k = \u03a9 + Delta_\u03a9\n        ik = i0 + Delta_i\n        rdotk = rdot + Delta_rdot\n        rfdotk = rfdot + Delta_rfdot\n\n        # Vectors\n        vM = np.array([- sin(\u03a9k) * cos(ik), cos(\u03a9k) * cos(ik), sin(ik)])\n        vN = np.array([cos(\u03a9k), sin(\u03a9k), 0])\n\n        vU = vM * sin(uk) + vN * cos(uk)\n        vV = vM * cos(uk) - vN * sin(uk)\n\n        vR = rk * vU * r_e\n        vRdot = (rdotk * vU + rfdotk * vV) * (r_e * k_e / 60.)\n\n        vector = np.concatenate((vR, vRdot)) * 1000  # conversion to meters\n\n        return self.tle.__class__(date, vector, 'cartesian', 'TEME', self.__class__(), **self.tle.complements)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _float(text):\n\n    text = text.strip()\n\n    if text[0] in ('-', '+'):\n        text = \"%s.%s\" % (text[0], text[1:])\n    else:\n        text = \"+.%s\" % text\n\n    if \"+\" in text[1:] or \"-\" in text[1:]:\n        value, exp_sign, expo = text.rpartition('+') if '+' in text[1:] else text.rpartition('-')\n        v = float('{value}e{exp_sign}{expo}'.format(value=value, exp_sign=exp_sign, expo=expo))\n    else:\n        v = float(text)\n\n    return v", "response": "Convert the decimal point assumed format of TLE to actual float"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfunction to convert float to decimal point assumed", "response": "def _unfloat(flt, precision=5):\n    \"\"\"Function to convert float to 'decimal point assumed' format\n\n    >>> _unfloat(0)\n    '00000-0'\n    >>> _unfloat(3.4473e-4)\n    '34473-3'\n    >>> _unfloat(-6.0129e-05)\n    '-60129-4'\n    >>> _unfloat(4.5871e-05)\n    '45871-4'\n    \"\"\"\n\n    if flt == 0.:\n        return \"{}-0\".format(\"0\" * precision)\n\n    num, _, exp = \"{:.{}e}\".format(flt, precision - 1).partition('e')\n    exp = int(exp)\n    num = num.replace('.', '')\n\n    return \"%s%d\" % (num, exp + 1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _check_validity(cls, text):\n\n        if not text[0].lstrip().startswith('1 ') or not text[1].lstrip().startswith('2 '):\n            raise ValueError(\"Line number check failed\")\n\n        for line in text:\n            line = line.strip()\n            if str(cls._checksum(line)) != line[-1]:\n                raise ValueError(\"Checksum validation failed\")", "response": "Check the validity of a TLE\n           "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _checksum(cls, line):\n        tr_table = str.maketrans({c: None for c in ascii_uppercase + \"+ .\"})\n        no_letters = line[:68].translate(tr_table).replace(\"-\", \"1\")\n        return sum([int(l) for l in no_letters]) % 10", "response": "Compute the checksum of a full line of a naculian object"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting the TLE to a list representation.", "response": "def to_list(self):\n        \"\"\"Convert the tle to a list representation, with the order as it can be found in the TLE\n        representation.\n        \"\"\"\n        return [self.i, self.\u03a9, self.e, self.\u03c9, self.M, self.n]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert TLE to Orbit object in order to make computations on it", "response": "def orbit(self):\n        \"\"\"Convert TLE to Orbit object, in order to make computations on it\n\n        Return:\n            ~beyond.orbits.orbit.Orbit:\n        \"\"\"\n        data = {\n            'bstar': self.bstar,\n            'ndot': self.ndot,\n            'ndotdot': self.ndotdot,\n            'tle': self.text\n        }\n        return Orbit(self.epoch, self.to_list(), \"TLE\", \"TEME\", 'Sgp4', **data)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert an orbit to it s TLE representation.", "response": "def from_orbit(cls, orbit, name=None, norad_id=None, cospar_id=None):\n        \"\"\"Convert an orbit to it's TLE representation\n\n        Args:\n            orbit (Orbit)\n            norad_id (str or int):\n            cospar_id (str):\n        Return:\n            str: TLE representation\n        \"\"\"\n\n        name = \"0 %s\\n\" % name if name is not None else \"\"\n        norad_id = norad_id if norad_id is not None else \"99999\"\n\n        if cospar_id is not None:\n            y, _, i = cospar_id.partition('-')\n            cospar_id = y[2:] + i\n        else:\n            cospar_id = \"\"\n\n        orbit = orbit.copy(form='TLE', frame='TEME')\n\n        date = orbit.date.datetime\n        i, \u03a9, e, \u03c9, M, n = orbit\n\n        line1 = \"1 {norad_id}U {cospar_id:<8} {date:%y}{day:012.8f} {ndot:>10} {ndotdot:>8} {bstar:>8} 0  999\".format(\n            norad_id=norad_id,\n            cospar_id=cospar_id,\n            date=date,\n            day=int(\"{:%j}\".format(date)) + date.hour / 24. + date.minute / 1440 + date.second / 86400 + date.microsecond / 86400000000.,\n            ndot=\"{: 0.8f}\".format(orbit.complements['ndot'] / 2).replace(\"0.\", \".\"),\n            ndotdot=_unfloat(orbit.complements['ndotdot'] / 6),\n            bstar=_unfloat(orbit.complements['bstar']),\n        )\n        line2 = \"2 {norad_id} {i:8.4f} {\u03a9:8.4f} {e} {\u03c9:8.4f} {M:8.4f} {n:11.8f}99999\".format(\n            norad_id=norad_id,\n            i=np.degrees(i),\n            \u03a9=np.degrees(\u03a9),\n            e=\"{:.7f}\".format(e)[2:],\n            \u03c9=np.degrees(\u03c9),\n            M=np.degrees(M),\n            n=n * 86400 / (2 * np.pi)\n        )\n\n        line1 += str(cls._checksum(line1))\n        line2 += str(cls._checksum(line2))\n\n        return cls(\"%s%s\\n%s\" % (name, line1, line2))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_string(cls, text, comments=\"#\", error=\"warn\"):\n\n        cache = []\n        for line in text.splitlines():\n\n            # If the line is empty or begins with a comment mark, we skip it.\n            if not line.strip() or line.startswith(comments):\n                continue\n\n            # The startswith conditions include a blank space in order to not take into account\n            # lines containing only a COSPAR ID, which happens when an object is detected but the\n            # JSpOc doesn't know what is the source yet.\n            if line.startswith('1 '):\n                cache.append(line)\n            elif line.startswith('2 '):\n                cache.append(line)\n                try:\n                    yield cls(\"\\n\".join(cache))\n                except ValueError as e:\n                    if error in ('raise', 'warn'):\n                        if error == \"raise\":\n                            raise\n                        else:\n                            log.warning(str(e))\n\n                cache = []\n            else:\n                # In the 3LE format, the first line (numbered 0, or unnumbered) contains the name\n                # of the satellite\n                # In the TLE format, this line doesn't exists.\n                cache = [line]", "response": "Generator of TLEs from a string containing many TLEs."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef orbit2frame(name, ref_orbit, orientation=None, center=None, bypass=False):\n\n    if orientation is None:\n        orientation = ref_orbit.frame.orientation\n    elif orientation.upper() in (\"RSW\", \"LVLH\"):\n        orientation = \"QSW\"\n    elif orientation.upper() not in (\"QSW\", \"TNW\"):\n        raise ValueError(\"Unknown orientation '%s'\" % orientation)\n\n    if center is None:\n        center = Earth\n\n    def _to_parent_frame(self):\n        \"\"\"Conversion from orbit frame to parent frame\n        \"\"\"\n        offset = ref_orbit.propagate(self.date).base.copy()\n\n        if orientation.upper() in (\"QSW\", \"TNW\"):\n\n            # propagation of the reference orbit to the date of the\n            # converted orbit\n            orb = ref_orbit.propagate(self.date)\n\n            m = to_qsw(orb) if orientation.upper() == \"QSW\" else to_tnw(orb)\n\n            # we transpose the matrix because it represents the conversion\n            # from inertial to local frame, and we'd like the other way around\n            rotation = Frame._convert(m, m).T\n        else:\n            # The orientation is the same as the parent reference frame\n            rotation = np.identity(6)\n\n        return rotation, offset\n\n    # define the name of the method of conversion\n    mtd = '_to_%s' % ref_orbit.frame.__name__\n\n    # dictionary which defines attributes of the created class\n    dct = {\n        mtd: _to_parent_frame,\n        \"orientation\": orientation,\n        \"center\": center,\n        \"bypass\": bypass,\n    }\n\n    # Creation of the class\n    cls = _MetaFrame(name, (Frame,), dct)\n\n    # Link to the parent\n    cls + ref_orbit.frame\n    return cls", "response": "Create a new frame based on an orbit."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchanges the frame of the orbit", "response": "def transform(self, new_frame):\n        \"\"\"Change the frame of the orbit\n\n        Args:\n            new_frame (str)\n        Return:\n            numpy.ndarray\n        \"\"\"\n\n        steps = self.__class__.steps(new_frame)\n\n        orbit = self.orbit\n\n        for _from, _to in steps:\n\n            from_obj = _from(self.date, orbit)\n            direct = \"_to_%s\" % _to\n\n            if hasattr(from_obj, direct):\n                rotation, offset = getattr(from_obj, direct)()\n            else:\n                to_obj = _to(self.date, orbit)\n                inverse = \"_to_%s\" % _from\n                if hasattr(to_obj, inverse):\n                    rotation, offset = getattr(to_obj, inverse)()\n                    rotation = rotation.T\n                    offset = - offset\n                else:\n                    raise NotImplementedError(\"Unknown transformation {} to {}\".format(_from, _to))\n\n            if getattr(_from, \"_rotation_before_translation\", False):\n                # In case of topocentric frame, the rotation is done before the translation\n                orbit = offset + (rotation @ orbit)\n            else:\n                orbit = rotation @ (offset + orbit)\n\n        return orbit"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _scale_tdb_minus_tt(self, mjd, eop):\n        jd = mjd + Date.JD_MJD\n        jj = Date._julian_century(jd)\n        m = radians(357.5277233 + 35999.05034 * jj)\n        delta_lambda = radians(246.11 + 0.90251792 * (jd - 2451545.))\n        return 0.001657 * sin(m) + 0.000022 * sin(delta_lambda)", "response": "Definition of the Barycentric Dynamic Time scale relatively to Terrestrial Time\n       "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef offset(self, mjd, new_scale, eop):\n\n        delta = 0\n        for one, two in self.steps(new_scale):\n            one = one.name.lower()\n            two = two.name.lower()\n            # find the operation\n            oper = \"_scale_{}_minus_{}\".format(two, one)\n            # find the reverse operation\n            roper = \"_scale_{}_minus_{}\".format(one, two)\n            if hasattr(self, oper):\n                delta += getattr(self, oper)(mjd, eop)\n            elif hasattr(self, roper):\n                delta -= getattr(self, roper)(mjd, eop)\n            else:  # pragma: no cover\n                raise DateError(\"Unknown convertion {} => {}\".format(one, two))\n\n        return delta", "response": "Compute the offset necessary in order to convert from one time - scale to another time - scale."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _convert_to_scale(self):\n        d = self._d\n        s = (self._s - self._offset) % 86400.\n        d -= int((s + self._offset) // 86400)\n        return d, s", "response": "Convert the inner value of the object into the given scale\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef datetime(self):\n        if 'dt_scale' not in self._cache.keys():\n            self._cache['dt_scale'] = self._datetime - timedelta(seconds=self._offset)\n        return self._cache['dt_scale']", "response": "Conversion of the Date object into a datetime. datetime object"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _datetime(self):\n        if 'dt' not in self._cache.keys():\n            self._cache['dt'] = self.MJD_T0 + timedelta(days=self._d, seconds=self._s)\n        return self._cache['dt']", "response": "Conversion of the Date object into a datetime. datetime."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting a string representation of a date to a Date object", "response": "def strptime(cls, data, format, scale=DEFAULT_SCALE):  # pragma: no cover\n        \"\"\"Convert a string representation of a date to a Date object\n        \"\"\"\n        return cls(datetime.strptime(data, format), scale=scale)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a new object with the new scale adjusted.", "response": "def change_scale(self, new_scale):\n        \"\"\"\n        Args:\n            new_scale (str)\n        Return:\n            Date\n        \"\"\"\n        offset = self.scale.offset(self._mjd, new_scale, self.eop)\n        result = self.datetime + timedelta(seconds=offset)\n\n        return self.__class__(result, scale=new_scale)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the rotation matrix around the X - axis of the base object", "response": "def rot1(theta):\n    \"\"\"\n    Args:\n        theta (float): Angle in radians\n    Return:\n        Rotation matrix of angle theta around the X-axis\n    \"\"\"\n    return np.array([\n        [1, 0, 0],\n        [0, np.cos(theta), np.sin(theta)],\n        [0, -np.sin(theta), np.cos(theta)]\n    ])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef rot2(theta):\n    return np.array([\n        [np.cos(theta), 0, -np.sin(theta)],\n        [0, 1, 0],\n        [np.sin(theta), 0, np.cos(theta)]\n    ])", "response": "Returns the rotation matrix around the Y - axis"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef rot3(theta):\n    return np.array([\n        [np.cos(theta), np.sin(theta), 0],\n        [-np.sin(theta), np.cos(theta), 0],\n        [0, 0, 1]\n    ])", "response": "Returns the rotation matrix around the Z - axis"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving a value from the config", "response": "def get(self, *keys, fallback=None):\n        \"\"\"Retrieve a value in the config, if the value is not available\n        give the fallback value specified.\n        \"\"\"\n\n        section, *keys = keys\n        out = super().get(section, fallback)\n\n        while isinstance(out, dict):\n            key = keys.pop(0)\n            out = out.get(key, fallback)\n\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets a value in the config dictionary.", "response": "def set(self, *args):\n        \"\"\"Set a value in the config dictionary\n\n        The last argument is the value to set\n\n        Example:\n\n        .. code-block:: python\n\n            config.set('aaa', 'bbb', True)\n            print(config)\n            # {\n            #     'aaa': {\n            #         'bbb': True\n            #     }\n            # }\n        \"\"\"\n\n        # split arguments in keys and value\n        *first_keys, last_key, value = args\n\n        subdict = self\n        for k in first_keys:\n            subdict.setdefault(k, {})\n            subdict = subdict[k]\n\n        subdict[last_key] = value"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _newton(self, orb, step):\n\n        date = orb.date + step\n\n        new_body = zeros(6)\n        new_body[:3] = orb[3:]\n\n        for body in self.bodies:\n            # retrieve the position of the body at the given date\n            orb_body = body.propagate(date)\n            orb_body.frame = orb.frame\n\n            # Compute induced attraction to the object of interest\n            diff = orb_body[:3] - orb[:3]\n            norm = sqrt(sum(diff ** 2)) ** 3\n            new_body[3:] += G * body.mass * diff / norm\n\n        return new_body", "response": "Compute the newton s body for a given Universal Gravitation."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _make_step(self, orb, step):\n\n        method = self.BUTCHER[self.method]\n        a, b, c = method['a'], method['b'], method['c']\n\n        y_n = orb.copy()\n        ks = [self._newton(y_n, timedelta(0))]\n\n        for a, c in zip(a[1:], c[1:]):\n            k_plus1 = self._newton(y_n + a @ ks * step.total_seconds(), step * c)\n            ks.append(k_plus1)\n\n        y_n_1 = y_n + step.total_seconds() * b @ ks\n        y_n_1.date = y_n.date + step\n\n        # Error estimation, in cases where adaptively methods are used\n        # if 'b_star' in method:\n        #     error = step.total_seconds() * (b - method['b_star']) @ ks\n\n        for man in self.orbit.maneuvers:\n            if man.check(orb, step):\n                log.debug(\"Applying maneuver at {} ({})\".format(man.date, man.comment))\n                y_n_1[3:] += man.dv(y_n_1)\n\n        return y_n_1", "response": "Compute the next step with the selected method"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nevaluates the need for SOI transition by comparing the radial distance between the considered body and the spacecraft", "response": "def _soi(self, orb):\n        \"\"\"Evaluate the need for SOI transition, by comparing the radial distance\n        between the considered body and the spacecraft\n\n        If therer is no body in sight, default to central body.\n        \"\"\"\n\n        for body in self.alt:\n            soi = self.SOI[body.name]\n            sph = orb.copy(frame=soi.frame, form='spherical')\n            if sph.r < soi.radius:\n                active = body\n                break\n        else:\n            active = self.central\n\n        return active"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _change_soi(self, body):\n\n        if body == self.central:\n            self.bodies = [self.central]\n            self.step = self.central_step\n            self.active = self.central.name\n            self.frame = self.central.name\n        else:\n            soi = self.SOI[body.name]\n            self.bodies = [body]\n            self.step = self.alt_step\n            self.active = body.name\n            self.frame = soi.frame", "response": "Modify the inner parameters of the Kepler propagator in order to place\n            the spacecraft in the right Sphere of Influence\n           "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nproviding the last and next leap - second events relative to a date in MJD", "response": "def get_last_next(self, date):\n        \"\"\"Provide the last and next leap-second events relative to a date\n\n        Args:\n            date (float): Date in MJD\n        Return:\n            tuple:\n        \"\"\"\n        past, future = (None, None), (None, None)\n\n        for mjd, value in reversed(self.data):\n            if mjd <= date:\n                past = (mjd, value)\n                break\n            future = (mjd, value)\n\n        return past, future"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef db(cls, dbname=None):\n\n        cls._load_entry_points()\n\n        dbname = dbname or config.get('eop', 'dbname', fallback=cls.DEFAULT_DBNAME)\n\n        if dbname not in cls._dbs.keys():\n            raise EopError(\"Unknown database '%s'\" % dbname)\n\n        if isclass(cls._dbs[dbname]):\n            # Instanciation\n            try:\n                cls._dbs[dbname] = cls._dbs[dbname]()\n            except Exception as e:\n                # Keep the exception in cache in order to not retry instanciation\n                # every single time EopDb.db() is called, as instanciation\n                # of database is generally a time consumming operation.\n                # If it failed once, it will most probably fail again\n                cls._dbs[dbname] = e\n\n        if isinstance(cls._dbs[dbname], Exception):\n            raise EopError(\"Problem at database instanciation\") from cls._dbs[dbname]\n\n        return cls._dbs[dbname]", "response": "Retrieve the database with the specified name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get(cls, mjd: float, dbname: str = None) -> Eop:\n\n        try:\n            value = cls.db(dbname)[mjd]\n        except (EopError, KeyError) as e:\n            if isinstance(e, KeyError):\n                msg = \"Missing EOP data for mjd = '%s'\" % e\n            else:\n                msg = str(e)\n\n            if cls.policy() == cls.WARN:\n                log.warning(msg)\n            elif cls.policy() == cls.ERROR:\n                raise\n\n            value = Eop(x=0, y=0, dx=0, dy=0, deps=0, dpsi=0, lod=0, ut1_utc=0, tai_utc=0)\n\n        return value", "response": "Retrieve Earth Orientation Parameters and timescales differences differences\n            for a given date."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nregistering an Eop Database with the specified name.", "response": "def register(cls, klass, name=DEFAULT_DBNAME):\n        \"\"\"Register an Eop Database\n\n        The only requirement of this database is that it should have ``__getitem__``\n        method accepting MJD as float.\n        \"\"\"\n\n        if name in cls._dbs:\n            msg = \"'{}' is already registered for an Eop database. Skipping\".format(name)\n            log.warning(msg)\n        else:\n            cls._dbs[name] = klass"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _tab(max_i=None):\n\n    filepath = Path(__file__).parent / \"data\" / \"tab5.1.txt\"\n\n    result = []\n    with filepath.open() as fhd:\n        i = 0\n        for line in fhd.read().splitlines():\n            if line.startswith(\"#\") or not line.strip():\n                continue\n\n            fields = line.split()\n            result.append(([int(x) for x in fields[:5]], [float(x) for x in fields[6:]]))\n\n            i += 1\n            if max_i and i >= max_i:\n                break\n\n    return result", "response": "Extract and caching of IAU1980 nutation coefficients from a tab file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a rotation matrix that is a rotation matrix that rotates the precession of the given date.", "response": "def precesion(date):  # pragma: no cover\n    \"\"\"Precession as a rotation matrix\n    \"\"\"\n    zeta, theta, z = np.deg2rad(_precesion(date))\n    return rot3(zeta) @ rot2(-theta) @ rot3(z)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _nutation(date, eop_correction=True, terms=106):\n\n    ttt = date.change_scale('TT').julian_century\n\n    r = 360.\n\n    # in arcsecond\n    epsilon_bar = 84381.448 - 46.8150 * ttt - 5.9e-4 * ttt ** 2\\\n        + 1.813e-3 * ttt ** 3\n\n    # Conversion to degrees\n    epsilon_bar /= 3600.\n\n    # mean anomaly of the moon\n    m_m = 134.96298139 + (1325 * r + 198.8673981) * ttt\\\n        + 0.0086972 * ttt ** 2 + 1.78e-5 * ttt ** 3\n\n    # mean anomaly of the sun\n    m_s = 357.52772333 + (99 * r + 359.0503400) * ttt\\\n        - 0.0001603 * ttt ** 2 - 3.3e-6 * ttt ** 3\n\n    # L - Omega\n    u_m_m = 93.27191028 + (1342 * r + 82.0175381) * ttt\\\n        - 0.0036825 * ttt ** 2 + 3.1e-6 * ttt ** 3\n\n    # Mean elongation of the moon from the sun\n    d_s = 297.85036306 + (1236 * r + 307.11148) * ttt\\\n        - 0.0019142 * ttt ** 2 + 5.3e-6 * ttt ** 3\n\n    # Mean longitude of the ascending node of the moon\n    om_m = 125.04452222 - (5 * r + 134.1362608) * ttt\\\n        + 0.0020708 * ttt ** 2 + 2.2e-6 * ttt ** 3\n\n    delta_psi = 0.\n    delta_eps = 0.\n    for integers, reals in _tab(terms):\n        a1, a2, a3, a4, a5 = integers\n        # Conversion from 0.1 mas to mas\n        A, B, C, D = np.array(list(reals)) / 36000000.\n        a_p = a1 * m_m + a2 * m_s + a3 * u_m_m + a4 * d_s + a5 * om_m\n        # a_p %= 360.\n        delta_psi += (A + B * ttt) * np.sin(np.deg2rad(a_p))\n        delta_eps += (C + D * ttt) * np.cos(np.deg2rad(a_p))\n\n    if eop_correction:\n        delta_eps += date.eop.deps / 3600000.\n        delta_psi += date.eop.dpsi / 3600000.\n\n    return epsilon_bar, delta_psi, delta_eps", "response": "Return the nutation model for a given date."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef equinox(date, eop_correction=True, terms=106, kinematic=True):\n    epsilon_bar, delta_psi, delta_eps = _nutation(date, eop_correction, terms)\n\n    equin = delta_psi * 3600. * np.cos(np.deg2rad(epsilon_bar))\n\n    if date.d >= 50506 and kinematic:\n        # Starting 1992-02-27, we apply the effect of the moon\n        ttt = date.change_scale('TT').julian_century\n        om_m = 125.04455501 - (5 * 360. + 134.1361851) * ttt\\\n            + 0.0020756 * ttt ** 2 + 2.139e-6 * ttt ** 3\n\n        equin += 0.00264 * np.sin(np.deg2rad(om_m)) + 6.3e-5 * np.sin(np.deg2rad(2 * om_m))\n\n    # print(\"equinox = {}\\n\".format(equin / 3600))\n    return equin / 3600.", "response": "Equinox equation in degrees"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _sideral(date, longitude=0., model='mean', eop_correction=True, terms=106):\n\n    t = date.change_scale('UT1').julian_century\n\n    # Compute GMST in seconds\n    theta = 67310.54841 + (876600 * 3600 + 8640184.812866) * t + 0.093104 * t ** 2\\\n        - 6.2e-6 * t ** 3\n\n    # Conversion from second (time) to degrees (angle)\n    theta /= 240.\n\n    if model == 'apparent':\n        theta += equinox(date, eop_correction, terms)\n\n    # Add local longitude to the sideral time\n    theta += longitude\n    # Force to 0-360 degrees range\n    theta %= 360.\n\n    return theta", "response": "Compute the sideral time at a date"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sideral(date, longitude=0., model='mean', eop_correction=True, terms=106):  # pragma: no cover\n    theta = _sideral(date, longitude, model, eop_correction, terms)\n    return rot3(np.deg2rad(-theta))", "response": "Return a rotation matrix for sideral time."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes the velocity increment in the reference frame of the orbit", "response": "def dv(self, orb):\n        \"\"\"Computation of the velocity increment in the reference frame of the orbit\n\n        Args:\n            orb (Orbit):\n        Return:\n            numpy.array: Velocity increment, length 3\n        \"\"\"\n\n        orb = orb.copy(form=\"cartesian\")\n\n        if self.frame == \"QSW\":\n            mat = to_qsw(orb).T\n        elif self.frame == \"TNW\":\n            mat = to_tnw(orb).T\n        else:\n            mat = np.identity(3)\n\n        # velocity increment in the same reference frame as the orbit\n        return mat @ self._dv"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprovide a new instance of the same point in space - time.", "response": "def copy(self, *, frame=None, form=None):\n        \"\"\"Provide a new instance of the same point in space-time\n\n        Keyword Args:\n            frame (str or Frame): Frame to convert the new instance into\n            form (str or Form): Form to convert the new instance into\n        Return:\n            Orbit:\n\n        Override :py:meth:`numpy.ndarray.copy()` to include additional\n        fields\n        \"\"\"\n\n        new_compl = {}\n        for k, v in self.complements.items():\n            new_compl[k] = v.copy() if hasattr(v, 'copy') else v\n\n        new_obj = self.__class__(\n            self.date, self.base.copy(), self.form,\n            self.frame, self.propagator.copy() if self.propagator is not None else None,\n            **new_compl\n        )\n        if frame and frame != self.frame:\n            new_obj.frame = frame\n        if form and form != self.form:\n            new_obj.form = form\n        return new_obj"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef maneuvers(self):\n        mans = self.complements.setdefault('maneuvers', [])\n\n        if isinstance(mans, Maneuver):\n            mans = [mans]\n            self.complements['maneuvers'] = mans\n\n        return mans", "response": "list of maneuver descriptions usable by the\nAttributeNames propagator."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef propagate(self, date):\n\n        if self.propagator.orbit is not self:\n            self.propagator.orbit = self\n\n        return self.propagator.propagate(date)", "response": "Propagate the orbit to a new date"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsees : py : meth : Propagator. iter", "response": "def iter(self, **kwargs):\n        \"\"\"see :py:meth:`Propagator.iter() <beyond.propagators.base.Propagator.iter>`\n        \"\"\"\n        if self.propagator.orbit is not self:\n            self.propagator.orbit = self\n\n        return self.propagator.iter(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ephemeris(self, **kwargs):\n\n        for orb in self.iter(inclusive=True, **kwargs):\n            yield orb", "response": "Generator giving the propagation of the orbit at different dates\n            start stop and step"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn an OrbitInfos object of the current object.", "response": "def infos(self):\n        \"\"\":py:class:`OrbitInfos` object of ``self``\n        \"\"\"\n        if not hasattr(self, '_infos'):\n            self._infos = OrbitInfos(self)\n        return self._infos"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef path(self, goal):\n        if goal == self.name:\n            return [self]\n\n        if goal not in self.routes:\n            raise ValueError(\"Unknown '{0}'\".format(goal))\n\n        obj = self\n        path = [obj]\n        while True:\n            obj = obj.routes[goal].direction\n            path.append(obj)\n            if obj.name == goal:\n                break\n        return path", "response": "Get the shortest way between two nodes of the graph."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the list of individual relations leading to the targeted node", "response": "def steps(self, goal):\n        \"\"\"Get the list of individual relations leading to the targeted node\n\n        Args:\n            goal (str): Name of the targeted node\n        Return:\n            list of tuple of Node\n        \"\"\"\n\n        path = self.path(goal)\n        for i in range(len(path) - 1):\n            yield path[i], path[i + 1]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting unicode objects to str", "response": "def deunicode(item):\n    \"\"\" Convert unicode objects to str \"\"\"\n    if item is None:\n        return None\n    if isinstance(item, str):\n        return item\n    if isinstance(item, six.text_type):\n        return item.encode('utf-8')\n    if isinstance(item, dict):\n        return {\n            deunicode(key): deunicode(value)\n            for (key, value) in item.items()\n        }\n    if isinstance(item, list):\n        return [deunicode(x) for x in item]\n    raise TypeError('Unhandled item type: {!r}'.format(item))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef diff_with_models(self):\n        missing_from_conf = defaultdict(set)\n\n        for model in get_models():\n            db_tables_and_columns = get_db_tables_and_columns_of_model(model)\n            for (table_name, columns) in db_tables_and_columns.items():\n                model_strategy = self.strategy.get(table_name)\n                for column in columns:\n                    if not model_strategy or column not in model_strategy:\n                        missing_from_conf[table_name].add(column)\n        return missing_from_conf", "response": "Return a dict stating the differences between current state of models\n        and the configuration itself."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef assert_true(expr, msg_fmt=\"{msg}\"):\n\n    if not expr:\n        msg = \"{!r} is not truthy\".format(expr)\n        fail(msg_fmt.format(msg=msg, expr=expr))", "response": "Fail the test unless the expression is truthy."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef assert_false(expr, msg_fmt=\"{msg}\"):\n\n    if expr:\n        msg = \"{!r} is not falsy\".format(expr)\n        fail(msg_fmt.format(msg=msg, expr=expr))", "response": "Fail the test unless the expression is falsy."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfails the test unless the expression is the constant True.", "response": "def assert_boolean_true(expr, msg_fmt=\"{msg}\"):\n    \"\"\"Fail the test unless the expression is the constant True.\n\n    >>> assert_boolean_true(True)\n    >>> assert_boolean_true(\"Hello World!\")\n    Traceback (most recent call last):\n        ...\n    AssertionError: 'Hello World!' is not True\n\n    The following msg_fmt arguments are supported:\n    * msg - the default error message\n    * expr - tested expression\n    \"\"\"\n\n    if expr is not True:\n        msg = \"{!r} is not True\".format(expr)\n        fail(msg_fmt.format(msg=msg, expr=expr))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfails the test unless the expression is constant False.", "response": "def assert_boolean_false(expr, msg_fmt=\"{msg}\"):\n    \"\"\"Fail the test unless the expression is the constant False.\n\n    >>> assert_boolean_false(False)\n    >>> assert_boolean_false(0)\n    Traceback (most recent call last):\n        ...\n    AssertionError: 0 is not False\n\n    The following msg_fmt arguments are supported:\n    * msg - the default error message\n    * expr - tested expression\n    \"\"\"\n\n    if expr is not False:\n        msg = \"{!r} is not False\".format(expr)\n        fail(msg_fmt.format(msg=msg, expr=expr))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfailing if the expression is not None.", "response": "def assert_is_none(expr, msg_fmt=\"{msg}\"):\n    \"\"\"Fail if the expression is not None.\n\n    >>> assert_is_none(None)\n    >>> assert_is_none(False)\n    Traceback (most recent call last):\n        ...\n    AssertionError: False is not None\n\n    The following msg_fmt arguments are supported:\n    * msg - the default error message\n    * expr - tested expression\n    \"\"\"\n\n    if expr is not None:\n        msg = \"{!r} is not None\".format(expr)\n        fail(msg_fmt.format(msg=msg, expr=expr))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef assert_is_not_none(expr, msg_fmt=\"{msg}\"):\n    if expr is None:\n        msg = \"expression is None\"\n        fail(msg_fmt.format(msg=msg, expr=expr))", "response": "Fail if the expression is None."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfailing unless first equals second.", "response": "def assert_equal(first, second, msg_fmt=\"{msg}\"):\n    \"\"\"Fail unless first equals second, as determined by the '==' operator.\n\n    >>> assert_equal(5, 5.0)\n    >>> assert_equal(\"Hello World!\", \"Goodbye!\")\n    Traceback (most recent call last):\n        ...\n    AssertionError: 'Hello World!' != 'Goodbye!'\n\n    The following msg_fmt arguments are supported:\n    * msg - the default error message\n    * first - the first argument\n    * second - the second argument\n    \"\"\"\n\n    if isinstance(first, dict) and isinstance(second, dict):\n        assert_dict_equal(first, second, msg_fmt)\n    elif not first == second:\n        msg = \"{!r} != {!r}\".format(first, second)\n        fail(msg_fmt.format(msg=msg, first=first, second=second))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfails if first and second are not equal.", "response": "def assert_not_equal(first, second, msg_fmt=\"{msg}\"):\n    \"\"\"Fail if first equals second, as determined by the '==' operator.\n\n    >>> assert_not_equal(5, 8)\n    >>> assert_not_equal(-7, -7.0)\n    Traceback (most recent call last):\n        ...\n    AssertionError: -7 == -7.0\n\n    The following msg_fmt arguments are supported:\n    * msg - the default error message\n    * first - the first argument\n    * second - the second argument\n    \"\"\"\n\n    if first == second:\n        msg = \"{!r} == {!r}\".format(first, second)\n        fail(msg_fmt.format(msg=msg, first=first, second=second))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfail if first and second are not equal after rounding.", "response": "def assert_almost_equal(\n    first, second, msg_fmt=\"{msg}\", places=None, delta=None\n):\n    \"\"\"Fail if first and second are not equal after rounding.\n\n    By default, the difference between first and second is rounded to\n    7 decimal places. This can be configured with the places argument.\n    Alternatively, delta can be used to specify the maximum allowed\n    difference between first and second.\n\n    If first and second can not be rounded or both places and delta are\n    supplied, a TypeError is raised.\n\n    >>> assert_almost_equal(5, 5.00000001)\n    >>> assert_almost_equal(5, 5.001)\n    Traceback (most recent call last):\n        ...\n    AssertionError: 5 != 5.001 within 7 places\n    >>> assert_almost_equal(5, 5.001, places=2)\n    >>> assert_almost_equal(5, 5.001, delta=0.1)\n\n    The following msg_fmt arguments are supported:\n    * msg - the default error message\n    * first - the first argument\n    * second - the second argument\n    * places - number of places to compare or None\n    * delta - delta or None\n    \"\"\"\n\n    if delta is not None and places is not None:\n        raise TypeError(\"'places' and 'delta' are mutually exclusive\")\n    if delta is not None:\n        if delta <= 0:\n            raise ValueError(\"delta must be larger than 0\")\n        diff = abs(second - first)\n        success = diff < delta\n        detail_msg = \"with delta={}\".format(delta)\n    else:\n        if places is None:\n            places = 7\n        success = not round(second - first, places)\n        detail_msg = \"within {} places\".format(places)\n    if not success:\n        msg = \"{!r} != {!r} {}\".format(first, second, detail_msg)\n        fail(\n            msg_fmt.format(\n                msg=msg, first=first, second=second, places=places, delta=delta\n            )\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfail unless first dictionary equals second.", "response": "def assert_dict_equal(\n    first, second, key_msg_fmt=\"{msg}\", value_msg_fmt=\"{msg}\"\n):\n    \"\"\"Fail unless first dictionary equals second.\n\n    The dictionaries are considered equal, if they both contain the same\n    keys, and their respective values are also equal.\n\n    >>> assert_dict_equal({\"foo\": 5}, {\"foo\": 5})\n    >>> assert_dict_equal({\"foo\": 5}, {})\n    Traceback (most recent call last):\n        ...\n    AssertionError: key 'foo' missing from right dict\n\n    The following key_msg_fmt arguments are supported, if the keys do not\n    match:\n    * msg - the default error message\n    * first - the first dict\n    * second - the second dict\n    * missing_keys - list of keys missing from right\n    * extra_keys - list of keys missing from left\n\n    The following value_msg_fmt arguments are supported, if a value does not\n    match:\n    * msg - the default error message\n    * first - the first dict\n    * second - the second dict\n    * key - the key where the value does not match\n    * first_value - the value in the first dict\n    * second_value - the value in the second dict\n    \"\"\"\n    first_keys = set(first.keys())\n    second_keys = set(second.keys())\n    missing_keys = list(first_keys - second_keys)\n    extra_keys = list(second_keys - first_keys)\n    if missing_keys or extra_keys:\n        if missing_keys:\n            if len(missing_keys) == 1:\n                msg = \"key {!r} missing from right dict\".format(\n                    missing_keys[0]\n                )\n            else:\n                keys = \", \".join(sorted(repr(k) for k in missing_keys))\n                msg = \"keys {} missing from right dict\".format(keys)\n        else:\n            if len(extra_keys) == 1:\n                msg = \"extra key {!r} in right dict\".format(extra_keys[0])\n            else:\n                keys = \", \".join(sorted(repr(k) for k in extra_keys))\n                msg = \"extra keys {} in right dict\".format(keys)\n        if key_msg_fmt:\n            msg = key_msg_fmt.format(\n                msg=msg,\n                first=first,\n                second=second,\n                missing_keys=missing_keys,\n                extra_keys=extra_keys,\n            )\n        raise AssertionError(msg)\n    for key in first:\n        first_value = first[key]\n        second_value = second[key]\n        msg = \"key '{}' differs: {!r} != {!r}\".format(\n            key, first_value, second_value\n        )\n        if value_msg_fmt:\n            msg = value_msg_fmt.format(\n                msg=msg,\n                first=first,\n                second=second,\n                key=key,\n                first_value=first_value,\n                second_value=second_value,\n            )\n        msg = msg.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n        assert_equal(first_value, second_value, msg_fmt=msg)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef assert_less(first, second, msg_fmt=\"{msg}\"):\n\n    if not first < second:\n        msg = \"{!r} is not less than {!r}\".format(first, second)\n        fail(msg_fmt.format(msg=msg, first=first, second=second))", "response": "Fail if first is not less than second."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef assert_less_equal(first, second, msg_fmt=\"{msg}\"):\n\n    if not first <= second:\n        msg = \"{!r} is not less than or equal to {!r}\".format(first, second)\n        fail(msg_fmt.format(msg=msg, first=first, second=second))", "response": "Fail if first is not less than or equal to second."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfail if first is not greater than second.", "response": "def assert_greater(first, second, msg_fmt=\"{msg}\"):\n    \"\"\"Fail if first is not greater than second.\n\n    >>> assert_greater('foo', 'bar')\n    >>> assert_greater(5, 5)\n    Traceback (most recent call last):\n        ...\n    AssertionError: 5 is not greater than 5\n\n    The following msg_fmt arguments are supported:\n    * msg - the default error message\n    * first - the first argument\n    * second - the second argument\n    \"\"\"\n\n    if not first > second:\n        msg = \"{!r} is not greater than {!r}\".format(first, second)\n        fail(msg_fmt.format(msg=msg, first=first, second=second))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef assert_greater_equal(first, second, msg_fmt=\"{msg}\"):\n\n    if not first >= second:\n        msg = \"{!r} is not greater than or equal to {!r}\".format(first, second)\n        fail(msg_fmt.format(msg=msg, first=first, second=second))", "response": "Fail if first is not greater than or equal to second."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef assert_regex(text, regex, msg_fmt=\"{msg}\"):\n\n    compiled = re.compile(regex)\n    if not compiled.search(text):\n        msg = \"{!r} does not match {!r}\".format(text, compiled.pattern)\n        fail(msg_fmt.format(msg=msg, text=text, pattern=compiled.pattern))", "response": "Fail if text does not match the regular expression."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfail if first and second do not refer to the same object.", "response": "def assert_is(first, second, msg_fmt=\"{msg}\"):\n    \"\"\"Fail if first and second do not refer to the same object.\n\n    >>> list1 = [5, \"foo\"]\n    >>> list2 = [5, \"foo\"]\n    >>> assert_is(list1, list1)\n    >>> assert_is(list1, list2)\n    Traceback (most recent call last):\n        ...\n    AssertionError: [5, 'foo'] is not [5, 'foo']\n\n    The following msg_fmt arguments are supported:\n    * msg - the default error message\n    * first - the first argument\n    * second - the second argument\n    \"\"\"\n\n    if first is not second:\n        msg = \"{!r} is not {!r}\".format(first, second)\n        fail(msg_fmt.format(msg=msg, first=first, second=second))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfails if first and second refer to the same object.", "response": "def assert_is_not(first, second, msg_fmt=\"{msg}\"):\n    \"\"\"Fail if first and second refer to the same object.\n\n    >>> list1 = [5, \"foo\"]\n    >>> list2 = [5, \"foo\"]\n    >>> assert_is_not(list1, list2)\n    >>> assert_is_not(list1, list1)\n    Traceback (most recent call last):\n        ...\n    AssertionError: both arguments refer to [5, 'foo']\n\n    The following msg_fmt arguments are supported:\n    * msg - the default error message\n    * first - the first argument\n    * second - the second argument\n    \"\"\"\n\n    if first is second:\n        msg = \"both arguments refer to {!r}\".format(first)\n        fail(msg_fmt.format(msg=msg, first=first, second=second))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfails if first is not in collection second.", "response": "def assert_in(first, second, msg_fmt=\"{msg}\"):\n    \"\"\"Fail if first is not in collection second.\n\n    >>> assert_in(\"foo\", [4, \"foo\", {}])\n    >>> assert_in(\"bar\", [4, \"foo\", {}])\n    Traceback (most recent call last):\n        ...\n    AssertionError: 'bar' not in [4, 'foo', {}]\n\n    The following msg_fmt arguments are supported:\n    * msg - the default error message\n    * first - the element looked for\n    * second - the container looked in\n    \"\"\"\n\n    if first not in second:\n        msg = \"{!r} not in {!r}\".format(first, second)\n        fail(msg_fmt.format(msg=msg, first=first, second=second))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfail if first is in second.", "response": "def assert_not_in(first, second, msg_fmt=\"{msg}\"):\n    \"\"\"Fail if first is in a collection second.\n\n    >>> assert_not_in(\"bar\", [4, \"foo\", {}])\n    >>> assert_not_in(\"foo\", [4, \"foo\", {}])\n    Traceback (most recent call last):\n        ...\n    AssertionError: 'foo' is in [4, 'foo', {}]\n\n    The following msg_fmt arguments are supported:\n    * msg - the default error message\n    * first - the element looked for\n    * second - the container looked in\n    \"\"\"\n    if first in second:\n        msg = \"{!r} is in {!r}\".format(first, second)\n        fail(msg_fmt.format(msg=msg, first=first, second=second))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef assert_count_equal(sequence1, sequence2, msg_fmt=\"{msg}\"):\n\n    def compare():\n        missing1 = list(sequence2)\n        missing2 = []\n        for item in sequence1:\n            try:\n                missing1.remove(item)\n            except ValueError:\n                missing2.append(item)\n        return missing1, missing2\n\n    def build_message():\n        msg = \"\"\n        if missing_from_1:\n            msg += \"missing from sequence 1: \" + \", \".join(\n                repr(i) for i in missing_from_1\n            )\n        if missing_from_1 and missing_from_2:\n            msg += \"; \"\n        if missing_from_2:\n            msg += \"missing from sequence 2: \" + \", \".join(\n                repr(i) for i in missing_from_2\n            )\n        return msg\n\n    missing_from_1, missing_from_2 = compare()\n    if missing_from_1 or missing_from_2:\n        fail(\n            msg_fmt.format(\n                msg=build_message(), first=sequence1, second=sequence2\n            )\n        )", "response": "Compare the items of two sequences ignoring order."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef assert_between(lower_bound, upper_bound, expr, msg_fmt=\"{msg}\"):\n\n    if not lower_bound <= expr <= upper_bound:\n        msg = \"{!r} is not between {} and {}\".format(\n            expr, lower_bound, upper_bound\n        )\n        fail(\n            msg_fmt.format(\n                msg=msg, lower=lower_bound, upper=upper_bound, expr=expr\n            )\n        )", "response": "Fail if an expression is not between certain bounds."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef assert_is_instance(obj, cls, msg_fmt=\"{msg}\"):\n    if not isinstance(obj, cls):\n        msg = \"{!r} is an instance of {!r}, expected {!r}\".format(\n            obj, obj.__class__, cls\n        )\n        types = cls if isinstance(cls, tuple) else (cls,)\n        fail(msg_fmt.format(msg=msg, obj=obj, types=types))", "response": "Fail if an object is not an instance of a class or tuple of classes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfail is an object does not have an attribute.", "response": "def assert_has_attr(obj, attribute, msg_fmt=\"{msg}\"):\n    \"\"\"Fail is an object does not have an attribute.\n\n    >>> assert_has_attr([], \"index\")\n    >>> assert_has_attr([], \"i_do_not_have_this\")\n    Traceback (most recent call last):\n        ...\n    AssertionError: [] does not have attribute 'i_do_not_have_this'\n\n    The following msg_fmt arguments are supported:\n    * msg - the default error message\n    * obj - object to test\n    * attribute - name of the attribute to check\n    \"\"\"\n\n    if not hasattr(obj, attribute):\n        msg = \"{!r} does not have attribute '{}'\".format(obj, attribute)\n        fail(msg_fmt.format(msg=msg, obj=obj, attribute=attribute))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfail if a datetime object is not within 5 seconds of the local time.", "response": "def assert_datetime_about_now(actual, msg_fmt=\"{msg}\"):\n    \"\"\"Fail if a datetime object is not within 5 seconds of the local time.\n\n    >>> assert_datetime_about_now(datetime.now())\n    >>> assert_datetime_about_now(datetime(1900, 1, 1, 12, 0, 0))\n    Traceback (most recent call last):\n        ...\n    AssertionError: datetime.datetime(1900, 1, 1, 12, 0) is not close to current date/time\n\n    The following msg_fmt arguments are supported:\n    * msg - the default error message\n    * actual - datetime object to check\n    * now - current datetime that was tested against\n    \"\"\"\n\n    now = datetime.now()\n    if actual is None:\n        msg = \"None is not a valid date/time\"\n        fail(msg_fmt.format(msg=msg, actual=actual, now=now))\n    lower_bound = now - timedelta(seconds=_EPSILON_SECONDS)\n    upper_bound = now + timedelta(seconds=_EPSILON_SECONDS)\n    if not lower_bound <= actual <= upper_bound:\n        msg = \"{!r} is not close to current date/time\".format(actual)\n        fail(msg_fmt.format(msg=msg, actual=actual, now=now))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef assert_raises_regex(exception, regex, msg_fmt=\"{msg}\"):\n\n    def test(exc):\n        compiled = re.compile(regex)\n        if not exc.args:\n            msg = \"{} without message\".format(exception.__name__)\n            fail(\n                msg_fmt.format(\n                    msg=msg,\n                    text=None,\n                    pattern=compiled.pattern,\n                    exc_type=exception,\n                    exc_name=exception.__name__,\n                )\n            )\n        text = exc.args[0]\n        if not compiled.search(text):\n            msg = \"{!r} does not match {!r}\".format(text, compiled.pattern)\n            fail(\n                msg_fmt.format(\n                    msg=msg,\n                    text=text,\n                    pattern=compiled.pattern,\n                    exc_type=exception,\n                    exc_name=exception.__name__,\n                )\n            )\n\n    context = AssertRaisesRegexContext(exception, regex, msg_fmt)\n    context.add_test(test)\n    return context", "response": "Fail unless an exception with a message that matches a regular expression is raised within the context."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef assert_raises_errno(exception, errno, msg_fmt=\"{msg}\"):\n\n    def check_errno(exc):\n        if errno != exc.errno:\n            msg = \"wrong errno: {!r} != {!r}\".format(errno, exc.errno)\n            fail(\n                msg_fmt.format(\n                    msg=msg,\n                    exc_type=exception,\n                    exc_name=exception.__name__,\n                    expected_errno=errno,\n                    actual_errno=exc.errno,\n                )\n            )\n\n    context = AssertRaisesErrnoContext(exception, errno, msg_fmt)\n    context.add_test(check_errno)\n    return context", "response": "Fail unless an exception with a specific errno is raised with the\n     context."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef assert_succeeds(exception, msg_fmt=\"{msg}\"):\n\n    class _AssertSucceeds(object):\n        def __enter__(self):\n            pass\n\n        def __exit__(self, exc_type, exc_val, exc_tb):\n            if exc_type and issubclass(exc_type, exception):\n                msg = exception.__name__ + \" was unexpectedly raised\"\n                fail(\n                    msg_fmt.format(\n                        msg=msg,\n                        exc_type=exception,\n                        exc_name=exception.__name__,\n                        exception=exc_val,\n                    )\n                )\n\n    return _AssertSucceeds()", "response": "Fail if a specific exception is raised within the context."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef assert_warns_regex(warning_type, regex, msg_fmt=\"{msg}\"):\n\n    def test(warning):\n        return re.search(regex, str(warning.message)) is not None\n\n    context = AssertWarnsRegexContext(warning_type, regex, msg_fmt)\n    context.add_test(test)\n    return context", "response": "Fail unless a warning with a message is issued inside the context."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nassert that a JSON object or array is a subset of another JSON object or array.", "response": "def assert_json_subset(first, second):\n    \"\"\"Assert that a JSON object or array is a subset of another JSON object\n    or array.\n\n    The first JSON object or array must be supplied as a JSON-compatible\n    dict or list, the JSON object or array to check must be a string, an\n    UTF-8 bytes object, or a JSON-compatible list or dict.\n\n    A JSON non-object, non-array value is the subset of another JSON value,\n    if they are equal.\n\n    A JSON object is the subset of another JSON object if for each name/value\n    pair in the former there is a name/value pair in the latter with the same\n    name. Additionally the value of the former pair must be a subset of the\n    value of the latter pair.\n\n    A JSON array is the subset of another JSON array, if they have the same\n    number of elements and each element in the former is a subset of the\n    corresponding element in the latter.\n\n    >>> assert_json_subset({}, '{}')\n    >>> assert_json_subset({}, '{\"foo\": \"bar\"}')\n    >>> assert_json_subset({\"foo\": \"bar\"}, '{}')\n    Traceback (most recent call last):\n    ...\n    AssertionError: element 'foo' missing from element $\n    >>> assert_json_subset([1, 2], '[1, 2]')\n    >>> assert_json_subset([2, 1], '[1, 2]')\n    Traceback (most recent call last):\n    ...\n    AssertionError: element $[0] differs: 2 != 1\n    >>> assert_json_subset([{}], '[{\"foo\": \"bar\"}]')\n    >>> assert_json_subset({}, \"INVALID JSON\")\n    Traceback (most recent call last):\n    ...\n    json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n    \"\"\"\n\n    if not isinstance(second, (dict, list, str, bytes)):\n        raise TypeError(\"second must be dict, list, str, or bytes\")\n    if isinstance(second, bytes):\n        second = second.decode(\"utf-8\")\n    if isinstance(second, _Str):\n        parsed_second = json_loads(second)\n    else:\n        parsed_second = second\n\n    if not isinstance(parsed_second, (dict, list)):\n        raise AssertionError(\n            \"second must decode to dict or list, not {}\".format(\n                type(parsed_second)\n            )\n        )\n\n    comparer = _JSONComparer(_JSONPath(\"$\"), first, parsed_second)\n    comparer.assert_()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _to_bytes(self, data, key='', expired=None, noc=0, ncalls=0):\n\n        data_tuple = (data, expired, noc, ncalls)\n\n        if not can_encrypt and key:\n            # TODO: Probably not only Pycrypto will be using for encryption!!!\n            # Clarification needed\n            warnings.warn(\"Pycrypto is not installed. The data will not be encrypted\",\n                          UserWarning)\n            result = encode_safely(data_tuple)\n        elif can_encrypt and key:\n            if PY3:\n                cipher = AESCipher(key.encode(settings.DEFAULT_ENCODING))\n            else:\n                cipher = AESCipher(key)\n            result = cipher.encrypt(encode_safely(data_tuple))\n        else:\n            result = encode_safely(data_tuple)\n        return result", "response": "Serialize the data and represent it as bytes."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_data(self, data_key, key=''):\n\n        flag = False  # set to True if data was successfully extracted.\n        extracted = self.get(data_key, -1)\n\n        if extracted != -1:\n            try:\n                data, expired, noc, ncalls = self._from_bytes(extracted, key=key)\n                flag = True\n            except ValueError:\n                return None, flag\n\n            if noc:\n                ncalls += 1\n                self[data_key] = self._to_bytes(data, expired=expired,\n                                                key=key, noc=noc,\n                                                ncalls=ncalls)\n                if ncalls >= noc:\n                    self.remove(data_key)\n                    flag = False\n\n            if expired and datetime.datetime.now() > expired:\n                self.remove(data_key)\n                flag = False\n\n        return (data, flag) if flag else (None, flag)", "response": "Get the data from the cache."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndecode the data into a dict.", "response": "def decode_safely(self, encoded_data):\n        \"\"\"Inverse for the `encode_safely` function.\n        \"\"\"\n\n        decoder = self.base_decoder\n        result = settings.null\n\n        try:\n            result = pickle.loads(decoder(encoded_data))\n        except:\n            warnings.warn(\"Could not load and deserialize the data.\", RuntimeWarning)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the hash of the function to be evaluated.", "response": "def get_function_hash(self, func, args=None, kwargs=None, ttl=None, key=None, noc=None):\n        \"\"\"Compute the hash of the function to be evaluated.\n        \"\"\"\n\n        base_hash = settings.HASH_FUNCTION()\n\n        if PY3:\n            base_hash.update(func.__name__.encode(settings.DEFAULT_ENCODING))\n        else:\n            base_hash.update(func.__name__)\n\n        if args:\n            for a in args:\n                if PY3:\n                    base_hash.update(repr(a).encode(settings.DEFAULT_ENCODING))\n                else:\n                    base_hash.update(repr(a))\n\n        if kwargs:\n            for k in sorted(kwargs):\n                if PY3:\n                    base_hash.update((\"{}={}\".format(k, repr(kwargs[k]))).encode(settings.DEFAULT_ENCODING))\n                else:\n                    base_hash.update((\"{}={}\".format(k, repr(kwargs[k]))))\n\n        if ttl:\n            base_hash.update(str(ttl).encode(settings.DEFAULT_ENCODING))\n\n        if key and can_encrypt:\n            if PY3:\n                base_hash.update(key.encode(settings.DEFAULT_ENCODING))\n            else:\n                base_hash.update(key)\n\n        if noc:\n            base_hash.update(str(noc).encode(settings.DEFAULT_ENCODING))\n\n        base_hash_hex = base_hash.hexdigest()\n\n        return base_hash_hex"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_bytes(obj):\n\n    if PY3:\n        if isinstance(obj, str):\n            return obj.encode(settings.DEFAULT_ENCODING)\n        else:\n            return obj if isinstance(obj, bytes) else b''\n    else:\n        if isinstance(obj, str):\n            return obj\n        else:\n            return obj.encode(settings.DEFAULT_ENCODING) if isinstance(obj, unicode) else ''", "response": "Ensures that the object is of byte - type."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfilling a bytes - like object with arbitrary symbols to make its length divisible by bs.", "response": "def padding(s, bs=AES.block_size):\n    \"\"\"Fills a bytes-like object with arbitrary symbols to make its length divisible by `bs`.\n    \"\"\"\n    \n    s = to_bytes(s)\n\n    if len(s) % bs == 0:\n        res = s + b''.join(map(to_bytes, [random.SystemRandom().choice(string.ascii_lowercase + string.digits) for _ in range(bs - 1)])) + to_bytes(chr(96 - bs))\n    elif len(s) % bs > 0 and len(s) > bs:\n        res = s + b''.join(map(to_bytes, [random.SystemRandom().choice(string.ascii_lowercase + string.digits) for _ in range(bs - len(s) % bs - 1)])) + to_bytes(chr(96 + len(s) % bs - bs))\n    else:\n        res = s + b''.join(map(to_bytes, [random.SystemRandom().choice(string.ascii_lowercase + string.digits) for _ in range(bs - len(s) - 1)])) + to_bytes(chr(96 + len(s) - bs))\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreverse operation to padding.", "response": "def unpadding(s, bs=AES.block_size):\n    \"\"\"Reverse operation to padding (see above).\n\n    Parameters\n    ==========\n\n        :param s: bytes-like object;\n        :param bs: encryption block size.\n    \"\"\"\n\n    if PY3:\n        return s[:s[-1] - 96] if len(s) % bs == 0 else ''\n    else:\n        return s[:ord(s[-1]) - 96] if len(s) % bs == 0 else ''"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nstops the reveiver thread.", "response": "def stop(self):\n        \"\"\"Called to stop the reveiver thread.\"\"\"\n        self._stop_thread.set()\n        # f you want to close the connection in a timely fashion,\n        # call shutdown() before close().\n        with self._lock:  # Receive thread might use the socket\n            self.receive_socket.shutdown(socket.SHUT_RDWR)\n            self.receive_socket.close()\n\n        self.send_socket.shutdown(socket.SHUT_RDWR)\n        self.send_socket.close()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run(self):  # Thread for receiving data from pilight\n        logging.debug('Pilight receiver thread started')\n        if not self.callback:\n            raise RuntimeError('No callback function set, cancel readout thread')\n\n        def handle_messages(messages):\n            \"\"\"Call callback on each receive message.\"\"\"\n            for message in messages:  # Loop over received  messages\n                if message:  # Can be empty due to splitlines\n                    message_dict = json.loads(message.decode())\n                    if self.recv_codes_only:\n                        # Filter: Only use receiver messages\n                        if 'receiver' in message_dict['origin']:\n                            if self.veto_repeats:\n                                if message_dict.get('repeats', 1) == 1:\n                                    self.callback(message_dict)\n                            else:\n                                self.callback(message_dict)\n                    else:\n                        self.callback(message_dict)\n\n        while not self._stop_thread.isSet():\n            try:  # Read socket in a non blocking call and interpret data\n                # Sometimes more than one JSON object is in the stream thus\n                # split at \\n\n                with self._lock:\n                    messages = self.receive_socket.recv(1024).splitlines()\n                handle_messages(messages)\n            except (socket.timeout, ValueError):  # No data\n                pass\n        logging.debug('Pilight receiver thread stopped')", "response": "Main thread for receiving data from pilight."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef send_code(self, data, acknowledge=True):\n        if \"protocol\" not in data:\n            raise ValueError(\n                'Pilight data to send does not contain a protocol info. '\n                'Check the pilight-send doku!', str(data))\n\n        # Create message to send\n        message = {\n            \"action\": \"send\",  # Tell pilight daemon to send the data\n            \"code\": data,\n        }\n\n        # If connection is closed IOError is raised\n        self.send_socket.sendall(json.dumps(message).encode())\n\n        if acknowledge:  # Check if command is acknowledged by pilight daemon\n            messages = self.send_socket.recv(1024).splitlines()\n            received = False\n            for message in messages:  # Loop over received messages\n                if message:  # Can be empty due to splitlines\n                    acknowledge_message = json.loads(message.decode())\n                    # Filter correct message\n                    if ('status' in acknowledge_message and\n                            acknowledge_message['status'] == 'success'):\n                        received = True\n            if not received:\n                raise IOError('Send code failed. Code: %s', str(data))", "response": "Send a RF code to the pilight - daemon."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the filepath from a tileset definition", "response": "def get_filepath(filepath):\n    \"\"\"\n    Get the filepath from a tileset definition\n    Parameters\n    ----------\n    tileset_def: { 'filepath': ..., 'uid': ..., 'filetype': ...}\n        The tileset definition\n    returns: string\n        The filepath, either as specified in the tileset_def or\n        None\n    \"\"\"\n\n    if filepath[:7] == \"http://\":\n        filepath = fuse.http_directory + filepath[6:] + \"..\"\n    if filepath[:8] == \"https://\":\n        filepath = fuse.https_directory + filepath[7:] + \"..\"\n\n    print(\"******** filepath:\", filepath)\n\n    return filepath"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets up filesystem in user space for http and https.", "response": "def setup(self):\n        \"\"\"\n        Set up filesystem in user space for http and https\n        so that we can retrieve tiles from remote sources.\n\n        Parameters\n        ----------\n        tmp_dir: string\n            The temporary directory where to create the\n            http and https directories\n        \"\"\"\n        from simple_httpfs import HttpFs\n\n        if not op.exists(self.http_directory):\n            os.makedirs(self.http_directory)\n        if not op.exists(self.https_directory):\n            os.makedirs(self.https_directory)\n        if not op.exists(self.diskcache_directory):\n            os.makedirs(self.diskcache_directory)\n\n        self.teardown()\n\n        disk_cache_size = 2 ** 25\n        disk_cache_dir = self.diskcache_directory\n        lru_capacity = 400\n        print(\n            \"self.diskcache_directory\",\n            self.diskcache_directory,\n            op.exists(self.diskcache_directory),\n        )\n\n        def start_fuse(directory, protocol):\n            print(\"starting fuse\")\n            fuse = FUSE(\n                HttpFs(\n                    protocol,\n                    disk_cache_size=disk_cache_size,\n                    disk_cache_dir=self.diskcache_directory,\n                    lru_capacity=lru_capacity,\n                ),\n                directory,\n                foreground=False,\n                allow_other=True\n            )\n        proc1 = mp.Process(target=start_fuse, args=[self.http_directory, 'http'])\n        proc1.start()\n        proc1.join()\n\n        proc2 = mp.Process(target=start_fuse, args=[self.https_directory, 'https'])\n        proc2.start()\n        proc2.join()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nstart a lightweight higlass server.", "response": "def start(self, log_file=\"/tmp/hgserver.log\", log_level=logging.INFO):\n        \"\"\"\n        Start a lightweight higlass server.\n\n        Parameters\n        ----------\n        log_file: string\n            Where to place diagnostic log files\n        log_level: logging.*\n            What level to log at\n        \"\"\"\n        for puid in list(self.processes.keys()):\n            print(\"terminating:\", puid)\n            self.processes[puid].terminate()\n            del self.processes[puid]\n\n        self.app = create_app(\n            self.tilesets,\n            __name__,\n            log_file=log_file,\n            log_level=log_level,\n            file_ids=self.file_ids,\n        )\n\n        # we're going to assign a uuid to each server process so that if anything\n        # goes wrong, the variable referencing the process doesn't get lost\n        uuid = slugid.nice()\n        if self.port is None:\n            self.port = get_open_port()\n        target = partial(\n            self.app.run,\n            threaded=True,\n            debug=True,\n            host=\"0.0.0.0\",\n            port=self.port,\n            use_reloader=False,\n        )\n        self.processes[uuid] = mp.Process(target=target)\n        self.processes[uuid].start()\n        self.connected = False\n        while not self.connected:\n            try:\n                url = \"http://{}:{}/api/v1\".format(self.host, self.port)\n                r = requests.head(url)\n                if r.ok:\n                    self.connected = True\n            except requests.ConnectionError as err:\n                time.sleep(0.2)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef stop(self):\n        # unsetup_fuse()\n        self.fuse_process.teardown()\n        for uuid in self.processes:\n            self.processes[uuid].terminate()", "response": "Stop this server so that the calling process can exit."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef tileset_info(self, uid):\n        url = \"http://{host}:{port}/api/v1/tileset_info/?d={uid}\".format(\n            host=self.host, port=self.port, uid=uid\n        )\n\n        req = requests.get(url)\n        if req.status_code != 200:\n            raise ServerError(\"Error fetching tileset_info:\", req.content)\n\n        content = json.loads(req.content)\n        return content[uid]", "response": "Returns the tileset info for the given uid"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef tiles(self, uid, z, x, y=None):\n        tile_id = \"{uid}.{z}.{x}\".format(uid=uid, z=z, x=x)\n        if y is not None:\n            tile_id += \".{y}\".format(y=y)\n        url = \"http://{host}:{port}/api/v1/tiles/?d={tile_id}\".format(\n            host=self.host, port=self.port, tile_id=tile_id\n        )\n\n        print(\"url:\", url)\n\n        req = requests.get(url)\n        if req.status_code != 200:\n            raise ServerError(\"Error fetching tile:\", req.content)\n\n        content = json.loads(req.content)\n        return content[tile_id]", "response": "Return the tiles from the specified dataset ( uid z x y"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef chromsizes(self, uid):\n        url = \"http://{host}:{port}/api/v1/chrom-sizes/?id={uid}\".format(\n            host=self.host, port=self.port, uid=uid\n        )\n\n        req = requests.get(url)\n        if req.status_code != 200:\n            raise ServerError(\"Error fetching chromsizes:\", req.content)\n\n        return req.content", "response": "Return the chromosome sizes from the given filename\n           "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a higlass viewer that displays the specified tilesets", "response": "def view(tilesets):\n    '''\n    Create a higlass viewer that displays the specified tilesets\n\n    Parameters:\n    -----------\n\n    Returns\n    -------\n        Nothing\n    '''\n    from .server import Server\n    from .client import View\n\n    curr_view = View()\n    server = Server()\n    server.start(tilesets)\n\n    for ts in tilesets:\n        if (ts.track_type is not None\n                and ts.track_position is not None):\n            curr_view.add_track(ts.track_type,\n                    ts.track_position,\n                    api_url=server.api_address,\n                    tileset_uuid=ts.uuid,\n                )\n\n    curr_view.server = server\n    return curr_view"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef change_attributes(self, **kwargs):\n        '''\n        Change an attribute of this track and return a new copy.\n        '''\n        new_track = Track(self.viewconf['type'])\n        new_track.position = self.position\n        new_track.tileset = self.tileset\n        new_track.viewconf = json.loads(json.dumps(self.viewconf))\n        new_track.viewconf = {**new_track.viewconf, **kwargs}\n\n        return new_track", "response": "Change an attribute of this track and return a new copy."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchanging one of the track s options in the viewconf", "response": "def change_options(self, **kwargs):\n        '''\n        Change one of the track's options in the viewconf\n        '''\n        new_options = json.loads(json.dumps(self.viewconf['options']))\n        new_options = {**new_options, **kwargs}\n\n        return self.change_attributes(options=new_options)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a track to the current position.", "response": "def add_track(self, *args, **kwargs):\n        \"\"\"\n        Add a track to a position.\n\n        Parameters\n        ----------\n        track_type: string\n            The type of track to add (e.g. \"heatmap\", \"line\")\n        position: string\n            One of 'top', 'bottom', 'center', 'left', 'right'\n        tileset: hgflask.tilesets.Tileset\n            The tileset to be plotted in this track\n        server: string\n            The server serving this track\n        height: int\n            The height of the track, if it is a top, bottom or a center track\n        width: int\n            The width of the track, if it is a left, right or a center track\n        \"\"\"\n        new_track = Track(*args, **kwargs)\n        self.tracks = self.tracks + [new_track]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_dict(self):\n        viewconf = json.loads(json.dumps(self.viewconf))\n\n        for track in self.tracks:\n            if track.position is None:\n                raise ValueError(\n                    \"Track has no position: {}\".format(track.viewconf[\"type\"])\n                )\n            viewconf[\"tracks\"][track.position] += [track.to_dict()]\n\n        return viewconf", "response": "Convert the existing track to a JSON representation."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_view(self, *args, **kwargs):\n        new_view = View(*args, **kwargs)\n\n        for view in self.views:\n            if view.uid == new_view.uid:\n                raise ValueError(\"View with this uid already exists\")\n\n        self.views += [new_view]\n        return new_view", "response": "Add a view to the list of views in the current version of the entry."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrap a function that returns a string, adding the 'slugify' argument. >>> slugified_fn = slugify_argument(lambda *args, **kwargs: \"YOU ARE A NICE LADY\") >>> slugified_fn() 'YOU ARE A NICE LADY' >>> slugified_fn(slugify=True) 'you-are-a-nice-lady'", "response": "def slugify_argument(func):\n    \"\"\"\n    Wraps a function that returns a string, adding the 'slugify' argument.\n\n    >>> slugified_fn = slugify_argument(lambda *args, **kwargs: \"YOU ARE A NICE LADY\")\n    >>> slugified_fn()\n    'YOU ARE A NICE LADY'\n    >>> slugified_fn(slugify=True)\n    'you-are-a-nice-lady'\n\n    \"\"\"\n    @six.wraps(func)\n    def wrapped(*args, **kwargs):\n        if \"slugify\" in kwargs and kwargs['slugify']:\n            return _slugify(func(*args, **kwargs))\n        else:\n            return func(*args, **kwargs)\n    return wrapped"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef capitalize_argument(func):\n    @six.wraps(func)\n    def wrapped(*args, **kwargs):\n        if \"capitalize\" in kwargs and kwargs['capitalize']:\n            return func(*args, **kwargs).title()\n        else:\n            return func(*args, **kwargs)\n    return wrapped", "response": "A decorator that returns a string adding the capitalize argument."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a random datetime from the past or the future!", "response": "def datetime(past=True, random=random):\n    \"\"\"\n    Returns a random datetime from the past... or the future!\n\n    >>> mock_random.seed(0)\n    >>> datetime(random=mock_random).isoformat()\n    '1950-02-03T03:04:05'\n\n    \"\"\"\n\n    def year():\n        if past:\n            return random.choice(range(1950,2005))\n        else:\n            return _datetime.datetime.now().year + random.choice(range(1, 50))\n\n    def month():\n        return random.choice(range(1,12))\n\n    def day():\n        return random.choice(range(1,31))\n\n    def hour():\n        return random.choice(range(0,23))\n\n    def minute():\n        return random.choice(range(0,59))\n\n    def second():\n        return random.choice(range(0,59))\n\n    try:\n        return _datetime.datetime(year=year(),\n                                  month=month(),\n                                  day=day(),\n                                  hour=hour(),\n                                  minute=minute(),\n                                  second=second())\n    except ValueError:\n        return datetime(past=past)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef a_noun(random=random, *args, **kwargs):\n    return inflectify.a(noun(random=random))", "response": "Return a noun but with an in front of it."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef plural(random=random, *args, **kwargs):\n    return inflectify.plural(random.choice(nouns))", "response": "Return a plural noun."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef lastname(random=random, *args, **kwargs):\n    types = [\n        \"{noun}\",\n        \"{adjective}\",\n        \"{noun}{second_noun}\",\n        \"{adjective}{noun}\",\n        \"{adjective}{plural}\",\n        \"{noun}{verb}\",\n        \"{noun}{container}\",\n        \"{verb}{noun}\",\n        \"{adjective}{verb}\",\n        \"{noun}{adjective}\",\n        \"{noun}{firstname}\",\n        \"{noun}{title}\",\n        \"{adjective}{title}\",\n        \"{adjective}-{noun}\",\n        \"{adjective}-{plural}\"\n    ]\n\n    return random.choice(types).format(noun=noun(random=random),\n                                       second_noun=noun(random=random),\n                                       adjective=adjective(random=random),\n                                       plural=plural(random=random),\n                                       container=container(random=random),\n                                       verb=verb(random=random),\n                                       firstname=firstname(random=random),\n                                       title=title(random=random))", "response": "Return a random last name of the object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a number that is spelled out.", "response": "def numberwang(random=random, *args, **kwargs):\n    \"\"\"\n    Return a number that is spelled out.\n\n    >>> numberwang(random=mock_random)\n    'two'\n    >>> numberwang(random=mock_random, capitalize=True)\n    'Two'\n    >>> numberwang(random=mock_random, slugify=True)\n    'two'\n\n    \"\"\"\n    n = random.randint(2, 150)\n    return inflectify.number_to_words(n)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a random thing.", "response": "def thing(random=random, *args, **kwargs):\n    \"\"\"\n    Return a ... thing.\n\n    >>> mock_random.seed(0)\n    >>> thing(random=mock_random)\n    'two secrets'\n    >>> mock_random.seed(1)\n    >>> thing(random=mock_random, capitalize=True)\n    'Mighty Poop'\n    >>> mock_random.seed(2)\n    >>> thing(random=mock_random, slugify=True)\n    'poop'\n    >>> mock_random.seed(4)\n    >>> thing(random=mock_random, slugify=True)\n    'two-chimps'\n\n    \"\"\"\n\n    def noun_or_adjective_noun():\n        if random.choice([True, False]):\n            return noun(random=random)\n        else:\n            return adjective(random=random) + \" \" + noun(random=random)\n\n    def plural_or_adjective_plural():\n        if random.choice([True, False]):\n            return plural(random=random)\n        else:\n            return adjective(random=random) + \" \" + plural(random=random)\n\n    def container_of_nouns():\n        return container(random=random) + \" of \" + plural_or_adjective_plural()\n\n    def number_of_plurals():\n        return numberwang(random=random) + \" \" + plural_or_adjective_plural()\n\n    if \"an\" in kwargs and kwargs['an']:\n        return random.choice([\n            inflectify.a(noun_or_adjective_noun()),\n            inflectify.a(container_of_nouns()),\n            number_of_plurals(),\n        ])\n    else:\n        return random.choice([\n            noun_or_adjective_noun(),\n            container_of_nouns(),\n            number_of_plurals(),\n        ])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef a_thing(random=random, *args, **kwargs):\n    return thing(random=random, an=True, *args, **kwargs)", "response": "Return a ... thing.\n\n    >>> mock_random.seed(0)\n    >>> a_thing(random=mock_random)\n    'two secrets'\n    >>> mock_random.seed(1)\n    >>> a_thing(random=mock_random, capitalize=True)\n    'A Mighty Poop'\n    >>> mock_random.seed(2)\n    >>> a_thing(random=mock_random, slugify=True)\n    'a-poop'\n    >>> mock_random.seed(4)\n    >>> a_thing(random=mock_random, slugify=True)\n    'two-chimps'"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef things(random=random, *args, **kwargs):\n    return inflectify.join([a_thing(random=random), a_thing(random=random), a_thing(random=random)])", "response": "Return a set of things."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning someone s name", "response": "def name(random=random, *args, **kwargs):\n    \"\"\"\n    Return someone's name\n\n    >>> mock_random.seed(0)\n    >>> name(random=mock_random)\n    'carl poopbritches'\n    >>> mock_random.seed(7)\n    >>> name(random=mock_random, capitalize=True)\n    'Duke Testy Wonderful'\n\n    \"\"\"\n    if random.choice([True, True, True, False]):\n        return firstname(random=random) + \" \" + lastname(random=random)\n    elif random.choice([True, False]):\n        return title(random=random) + \" \" + firstname(random=random) + \" \" + lastname(random=random)\n    else:\n        return title(random=random) + \" \" + lastname(random=random)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef domain(random=random, *args, **kwargs):\n    words = random.choice([\n        noun(random=random),\n        thing(random=random),\n        adjective(random=random)+noun(random=random),\n    ])\n    return _slugify(words)+tld(random=random)", "response": "Return a domain of the current language."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef email(random=random, *args, **kwargs):\n    if 'name' in kwargs and kwargs['name']:\n        words = kwargs['name']\n    else:\n        words = random.choice([\n            noun(random=random),\n            name(random=random),\n            name(random=random)+\"+spam\",\n        ])\n    return _slugify(words)+\"@\"+domain(random=random)", "response": "Return an e - mail address of the current user."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a phone number in the order of the number.", "response": "def phone_number(random=random, *args, **kwargs):\n    \"\"\"\n    Return a phone number\n\n    >>> mock_random.seed(0)\n    >>> phone_number(random=mock_random)\n    '555-0000'\n    >>> phone_number(random=mock_random)\n    '1-604-555-0000'\n    >>> phone_number(random=mock_random)\n    '864-70-555-0000'\n\n    \"\"\"\n    return random.choice([\n        '555-{number}{other_number}{number}{other_number}',\n        '1-604-555-{number}{other_number}{number}{other_number}',\n        '864-70-555-{number}{other_number}{number}{other_number}',\n        '867-5309'\n    ]).format(number=number(random=random),\n              other_number=number(random=random))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a whole sentence", "response": "def sentence(random=random, *args, **kwargs):\n    \"\"\"\n    Return a whole sentence\n\n    >>> mock_random.seed(0)\n    >>> sentence(random=mock_random)\n    \"Agatha Incrediblebritches can't wait to smell two chimps in Boatbencheston.\"\n\n    >>> mock_random.seed(2)\n    >>> sentence(random=mock_random, slugify=True)\n    'blistersecret-studios-is-the-best-company-in-liveronion'\n\n    \"\"\"\n    if 'name' in kwargs and kwargs['name']:\n        nm = kwargs(name)\n    elif random.choice([True, False, False]):\n        nm = name(capitalize=True, random=random)\n    else:\n        nm = random.choice(people)\n\n    def type_one():\n        return \"{name} will {verb} {thing}.\".format(name=nm,\n                                                    verb=verb(random=random),\n                                                    thing=random.choice([a_thing(random=random),\n                                                                         things(random=random)]))\n\n    def type_two():\n        return \"{city} is in {country}.\".format(city=city(capitalize=True, random=random),\n                                                country=country(capitalize=True, random=random))\n\n    def type_three():\n        return \"{name} can't wait to {verb} {thing} in {city}.\".format(name=nm,\n                                                                      verb=verb(random=random),\n                                                                      thing=a_thing(random=random),\n                                                                      city=city(capitalize=True, random=random))\n\n    def type_four():\n        return \"{name} will head to {company} to buy {thing}.\".format(name=nm,\n                                                                     company=company(capitalize=True, random=random),\n                                                                     thing=a_thing(random=random))\n\n\n    def type_five():\n        return \"{company} is the best company in {city}.\".format(city=city(capitalize=True, random=random),\n                                                                 company=company(capitalize=True, random=random))\n\n    def type_six():\n        return \"To get to {country}, you need to go to {city}, then drive {direction}.\".format(\n            country=country(capitalize=True, random=random),\n            city=city(capitalize=True, random=random),\n            direction=direction(random=random))\n\n    def type_seven():\n        return \"{name} needs {thing}, badly.\".format(name=nm, thing=a_thing(random=random))\n\n    def type_eight():\n        return \"{verb} {noun}!\".format(verb=verb(capitalize=True, random=random), noun=noun(random=random))\n\n    return random.choice([type_one,\n                          type_two,\n                          type_three,\n                          type_four,\n                          type_five,\n                          type_six,\n                          type_seven,\n                          type_eight])()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate a paragraph of text.", "response": "def paragraph(random=random, length=10, *args, **kwargs):\n    \"\"\"\n    Produces a paragraph of text.\n\n    >>> mock_random.seed(0)\n    >>> paragraph(random=mock_random, length=2)\n    \"Agatha Incrediblebritches can't wait to smell two chimps in Boatbencheston. Wonderfulsecretsound is in Gallifrey.\"\n\n    >>> mock_random.seed(2)\n    >>> paragraph(random=mock_random, length=2, slugify=True)\n    'blistersecret-studios-is-the-best-company-in-liveronion-wonderfulsecretsound-is-in-gallifrey'\n\n    \"\"\"\n    return \" \".join([sentence(random=random) for x in range(0, length)])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef markdown(random=random, length=10, *args, **kwargs):\n\n    def title_sentence():\n        return \"\\n\" + \"#\"*random.randint(1,5) + \" \" + sentence(capitalize=True, random=random)\n\n    def embellish(word):\n        return random.choice([word, word, word, \"**\"+word+\"**\", \"_\"+word+\"_\"])\n\n    def randomly_markdownify(string):\n        return \" \".join([embellish(word) for word in string.split(\" \")])\n\n    sentences = []\n    for i in range(0, length):\n        sentences.append(random.choice([\n            title_sentence(),\n            sentence(random=random),\n            sentence(random=random),\n            randomly_markdownify(sentence(random=random))\n        ]))\n    return \"\\n\".join(sentences)", "response": "Generates a bunch of markdown text."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef company(random=random, *args, **kwargs):\n    return random.choice([\n        \"faculty of applied {noun}\",\n        \"{noun}{second_noun} studios\",\n        \"{noun}{noun}{noun} studios\",\n        \"{noun}shop\",\n        \"{noun} studies department\",\n        \"the law offices of {lastname}, {noun}, and {other_lastname}\",\n        \"{country} ministry of {plural}\",\n        \"{city} municipal {noun} department\",\n        \"{city} plumbing\",\n        \"department of {noun} studies\",\n        \"{noun} management systems\",\n        \"{plural} r us\",\n        \"inter{verb}\",\n        \"the {noun} warehouse\",\n        \"integrated {noun} and {second_noun}\",\n        \"the {noun} and {second_noun} pub\",\n        \"e-cyber{verb}\",\n        \"{adjective}soft\",\n        \"{domain} Inc.\",\n        \"{thing} incorporated\",\n        \"{noun}co\",\n    ]).format(noun=noun(random=random),\n              plural=plural(random=random),\n              country=country(random=random),\n              city=city(random=random),\n              adjective=adjective(random=random),\n              lastname=lastname(random=random),\n              other_lastname=lastname(random=random),\n              domain=domain(random=random),\n              second_noun=noun(random=random),\n              verb=verb(random=random),\n              thing=thing(random=random))", "response": "Produce a company name"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef country(random=random, *args, **kwargs):\n    return random.choice([\n        \"{country}\",\n        \"{direction} {country}\"\n    ]).format(country=random.choice(countries),\n              direction=direction(random=random))", "response": "Produce a country name"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nproducing a city name", "response": "def city(random=random, *args, **kwargs):\n    \"\"\"\n    Produce a city name\n\n    >>> mock_random.seed(0)\n    >>> city(random=mock_random)\n    'east mysteryhall'\n    >>> city(random=mock_random, capitalize=True)\n    'Birmingchimp'\n    >>> city(random=mock_random, slugify=True)\n    'wonderfulsecretsound'\n\n    \"\"\"\n    return random.choice([\n        \"{direction} {noun}{city_suffix}\",\n        \"{noun}{city_suffix}\",\n        \"{adjective}{noun}{city_suffix}\",\n        \"{plural}{city_suffix}\",\n        \"{adjective}{city_suffix}\",\n        \"liver{noun}\",\n        \"birming{noun}\",\n        \"{noun}{city_suffix} {direction}\"\n    ]).format(direction=direction(random=random),\n              adjective=adjective(random=random),\n              plural=plural(random=random),\n              city_suffix=city_suffix(random=random),\n              noun=noun(random=random))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nproduces something that vaguely resembles a postal code", "response": "def postal_code(random=random, *args, **kwargs):\n    \"\"\"\n    Produce something that vaguely resembles a postal code\n\n    >>> mock_random.seed(0)\n    >>> postal_code(random=mock_random)\n    'b0b 0c0'\n    >>> postal_code(random=mock_random, capitalize=True)\n    'E0E 0F0'\n    >>> postal_code(random=mock_random, slugify=True)\n    'h0h-0i0'\n\n    \"\"\"\n    return random.choice([\n        \"{letter}{number}{letter} {other_number}{other_letter}{other_number}\",\n        \"{number}{other_number}{number}{number}{other_number}\",\n        \"{number}{letter}{number}{other_number}{other_letter}\"\n    ]).format(\n        number=number(random=random),\n        other_number=number(random=random),\n        letter=letter(random=random),\n        other_letter=letter(random=random)\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nproduce something that sounds like a street name", "response": "def street(random=random, *args, **kwargs):\n    \"\"\"\n    Produce something that sounds like a street name\n\n    >>> mock_random.seed(0)\n    >>> street(random=mock_random)\n    'chimp place'\n    >>> street(random=mock_random, capitalize=True)\n    'Boatbench Block'\n    >>> mock_random.seed(3)\n    >>> street(random=mock_random, slugify=True)\n    'central-britches-boulevard'\n\n    \"\"\"\n    return random.choice([\n        \"{noun} {street_type}\",\n        \"{adjective}{verb} {street_type}\",\n        \"{direction} {adjective}{verb} {street_type}\",\n        \"{direction} {noun} {street_type}\",\n        \"{direction} {lastname} {street_type}\",\n    ]).format(noun=noun(random=random),\n              lastname=lastname(random=random),\n              direction=direction(random=random),\n              adjective=adjective(random=random),\n              verb=verb(random=random),\n              street_type=random.choice(streets))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef address(random=random, *args, **kwargs):\n    return random.choice([\n        \"{number}{other_number}{number}{other_number} {street}\",\n        \"{number}{other_number} {street}\",\n        \"{numberwang} {street}\",\n        \"apt {numberwang}, {number}{other_number}{other_number} {street}\",\n        \"apt {number}{other_number}{number}, {numberwang} {street}\",\n        \"po box {number}{other_number}{number}{other_number}\",\n    ]).format(number=number(random=random),\n              other_number=number(random=random),\n              numberwang=numberwang(random=random),\n              street=street(random=random))", "response": "A street name plus a number!\n\n    >>> mock_random.seed(0)\n    >>> address(random=mock_random)\n    '0000 amazingslap boardwalk'\n    >>> address(random=mock_random, capitalize=True)\n    '0000 South Throbbingjump Boulevard'\n    >>> address(random=mock_random, slugify=True)\n    'two-central-britches-boulevard'"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating the address of a placeholder image.", "response": "def image(random=random, width=800, height=600, https=False, *args, **kwargs):\n    \"\"\"\n    Generate the address of a placeholder image.\n\n    >>> mock_random.seed(0)\n    >>> image(random=mock_random)\n    'http://dummyimage.com/800x600/292929/e3e3e3&text=mighty poop'\n    >>> image(random=mock_random, width=60, height=60)\n    'http://placekitten.com/60/60'\n    >>> image(random=mock_random, width=1920, height=1080)\n    'http://dummyimage.com/1920x1080/292929/e3e3e3&text=To get to Westeros, you need to go to Britchestown, then drive west.'\n    >>> image(random=mock_random, https=True, width=1920, height=1080)\n    'https://dummyimage.com/1920x1080/292929/e3e3e3&text=East Mysteryhall is in Westeros.'\n\n    \"\"\"\n    target_fn = noun\n\n    if width+height > 300:\n        target_fn = thing\n    if width+height > 2000:\n        target_fn = sentence\n\n    s = \"\"\n    if https:\n        s = \"s\"\n\n    if random.choice([True, False]):\n        return \"http{s}://dummyimage.com/{width}x{height}/292929/e3e3e3&text={text}\".format(\n                s=s,\n                width=width,\n                height=height,\n                text=target_fn(random=random))\n    else:\n        return \"http{s}://placekitten.com/{width}/{height}\".format(s=s, width=width, height=height)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef decode_copy_value(value):\n    # Test for null values first.\n    if value == POSTGRES_COPY_NULL_VALUE:\n        return None\n\n    # If there is no backslash present, there's nothing to decode.\n    #\n    # This early return provides a little speed-up, because it's very\n    # common to not have anything to decode and then simple search for\n    # backslash is faster than the regex sub below.\n    if '\\\\' not in value:\n        return value\n\n    return DECODE_REGEX.sub(unescape_single_character, value)", "response": "Decodes a value received as part of the POSTGRES COPY command."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef unescape_single_character(match):\n    try:\n        return DECODE_MAP[match.group(0)]\n    except KeyError:\n        value = match.group(0)\n        if value == '\\\\':\n            raise ValueError(\"Unterminated escape sequence encountered\")\n\n        raise ValueError(\n            \"Unrecognized escape sequence encountered: {}\".format(value))", "response": "Unescape a single character in a single escape sequence."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nextracting database dump from given database URL and outputs sanitized version of it into given stream.", "response": "def run(url, output, config):\n    \"\"\"\n    Extracts database dump from given database URL and outputs sanitized\n    copy of it into given stream.\n\n    :param url: URL to the database which is to be sanitized.\n    :type url: str\n\n    :param output: Stream where sanitized copy of the database dump will be\n                   written into.\n    :type output: file\n\n    :param config: Optional sanitizer configuration to be used for sanitation\n                   of the values stored in the database.\n    :type config: database_sanitizer.config.Configuration|None\n    \"\"\"\n    parsed_url = urlparse.urlparse(url)\n    db_module_path = SUPPORTED_DATABASE_MODULES.get(parsed_url.scheme)\n    if not db_module_path:\n        raise ValueError(\"Unsupported database scheme: '%s'\" % (parsed_url.scheme,))\n    db_module = importlib.import_module(db_module_path)\n    session.reset()\n    for line in db_module.sanitize(url=parsed_url, config=config):\n        output.write(line + \"\\n\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sanitize(url, config):\n    if url.scheme not in (\"postgres\", \"postgresql\", \"postgis\"):\n        raise ValueError(\"Unsupported database type: '%s'\" % (url.scheme,))\n\n    process = subprocess.Popen(\n        (\n            \"pg_dump\",\n            # Force output to be UTF-8 encoded.\n            \"--encoding=utf-8\",\n            # Quote all table and column names, just in case.\n            \"--quote-all-identifiers\",\n            # Luckily `pg_dump` supports DB URLs, so we can just pass it the\n            # URL as argument to the command.\n            \"--dbname\",\n            url.geturl().replace('postgis://', 'postgresql://'),\n        ),\n        stdout=subprocess.PIPE,\n    )\n\n    sanitize_value_line = None\n    current_table = None\n    current_table_columns = None\n\n    for line in io.TextIOWrapper(process.stdout, encoding=\"utf-8\"):\n        # Eat the trailing new line.\n        line = line.rstrip(\"\\n\")\n\n        # Are we currently in middle of `COPY` statement?\n        if current_table:\n            # Backslash following a dot marks end of an `COPY` statement.\n            if line == \"\\\\.\":\n                current_table = None\n                current_table_columns = None\n                yield \"\\\\.\"\n                continue\n\n            if not sanitize_value_line:\n                yield line\n                continue\n\n            yield sanitize_value_line(line)\n            continue\n\n        # Is the line beginning of `COPY` statement?\n        copy_line_match = COPY_LINE_PATTERN.match(line)\n        if not copy_line_match:\n            yield line\n            continue\n\n        current_table = copy_line_match.group(\"table\")\n        current_table_columns = parse_column_names(copy_line_match.group(\"columns\"))\n\n        sanitize_value_line = get_value_line_sanitizer(\n            config, current_table, current_table_columns)\n\n        yield line", "response": "Sanitize a single database."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_column_names(text):\n    return tuple(\n        re.sub(r\"^\\\"(.*)\\\"$\", r\"\\1\", column_name.strip())\n        for column_name in text.split(\",\")\n    )", "response": "Extracts column names from a string containing quoted and comma separated column names."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of command line arguments and environment variables that can be passed to MySQL dump .", "response": "def get_mysqldump_args_and_env_from_url(url):\n    \"\"\"\n    Constructs list of command line arguments and dictionary of environment\n    variables that can be given to `mysqldump` executable to obtain database\n    dump of the database described in given URL.\n\n    :param url: Parsed database URL.\n    :type url: urllib.urlparse.ParseResult\n\n    :return: List of command line arguments as well as dictionary of\n             environment variables that can be used to launch the MySQL dump\n             process to obtain dump of the database.\n    :rtype: tuple[list[str],dict[str,str]]\n    \"\"\"\n    args = [\n        # Without this, `INSERT INTO` statements will exclude column names from\n        # the output, which are required for sanitation.\n        \"--complete-insert\",\n\n        # This enables use for \"exteded inserts\" where multiple rows of a table\n        # are included in a single `INSERT INTO` statement (contents of the\n        # entire table even, if it's within limits). We use it to increase the\n        # performance of the sanitation and to decrease the dump size.\n        \"--extended-insert\",\n\n        # This makes the `mysqldump` to attempt to limit size of a single line\n        # into 10 megabytes. We use it to reduce memory consumption.\n        \"--net_buffer_length=10240\",\n\n        # Hostname of the database to connect into, should be always present in\n        # the parsed database URL.\n        \"-h\",\n        url.hostname,\n    ]\n    env = {}\n\n    if url.port is not None:\n        args.extend((\"-P\", six.text_type(url.port)))\n\n    if url.username:\n        args.extend((\"-u\", url.username))\n\n    if url.password:\n        env[\"MYSQL_PWD\"] = url.password\n\n    if len(url.path) < 2 or not url.path.startswith(\"/\"):\n        raise ValueError(\"Name of the database is missing from the URL\")\n\n    args.append(url.path[1:])\n\n    return args, env"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef decode_mysql_literal(text):\n    if MYSQL_NULL_PATTERN.match(text):\n        return None\n\n    if MYSQL_BOOLEAN_PATTERN.match(text):\n        return text.lower() == \"true\"\n\n    if MYSQL_FLOAT_PATTERN.match(text):\n        return float(text)\n\n    if MYSQL_INT_PATTERN.match(text):\n        return int(text)\n\n    if MYSQL_STRING_PATTERN.match(text):\n        return decode_mysql_string_literal(text)\n\n    raise ValueError(\"Unable to decode given value: %r\" % (text,))", "response": "Decodes given MySQL literal into Python value."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove quotes and decodes escape sequences from given MySQL string literal.", "response": "def decode_mysql_string_literal(text):\n    \"\"\"\n    Removes quotes and decodes escape sequences from given MySQL string literal\n    returning the result.\n\n    :param text: MySQL string literal, with the quotes still included.\n    :type text: str\n\n    :return: Given string literal with quotes removed and escape sequences\n             decoded.\n    :rtype: str\n    \"\"\"\n    assert text.startswith(\"'\")\n    assert text.endswith(\"'\")\n\n    # Ditch quotes from the string literal.\n    text = text[1:-1]\n\n    return MYSQL_STRING_ESCAPE_SEQUENCE_PATTERN.sub(\n        unescape_single_character,\n        text,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef unescape_single_character(match):\n    value = match.group(0)\n    assert value.startswith(\"\\\\\")\n    return MYSQL_STRING_ESCAPE_SEQUENCE_MAPPING.get(value) or value[1:]", "response": "Unescapes a single character in a MySQL string literal."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef wait_for(self, text, seconds):\n        found = False\n        stream = self.stream\n        start_time = time.time()\n        while not found:\n            if time.time() - start_time > seconds:\n                break\n            stream.data_available.wait(0.5)\n            stream.data_unoccupied.clear()\n            while stream.data:\n                line = stream.data.pop(0)\n                value = line.getvalue()\n                if text in value:\n                    found = True\n                self.lines.append(value)\n                stream.data_available.clear()\n                stream.data_unoccupied.set()\n                if time.time() - start_time > seconds:\n                    break\n        return found", "response": "Waits for the specified text to appear in a line of the\n            output. Returns True if the specified text has appeared in a line of the\n            output or False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef command_executor(self):\n        ip_address = self.ip_address if self.ip_address else '127.0.0.1'\n        return 'http://{}:{}/wd/hub'.format(ip_address, self.port)", "response": "Get the appropriate command executor URL for the Selenium server\n        running in the Docker container."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nstart the Docker container", "response": "def start(self):\n        \"\"\"Start the Docker container\"\"\"\n        if self.container_id is not None:\n            msg = 'The Docker container is already running with ID {}'\n            raise Exception(msg.format(self.container_id))\n\n        process = Popen(['docker ps | grep \":{}\"'.format(self.port)],\n                        shell=True, stdout=PIPE)\n        (grep_output, _grep_error) = process.communicate()\n        lines = grep_output.split('\\n')\n        for line in lines:\n            if ':{}'.format(self.port) in line:\n                other_id = line.split()[0]\n                msg = 'Port {} is already being used by container {}'\n                raise Exception(msg.format(self.port, other_id))\n\n        self.container_id = check_output(self.command, shell=True).strip()\n        try:\n            self.ip_address = check_output(['docker-machine', 'ip']).strip()\n        except (CalledProcessError, OSError):\n            self.ip_address = '127.0.0.1'\n\n        output = OutputMonitor()\n        logs_process = Popen(['docker', 'logs', '-f', self.container_id],\n                             stdout=output.stream.input,\n                             stderr=open(os.devnull, 'w'))\n        ready_log_line = 'Selenium Server is up and running'\n        if not output.wait_for(ready_log_line, 10):\n            logs_process.kill()\n            msg = 'Timeout starting the Selenium server Docker container:\\n'\n            msg += '\\n'.join(output.lines)\n            raise Exception(msg)\n        logs_process.kill()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nstopping the Docker container", "response": "def stop(self):\n        \"\"\"Stop the Docker container\"\"\"\n        if self.container_id is None:\n            raise Exception('No Docker Selenium container was running')\n        check_call(['docker', 'stop', self.container_id])\n        self.container_id = None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nhash a text value to a sequence of integers.", "response": "def hash_text_to_ints(value, bit_lengths=(16, 16, 16, 16)):\n    # type: (str, Sequence[int]) -> Sequence[int]\n    \"\"\"\n    Hash a text value to a sequence of integers.\n\n    Generates a sequence of integer values with given bit-lengths\n    similarly to `hash_text_to_int`, but allowing generating many\n    separate numbers with a single call.\n\n    :param bit_lengths:\n      Tuple of bit lengths for the resulting integers.  Defines also the\n      length of the result tuple.\n    :return:\n      Tuple of ``n`` integers ``(R_1, ... R_n)`` with the requested\n      bit-lengths ``(L_1, ..., L_n)`` and values ranging within\n      ``0 <= R_i < 2**L_i`` for each ``i``.\n    \"\"\"\n    hash_value = hash_text(value)\n    hex_lengths = [x // 4 for x in bit_lengths]\n    hex_ranges = (\n        (sum(hex_lengths[0:i]), sum(hex_lengths[0:(i + 1)]))\n        for i in range(len(hex_lengths)))\n    return tuple(int(hash_value[a:b], 16) for (a, b) in hex_ranges)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef hash_text(value, hasher=hashlib.sha256, encoding='utf-8'):\n    # type: (str, Callable, str) -> str\n    \"\"\"\n    Generate a hash for a text value.\n\n    The hash will be generated by encoding the text to bytes with given\n    encoding and then generating a hash with HMAC using the session\n    secret as the key and the given hash function.\n\n    :param value: Text value to hash\n    :param hasher: Hash function to use, SHA256 by default\n    :param encoding: Encoding to use, UTF-8 by default\n    :return: Hexadecimal presentation of the hash as a string\n    \"\"\"\n    return hash_bytes(value.encode(encoding), hasher)", "response": "Generates a hash for a text value."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef hash_bytes(value, hasher=hashlib.sha256):\n    # type: (bytes, Callable) -> str\n    \"\"\"\n    Generate a hash for a bytes value.\n\n    The hash will be generated by generating a hash with HMAC using the\n    session secret as the key and the given hash function.\n\n    :param value: Bytes value to hash\n    :param hasher: Hash function to use.\n    :return: Hexadecimal presentation of the hash as a string\n    \"\"\"\n    return hmac.new(get_secret(), value, hasher).hexdigest()", "response": "Generates a hash for a bytes value."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _initialize_session():\n    # type: () -> None\n    \"\"\"\n    Generate a new session key and store it to thread local storage.\n    \"\"\"\n    sys_random = random.SystemRandom()\n    _thread_local_storage.secret_key = b''.join(\n        int2byte(sys_random.randint(0, 255))\n        for _ in range(SECRET_KEY_BITS // 8))", "response": "Generate a new session key and store it to thread local storage."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_proxy_object(self, obj, ProxyKlass, proxy_klass_attribute):\n        proxy_object = obj\n        if not isinstance(obj, ProxyKlass):\n            proxy_object = ProxyKlass(**{proxy_klass_attribute: obj})\n        return proxy_object", "response": "Returns the proxy object for an input object"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading configuration from given path to a file in local file system and returns parsed version of it.", "response": "def from_file(cls, filename):\n        \"\"\"\n        Reads configuration from given path to a file in local file system and\n        returns parsed version of it.\n\n        :param filename: Path to the YAML file in local file system where the\n                         configuration will be read from.\n        :type filename: str\n\n        :return: Configuration instance parsed from given configuration file.\n        :rtype: Configuration\n        \"\"\"\n        instance = cls()\n\n        with open(filename, \"rb\") as file_stream:\n            config_data = yaml.load(file_stream)\n\n        instance.load(config_data)\n\n        return instance"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load(self, config_data):\n        if not isinstance(config_data, dict):\n            raise ConfigurationError(\n                \"Configuration data is %s instead of dict.\" % (\n                    type(config_data),\n                )\n            )\n\n        self.load_addon_packages(config_data)\n        self.load_sanitizers(config_data)", "response": "Loads sanitizers and addons according to rulesets defined in given already parsed\n        configuration file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload the addon packages from which the configuration will attempt to load sanitizers from.", "response": "def load_addon_packages(self, config_data):\n        \"\"\"\n        Loads the module paths from which the configuration will attempt to\n        load sanitizers from. These must be stored as a list of strings under\n        \"config.addons\" section of the configuration data.\n\n        :param config_data: Already parsed configuration data, as dictionary.\n        :type config_data: dict[str,any]\n        \"\"\"\n        section_config = config_data.get(\"config\")\n        if not isinstance(section_config, dict):\n            if section_config is None:\n                return\n            raise ConfigurationError(\n                \"'config' is %s instead of dict\" % (\n                    type(section_config),\n                ),\n            )\n\n        section_addons = section_config.get(\"addons\", [])\n        if not isinstance(section_addons, list):\n            raise ConfigurationError(\n                \"'config.addons' is %s instead of list\" % (\n                    type(section_addons),\n                ),\n            )\n\n        for index, module_path in enumerate(section_addons):\n            if not isinstance(module_path, six.text_type):\n                raise ConfigurationError(\n                    \"Item %d in 'config.addons' is %s instead of string\" % (\n                        index,\n                        type(module_path),\n                    ),\n                )\n\n        self.addon_packages = list(section_addons)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_sanitizers(self, config_data):\n        section_strategy = config_data.get(\"strategy\")\n        if not isinstance(section_strategy, dict):\n            if section_strategy is None:\n                return\n            raise ConfigurationError(\n                \"'strategy' is %s instead of dict\" % (\n                    type(section_strategy),\n                ),\n            )\n\n        for table_name, column_data in six.iteritems(section_strategy):\n            if not isinstance(column_data, dict):\n                if column_data is None:\n                    continue\n                raise ConfigurationError(\n                    \"'strategy.%s' is %s instead of dict\" % (\n                        table_name,\n                        type(column_data),\n                    ),\n                )\n\n            for column_name, sanitizer_name in six.iteritems(column_data):\n                if sanitizer_name is None:\n                    continue\n\n                if not isinstance(sanitizer_name, six.text_type):\n                    raise ConfigurationError(\n                        \"'strategy.%s.%s' is %s instead of string\" % (\n                            table_name,\n                            column_name,\n                            type(sanitizer_name),\n                        ),\n                    )\n\n                sanitizer_callback = self.find_sanitizer(sanitizer_name)\n                sanitizer_key = \"%s.%s\" % (table_name, column_name)\n                self.sanitizers[sanitizer_key] = sanitizer_callback", "response": "Loads sanitizers from the configuration data."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find_sanitizer(self, name):\n        # Split the sanitizer name into two parts, one containing the Python\n        # module name, while second containing portion of the function name\n        # we are looking for.\n        name_parts = name.split(\".\")\n        if len(name_parts) < 2:\n            raise ConfigurationError(\n                \"Unable to separate module name from function name in '%s'\" % (\n                    name,\n                ),\n            )\n\n        module_name_suffix = \".\".join(name_parts[:-1])\n        function_name = \"sanitize_%s\" % (name_parts[-1],)\n\n        # Phase 1: Look for custom sanitizer under a top level package called\n        # \"sanitizers\".\n        module_name = \"sanitizers.%s\" % (module_name_suffix,)\n        callback = self.find_sanitizer_from_module(\n            module_name=module_name,\n            function_name=function_name,\n        )\n        if callback:\n            return callback\n\n        # Phase 2: Look for the sanitizer under \"addon\" packages, if any of\n        # such have been defined.\n        for addon_package_name in self.addon_packages:\n            module_name = \"%s.%s\" % (\n                addon_package_name,\n                module_name_suffix,\n            )\n            callback = self.find_sanitizer_from_module(\n                module_name=module_name,\n                function_name=function_name,\n            )\n            if callback:\n                return callback\n\n        # Phase 3: Look from builtin sanitizers.\n        module_name = \"database_sanitizer.sanitizers.%s\" % (module_name_suffix,)\n        callback = self.find_sanitizer_from_module(\n            module_name=module_name,\n            function_name=function_name,\n        )\n        if callback:\n            return callback\n\n        # Give up.\n        raise ConfigurationError(\"Unable to find sanitizer called '%s'\" % (\n            name,\n        ))", "response": "Searches for a sanitizer function with given name."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfind the sanitizer function from given module and function name.", "response": "def find_sanitizer_from_module(module_name, function_name):\n        \"\"\"\n        Attempts to find sanitizer function from given module. If the module\n        cannot be imported, or function with given name does not exist in it,\n        nothing will be returned by this method. Otherwise the found sanitizer\n        function will be returned.\n\n        :param module_name: Name of the module to import the function from.\n        :type module_name: str\n\n        :param function_name: Name of the function to look for inside the\n                              module.\n        :type function_name: str\n\n        :return: Sanitizer function found from the module, if it can be\n                 imported and it indeed contains function with the given name.\n                 Otherwise None will be returned instead.\n        :rtype: callback|None\n        \"\"\"\n        try:\n            module = importlib.import_module(module_name)\n        except ImportError:\n            return None\n\n        # Look for the function inside the module. At this point it could be\n        # pretty much anything.\n        callback = getattr(module, function_name, None)\n\n        # Function does not exist in this module? Give up.\n        if callback is None:\n            return None\n\n        # It's actually callable function? Return it.\n        if callable(callback):\n            return callback\n\n        # Sanitizer seems to be something else than a function. Throw an\n        # exception to report such problem.\n        raise ConfigurationError(\"'%s' in '%s' is %s instead of function\" % (\n            function_name,\n            module_name,\n            type(callback),\n        ))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget sanitizer function for given table and column name.", "response": "def get_sanitizer_for(self, table_name, column_name):\n        \"\"\"\n        Get sanitizer for given table and column name.\n\n        :param table_name: Name of the database table.\n        :type table_name: str\n\n        :param column_name: Name of the database column.\n        :type column_name: str\n\n        :return: Sanitizer function or None if nothing is configured\n        :rtype: Optional[Callable[[Optional[str]], Optional[str]]]\n        \"\"\"\n        sanitizer_key = \"%s.%s\" % (table_name, column_name)\n        return self.sanitizers.get(sanitizer_key)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sanitize(self, table_name, column_name, value):\n        sanitizer_callback = self.get_sanitizer_for(table_name, column_name)\n        return sanitizer_callback(value) if sanitizer_callback else value", "response": "Sanitize the given value extracted from the database according to the sanitation configuration."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_arguments(self, parser):\n        # Add the underlying test command arguments first\n        test_command = TestCommand()\n        test_command.add_arguments(parser)\n\n        for option in OPTIONS:\n            parser.add_argument(*option[0], **option[1])", "response": "Add command line arguments for Django 1. 8 +"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\noverrides the base create_parser method to add custom option list in Django 1. 7 and below.", "response": "def create_parser(self, prog_name, subcommand):\n        \"\"\"\n        Override the base create_parser() method to add this command's custom\n        options in Django 1.7 and below.\n        \"\"\"\n        if not self.use_argparse:\n            self.__class__.option_list = TestCommand.option_list + self.custom_options\n        parser = super(Command, self).create_parser(prog_name, subcommand)\n        return parser"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clean():\n        screenshot_dir = settings.SELENIUM_SCREENSHOT_DIR\n        if screenshot_dir and os.path.isdir(screenshot_dir):\n            rmtree(screenshot_dir, ignore_errors=True)", "response": "Clear out any old screenshots"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef handle(self, *args, **options):\n        browser_name = options['browser_name']\n        count = options['count']\n        if len(args) > 0:\n            tests = list(args)\n        else:\n            tests = settings.SELENIUM_DEFAULT_TESTS\n\n        # Kill any orphaned chromedriver processes\n        process = Popen(['killall', 'chromedriver'],\n                        stderr=open(os.devnull, 'w'))\n        process.wait()\n\n        # Clear any old log and screenshots\n        self.clean()\n\n        docker = None\n        sc_process = None\n        selenium_process = None\n        if options['docker']:\n            if browser_name not in ['chrome', 'firefox']:\n                self.stdout.write('Only chrome and firefox can currently be run in a Docker container')\n                return\n            docker = DockerSelenium(browser=browser_name,\n                                    port=settings.SELENIUM_DOCKER_PORT,\n                                    tag=settings.SELENIUM_DOCKER_TAG,\n                                    debug=settings.SELENIUM_DOCKER_DEBUG)\n        elif 'platform' in options and settings.SELENIUM_SAUCE_CONNECT_PATH:\n            running, sc_process = self.verify_sauce_connect_is_running(options)\n            if not running:\n                return\n        elif browser_name in ['opera', 'safari']:\n            running, selenium_process = self.verify_selenium_server_is_running()\n            if not running:\n                return\n        elif browser_name in ['ipad', 'iphone']:\n            if not self.verify_appium_is_running():\n                return\n\n        # Make it so django-nose won't have nosetests choke on our parameters\n        TestRunner = get_runner(django_settings)\n        if hasattr(TestRunner, 'django_opts'):\n            for option in OPTIONS:\n                TestRunner.django_opts.extend(option[0])\n\n        # Configure and run the tests\n        try:\n            if docker:\n                docker.start()\n                options['command_executor'] = docker.command_executor()\n            self.update_environment(options)\n            self.run_tests(tests, browser_name, count)\n        finally:\n            # Stop the Selenium Docker container, if running\n            if docker and docker.container_id:\n                docker.stop()\n\n        # Kill Sauce Connect, if running\n        if sc_process:\n            sc_process.kill()\n\n        # Kill the Selenium standalone server, if running\n        if selenium_process:\n            selenium_process.kill()", "response": "Handles the command line and runs the specified Selenium tests."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npopulates the environment variables that need to be set for the test process.", "response": "def update_environment(options):\n        \"\"\"\n        Populate the environment variables that need to be added for test\n        execution to work correctly.  Most (but not all) of these are to match\n        what the Jenkins Sauce OnDemand plugin would use for the test\n        configuration that was specified on the command line.\n        \"\"\"\n        env = os.environ\n        # https://docs.djangoproject.com/en/1.6/topics/testing/tools/#liveservertestcase\n        env['DJANGO_LIVE_TEST_SERVER_ADDRESS'] = settings.DJANGO_LIVE_TEST_SERVER_ADDRESS\n        tunnel_id = options['tunnel_id']\n        if tunnel_id:\n            env['SAUCE_TUNNEL_ID'] = tunnel_id\n        if 'SAUCE_API_KEY' in env:\n            # Jenkins plugin has already configured the environment for us\n            return\n        env['SELENIUM_BROWSER'] = options['browser_name']\n        if options['command_executor']:\n            env['SELENIUM_COMMAND_EXECUTOR'] = options['command_executor']\n        platform = options['platform']\n        browser_version = options['browser_version']\n        if not platform or not browser_version:\n            # None of the following Sauce OnDemand stuff applies\n            return\n        if settings.SELENIUM_SAUCE_CONNECT_PATH:\n            host = 'localhost'\n            port = '4445'\n        else:\n            host = 'ondemand.saucelabs.com'\n            port = '80'\n        env.update({\n            'SAUCE_API_KEY': settings.SELENIUM_SAUCE_API_KEY,\n            'SAUCE_USER_NAME': settings.SELENIUM_SAUCE_USERNAME,\n            'SELENIUM_BROWSER': options['browser_name'],\n            'SELENIUM_HOST': host,\n            'SELENIUM_PLATFORM': platform,\n            'SELENIUM_PORT': port,\n            'SELENIUM_VERSION': browser_version,\n        })"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef verify_sauce_connect_is_running(self, options):\n        sc_path = settings.SELENIUM_SAUCE_CONNECT_PATH\n        if len(sc_path) < 2:\n            self.stdout.write('You need to configure SELENIUM_SAUCE_CONNECT_PATH')\n            return False, None\n        username = settings.SELENIUM_SAUCE_USERNAME\n        if not username:\n            self.stdout.write('You need to configure SELENIUM_SAUCE_USERNAME')\n            return False, None\n        key = settings.SELENIUM_SAUCE_API_KEY\n        if not key:\n            self.stdout.write('You need to configure SELENIUM_SAUCE_API_KEY')\n            return False, None\n        # Is it already running?\n        process = Popen(['ps -e | grep \"%s\"' % key],\n                        shell=True, stdout=PIPE)\n        (grep_output, _grep_error) = process.communicate()\n        grep_command = 'grep {}'.format(key)\n        lines = grep_output.split('\\n')\n        for line in lines:\n            if 'sc' in line and username in line and grep_command not in line:\n                self.stdout.write('Sauce Connect is already running')\n                return True, None\n        self.stdout.write('Starting Sauce Connect')\n        output = OutputMonitor()\n        command = [sc_path, '-u', username, '-k', key]\n        tunnel_id = options['tunnel_id']\n        if tunnel_id:\n            command.extend(['-i', tunnel_id])\n        sc_process = Popen(command,\n                           stdout=output.stream.input,\n                           stderr=open(os.devnull, 'w'),\n                           universal_newlines=True)\n        ready_log_line = 'Connection established.'\n        if not output.wait_for(ready_log_line, 60):\n            self.stdout.write('Timeout starting Sauce Connect:\\n')\n            self.stdout.write('\\n'.join(output.lines))\n            return False, None\n        return True, sc_process", "response": "Verify that the Sauce Connect process is running."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef verify_selenium_server_is_running(self):\n        selenium_jar = settings.SELENIUM_JAR_PATH\n        if len(selenium_jar) < 5:\n            self.stdout.write('You need to configure SELENIUM_JAR_PATH')\n            return False, None\n        _jar_dir, jar_name = os.path.split(selenium_jar)\n        # Is it already running?\n        process = Popen(['ps -e | grep \"%s\"' % jar_name[:-4]],\n                        shell=True, stdout=PIPE)\n        (grep_output, _grep_error) = process.communicate()\n        lines = grep_output.split('\\n')\n        for line in lines:\n            if jar_name in line:\n                self.stdout.write('Selenium standalone server is already running')\n                return True, None\n        self.stdout.write('Starting the Selenium standalone server')\n        output = OutputMonitor()\n        selenium_process = Popen(['java', '-jar', selenium_jar],\n                                 stdout=open(os.devnull, 'w'),\n                                 stderr=output.stream.input)\n        ready_log_line = 'Selenium Server is up and running'\n        if not output.wait_for(ready_log_line, 10):\n            self.stdout.write('Timeout starting the Selenium server:\\n')\n            self.stdout.write('\\n'.join(output.lines))\n            return False, None\n        return True, selenium_process", "response": "Verify that the Selenium standalone server is running."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef verify_appium_is_running(self):\n        process = Popen(['ps -e | grep \"Appium\"'], shell=True, stdout=PIPE)\n        (grep_output, _grep_error) = process.communicate()\n        lines = grep_output.split('\\n')\n        for line in lines:\n            if 'Appium.app' in line:\n                self.stdout.write('Appium is already running')\n                return True\n        self.stdout.write('Please launch and configure Appium first')\n        return False", "response": "Verify that Appium is running so it can be used for local iOS tests."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef hash(obj, hash_name='md5', coerce_mmap=False):\n    if 'numpy' in sys.modules:\n        hasher = NumpyHasher(hash_name=hash_name, coerce_mmap=coerce_mmap)\n    else:\n        hasher = Hasher(hash_name=hash_name)\n    return hasher.hash(obj)", "response": "Quick calculation of a hash for uniquely Python objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef save(self, obj):\n        if isinstance(obj, self.np.ndarray) and not obj.dtype.hasobject:\n            # Compute a hash of the object\n            # The update function of the hash requires a c_contiguous buffer.\n            if obj.shape == ():\n                # 0d arrays need to be flattened because viewing them as bytes\n                # raises a ValueError exception.\n                obj_c_contiguous = obj.flatten()\n            elif obj.flags.c_contiguous:\n                obj_c_contiguous = obj\n            elif obj.flags.f_contiguous:\n                obj_c_contiguous = obj.T\n            else:\n                # Cater for non-single-segment arrays: this creates a\n                # copy, and thus aleviates this issue.\n                # XXX: There might be a more efficient way of doing this\n                obj_c_contiguous = obj.flatten()\n\n            # memoryview is not supported for some dtypes, e.g. datetime64, see\n            # https://github.com/numpy/numpy/issues/4983. The\n            # workaround is to view the array as bytes before\n            # taking the memoryview.\n            self._hash.update(\n                self._getbuffer(obj_c_contiguous.view(self.np.uint8)))\n\n            # We store the class, to be able to distinguish between\n            # Objects with the same binary content, but different\n            # classes.\n            if self.coerce_mmap and isinstance(obj, self.np.memmap):\n                # We don't make the difference between memmap and\n                # normal ndarrays, to be able to reload previously\n                # computed results with memmap.\n                klass = self.np.ndarray\n            else:\n                klass = obj.__class__\n            # We also return the dtype and the shape, to distinguish\n            # different views on the same data with different dtypes.\n\n            # The object will be pickled by the pickler hashed at the end.\n            obj = (klass, ('HASHED', obj.dtype, obj.shape, obj.strides))\n        elif isinstance(obj, self.np.dtype):\n            # Atomic dtype objects are interned by their default constructor:\n            # np.dtype('f8') is np.dtype('f8')\n            # This interning is not maintained by a\n            # pickle.loads + pickle.dumps cycle, because __reduce__\n            # uses copy=True in the dtype constructor. This\n            # non-deterministic behavior causes the internal memoizer\n            # of the hasher to generate different hash values\n            # depending on the history of the dtype object.\n            # To prevent the hash from being sensitive to this, we use\n            # .descr which is a full (and never interned) description of\n            # the array dtype according to the numpy doc.\n            klass = obj.__class__\n            obj = (klass, ('HASHED', obj.descr))\n        Hasher.save(self, obj)", "response": "Subclass the save method to hash ndarray subclass rather\n            than pickling them."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sanitize_random(value):\n    if not value:\n        return value\n    return ''.join(random.choice(CHARACTERS) for _ in range(len(value)))", "response": "Sanitize a random string."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sanitize(url, config):\n    if url.scheme != \"mysql\":\n        raise ValueError(\"Unsupported database type: '%s'\" % (url.scheme,))\n\n    args, env = get_mysqldump_args_and_env_from_url(url=url)\n\n    process = subprocess.Popen(\n        args=[\"mysqldump\"] + args,\n        env=env,\n        stdout=subprocess.PIPE,\n    )\n\n    return sanitize_from_stream(stream=process.stdout, config=config)", "response": "Sanitize MySQL database by executing mysqldump command and sanitizes it output."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sanitize_from_stream(stream, config):\n    for line in io.TextIOWrapper(stream, encoding=\"utf-8\"):\n        # Eat the trailing new line.\n        line = line.rstrip(\"\\n\")\n\n        # If there is no configuration it means that there are no sanitizers\n        # available.\n        if not config:\n            yield line\n            continue\n\n        # Does the line contain `INSERT INTO` statement? If not, use the line\n        # as-is and continue into next one.\n        insert_into_match = INSERT_INTO_PATTERN.match(line)\n        if not insert_into_match:\n            yield line\n            continue\n\n        table_name = insert_into_match.group(\"table\")\n        column_names = parse_column_names(insert_into_match.group(\"columns\"))\n\n        # Collect sanitizers possibly used for this table and place them into\n        # a dictionary from which we can look them up by index later.\n        sanitizers = {}\n        for index, column_name in enumerate(column_names):\n            sanitizer = config.get_sanitizer_for(\n                table_name=table_name,\n                column_name=column_name,\n            )\n            if sanitizer:\n                sanitizers[index] = sanitizer\n\n        # If this table has no sanitizers available, use the line as-is and\n        # continue into next line.\n        if len(sanitizers) == 0:\n            yield line\n            continue\n\n        # Constructs list of tuples containing sanitized column names.\n        sanitized_value_tuples = []\n        for values in parse_values(insert_into_match.group(\"values\")):\n            if len(column_names) != len(values):\n                raise ValueError(\"Mismatch between column names and values\")\n            sanitized_values = []\n            for index, value in enumerate(values):\n                sanitizer_callback = sanitizers.get(index)\n                if sanitizer_callback:\n                    value = sanitizer_callback(value)\n                sanitized_values.append(encode_mysql_literal(value))\n            sanitized_value_tuples.append(sanitized_values)\n\n        # Finally create new `INSERT INTO` statement from the sanitized values.\n        yield \"INSERT INTO `%s` (%s) VALUES %s;\" % (\n            table_name,\n            \", \".join(\"`\" + column_name + \"`\" for column_name in column_names),\n            \",\".join(\n                \"(\" + \",\".join(value_tuple) + \")\"\n                for value_tuple in sanitized_value_tuples\n            ),\n        )", "response": "Reads dump of MySQL database from given stream and sanitizes it."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nextracts column names from a string containing quoted and comma separated column names of a table.", "response": "def parse_column_names(text):\n    \"\"\"\n    Extracts column names from a string containing quoted and comma separated\n    column names of a table.\n\n    :param text: Line extracted from MySQL's `INSERT INTO` statement containing\n                 quoted and comma separated column names.\n    :type text: str\n\n    :return: Tuple containing just the column names.\n    :rtype: tuple[str]\n    \"\"\"\n    return tuple(\n        re.sub(r\"^`(.*)`$\", r\"\\1\", column_data.strip())\n        for column_data in text.split(\",\")\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing a string containing values from an extended format INSERT INTO statement.", "response": "def parse_values(text):\n    \"\"\"\n    Parses values from a string containing values from extended format `INSERT\n    INTO` statement. Values will be yielded from the function as tuples, with\n    one tuple per row in the table.\n\n    :param text: Text extracted from MySQL's `INSERT INTO` statement containing\n                 quoted and comma separated column values.\n    :type text: str\n    \"\"\"\n    assert text.startswith(\"(\")\n    pos = 1\n    values = []\n    text_len = len(text)\n    while pos < text_len:\n        match = VALUE_PATTERN.match(text, pos)\n        if not match:\n            break\n        value = match.group(1)\n        values.append(decode_mysql_literal(value.strip()))\n        pos += len(value) + 1\n        if match.group(2) == \")\":\n            # Skip comma and open parenthesis \",(\"\n            pos += 2\n            yield tuple(values)\n            values = []"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef persist(self):\n        if self.hash:\n            with open(self.file_name, 'wb') as f:\n                pickle.dump(self.object_property, f)\n            return True\n        return False", "response": "a private method that persists an object to the filesystem"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_sql_server_ver(self):\n        if self._ss_ver is not None:\n            return self._ss_ver\n        cur = self.connection.cursor()\n        ver_code = None\n        if not self.is_db2 and not self.is_openedge:\n            cur.execute(\"SELECT CAST(SERVERPROPERTY('ProductVersion') as varchar)\")\n            ver_code = cur.fetchone()[0]\n            ver_code = int(ver_code.split('.')[0])\n        else:\n            ver_code = 0\n        if ver_code >= 11:\n            self._ss_ver = 2012\n        elif ver_code == 10:\n            self._ss_ver = 2008\n        elif ver_code == 9:\n            self._ss_ver = 2005\n        else:\n            self._ss_ver = 2000\n        return self._ss_ver", "response": "Returns the SQL Server version of the SQL Server in use."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the SQL that will convert field_name to UTC from tzname.", "response": "def _switch_tz_offset_sql(self, field_name, tzname):\n        \"\"\"\n        Returns the SQL that will convert field_name to UTC from tzname.\n        \"\"\"\n        field_name = self.quote_name(field_name)\n        if settings.USE_TZ:\n            if pytz is None:\n                from django.core.exceptions import ImproperlyConfigured\n                raise ImproperlyConfigured(\"This query requires pytz, \"\n                                           \"but it isn't installed.\")\n            tz = pytz.timezone(tzname)\n            td = tz.utcoffset(datetime.datetime(2000, 1, 1))\n\n            def total_seconds(td):\n                if hasattr(td, 'total_seconds'):\n                    return td.total_seconds()\n                else:\n                    return td.days * 24 * 60 * 60 + td.seconds\n\n            total_minutes = total_seconds(td) // 60\n            hours, minutes = divmod(total_minutes, 60)\n            tzoffset = \"%+03d:%02d\" % (hours, minutes)\n            field_name = \"CAST(SWITCHOFFSET(TODATETIMEOFFSET(%s, '+00:00'), '%s') AS DATETIME2)\" % (field_name, tzoffset)\n        return field_name"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the SQL that truncates the given datetime field_name to a datetime object with only the given specificity and a tuple of parameters.", "response": "def datetime_trunc_sql(self, lookup_type, field_name, tzname):\n        \"\"\"\n        Given a lookup_type of 'year', 'month', 'day', 'hour', 'minute' or\n        'second', returns the SQL that truncates the given datetime field\n        field_name to a datetime object with only the given specificity, and\n        a tuple of parameters.\n        \"\"\"\n        field_name = self._switch_tz_offset_sql(field_name, tzname)\n        reference_date = '0' # 1900-01-01\n        if lookup_type in ['minute', 'second']:\n            # Prevent DATEDIFF overflow by using the first day of the year as\n            # the reference point. Only using for minute and second to avoid any\n            # potential performance hit for queries against very large datasets.\n            reference_date = \"CONVERT(datetime2, CONVERT(char(4), {field_name}, 112) + '0101', 112)\".format(\n                field_name=field_name,\n            )\n        sql = \"DATEADD({lookup}, DATEDIFF({lookup}, {reference_date}, {field_name}), {reference_date})\".format(\n            lookup=lookup_type,\n            field_name=field_name,\n            reference_date=reference_date,\n        )\n        return sql, []"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngive a cursor object that has just performed an INSERT statement into a table that has an auto-incrementing ID, returns the newly created ID. This method also receives the table name and the name of the primary-key column.", "response": "def last_insert_id(self, cursor, table_name, pk_name):\n        \"\"\"\n        Given a cursor object that has just performed an INSERT statement into\n        a table that has an auto-incrementing ID, returns the newly created ID.\n\n        This method also receives the table name and the name of the primary-key\n        column.\n        \"\"\"\n        # TODO: Check how the `last_insert_id` is being used in the upper layers\n        #       in context of multithreaded access, compare with other backends\n\n        # IDENT_CURRENT:  http://msdn2.microsoft.com/en-us/library/ms175098.aspx\n        # SCOPE_IDENTITY: http://msdn2.microsoft.com/en-us/library/ms190315.aspx\n        # @@IDENTITY:     http://msdn2.microsoft.com/en-us/library/ms187342.aspx\n\n        # IDENT_CURRENT is not limited by scope and session; it is limited to\n        # a specified table. IDENT_CURRENT returns the value generated for\n        # a specific table in any session and any scope.\n        # SCOPE_IDENTITY and @@IDENTITY return the last identity values that\n        # are generated in any table in the current session. However,\n        # SCOPE_IDENTITY returns values inserted only within the current scope;\n        # @@IDENTITY is not limited to a specific scope.\n\n        table_name = self.quote_name(table_name)\n        cursor.execute(\"SELECT CAST(IDENT_CURRENT(%s) as bigint)\", [table_name])\n        return cursor.fetchone()[0]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef quote_name(self, name):\n        if name.startswith(self.left_sql_quote) and name.endswith(self.right_sql_quote):\n            return name # Quoting once is enough.\n        return '%s%s%s' % (self.left_sql_quote, name, self.right_sql_quote)", "response": "Quote the given name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef last_executed_query(self, cursor, sql, params):\n        return super(DatabaseOperations, self).last_executed_query(cursor, cursor.last_sql, cursor.last_params)", "response": "Returns the last executed query."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nflush the given database tables and sequences.", "response": "def sql_flush(self, style, tables, sequences, allow_cascade=False):\n        \"\"\"\n        Returns a list of SQL statements required to remove all data from\n        the given database tables (without actually removing the tables\n        themselves).\n\n        The `style` argument is a Style object as returned by either\n        color_style() or no_style() in django.core.management.color.\n        \"\"\"\n        if tables:\n            # Cannot use TRUNCATE on tables that are referenced by a FOREIGN KEY\n            # So must use the much slower DELETE\n            from django.db import connections\n            cursor = connections[self.connection.alias].cursor()\n            # Try to minimize the risks of the braindeaded inconsistency in\n            # DBCC CHEKIDENT(table, RESEED, n) behavior.\n            seqs = []\n            for seq in sequences:\n                cursor.execute(\"SELECT COUNT(*) FROM %s\" % self.quote_name(seq[\"table\"]))\n                rowcnt = cursor.fetchone()[0]\n                elem = {}\n                if rowcnt:\n                    elem['start_id'] = 0\n                else:\n                    elem['start_id'] = 1\n                elem.update(seq)\n                seqs.append(elem)\n            cursor.execute(\"SELECT TABLE_NAME, CONSTRAINT_NAME FROM INFORMATION_SCHEMA.TABLE_CONSTRAINTS WHERE CONSTRAINT_TYPE not in ('PRIMARY KEY','UNIQUE')\")\n            fks = cursor.fetchall()\n            sql_list = ['ALTER TABLE %s NOCHECK CONSTRAINT %s;' % \\\n                    (self.quote_name(fk[0]), self.quote_name(fk[1])) for fk in fks]\n            sql_list.extend(['%s %s %s;' % (style.SQL_KEYWORD('DELETE'), style.SQL_KEYWORD('FROM'),\n                             style.SQL_FIELD(self.quote_name(table)) ) for table in tables])\n\n            if self.on_azure_sql_db:\n                import warnings\n                warnings.warn(\"The identity columns will never be reset \" \\\n                              \"on Windows Azure SQL Database.\",\n                              RuntimeWarning)\n            else:\n                # Then reset the counters on each table.\n                sql_list.extend(['%s %s (%s, %s, %s) %s %s;' % (\n                    style.SQL_KEYWORD('DBCC'),\n                    style.SQL_KEYWORD('CHECKIDENT'),\n                    style.SQL_FIELD(self.quote_name(seq[\"table\"])),\n                    style.SQL_KEYWORD('RESEED'),\n                    style.SQL_FIELD('%d' % seq['start_id']),\n                    style.SQL_KEYWORD('WITH'),\n                    style.SQL_KEYWORD('NO_INFOMSGS'),\n                    ) for seq in seqs])\n\n            sql_list.extend(['ALTER TABLE %s CHECK CONSTRAINT %s;' % \\\n                    (self.quote_name(fk[0]), self.quote_name(fk[1])) for fk in fks])\n            return sql_list\n        else:\n            return []"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef adapt_datetimefield_value(self, value):\n        if value is None:\n            return None\n        if self.connection._DJANGO_VERSION >= 14 and settings.USE_TZ:\n            if timezone.is_aware(value):\n                # pyodbc donesn't support datetimeoffset\n                value = value.astimezone(timezone.utc)\n        if not self.connection.features.supports_microsecond_precision:\n            value = value.replace(microsecond=0)\n        return value", "response": "Transform a datetime value to an object compatible with what is expected\n           ."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntransforming a time value to an object compatible with what is expected .", "response": "def adapt_timefield_value(self, value):\n        \"\"\"\n        Transform a time value to an object compatible with what is expected\n        by the backend driver for time columns.\n        \"\"\"\n        if value is None:\n            return None\n        # SQL Server doesn't support microseconds\n        if isinstance(value, string_types):\n            return datetime.datetime(*(time.strptime(value, '%H:%M:%S')[:6]))\n        return datetime.datetime(1900, 1, 1, value.hour, value.minute, value.second)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntransforms a value to an object compatible with what is expected by the backend driver for decimal columns.", "response": "def adapt_decimalfield_value(self, value, max_digits, decimal_places):\n        \"\"\"\n        Transform a decimal.Decimal value to an object compatible with what is\n        expected by the backend driver for decimal (numeric) columns.\n        \"\"\"\n        if value is None:\n            return None\n        if isinstance(value, decimal.Decimal):\n            context = decimal.getcontext().copy()\n            context.prec = max_digits\n            #context.rounding = ROUND_FLOOR\n            return \"%.*f\" % (decimal_places + 1, value.quantize(decimal.Decimal(\".1\") ** decimal_places, context=context))\n        else:\n            return \"%.*f\" % (decimal_places + 1, value)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef convert_values(self, value, field):\n        if value is None:\n            return None\n        if field and field.get_internal_type() == 'DateTimeField':\n            if isinstance(value, string_types) and value:\n                value = parse_datetime(value)\n            return value\n        elif field and field.get_internal_type() == 'DateField':\n            if isinstance(value, datetime.datetime):\n                value = value.date() # extract date\n            elif isinstance(value, string_types):\n                value = parse_date(value)\n        elif field and field.get_internal_type() == 'TimeField':\n            if (isinstance(value, datetime.datetime) and value.year == 1900 and value.month == value.day == 1):\n                value = value.time() # extract time\n            elif isinstance(value, string_types):\n                # If the value is a string, parse it using parse_time.\n                value = parse_time(value)\n        # Some cases (for example when select_related() is used) aren't\n        # caught by the DateField case above and date fields arrive from\n        # the DB as datetime instances.\n        # Implement a workaround stealing the idea from the Oracle\n        # backend. It's not perfect so the same warning applies (i.e. if a\n        # query results in valid date+time values with the time part set\n        # to midnight, this workaround can surprise us by converting them\n        # to the datetime.date Python type).\n        elif isinstance(value, datetime.datetime) and value.hour == value.minute == value.second == value.microsecond == 0:\n            value = value.date()\n        # Force floats to the correct type\n        elif value is not None and field and field.get_internal_type() == 'FloatField':\n            value = float(value)\n        return value", "response": "Converts the value returned by the database backend into a consistent version of the type that is compatible with the field type."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of table names in the current database.", "response": "def get_table_list(self, cursor):\n        \"\"\"\n        Returns a list of table names in the current database.\n        \"\"\"\n        # TABLES: http://msdn2.microsoft.com/en-us/library/ms186224.aspx\n        # TODO: Believe the below queries should actually select `TABLE_NAME, TABLE_TYPE`\n        if cursor.db_wrpr.limit_table_list:\n            cursor.execute(\"SELECT TABLE_NAME, 't' FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_TYPE = 'BASE TABLE' AND TABLE_SCHEMA = 'dbo'\")\n        else:\n            cursor.execute(\"SELECT TABLE_NAME, 't' FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_TYPE = 'BASE TABLE'\")\n\n        return [row_to_table_info(row) for row in cursor.fetchall()]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks whether the column is Identity.", "response": "def _is_auto_field(self, cursor, table_name, column_name):\n        \"\"\"\n        Checks whether column is Identity\n        \"\"\"\n        # COLUMNPROPERTY: http://msdn2.microsoft.com/en-us/library/ms174968.aspx\n\n        #from django.db import connection\n        #cursor.execute(\"SELECT COLUMNPROPERTY(OBJECT_ID(%s), %s, 'IsIdentity')\",\n        #                 (connection.ops.quote_name(table_name), column_name))\n        cursor.execute(\"SELECT COLUMNPROPERTY(OBJECT_ID(%s), %s, 'IsIdentity')\",\n                         (self.connection.ops.quote_name(table_name), column_name))\n        return cursor.fetchall()[0][0]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_table_description(self, cursor, table_name, identity_check=True):\n\n        # map pyodbc's cursor.columns to db-api cursor description\n        columns = [[c[3], c[4], None, c[6], c[6], c[8], c[10]] for c in cursor.columns(table=table_name)]\n        items = []\n        for column in columns:\n            if identity_check and self._is_auto_field(cursor, table_name, column[0]):\n                column[1] = SQL_AUTOFIELD\n            # The conversion from TextField to CharField below is unwise.\n            #   A SQLServer db field of type \"Text\" is not interchangeable with a CharField, no matter how short its max_length.\n            #   For example, model.objects.values(<text_field_name>).count() will fail on a sqlserver 'text' field\n            if column[1] == Database.SQL_WVARCHAR and column[3] < 4000:\n                column[1] = Database.SQL_WCHAR\n            items.append(column)\n        return items", "response": "Returns a description of the table with DB - API cursor. description interface."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _name_to_index(self, cursor, table_name):\n        return dict([(d[0], i) for i, d in enumerate(self.get_table_description(cursor, table_name, identity_check=False))])", "response": "Returns a dictionary of field_name to field_index."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_relations(self, cursor, table_name):\n        # CONSTRAINT_COLUMN_USAGE: http://msdn2.microsoft.com/en-us/library/ms174431.aspx\n        # CONSTRAINT_TABLE_USAGE:  http://msdn2.microsoft.com/en-us/library/ms179883.aspx\n        # REFERENTIAL_CONSTRAINTS: http://msdn2.microsoft.com/en-us/library/ms179987.aspx\n        # TABLE_CONSTRAINTS:       http://msdn2.microsoft.com/en-us/library/ms181757.aspx\n\n        table_index = self._name_to_index(cursor, table_name)\n        sql = \"\"\"\nSELECT e.COLUMN_NAME AS column_name,\n  c.TABLE_NAME AS referenced_table_name,\n  d.COLUMN_NAME AS referenced_column_name\nFROM INFORMATION_SCHEMA.TABLE_CONSTRAINTS AS a\nINNER JOIN INFORMATION_SCHEMA.REFERENTIAL_CONSTRAINTS AS b\n  ON a.CONSTRAINT_NAME = b.CONSTRAINT_NAME\nINNER JOIN INFORMATION_SCHEMA.CONSTRAINT_TABLE_USAGE AS c\n  ON b.UNIQUE_CONSTRAINT_NAME = c.CONSTRAINT_NAME\nINNER JOIN INFORMATION_SCHEMA.CONSTRAINT_COLUMN_USAGE AS d\n  ON c.CONSTRAINT_NAME = d.CONSTRAINT_NAME\nINNER JOIN INFORMATION_SCHEMA.CONSTRAINT_COLUMN_USAGE AS e\n  ON a.CONSTRAINT_NAME = e.CONSTRAINT_NAME\nWHERE a.TABLE_NAME = %s AND a.CONSTRAINT_TYPE = 'FOREIGN KEY'\"\"\"\n        cursor.execute(sql, (table_name,))\n        return dict([(table_index[item[0]], (self._name_to_index(cursor, item[1])[item[2]], item[1]))\n                     for item in cursor.fetchall()])", "response": "Returns a dictionary of field_index = > field_index_other_table other_table = > other_table"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_key_columns(self, cursor, table_name):\n        source_field_dict = self._name_to_index(cursor, table_name)\n\n        sql = \"\"\"\nselect\n    COLUMN_NAME = fk_cols.COLUMN_NAME,\n    REFERENCED_TABLE_NAME = pk.TABLE_NAME,\n    REFERENCED_COLUMN_NAME = pk_cols.COLUMN_NAME\nfrom INFORMATION_SCHEMA.REFERENTIAL_CONSTRAINTS ref_const\njoin INFORMATION_SCHEMA.TABLE_CONSTRAINTS fk\n    on ref_const.CONSTRAINT_CATALOG = fk.CONSTRAINT_CATALOG\n    and ref_const.CONSTRAINT_SCHEMA = fk.CONSTRAINT_SCHEMA\n    and ref_const.CONSTRAINT_NAME = fk.CONSTRAINT_NAME\n    and fk.CONSTRAINT_TYPE = 'FOREIGN KEY'\n\njoin INFORMATION_SCHEMA.TABLE_CONSTRAINTS pk\n    on ref_const.UNIQUE_CONSTRAINT_CATALOG = pk.CONSTRAINT_CATALOG\n    and ref_const.UNIQUE_CONSTRAINT_SCHEMA = pk.CONSTRAINT_SCHEMA\n    and ref_const.UNIQUE_CONSTRAINT_NAME = pk.CONSTRAINT_NAME\n    And pk.CONSTRAINT_TYPE = 'PRIMARY KEY'\n\njoin INFORMATION_SCHEMA.KEY_COLUMN_USAGE fk_cols\n    on ref_const.CONSTRAINT_NAME = fk_cols.CONSTRAINT_NAME\n\njoin INFORMATION_SCHEMA.KEY_COLUMN_USAGE pk_cols\n    on pk.CONSTRAINT_NAME = pk_cols.CONSTRAINT_NAME\nwhere\n    fk.TABLE_NAME = %s\"\"\"\n\n        cursor.execute(sql,[table_name])\n        relations = cursor.fetchall()\n\n        key_columns = []\n        key_columns.extend([(source_column, target_table, target_column) \\\n            for source_column, target_table, target_column in relations])\n        return key_columns", "response": "Returns a list of tuples of column_name referenced_table_name referenced_column_name for all key columns in given table."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbreaking a string s into the part before the substring to find and the part including and after the substring.", "response": "def _break(s, find):\n    \"\"\"Break a string s into the part before the substring to find,\n    and the part including and after the substring.\"\"\"\n    i = s.find(find)\n    return s[:i], s[i:]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfix the aggregate functions that are not supported by other django backends.", "response": "def _fix_aggregates(self):\n        \"\"\"\n        MSSQL doesn't match the behavior of the other backends on a few of\n        the aggregate functions; different return type behavior, different\n        function names, etc.\n\n        MSSQL's implementation of AVG maintains datatype without proding. To\n        match behavior of other django backends, it needs to not drop remainders.\n        E.g. AVG([1, 2]) needs to yield 1.5, not 1\n        \"\"\"\n        try:\n            # for django 1.10 and up (works starting in 1.8 so I am told)\n            select = self.query.annotation_select\n        except AttributeError:\n            # older\n            select = self.query.aggregate_select\n\n        for alias, aggregate in select.items():\n            if not hasattr(aggregate, 'sql_function'):\n                continue\n            if aggregate.sql_function == 'AVG':# and self.connection.cast_avg_to_float:\n                # Embed the CAST in the template on this query to\n                # maintain multi-db support.\n                select[alias].sql_template = \\\n                    '%(function)s(CAST(%(field)s AS FLOAT))'\n            # translate StdDev function names\n            elif aggregate.sql_function == 'STDDEV_SAMP':\n                select[alias].sql_function = 'STDEV'\n            elif aggregate.sql_function == 'STDDEV_POP':\n                select[alias].sql_function = 'STDEVP'\n            # translate Variance function names\n            elif aggregate.sql_function == 'VAR_SAMP':\n                select[alias].sql_function = 'VAR'\n            elif aggregate.sql_function == 'VAR_POP':\n                select[alias].sql_function = 'VARP'"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\napply any necessary fixes to the outer_fields inner_select and order strings due to slicing.", "response": "def _fix_slicing_order(self, outer_fields, inner_select, order, inner_table_name):\n        \"\"\"\n        Apply any necessary fixes to the outer_fields, inner_select, and order\n        strings due to slicing.\n        \"\"\"\n        # Using ROW_NUMBER requires an ordering\n        if order is None:\n            meta = self.query.get_meta()\n            column = meta.pk.db_column or meta.pk.get_attname()\n            order = '{0}.{1} ASC'.format(\n                inner_table_name,\n                self.connection.ops.quote_name(column),\n            )\n        else:\n            alias_id = 0\n            # remap order for injected subselect\n            new_order = []\n            for x in order.split(','):\n                # find the ordering direction\n                m = _re_find_order_direction.search(x)\n                if m:\n                    direction = m.groups()[0]\n                else:\n                    direction = 'ASC'\n                # remove the ordering direction\n                x = _re_find_order_direction.sub('', x)\n                # remove any namespacing or table name from the column name\n                col = x.rsplit('.', 1)[-1]\n                # Is the ordering column missing from the inner select?\n                # 'inner_select' contains the full query without the leading 'SELECT '.\n                # It's possible that this can get a false hit if the ordering\n                # column is used in the WHERE while not being in the SELECT. It's\n                # not worth the complexity to properly handle that edge case.\n                if x not in inner_select:\n                    # Ordering requires the column to be selected by the inner select\n                    alias_id += 1\n                    # alias column name\n                    col = '{left_sql_quote}{0}___o{1}{right_sql_quote}'.format(\n                        col.strip(self.connection.ops.left_sql_quote+self.connection.ops.right_sql_quote),\n                        alias_id,\n                        left_sql_quote=self.connection.ops.left_sql_quote,\n                        right_sql_quote=self.connection.ops.right_sql_quote,\n                    )\n                    # add alias to inner_select\n                    inner_select = '({0}) AS {1}, {2}'.format(x, col, inner_select)\n                new_order.append('{0}.{1} {2}'.format(inner_table_name, col, direction))\n            order = ', '.join(new_order)\n        return outer_fields, inner_select, order"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _alias_columns(self, sql):\n        qn = self.connection.ops.quote_name\n\n        outer = list()\n        inner = list()\n        names_seen = list()\n\n        # replace all parens with placeholders\n        paren_depth, paren_buf = 0, ['']\n        parens, i = {}, 0\n        for ch in sql:\n            if ch == '(':\n                i += 1\n                paren_depth += 1\n                paren_buf.append('')\n            elif ch == ')':\n                paren_depth -= 1\n                key = '_placeholder_{0}'.format(i)\n                buf = paren_buf.pop()\n\n                # store the expanded paren string\n                buf = re.sub(r'%([^\\(])', r'$$$\\1', buf)\n                parens[key] = buf% parens\n                parens[key] = re.sub(r'\\$\\$\\$([^\\(])', r'%\\1', parens[key])\n                #cannot use {} because IBM's DB2 uses {} as quotes\n                paren_buf[paren_depth] += '(%(' + key + ')s)'\n            else:\n                paren_buf[paren_depth] += ch\n\n        def _replace_sub(col):\n            \"\"\"Replace all placeholders with expanded values\"\"\"\n            while _re_col_placeholder.search(col):\n                col = col.format(**parens)\n            return col\n\n        temp_sql = ''.join(paren_buf)\n\n        # replace any bare %s with placeholders.  Needed when the WHERE\n        # clause only contains one condition, and isn't wrapped in parens.\n        # the placeholder_data is used to prevent the variable \"i\" from\n        # being interpreted as a local variable in the replacement function\n        placeholder_data = { \"i\": i }\n        def _alias_placeholders(val):\n            i = placeholder_data[\"i\"]\n            i += 1\n            placeholder_data[\"i\"] = i\n            key = \"_placeholder_{0}\".format(i)\n            parens[key] = \"%s\"\n            return \"%(\" + key + \")s\"\n\n        temp_sql = re.sub(\"%s\", _alias_placeholders, temp_sql)\n\n        select_list, from_clause = _break(temp_sql, ' FROM ' + self.connection.ops.left_sql_quote)\n\n        for col in [x.strip() for x in select_list.split(',')]:\n            match = self._re_pat_col.search(col)\n            if match:\n                col_name = match.group(1)\n                col_key = col_name.lower()\n\n                if col_key in names_seen:\n                    alias = qn('{0}___{1}'.format(col_name, names_seen.count(col_key)))\n                    outer.append(alias)\n                    inner.append('{0} as {1}'.format(_replace_sub(col), alias))\n                else:\n                    outer.append(qn(col_name))\n                    inner.append(_replace_sub(col))\n\n                names_seen.append(col_key)\n            else:\n                raise Exception('Unable to find a column name when parsing SQL: {0}'.format(col))\n\n        return ', '.join(outer), ', '.join(inner) + (from_clause % parens)", "response": "Return tuple of SELECT and FROM clauses aliasing duplicate column names."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrap the passed SQL with IDENTITY_INSERT statements and apply fixes.", "response": "def _fix_insert(self, sql, params):\n        \"\"\"\n        Wrap the passed SQL with IDENTITY_INSERT statements and apply\n        other necessary fixes.\n        \"\"\"\n        meta = self.query.get_meta()\n\n        if meta.has_auto_field:\n            if hasattr(self.query, 'fields'):\n                # django 1.4 replaced columns with fields\n                fields = self.query.fields\n                auto_field = meta.auto_field\n            else:\n                # < django 1.4\n                fields = self.query.columns\n                auto_field = meta.auto_field.db_column or meta.auto_field.column\n\n            auto_in_fields = auto_field in fields\n\n            quoted_table = self.connection.ops.quote_name(meta.db_table)\n            if not fields or (auto_in_fields and len(fields) == 1 and not params):\n                # convert format when inserting only the primary key without\n                # specifying a value\n                sql = 'INSERT INTO {0} DEFAULT VALUES'.format(\n                    quoted_table\n                )\n                params = []\n            elif auto_in_fields:\n                # wrap with identity insert\n                sql = 'SET IDENTITY_INSERT {table} ON;{sql};SET IDENTITY_INSERT {table} OFF'.format(\n                    table=quoted_table,\n                    sql=sql,\n                )\n\n        # mangle SQL to return ID from insert\n        # http://msdn.microsoft.com/en-us/library/ms177564.aspx\n        if self.return_id and self.connection.features.can_return_id_from_insert:\n            col = self.connection.ops.quote_name(meta.pk.db_column or meta.pk.get_attname())\n\n            # Determine datatype for use with the table variable that will return the inserted ID\n            pk_db_type = _re_data_type_terminator.split(meta.pk.db_type(self.connection))[0]\n\n            # NOCOUNT ON to prevent additional trigger/stored proc related resultsets\n            sql = 'SET NOCOUNT ON;{declare_table_var};{sql};{select_return_id}'.format(\n                sql=sql,\n                declare_table_var=\"DECLARE @sqlserver_ado_return_id table ({col_name} {pk_type})\".format(\n                    col_name=col,\n                    pk_type=pk_db_type,\n                ),\n                select_return_id=\"SELECT * FROM @sqlserver_ado_return_id\",\n            )\n\n            output = self._values_repl.format(col=col)\n            sql = self._re_values_sub.sub(output, sql)\n\n        return sql, params"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nformat the results of a query into a tuple of tuples.", "response": "def format_results(self, rows):\n        \"\"\"\n        Decode data coming from the database if needed and convert rows to tuples\n        (pyodbc Rows are not sliceable).\n        \"\"\"\n        needs_utc = _DJANGO_VERSION >= 14 and settings.USE_TZ\n        if not (needs_utc or not self.driver_supports_utf8):\n            return tuple(rows)\n        # FreeTDS (and other ODBC drivers?) don't support Unicode yet, so we\n        # need to decode UTF-8 data coming from the DB\n        fr = []\n        for row in rows:\n            if not self.driver_supports_utf8 and isinstance(row, binary_type):\n                row = row.decode(self.encoding)\n\n            elif needs_utc and isinstance(row, datetime.datetime):\n                row = row.replace(tzinfo=timezone.utc)\n            fr.append(row)\n        return tuple(fr)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef head(line, n: int):\n    global counter\n    counter += 1\n\n    if counter > n:\n        raise cbox.Stop()  # can also raise StopIteration()\n    return line", "response": "returns the first n lines"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef stream(input_type='lines', output_type=None, worker_type='simple',\n           max_workers=1, workers_window=100):\n    \"\"\"wrapper for processing data from input stream into output into output\n    stream while passing each data piece into the function.\n    function should take at least one argument (an input stream piece) and\n    return an `str` to be written into the output stream.\n\n    Example Usage:\n\n        >>> import cbox\n        >>>\n        >>> @cbox.stream()\n        >>> def firstchar(line):\n        >>>    '''extracts the first char out of each line'''\n        >>>    return line[0] if line else ''\n\n\n    :param str input_type: defines how the input stream is split. one of\n      `lines`, `chars` or `raw`.\n    :param str output_type: defines how to write into output stream\n      (similarly to input stream). if `None`, split the output stream in the\n      same way of `input_type`. one of `None`, `lines`, `chars` or `raw`.\n    :param str worker_type: one of `simple`, `thread` or `asyncio`.\n    :param int max_workers: how many max workers (i.e. threads) to run in\n      parallel. only affect if `worker_type=thread`.\n    :param int workers_window: how many tasks to execute in parallel before\n      waiting for them to be completed. only affect if `worker_type`\n      is not simple.\n    \"\"\"\n    def inner(f):\n\n        @wraps(f)\n        def wrapper(input_stream, output_stream, error_stream, **kwargs):\n            in_parser = streams.get_input_parser(input_type)\n            out_parser = streams.get_output_parser(output_type, input_type)\n            runner = concurrency.get_runner(\n                worker_type=worker_type,\n                max_workers=max_workers,\n                workers_window=workers_window,\n            )\n            items = in_parser(input_stream)\n            output = runner(f, items, kwargs)\n            return out_parser(output_stream, error_stream, output)\n\n        setattr(wrapper, executors.EXECUTOR_ATTR, executors.STREAM)\n        return wrapper\n    return inner", "response": "wrapper for processing data from input stream into output stream"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cmd(f):\n    @wraps(f)\n    def wrapper(*args, **kwargs):\n        return f(*args, **kwargs)\n\n    setattr(wrapper, executors.EXECUTOR_ATTR, executors.CMD)\n    return wrapper", "response": "wrapper for easily exposing a function as a CLI command."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef main(func=None, argv=None, input_stream=stdin, output_stream=stdout,\n         error_stream=stderr, exit=True):\n    \"\"\"runs a function as a command.\n    runs a function as a command - reading input from `input_stream`, writing\n    output into `output_stream` and providing arguments from `argv`.\n\n    Example Usage:\n\n        >>> import cbox\n        >>>\n        >>> @cbox.cmd\n        >>> def hello(name: str):\n        >>>     print('hello {}!'.format(name))\n        >>>\n        >>> if __name__ == '__main__':\n        >>>    cbox.main(hello)\n\n    more examples on `README.md`\n\n\n    :param callable func: the function to execute, must be decorated by\n      `@cbox.cmd` or `@cbox.stream`.\n    :param list[str] argv: command arguments (default `sys.argv`)\n    :param input_stream: readable file-like object (default `stdin`)\n    :param output_stream: writable file-like object (default `stdout`)\n    :param error_stream: writable file-like object (default `stderr`)\n    :param bool exit: if True, exits (i.e. `sys.exit(exitcode)`)\n      with the `exitcode`, else returns the `exitcode`.\n      the code is 0 if no errors, else 2.\n    :return: the exit code if `exit` is False, else raises `SystemExit(code)`\n    \"\"\"\n    executor = executors.get_func_executor(func)\n    exitcode = executor(func, argv, input_stream, output_stream, error_stream)\n\n    if exit:\n        sys.exit(exitcode)\n    return exitcode", "response": "A function that runs a function as a command."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_cli_parser(func, skip_first=0, parser=None):\n    help_msg, func_args = _get_func_args(func)\n    if not parser:\n        parser = ArgumentParser(description=help_msg)\n\n    for i, arg in enumerate(func_args):\n        arg_name, arg_type, arg_default, arg_required, arg_help = arg\n        if i < skip_first:\n            continue\n\n        if arg_default is not _empty:\n            parser.add_argument(\n                arg_name, type=arg_type, default=arg_default,\n                required=arg_required, help=arg_help\n            )\n        else:\n            parser.add_argument(\n                arg_name, type=arg_type, required=arg_required, help=arg_help\n            )\n    return parser", "response": "returns a parser for parsing cli arguments for the given function"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_cli_multi_parser(funcs, skip_first=0):\n    parser = ArgumentParser(description='which subcommand do you want?')\n    subparsers = parser.add_subparsers(\n        title='subcommands', dest='subcmd', help=''\n    )\n    for func in funcs:\n        help_msg, func_args = _get_func_args(func)\n        sub_parser = subparsers.add_parser(func.__name__, help=help_msg)\n        get_cli_parser(func, skip_first=skip_first, parser=sub_parser)\n    return parser", "response": "makes a parser for parsing cli arguments for all functions"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the docstring into its help message and params", "response": "def _parse_docstring(docstring):\n    \"\"\"parses docstring into its help message and params\"\"\"\n    params = {}\n\n    if not docstring:\n        return None, params\n\n    try:\n        help_msg = _DOCSTRING_REGEX.search(docstring).group()\n        help_msg = _strip_lines(help_msg)\n    except AttributeError:\n        help_msg = None\n\n    for param in _DOCSTRING_PARAM_REGEX.finditer(docstring):\n        param_definition = param.group(1).rsplit(' ', 1)\n        if len(param_definition) == 2:\n            param_type, param_name = param_definition\n        else:\n            param_type = None\n            param_name = param_definition[0]\n\n        param_help = param.group(2).strip()\n        params[param_name] = param_type, _strip_lines(param_help)\n    return help_msg, params"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the formatted stacktrace of the exception e.", "response": "def error2str(e):\n    \"\"\"returns the formatted stacktrace of the exception `e`.\n\n    :param BaseException e: an exception to format into str\n    :rtype: str\n    \"\"\"\n    out = StringIO()\n    traceback.print_exception(None, e, e.__traceback__, file=out)\n    out.seek(0)\n    return out.read()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_runner(worker_type, max_workers=None, workers_window=None):\n    worker_func = _runners_mapping[worker_type]\n    return partial(\n        worker_func, max_workers=max_workers, workers_window=workers_window\n    )", "response": "returns a runner callable."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a function decorated by cbox. stream decorator.", "response": "def get_inline_func(inline_str, modules=None, **stream_kwargs):\n    \"\"\"returns a function decorated by `cbox.stream` decorator.\n\n    :param str inline_str: the inline function to execute,\n      can use `s` - local variable as the input line/char/raw\n      (according to `input_type` param).\n    :param str modules: comma separated list of modules to import before\n      running the inline function.\n    :param dict stream_kwargs: optional arguments to `cbox.stream` decorator\n    :rtype: callable\n    \"\"\"\n    if not _is_compilable(inline_str):\n        raise ValueError(\n            'cannot compile the inline expression - \"%s\"' % inline_str\n        )\n\n    inline_globals = _import_inline_modules(modules)\n    func = _inline2func(inline_str, inline_globals, **stream_kwargs)\n    return func"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrunning inline function - more info run cbox -- help", "response": "def main(argv=None, input_stream=stdin, output_stream=stdout,\n         error_stream=stderr):\n    \"\"\"runs inline function - more info run `cbox --help`\"\"\"\n    args = _parse_args(argv)\n    args_dict = args.__dict__.copy()\n    inline_str = args_dict.pop('inline')\n    modules = args_dict.pop('modules')\n\n    func = get_inline_func(inline_str, modules, **args_dict)\n\n    return cbox.main(\n        func=func, argv=[], input_stream=input_stream,\n        output_stream=output_stream, error_stream=error_stream, exit=False,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the mode of the .", "response": "def setmode(mode):\n    \"\"\"\n    You must call this method prior to using all other calls.\n\n    :param mode: the mode, one of :py:attr:`GPIO.BOARD`, :py:attr:`GPIO.BCM`,\n        :py:attr:`GPIO.SUNXI`, or a `dict` or `object` representing a custom\n        pin mapping.\n    \"\"\"\n    if hasattr(mode, '__getitem__'):\n        set_custom_pin_mappings(mode)\n        mode = CUSTOM\n\n    assert mode in [BCM, BOARD, SUNXI, CUSTOM]\n    global _mode\n    _mode = mode"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef input(channel):\n    _check_configured(channel)  # Can read from a pin configured for output\n    pin = get_gpio_pin(_mode, channel)\n    return sysfs.input(pin)", "response": "Read the value of a GPIO pin."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef output(channel, state):\n    if isinstance(channel, list):\n        for ch in channel:\n            output(ch, state)\n    else:\n        _check_configured(channel, direction=OUT)\n        pin = get_gpio_pin(_mode, channel)\n        return sysfs.output(pin, state)", "response": "Sets the output state of a numbering system."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef wait_for_edge(channel, trigger, timeout=-1):\n    _check_configured(channel, direction=IN)\n    pin = get_gpio_pin(_mode, channel)\n    if event.blocking_wait_for_edge(pin, trigger, timeout) is not None:\n        return channel", "response": "This function is designed to block execution of your program until an edge is detected on the specified channel."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove the event detection for the specified channel.", "response": "def remove_event_detect(channel):\n    \"\"\"\n    :param channel: the channel based on the numbering system you have specified\n        (:py:attr:`GPIO.BOARD`, :py:attr:`GPIO.BCM` or :py:attr:`GPIO.SUNXI`).\n    \"\"\"\n    _check_configured(channel, direction=IN)\n    pin = get_gpio_pin(_mode, channel)\n    event.remove_edge_detect(pin)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a callback to the event bus for the specified channel.", "response": "def add_event_callback(channel, callback, bouncetime=None):\n    \"\"\"\n    :param channel: the channel based on the numbering system you have specified\n        (:py:attr:`GPIO.BOARD`, :py:attr:`GPIO.BCM` or :py:attr:`GPIO.SUNXI`).\n    :param callback: TODO\n    :param bouncetime: (optional) TODO\n    \"\"\"\n    _check_configured(channel, direction=IN)\n\n    if bouncetime is not None:\n        if _gpio_warnings:\n            warnings.warn(\"bouncetime is not (yet) fully supported, continuing anyway. Use GPIO.setwarnings(False) to disable warnings.\", stacklevel=2)\n\n    pin = get_gpio_pin(_mode, channel)\n    event.add_edge_callback(pin, __wrap(callback, channel))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cleanup(channel=None):\n    if channel is None:\n        cleanup(list(_exports.keys()))\n        setwarnings(True)\n        global _mode\n        _mode = None\n    elif isinstance(channel, list):\n        for ch in channel:\n            cleanup(ch)\n    else:\n        _check_configured(channel)\n        pin = get_gpio_pin(_mode, channel)\n        event.cleanup(pin)\n        sysfs.unexport(pin)\n        del _exports[channel]", "response": "This function cleans up all resources associated with a specific GPIO channel."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses the README. md file and return the content of the file.", "response": "def parse_markdown_readme():\n    \"\"\"\n    Convert README.md to RST via pandoc, and load into memory\n    (fallback to LONG_DESCRIPTION on failure)\n    \"\"\"\n    # Attempt to run pandoc on markdown file\n    import subprocess\n    try:\n        subprocess.call(\n            ['pandoc', '-t', 'rst', '-o', 'README.rst', 'README.md']\n        )\n    except OSError:\n        return LONG_DESCRIPTION\n\n    # Attempt to load output\n    try:\n        readme = open(os.path.join(\n            os.path.dirname(__file__),\n            'README.rst'\n        ))\n    except IOError:\n        return LONG_DESCRIPTION\n    return readme.read()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the name to use for the given app_label and model.", "response": "def swappable_setting(app_label, model):\n    \"\"\"\n    Returns the setting name to use for the given model (i.e. AUTH_USER_MODEL)\n    \"\"\"\n    prefix = _prefixes.get(app_label, app_label)\n    setting = \"{prefix}_{model}_MODEL\".format(\n        prefix=prefix.upper(),\n        model=model.upper()\n    )\n\n    # Ensure this attribute exists to avoid migration issues in Django 1.7\n    if not hasattr(settings, setting):\n        setattr(settings, setting, join(app_label, model))\n\n    return setting"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_swapped(app_label, model):\n    default_model = join(app_label, model)\n    setting = swappable_setting(app_label, model)\n    value = getattr(settings, setting, default_model)\n    if value != default_model:\n        return value\n    else:\n        return False", "response": "Returns the value of the swapped setting."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a tuple that can be used to include a migration in a Seaborn migration.", "response": "def dependency(app_label, model):\n    \"\"\"\n    Returns a Django 1.7+ style dependency tuple for inclusion in\n    migration.dependencies[]\n    \"\"\"\n    from django.db.migrations import swappable_dependency\n    return swappable_dependency(get_model_name(app_label, model))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_model_names(app_label, models):\n    return dict(\n        (model, get_model_name(app_label, model))\n        for model in models\n    )", "response": "Map model names to their swapped equivalents for the given app_label."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_model(app_label, model, orm=None, required=True):\n    swapped = is_swapped(app_label, model)\n    if swapped:\n        app_label, model = split(swapped)\n    else:\n        if orm is not None:\n            return orm[join(app_label, model)]\n\n    try:\n        try:\n            # django >= 1.7\n            from django.apps import apps\n            cls = apps.get_model(app_label, model)\n        except ImportError:\n            # django < 1.7\n            from django.db.models import get_model\n            cls = get_model(app_label, model)\n    except LookupError:\n        # both get_model versions can raise a LookupError\n        cls = None\n\n    if cls is None and required:\n        raise ImproperlyConfigured(\n            \"Could not find {name}!\".format(name=join(app_label, model))\n        )\n    return cls", "response": "Load the specified model class or the class it was swapped out for."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update(self, entries = {}, *args, **kwargs):\n        if isinstance(entries, dict):\n            entries = self._reject_reserved_keys(entries)\n\n        for key, value in dict(entries, *args, **kwargs).items():\n            if isinstance(value, dict):\n                self.__dict__[key] = AttributeDict(value)\n            else:\n                self.__dict__[key] = value\n\n        self._refresh()", "response": "Update dictionary.\n\n        @example:\n\n            object.update({'foo': {'bar': 1}})"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse the data files from the data directory and return a dictionary of the data files as a list of dictionaries.", "response": "def read_table(filename, usecols=(0, 1), sep='\\t', comment='#', encoding='utf-8', skip=0):\n    \"\"\"Parse data files from the data directory\n\n    Parameters\n    ----------\n    filename: string\n        Full path to file\n\n    usecols: list, default [0, 1]\n        A list of two elements representing the columns to be parsed into a dictionary.\n        The first element will be used as keys and the second as values. Defaults to\n        the first two columns of `filename`.\n\n    sep : string, default '\\t'\n        Field delimiter.\n\n    comment : str, default '#'\n        Indicates remainder of line should not be parsed. If found at the beginning of a line,\n        the line will be ignored altogether. This parameter must be a single character.\n\n    encoding : string, default 'utf-8'\n        Encoding to use for UTF when reading/writing (ex. `utf-8`)\n\n    skip: int, default 0\n        Number of lines to skip at the beginning of the file\n\n    Returns\n    -------\n    A dictionary with the same length as the number of lines in `filename`\n    \"\"\"\n\n    with io.open(filename, 'r', encoding=encoding) as f:\n        # skip initial lines\n        for _ in range(skip):\n            next(f)\n\n        # filter comment lines\n        lines = (line for line in f if not line.startswith(comment))\n\n        d = dict()\n        for line in lines:\n            columns = line.split(sep)\n            key = columns[usecols[0]].lower()\n            value = columns[usecols[1]].rstrip('\\n')\n            d[key] = value\n    return d"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef build_index():\n\n    nationalities = read_table(get_data_path('nationalities.txt'), sep=':')\n\n    # parse http://download.geonames.org/export/dump/countryInfo.txt\n    countries = read_table(\n        get_data_path('countryInfo.txt'), usecols=[4, 0], skip=1)\n\n    # parse http://download.geonames.org/export/dump/cities15000.zip\n    cities = read_table(get_data_path('cities15000.txt'), usecols=[1, 8])\n\n    # load and apply city patches\n    city_patches = read_table(get_data_path('citypatches.txt'))\n    cities.update(city_patches)\n\n    Index = namedtuple('Index', 'nationalities cities countries')\n    return Index(nationalities, cities, countries)", "response": "Load information from the data directory\n    Returns ------- Index A namedtuple with three fields : nationalities cities countries"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _find_executables(name):\n    exe_name = name + '.exe' * sys.platform.startswith('win')\n    env_path = os.environ.get(name.upper()+ '_PATH', '')\n    \n    possible_locations = []\n    def add(*dirs):\n        for d in dirs:\n            if d and d not in possible_locations and os.path.isdir(d):\n                possible_locations.append(d)\n    \n    # Get list of possible locations\n    add(env_path)\n    try:\n        add(os.path.dirname(os.path.abspath(__file__)))\n    except NameError:  # __file__ may not exist\n        pass\n    add(os.path.dirname(sys.executable))\n    add(os.path.expanduser('~'))\n    \n    # Platform specific possible locations\n    if sys.platform.startswith('win'):\n        add('c:\\\\program files', os.environ.get('PROGRAMFILES'),\n            'c:\\\\program files (x86)', os.environ.get('PROGRAMFILES(x86)'))\n    else:\n        possible_locations.extend(['/usr/bin','/usr/local/bin','/opt/local/bin'])\n    \n    def do_check_version(exe):\n        try:\n            return subprocess.check_output([exe, '--version']).decode().strip()\n        except Exception:\n            # print('not a good exe', exe)\n            return False\n    \n    # If env path is the exe itself ...\n    if os.path.isfile(env_path):\n        ver = do_check_version(env_path)\n        if ver:\n            return env_path, ver\n    \n    # First try to find obvious locations\n    for d in possible_locations:\n        for exe in [os.path.join(d, exe_name), os.path.join(d, name, exe_name)]:\n            if os.path.isfile(exe):\n                ver = do_check_version(exe)\n                if ver:\n                    return exe, ver\n    \n    # Maybe the exe is on the PATH\n    ver = do_check_version(exe_name)\n    if ver:\n        return exe_name, ver\n        \n    # Try harder\n    for d in possible_locations:\n        for sub in reversed(sorted(os.listdir(d))):\n            if sub.startswith(name):\n                exe = os.path.join(d, sub, exe_name)\n                if os.path.isfile(exe):\n                    ver = do_check_version(exe)\n                    if ver:\n                        return exe, ver\n    \n    return None, None", "response": "Try to find an executable."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the executables for elastix and transformix. Raises an error if they cannot be found.", "response": "def get_elastix_exes():\n    \"\"\" Get the executables for elastix and transformix. Raises an error\n    if they cannot be found.\n    \"\"\"\n    if EXES:\n        if EXES[0]:\n            return EXES\n        else:\n            raise RuntimeError('No Elastix executable.')\n    \n    # Find exe\n    elastix, ver = _find_executables('elastix')\n    if elastix:\n        base, ext = os.path.splitext(elastix)\n        base = os.path.dirname(base)\n        transformix = os.path.join(base, 'transformix' + ext)\n        EXES.extend([elastix, transformix])\n        print('Found %s in %r' % (ver, elastix))\n        return EXES\n    else:\n        raise RuntimeError('Could not find Elastix executable. Download '\n                           'Elastix from http://elastix.isi.uu.nl/. Pyelastix '\n                           'looks for the exe in a series of common locations. '\n                           'Set ELASTIX_PATH if necessary.')"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves a directory and its contents. Ignore any failures.", "response": "def _clear_dir(dirName):\n    \"\"\" Remove a directory and it contents. Ignore any failures.\n    \"\"\"\n    # If we got here, clear dir  \n    for fname in os.listdir(dirName):\n        try:\n            os.remove( os.path.join(dirName, fname) )\n        except Exception:\n            pass\n    try:\n        os.rmdir(dirName)\n    except Exception:\n        pass"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_tempdir():\n    tempdir = os.path.join(tempfile.gettempdir(), 'pyelastix')\n    \n    # Make sure it exists\n    if not os.path.isdir(tempdir):\n        os.makedirs(tempdir)\n    \n    # Clean up all directories for which the process no longer exists\n    for fname in os.listdir(tempdir):\n        dirName = os.path.join(tempdir, fname)\n        # Check if is right kind of dir\n        if not (os.path.isdir(dirName) and  fname.startswith('id_')):\n            continue\n        # Get pid and check if its running\n        try:\n            pid = int(fname.split('_')[1])\n        except Exception:\n            continue\n        if not _is_pid_running(pid):\n            _clear_dir(dirName)\n    \n    # Select dir that included process and thread id\n    tid = id(threading.current_thread() if hasattr(threading, 'current_thread')\n                                        else threading.currentThread())\n    dir = os.path.join(tempdir, 'id_%i_%i' % (os.getpid(), tid))\n    if not os.path.isdir(dir):\n        os.mkdir(dir)\n    return dir", "response": "Get the temporary directory where pyelastix stores its temporary\n    files."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nclearing the temporary directory.", "response": "def _clear_temp_dir():\n    \"\"\" Clear the temporary directory.\n    \"\"\"\n    tempdir = get_tempdir()\n    for fname in os.listdir(tempdir):\n        try:\n            os.remove( os.path.join(tempdir, fname) )\n        except Exception:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_image_paths(im1, im2):\n    \n    paths = []\n    for im in [im1, im2]:\n        if im is None:\n            # Groupwise registration: only one image (ndim+1 dimensions)\n            paths.append(paths[0])\n            continue\n        \n        if isinstance(im, str):\n            # Given a location\n            if os.path.isfile(im1):\n                paths.append(im)\n            else:\n                raise ValueError('Image location does not exist.')\n        \n        elif isinstance(im, np.ndarray):\n            # Given a numpy array\n            id = len(paths)+1\n            p = _write_image_data(im, id)\n            paths.append(p)\n        \n        else:\n            # Given something else ...\n            raise ValueError('Invalid input image.')\n    \n    # Done\n    return tuple(paths)", "response": "Returns the paths to the files in the order they appear in the image tree."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nexecuting a command in a subprocess and wait for it to finish.", "response": "def _system3(cmd, verbose=False):\n    \"\"\" Execute the given command in a subprocess and wait for it to finish.\n    A thread is run that prints output of the process if verbose is True.\n    \"\"\"\n    \n    # Init flag\n    interrupted = False\n    \n    # Create progress\n    if verbose > 0:\n        progress = Progress()\n    \n    stdout = []\n    def poll_process(p):\n        while not interrupted:\n            msg = p.stdout.readline().decode()\n            if msg:\n                stdout.append(msg)\n                if 'error' in msg.lower():\n                    print(msg.rstrip())\n                    if verbose == 1:\n                        progress.reset()\n                elif verbose > 1:\n                    print(msg.rstrip())\n                elif verbose == 1:\n                    progress.update(msg)\n            else:\n                break\n            time.sleep(0.01)\n        #print(\"thread exit\")\n    \n    # Start process that runs the command\n    p = subprocess.Popen(cmd, shell=True, \n                         stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n    \n    # Keep reading stdout from it\n    # thread.start_new_thread(poll_process, (p,))  Python 2.x\n    my_thread = threading.Thread(target=poll_process, args=(p,))\n    my_thread.setDaemon(True)\n    my_thread.start()\n    \n    # Wait here\n    try:\n        while p.poll() is None:\n            time.sleep(0.01)\n    except KeyboardInterrupt:\n        # Set flag\n        interrupted = True\n        # Kill subprocess\n        pid = p.pid\n        if hasattr(os,'kill'):\n            import signal\n            os.kill(pid, signal.SIGKILL)\n        elif sys.platform.startswith('win'):\n            kernel32 = ctypes.windll.kernel32\n            handle = kernel32.OpenProcess(1, 0, pid)\n            kernel32.TerminateProcess(handle, 0)\n            #os.system(\"TASKKILL /PID \" + str(pid) + \" /F\")\n    \n    # All good?\n    if interrupted:\n        raise RuntimeError('Registration process interrupted by the user.')\n    if p.returncode:\n        stdout.append(p.stdout.read().decode())\n        print(''.join(stdout))\n        raise RuntimeError('An error occured during the registration.')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_dtype_maps():\n    \n    # Define pairs\n    tmp = [ (np.float32, 'MET_FLOAT'),  (np.float64, 'MET_DOUBLE'),\n            (np.uint8, 'MET_UCHAR'),    (np.int8, 'MET_CHAR'),\n            (np.uint16, 'MET_USHORT'),  (np.int16, 'MET_SHORT'),\n            (np.uint32, 'MET_UINT'),    (np.int32, 'MET_INT'),\n            (np.uint64, 'MET_ULONG'),   (np.int64, 'MET_LONG') ]\n    \n    # Create dictionaries\n    map1, map2 = {}, {}\n    for np_type, itk_type in tmp:\n        map1[np_type.__name__] = itk_type\n        map2[itk_type] = np_type.__name__\n    \n    # Done\n    return map1, map2", "response": "Get dictionaries to map numpy data types to ITK types and the \n    other way around."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _write_image_data(im, id):\n    im = im* (1.0/3000)\n    # Create text\n    lines = [   \"ObjectType = Image\",\n                \"NDims = <ndim>\",\n                \"BinaryData = True\",\n                \"BinaryDataByteOrderMSB = False\",\n                \"CompressedData = False\",\n                #\"TransformMatrix = <transmatrix>\",\n                \"Offset = <origin>\",\n                \"CenterOfRotation = <centrot>\",\n                \"ElementSpacing = <sampling>\",\n                \"DimSize = <shape>\",\n                \"ElementType = <dtype>\",\n                \"ElementDataFile = <fname>\",\n                \"\" ]\n    text = '\\n'.join(lines)\n    \n    # Determine file names\n    tempdir = get_tempdir()\n    fname_raw_ = 'im%i.raw' % id\n    fname_raw = os.path.join(tempdir, fname_raw_)\n    fname_mhd = os.path.join(tempdir, 'im%i.mhd' % id)\n    \n    # Get shape, sampling and origin\n    shape = im.shape\n    if hasattr(im, 'sampling'): sampling = im.sampling\n    else: sampling = [1 for s in im.shape]\n    if hasattr(im, 'origin'): origin = im.origin\n    else: origin = [0 for s in im.shape]\n    \n    # Make all shape stuff in x-y-z order and make it string\n    shape = ' '.join([str(s) for s in reversed(shape)])\n    sampling = ' '.join([str(s) for s in reversed(sampling)])\n    origin = ' '.join([str(s) for s in reversed(origin)])\n    \n    # Get data type\n    dtype_itk = DTYPE_NP2ITK.get(im.dtype.name, None)\n    if dtype_itk is None:\n        raise ValueError('Cannot convert data of this type: '+ str(im.dtype))\n    \n    # Set mhd text\n    text = text.replace('<fname>', fname_raw_)\n    text = text.replace('<ndim>', str(im.ndim))\n    text = text.replace('<shape>', shape)\n    text = text.replace('<sampling>', sampling)\n    text = text.replace('<origin>', origin)\n    text = text.replace('<dtype>', dtype_itk)\n    text = text.replace('<centrot>', ' '.join(['0' for s in im.shape]))\n    if im.ndim==2:\n        text = text.replace('<transmatrix>', '1 0 0 1')\n    elif im.ndim==3:\n        text = text.replace('<transmatrix>', '1 0 0 0 1 0 0 0 1')\n    elif im.ndim==4:\n        pass # ???\n    \n    # Write data file\n    f = open(fname_raw, 'wb')\n    try:\n        f.write(im.data)\n    finally:\n        f.close()\n    \n    # Write mhd file\n    f = open(fname_mhd, 'wb')\n    try:\n        f.write(text.encode('utf-8'))\n    finally:\n        f.close()\n    \n    # Done, return path of mhd file\n    return fname_mhd", "response": "Write a numpy array to disk in the form of a. raw and. mhd file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _read_image_data( mhd_file):\n    tempdir = get_tempdir()\n    \n    # Load description from mhd file\n    fname = tempdir + '/' + mhd_file\n    des = open(fname, 'r').read()\n    \n    # Get data filename and load raw data\n    match = re.findall('ElementDataFile = (.+?)\\n', des)\n    fname = tempdir + '/' + match[0]\n    data = open(fname, 'rb').read()\n    \n    # Determine dtype\n    match = re.findall('ElementType = (.+?)\\n', des)\n    dtype_itk = match[0].upper().strip()\n    dtype = DTYPE_ITK2NP.get(dtype_itk, None)\n    if dtype is None:\n        raise RuntimeError('Unknown ElementType: ' + dtype_itk)\n    \n    # Create numpy array\n    a = np.frombuffer(data, dtype=dtype)\n    \n    # Determine shape, sampling and origin of the data\n    match = re.findall('DimSize = (.+?)\\n', des)\n    shape = [int(i) for i in match[0].split(' ')]\n    #\n    match = re.findall('ElementSpacing = (.+?)\\n', des)\n    sampling = [float(i) for i in match[0].split(' ')]\n    #\n    match = re.findall('Offset = (.+?)\\n', des)\n    origin = [float(i) for i in match[0].split(' ')]\n    \n    # Reverse shape stuff to make z-y-x order\n    shape = [s for s in reversed(shape)]\n    sampling = [s for s in reversed(sampling)]\n    origin = [s for s in reversed(origin)]\n    \n    # Take vectors/colours into account\n    N = np.prod(shape)\n    if N != a.size:\n        extraDim = int( a.size / N )\n        shape = tuple(shape) + (extraDim,)\n        sampling = tuple(sampling) + (1.0,)\n        origin = tuple(origin) + (0,)\n    \n    # Check shape\n    N = np.prod(shape)\n    if N != a.size:\n        raise RuntimeError('Cannot apply shape to data.')\n    else:\n        a.shape = shape\n        a = Image(a)\n        a.sampling = sampling\n        a.origin = origin\n    return a", "response": "Read the resulting image data and return it as a numpy array."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_fixed_params(im):\n    \n    p = Parameters()\n    \n    if not isinstance(im, np.ndarray):\n        return p\n    \n    # Dimension of the inputs\n    p.FixedImageDimension = im.ndim\n    p.MovingImageDimension = im.ndim\n    \n    # Always write result, so I can verify\n    p.WriteResultImage = True\n    \n    # How to write the result\n    tmp = DTYPE_NP2ITK[im.dtype.name]\n    p.ResultImagePixelType = tmp.split('_')[-1].lower()\n    p.ResultImageFormat = \"mhd\"\n    \n    # Done\n    return p", "response": "Returns a Parameters object that is used to set the fixed parameters for the user."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a Parameters struct with parameters that most users do not want to think about.", "response": "def get_advanced_params():\n    \"\"\" Get `Parameters` struct with parameters that most users do not\n    want to think about.\n    \"\"\"\n    \n    p = Parameters()\n    \n    # Internal format used during the registration process\n    p.FixedInternalImagePixelType = \"float\"\n    p.MovingInternalImagePixelType = \"float\"\n    \n    # Image direction\n    p.UseDirectionCosines = True\n    \n    # In almost all cases you'd want multi resolution\n    p.Registration = 'MultiResolutionRegistration'\n    \n    # Pyramid options\n    # *RecursiveImagePyramid downsamples the images\n    # *SmoothingImagePyramid does not downsample\n    p.FixedImagePyramid = \"FixedRecursiveImagePyramid\"\n    p.MovingImagePyramid = \"MovingRecursiveImagePyramid\"\n    \n    # Whether transforms are combined by composition or by addition.\n    # It does not influence the results very much.\n    p.HowToCombineTransforms = \"Compose\"\n    \n    # For out of range pixels\n    p.DefaultPixelValue = 0\n    \n    # Interpolator used during interpolation and its order\n    # 1 means linear interpolation, 3 means cubic.\n    p.Interpolator = \"BSplineInterpolator\"\n    p.BSplineInterpolationOrder = 1\n    \n    # Interpolator used during interpolation of final level, and its order\n    p.ResampleInterpolator = \"FinalBSplineInterpolator\"\n    p.FinalBSplineInterpolationOrder = 3\n    \n    # According to the manual, there is currently only one resampler\n    p.Resampler = \"DefaultResampler\"\n    \n    # Done\n    return p"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting default parameters for a given type of allowed transform and can be used for the new parameters.", "response": "def get_default_params(type='BSPLINE'):\n    \"\"\" get_default_params(type='BSPLINE')\n    \n    Get `Parameters` struct with parameters that users may want to tweak.\n    The given `type` specifies the type of allowed transform, and can\n    be 'RIGID', 'AFFINE', 'BSPLINE'.\n    \n    For detail on what parameters are available and how they should be used,\n    we refer to the Elastix documentation. Here is a description of the\n    most common parameters:\n    \n    * Transform (str):\n        Can be 'BSplineTransform', 'EulerTransform', or\n        'AffineTransform'. The transformation to apply. Chosen based on `type`.\n    * FinalGridSpacingInPhysicalUnits (int):\n        When using the BSplineTransform, the final spacing of the grid.\n        This controls the smoothness of the final deformation.\n    * AutomaticScalesEstimation (bool):\n        When using a rigid or affine transform. Scales the affine matrix\n        elements compared to the translations, to make sure they are in\n        the same range. In general, it's best to use automatic scales\n        estimation.\n    * AutomaticTransformInitialization (bool):\n        When using a rigid or affine transform. Automatically guess an\n        initial translation by aligning the geometric centers of the \n        fixed and moving.\n    * NumberOfResolutions (int):\n        Most registration algorithms adopt a multiresolution approach\n        to direct the solution towards a global optimum and to speed\n        up the process. This parameter specifies the number of scales\n        to apply the registration at. (default 4)\n    * MaximumNumberOfIterations (int):\n        Maximum number of iterations in each resolution level.\n        200-2000 works usually fine for nonrigid registration.\n        The more, the better, but the longer computation time.\n        This is an important parameter! (default 500).\n    \"\"\"\n    \n    # Init\n    p = Parameters()\n    type = type.upper()\n    \n    \n    # ===== Metric to use =====\n    p.Metric = 'AdvancedMattesMutualInformation'\n    \n    # Number of grey level bins in each resolution level,\n    # for the mutual information. 16 or 32 usually works fine.\n    # sets default value for NumberOf[Fixed/Moving]HistogramBins\n    p.NumberOfHistogramBins = 32\n    \n    # Taking samples for mutual information\n    p.ImageSampler = 'RandomCoordinate'\n    p.NumberOfSpatialSamples = 2048\n    p.NewSamplesEveryIteration = True\n    \n    \n    # ====== Transform to use ======\n    \n    # The number of levels in the image pyramid\n    p.NumberOfResolutions = 4\n    \n    if type in ['B', 'BSPLINE', 'B-SPLINE']:\n        \n        # Bspline transform\n        p.Transform = 'BSplineTransform'\n        \n        # The final grid spacing (at the smallest level)\n        p.FinalGridSpacingInPhysicalUnits = 16\n    \n    if type in ['RIGID', 'EULER', 'AFFINE']:\n        \n        # Affine or Euler transform\n        if type in ['RIGID', 'EULER']:\n            p.Transform = 'EulerTransform'\n        else:\n            p.Transform = 'AffineTransform'\n        \n        # Scales the affine matrix elements compared to the translations, \n        # to make sure they are in the same range. In general, it's best to\n        # use automatic scales estimation.\n        p.AutomaticScalesEstimation = True\n        \n        # Automatically guess an initial translation by aligning the\n        # geometric centers of the fixed and moving.\n        p.AutomaticTransformInitialization = True\n    \n    \n    # ===== Optimizer to use =====\n    p.Optimizer = 'AdaptiveStochasticGradientDescent'\n    \n    # Maximum number of iterations in each resolution level:\n    # 200-2000 works usually fine for nonrigid registration.\n    # The more, the better, but the longer computation time.\n    # This is an important parameter!\n    p.MaximumNumberOfIterations = 500\n    \n    # The step size of the optimizer, in mm. By default the voxel size is used.\n    # which usually works well. In case of unusual high-resolution images\n    # (eg histology) it is necessary to increase this value a bit, to the size\n    # of the \"smallest visible structure\" in the image:\n    #p.MaximumStepLength = 1.0 Default uses voxel spaceing\n    \n    # Another optional parameter for the AdaptiveStochasticGradientDescent\n    #p.SigmoidInitialTime = 4.0\n    \n    \n    # ===== Also interesting parameters =====\n    \n    #p.FinalGridSpacingInVoxels = 16\n    #p.GridSpacingSchedule = [4.0, 4.0, 2.0, 1.0]\n    #p.ImagePyramidSchedule = [8 8  4 4  2 2  1 1]\n    #p.ErodeMask = \"false\"\n    \n    # Done\n    return p"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _compile_params(params, im1):\n    \n    # Compile parameters\n    p = _get_fixed_params(im1) + get_advanced_params()\n    p = p + params\n    params = p.as_dict()\n    \n    # Check parameter dimensions\n    if isinstance(im1, np.ndarray):\n        lt = (list, tuple)\n        for key in [    'FinalGridSpacingInPhysicalUnits',\n                        'FinalGridSpacingInVoxels' ]:\n            if key in params.keys() and not isinstance(params[key], lt):\n                params[key] = [params[key]] * im1.ndim\n    \n    # Check parameter removal\n    if 'FinalGridSpacingInVoxels' in params:\n        if 'FinalGridSpacingInPhysicalUnits' in params:\n            params.pop('FinalGridSpacingInPhysicalUnits')\n    \n    # Done\n    return params", "response": "Compile the params dictionary from the source image and the target image."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrite the parameter file in the format that elaxtix likes.", "response": "def _write_parameter_file(params):\n    \"\"\" Write the parameter file in the format that elaxtix likes.\n    \"\"\"\n    \n    # Get path\n    path = os.path.join(get_tempdir(), 'params.txt')\n    \n    # Define helper function\n    def valToStr(val):\n        if val in [True, False]:\n            return '\"%s\"' % str(val).lower()\n        elif isinstance(val, int):\n            return str(val)\n        elif isinstance(val, float):\n            tmp = str(val)\n            if not '.' in tmp:\n                tmp += '.0'\n            return tmp\n        elif isinstance(val, str):\n            return '\"%s\"' % val\n    \n    # Compile text\n    text = ''\n    for key in params:\n        val = params[key]\n        # Make a string of the values\n        if isinstance(val, (list, tuple)):\n            vals = [valToStr(v) for v in val]\n            val_ = ' '.join(vals)\n        else:\n            val_ = valToStr(val)\n        # Create line and add\n        line = '(%s %s)' % (key, val_)\n        text += line + '\\n'\n    \n    # Write text\n    f = open(path, 'wb')\n    try:\n        f.write(text.encode('utf-8'))\n    finally:\n        f.close()\n    \n    # Done\n    return path"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _highlight(html):\n\n    formatter = pygments.formatters.HtmlFormatter(nowrap=True)\n\n    code_expr = re.compile(\n        r'<pre><code class=\"language-(?P<lang>.+?)\">(?P<code>.+?)'\n        r'</code></pre>', re.DOTALL)\n\n    def replacer(match):\n        try:\n            lang = match.group('lang')\n            lang = _LANG_ALIASES.get(lang, lang)\n            lexer = pygments.lexers.get_lexer_by_name(lang)\n        except ValueError:\n            lexer = pygments.lexers.TextLexer()\n\n        code = match.group('code')\n\n        # Decode html entities in the code. cmark tries to be helpful and\n        # translate '\"' to '&quot;', but it confuses pygments. Pygments will\n        # escape any html entities when re-writing the code, and we run\n        # everything through bleach after.\n        code = html_parser.HTMLParser().unescape(code)\n\n        highlighted = pygments.highlight(code, lexer, formatter)\n\n        return '<pre>{}</pre>'.format(highlighted)\n\n    result = code_expr.sub(replacer, html)\n\n    return result", "response": "Syntax - highlights HTML - rendered Markdown."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check_restructuredtext(self):\n        # Warn that this command is deprecated\n        # Don't use self.warn() because it will cause the check to fail.\n        Command.warn(\n            self,\n            \"This command has been deprecated. Use `twine check` instead: \"\n            \"https://packaging.python.org/guides/making-a-pypi-friendly-readme\"\n            \"#validating-restructuredtext-markup\"\n        )\n\n        data = self.distribution.get_long_description()\n        content_type = getattr(\n            self.distribution.metadata, 'long_description_content_type', None)\n\n        if content_type:\n            content_type, _ = cgi.parse_header(content_type)\n            if content_type != 'text/x-rst':\n                self.warn(\n                    \"Not checking long description content type '%s', this \"\n                    \"command only checks 'text/x-rst'.\" % content_type)\n                return\n\n        # None or empty string should both trigger this branch.\n        if not data or data == 'UNKNOWN':\n            self.warn(\n                \"The project's long_description is either missing or empty.\")\n            return\n\n        stream = _WarningStream()\n        markup = render(data, stream=stream)\n\n        if markup is None:\n            self.warn(\n                \"The project's long_description has invalid markup which will \"\n                \"not be rendered on PyPI. The following syntax errors were \"\n                \"detected:\\n%s\" % stream)\n            return\n\n        self.announce(\n            \"The project's long description is valid RST.\",\n            level=distutils.log.INFO)", "response": "Checks if the long string fields are reST - compliant."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fetch_gist(gist_id, filename=None):\n    import requests\n\n    url = gist_url(gist_id, filename)\n    response = requests.get(url)\n\n    if response.status_code != 200:\n        raise Exception('Got a bad status looking up gist.')\n    body = response.text\n    if not body:\n        raise Exception('Unable to get the gist contents.')\n\n    return body", "response": "Fetch a gist and return the contents as a string."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef setup_gist(pelican):\n    pelican.settings.setdefault('GIST_CACHE_ENABLED', True)\n    pelican.settings.setdefault('GIST_CACHE_LOCATION',\n                                '/tmp/gist-cache')\n    pelican.settings.setdefault('GIST_PYGMENTS_STYLE', 'default')\n    pelican.settings.setdefault('GIST_PYGMENTS_LINENUM', False)\n\n    # Make sure the gist cache directory exists\n    cache_base = pelican.settings.get('GIST_CACHE_LOCATION')\n    if not os.path.exists(cache_base):\n        os.makedirs(cache_base)", "response": "Setup the gist cache."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrender a piece of code into HTML.", "response": "def render_code(code, filetype, pygments_style):\n    \"\"\"Renders a piece of code into HTML. Highlights syntax if filetype is specfied\"\"\"\n    if filetype:\n        lexer = pygments.lexers.get_lexer_by_name(filetype)\n        formatter = pygments.formatters.HtmlFormatter(style=pygments_style)\n        return pygments.highlight(code, lexer, formatter)\n    else:\n        return \"<pre><code>{}</code></pre>\".format(code)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef replace_gist_tags(generator):\n    from jinja2 import Template\n    template = Template(gist_template)\n\n    should_cache = generator.context.get('GIST_CACHE_ENABLED')\n    cache_location = generator.context.get('GIST_CACHE_LOCATION')\n    pygments_style = generator.context.get('GIST_PYGMENTS_STYLE')\n    \n    body = None\n\n    for article in generator.articles:\n        for match in gist_regex.findall(article._content):\n            gist_id = match[1]\n            filename = None\n            filetype = None\n            if match[3]:\n                filename = match[3]\n            if match[5]:\n                filetype = match[5]\n            logger.info('[gist]: Found gist id {} with filename {} and filetype {}'.format(\n                gist_id,\n                filename,\n                filetype,\n            ))\n\n            if should_cache:\n                body = get_cache(cache_location, gist_id, filename)\n\n            # Fetch the gist\n            if not body:\n                logger.info('[gist]: Gist did not exist in cache, fetching...')\n                body = fetch_gist(gist_id, filename)\n\n                if should_cache:\n                    logger.info('[gist]: Saving gist to cache...')\n                    set_cache(cache_location, gist_id, body, filename)\n            else:\n                logger.info('[gist]: Found gist in cache.')\n\n            # Create a context to render with\n            context = generator.context.copy()\n            context.update({\n                'script_url': script_url(gist_id, filename),\n                'code': render_code(body, filetype, pygments_style)\n            })\n\n            # Render the template\n            replacement = template.render(context)\n\n            article._content = article._content.replace(match[0], replacement)", "response": "Replace gist tags in the article content."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef delete_object(self, id):\n        # x=self.request(id, post_args={\"method\": \"delete\"})\n        params = urllib.parse.urlencode({\"method\": \"delete\", 'access_token': str(id)})\n        u = requests.get(\"https://graph.facebook.com/\" + str(id) + \"?\" + params)\n        groups = u.json()\n        return groups", "response": "Deletes the object with the given ID from the graph."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fql(self, query, args=None, post_args=None):\n        args = args or {}\n        if self.access_token:\n            if post_args is not None:\n                post_args[\"access_token\"] = self.access_token\n            else:\n                args[\"access_token\"] = self.access_token\n        post_data = None if post_args is None else urllib.urlencode(post_args)\n\n        \"\"\"Check if query is a dict and\n           use the multiquery method\n           else use single query\n        \"\"\"\n        if not isinstance(query, basestring):\n            args[\"queries\"] = query\n            fql_method = 'fql.multiquery'\n        else:\n            args[\"query\"] = query\n            fql_method = 'fql.query'\n\n        args[\"format\"] = \"json\"\n\n        try:\n            file = urllib2.urlopen(\"https://api.facebook.com/method/\" +\n                                   fql_method + \"?\" + urllib.urlencode(args),\n                                   post_data, timeout=self.timeout)\n        except TypeError:\n            # Timeout support for Python <2.6\n            if self.timeout:\n                socket.setdefaulttimeout(self.timeout)\n            file = urllib2.urlopen(\"https://api.facebook.com/method/\" +\n                                   fql_method + \"?\" + urllib.urlencode(args),\n                                   post_data)\n\n        try:\n            content = file.read()\n            response = _parse_json(content)\n            #Return a list if success, return a dictionary if failed\n            if type(response) is dict and \"error_code\" in response:\n                raise GraphAPIError(response)\n        except (Exception, e):\n            raise e\n        finally:\n            file.close()\n\n        return response", "response": "FQL query.\n\n        Example query: \"SELECT affiliations FROM user WHERE uid = me()\""}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef extend_access_token(self, app_id, app_secret):\n        args = {\n            \"client_id\": app_id,\n            \"client_secret\": app_secret,\n            \"grant_type\": \"fb_exchange_token\",\n            \"fb_exchange_token\": self.access_token,\n        }\n        response = urllib2.urlopen(\"https://graph.facebook.com/oauth/\"\n                                   \"access_token?\" +\n                                   urllib.parse.urlencode(args)).read().decode('utf-8')\n        query_str = parse_qs(response)\n        if \"access_token\" in query_str:\n            result = {\"accesstoken\": query_str[\"access_token\"][0]}\n            if \"expires\" in query_str:\n                result[\"expire\"] = query_str[\"expires\"][0]\n            return result\n        else:\n            response = json.loads(response)\n            raise GraphAPIError(response)", "response": "Extend the expiration time of a valid OAuth access token."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _constraint_lb_and_ub_to_gurobi_sense_rhs_and_range_value(lb, ub):\n    if lb is None and ub is None:\n        raise Exception(\"Free constraint ...\")\n    elif lb is None:\n        sense = '<'\n        rhs = float(ub)\n        range_value = 0.\n    elif ub is None:\n        sense = '>'\n        rhs = float(lb)\n        range_value = 0.\n    elif lb == ub:\n        sense = '='\n        rhs = float(lb)\n        range_value = 0.\n    elif lb > ub:\n        raise ValueError(\"Lower bound is larger than upper bound.\")\n    else:\n        sense = '='\n        rhs = float(lb)\n        range_value = float(ub - lb)\n    return sense, rhs, range_value", "response": "Helper function used by Constraint and Model to convert from lower and upper bounds to gurobi sense rhs and range value."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _evolve_kwargs(self):\n        valid_evolve_kwargs = (\n            'max_generations', 'max_evaluations', 'pop_size', 'neighborhood_size', 'tournament_size', 'mutation_rate')\n        filtered_evolve_kwargs = dict()\n        for key in valid_evolve_kwargs:\n            attr_value = getattr(self, key)\n            if attr_value is not None:\n                filtered_evolve_kwargs[key] = attr_value\n        # return filtered_evolve_kwargs\n        return {}", "response": "Filter None keyword arguments. Intended to be passed on to algorithm. evolve(...)\""}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_optimization_expression(obj, linear=True, quadratic=False, expression=None, **kwargs):\n    if expression is None:\n        expression = obj.expression\n\n    if not (linear or quadratic):\n        if obj.is_Linear:\n            linear = True\n        elif obj.is_Quadratic:\n            quadratic = True\n        else:\n            raise ValueError(\"Expression is not linear or quadratic. Other expressions are not currently supported.\")\n\n    assert linear or quadratic\n\n    if quadratic:\n        offset, linear_coefficients, quadratic_coefficients = _parse_quadratic_expression(expression, **kwargs)\n    else:\n        offset, linear_coefficients = _parse_linear_expression(expression, **kwargs)\n        quadratic_coefficients = {}\n\n    return offset, linear_coefficients, quadratic_coefficients", "response": "Function for parsing an optimization expression of a constraint or objective object."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses the coefficients of a linear expression.", "response": "def _parse_linear_expression(expression, expanded=False, **kwargs):\n    \"\"\"\n    Parse the coefficients of a linear expression (linearity is assumed).\n\n    Returns a dictionary of variable: coefficient pairs.\n    \"\"\"\n    offset = 0\n    constant = None\n\n    if expression.is_Add:\n        coefficients = expression.as_coefficients_dict()\n    elif expression.is_Mul:\n        coefficients = {expression.args[1]: expression.args[0]}\n    elif expression.is_Symbol:\n        coefficients = {expression: 1}\n    elif expression.is_Number:\n        coefficients = {}\n    else:\n        raise ValueError(\"Expression {} seems to be invalid\".format(expression))\n\n    for var in coefficients:\n        if not (var.is_Symbol):\n            if var == one:\n                constant = var\n                offset = float(coefficients[var])\n            elif expanded:\n                raise ValueError(\"Expression {} seems to be invalid\".format(expression))\n            else:\n                coefficients = _parse_linear_expression(expression, expanded=True, **kwargs)\n    if constant is not None:\n        del coefficients[constant]\n    return offset, coefficients"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing a quadratic expression.", "response": "def _parse_quadratic_expression(expression, expanded=False):\n    \"\"\"\n    Parse a quadratic expression. It is assumed that the expression is known to be quadratic or linear.\n\n    The 'expanded' parameter tells whether the expression has already been expanded. If it hasn't the parsing\n    might fail and will expand the expression and try again.\n    \"\"\"\n    linear_coefficients = {}\n    quadratic_coefficients = {}\n    offset = 0\n\n    if expression.is_Number:  # Constant expression, no coefficients\n        return float(expression), linear_coefficients, quadratic_coefficients\n\n    if expression.is_Mul:\n        terms = (expression,)\n    elif expression.is_Add:\n        terms = expression.args\n    else:\n        raise ValueError(\"Expression of type {} could not be parsed.\".format(type(expression)))\n\n    try:\n        for term in terms:\n            if term.is_Number:\n                offset += float(term)\n                continue\n            if term.is_Pow:\n                term = 1.0 * term\n            assert term.is_Mul, \"What is this? {}\".format(type(term))\n            factors = term.args\n            coef = factors[0]\n            vars = factors[1:]\n            assert len(vars) <= 2, \"This should not happen. Is this expression quadratic?\"\n            if len(vars) == 2:\n                key = frozenset(vars)\n                quadratic_coefficients[key] = quadratic_coefficients.get(key, 0) + coef\n            else:\n                var = vars[0]\n                if var.is_Symbol:\n                    linear_coefficients[var] = linear_coefficients.get(var, 0) + coef\n                elif var.is_Pow:\n                    var, exponent = var.args\n                    if exponent != 2:\n                        raise ValueError(\"The expression is not quadratic\")\n                    key = frozenset((var,))\n                    quadratic_coefficients[key] = quadratic_coefficients.get(key, 0) + coef\n        if quadratic_coefficients:\n            assert all(var.is_Symbol for var in frozenset.union(*quadratic_coefficients))  # Raise an exception to trigger expand\n        if linear_coefficients:\n            assert all(var.is_Symbol for var in linear_coefficients)\n    except Exception as e:\n        if expanded:\n            raise e\n        else:\n            # Try to expand the expression and parse it again\n            return _parse_quadratic_expression(expression.expand(), expanded=True)\n\n    return offset, linear_coefficients, quadratic_coefficients"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef clone(cls, variable, **kwargs):\n        return cls(variable.name, lb=variable.lb, ub=variable.ub, type=variable.type, **kwargs)", "response": "Make a copy of another variable."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchanging the lower and upper bounds of a variable.", "response": "def set_bounds(self, lb, ub):\n        \"\"\"\n        Change the lower and upper bounds of a variable.\n        \"\"\"\n        if lb is not None and ub is not None and lb > ub:\n            raise ValueError(\n                \"The provided lower bound {} is larger than the provided upper bound {}\".format(lb, ub)\n            )\n        self._lb = lb\n        self._ub = ub\n        if self.problem is not None:\n            self.problem._pending_modifications.var_lb.append((self, lb))\n            self.problem._pending_modifications.var_ub.append((self, ub))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef to_json(self):\n        json_obj = {\n            \"name\": self.name,\n            \"lb\": self.lb,\n            \"ub\": self.ub,\n            \"type\": self.type\n        }\n        return json_obj", "response": "Returns a json - compatible object from the Variable that can be saved using the json module."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconstructing a Variable from the provided json - object.", "response": "def from_json(cls, json_obj):\n        \"\"\"\n        Constructs a Variable from the provided json-object.\n\n        Example\n        --------\n        >>> import json\n        >>> with open(\"path_to_file.json\") as infile:\n        >>>     var = Variable.from_json(json.load(infile))\n        \"\"\"\n        return cls(json_obj[\"name\"], lb=json_obj[\"lb\"], ub=json_obj[\"ub\"], type=json_obj[\"type\"])"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsubstitutes variables in the optimization expression with variables of the appropriate interface type.", "response": "def _substitute_variables(cls, expression, model=None, **kwargs):\n        \"\"\"Substitutes variables in (optimization)expression (constraint/objective) with variables of the appropriate interface type.\n        Attributes\n        ----------\n        expression: Constraint, Objective\n            An optimization expression.\n        model: Model or None, optional\n            A reference to an optimization model that should be searched for appropriate variables first.\n        \"\"\"\n        interface = sys.modules[cls.__module__]\n        variable_substitutions = dict()\n        for variable in expression.variables:\n            if model is not None and variable.name in model.variables:\n                # print(variable.name, id(variable.problem))\n                variable_substitutions[variable] = model.variables[variable.name]\n            else:\n                variable_substitutions[variable] = interface.Variable.clone(variable)\n        adjusted_expression = expression.expression.xreplace(variable_substitutions)\n        return adjusted_expression"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns True if expression is linear.", "response": "def is_Linear(self):\n        \"\"\"Returns True if expression is linear (a polynomial with degree 1 or 0) (read-only).\"\"\"\n        coeff_dict = self.expression.as_coefficients_dict()\n        for key in coeff_dict.keys():\n            if len(key.free_symbols) < 2 and (key.is_Add or key.is_Mul or key.is_Atom):\n                pass\n            else:\n                return False\n            if key.is_Pow and key.args[1] != 1:\n                return False\n        else:\n            return True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_Quadratic(self):\n        if self.expression.is_Atom:\n            return False\n        if all((len(key.free_symbols) < 2 and (key.is_Add or key.is_Mul or key.is_Atom)\n                for key in self.expression.as_coefficients_dict().keys())):\n            return False\n        if self.expression.is_Add:\n            terms = self.expression.args\n            is_quad = False\n            for term in terms:\n                if len(term.free_symbols) > 2:\n                    return False\n                if term.is_Pow:\n                    if not term.args[1].is_Number or term.args[1] > 2:\n                        return False\n                    else:\n                        is_quad = True\n                elif term.is_Mul:\n                    if len(term.free_symbols) == 2:\n                        is_quad = True\n                    if term.args[1].is_Pow:\n                        if not term.args[1].args[1].is_Number or term.args[1].args[1] > 2:\n                            return False\n                        else:\n                            is_quad = True\n            return is_quad\n        else:\n            if isinstance(self.expression, sympy.Basic):\n                sympy_expression = self.expression\n            else:\n                sympy_expression = sympy.sympify(self.expression)\n            # TODO: Find a way to do this with symengine (Poly is not part of symengine, 23 March 2017)\n            poly = sympy_expression.as_poly(*sympy_expression.atoms(sympy.Symbol))\n            if poly is None:\n                return False\n            else:\n                return poly.is_quadratic", "response": "Returns True if the expression is a polynomial with degree exactly 2."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a copy of another constraint.", "response": "def clone(cls, constraint, model=None, **kwargs):\n        \"\"\"\n        Make a copy of another constraint. The constraint being copied can be of the same type or belong to\n        a different solver interface.\n\n        Parameters\n        ----------\n        constraint: interface.Constraint (or subclass)\n            The constraint to copy\n        model: Model or None\n            The variables of the new constraint will be taken from this model. If None, new variables will be\n            constructed.\n\n        Example\n        ----------\n        >>> const_copy = Constraint.clone(old_constraint)\n        \"\"\"\n        return cls(cls._substitute_variables(constraint, model=model), lb=constraint.lb, ub=constraint.ub,\n                   indicator_variable=constraint.indicator_variable, active_when=constraint.active_when,\n                   name=constraint.name, sloppy=True, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a json - compatible object from the constraint that can be saved using the json module.", "response": "def to_json(self):\n        \"\"\"\n        Returns a json-compatible object from the constraint that can be saved using the json module.\n\n        Example\n        --------\n        >>> import json\n        >>> with open(\"path_to_file.json\", \"w\") as outfile:\n        >>>     json.dump(constraint.to_json(), outfile)\n        \"\"\"\n        if self.indicator_variable is None:\n            indicator = None\n        else:\n            indicator = self.indicator_variable.name\n        json_obj = {\n            \"name\": self.name,\n            \"expression\": expr_to_json(self.expression),\n            \"lb\": self.lb,\n            \"ub\": self.ub,\n            \"indicator_variable\": indicator,\n            \"active_when\": self.active_when\n        }\n        return json_obj"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef from_json(cls, json_obj, variables=None):\n        if variables is None:\n            variables = {}\n        expression = parse_expr(json_obj[\"expression\"], variables)\n        if json_obj[\"indicator_variable\"] is None:\n            indicator = None\n        else:\n            indicator = variables[json_obj[\"indicator_variable\"]]\n        return cls(\n            expression,\n            name=json_obj[\"name\"],\n            lb=json_obj[\"lb\"],\n            ub=json_obj[\"ub\"],\n            indicator_variable=indicator,\n            active_when=json_obj[\"active_when\"]\n        )", "response": "Constructs a Variable from the provided json - object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef clone(cls, objective, model=None, **kwargs):\n        return cls(cls._substitute_variables(objective, model=model), name=objective.name,\n                   direction=objective.direction, sloppy=True, **kwargs)", "response": "Create a copy of an objective."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef to_json(self):\n        json_obj = {\n            \"name\": self.name,\n            \"expression\": expr_to_json(self.expression),\n            \"direction\": self.direction\n        }\n        return json_obj", "response": "Returns a json - compatible object from the objective that can be saved using the json module."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconstructing an Objective from the provided json - object.", "response": "def from_json(cls, json_obj, variables=None):\n        \"\"\"\n        Constructs an Objective from the provided json-object.\n\n        Example\n        --------\n        >>> import json\n        >>> with open(\"path_to_file.json\") as infile:\n        >>>     obj = Objective.from_json(json.load(infile))\n        \"\"\"\n        if variables is None:\n            variables = {}\n        expression = parse_expr(json_obj[\"expression\"], variables)\n        return cls(\n            expression,\n            direction=json_obj[\"direction\"],\n            name=json_obj[\"name\"]\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmaking a copy of a model.", "response": "def clone(cls, model, use_json=True, use_lp=False):\n        \"\"\"\n        Make a copy of a model. The model being copied can be of the same type or belong to\n        a different solver interface. This is the preferred way of copying models.\n\n        Example\n        ----------\n        >>> new_model = Model.clone(old_model)\n        \"\"\"\n        model.update()\n        interface = sys.modules[cls.__module__]\n\n        if use_lp:\n            warnings.warn(\"Cloning with LP formats can change variable and constraint ID's.\")\n            new_model = cls.from_lp(model.to_lp())\n            new_model.configuration = interface.Configuration.clone(model.configuration, problem=new_model)\n            return new_model\n\n        if use_json:\n            new_model = cls.from_json(model.to_json())\n            new_model.configuration = interface.Configuration.clone(model.configuration, problem=new_model)\n            return new_model\n\n        new_model = cls()\n        for variable in model.variables:\n            new_variable = interface.Variable.clone(variable)\n            new_model._add_variable(new_variable)\n        for constraint in model.constraints:\n            new_constraint = interface.Constraint.clone(constraint, model=new_model)\n            new_model._add_constraint(new_constraint)\n        if model.objective is not None:\n            new_model.objective = interface.Objective.clone(model.objective, model=new_model)\n        new_model.configuration = interface.Configuration.clone(model.configuration, problem=new_model)\n        return new_model"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add(self, stuff, sloppy=False):\n        if self._pending_modifications.toggle == 'remove':\n            self.update()\n            self._pending_modifications.toggle = 'add'\n        if isinstance(stuff, collections.Iterable):\n            for elem in stuff:\n                self.add(elem, sloppy=sloppy)\n        elif isinstance(stuff, Variable):\n            if stuff.__module__ != self.__module__:\n                raise TypeError(\"Cannot add Variable %s of interface type %s to model of type %s.\" % (\n                    stuff, stuff.__module__, self.__module__))\n            self._pending_modifications.add_var.append(stuff)\n        elif isinstance(stuff, Constraint):\n            if stuff.__module__ != self.__module__:\n                raise TypeError(\"Cannot add Constraint %s of interface type %s to model of type %s.\" % (\n                    stuff, stuff.__module__, self.__module__))\n            if sloppy is True:\n                self._pending_modifications.add_constr_sloppy.append(stuff)\n            else:\n                self._pending_modifications.add_constr.append(stuff)\n        else:\n            raise TypeError(\"Cannot add %s. It is neither a Variable, or Constraint.\" % stuff)", "response": "Add variables and constraints to the internal list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving variables and constraints from the current solver instance.", "response": "def remove(self, stuff):\n        \"\"\"Remove variables and constraints.\n\n        Parameters\n        ----------\n        stuff : iterable, str, Variable, Constraint\n            Either an iterable containing variables and constraints to be removed from the model or a single variable or contstraint (or their names).\n\n        Returns\n        -------\n        None\n        \"\"\"\n        if self._pending_modifications.toggle == 'add':\n            self.update()\n            self._pending_modifications.toggle = 'remove'\n        if isinstance(stuff, str):\n            try:\n                variable = self.variables[stuff]\n                self._pending_modifications.rm_var.append(variable)\n            except KeyError:\n                try:\n                    constraint = self.constraints[stuff]\n                    self._pending_modifications.rm_constr.append(constraint)\n                except KeyError:\n                    raise LookupError(\n                        \"%s is neither a variable nor a constraint in the current solver instance.\" % stuff)\n        elif isinstance(stuff, Variable):\n            self._pending_modifications.rm_var.append(stuff)\n        elif isinstance(stuff, Constraint):\n            self._pending_modifications.rm_constr.append(stuff)\n        elif isinstance(stuff, collections.Iterable):\n            for elem in stuff:\n                self.remove(elem)\n        elif isinstance(stuff, Objective):\n            raise TypeError(\n                \"Cannot remove objective %s. Use model.objective = Objective(...) to change the current objective.\" % stuff)\n        else:\n            raise TypeError(\n                \"Cannot remove %s. It neither a variable or constraint.\" % stuff)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprocesses all pending model modifications.", "response": "def update(self, callback=int):\n        \"\"\"Process all pending model modifications.\"\"\"\n        # print(self._pending_modifications)\n        add_var = self._pending_modifications.add_var\n        if len(add_var) > 0:\n            self._add_variables(add_var)\n            self._pending_modifications.add_var = []\n        callback()\n\n        add_constr = self._pending_modifications.add_constr\n        if len(add_constr) > 0:\n            self._add_constraints(add_constr)\n            self._pending_modifications.add_constr = []\n\n        add_constr_sloppy = self._pending_modifications.add_constr_sloppy\n        if len(add_constr_sloppy) > 0:\n            self._add_constraints(add_constr_sloppy, sloppy=True)\n            self._pending_modifications.add_constr_sloppy = []\n\n        var_lb = self._pending_modifications.var_lb\n        var_ub = self._pending_modifications.var_ub\n        if len(var_lb) > 0 or len(var_ub) > 0:\n            self._set_variable_bounds_on_problem(var_lb, var_ub)\n            self._pending_modifications.var_lb = []\n            self._pending_modifications.var_ub = []\n\n        rm_var = self._pending_modifications.rm_var\n        if len(rm_var) > 0:\n            self._remove_variables(rm_var)\n            self._pending_modifications.rm_var = []\n        callback()\n\n        rm_constr = self._pending_modifications.rm_constr\n        if len(rm_constr) > 0:\n            self._remove_constraints(rm_constr)\n            self._pending_modifications.rm_constr = []"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsolve the optimization problem using the relevant solver back - end.", "response": "def optimize(self):\n        \"\"\"\n        Solve the optimization problem using the relevant solver back-end.\n        The status returned by this method tells whether an optimal solution was found,\n        if the problem is infeasible etc. Consult optlang.statuses for more elaborate explanations\n        of each status.\n\n        The objective value can be accessed from 'model.objective.value', while the solution can be\n        retrieved by 'model.primal_values'.\n\n        Returns\n        -------\n        status: str\n            Solution status.\n        \"\"\"\n        self.update()\n        status = self._optimize()\n        if status != OPTIMAL and self.configuration.presolve == \"auto\":\n            self.configuration.presolve = True\n            status = self._optimize()\n            self.configuration.presolve = \"auto\"\n        self._status = status\n        return status"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a json - compatible object from the model that can be saved using the json module.", "response": "def to_json(self):\n        \"\"\"\n        Returns a json-compatible object from the model that can be saved using the json module.\n        Variables, constraints and objective contained in the model will be saved. Configurations\n        will not be saved.\n\n        Example\n        --------\n        >>> import json\n        >>> with open(\"path_to_file.json\", \"w\") as outfile:\n        >>>     json.dump(model.to_json(), outfile)\n        \"\"\"\n        json_obj = {\n            \"name\": self.name,\n            \"variables\": [var.to_json() for var in self.variables],\n            \"constraints\": [const.to_json() for const in self.constraints],\n            \"objective\": self.objective.to_json()\n        }\n        return json_obj"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting a linear problem to dual.", "response": "def convert_linear_problem_to_dual(model, sloppy=False, infinity=None, maintain_standard_form=True, prefix=\"dual_\", dual_model=None):  # NOQA\n    \"\"\"\n    A mathematical optimization problem can be viewed as a primal and a dual problem. If the primal problem is\n    a minimization problem the dual is a maximization problem, and the optimal value of the dual is a lower bound of\n    the optimal value of the primal.\n    For linear problems, strong duality holds, which means that the optimal values of the primal and dual are equal\n    (duality gap = 0).\n\n    This functions takes an optlang Model representing a primal linear problem and returns a new Model representing\n    the dual optimization problem. The provided model must have a linear objective, linear constraints and only\n    continuous variables. Furthermore, the problem must be in standard form, i.e. all variables should be non-negative.\n    Both minimization and maximization problems are allowed. The objective direction of the dual will always be\n    opposite of the primal.\n\n    Attributes:\n    ----------\n    model: optlang.interface.Model\n        The primal problem to be dualized\n    sloppy: Boolean (default False)\n        If True, linearity, variable types and standard form will not be checked. Only use if you know the primal is\n        valid\n    infinity: Numeric or None\n        If not None this value will be used as bounds instead of unbounded variables.\n    maintain_standard_form: Boolean (default True)\n        If False the returned dual problem will not be in standard form, but will have fewer variables and/or constraints\n    prefix: str\n        The string that will be prepended to all variable and constraint names in the returned dual problem.\n    dual_model: optlang.interface.Model or None (default)\n        If not None, the dual variables and constraints will be added to this model. Note the objective will also be\n        set to the dual objective. If None a new model will be created.\n\n    Returns:\n    ----------\n    dual_problem: optlang.interface.Model (same solver as the primal)\n    \"\"\"\n    if dual_model is None:\n        dual_model = model.interface.Model()\n\n    maximization = model.objective.direction == \"max\"\n\n    if infinity is not None:\n        neg_infinity = -infinity\n    else:\n        neg_infinity = None\n\n    if maximization:\n        sign = 1\n    else:\n        sign = -1\n\n    coefficients = {}\n    dual_objective = {}\n\n    # Add dual variables from primal constraints:\n    for constraint in model.constraints:\n        if constraint.expression == 0:\n            continue  # Skip empty constraint\n        if not (sloppy or constraint.is_Linear):\n            raise ValueError(\"Non-linear problems are not supported: \" + str(constraint))\n        if constraint.lb is None and constraint.ub is None:\n            continue  # Skip free constraint\n        if not maintain_standard_form and constraint.lb == constraint.ub:\n            const_var = model.interface.Variable(prefix + constraint.name + \"_constraint\", lb=neg_infinity, ub=infinity)\n            dual_model.add(const_var)\n            if constraint.lb != 0:\n                dual_objective[const_var] = sign * constraint.lb\n            for variable, coef in constraint.expression.as_coefficients_dict().items():\n                if variable == 1:  # pragma: no cover  # For symengine\n                    continue\n                coefficients.setdefault(variable.name, {})[const_var] = sign * coef\n        else:\n            if constraint.lb is not None:\n                lb_var = model.interface.Variable(prefix + constraint.name + \"_constraint_lb\", lb=0, ub=infinity)\n                dual_model.add(lb_var)\n                if constraint.lb != 0:\n                    dual_objective[lb_var] = -sign * constraint.lb\n            if constraint.ub is not None:\n                ub_var = model.interface.Variable(prefix + constraint.name + \"_constraint_ub\", lb=0, ub=infinity)\n                dual_model.add(ub_var)\n                if constraint.ub != 0:\n                    dual_objective[ub_var] = sign * constraint.ub\n\n            assert constraint.expression.is_Add or constraint.expression.is_Mul, \\\n                \"Invalid expression type: \" + str(type(constraint.expression))\n            if constraint.expression.is_Add:\n                coefficients_dict = constraint.expression.as_coefficients_dict()\n            else:  # constraint.expression.is_Mul:\n                coefficients_dict = {constraint.expression.args[1]: constraint.expression.args[0]}\n\n            for variable, coef in coefficients_dict.items():\n                if variable == 1:  # pragma: no cover  # For symengine\n                    continue\n                if constraint.lb is not None:\n                    coefficients.setdefault(variable.name, {})[lb_var] = -sign * coef\n                if constraint.ub is not None:\n                    coefficients.setdefault(variable.name, {})[ub_var] = sign * coef\n\n    # Add dual variables from primal bounds\n    for variable in model.variables:\n        if not (sloppy or variable.type == \"continuous\"):\n            raise ValueError(\"Integer variables are not supported: \" + str(variable))\n        if not sloppy and (variable.lb is None or variable.lb < 0):\n            raise ValueError(\"Problem is not in standard form (\" + variable.name + \" can be negative)\")\n        if variable.lb > 0:\n            bound_var = model.interface.Variable(prefix + variable.name + \"_lb\", lb=0, ub=infinity)\n            dual_model.add(bound_var)\n            coefficients.setdefault(variable.name, {})[bound_var] = -sign * 1\n            dual_objective[bound_var] = -sign * variable.lb\n        if variable.ub is not None:\n            bound_var = model.interface.Variable(prefix + variable.name + \"_ub\", lb=0, ub=infinity)\n            dual_model.add(bound_var)\n            coefficients.setdefault(variable.name, {})[bound_var] = sign * 1\n            if variable.ub != 0:\n                dual_objective[bound_var] = sign * variable.ub\n\n    # Add dual constraints from primal objective\n    primal_objective_dict = model.objective.expression.as_coefficients_dict()\n    for variable in model.variables:\n        expr = optlang.symbolics.add([(coef * dual_var) for dual_var, coef in coefficients[variable.name].items()])\n        obj_coef = primal_objective_dict[variable]\n        if maximization:\n            const = model.interface.Constraint(expr, lb=obj_coef, name=prefix + variable.name)\n        else:\n            const = model.interface.Constraint(expr, ub=obj_coef, name=prefix + variable.name)\n        dual_model.add(const)\n\n    # Make dual objective\n    expr = optlang.symbolics.add([(coef * dual_var) for dual_var, coef in dual_objective.items() if coef != 0])\n    if maximization:\n        objective = model.interface.Objective(expr, direction=\"min\")\n    else:\n        objective = model.interface.Objective(expr, direction=\"max\")\n    dual_model.objective = objective\n\n    return dual_model"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsolve glpk problem with glpsol commandline solver. Mainly for testing purposes.", "response": "def solve_with_glpsol(glp_prob):\n    \"\"\"Solve glpk problem with glpsol commandline solver. Mainly for testing purposes.\n\n    # Examples\n    # --------\n\n    # >>> problem = glp_create_prob()\n    # ... glp_read_lp(problem, None, \"../tests/data/model.lp\")\n    # ... solution = solve_with_glpsol(problem)\n    # ... print 'asdf'\n    # 'asdf'\n    # >>> print solution\n    # 0.839784\n\n    # Returns\n    # -------\n    # dict\n    #     A dictionary containing the objective value (key ='objval')\n    #     and variable primals.\n    \"\"\"\n    from swiglpk import glp_get_row_name, glp_get_col_name, glp_write_lp, glp_get_num_rows, glp_get_num_cols\n\n    row_ids = [glp_get_row_name(glp_prob, i) for i in range(1, glp_get_num_rows(glp_prob) + 1)]\n\n    col_ids = [glp_get_col_name(glp_prob, i) for i in range(1, glp_get_num_cols(glp_prob) + 1)]\n\n    with tempfile.NamedTemporaryFile(suffix=\".lp\", delete=True) as tmp_file:\n        tmp_file_name = tmp_file.name\n        glp_write_lp(glp_prob, None, tmp_file_name)\n        cmd = ['glpsol', '--lp', tmp_file_name, '-w', tmp_file_name + '.sol', '--log', '/dev/null']\n        term = check_output(cmd)\n        log.info(term)\n\n    try:\n        with open(tmp_file_name + '.sol') as sol_handle:\n            # print sol_handle.read()\n            solution = dict()\n            for i, line in enumerate(sol_handle.readlines()):\n                if i <= 1 or line == '\\n':\n                    pass\n                elif i <= len(row_ids):\n                    solution[row_ids[i - 2]] = line.strip().split(' ')\n                elif i <= len(row_ids) + len(col_ids) + 1:\n                    solution[col_ids[i - 2 - len(row_ids)]] = line.strip().split(' ')\n                else:\n                    print(i)\n                    print(line)\n                    raise Exception(\"Argggh!\")\n    finally:\n        os.remove(tmp_file_name + \".sol\")\n    return solution"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef glpk_read_cplex(path):\n    from swiglpk import glp_create_prob, glp_read_lp\n\n    problem = glp_create_prob()\n    glp_read_lp(problem, None, path)\n    return problem", "response": "Reads a cplex file and returns glpk problem."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndetermining available solver interfaces.", "response": "def list_available_solvers():\n    \"\"\"Determine available solver interfaces (with python bindings).\n\n    Returns\n    -------\n    dict\n        A dict like {'GLPK': True, 'GUROBI': False, ...}\n    \"\"\"\n    solvers = dict(GUROBI=False, GLPK=False, MOSEK=False, CPLEX=False, SCIPY=False)\n    try:\n        import gurobipy\n\n        solvers['GUROBI'] = True\n        log.debug('Gurobi python bindings found at %s' % os.path.dirname(gurobipy.__file__))\n    except Exception:\n        log.debug('Gurobi python bindings not available.')\n    try:\n        import swiglpk\n\n        solvers['GLPK'] = True\n        log.debug('GLPK python bindings found at %s' % os.path.dirname(swiglpk.__file__))\n    except Exception:\n        log.debug('GLPK python bindings not available.')\n    try:\n        import mosek\n\n        solvers['MOSEK'] = True\n        log.debug('Mosek python bindings found at %s' % os.path.dirname(mosek.__file__))\n    except Exception:\n        log.debug('Mosek python bindings not available.')\n    try:\n        import cplex\n\n        solvers['CPLEX'] = True\n        log.debug('CPLEX python bindings found at %s' % os.path.dirname(cplex.__file__))\n    except Exception:\n        log.debug('CPLEX python bindings not available.')\n    try:\n        from scipy import optimize\n        optimize.linprog\n\n        solvers[\"SCIPY\"] = True\n        log.debug(\"Scipy linprog function found at %s\" % optimize.__file__)\n    except (ImportError, AttributeError):\n        log.debug(\"Scipy solver not available\")\n    return solvers"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef inheritdocstring(name, bases, attrs):\n    if '__doc__' not in attrs or not attrs[\"__doc__\"]:\n        # create a temporary 'parent' to (greatly) simplify the MRO search\n        temp = type('temporaryclass', bases, {})\n        for cls in inspect.getmro(temp):\n            if cls.__doc__ is not None:\n                attrs['__doc__'] = cls.__doc__\n                break\n\n    for attr_name, attr in attrs.items():\n        if not attr.__doc__:\n            for cls in inspect.getmro(temp):\n                try:\n                    if getattr(cls, attr_name).__doc__ is not None:\n                        attr.__doc__ = getattr(cls, attr_name).__doc__\n                        break\n                except (AttributeError, TypeError):\n                    continue\n\n    return type(name, bases, attrs)", "response": "Inherit docstring from parent class and method."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef expr_to_json(expr):\n    if isinstance(expr, symbolics.Mul):\n        return {\"type\": \"Mul\", \"args\": [expr_to_json(arg) for arg in expr.args]}\n    elif isinstance(expr, symbolics.Add):\n        return {\"type\": \"Add\", \"args\": [expr_to_json(arg) for arg in expr.args]}\n    elif isinstance(expr, symbolics.Symbol):\n        return {\"type\": \"Symbol\", \"name\": expr.name}\n    elif isinstance(expr, symbolics.Pow):\n        return {\"type\": \"Pow\", \"args\": [expr_to_json(arg) for arg in expr.args]}\n    elif isinstance(expr, (float, int)):\n        return {\"type\": \"Number\", \"value\": expr}\n    elif isinstance(expr, symbolics.Real):\n        return {\"type\": \"Number\", \"value\": float(expr)}\n    elif isinstance(expr, symbolics.Integer):\n        return {\"type\": \"Number\", \"value\": int(expr)}\n    else:\n        raise NotImplementedError(\"Type not implemented: \" + str(type(expr)))", "response": "Converts a Sympy expression to a json - compatible tree - structure."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_expr(expr, local_dict=None):\n    if local_dict is None:\n        local_dict = {}\n    if expr[\"type\"] == \"Add\":\n        return add([parse_expr(arg, local_dict) for arg in expr[\"args\"]])\n    elif expr[\"type\"] == \"Mul\":\n        return mul([parse_expr(arg, local_dict) for arg in expr[\"args\"]])\n    elif expr[\"type\"] == \"Pow\":\n        return Pow(parse_expr(arg, local_dict) for arg in expr[\"args\"])\n    elif expr[\"type\"] == \"Symbol\":\n        try:\n            return local_dict[expr[\"name\"]]\n        except KeyError:\n            return symbolics.Symbol(expr[\"name\"])\n    elif expr[\"type\"] == \"Number\":\n        return symbolics.sympify(expr[\"value\"])\n    else:\n        raise NotImplementedError(expr[\"type\"] + \" is not implemented\")", "response": "Parses a json - object created with expr_to_json into a Sympy expression."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_variable_bounds(self, name, lower, upper):\n        self.bounds[name] = (lower, upper)\n        self._reset_solution()", "response": "Set the bounds of a variable"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a variable to the problem", "response": "def add_variable(self, name):\n        \"\"\"Add a variable to the problem\"\"\"\n        if name in self._variables:\n            raise ValueError(\n                \"A variable named \" + name + \" already exists.\"\n            )\n        self._variables[name] = len(self._variables)\n        self.bounds[name] = (0, None)\n\n        new_col = np.zeros(shape=[len(self._constraints), 1])\n        self._add_col_to_A(new_col)\n        self._reset_solution()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_constraint(self, name, coefficients={}, ub=0):\n        if name in self._constraints:\n            raise ValueError(\n                \"A constraint named \" + name + \" already exists.\"\n            )\n        self._constraints[name] = len(self._constraints)\n        self.upper_bounds = np.append(self.upper_bounds, ub)\n\n        new_row = np.array([[coefficients.get(name, 0) for name in self._variables]])\n        self._add_row_to_A(new_row)\n        self._reset_solution()", "response": "Add a constraint to the problem."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves a variable from the problem.", "response": "def remove_variable(self, name):\n        \"\"\"Remove a variable from the problem.\"\"\"\n        index = self._get_var_index(name)\n        # Remove from matrix\n        self._A = np.delete(self.A, index, 1)\n        # Remove from bounds\n        del self.bounds[name]\n        # Remove from var list\n        del self._variables[name]\n        self._update_variable_indices()\n        self._reset_solution()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nremoving a constraint from the problem", "response": "def remove_constraint(self, name):\n        \"\"\"Remove a constraint from the problem\"\"\"\n        index = self._get_constraint_index(name)\n        # Remove from matrix\n        self._A = np.delete(self.A, index, 0)\n        # Remove from upper_bounds\n        self.upper_bounds = np.delete(self.upper_bounds, index)\n        # Remove from constraint list\n        del self._constraints[name]\n        self._update_constraint_indices()\n        self._reset_solution()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_constraint_bound(self, name, value):\n        index = self._get_constraint_index(name)\n        self.upper_bounds[index] = value\n        self._reset_solution()", "response": "Set the upper bound of a constraint."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the primal value of a variable. Returns None if the problem has not bee optimized.", "response": "def get_var_primal(self, name):\n        \"\"\"Get the primal value of a variable. Returns None if the problem has not bee optimized.\"\"\"\n        if self._var_primals is None:\n            return None\n        else:\n            index = self._get_var_index(name)\n            return self._var_primals[index]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_constraint_slack(self, name):\n        if self._slacks is None:\n            return None\n        else:\n            index = self._get_constraint_index(name)\n            return self._slacks[index]", "response": "Get the value of the slack variable of a constraint."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef optimize(self, method=\"simplex\", verbosity=False, tolerance=1e-9, **kwargs):\n        c = np.array([self.objective.get(name, 0) for name in self._variables])\n        if self.direction == \"max\":\n            c *= -1\n\n        bounds = list(six.itervalues(self.bounds))\n        solution = linprog(c, self.A, self.upper_bounds, bounds=bounds, method=method,\n                           options={\"maxiter\": 10000, \"disp\": verbosity, \"tol\": tolerance}, **kwargs)\n        self._solution = solution\n        self._status = solution.status\n        if SCIPY_STATUS[self._status] == interface.OPTIMAL:\n            self._var_primals = solution.x\n            self._slacks = solution.slack\n        else:\n            self._var_primals = None\n            self._slacks = None\n\n        self._f = solution.fun", "response": "Run the linprog function on the problem. Returns None."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef objective_value(self):\n        if self._f is None:\n            raise RuntimeError(\"Problem has not been optimized yet\")\n        if self.direction == \"max\":\n            return -self._f + self.offset\n        else:\n            return self._f + self.offset", "response": "Returns the optimal objective value"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run(self, command, name_addition=None, cmd_kwargs=None,\n            _cmd=\"sbatch\", tries=1, depends_on=None):\n        \"\"\"\n        command: a bash command that you want to run\n        name_addition: if not specified, the sha1 of the command to run\n                       appended to job name. if it is \"date\", the yyyy-mm-dd\n                       date will be added to the job name.\n        cmd_kwargs: dict of extra arguments to fill in command\n                   (so command itself can be a template).\n        _cmd: submit command (change to \"bash\" for testing).\n        tries: try to run a job either this many times or until the first\n               success.\n        depends_on: job ids that this depends on before it is run (users 'afterok')\n        \"\"\"\n        if name_addition is None:\n            name_addition = hashlib.sha1(command.encode(\"utf-8\")).hexdigest()\n\n        if self.date_in_name:\n            name_addition += \"-\" + str(datetime.date.today())\n        name_addition = name_addition.strip(\" -\")\n\n        if cmd_kwargs is None:\n            cmd_kwargs = {}\n\n        n = self.name\n        self.name = self.name.strip(\" -\")\n        self.name += (\"-\" + name_addition.strip(\" -\"))\n        args = []\n        for k, v in cmd_kwargs.items():\n            args.append(\"export %s=%s\" % (k, v))\n        args = \"\\n\".join(args)\n\n        tmpl = str(self).replace(\"__script__\", args + \"\\n###\\n\" + command)\n        if depends_on is None or (len(depends_on) == 1 and depends_on[0] is None):\n            depends_on = []\n\n        with open(self._tmpfile(), \"w\") as sh:\n            sh.write(tmpl)\n\n        job_id = None\n        for itry in range(1, tries + 1):\n            args = [_cmd]\n            args.extend([(\"--dependency=afterok:%d\" % int(d))\n                         for d in depends_on])\n            if itry > 1:\n                mid = \"--dependency=afternotok:%d\" % job_id\n                args.append(mid)\n            args.append(sh.name)\n            res = subprocess.check_output(args).strip()\n            print(res, file=sys.stderr)\n            self.name = n\n            if not res.startswith(b\"Submitted batch\"):\n                return None\n            j_id = int(res.split()[-1])\n            if itry == 1:\n                job_id = j_id\n        return job_id", "response": "Runs a bash command and returns a dictionary of job ids and the output of the command."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninstalling the AMQP application for the Tornado application.", "response": "def install(application, io_loop=None, **kwargs):\n    \"\"\"Call this to install AMQP for the Tornado application. Additional\n    keyword arguments are passed through to the constructor of the AMQP\n    object.\n\n    :param tornado.web.Application application: The tornado application\n    :param tornado.ioloop.IOLoop io_loop: The current IOLoop.\n    :rtype: bool\n\n    \"\"\"\n    if getattr(application, 'amqp', None) is not None:\n        LOGGER.warning('AMQP is already installed')\n        return False\n\n    kwargs.setdefault('io_loop', io_loop)\n\n    # Support AMQP_* and RABBITMQ_* variables\n    for prefix in {'AMQP', 'RABBITMQ'}:\n\n        key = '{}_URL'.format(prefix)\n        if os.environ.get(key) is not None:\n            LOGGER.debug('Setting URL to %s', os.environ[key])\n            kwargs.setdefault('url', os.environ[key])\n\n        key = '{}_CONFIRMATIONS'.format(prefix)\n        if os.environ.get(key) is not None:\n            value = os.environ[key].lower() in {'true', '1'}\n            LOGGER.debug('Setting enable_confirmations to %s', value)\n            kwargs.setdefault('enable_confirmations', value)\n\n        key = '{}_CONNECTION_ATTEMPTS'.format(prefix)\n        if os.environ.get(key) is not None:\n            value = int(os.environ[key])\n            LOGGER.debug('Setting connection_attempts to %s', value)\n            kwargs.setdefault('connection_attempts', value)\n\n        key = '{}_RECONNECT_DELAY'.format(prefix)\n        if os.environ.get(key) is not None:\n            value = float(os.environ[key])\n            LOGGER.debug('Setting reconnect_delay to %s', value)\n            kwargs.setdefault('reconnect_delay', value)\n\n    # Set the default AMQP app_id property\n    if application.settings.get('service') and \\\n            application.settings.get('version'):\n        default_app_id = '{}/{}'.format(\n            application.settings['service'], application.settings['version'])\n    else:\n        default_app_id = 'sprockets.mixins.amqp/{}'.format(__version__)\n    kwargs.setdefault('default_app_id', default_app_id)\n\n    # Default the default URL value if not already set\n    kwargs.setdefault('url', 'amqp://guest:guest@localhost:5672/%2f')\n\n    LOGGER.debug('kwargs: %r', kwargs)\n    setattr(application, 'amqp', Client(**kwargs))\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\npublishing a message to RabbitMQ", "response": "def amqp_publish(self, exchange, routing_key, body, properties=None):\n        \"\"\"Publish a message to RabbitMQ\n\n        :param str exchange: The exchange to publish the message to\n        :param str routing_key: The routing key to publish the message with\n        :param bytes body: The message body to send\n        :param dict properties: An optional dict of AMQP properties\n        :rtype: tornado.concurrent.Future\n\n        :raises: :exc:`sprockets.mixins.amqp.AMQPError`\n        :raises: :exc:`sprockets.mixins.amqp.NotReadyError`\n        :raises: :exc:`sprockets.mixins.amqp.PublishingError`\n\n        \"\"\"\n        properties = properties or {}\n        if hasattr(self, 'correlation_id') and getattr(self, 'correlation_id'):\n            properties.setdefault('correlation_id', self.correlation_id)\n        return self.application.amqp.publish(\n            exchange, routing_key, body, properties)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef publish(self, exchange, routing_key, body, properties=None):\n        future = concurrent.Future()\n\n        properties = properties or {}\n        properties.setdefault('app_id', self.default_app_id)\n        properties.setdefault('message_id', str(uuid.uuid4()))\n        properties.setdefault('timestamp', int(time.time()))\n\n        if self.ready:\n            if self.publisher_confirmations:\n                self.message_number += 1\n                self.messages[self.message_number] = future\n            else:\n                future.set_result(None)\n\n            try:\n                self.channel.basic_publish(\n                    exchange, routing_key, body,\n                    pika.BasicProperties(**properties), True)\n            except exceptions.AMQPError as error:\n                future.set_exception(\n                    PublishingFailure(\n                        properties['message_id'],\n                        exchange, routing_key,\n                        error.__class__.__name__))\n        else:\n            future.set_exception(NotReadyError(\n                self.state_description, properties['message_id']))\n        return future", "response": "Publish a message to RabbitMQ."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef on_delivery_confirmation(self, method_frame):\n        confirmation_type = method_frame.method.NAME.split('.')[1].lower()\n        LOGGER.debug('Received %s for delivery tag: %i',\n                     confirmation_type, method_frame.method.delivery_tag)\n\n        if method_frame.method.multiple:\n            confirmed = sorted([msg for msg in self.messages\n                                if msg <= method_frame.method.delivery_tag])\n        else:\n            confirmed = [method_frame.method.delivery_tag]\n\n        for msg in confirmed:\n            LOGGER.debug('RabbitMQ confirmed message %i', msg)\n            try:\n                if confirmation_type == 'ack':\n                    self.messages[msg].set_result(None)\n                elif confirmation_type == 'nack':\n                    self.messages[msg].set_exception(PublishingFailure(msg))\n            except KeyError:\n                LOGGER.warning('Tried to confirm a message missing in stack')\n            else:\n                del self.messages[msg]\n\n        LOGGER.debug('Published %i messages, %i have yet to be confirmed',\n                     self.message_number, len(self.messages))", "response": "Invoked by pika when RabbitMQ responds to a Basic. Publish RPC\n        command passing in either a Basic. Ack or Basic. Nack frame with the delivery tag of the message that was published."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef close(self):\n        if not self.closable:\n            LOGGER.warning('Closed called while %s', self.state_description)\n            raise ConnectionStateError(self.state_description)\n        self.state = self.STATE_CLOSING\n        LOGGER.info('Closing RabbitMQ connection')\n        self.connection.close()", "response": "Cleanly shutdown the connection to RabbitMQ."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nschedule the next connection attempt if the class is not currently closing.", "response": "def _reconnect(self):\n        \"\"\"Schedule the next connection attempt if the class is not currently\n        closing.\n\n        \"\"\"\n        if self.idle or self.closed:\n            LOGGER.debug('Attempting RabbitMQ reconnect in %s seconds',\n                         self.reconnect_delay)\n            self.io_loop.call_later(self.reconnect_delay, self.connect)\n            return\n        LOGGER.warning('Reconnect called while %s', self.state_description)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef on_connection_open(self, connection):\n        LOGGER.debug('Connection opened')\n        connection.add_on_connection_blocked_callback(\n            self.on_connection_blocked)\n        connection.add_on_connection_unblocked_callback(\n            self.on_connection_unblocked)\n        connection.add_backpressure_callback(self.on_back_pressure_detected)\n        self.channel = self._open_channel()", "response": "This method is called by pika when the connection to RabbitMQ has been established."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninvokes if the connection to RabbitMQ can not be made.", "response": "def on_connection_open_error(self, connection, error):\n        \"\"\"Invoked if the connection to RabbitMQ can not be made.\n\n        :type connection: pika.TornadoConnection\n        :param Exception error: The exception indicating failure\n\n        \"\"\"\n        LOGGER.critical('Could not connect to RabbitMQ (%s): %r',\n                        connection, error)\n        self.state = self.STATE_CLOSED\n        self._reconnect()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalling when the connection is unblocked.", "response": "def on_connection_unblocked(self, method_frame):\n        \"\"\"When RabbitMQ indicates the connection is unblocked, set the state\n        appropriately.\n\n        :param pika.amqp_object.Method method_frame: Unblocked method frame\n\n        \"\"\"\n        LOGGER.debug('Connection unblocked: %r', method_frame)\n        self.state = self.STATE_READY\n        if self.on_ready:\n            self.on_ready(self)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninvoking by the base class to handle the basic return message.", "response": "def on_basic_return(self, _channel, method, properties, body):\n        \"\"\"Invoke a registered callback or log the returned message.\n\n        :param _channel: The channel the message was sent on\n        :type _channel: pika.channel.Channel\n        :param pika.spec.Basic.Return method: The method object\n        :param pika.spec.BasicProperties properties: The message properties\n        :param str, unicode, bytes body: The message body\n\n        \"\"\"\n        if self.on_return:\n            self.on_return(method, properties, body)\n        else:\n            LOGGER.critical(\n                '%s message %s published to %s (CID %s) returned: %s',\n                method.exchange, properties.message_id,\n                method.routing_key, properties.correlation_id,\n                method.reply_text)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef on_channel_open(self, channel):\n        LOGGER.debug('Channel opened')\n        self.channel = channel\n        if self.publisher_confirmations:\n            self.channel.confirm_delivery(self.on_delivery_confirmation)\n        self.channel.add_on_close_callback(self.on_channel_closed)\n        self.channel.add_on_flow_callback(self.on_channel_flow)\n        self.channel.add_on_return_callback(self.on_basic_return)\n        self.state = self.STATE_READY\n        if self.on_ready:\n            self.on_ready(self)", "response": "This method is invoked by pika when the channel is opened."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ninvokes by pika when RabbitMQ unexpectedly closes the channel.", "response": "def on_channel_closed(self, channel, reply_code, reply_text):\n        \"\"\"Invoked by pika when RabbitMQ unexpectedly closes the channel.\n\n        Channels are usually closed if you attempt to do something that\n        violates the protocol, such as re-declare an exchange or queue with\n        different parameters.\n\n        In this case, we just want to log the error and create a new channel\n        after setting the state back to connecting.\n\n        :param pika.channel.Channel channel: The closed channel\n        :param int reply_code: The numeric reason the channel was closed\n        :param str reply_text: The text reason the channel was closed\n\n        \"\"\"\n        for future in self.messages.values():\n            future.set_exception(AMQPException(reply_code, reply_text))\n        self.messages = {}\n        if self.closing:\n            LOGGER.debug('Channel %s was intentionally closed (%s) %s',\n                         channel, reply_code, reply_text)\n        else:\n            LOGGER.warning('Channel %s was closed: (%s) %s',\n                           channel, reply_code, reply_text)\n            self.state = self.STATE_BLOCKED\n            if self.on_unavailable:\n                self.on_unavailable(self)\n            self.channel = self._open_channel()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef on_channel_flow(self, method):\n        if method.active:\n            LOGGER.info('Channel flow is active (READY)')\n            self.state = self.STATE_READY\n            if self.on_ready:\n                self.on_ready(self)\n        else:\n            LOGGER.warning('Channel flow is inactive (BLOCKED)')\n            self.state = self.STATE_BLOCKED\n            if self.on_unavailable:\n                self.on_unavailable(self)", "response": "When RabbitMQ indicates the connection is unblocked set the state\n            appropriately."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef object_to_json(obj):\n    if isinstance(obj, (datetime.datetime, datetime.date, datetime.time)):\n        return obj.isoformat()\n    return str(obj)", "response": "Convert object that cannot be natively serialized by python to JSON representation."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a Mail instance from a Guerrillamail response dict.", "response": "def from_response(cls, response_data):\n        \"\"\"\n        Factory method to create a Mail instance from a Guerrillamail response\n        dict.\n        \"\"\"\n        identity = lambda x: x\n        return Mail(**_transform_dict(response_data, {\n            'guid': ('mail_id', identity),\n            'subject': ('mail_subject', identity),\n            'sender': ('mail_from', identity),\n            'datetime': ('mail_timestamp', lambda x: datetime.utcfromtimestamp(int(x)).replace(tzinfo=utc)),\n            'read': ('mail_read', int),\n            'excerpt': ('mail_excerpt', identity),\n            'body': ('mail_body', identity),\n        }))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef prox_yline(y, step):\n    if not np.isscalar(y):\n        y= y[0]\n    if y > -0.75:\n        return np.array([-0.75])\n    else:\n        return np.array([y])", "response": "Projection onto line in y"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef prox_line(xy, step):\n    return np.concatenate((prox_xline(xy[0], step), prox_yline(xy[1], step)))", "response": "2D projection onto 2 lines"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nforwarding - backward gradient followed by projection", "response": "def prox_gradf_lim(xy, step, boundary=None):\n    \"\"\"Forward-backward step: gradient, followed by projection\"\"\"\n    return prox_lim(prox_gradf(xy,step), step, boundary=boundary)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef pgm(X, prox_f, step_f, accelerated=False, relax=None, e_rel=1e-6, max_iter=1000, traceback=None):\n\n    # init\n    stepper = utils.NesterovStepper(accelerated=accelerated)\n\n    if relax is not None:\n        assert relax > 0 and relax < 1.5\n\n    if traceback is not None:\n        traceback.update_history(0, X=X, step_f=step_f)\n        if accelerated:\n            traceback.update_history(0, omega=0)\n        if relax is not None:\n            traceback.update_history(0, relax=relax)\n\n    for it in range(max_iter):\n\n        # use Nesterov acceleration (if omega > 0), automatically incremented\n        omega = stepper.omega\n        if omega > 0:\n            _X = X + omega*(X - X_)\n        else:\n            _X = X\n        # make copy for convergence test and acceleration\n        X_ = X.copy()\n\n        # PGM step\n        X[:] = prox_f(_X, step_f)\n\n        if relax is not None:\n            X += (relax-1)*(X - X_)\n\n        if traceback is not None:\n            traceback.update_history(it+1, X=X, step_f=step_f)\n            if accelerated:\n                traceback.update_history(it+1, omega=omega)\n            if relax is not None:\n                traceback.update_history(it+1, relax=relax)\n\n        # test for fixed point convergence\n        converged = utils.l2sq(X - X_) <= e_rel**2*utils.l2sq(X)\n        if converged:\n            break\n\n    logger.info(\"Completed {0} iterations\".format(it+1))\n    if not converged:\n        logger.warning(\"Solution did not converge\")\n\n    return converged, X-X_", "response": "Proximal Gradient Method for Nesterov Optimization"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef admm(X, prox_f, step_f, prox_g=None, step_g=None, L=None, e_rel=1e-6, e_abs=0, max_iter=1000, traceback=None):\n\n    # use matrix adapter for convenient & fast notation\n    _L = utils.MatrixAdapter(L)\n    # get/check compatible step size for g\n    if prox_g is not None and step_g is None:\n        step_g = utils.get_step_g(step_f, _L.spectral_norm)\n\n    # init\n    Z,U = utils.initZU(X, _L)\n    it = 0\n\n    if traceback is not None:\n        traceback.update_history(it, X=X, step_f=step_f, Z=Z, U=U, R=np.zeros(Z.shape, dtype=Z.dtype), S=np.zeros(X.shape, dtype=X.dtype), step_g=step_g)\n\n    while it < max_iter:\n\n        # Update the variables, return LX and primal/dual residual\n        LX, R, S = utils.update_variables(X, Z, U, prox_f, step_f, prox_g, step_g, _L)\n\n        # Optionally store the variables in the history\n        if traceback is not None:\n            traceback.update_history(it+1, X=X, step_f=step_f, Z=Z, U=U, R=R, S=S,  step_g=step_g)\n\n        # convergence criteria, adapted from Boyd 2011, Sec 3.3.1\n        converged, error = utils.check_constraint_convergence(X, _L, LX, Z, U, R, S,\n                                                                step_f, step_g, e_rel, e_abs)\n\n        if converged:\n            break\n\n        it += 1\n\n        # if X and primal residual does not change: decrease step_f and step_g, and restart\n        if prox_g is not None:\n            if it > 1:\n                if (X == X_).all() and (R == R_).all():\n                    step_f /= 2\n                    step_g /= 2\n                    # re-init\n                    it = 0\n\n                    Z,U  = utils.initZU(X, _L)\n                    logger.info(\"Restarting with step_f = %.3f\" % step_f)\n                    if traceback is not None:\n                        traceback.reset()\n                        traceback.update_history(it, X=X, Z=Z, U=U, R=np.zeros(Z.shape, dtype=Z.dtype), S=np.zeros(X.shape, dtype=X.dtype), step_f=step_f, step_g=step_g)\n            X_ = X.copy()\n            R_ = R\n\n    logger.info(\"Completed {0} iterations\".format(it+1))\n    if not converged:\n        logger.warning(\"Solution did not converge\")\n\n    return converged, error", "response": "This method implements the linearized ADMM method of Multipliers."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sdmm(X, prox_f, step_f, proxs_g=None, steps_g=None, Ls=None, e_rel=1e-6, e_abs=0, max_iter=1000, traceback=None):\n\n    # fall-back to simple ADMM\n    if proxs_g is None or not hasattr(proxs_g, '__iter__'):\n        return admm(X, prox_f, step_f, prox_g=proxs_g, step_g=steps_g, L=Ls, e_rel=e_rel, max_iter=max_iter, traceback=traceback)\n\n    # from here on we know that proxs_g is a list\n    M = len(proxs_g)\n\n    # if steps_g / Ls are None or single: create M duplicates\n    if not hasattr(steps_g, \"__iter__\"):\n        steps_g = [steps_g] * M\n    if not hasattr(Ls, \"__iter__\"):\n        Ls = [Ls] * M\n    # check for cases in which a list was given\n    assert len(steps_g) == M\n    assert len(Ls) == M\n\n    # get/check compatible step sizes for g\n    # use matrix adapter for convenient & fast notation\n    _L = []\n    for i in range(M):\n        _L.append(utils.MatrixAdapter(Ls[i]))\n        # get/check compatible step size for g\n        if steps_g[i] is None:\n            steps_g[i] = utils.get_step_g(step_f, _L[i].spectral_norm, M=M)\n\n    # Initialization\n    Z,U = utils.initZU(X, _L)\n    it, omega = 0, 0\n\n    if traceback is not None:\n        traceback.update_history(it, X=X, step_f=step_f, omega=omega)\n        traceback.update_history(it, M=M, Z=Z, U=U, R=U, S=[np.zeros(X.shape, dtype=X.dtype) for n in range(M)], steps_g=steps_g)\n\n    while it < max_iter:\n\n        # update the variables\n        LX, R, S = utils.update_variables(X, Z, U, prox_f, step_f, proxs_g, steps_g, _L)\n\n        if traceback is not None:\n            traceback.update_history(it+1, X=X, step_f=step_f, omega=omega)\n            traceback.update_history(it+1, M=M, Z=Z, U=U, R=R, S=S, steps_g=steps_g)\n\n        # convergence criteria, adapted from Boyd 2011, Sec 3.3.1\n        converged, errors = utils.check_constraint_convergence(X, _L, LX, Z, U, R, S, step_f, steps_g,\n                                                                 e_rel, e_abs)\n\n        if converged:\n            break\n\n        it += 1\n\n        # if X and primal residual does not change: decrease step_f and step_g, and restart\n        if it > 1:\n            if (X == X_).all() and all([(R[i] == R_[i]).all() for i in range(M)]):\n                step_f /= 2\n                for i in range(M):\n                    steps_g[i] /= 2\n\n                # re-init\n                it = 0\n\n                Z,U  = utils.initZU(X, _L)\n                if traceback is not None:\n                    traceback.reset()\n                    traceback.update_history(it, X=X, step_f=step_f)\n                    traceback.update_history(it, M=M, Z=Z, U=U, R=U,\n                                      S=[np.zeros(X.shape, dtype=X.dtype) for n in range(M)], steps_g=steps_g)\n                logger.info(\"Restarting with step_f = %.3f\" % step_f)\n\n        R_ = R\n        X_ = X.copy()\n\n    logger.info(\"Completed {0} iterations\".format(it+1))\n    if not converged:\n        logger.warning(\"Solution did not converge\")\n\n    return converged, errors", "response": "This method is an extension of the linearized ADMM for multipliers."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef bpgm(X, proxs_f, steps_f_cb, update_order=None, accelerated=False, relax=None, max_iter=1000, e_rel=1e-6, traceback=None):\n    # Set up\n    N = len(X)\n    if np.isscalar(e_rel):\n        e_rel = [e_rel] * N\n\n    if relax is not None:\n        assert relax > 0 and relax < 1.5\n\n    if update_order is None:\n        update_order = range(N)\n    else:\n        # we could check that every component is in the list\n        # but one can think of cases when a component is *not* to be updated.\n        #assert len(update_order) == N\n        pass\n\n    # init\n    X_ = [None] * N\n    stepper = utils.NesterovStepper(accelerated=accelerated)\n\n    if traceback is not None:\n        for j in update_order:\n            traceback.update_history(0, j=j, X=X[j], steps_f=None)\n            if accelerated:\n                traceback.update_history(0, j=j, omega=0)\n            if relax is not None:\n                traceback.update_history(0, j=j, relax=relax)\n\n    for it in range(max_iter):\n        # use Nesterov acceleration (if omega > 0), automatically incremented\n        omega = stepper.omega\n\n        # iterate over blocks X_j\n        for j in update_order:\n\n            # tell prox the state of other variables\n            proxs_f_j = partial(proxs_f, j=j, Xs=X)\n            steps_f_j = steps_f_cb(j, X)\n\n            # acceleration?\n            # check for resizing: if resize ocurred, temporily skip acceleration\n            if omega > 0 and X[j].shape == X_[j].shape:\n                _X = X[j] + omega*(X[j] - X_[j])\n            else:\n                _X = X[j]\n\n            # keep copy for convergence test (and acceleration)\n            X_[j] = X[j].copy()\n\n            # PGM step, force inline update\n            X[j][:] = proxs_f_j(_X, steps_f_j)\n\n            if relax is not None:\n                X[j] += (relax-1)*(X[j] - X_[j])\n\n            if traceback is not None:\n                traceback.update_history(it+1, j=j, X=X[j], steps_f=steps_f_j)\n                if accelerated:\n                    traceback.update_history(it+1, j=j, omega=omega)\n                if relax is not None:\n                    traceback.update_history(it+1, j=j, relax=relax)\n\n        # test for fixed point convergence\n        # allowing for transparent resizing of X: need to check shape of X_\n        errors = [X[j] - X_[j] if X[j].shape == X_[j].shape else X[j] for j in range(N)]\n        converged = [utils.l2sq(errors[j]) <= e_rel[j]**2*utils.l2sq(X[j]) for j in range(N)]\n        if all(converged):\n            break\n\n    logger.info(\"Completed {0} iterations\".format(it+1))\n    if not all(converged):\n        logger.warning(\"Solution did not converge\")\n\n    return converged, errors", "response": "Block Proximal Gradient Method."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef bsdmm(X, proxs_f, steps_f_cb, proxs_g=None, steps_g=None, Ls=None, update_order=None, steps_g_update='steps_f', max_iter=1000, e_rel=1e-6, e_abs=0, traceback=None):\n\n    # Set up\n    N = len(X)\n    if proxs_g is None:\n        proxs_g = [None] * N\n    assert len(proxs_g) == N\n    assert steps_g_update.lower() in ['steps_f', 'fixed', 'relative']\n\n    if np.isscalar(e_rel):\n        e_rel = [e_rel] * N\n    if np.isscalar(e_abs):\n        e_abs = [e_abs] * N\n    steps_f = [None] * N\n\n    if update_order is None:\n        update_order = range(N)\n    else:\n        # we could check that every component is in the list\n        # but one can think of cases when a component is *not* to be updated.\n        #assert len(update_order) == N\n        pass\n\n    if steps_g_update.lower() == 'steps_f':\n        if steps_g is not None:\n            logger.debug(\"Setting steps_g = None for update strategy 'steps_f'.\")\n            steps_g = None\n    if steps_g_update.lower() in ['fixed', 'relative']:\n        if steps_g is None:\n            logger.debug(\"Ignoring steps_g update strategy '%s' because steps_g is None.\" % steps_g_update)\n            steps_g_update = 'steps_f'\n\n    # if steps_g / Ls are None or single: create N duplicates\n    if not hasattr(steps_g, \"__iter__\"):\n        steps_g = [steps_g] * N\n    if not hasattr(Ls, \"__iter__\"):\n        Ls = [Ls] * N\n    # check for cases in which a list was given\n    assert len(steps_g) == N\n    assert len(Ls) == N\n\n    M = [0] * N\n    for j in range(N):\n        if proxs_g[j] is not None:\n            if not hasattr(proxs_g[j], \"__iter__\"):\n                proxs_g[j] = [proxs_g[j]]\n            M[j] = len(proxs_g[j])\n            if not hasattr(steps_g[j], \"__iter__\"):\n                steps_g[j] = [steps_g[j]] * M[j]\n            if not hasattr(Ls[j], \"__iter__\"):\n                Ls[j] = [Ls[j]] * M[j]\n            assert len(steps_g[j]) == M[j]\n            assert len(Ls[j]) == M[j]\n\n    # need container for current-iteration steps_g and matrix adapters\n    steps_g_ = []\n    _L = []\n    for j in range(N):\n        if proxs_g[j] is None:\n            steps_g_.append(None)\n            _L.append(utils.MatrixAdapter(None))\n        else:\n            steps_g_.append([[None] for i in range(M[j])])\n            _L.append([ utils.MatrixAdapter(Ls[j][m]) for m in range(M[j])])\n\n    # Initialization\n    Z, U = [],[]\n    LX, R, S = [None] * N, [None] * N, [None] * N\n    for j in range(N):\n        Zj, Uj = utils.initZU(X[j], _L[j])\n        Z.append(Zj)\n        U.append(Uj)\n\n    # containers\n    converged, errors = [None] * N, [None] * N\n    slack = [1.] * N\n    it = 0\n\n    if traceback is not None:\n        for j in update_order:\n            if M[j]>0:\n                _S = [np.zeros(X[j].shape, dtype=X[j].dtype) for n in range(M[j])]\n            else:\n                _S = np.zeros(X[j].shape, dtype=X[j].dtype)\n            traceback.update_history(it, j=j, X=X[j], steps_f=steps_f[j])\n            traceback.update_history(it, j=j, M=M[j], steps_g=steps_g_[j], Z=Z[j], U=U[j],\n                              R=U[j],\n                              S=[np.zeros(X[j].shape, dtype=X[j].dtype) for n in range(M[j])])\n\n    while it < max_iter:\n\n        # iterate over blocks X_j\n        for j in update_order:\n            proxs_f_j = partial(proxs_f, j=j, Xs=X)\n            steps_f_j = steps_f_cb(j, X) * slack[j]\n\n            # update steps_g relative to change of steps_f ...\n            if steps_g_update.lower() == 'relative':\n                for i in range(M[j]):\n                    steps_g[j][i] *= steps_f_j / steps_f[j]\n            steps_f[j] = steps_f_j\n            # ... or update them as required by the most conservative limit\n            if steps_g_update.lower() == 'steps_f':\n                for i in range(M[j]):\n                    steps_g_[j][i] = utils.get_step_g(steps_f[j], _L[j][i].spectral_norm, N=N, M=M[j])\n\n            # update the variables\n            LX[j], R[j], S[j] = utils.update_variables(X[j], Z[j], U[j], proxs_f_j, steps_f[j], proxs_g[j], steps_g_[j], _L[j])\n\n            # convergence criteria, adapted from Boyd 2011, Sec 3.3.1\n            converged[j], errors[j] = utils.check_constraint_convergence(X[j], _L[j], LX[j], Z[j], U[j],\n                R[j], S[j], steps_f[j],steps_g_[j],e_rel[j], e_abs[j])\n            # Optionally update the new state\n            if traceback is not None:\n                traceback.update_history(it+1, j=j, X=X[j], steps_f=steps_f[j])\n                traceback.update_history(it+1, j=j, M=M[j], steps_g=steps_g_[j], Z=Z[j], U=U[j], R=R[j], S=S[j])\n\n        if all(converged):\n            break\n        it += 1\n\n    logger.info(\"Completed {0} iterations\".format(it+1))\n    if not all(converged):\n        logger.warning(\"Solution did not converge\")\n\n    return converged, errors", "response": "Block - Simultaneous Method of Multipliers."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_step_f(step_f, lR2, lS2):\n    mu, tau = 10, 2\n    if lR2 > mu*lS2:\n        return step_f * tau\n    elif lS2 > mu*lR2:\n        return step_f / tau\n    return step_f", "response": "Update the stepsize of given the primal and dual errors."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update_variables(X, Z, U, prox_f, step_f, prox_g, step_g, L):\n    if not hasattr(prox_g, '__iter__'):\n        if prox_g is not None:\n            dX = step_f/step_g * L.T.dot(L.dot(X) - Z + U)\n            X[:] = prox_f(X - dX, step_f)\n            LX, R, S = do_the_mm(X, step_f, Z, U, prox_g, step_g, L)\n        else:\n            # fall back to simple fixed-point method for f\n            # see do_the_mm for normal definitions of LX,Z,R,S\n            S = -X.copy()\n            X[:] = prox_f(X, step_f)\n            LX = X\n            Z[:] = X[:]\n            R = np.zeros(X.shape, dtype=X.dtype)\n            S += X\n\n    else:\n        M = len(prox_g)\n        dX = np.sum([step_f/step_g[i] * L[i].T.dot(L[i].dot(X) - Z[i] + U[i]) for i in range(M)], axis=0)\n        X[:] = prox_f(X - dX, step_f)\n        LX = [None] * M\n        R = [None] * M\n        S = [None] * M\n        for i in range(M):\n            LX[i], R[i], S[i] = do_the_mm(X, step_f, Z[i], U[i], prox_g[i], step_g[i], L[i])\n    return LX, R, S", "response": "Update the primal and dual variables of the log - likelihood and dual variables of the log - likelihood and dual variables of the log - likelihood."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_variable_errors(X, L, LX, Z, U, step_g, e_rel, e_abs=0):\n    n = X.size\n    p = Z.size\n    e_pri2 = np.sqrt(p)*e_abs/L.spectral_norm + e_rel*np.max([l2(LX), l2(Z)])\n    if step_g is not None:\n        e_dual2 = np.sqrt(n)*e_abs/L.spectral_norm + e_rel*l2(L.T.dot(U)/step_g)\n    else:\n        e_dual2 = np.sqrt(n)*e_abs/L.spectral_norm + e_rel*l2(L.T.dot(U))\n    return e_pri2, e_dual2", "response": "Get the errors in a single multiplier method step\n\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_constraint_convergence(X, L, LX, Z, U, R, S, step_f, step_g, e_rel, e_abs):\n\n    if isinstance(L, list):\n        M = len(L)\n        convergence = True\n        errors = []\n        # recursive call\n        for i in range(M):\n            c, e = check_constraint_convergence(X, L[i], LX[i], Z[i], U[i], R[i], S[i],\n                                                step_f, step_g[i], e_rel, e_abs)\n            convergence &= c\n            errors.append(e)\n        return convergence, errors\n    else:\n        # check convergence of prime residual R and dual residual S\n        e_pri, e_dual = get_variable_errors(X, L, LX, Z, U, step_g, e_rel, e_abs)\n        lR = l2(R)\n        lS = l2(S)\n        convergence = (lR <= e_pri) and (lS <= e_dual)\n        return convergence, (e_pri, e_dual, lR, lS)", "response": "Calculate if all constraints have converged."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_convergence(newX, oldX, e_rel):\n    # Calculate the norm for columns and rows, which can be used for debugging\n    # Otherwise skip, since it takes extra processing time\n    new_old = newX*oldX\n    old2 = oldX**2\n    norms = [np.sum(new_old), np.sum(old2)]\n    convergent = norms[0] >= (1-e_rel**2)*norms[1]\n    return convergent, norms", "response": "Checks that the algorithm converges using Langville 2014 criteria\n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstoring a copy of the variable in the history", "response": "def _store_variable(self, j, key, m, value):\n        \"\"\"Store a copy of the variable in the history\n        \"\"\"\n        if hasattr(value, 'copy'):\n            v = value.copy()\n        else:\n            v = value\n\n        self.history[j][key][m].append(v)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_history(self, it, j=0, M=None, **kwargs):\n        # Create a new entry in the history for new variables (if they don't exist)\n        if not np.any([k in self.history[j] for k in kwargs]):\n            for k in kwargs:\n                if M is None or M == 0:\n                    self.history[j][k] = [[]]\n                else:\n                    self.history[j][k] = [[] for m in range(M)]\n        \"\"\"\n        # Check that the variables have been updated once per iteration\n        elif np.any([[len(h)!=it+self.offset for h in self.history[j][k]] for k in kwargs.keys()]):\n            for k in kwargs.keys():\n                for n,h in enumerate(self.history[j][k]):\n                    if len(h) != it+self.offset:\n                        err_str = \"At iteration {0}, {1}[{2}] already has {3} entries\"\n                        raise Exception(err_str.format(it, k, n, len(h)-self.offset))\n        \"\"\"\n        # Add the variables to the history\n        for k,v in kwargs.items():\n            if M is None or M == 0:\n                self._store_variable(j, k, 0, v)\n            else:\n                for m in range(M):\n                    self._store_variable(j, k, m, v[m])", "response": "Update the history for the current state of the variable at the given iteration."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating oscillating components to be mixed", "response": "def generateComponent(m):\r\n    \"\"\"Creates oscillating components to be mixed\"\"\"\r\n    freq = 25*np.random.random()\r\n    phase = 2*np.pi*np.random.random()\r\n    x = np.arange(m)\r\n    return np.cos(x/freq-phase)**2"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_noise(Y, sigma):\r\n    return Y + np.random.normal(0, sigma, Y.shape)", "response": "Adds noise to Y"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef match(A, S, trueS):\r\n    cov = np.cov(trueS, S)\r\n    k = S.shape[0]\r\n    corr = np.zeros([k,k])\r\n    for i in range(k):\r\n        for j in range(k):\r\n            corr[i][j] = cov[i + k][j]/np.sqrt(cov[i + k][i + k]*cov[j][j])\r\n    arrangement = linear_sum_assignment(-corr)\r\n    resS = np.zeros_like(S)\r\n    resAT = np.zeros_like(A.T)\r\n    for t in range(k):\r\n        resS[arrangement[1][t]] = S[arrangement[0][t]]\r\n        resAT[arrangement[1][t]] = A.T[arrangement[0][t]]\r\n    return resAT.T, resS", "response": "Rearranges columns of A to best fit the components they likely represent"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef nmf(Y, A, S, W=None, prox_A=operators.prox_plus, prox_S=operators.prox_plus, proxs_g=None, steps_g=None, Ls=None, slack=0.9, update_order=None, steps_g_update='steps_f', max_iter=1000, e_rel=1e-3, e_abs=0, traceback=None):\n\n    # create stepsize callback, needs max of W\n    if W is not None:\n        # normalize in pixel and band directions to have similar update speeds\n        WA = normalizeMatrix(W, 1)\n        WS = normalizeMatrix(W, 0)\n    else:\n        WA = WS = 1\n    steps_f = Steps_AS(WA=WA, WS=WS, slack=slack)\n\n    # gradient step, followed by direct application of prox_S or prox_A\n    from functools import partial\n    f = partial(prox_likelihood, Y=Y, WA=WA, WS=WS, prox_S=prox_S, prox_A=prox_A)\n\n    X = [A, S]\n    # use accelerated block-PGM if there's no proxs_g\n    if proxs_g is None or not utils.hasNotNone(proxs_g):\n        return algorithms.bpgm(X, f, steps_f, accelerated=True, update_order=update_order, max_iter=max_iter, e_rel=e_rel, traceback=traceback)\n    else:\n        return algorithms.bsdmm(X, f, steps_f, proxs_g, steps_g=steps_g, Ls=Ls, update_order=update_order, steps_g_update=steps_g_update, max_iter=max_iter, e_rel=e_rel, e_abs=e_abs, traceback=traceback)", "response": "This method solves the non - negative matrix factorization problem for A and S."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef prox_zero(X, step):\n    return np.zeros(X.shape, dtype=X.dtype)", "response": "Proximal operator to project onto zero"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef prox_components(X, step, prox=None, axis=0):\n    K = X.shape[axis]\n\n    if not hasattr(prox_list, '__iter__'):\n        prox = [prox] * K\n    assert len(prox_list) == K\n\n    if axis == 0:\n        Pk = [prox_list[k](X[k], step) for k in range(K)]\n    if axis == 1:\n        Pk = [prox_list[k](X[:,k], step) for k in range(K)]\n    return np.stack(Pk, axis=axis)", "response": "Split X along axis and apply prox to each chunk."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef prox_hard(X, step, thresh=0):\n    thresh_ = _step_gamma(step, thresh)\n    below = np.abs(X) < thresh_\n    X[below] = 0\n    return X", "response": "Hard thresholding\n    X if |X| < thresh otherwise 0"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef prox_hard_plus(X, step, thresh=0):\n    return prox_plus(prox_hard(X, step, thresh=thresh), step)", "response": "Hard thresholding with projection onto non - negative numbers."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef prox_soft(X, step, thresh=0):\n    thresh_ = _step_gamma(step, thresh)\n    return np.sign(X)*prox_plus(np.abs(X) - thresh_, step)", "response": "Soft thresholding proximal operator"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef prox_max_entropy(X, step, gamma=1):\n    from scipy.special import lambertw\n    gamma_ = _step_gamma(step, gamma)\n    # minimize entropy: return gamma_ * np.real(lambertw(np.exp((X - gamma_) / gamma_) / gamma_))\n    above = X > 0\n    X[above] = gamma_ * np.real(lambertw(np.exp(X[above]/gamma_ - 1) / gamma_))\n    return X", "response": "Proximal operator for maximum entropy regularization."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncalculate the gradient in the y direction to the line at py", "response": "def get_gradient_y(shape, py):\n    \"\"\"Calculate the gradient in the y direction to the line at py\n\n    The y gradient operator is a block matrix, where each block is the size of the image width.\n    The matrix itself is made up of (img_height x img_height) blocks, most of which are all zeros.\n    \"\"\"\n    import scipy.sparse\n\n    height, width = shape\n    rows = []\n    empty = scipy.sparse.dia_matrix((width, width))\n    identity = scipy.sparse.identity(width)\n\n    # Create the blocks by row, beginning with blocks leading up to the peak row from the top\n    for n in range(py):\n        row = [empty]*n\n        row += [-identity, identity]\n        row += [empty]*(height-n-2)\n        rows.append(row)\n    # Set all elements in the peak row to zero\n    rows.append([empty]*height)\n    # Create the blocks for the rows leading up to the peak row from the bottom\n    for n in range(height-py-1):\n        row = [empty]*(py+n)\n        row += [identity, -identity]\n        row += [empty]*(height-py-n-2)\n        rows.append(row)\n    return scipy.sparse.bmat(rows)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate the gradient in the x direction to the line at px", "response": "def get_gradient_x(shape, px):\n    \"\"\"Calculate the gradient in the x direction to the line at px\n\n    The y gradient operator is a block diagonal matrix, where each block is the size of the image width.\n    The matrix itself is made up of (img_height x img_height) blocks, most of which are all zeros.\n    \"\"\"\n    import scipy.sparse\n\n    height, width = shape\n    size = height * width\n\n    # Set the diagonal to -1, except for the value at the peak, which is zero\n    c = -np.ones((width,))\n    c[px] = 0\n    # Set the pixels leading up to the peak from the left\n    r = np.zeros(c.shape, dtype=c.dtype)\n    r[:px] = 1\n    # Set the pixels leading up to the peak from the right\n    l = np.zeros(c.shape, dtype=c.dtype)\n    l[px:] = 1\n    # Make a block for a single row in the image\n    block = scipy.sparse.diags([l, c, r], [-1, 0,1], shape=(width,width))\n    # Use the same block for each row\n    op = scipy.sparse.block_diag([block for n in range(height)])\n    return op"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading the given path into the tree.", "response": "def read_dir(self, path):\n        \"\"\"\n        Reads the given path into the tree\n        \"\"\"\n        self.tree = {}\n        self.file_count = 0\n        self.path = path\n\n        for root, _, filelist in os.walk(path):\n            rel = root[len(path):].lstrip('/\\\\')\n\n            # empty rel, means file is in root dir\n            if not rel:\n                rel = ' '\n\n            for filename in filelist:\n                filename = filename.split('.')\n                if len(filename) <= 1:\n                    raise RuntimeError(\"Files without an extension are not supported: {0}\".format(\n                                       repr(os.path.join(root, '.'.join(filename))),\n                                       ))\n\n                ext = filename[-1]\n                filename = '.'.join(filename[:-1])\n\n                if ext not in self.tree:\n                    self.tree[ext] = {}\n                if rel not in self.tree[ext]:\n                    self.tree[ext][rel] = []\n\n                self.tree[ext][rel].append(filename)\n                self.file_count += 1\n\n        self.tree_length = self.calculate_tree_length()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef calculate_tree_length(self):\n        tree_length = 0\n\n        for ext in self.tree:\n            tree_length += len(ext) + 2\n\n            for relpath in self.tree[ext]:\n                tree_length += len(relpath) + 2\n\n                for filename in self.tree[ext][relpath]:\n                    tree_length += len(filename) + 1 + 18\n\n        return tree_length + 1", "response": "Calculates the length of the tree"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsave the VPK at the given path.", "response": "def save(self, vpk_output_path):\n        \"\"\"\n        Saves the VPK at the given path\n        \"\"\"\n        with fopen(vpk_output_path, 'wb') as f:\n            # write VPK1 header\n            f.write(struct.pack(\"3I\", self.signature,\n                                      self.version,\n                                      self.tree_length))\n\n            self.header_length = f.tell()\n\n            data_offset = self.header_length + self.tree_length\n\n            # write file tree\n            for ext in self.tree:\n                f.write(\"{0}\\x00\".format(ext).encode('latin-1'))\n\n                for relpath in self.tree[ext]:\n                    f.write(\"{0}\\x00\".format(relpath).encode('latin-1'))\n\n                    for filename in self.tree[ext][relpath]:\n                        f.write(\"{0}\\x00\".format(filename).encode('latin-1'))\n\n                        # append file data\n                        metadata_offset = f.tell()\n                        file_offset = data_offset\n                        real_filename = filename if not ext else \"{0}.{1}\".format(filename, ext)\n                        checksum = 0\n                        f.seek(data_offset)\n\n                        with fopen(os.path.join(self.path,\n                                                '' if relpath == ' ' else relpath,\n                                                real_filename\n                                                ),\n                                   'rb') as pakfile:\n                            for chunk in iter(lambda: pakfile.read(1024), b''):\n                                checksum = crc32(chunk, checksum)\n                                f.write(chunk)\n\n                        data_offset = f.tell()\n                        file_length = f.tell() - file_offset\n                        f.seek(metadata_offset)\n\n                        # metadata\n\n                        # crc32\n                        # preload_length\n                        # archive_index\n                        # archive_offset\n                        # file_length\n                        # suffix\n                        f.write(struct.pack(\"IHHIIH\", checksum & 0xFFffFFff,\n                                                      0,\n                                                      0x7fff,\n                                                      file_offset - self.tree_length - self.header_length,\n                                                      file_length,\n                                                      0xffff\n                                                      ))\n\n\n                    # next relpath\n                    f.write(b\"\\x00\")\n                # next ext\n                f.write(b\"\\x00\")\n            # end of file tree\n            f.write(b\"\\x00\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a VPKFile instance for the given path", "response": "def get_file(self, path):\n        \"\"\"\n        Returns VPKFile instance for the given path\n        \"\"\"\n        metadata = self.get_file_meta(path)\n        return self.get_vpkfile_instance(path, metadata)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn metadata for given file path", "response": "def get_file_meta(self, path):\n        \"\"\"\n        Returns metadata for given file path\n        \"\"\"\n        if self.tree is None:\n            self.read_index()\n\n        if path not in self.tree:\n            raise KeyError(\"Path doesn't exist\")\n\n        return self._make_meta_dict(self.tree[path])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read_header(self):\n        with fopen(self.vpk_path, 'rb') as f:\n            (self.signature,\n             self.version,\n             self.tree_length\n             ) = struct.unpack(\"3I\", f.read(3*4))\n\n            # original format - headerless\n            if self.signature != 0x55aa1234:\n                raise ValueError(\"File is not VPK (invalid magic)\")\n            # v1\n            elif self.version == 1:\n                self.header_length += 4*3\n            # v2 with extended header\n            #\n            # according to http://forum.xentax.com/viewtopic.php?f=10&t=11208\n            # struct VPKDirHeader_t\n            # {\n            #    int32 m_nHeaderMarker;\n            #    int32 m_nVersion;\n            #    int32 m_nDirectorySize;\n            #    int32 m_nEmbeddedChunkSize;\n            #    int32 m_nChunkHashesSize;\n            #    int32 m_nSelfHashesSize;\n            #    int32 m_nSignatureSize;\n            # }\n            elif self.version == 2:\n                (self.embed_chunk_length,\n                 self.chunk_hashes_length,\n                 self.self_hashes_length,\n                 self.signature_length\n                 ) = struct.unpack(\"4I\", f.read(4*4))\n                self.header_length += 4*7\n\n                f.seek(self.tree_length + self.embed_chunk_length + self.chunk_hashes_length, 1)\n\n                assert self.self_hashes_length == 48, \"Self hashes section size mismatch\"\n\n                (self.tree_checksum,\n                 self.chunk_hashes_checksum,\n                 self.file_checksum,\n                 ) = struct.unpack(\"16s16s16s\", f.read(16*3))\n            else:\n                raise ValueError(\"Invalid header, or unsupported version\")", "response": "Reads the VPK file header from the file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread the index and populates the directory tree with the metadata", "response": "def read_index(self):\n        \"\"\"\n        Reads the index and populates the directory tree\n        \"\"\"\n        if not isinstance(self.tree, dict):\n            self.tree = dict()\n\n        self.tree.clear()\n\n        for path, metadata in self.read_index_iter():\n            self.tree[path] = metadata"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read_index_iter(self):\n\n        with fopen(self.vpk_path, 'rb') as f:\n            f.seek(self.header_length)\n\n            while True:\n                if self.version > 0 and f.tell() > self.tree_length + self.header_length:\n                    raise ValueError(\"Error parsing index (out of bounds)\")\n\n                ext = _read_cstring(f)\n                if ext == '':\n                    break\n\n                while True:\n                    path = _read_cstring(f)\n                    if path == '':\n                        break\n                    if path != ' ':\n                        path = os.path.join(path, '')\n                    else:\n                        path = ''\n\n                    while True:\n                        name = _read_cstring(f)\n                        if name == '':\n                            break\n\n                        (crc32,\n                         preload_length,\n                         archive_index,\n                         archive_offset,\n                         file_length,\n                         suffix,\n                         ) = metadata = list(struct.unpack(\"IHHIIH\", f.read(18)))\n\n                        if suffix != 0xffff:\n                            raise ValueError(\"Error while parsing index\")\n\n                        if archive_index == 0x7fff:\n                            metadata[3] = self.header_length + self.tree_length + archive_offset\n\n                        metadata = (f.read(preload_length),) + tuple(metadata[:-1])\n\n                        yield path + name + '.' + ext, metadata", "response": "Generator function that reads the file index from the vpk file and yields the file name and metadata."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef save(self, path):\n        # remember and restore file position\n        pos = self.tell()\n        self.seek(0)\n\n        with fopen(path, 'wb') as output:\n            output.truncate(self.length)\n            for chunk in iter(lambda: self.read(1024), b''):\n                output.write(chunk)\n\n        self.seek(pos)", "response": "Save the file to the specified path."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nverify the CRC32 attribute of the current file.", "response": "def verify(self):\n        \"\"\"\n        Returns True if the file contents match with the CRC32 attribute\n\n        note: reset\n        \"\"\"\n\n        # remember file pointer\n        pos = self.tell()\n        self.seek(0)\n\n        checksum = 0\n        for chunk in iter(lambda: self.read(1024), b''):\n            checksum = crc32(chunk, checksum)\n\n        # restore file pointer\n        self.seek(pos)\n\n        return self.crc32 == checksum & 0xffffffff"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef profile(ctx, filepath, calltree=False):\n\n    filepath = pathlib.Path(filepath)\n    if not filepath.is_file():\n        report.error(ctx, \"profile\", f\"no such script {filepath!s}\")\n    else:\n        if calltree:\n            report.info(ctx, \"profile\", f\"profiling script {filepath!s} calltree\")\n            ctx.run(\n                (\n                    f\"python -m cProfile -o .profile.cprof {filepath!s}\"\n                    \" && pyprof2calltree -k -i .profile.cprof\"\n                    \" && rm -rf .profile.cprof\"\n                )\n            )\n        else:\n            report.info(ctx, \"profile\", f\"profiling script {filepath!s}\")\n            ctx.run(f\"vprof -c cmhp {filepath!s}\")", "response": "Run and profile a given Python script."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef publish(ctx, test=False, force=False, draft=False):\n\n    previous_version = get_previous_version(ctx)\n    current_version = parver.Version.parse(metadata[\"version\"])\n\n    if current_version <= previous_version and not force:\n        error_message = (\n            f\"current version ({current_version!s}) is <= to previous version \"\n            f\"({previous_version!s}), use 'package.version' to update current version\"\n        )\n        report.error(ctx, \"publish\", error_message)\n        raise ValueError(error_message)\n\n    report.info(ctx, \"publish\", f\"publishing project {ctx.metadata['name']!r}\")\n    report.warning(\n        ctx,\n        \"publish\",\n        f\"drafting publish for project {ctx.metadata['name']!r} (has no effect)\",\n    )\n\n    commit_message = f\"Release {current_version!s}\"\n    report.info(ctx, \"publish\", f\"git commiting release {commit_message!r}\")\n    git_commit_command = f\"git commit -asm {commit_message!r}\"\n    if not draft:\n        ctx.run(git_commit_command)\n\n    tag_content = get_tag_content(ctx).replace('\"', '\\\\\"')\n    git_tag_command = (\n        f'git tag -a \"v{current_version!s}\" -m '\n        f'\"Version {current_version!s}\\n\\n{tag_content}\"'\n    )\n    report.info(\n        ctx, \"publish\", f\"git tagging commit as release for version {current_version!s}\"\n    )\n    if not draft:\n        ctx.run(git_tag_command)\n\n    artifact_paths = [f\"{_.as_posix()!r}\" for _ in get_artifact_paths(ctx)]\n    for artifact_path in artifact_paths:\n        report.debug(ctx, \"publish\", f\"publishing artifact {artifact_path}\")\n    publish_command = f\"twine upload {' '.join(artifact_paths)}\"\n    if test:\n        publish_command += \" --repository 'https://test.pypi.org/legacy/'\"\n\n    # get user to confirm publish\n    try:\n        input(\n            report._get_text(\n                ctx,\n                \"success\",\n                \"publish\",\n                \"about to publish, [Enter] to continue, [Ctrl-C] to abort: \",\n            )\n        )\n\n        while True:\n            (username, password) = get_username_password(\n                ctx, \"PyPi Username: \", \"PyPi Password: \"\n            )\n            # TODO: check if username and password are valid before tyring to post\n            report.info(ctx, \"publish\", f\"publishing project {ctx.metadata['name']!s}\")\n            if not draft:\n                publish_command += f\" -u {username!r} -p {password!r}\"\n                publish_result = ctx.run(publish_command, warn=True)\n                if publish_result.exited:\n                    report.error(\n                        ctx,\n                        \"publish\",\n                        f\"failed to publish {ctx.metadata['name']!s} (retrying)\",\n                    )\n                    continue\n            break\n\n        git_push_command = \"git push --tags\"\n        report.info(ctx, \"publish\", f\"pushing git tags\")\n        if not draft:\n            ctx.run(git_push_command)\n    except KeyboardInterrupt:\n        print()\n        report.error(ctx, \"publish\", \"aborting publish!\")\n        git_remove_tag_command = f\"git tag -d {current_version!s}\"\n        report.warn(ctx, \"publish\", \"removing git tags\")\n        if not draft:\n            ctx.run(git_remove_tag_command)\n        git_reset_command = f\"git reset --soft HEAD^\"\n        report.warn(ctx, \"publish\", \"softly reseting commit\")\n        if not draft:\n            ctx.run(git_reset_command)", "response": "Publish the project.\n\n    :param bool test: Publishes to PyPi test server (defaults to False)\n    :param bool force: Skip version check (defaults to False)\n    :param bool draft: Sample publish (has no effect) (defaults to False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef on_configparser_dumps(self, configparser, config, dictionary, **kwargs):\n\n        root_section = kwargs.pop(\"root\")\n        if not isinstance(root_section, str):\n            root_section = config.__name__\n\n        delimiter = kwargs.pop(\"delimiter\", \":\")\n        if delimiter in root_section:\n            warnings.warn(\n                f\"root section {root_section!r} contains delimiter character \"\n                f\"{delimiter!r}, loading from the resulting content will likely fail\"\n            )\n\n        try:\n            return INIParser.from_dict(\n                dictionary,\n                root_section=root_section,\n                delimiter=kwargs.pop(\"delimiter\", \":\"),\n                empty_sections=kwargs.pop(\"empty_sections\", False),\n            ).to_ini()\n        except ValueError:\n            raise ValueError(\"INI cannot handle this config, try using toml instead\")", "response": "The : mod : configparser dumps method."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef on_configparser_loads(self, configparser, config, content, **kwargs):\n\n        return INIParser.from_ini(content).to_dict(\n            delimiter=kwargs.pop(\"delimiter\", \":\")\n        )", "response": "The :mod:`configparser` loads method.\n\n        :param module configparser: The ``configparser`` module\n        :param class config: The loading config class\n        :param str content: The content to deserialize\n        :return: The deserialized dictionary\n        :rtype: dict"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _handle_dumps(self, handler, **kwargs):\n\n    return handler.dumps(self.__class__, to_dict(self), **kwargs)", "response": "Dumps the object to a string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _handle_dump(self, handler, file_object, **kwargs):\n\n    return handler.dump(self.__class__, to_dict(self), file_object, **kwargs)", "response": "Dump the object to a file object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload caller used by dynamic handler assignments.", "response": "def _handle_loads(cls, handler, content, validate=False, **kwargs):\n    \"\"\" Loads caller, used by partial method for dynamic handler assignments.\n\n    :param object handler: The loads handler\n    :param str content: The content to load from\n    :param bool validate: Performs content validation before loading,\n        defaults to False, optional\n    :return: The loaded instance\n    :rtype: object\n    \"\"\"\n\n    return from_dict(cls, handler.loads(cls, content, **kwargs), validate=validate)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _handle_load(cls, handler, file_object, validate=False, **kwargs):\n\n    return from_dict(cls, handler.load(cls, file_object, **kwargs), validate=validate)", "response": "Loads the object from file_object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef config(maybe_cls=None, these=None, title=None, description=None):\n\n    def wrap(config_cls):\n        \"\"\" The wrapper function.\n\n        :param class config_cls: The class to wrap\n        :return: The config_cls wrapper\n        :rtype: class\n        \"\"\"\n\n        setattr(config_cls, CONFIG_KEY, dict(title=title, description=description))\n        # dynamically assign available handlers to the wrapped class\n        for handler_name in handlers.__all__:\n            handler = getattr(handlers, handler_name)\n            if handler.available:\n                handler = handler()\n                setattr(\n                    config_cls,\n                    f\"dumps_{handler.name}\",\n                    partialmethod(_handle_dumps, handler),\n                )\n                setattr(\n                    config_cls,\n                    f\"dump_{handler.name}\",\n                    partialmethod(_handle_dump, handler),\n                )\n                setattr(\n                    config_cls,\n                    f\"loads_{handler.name}\",\n                    partialmethod(_handle_loads, handler),\n                )\n                setattr(\n                    config_cls,\n                    f\"load_{handler.name}\",\n                    partialmethod(_handle_load, handler),\n                )\n        config_vars = these if isinstance(these, dict) else None\n        return attr.s(config_cls, these=config_vars, slots=True)\n\n    if maybe_cls is None:\n        return wrap\n    else:\n        return wrap(maybe_cls)", "response": "A file config class decorator."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a config variable.", "response": "def var(\n    type=None,  # noqa\n    default=None,\n    name=None,\n    title=None,\n    description=None,\n    required=True,\n    examples=None,\n    encoder=None,\n    decoder=None,\n    min=None,  # noqa\n    max=None,  # noqa\n    unique=None,\n    contains=None,\n    **kwargs,\n):\n    \"\"\" Creates a config variable.\n\n    Use this method to create the class variables of your\n    :func:`config <file_config._file_config.config>` decorated class.\n\n    >>> import file_config\n    >>> @file_config.config\n        class MyConfig(object):\n            name = file_config.var(str)\n\n    :param type type: The expected type of the variable, defaults to None, optional\n    :param default: The default value of the var, defaults to None, optional\n    :param str name: The serialized name of the variable, defaults to None, optional\n    :param str title: The validation title of the variable, defaults to None, optional\n    :param str description: The validation description of the variable,\n        defaults to None, optional\n    :param bool required: Flag to indicate if variable is required during validation,\n        defaults to True, optional\n    :param list examples: A list of validation examples, if necessary,\n        defaults to None, optional\n    :param encoder: The encoder to use for the var, defaults to None, optional\n    :param decoder: The decoder to use for the var, defaults to None, optional\n    :param int min: The minimum constraint of the variable, defaults to None, optional\n    :param int max: The maximum constraint of the variable, defaults to None, optional\n    :param bool unique: Flag to indicate if variable should be unique,\n        may not apply to all variable types, defaults to None, optional\n    :param contains: Value that list varaible should contain in validation,\n        may not apply to all variable types, defaults to None, optional\n    :return: A new config variable\n    :rtype: attr.Attribute\n    \"\"\"\n\n    # NOTE: this method overrides some of the builtin Python method names on purpose in\n    # order to supply a readable and easy to understand api\n    # In this case it is not dangerous as they are only overriden in the scope and are\n    # never used within the scope\n    kwargs.update(dict(default=default, type=type))\n    return attr.ib(\n        metadata={\n            CONFIG_KEY: _ConfigEntry(\n                type=type,\n                default=default,\n                name=name,\n                title=title,\n                description=description,\n                required=required,\n                examples=examples,\n                encoder=encoder,\n                decoder=decoder,\n                min=min,\n                max=max,\n                unique=unique,\n                contains=contains,\n            )\n        },\n        **kwargs,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef make_config(name, var_dict, title=None, description=None, **kwargs):\n\n    return config(\n        attr.make_class(name, attrs={}, **kwargs),\n        these=var_dict,\n        title=title,\n        description=description,\n    )", "response": "Creates a new config instance from scratch."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nbuilding an instance of config_cls from a dictionary.", "response": "def _build(config_cls, dictionary, validate=False):  # noqa\n    \"\"\" Builds an instance of ``config_cls`` using ``dictionary``.\n\n    :param type config_cls: The class to use for building\n    :param dict dictionary: The dictionary to use for building ``config_cls``\n    :param bool validate: Performs validation before building ``config_cls``,\n        defaults to False, optional\n    :return: An instance of ``config_cls``\n    :rtype: object\n    \"\"\"\n\n    if not is_config_type(config_cls):\n        raise ValueError(\n            f\"cannot build {config_cls!r} from {dictionary!r}, \"\n            f\"{config_cls!r} is not a config\"\n        )\n\n    # perform jsonschema validation on the given dictionary\n    # (simplifys dynamic typecasting)\n    if validate:\n        jsonschema.validate(dictionary, build_schema(config_cls))\n\n    kwargs = {}\n    for var in attr.fields(config_cls):\n        if not is_config_var(var):\n            continue\n\n        entry = var.metadata[CONFIG_KEY]\n        arg_key = entry.name if entry.name else var.name\n        arg_default = var.default if var.default is not None else None\n\n        if callable(entry.decoder):\n            kwargs[var.name] = entry.decoder(dictionary.get(arg_key, arg_default))\n            continue\n\n        if is_array_type(entry.type):\n            if is_typing_type(entry.type) and len(entry.type.__args__) > 0:\n                nested_type = entry.type.__args__[0]\n                if is_config_type(nested_type):\n                    kwargs[var.name] = [\n                        _build(nested_type, item)\n                        for item in dictionary.get(arg_key, [])\n                    ]\n                else:\n                    kwargs[var.name] = typecast(entry.type, dictionary.get(arg_key, []))\n        elif is_object_type(entry.type):\n            item = dictionary.get(arg_key, {})\n            if is_typing_type(entry.type) and len(entry.type.__args__) == 2:\n                (_, value_type) = entry.type.__args__\n                kwargs[var.name] = {\n                    key: _build(value_type, value)\n                    if is_config_type(value_type)\n                    else typecast(value_type, value)\n                    for (key, value) in item.items()\n                }\n            else:\n                kwargs[var.name] = typecast(entry.type, item)\n        elif is_config_type(entry.type):\n            if arg_key not in dictionary:\n                # if the default value for a nested config is the nested config class\n                # then build the empty state of the nested config\n                if is_config_type(arg_default) and entry.type == arg_default:\n                    kwargs[var.name] = _build(entry.type, {})\n                else:\n                    kwargs[var.name] = arg_default\n            else:\n                kwargs[var.name] = _build(\n                    entry.type, dictionary.get(arg_key, arg_default)\n                )\n        else:\n            if arg_key not in dictionary:\n                kwargs[var.name] = arg_default\n            else:\n                kwargs[var.name] = typecast(\n                    entry.type, dictionary.get(arg_key, arg_default)\n                )\n\n    return config_cls(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _dump(config_instance, dict_type=OrderedDict):\n\n    if not is_config(config_instance):\n        raise ValueError(\n            f\"cannot dump instance {config_instance!r} to dict, \"\n            \"instance is not a config class\"\n        )\n\n    result = dict_type()\n    for var in attr.fields(config_instance.__class__):\n        if not is_config_var(var):\n            continue\n\n        entry = var.metadata[CONFIG_KEY]\n        dump_key = entry.name if entry.name else var.name\n        dump_default = var.default if var.default else None\n\n        if callable(entry.encoder):\n            result[dump_key] = entry.encoder(\n                getattr(config_instance, var.name, dump_default)\n            )\n            continue\n\n        if is_array_type(entry.type):\n            items = getattr(config_instance, var.name, [])\n            if items is not None:\n                result[dump_key] = [\n                    (_dump(item, dict_type=dict_type) if is_config(item) else item)\n                    for item in items\n                ]\n        elif is_enum_type(entry.type):\n            dump_value = getattr(config_instance, var.name, dump_default)\n            result[dump_key] = (\n                dump_value.value if dump_value in entry.type else dump_value\n            )\n        elif is_bytes_type(entry.type):\n            result[dump_key] = encode_bytes(\n                getattr(config_instance, var.name, dump_default)\n            )\n        else:\n            if is_config_type(entry.type):\n                result[dump_key] = _dump(\n                    getattr(config_instance, var.name, {}), dict_type=dict_type\n                )\n            else:\n                dump_value = getattr(config_instance, var.name, dump_default)\n                if is_object_type(type(dump_value)):\n                    dump_value = {\n                        key: (\n                            _dump(value, dict_type=dict_type)\n                            if is_config(value)\n                            else value\n                        )\n                        for (key, value) in dump_value.items()\n                    }\n\n                if dump_value is not None:\n                    result[dump_key] = dump_value\n\n    return result", "response": "Dumps an instance to a dictionary type mapping."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef validate(instance):\n\n    jsonschema.validate(\n        to_dict(instance, dict_type=dict), build_schema(instance.__class__)\n    )", "response": "Validates a given instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads an instance of config_cls from a dictionary.", "response": "def from_dict(config_cls, dictionary, validate=False):\n    \"\"\" Loads an instance of ``config_cls`` from a dictionary.\n\n    :param type config_cls: The class to build an instance of\n    :param dict dictionary: The dictionary to load from\n    :param bool validate: Preforms validation before building ``config_cls``,\n        defaults to False, optional\n    :return: An instance of ``config_cls``\n    :rtype: object\n    \"\"\"\n\n    return _build(config_cls, dictionary, validate=validate)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the imported handler module.", "response": "def imported(self):\n        \"\"\" The imported handler module.\n\n        :return: The imported handler module.\n        :rtype: module\n        \"\"\"\n\n        if not hasattr(self, \"_imported\"):\n            self._imported = self._discover_import()\n        return self._imported"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns True if any of the supported modules from packages exist in the system.", "response": "def available(self):\n        \"\"\" True if any of the supported modules from ``packages`` is available for use.\n\n        :return: True if any modules from ``packages`` exist\n        :rtype: bool\n        \"\"\"\n\n        for module_name in self.packages:\n            if importlib.util.find_spec(module_name):\n                return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _discover_import(self, prefer=None):\n\n        available_packages = self.packages\n        if isinstance(prefer, str):\n            available_packages = (prefer,)\n\n        for module_name in available_packages:\n            spec = importlib.util.find_spec(module_name)\n            if spec is not None:\n                importlib.import_module(module_name)\n                imported_hook = getattr(self, f\"on_{module_name}_imported\", None)\n                if callable(imported_hook):\n                    imported_hook(sys.modules[module_name])\n                return module_name\n        raise ModuleNotFoundError(f\"no modules in {available_packages!r} found\")", "response": "Discovers and imports the best available module from packages."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprefer a serializtion handler over other handlers.", "response": "def _prefer_package(self, package):\n        \"\"\" Prefer a serializtion handler over other handlers.\n\n        :param str package: The name of the package to use\n        :raises ValueError: When the given package name is not one of the available\n            supported serializtion packages for this handler\n        :return: The name of the serialization handler\n        :rtype: str\n        \"\"\"\n\n        if isinstance(package, str) and package != self.imported:\n            if package not in self.packages:\n                raise ValueError(\n                    f\"preferred package {package!r} does not exist, allowed are \"\n                    f\"{self.packages!r}\"\n                )\n            # clear out current serialization handler (if exists)\n            if hasattr(self, \"_handler\"):\n                del self._handler\n            # manually update imported handlers with a given preference\n            self._imported = self._discover_import(prefer=package)\n            return package\n        return self.imported"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef loads(self, config, content, prefer=None):\n\n        loader = self._prefer_package(prefer)\n        loads_hook_name = f\"on_{loader}_loads\"\n        loads_hook = getattr(self, loads_hook_name, None)\n        if not callable(loads_hook):\n            raise ValueError(\n                f\"no loads handler for {self.imported!r}, requires method \"\n                f\"{loads_hook_name!r} in {self!r}\"\n            )\n        return loads_hook(self.handler, config, content)", "response": "An abstract loads method which loads an instance from some content."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dump(self, config, instance, file_object, prefer=None, **kwargs):\n\n        file_object.write(self.dumps(config, instance, prefer=prefer, **kwargs))", "response": "An abstract method that dumps to a given file object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load(self, config, file_object, prefer=None):\n\n        return self.loads(config, file_object.read(), prefer=prefer)", "response": "An abstract method that loads from a given file object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef on_json_dumps(self, json, config, dictionary, **kwargs):\n\n        return json.dumps(dictionary, **kwargs)", "response": "The json. dumps method."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef on_ujson_dumps(self, ujson, config, dictionary, **kwargs):\n\n        if not kwargs.get(\"indent\", None):\n            kwargs[\"indent\"] = 0\n        return ujson.dumps(dictionary, **kwargs)", "response": "The ujson. dumps method."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef on_lxml_loads(self, lxml, config, content, **kwargs):\n\n        # NOTE: lazy import of XMLParser because class requires lxml to exist on import\n        from ..contrib.xml_parser import XMLParser\n\n        return XMLParser.from_xml(\n            content, encoding=kwargs.pop(\"encoding\", \"utf-8\")\n        ).to_dict()", "response": "The lxml loads method."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef on_tomlkit_dumps(self, tomlkit, config, dictionary, **kwargs):\n\n        inline_tables = set(kwargs.get(\"inline_tables\", []))\n\n        def _dump_dict(dictionary, source, source_path=[]):\n            for (key, value) in dictionary.items():\n                if isinstance(value, dict):\n                    # checks the current path with fnmatch to see if current table\n                    # should be an inline table\n                    is_inline = any(\n                        [\n                            fnmatch.fnmatch(\".\".join(source_path + [key]), pattern)\n                            for pattern in inline_tables\n                        ]\n                    )\n                    if is_inline:\n                        table = tomlkit.inline_table()\n                        # NOTE: manual dictionary assignment because `tomlkit` does not\n                        # impelment `dict.update`\n                        for (inline_key, inline_value) in value.items():\n                            if isinstance(inline_value, dict):\n                                table[inline_key] = _dump_dict(\n                                    inline_value,\n                                    tomlkit.inline_table(),\n                                    source_path=source_path + [inline_key],\n                                )\n                            else:\n                                table[inline_key] = inline_value\n                        source[key] = table\n                    else:\n                        source[key] = _dump_dict(\n                            value, tomlkit.table(), source_path=source_path + [key]\n                        )\n                else:\n                    source[key] = value\n            return source\n\n        return tomlkit.dumps(_dump_dict(dictionary, tomlkit.document()))", "response": "The TOML serialization method."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef on_toml_dumps(self, toml, config, dictionary, **kwargs):\n\n        inline_tables = set(kwargs.get(\"inline_tables\", []))\n\n        def _dump_dict(dictionary, source, source_path=[]):\n            for (key, value) in dictionary.items():\n                if isinstance(value, dict):\n                    is_inline = any(\n                        [\n                            fnmatch.fnmatch(\".\".join(source_path + [key]), pattern)\n                            for pattern in inline_tables\n                        ]\n                    )\n                    if is_inline:\n                        source[key] = toml.TomlDecoder().get_empty_inline_table()\n                    else:\n                        source[key] = {}\n                    source[key].update(\n                        _dump_dict(value, {}, source_path=source_path + [key])\n                    )\n                else:\n                    source[key] = value\n            return source\n\n        encoder = toml.TomlEncoder(preserve=True)\n        return toml.dumps(_dump_dict(dictionary, {}), encoder=encoder)", "response": "The TOML dump method."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef on_msgpack_loads(self, msgpack, config, content, **kwargs):\n\n        return msgpack.loads(content, raw=False)", "response": "The msgpack loads method."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _build_attribute_modifiers(var, attribute_mapping, ignore=None):\n\n    if not isinstance(ignore, list):\n        ignore = [\"type\", \"name\", \"required\", \"default\"]\n    if not is_config_var(var):\n        raise ValueError(\n            f\"cannot build field modifiers for {var!r}, is not a config var\"\n        )\n\n    entry = var.metadata[CONFIG_KEY]\n    modifiers = {}\n\n    for (entry_attribute, entry_value) in zip(\n        attr.fields(type(entry)), attr.astuple(entry)\n    ):\n        if entry_value is not None:\n            if entry_attribute.name in ignore:\n                continue\n            elif entry_attribute.name in attribute_mapping:\n                # NOTE: stupid type comparisons required for off case where\n                # bool is a subclass of int `isinstance(True, (int, float)) == True`\n                if entry_attribute.type is not None and (\n                    type(entry_value) in entry_attribute.type\n                    if isinstance(entry_attribute.type, (list, tuple, set))\n                    else type(entry_value) == entry_attribute.type\n                ):  # noqa\n                    modifiers[attribute_mapping[entry_attribute.name]] = entry_value\n                else:\n                    raise ValueError(\n                        f\"invalid modifier type for modifier {entry_attribute.name!r} \"\n                        f\"on var {var.name!r}, expected type {entry_attribute.type!r}, \"\n                        f\"received {entry_value!r} of type {type(entry_value)!r}\"\n                    )\n            else:\n                warnings.warn(\n                    f\"field modifier {entry_attribute.name!r} has no effect on var \"\n                    f\"{var.name!r} of type {entry.type!r}\"\n                )\n\n    return modifiers", "response": "Builds the jsonschema modifiers for a given config var and some mapping of attributes to jsonschema modifiers."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _build_enum_type(var, property_path=None):\n\n    if not property_path:\n        property_path = []\n\n    entry = var.metadata[CONFIG_KEY]\n    enum_values = [member.value for member in entry.type.__members__.values()]\n    schema = {\"enum\": enum_values}\n\n    for (type_name, check) in dict(\n        bool=is_bool_type,\n        string=is_string_type,\n        number=is_number_type,\n        integer=is_integer_type,\n    ).items():\n        if all(check(type(_)) for _ in enum_values):\n            schema[\"type\"] = type_name\n            break\n\n    return schema", "response": "Builds the schema definitions for enum type values."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _build_string_type(var, property_path=None):\n\n    if not property_path:\n        property_path = []\n\n    schema = {\"type\": \"string\"}\n    if is_builtin_type(var):\n        return schema\n\n    if is_regex_type(var):\n        schema[\"pattern\"] = var.__supertype__.pattern\n        return schema\n\n    if is_config_var(var):\n        schema.update(\n            _build_attribute_modifiers(var, {\"min\": \"minLength\", \"max\": \"maxLength\"})\n        )\n        if is_regex_type(var.type):\n            schema[\"pattern\"] = var.type.__supertype__.pattern\n\n    return schema", "response": "Builds the schema definitions for string type values."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _build_integer_type(var, property_path=None):\n\n    if not property_path:\n        property_path = []\n\n    schema = {\"type\": \"integer\"}\n    if is_builtin_type(var):\n        return schema\n\n    if is_config_var(var):\n        schema.update(\n            _build_attribute_modifiers(var, {\"min\": \"minimum\", \"max\": \"maximum\"})\n        )\n\n    return schema", "response": "Builds the schema definitions for integer type values."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbuild the schema definitions for a number type value.", "response": "def _build_number_type(var, property_path=None):\n    \"\"\" Builds schema definitions for number type values.\n\n    :param var: The number type value\n    :param List[str] property_path: The property path of the current type,\n        defaults to None, optional\n    :param property_path: [type], optional\n    :return: The built schema definition\n    :rtype: Dict[str, Any]\n    \"\"\"\n\n    if not property_path:\n        property_path = []\n\n    schema = {\"type\": \"number\"}\n    if is_builtin_type(var):\n        return schema\n\n    if is_config_var(var):\n        schema.update(\n            _build_attribute_modifiers(var, {\"min\": \"minimum\", \"max\": \"maximum\"})\n        )\n\n    return schema"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbuild the schema definitions for array types.", "response": "def _build_array_type(var, property_path=None):\n    \"\"\" Builds schema definitions for array type values.\n\n    :param var: The array type value\n    :param List[str] property_path: The property path of the current type,\n        defaults to None, optional\n    :param property_path: [type], optional\n    :return: The built schema definition\n    :rtype: Dict[str, Any]\n    \"\"\"\n\n    if not property_path:\n        property_path = []\n\n    schema = {\"type\": \"array\", \"items\": {\"$id\": f\"#/{'/'.join(property_path)}/items\"}}\n    if is_builtin_type(var):\n        return schema\n\n    if is_config_var(var):\n        schema.update(\n            _build_attribute_modifiers(\n                var,\n                {\n                    \"min\": \"minItems\",\n                    \"max\": \"maxItems\",\n                    \"unique\": \"uniqueItems\",\n                    \"contains\": \"contains\",\n                },\n            )\n        )\n\n        if is_typing_type(var.type) and len(var.type.__args__) > 0:\n            # NOTE: typing.List only allows one typing argument\n            nested_type = var.type.__args__[0]\n            schema[\"items\"].update(\n                _build(nested_type, property_path=property_path + [\"items\"])\n            )\n    elif is_typing_type(var):\n        nested_type = var.__args__[0]\n        schema[\"items\"].update(\n            _build(nested_type, property_path=property_path + [\"items\"])\n        )\n\n    return schema"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbuilding the schema definitions for object types.", "response": "def _build_object_type(var, property_path=None):\n    \"\"\" Builds schema definitions for object type values.\n\n    :param var: The object type value\n    :param List[str] property_path: The property path of the current type,\n        defaults to None, optional\n    :param property_path: [type], optional\n    :return: The built schema definition\n    :rtype: Dict[str, Any]\n    \"\"\"\n\n    if not property_path:\n        property_path = []\n\n    schema = {\"type\": \"object\"}\n    if is_builtin_type(var):\n        return schema\n\n    entry = var.metadata[CONFIG_KEY]\n\n    if isinstance(entry.min, int):\n        schema[\"minProperties\"] = entry.min\n    if isinstance(entry.max, int):\n        schema[\"maxProperties\"] = entry.max\n\n    # NOTE: typing.Dict only accepts two typing arguments\n    if is_typing_type(var.type) and len(var.type.__args__) == 2:\n        (key_type, value_type) = var.type.__args__\n\n        key_pattern = \"^(.*)$\"\n        if is_regex_type(key_type):\n            key_pattern = key_type.__supertype__.pattern\n        elif not is_string_type(key_type):\n            raise ValueError(\n                f\"cannot serialize object with key of type {key_type!r}, \"\n                f\"located in var {var.name!r}\"\n            )\n\n        schema[\"patternProperties\"] = {\n            key_pattern: _build(value_type, property_path=property_path)\n        }\n\n    return schema"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbuild the schema definition based on the given type and value.", "response": "def _build_type(type_, value, property_path=None):\n    \"\"\" Builds the schema definition based on the given type for the given value.\n\n    :param type_: The type of the value\n    :param value: The value to build the schema definition for\n    :param List[str] property_path: The property path of the current type,\n        defaults to None, optional\n    :return: The built schema definition\n    :rtype: Dict[str, Any]\n    \"\"\"\n\n    if not property_path:\n        property_path = []\n\n    for (type_check, builder) in (\n        (is_enum_type, _build_enum_type),\n        (is_null_type, _build_null_type),\n        (is_bool_type, _build_bool_type),\n        (is_string_type, _build_string_type),\n        (is_integer_type, _build_integer_type),\n        (is_number_type, _build_number_type),\n        (is_array_type, _build_array_type),\n        (is_object_type, _build_object_type),\n    ):\n        if type_check(type_):\n            return builder(value, property_path=property_path)\n\n    # NOTE: warning ignores type None (as that is the config var default)\n    if type_:\n        warnings.warn(f\"unhandled translation for type {type_!r} with value {value!r}\")\n    return {}"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _build_var(var, property_path=None):\n\n    if not property_path:\n        property_path = []\n\n    if not is_config_var(var):\n        raise ValueError(f\"var {var!r} is not a config var\")\n\n    entry = var.metadata[CONFIG_KEY]\n    var_name = entry.name if entry.name else var.name\n    schema = {\"$id\": f\"#/{'/'.join(property_path)}/{var_name}\"}\n\n    if var.default is not None:\n        schema[\"default\"] = var.default\n\n    if entry is not None:\n        if isinstance(entry.title, str):\n            schema[\"title\"] = entry.title\n        if isinstance(entry.description, str):\n            schema[\"description\"] = entry.description\n        if isinstance(entry.examples, collections.Iterable) and len(entry.examples) > 0:\n            schema[\"examples\"] = entry.examples\n\n    # handle typing.Union types by simply using the \"anyOf\" key\n    if is_union_type(var.type):\n        type_union = {\"anyOf\": []}\n        for allowed_type in var.type.__args__:\n            # NOTE: requires jsonschema draft-07\n            type_union[\"anyOf\"].append(\n                _build_type(\n                    allowed_type, allowed_type, property_path=property_path + [var_name]\n                )\n            )\n        schema.update(type_union)\n    else:\n        schema.update(\n            _build_type(var.type, var, property_path=property_path + [var_name])\n        )\n    return schema", "response": "Builds a schema definition for a given config var."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbuilds the schema definition for a given config class.", "response": "def _build_config(config_cls, property_path=None):\n    \"\"\" Builds the schema definition for a given config class.\n\n    :param class config_cls: The config class to build a schema definition for\n    :param List[str] property_path: The property path of the current type,\n        defaults to None, optional\n    :raises ValueError: When the given ``config_cls`` is not a config decorated class\n    :return: The built schema definition\n    :rtype: Dict[str, Any]\n    \"\"\"\n\n    if not property_path:\n        property_path = []\n\n    if not is_config_type(config_cls):\n        raise ValueError(f\"class {config_cls!r} is not a config class\")\n\n    schema = {\"type\": \"object\", \"required\": [], \"properties\": {}}\n    cls_entry = getattr(config_cls, CONFIG_KEY)\n\n    # add schema title, defaults to config classes `__qualname__`\n    schema_title = cls_entry.get(\"title\", config_cls.__qualname__)\n    if isinstance(schema_title, str):\n        schema[\"title\"] = schema_title\n\n    schema_description = cls_entry.get(\"description\")\n    if isinstance(schema_description, str):\n        schema[\"description\"] = schema_description\n\n    # if the length of the property path is 0, assume that current object is root\n    if len(property_path) <= 0:\n        schema[\"$id\"] = f\"{config_cls.__qualname__}.json\"\n        # NOTE: requires draft-07 for typing.Union type schema generation\n        schema[\"$schema\"] = \"http://json-schema.org/draft-07/schema#\"\n    else:\n        schema[\"$id\"] = f\"#/{'/'.join(property_path)}\"\n\n    property_path.append(\"properties\")\n    for var in attr.fields(config_cls):\n        if not is_config_var(var):\n            # encountered attribute is not a serialized field (i.e. missing CONFIG_KEY)\n            continue\n        entry = var.metadata[CONFIG_KEY]\n        var_name = entry.name if entry.name else var.name\n        if entry.required:\n            schema[\"required\"].append(var_name)\n\n        if is_config_type(var.type):\n            schema[\"properties\"][var_name] = _build_config(\n                var.type, property_path=property_path + [var_name]\n            )\n        else:\n            schema[\"properties\"][var_name] = _build_var(\n                var, property_path=property_path\n            )\n\n    return schema"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _build(value, property_path=None):\n\n    if not property_path:\n        property_path = []\n\n    if is_config_type(value):\n        return _build_config(value, property_path=property_path)\n    elif is_config_var(value):\n        return _build_var(value, property_path=property_path)\n    elif is_builtin_type(value):\n        return _build_type(value, value, property_path=property_path)\n    elif is_regex_type(value):\n        # NOTE: building regular expression types assumes type is string\n        return _build_type(str, value, property_path=property_path)\n    elif is_typing_type(value):\n        return _build_type(value, value, property_path=property_path)\n    return _build_type(type(value), value, property_path=property_path)", "response": "Builds a schema definition for the current type."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_types(type_):\n\n    return list(\n        itertools.chain.from_iterable(\n            map(lambda x: TYPE_MAPPINGS[x].get(type_, []), TYPE_MAPPINGS)\n        )\n    )", "response": "Returns all types within the TYPE_MAPPINGS for a specific Types value."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndecode a base64 string into bytes.", "response": "def decode_bytes(string):\n    \"\"\" Decodes a given base64 string into bytes.\n\n    :param str string: The string to decode\n    :return: The decoded bytes\n    :rtype: bytes\n    \"\"\"\n\n    if is_string_type(type(string)):\n        string = bytes(string, \"utf-8\")\n    return base64.decodebytes(string)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks if the given value is a valid file_config. var.", "response": "def is_config_var(var):\n    \"\"\" Checks if the given value is a valid ``file_config.var``.\n\n    :param var: The value to check\n    :return: True if the given value is a var, otherwise False\n    :rtype: bool\n    \"\"\"\n\n    return (\n        isinstance(var, (attr._make.Attribute, attr._make._CountingAttr))\n        and hasattr(var, \"metadata\")\n        and CONFIG_KEY in var.metadata\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if the given type is config decorated.", "response": "def is_config_type(type_):\n    \"\"\" Checks if the given type is ``file_config.config`` decorated.\n\n    :param type_: The type to check\n    :return: True if the type is config decorated, otherwise False\n    :rtype: bool\n    \"\"\"\n\n    return (\n        isinstance(type_, type)\n        and hasattr(type_, \"__attrs_attrs__\")\n        and hasattr(type_, CONFIG_KEY)\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks if the given type is an enum type.", "response": "def is_enum_type(type_):\n    \"\"\" Checks if the given type is an enum type.\n\n    :param type_: The type to check\n    :return: True if the type is a enum type, otherwise False\n    :rtype: bool\n    \"\"\"\n\n    return isinstance(type_, type) and issubclass(type_, tuple(_get_types(Types.ENUM)))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if the given type is a regex type.", "response": "def is_regex_type(type_):\n    \"\"\" Checks if the given type is a regex type.\n\n    :param type_: The type to check\n    :return: True if the type is a regex type, otherwise False\n    :rtype: bool\n    \"\"\"\n\n    return (\n        callable(type_)\n        and getattr(type_, \"__name__\", None) == REGEX_TYPE_NAME\n        and hasattr(type_, \"__supertype__\")\n        and is_compiled_pattern(type_.__supertype__)\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_union_type(type_):\n\n    if is_typing_type(type_) and hasattr(type_, \"__origin__\"):\n        # NOTE: union types can only be from typing module\n        return type_.__origin__ in _get_types(Types.UNION)\n    return False", "response": "Checks if the given type is a union type."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if the given type is a string type.", "response": "def is_string_type(type_):\n    \"\"\" Checks if the given type is a string type.\n\n    :param type_: The type to check\n    :return: True if the type is a string type, otherwise False\n    :rtype: bool\n    \"\"\"\n\n    string_types = _get_types(Types.STRING)\n    if is_typing_type(type_):\n        return type_ in string_types or is_regex_type(type_)\n    return type_ in string_types"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if the given type is a array type.", "response": "def is_array_type(type_):\n    \"\"\" Checks if the given type is a array type.\n\n    :param type_: The type to check\n    :return: True if the type is a array type, otherwise False\n    :rtype: bool\n    \"\"\"\n\n    array_types = _get_types(Types.ARRAY)\n    if is_typing_type(type_):\n        return type_ in array_types or (\n            hasattr(type_, \"__origin__\") and type_.__origin__ in array_types\n        )\n    return type_ in array_types"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if the given type is an object type.", "response": "def is_object_type(type_):\n    \"\"\" Checks if the given type is a object type.\n\n    :param type_: The type to check\n    :return: True if the type is a object type, otherwise False\n    :rtype: bool\n    \"\"\"\n\n    object_types = _get_types(Types.OBJECT)\n    if is_typing_type(type_):\n        return type_ in object_types or (\n            hasattr(type_, \"__origin__\") and type_.__origin__ in object_types\n        )\n    return type_ in object_types"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntries to typecast the given value with the given type.", "response": "def typecast(type_, value):\n    \"\"\" Tries to smartly typecast the given value with the given type.\n\n    :param type_: The type to try to use for the given value\n    :param value: The value to try and typecast to the given type\n    :return: The typecasted value if possible, otherwise just the original value\n    \"\"\"\n\n    # NOTE: does not do any special validation of types before casting\n    # will just raise errors on type casting failures\n    if is_builtin_type(type_) or is_collections_type(type_) or is_enum_type(type_):\n        # FIXME: move to Types enum and TYPE_MAPPING entry\n        if is_bytes_type(type_):\n            return decode_bytes(value)\n        return type_(value)\n    elif is_regex_type(type_):\n        return typecast(str, value)\n    elif is_typing_type(type_):\n        try:\n            base_type = type_.__extra__\n        except AttributeError:\n            # NOTE: when handling typing._GenericAlias __extra__ is actually __origin__\n            base_type = type_.__origin__\n        arg_types = type_.__args__\n\n        if is_array_type(type_):\n            if len(arg_types) == 1:\n                item_type = arg_types[0]\n                return base_type([typecast(item_type, item) for item in value])\n            else:\n                return base_type(value)\n        elif is_object_type(type_):\n            if len(arg_types) == 2:\n                (key_type, item_type) = arg_types\n                return base_type(\n                    {\n                        typecast(key_type, key): typecast(item_type, item)\n                        for (key, item) in value.items()\n                    }\n                )\n            else:\n                return base_type(value)\n        else:\n            return base_type(value)\n    else:\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbuild and view documentation.", "response": "def view(ctx):\n    \"\"\" Build and view docs.\n    \"\"\"\n\n    report.info(ctx, \"docs.view\", f\"viewing documentation\")\n    build_path = ctx.docs.directory / \"build\" / \"html\" / \"index.html\"\n    build_path = pathname2url(build_path.as_posix())\n    webbrowser.open(f\"file:{build_path!s}\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nencode a variable to the appropriate string format for ini files.", "response": "def _encode_var(cls, var):\n        \"\"\" Encodes a variable to the appropriate string format for ini files.\n\n        :param var: The variable to encode\n        :return: The ini representation of the variable\n        :rtype: str\n        \"\"\"\n\n        if isinstance(var, str):\n            if any(_ in var for _ in cls.requires_quotes):\n                # NOTE: quoted strings should just use '\"' according to the spec\n                return '\"' + var.replace('\"', '\\\\\"') + '\"'\n            return var\n        else:\n            return str(var)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _decode_var(cls, string):\n\n        str_match = cls.quoted_string_regex.match(string)\n        if str_match:\n            return string.strip(\"'\" if str_match.groups()[0] else '\"')\n        # NOTE: \"\u00b9\".isdigit() results in True because they are idiots\n        elif string.isdigit() and cls.is_digit_regex.match(string) is not None:\n            return int(string)\n        elif string.lower() in (\"true\", \"false\"):\n            return string.lower() == \"true\"\n        elif string.lstrip(\"-\").isdigit():\n            try:\n                return int(string)\n            except ValueError:\n                # case where we mistake something like \"--0\" as a int\n                return string\n        elif \".\" in string.lstrip(\"-\"):\n            try:\n                return float(string)\n            except ValueError:\n                # one off case where we mistake a single \".\" as a float\n                return string\n        else:\n            return string", "response": "Decodes a given string into the appropriate type in Python."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbuilding a dictionary of dict_type given the parser. _sections dict.", "response": "def _build_dict(\n        cls, parser_dict, delimiter=DEFAULT_DELIMITER, dict_type=collections.OrderedDict\n    ):\n        \"\"\" Builds a dictionary of ``dict_type`` given the ``parser._sections`` dict.\n\n        :param dict parser_dict: The ``parser._sections`` mapping\n        :param str delimiter: The delimiter for nested dictionaries,\n            defaults to \":\", optional\n        :param class dict_type: The dictionary type to use for building the dict,\n            defaults to :class:`collections.OrderedDict`, optional\n        :return: The resulting dictionary\n        :rtype: dict\n        \"\"\"\n\n        result = dict_type()\n        for (key, value) in parser_dict.items():\n            if isinstance(value, dict):\n                nestings = key.split(delimiter)\n\n                # build nested dictionaries if they don't exist (up to 2nd to last key)\n                base_dict = result\n                for nested_key in nestings[:-1]:\n                    if nested_key not in base_dict:\n                        base_dict[nested_key] = dict_type()\n                    base_dict = base_dict[nested_key]\n\n                base_dict[nestings[-1]] = cls._build_dict(\n                    parser_dict.get(key), delimiter=delimiter, dict_type=dict_type\n                )\n            else:\n                if \"\\n\" in value:\n                    result[key] = [\n                        cls._decode_var(_) for _ in value.lstrip(\"\\n\").split(\"\\n\")\n                    ]\n                else:\n                    result[key] = cls._decode_var(value)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _build_parser(\n        cls,\n        dictionary,\n        parser,\n        section_name,\n        delimiter=DEFAULT_DELIMITER,\n        empty_sections=False,\n    ):\n        \"\"\" Populates a parser instance with the content of a dictionary.\n\n        :param dict dictionary: The dictionary to use for populating the parser instance\n        :param configparser.ConfigParser parser: The parser instance\n        :param str section_name: The current section name to add the dictionary keys to\n        :param str delimiter: The nested dictionary delimiter character,\n            defaults to \":\", optional\n        :param bool empty_sections: Flag to allow the representation of empty sections\n            to exist, defaults to False, optional\n        :return: The populated parser\n        :rtype: configparser.ConfigParser\n        \"\"\"\n\n        for (key, value) in dictionary.items():\n            if isinstance(value, dict):\n                nested_section = delimiter.join([section_name, key])\n                is_empty = all(isinstance(_, dict) for _ in value.values())\n                if not is_empty or empty_sections:\n                    parser.add_section(nested_section)\n                cls._build_parser(value, parser, nested_section, delimiter=delimiter)\n            elif isinstance(value, (list, tuple, set, frozenset)):\n                if any(isinstance(_, dict) for _ in value):\n                    raise ValueError(\n                        f\"INI files cannot support arrays with mappings, \"\n                        f\"found in key {key!r}\"\n                    )\n                parser.set(\n                    section_name, key, \"\\n\".join(cls._encode_var(_) for _ in value)\n                )\n            else:\n                parser.set(section_name, key, cls._encode_var(value))\n        return parser", "response": "Builds a configparser. ConfigParser instance with the content of a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating an instance of INIParser from a dictionary.", "response": "def from_dict(\n        cls,\n        dictionary,\n        root_section=\"root\",\n        delimiter=DEFAULT_DELIMITER,\n        empty_sections=False,\n    ):\n        \"\"\" Create an instance of ``INIParser`` from a given dictionary.\n\n        :param dict dictionary: The dictionary to create an instance from\n        :param str root_section: The root key of the ini content, defaults to \"root\",\n            optional\n        :param str delimiter: The delimiter character to use for nested dictionaries,\n            defaults to \":\", optional\n        :param bool empty_sections: Flag to allow representation of empty sections to\n            exist, defaults to False, optional\n        :return: The new ``INIParser`` instance\n        :rtype: INIParser\n        \"\"\"\n\n        parser = cls()\n        parser.add_section(root_section)\n        return cls._build_parser(\n            dictionary,\n            parser,\n            root_section,\n            delimiter=delimiter,\n            empty_sections=empty_sections,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the dictionary representation of the current parser instance.", "response": "def to_dict(self, delimiter=DEFAULT_DELIMITER, dict_type=collections.OrderedDict):\n        \"\"\" Get the dictionary representation of the current parser.\n\n        :param str delimiter: The delimiter used for nested dictionaries,\n            defaults to \":\", optional\n        :param class dict_type: The dictionary type to use for building the dictionary\n            reperesentation, defaults to collections.OrderedDict, optional\n        :return: The dictionary representation of the parser instance\n        :rtype: dict\n        \"\"\"\n\n        root_key = self.sections()[0]\n        return self._build_dict(\n            self._sections, delimiter=delimiter, dict_type=dict_type\n        ).get(root_key, {})"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_ini(self):\n\n        fake_io = io.StringIO()\n        self.write(fake_io)\n        return fake_io.getvalue()", "response": "Get the ini string of the current parser."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clean(ctx):\n\n    clean_command = \"python setup.py clean\"\n    report.info(ctx, \"package.clean\", \"cleaning up built package artifacts\")\n    ctx.run(clean_command)\n\n    egg_name = f\"{ctx.metadata['package_name']}.egg-info\"\n    report.info(ctx, \"pacakge.clean\", \"removing build directories\")\n    for artifact in (\"dist\", \"build\", egg_name, f\"src/{egg_name}\"):\n        artifact_dir = ctx.directory / artifact\n        if artifact_dir.is_dir():\n            report.debug(ctx, \"package.clean\", f\"removing directory {artifact_dir!s}\")\n            ctx.run(f\"rm -rf {artifact_dir!s}\")", "response": "Clean previously built package artifacts."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck built package is valid.", "response": "def check(ctx):\n    \"\"\" Check built package is valid.\n    \"\"\"\n\n    check_command = f\"twine check {ctx.directory!s}/dist/*\"\n    report.info(ctx, \"package.check\", \"checking package\")\n    ctx.run(check_command)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef version(ctx, version=None, force=False):\n\n    # define replacement strategies for files where the version needs to be in sync\n    updates = {\n        ctx.directory.joinpath(\"setup.cfg\"): [\n            (r\"^(version\\s?=\\s?)(.*)\", \"\\\\g<1>{version}\")\n        ],\n        ctx.package.directory.joinpath(\"__version__.py\"): [\n            (r\"(__version__\\s?=\\s?)(.*)\", '\\\\g<1>\"{version}\"')\n        ],\n    }\n\n    previous_version = get_previous_version(ctx)\n    if isinstance(version, str):\n        version = parver.Version.parse(version)\n        if not force and version <= previous_version:\n            error_message = (\n                f\"version {version!s} is <= to previous version {previous_version!s}\"\n            )\n            report.error(ctx, \"package.version\", error_message)\n            raise ValueError(error_message)\n    else:\n        version = previous_version.bump_release(index=len(previous_version.release) - 1)\n\n    report.info(ctx, \"package.version\", f\"updating version to {version!s}\")\n    for (path, replacements) in updates.items():\n        if path.is_file():\n            content = path.read_text()\n            for (pattern, sub) in replacements:\n                report.debug(\n                    ctx,\n                    \"package.version\",\n                    f\"applying replacement ({pattern!r}, {sub!r}) to {path!s}\",\n                )\n                content = re.sub(pattern, sub.format(version=version), content, re.M)\n            path.write_text(content)", "response": "Update the version of the package."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef stub(ctx):\n\n    report.info(ctx, \"package.stub\", f\"generating typing stubs for package\")\n    ctx.run(\n        f\"stubgen --include-private --no-import \"\n        f\"--output {ctx.directory.joinpath('stubs')!s} \"\n        f\"--search-path {ctx.directory.joinpath('src')!s} \"\n        f\"--package {ctx.metadata['package_name']}\"\n    )", "response": "Generate typing stubs for the package."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads the configuration file given as parameter", "response": "def parse_config(file_path):\n    \"\"\"Loads the configuration file given as parameter\"\"\"\n    config_parser = configparser.ConfigParser()\n    config_parser.read(file_path)\n    plugin_config = {}\n    options = config_parser.options(CONFIG_OPTION)\n    for option in options:\n        try:\n            plugin_config[option] = config_parser.get(CONFIG_OPTION, option)\n            if plugin_config[option] == -1:\n                print(\"skip: %s\" % option)\n        except Exception as e:\n            print(\"exception on %s!\" % option)\n            print(e.message)\n            plugin_config[option] = None\n    return plugin_config"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef start_plugin(file_path=None):\n    if os.getenv('CONFIG_PATH'):\n        file_path = os.getenv('CONFIG_PATH')\n    else:\n        file_path = file_path\n    if file_path is not None:\n        config = parse_config(file_path)\n        plugin_instance = load_plugin(config)\n    else:\n        plugin_instance = load_plugin\n    return plugin_instance", "response": "This function initializes the Ocean plugin"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef decode_unicode(data, replace_boo=True):\n  # dictionary which direct maps unicode values to its letters\n  dictionary = {'0030':'0','0031':'1','0032':'2','0033':'3','0034':'4','0035':'5','0036':'6','0037':'7','0038':'8','0039':'9','0024':'$','0040':'@','00A2':'cents','00A3':'pounds','00A5':'yen','00C7':'C','00D0':'D','00D1':'N','00DD':'Y','00E7':'c','00F1':'n','00FD':'y','00FF':'y','010E':'D','010F':'F','0110':'D','0111':'D','0130':'I','0134':'J','0135':'J','0136':'K','0137':'K','0138':'K','0160':'S','0161':'S','0191':'F','0192':'F','0193':'G','0198':'K','0199':'K',\n  '019D':'N','019E':'N','01A4':'P','01A5':'P','01AC':'T','01AF':'U','01B5':'Z','01CD':'A','01CE':'A','01CF':'I','01D0':'I','01D1':'O','01D2':'O','01DE':'A','01DF':'A','01E0':'A','01E1':'A','01F4':'G','01F5':'G','01F8':'N','01F9':'N','01FA':'A','01FB':'A','021E':'H',\n  '021F':'H','0224':'Z','2113':'L','2718':'X','0225':'Z','2134':'O','0226':'A','0227':'A','0228':'E','0229':'E','0386':'A','0388':'E','0389':'H','038A':'I','0391':'A','0392':'B','0395':'E','0396':'Z','0397':'H','0399':'I','039A':'K','039C':'M','039D':'N','039F':'O','03A1':'P','03A4':'T','03A5':'Y','03A7':'X','03AA':'I','03AB':'B','1E10':'D','1E11':'D','1E12':'D','1E13':'D','1E1E':'F','1E1F':'F','1E20':'G','1E21':'H','1E2C':'I','1E2D':'I','1E2E':'I','1E2F':'I','1E3E':'M','1E3F':'M','1E70':'T','1E71':'T','1E8E':'Y','1E8F':'Y','1EE0':'O','1EE1':'O','1EE2':'O','1EE3':'O','1EE4':'O','1EF0':'U','1EF1':'U'}\n  # dictionary in which patterns (prefixes and suffixes) are matched to possible letter choices\n  pattern_dict = {'00C':'AEI', '00D':'OU','00E':'AEI','00F':'OU','010':'AC','011':'EG','012':'GHI','013':'L','014':'LNO','015':'RS','016':'TU','017':'UWYZ', '018':'BCD','01D':'U','01E':'GKO','020':'AEIO','021':'RUST','022':'O','1E0':'ABCD','1E1':'E','1E3':'KL','1E4':'MNO','1E5':'OPR','1E6':'ST','1E7':'UV','1E8':'WX','1E9':'Z','1EB':'A','1EC':'EIO','1ED':'O','1EE':'U','1EF':'Y','216':'greeknum','217':'greeknum','246':'consecnum','247':'numfrom17'}\n  #dictionary which matches patterns for emoticons\n  hex_dict = {'A':'10','B':'11','C':'12','D':'13','E':'14','F':'15','a':'10','b':'11','c':'12','d':'13','e':'14','f':'15'}\n  happy_dict = ['1F600','263A','1F601','1F602','1F603','1F604','1F605','1F606','1F60A','263A','1F642','1F607','1F60C','1F643','1F62C','1F63A','1F638','1F639']\n  sad_dict = ['1F610','1F611','1F623','1F494','1F625','1F62B','1F613','1F614','1F615','2639','1F641','1F616','1F61E','1F61F','1F624','1F622','1F62D','1F629','1F630','1F620']\n  sexual_dict = ['1F609','1F6C0','2B50','1F445','1F525','1F36D','2606','1F60D','1F460','1F618','1F617','1F61A','1F917','1F60F','1F63B','1F63D','1F483','1F46F','1F48F','1F444','1F48B','1F459','1F484','1F34C','1F4AF','264B']\n  hearts=['1F498','2664','2764','2661','2665','1F493','1F495','1F496','1F497','1F499','1F49A','1F49B','1F49C','1F49D','1F49E','1F49F','2763']\n  baseball_dict=['26BE', '1F3C0', '1F3CF']\n  count=0\n\n  misc_code = ' *misc* '\n  if not replace_boo:\n    misc_code = ''\n\n  retval=''\n  # first I am filtering out all the non-unicode characters from the data \n  regex=re.compile(r'\\\\u[0-9ABCDEFabcdef]{1,4}')\n  regex2=re.compile(r'\\\\U[0-9ABCDEFabcdef]{1,8}') #this is so that both types of unicode representations are filtered\n  lowers = list('abcdef')\n  uppers = [c.upper() for c in lowers]\n  ndata = set()\n  data = data.encode('unicode-escape').decode('utf-8')\n  data = re.sub(r'(?:\\\\x(?:[0-9]|[a-f]){2})+', ' ', data, flags=re.IGNORECASE)\n  for val in re.finditer(regex,data):\n    to_append=val.group()\n    #converting unicode to standard representation\n    for c in lowers:\n      if c in to_append:\n        to_append = to_append.replace(c, c.lower())\n    ndata.add(to_append)\n  for val in re.finditer(regex2,data):\n    to_append = '\\u' + val.group()[5:]\n    for c in lowers:\n      if c in to_append:\n        to_append = to_append.replace(c, c.lower())\n    ndata.add(to_append)\n  ndata = list(ndata)\n  \"\"\"\n  Process of parsing:\n  -> Convert unicode into standard form\n  -> Convert each character of the unicode symbol to its numerical equivalent\n  -> Mapping Process:\n    -  First check in pattern dictionary to map suffix/prefix\n    -  Check Emoticon Dictionary \n    -  Replace value pair with Key whenever found\n    -  Then check direct dictionary\n    -  Append to .txt file if unicode not found in any dictionary \n  \"\"\"\n  for unicode_str in ndata:\n    uni=unicode_str[2:]\n    if unicode_str not in data:\n      unicode_str='\\U000' + unicode_str[2:]\n      #converting to standard representation\n      for c in uppers:\n        if c in unicode_str:\n          unicode_str = unicode_str.replace(c, c.lower())\n    if uni in baseball_dict:\n      retval+=' *baseball* '\n      #detecting baseball emoticons and converting to '*baseball*' and similar conversions for other categories of emoticons\n      data=string.replace(data,unicode_str,' *baseball* ')\n    if uni in happy_dict:\n      retval+=' *happy* '\n      if replace_boo:\n        data=string.replace(data,unicode_str,' *happy* ')\n      else:\n        data=string.replace(data,unicode_str,' ')\n    elif uni in sad_dict:\n      retval+=' *sad* '\n      if replace_boo:\n        data=string.replace(data,unicode_str,' *sad* ')\n      else:\n        data=string.replace(data,unicode_str,' ')\n    elif uni in sexual_dict:\n      retval+=' *sexual* '\n      if replace_boo:\n        data=string.replace(data,unicode_str,' *sexual* ')\n      else:\n        data=string.replace(data,unicode_str,' ')\n    elif uni in hearts:\n      retval+=' *hearts* '\n      if replace_boo:\n        data=string.replace(data,unicode_str,' *hearts* ')\n      else:\n        data=string.replace(data,unicode_str,' ')\n    elif uni in dictionary:\n      retval+=dictionary[uni]\n      data=string.replace(data,unicode_str,dictionary[uni])\n    elif uni[0:3]=='004' or uni[0:3]=='005':\n      #replacing unicodes for digits and before that, replacing hexadecimals with their numerical value\n      last_dig=uni[3:]\n      if last_dig in hex_dict:\n        last_dig=int(hex_dict[last_dig])\n      else:\n        last_dig=int(last_dig)\n      second_last_dig= int(uni[2:3])\n      num= (second_last_dig-4)*16 + last_dig\n      retval+=chr(64+num)\n      data=string.replace(data,unicode_str,chr(64+num))\n    elif uni[0:3]=='006' or uni[0:3]=='007':\n      last_dig=uni[3:]\n      if last_dig in hex_dict:\n        last_dig=int(hex_dict[last_dig])\n      else:\n        last_dig=int(last_dig)\n      second_last_dig= int(uni[2:3])\n      #parsing letters\n      num= (second_last_dig-6)*16 + last_dig\n      retval+=chr(64+num)\n      data=string.replace(data,unicode_str,chr(64+num))\n    elif uni[0:3] in pattern_dict:\n      val = pattern_dict[uni[0:3]]\n      if len(val)==1:\n        retval+=val\n        data=string.replace(data,unicode_str,val)\n      elif uni[0:3]=='00C':\n        last=uni[3:]\n        if last in hex_dict:\n          last=hex_dict[last]\n        try:\n          last=int(last)\n        except:\n          retval+=misc_code\n          #parsing miscelleneous \n          data=string.replace(data,unicode_str,misc_code)\n        if last>=0 and last<=5:\n          retval+=val[0]\n          data=string.replace(data,unicode_str,val[0])\n        elif last<=11:\n          retval+=val[1]\n          data=string.replace(data,unicode_str,val[1])\n        else:\n          retval+=val[2]\n          data=string.replace(data,unicode_str,val[2])\n      elif uni[0:3]=='00D':\n        last=uni[3:]\n        if last in hex_dict:\n          last=hex_dict[last]\n        try:\n          last=int(last)\n        except:\n          pass\n        if last>=2 and last<=6:\n          retval+=val[0]\n          data=string.replace(data,unicode_str,val[0])\n        elif last>=9 and last<=12:\n          retval+=val[1]\n          data=string.replace(data,unicode_str,val[1])\n        else:\n          retval+=misc_code\n          data=string.replace(data,unicode_str,misc_code)\n      elif uni[0:3]=='00E':\n        last=uni[3:]\n        if last in hex_dict:\n          last=hex_dict[last]\n        try:\n          last=int(last)\n        except:\n          retval+=misc_code\n          data=string.replace(data,unicode_str,misc_code)\n        if last>=0 and last<=5:\n          retval+=val[0]\n          data=string.replace(data,unicode_str,val[0])\n        elif last<=11:\n          retval+=val[1]\n          data=string.replace(data,unicode_str,val[1])\n        else:\n          retval+=val[2]\n          data=string.replace(data,unicode_str,val[2])\n      elif uni[0:3]=='00F':\n        last=uni[3:]\n        if last in hex_dict:\n          last=hex_dict[last]\n        try:\n          last=int(last)\n        except:\n          retval+=misc_code\n          data=string.replace(data,unicode_str,misc_code)\n        if last>=2 and last<=6:\n          retval+=val[0]\n          data=string.replace(data,unicode_str,val[0])\n        elif last>=9 and last<=12:\n          retval+=val[1]\n          data=string.replace(data,unicode_str,val[1])\n        else:\n          retval+=misc_code\n          data=string.replace(data,unicode_str,misc_code)\n      elif uni[0:3]=='010':\n        last=uni[3:]\n        if last in hex_dict:\n          last=hex_dict[last]\n        try:\n          last=int(last)\n        except:\n          retval+=misc_code\n          data=string.replace(data,unicode_str,misc_code)\n        if last>=0 and last<=5:\n          retval+=val[0]\n          data=string.replace(data,unicode_str,val[0])\n        elif last>=6 and last<=13:\n          retval+=val[1]\n          data=string.replace(data,unicode_str,val[1])\n        else:\n          retval+=misc_code\n          data=string.replace(data,unicode_str,misc_code)\n      elif uni[0:3]=='011':\n        last=uni[3:]\n        if last in hex_dict:\n          last=hex_dict[last]\n        try:\n          last=int(last)\n        except:\n          retval+=misc_code\n          data=string.replace(data,unicode_str,misc_code)\n        if last>=2 and last<=11:\n          retval+=val[0]\n          data=string.replace(data,unicode_str,val[0])\n        elif last>=12 and last<=15:\n          retval+=val[1]\n          data=string.replace(data,unicode_str,val[1])\n        else:\n          retval+=misc_code\n          data=string.replace(data,unicode_str,misc_code)\n      elif uni[0:3]=='012':\n        last=uni[3:]\n        if last in hex_dict:\n          last=hex_dict[last]\n        try:\n          last=int(last)\n        except:\n          retval+=misc_code\n          data=string.replace(data,unicode_str,misc_code)\n        if last>=0 and last<=3:\n          retval+=val[0]\n          data=string.replace(data,unicode_str,val[0])\n        elif last<=7:\n          retval+=val[1]\n          data=string.replace(data,unicode_str,val[1])\n        elif last<=15:\n          retval+=val[2]\n          data=string.replace(data,unicode_str,val[2])\n        else:\n          retval+=misc_code\n          data=string.replace(data,unicode_str,misc_code)\n      elif uni[0:3]=='014':\n        last=uni[3:]\n        if last in hex_dict:\n          last=hex_dict[last]\n        try:\n          last=int(last)\n        except:\n          retval+=misc_code\n          data=string.replace(data,unicode_str,misc_code)\n        if last>=0 and last<=2:\n          retval+=val[0]\n          data=string.replace(data,unicode_str,val[0])\n        elif last<=11:\n          retval+=val[1]\n          data=string.replace(data,unicode_str,val[1])\n        elif last<=15:\n          retval+=val[2]\n          data=string.replace(data,unicode_str,val[2])\n        else:\n          retval+=misc_code\n          data=string.replace(data,unicode_str,misc_code)\n      elif uni[0:3]=='015':\n        last=uni[3:]\n        if last in hex_dict:\n          last=hex_dict[last]\n        try:\n          last=int(last)\n        except:\n          retval+=misc_code\n          data=string.replace(data,unicode_str,misc_code)\n        if last>=4 and last<=9:\n          retval+=val[0]\n          data=string.replace(data,unicode_str,val[0])\n        elif last>=10 and last<=15:\n          retval+=val[1]\n          data=string.replace(data,unicode_str,val[1])\n        else:\n          retval+=misc_code\n          data=string.replace(data,unicode_str,misc_code)\n      elif uni[0:3]=='016':\n        last=uni[3:]\n        if last in hex_dict:\n          last=hex_dict[last]\n        try:\n          last=int(last)\n        except:\n          retval+=misc_code\n          data=string.replace(data,unicode_str,misc_code)\n        if last>=2 and last<=7:\n          retval+=val[0]\n          data=string.replace(data,unicode_str,val[0])\n        elif last>=8 and last<=15:\n          retval+=val[1]\n          data=string.replace(data,unicode_str,val[1])\n        else:\n          retval+=misc_code\n          data=string.replace(data,unicode_str,misc_code)\n      elif uni[0:3]=='017':\n        last=uni[3:]\n        if last in hex_dict:\n          last=hex_dict[last]\n        try:\n          last=int(last)\n        except:\n          retval+=misc_code\n          data=string.replace(data,unicode_str,misc_code)\n        if last>=0 and last<=3:\n          retval+=val[0]\n          data=string.replace(data,unicode_str,val[0])\n        elif last<=5:\n          retval+=val[1]\n          data=string.replace(data,unicode_str,val[1])\n        elif last<=8:\n          retval+=val[2]\n          data=string.replace(data,unicode_str,val[2])\n        elif last<=14:\n          retval+=val[3]\n          data=string.replace(data,unicode_str,val[3])\n        else:\n          retval+=misc_code\n          data=string.replace(data,unicode_str,misc_code)\n      elif uni[0:3]=='018':\n        last=uni[3:]\n        if last in hex_dict:\n          last=hex_dict[last]\n        try:\n          last=int(last)\n        except:\n          retval+=misc_code\n          data=string.replace(data,unicode_str,misc_code)\n        if last>=0 and last<=5:\n          retval+=val[0]\n          data=string.replace(data,unicode_str,val[0])\n        elif last<=8:\n          retval+=val[1]\n          data=string.replace(data,unicode_str,val[1])\n        elif last<=12:\n          retval+=val[2]\n          data=string.replace(data,unicode_str,val[2])\n        else:\n          retval+=misc_code\n          data=string.replace(data,unicode_str,misc_code)\n      elif uni[0:3]=='01E':\n        last=uni[3:]\n        if last in hex_dict:\n          last=hex_dict[last]\n        try:\n          last=int(last)\n        except:\n          retval+=misc_code\n          data=string.replace(data,unicode_str,misc_code)\n        if last>=4 and last<=7:\n          retval+=val[0]\n          data=string.replace(data,unicode_str,val[0])\n        elif last<=9:\n          retval+=val[1]\n          data=string.replace(data,unicode_str,val[1])\n        elif last<=13:\n          retval+=val[2]\n          data=string.replace(data,unicode_str,val[2])\n        else:\n          retval+=misc_code\n          data=string.replace(data,unicode_str,misc_code)\n      elif uni[0:3]=='020':\n        last=uni[3:]\n        if last in hex_dict:\n          last=hex_dict[last]\n        try:\n          last=int(last)\n        except:\n          retval+=misc_code\n          data=string.replace(data,unicode_str,misc_code)\n        if last>=0 and last<=3:\n          retval+=val[0]\n          data=string.replace(data,unicode_str,val[0])\n        elif last<=7:\n          retval+=val[1]\n          data=string.replace(data,unicode_str,val[1])\n        elif last<=11:\n          retval+=val[2]\n          data=string.replace(data,unicode_str,val[2])\n        elif last<=15:\n          retval+=val[3]\n          data=string.replace(data,unicode_str,val[3])\n        else:\n          retval+=misc_code\n          data=string.replace(data,unicode_str,misc_code)\n      elif uni[0:3]=='021':\n        last=uni[3:]\n        if last in hex_dict:\n          last=hex_dict[last]\n        try:\n          last=int(last)\n        except:\n          retval+=misc_code\n          data=string.replace(data,unicode_str,misc_code)\n        if last>=0 and last<=3:\n          retval+=val[0]\n          data=string.replace(data,unicode_str,val[0])\n        elif last<=7:\n          retval+=val[1]\n          data=string.replace(data,unicode_str,val[1])\n        elif last<=9:\n          retval+=val[2]\n          data=string.replace(data,unicode_str,val[2])\n        elif last<=11:\n          retval+=val[3]\n          data=string.replace(data,unicode_str,val[3])\n        else:\n          retval+=misc_code\n          data=string.replace(data,unicode_str,misc_code)\n      elif uni[0:3]=='1E0':\n        last=uni[3:]\n        if last in hex_dict:\n          last=hex_dict[last]\n        try:\n          last=int(last)\n        except:\n          retval+=misc_code\n          data=string.replace(data,unicode_str,misc_code)\n        if last>=0 and last<=1:\n          retval+=val[0]\n          data=string.replace(data,unicode_str,val[0])\n        elif last<=7:\n          retval+=val[1]\n          data=string.replace(data,unicode_str,val[1])\n        elif last<=9:\n          retval+=val[2]\n          data=string.replace(data,unicode_str,val[2])\n        elif last<=15:\n          retval+=val[3]\n          data=string.replace(data,unicode_str,val[3])\n        else:\n          retval+=misc_code\n          data=string.replace(data,unicode_str,misc_code)\n      elif uni[0:3]=='1E3':\n        last=uni[3:]\n        if last in hex_dict:\n          last=hex_dict[last]\n        try:\n          last=int(last)\n        except:\n          retval+=misc_code\n          data=string.replace(data,unicode_str,misc_code)\n        if last>=0 and last<=5:\n          retval+=val[0]\n          data=string.replace(data,unicode_str,val[0])\n        elif last<=13:\n          retval+=val[1]\n          data=string.replace(data,unicode_str,val[1])\n        else:\n          retval+=misc_code\n          data=string.replace(data,unicode_str,misc_code)\n      elif uni[0:3]=='1E4':\n        last=uni[3:]\n        if last in hex_dict:\n          last=hex_dict[last]\n        try:\n          last=int(last)\n        except:\n          pass\n        if last>=0 and last<=3:\n          retval+=val[0]\n          data=string.replace(data,unicode_str,val[0])\n        elif last<=11:\n          retval+=val[1]\n          data=string.replace(data,unicode_str,val[1])\n        elif last<=15:\n          retval+=val[2]\n          data=string.replace(data,unicode_str,val[2])\n        else:\n          retval+=misc_code\n          data=string.replace(data,unicode_str,misc_code)\n      elif uni[0:3]=='1E5':\n        last=uni[3:]\n        if last in hex_dict:\n          last=hex_dict[last]\n        try:\n          last=int(last)\n        except:\n          pass\n        if last>=0 and last<=3:\n          retval+=val[0]\n          data=string.replace(data,unicode_str,val[0])\n        elif last<=7:\n          retval+=val[1]\n          data=string.replace(data,unicode_str,val[1])\n        elif last<=15:\n          retval+=val[2]\n          data=string.replace(data,unicode_str,val[2])\n        else:\n          retval+=misc_code\n          data=string.replace(data,unicode_str,misc_code)\n      elif uni[0:3]=='1E6':\n        last=uni[3:]\n        if last in hex_dict:\n          last=hex_dict[last]\n        try:\n          last=int(last)\n        except:\n          retval+=misc_code\n          data=string.replace(data,unicode_str,misc_code)\n        if last>=0 and last<=9:\n          retval+=val[0]\n          data=string.replace(data,unicode_str,val[0])\n        elif last>=10 and last<=15:\n          retval+=val[1]\n          data=string.replace(data,unicode_str,val[1])\n        else:\n          retval+=misc_code\n          data=string.replace(data,unicode_str,misc_code)\n      elif uni[0:3]=='1E7':\n        last=uni[3:]\n        if last in hex_dict:\n          last=hex_dict[last]\n        try:\n          last=int(last)\n        except:\n          retval+=misc_code\n          data=string.replace(data,unicode_str,misc_code)\n        if last>=2 and last<=11:\n          retval+=val[0]\n          data=string.replace(data,unicode_str,val[0])\n        elif last>=12 and last<=15:\n          retval+=val[1]\n          data=string.replace(data,unicode_str,val[1])\n        else:\n          retval+=misc_code\n          data=string.replace(data,unicode_str,misc_code)\n      elif uni[0:3]=='1E8':\n        last=uni[3:]\n        if last in hex_dict:\n          last=hex_dict[last]\n        try:\n          last=int(last)\n        except:\n          retval+=misc_code\n          data=string.replace(data,unicode_str,misc_code)\n        if last>=0 and last<=9:\n          retval+=val[0]\n          data=string.replace(data,unicode_str,val[0])\n        elif last>=10 and last<=13:\n          retval+=val[1]\n          data=string.replace(data,unicode_str,val[1])\n        else:\n          retval+=misc_code\n          data=string.replace(data,unicode_str,misc_code)\n      elif uni[0:3]=='1EC':\n        last=uni[3:]\n        if last in hex_dict:\n          last=hex_dict[last]\n        try:\n          last=int(last)\n        except:\n          retval+=misc_code\n          data=string.replace(data,unicode_str,misc_code)\n        if last>=0 and last<=7:\n          retval+=val[0]\n          data=string.replace(data,unicode_str,val[0])\n        elif last<=11:\n          retval+=val[1]\n          data=string.replace(data,unicode_str,val[1])\n        elif last<=15:\n          retval+=val[2]\n          data=string.replace(data,unicode_str,val[2])\n        else:\n          retval+=misc_code\n          data=string.replace(data,unicode_str,misc_code)\n      elif uni[0:3]=='216' or uni[0:3]=='217':\n        last=uni[3:]\n        if last in hex_dict:\n          last=hex_dict[last]\n        try:\n          last=int(last)\n        except:\n          retval+=misc_code\n          data=string.replace(data,unicode_str,misc_code)\n        if last<=12:\n          retval+=str(last+1)\n          data=string.replace(data,unicode_str,str(last+1))\n        else:\n          retval+=misc_code\n          data=string.replace(data,unicode_str,misc_code)\n      elif uni[0:3]=='246' or uni[0:3]=='247':\n        last=uni[3:]\n        if last in hex_dict:\n          last=hex_dict[last]\n        try:\n          last=int(last)\n        except:\n          retval+=misc_code\n          data=string.replace(data,unicode_str,misc_code)\n        if uni[0:3]=='246':\n          retval+=str(last+1)\n          data=string.replace(data,unicode_str,str(last+1))\n        elif last<=3:\n          retval+=str(last+17)\n          data=string.replace(data,unicode_str,str(last+17))\n        else:\n          retval+=misc_code\n          data=string.replace(data,unicode_str,misc_code)\n    else:\n      retval+=misc_code\n      data = data.replace(unicode_str,misc_code)\n\n  if len(retval)==0:\n    retval=\"Sorry, no unicode strings were present\"\n  try:\n    data = data.decode('unicode-escape')\n  except UnicodeDecodeError:\n    pass\n  retval = retval.encode('unicode-escape').decode('unicode-escape')\n  return (retval, data)", "response": "Decode a string into a dictionary of unicode values."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprocesses a single document and store the results in the database.", "response": "def process_document(self, doc: Document):\n        \"\"\"\n        Add your code for processing the document\n        \"\"\"\n\n        text_segments = doc.select_segments(\"lexisnexis.doc_description\")\n        for text_segment in text_segments:\n            split_sentences = doc.extract(self.sentence_extractor, text_segment)\n            doc.store(split_sentences, 'split_sentences')\n        # for t, u in zip(text_to_be_split, units_of_text):\n        #     split_sentences = doc.extract(self.sentence_extractor, t)\n        #     u.store(split_sentences, \"split_sentences\")\n        return list()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_arguments(parser):\n    parser.description = 'Examples:\\n' \\\n                         'python -m etk regex_extractor pattern /tmp/date.txt\\n' \\\n                         'cat /tmp/date.txt | python -m etk regex_extractor pattern'\n    parser.add_argument('pattern', nargs='?', type=str, default=sys.stdin)\n    parser.add_argument('input_file', nargs='?', type=argparse.FileType('r'), default=sys.stdin)", "response": "Add command line arguments to the argument parser."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nextracts the content of the current language from the input file.", "response": "def run(args):\n    \"\"\"\n    Args:\n        args (argparse.Namespace)\n    \"\"\"\n    html_metadata_extractor = HTMLMetadataExtractor()\n\n    with warnings.catch_warnings():\n        warnings.simplefilter('ignore')\n\n        extractions = html_metadata_extractor.extract(html_text=args.input_file)\n        for e in extractions:\n            print(e.value)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef store(self, extractions: List[Extraction], attribute: str, group_by_tags: bool = True) -> None:\n        if not isinstance(self._value, dict):\n            raise StoreExtractionError(\"segment is type: \" + str(type(self._value)))\n\n        if not len(extractions):\n            return\n\n        if group_by_tags:\n            try:\n                next(x for x in extractions if x.tag)  # if there is at least one extraction with a tag\n                if attribute not in self._extractions:\n                    self._extractions[attribute] = set([])\n                    self._value[attribute] = {}\n                extraction_provenances = {}\n                for e in extractions:\n                    tag = e.tag if e.tag else 'NO_TAGS'\n                    if tag not in self.value[attribute]:\n                        self.value[attribute][tag] = [e.value]\n                    else:\n                        if e.value not in self.value[attribute][tag]:\n                            self.value[attribute][tag].append(e.value)\n                    # TODO: handle provenance of non literals\n                    if isinstance(e.value, Number) or isinstance(e.value, str):\n                        extraction_provenances[e.value] = e.prov_id\n                self._extractions[attribute] = self._extractions[attribute].union(extractions)\n                new_id = self._document.provenance_id_index  # for the purpose of provenance hierarrchy tracking\n                storage_provenance_record: StorageProvenanceRecord = StorageProvenanceRecord(new_id, self.json_path,\n                                                                                             attribute,\n                                                                                             extraction_provenances,\n                                                                                             self.document)\n                self._document.provenance_id_index_incrementer()\n                self._document.provenances[new_id] = storage_provenance_record\n                self.create_storage_provenance(storage_provenance_record)\n                return\n            except StopIteration:\n                pass\n\n        if attribute not in self._extractions:\n            self._extractions[attribute] = set([])\n            self._value[attribute] = list()\n\n        self._extractions[attribute] = self._extractions[attribute].union(extractions)\n        extraction_provenances = dict()\n        for a_extraction in extractions:\n            # TODO: handle provenance of non literals\n            if isinstance(a_extraction.value, Number) or isinstance(a_extraction.value, str):\n                extraction_provenances[a_extraction.value] = a_extraction.prov_id\n            if a_extraction.value not in self._value[attribute]:\n                self._value[attribute].append(a_extraction.value)\n\n        new_id = self._document.provenance_id_index  # for the purpose of provenance hierarchy tracking\n        storage_provenance_record: StorageProvenanceRecord = StorageProvenanceRecord(new_id, self.json_path, attribute,\n                                                                                     extraction_provenances,\n                                                                                     self.document)\n        self._document.provenance_id_index_incrementer()\n        self._document.provenances[new_id] = storage_provenance_record\n        self.create_storage_provenance(storage_provenance_record)", "response": "Stores the value of the given attribute in the provenance."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nextract email message information if it uses the old Mailman format", "response": "def old_format(self, content: BeautifulSoup) -> List[str]:\n\n        \"\"\"\n        Extracts email message information if it uses the old Mailman format\n        Args:\n            content: BeautifulSoup\n\n        Returns: List[str]\n        \"\"\"\n        \n        b = content.find('body')\n        sender, date, nxt, rep_to = None, None, None, None\n        strongs = b.findAll('strong', recursive=False)\n        for s in strongs:\n            field = str(s).split(\">\")[1].split(\"<\")[0]\n            if 'From' in field:\n                sender = s.next_sibling.split(\"(\")[0].strip()\n            elif 'Date' in field:\n                date_str = s.next_sibling.strip().replace(\"-\",\"\").replace(\"  \",\" \").strip()\n                try:\n                    date = parsedate_to_datetime(date_str).isoformat()[:19]\n                except:\n                    date = None\n        sender = b.find('b').text if sender == None else sender\n        sender = b.find('a').text if len(sender) == 0 else sender\n        date = b.find('i').text[:19] if date == None else date\n\n        try:\n            nav = content.find('ul').findAll('li')\n        except:\n            nav = None\n        if nav != None:\n            for l in nav:\n                s = l.text\n                if 'Next in thread' in s:\n                    nxt = '/'.join(self.email_url.split('/')[:-1]) + '/' + l.find('a')['href']\n                    nxt = nxt[1:] if nxt[0] == '/' else nxt\n                elif 'reply to' in s:\n                    rep_to = '/'.join(self.email_url.split('/')[:-1]) + '/' + l.find('a')['href']\n                    rep_to = rep_to[1:] if rep_to[0] == '/' else rep_to\n        body = content.find('pre')\n        body = body.text.strip() if body != None else None\n        return [str(i) for i in [sender, date, body, nxt, rep_to]]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nextract email message information if it uses the new Mailman format", "response": "def new_format(self, navbar: BeautifulSoup, content: BeautifulSoup) -> List[str]:\n\n        \"\"\"\n        Extracts email message information if it uses the new Mailman format\n        Args:\n            content: BeautifulSoup\n\n        Returns: List[str]\n        \"\"\"\n        \n        sender = content.find(id='from').text.split('via')[0][6:].strip()\n        date_str = content.find(id='date').text.split(': ')[1].strip()\n        date = parsedate_to_datetime(date_str).isoformat()[:19]\n        body = content.find(id='body').text.strip()\n        nxt, rep_to = None, None\n        \n        links = navbar.findAll('a')\n        for l in links:\n            if 'Next in thread' in str(l):\n                nxt = '/'.join(self.email_url.split('/')[:-1]) + '/' + l['href']\n                nxt = nxt[1:] if nxt[0] == '/' else nxt\n            elif 'reply to' in str(l):\n                rep_to = '/'.join(self.email_url.split('/')[:-1]) + '/' + l['href']\n                rep_to = rep_to[1:] if rep_to[0] == '/' else rep_to\n        return [str(i) for i in [sender, date, body, nxt, rep_to]]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nextracts and structures email message from UTF8 - encoded text.", "response": "def extract(self, text: str) -> List[Extraction]:\n\n        \"\"\"\n        Extracts and structures email message from UTF8-encoded text\n        Args:\n            text: str\n\n        Returns: Extraction\n        \"\"\"\n        \n        content = BeautifulSoup(text, 'html5lib')\n        subject = content.find('h1').text.strip()\n        recip = self.mailing_list_name\n        \n        navbar = content.find(id='navbar')\n        if navbar == None:\n            info = self.old_format(content)\n        else:\n            info = self.new_format(navbar, content)\n        for i in info[0:3]:\n            if i == 'None':\n                print('missed something important')\n        sender = info[0]\n        date   = info[1]\n        body   = info[2]\n        nxt    = info[3]\n        rep_to = info[4]\n        pub = 'SeeSat_Obs'\n        dRec = datetime.datetime.now().isoformat()\n        \n        msg_obj = { \n            'url' : self.email_url,\n            '@context' : {\n                '@vocab' : 'schema.org'\n            },\n            'subject' : subject,\n            'recip' : recip,\n            'sender' : sender\n        }\n        if date != 'None':\n            msg_obj['dateReceived'] = date\n        if body != 'None':\n            msg_obj['body'] = body\n        if nxt != 'None':\n            msg_obj['nxt'] = nxt\n        if rep_to != 'None':\n            msg_obj['replyToMessage'] = rep_to\n        return Extraction(value=msg_obj, extractor_name=self.name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef loadRule(rule_json_object):\n    name = rule_json_object['name']\n    rule_type = rule_json_object['rule_type']\n    validation_regex = None\n    required = False\n    removehtml = False\n    include_end_regex = False #Default to false for bakward compatibility\n    strip_end_regex = None\n    sub_rules = []\n    begin_stripe_id = None\n    end_stripe_id = None\n    begin_shift = 0\n    end_shift = 0\n\n    if 'sub_rules' in rule_json_object:\n        sub_rules = rule_json_object['sub_rules']\n    \n    if 'validation_regex' in rule_json_object:\n        validation_regex = rule_json_object['validation_regex']\n    \n    if 'required' in rule_json_object:\n        required = rule_json_object['required']\n    \n    if 'removehtml' in rule_json_object:\n        removehtml = rule_json_object['removehtml']\n        \n    if 'include_end_regex' in rule_json_object:\n        include_end_regex = rule_json_object['include_end_regex']\n    \n    if 'strip_end_regex' in rule_json_object:\n        strip_end_regex = rule_json_object['strip_end_regex']\n\n    if 'begin_stripe_id' in rule_json_object:\n        begin_stripe_id = rule_json_object['begin_stripe_id']\n\n    if 'end_stripe_id' in rule_json_object:\n        end_stripe_id = rule_json_object['end_stripe_id']\n\n    if 'begin_shift' in rule_json_object:\n        begin_shift = rule_json_object['begin_shift']\n\n    if 'end_shift' in rule_json_object:\n        end_shift = rule_json_object['end_shift']\n\n    rule = {}\n    \n    \"\"\" This is where we add our new type \"\"\"\n    if rule_type == ITEM_RULE or rule_type == 'RegexRule':\n        begin_regex = rule_json_object['begin_regex']\n        end_regex = rule_json_object['end_regex']\n        rule = ItemRule(name, begin_regex, end_regex, include_end_regex, strip_end_regex, validation_regex, required,\n                        removehtml, sub_rules, begin_stripe_id, end_stripe_id, begin_shift, end_shift)\n    if rule_type == ITERATION_RULE or rule_type == 'RegexIterationRule':\n        begin_regex = rule_json_object['begin_regex']\n        end_regex = rule_json_object['end_regex']\n        iter_begin_regex = rule_json_object['iter_begin_regex']\n        iter_end_regex = rule_json_object['iter_end_regex']\n        no_first_begin_iter_rule = False\n        if 'no_first_begin_iter_rule' in rule_json_object:\n            no_first_begin_iter_rule = rule_json_object['no_first_begin_iter_rule']\n        no_last_end_iter_rule = False\n        if 'no_last_end_iter_rule' in rule_json_object:\n            no_last_end_iter_rule = rule_json_object['no_last_end_iter_rule']\n        \n        rule = IterationRule(name, begin_regex, end_regex, iter_begin_regex, iter_end_regex,\n                                  include_end_regex, strip_end_regex, no_first_begin_iter_rule,\n                                  no_last_end_iter_rule, validation_regex, required, removehtml,\n                                  sub_rules, begin_shift=begin_shift, end_shift=end_shift)\n\n    if 'id' in rule_json_object:\n        rule.id = rule_json_object['id']\n    \n    return rule", "response": "Method to load the rules from a json object"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading the input file/content and return a list of Document(s) Args: table_str: use this parameter, if you are 100% sure that the content is a csv filename: use this parameter if the file extension is one of tab, csv, tsv, xls, xlsx file_content: if the input has some arbitrary extension, read it yourself and pass the contents along file_type: use this parameter with file_content, can be tsv, csv, etc sheet_name: sheet name as in xls or xlsx files dataset: user provided string to be added to output Document(s) nested_key: user provided string to be added to output Document(s) doc_id_field: specify this field(should be present in the input file), its value will be used as doc_id Returns: List[Document]", "response": "def tabular_extractor(self, table_str: str = None, filename: str = None,\n                          file_content=None,\n                          file_type=None,\n                          sheet_name: str = None,\n                          dataset: str = None,\n                          nested_key: str = None,\n                          doc_id_field: str = None,\n                          encoding=None) -> List[Document]:\n        \"\"\"\n        Read the input file/content and return a list of Document(s)\n        Args:\n            table_str: use this parameter, if you are 100% sure that the content is a csv\n            filename: use this parameter if the file extension is one of tab, csv, tsv, xls, xlsx\n            file_content: if the input has some arbitrary extension, read it yourself and pass the contents along\n            file_type: use this parameter with file_content, can be tsv, csv, etc\n            sheet_name: sheet name as in xls or xlsx files\n            dataset: user provided string to be added to output Document(s)\n            nested_key: user provided string to be added to output Document(s)\n            doc_id_field: specify this field(should be present in the input file), its value will be used as doc_id\n\n        Returns: List[Document]\n\n        \"\"\"\n        data = list()\n\n        if table_str is not None and filename is not None:\n            raise InvalidArgumentsError(message=\"for arguments 'table_str' and 'filename', please specify only one \"\n                                                \"argument!\")\n\n        elif table_str is not None:\n            f = StringIO(table_str)\n            reader = csv.reader(f, delimiter=',')\n            for row in reader:\n                data.append(row)\n        elif filename is not None:\n            # always read the entire file first\n            fn, extension = os.path.splitext(filename)\n            extension = extension.lower()\n\n            if extension in self._get_data_function:\n                get_data = self._get_data_function[extension]\n            else:\n                # in pyexcel we trust\n                # if there is an extension we have not mapped, just let pyexcel figure it out\n                get_data = pyexcel_io.get_data\n\n            try:\n                if file_content and file_type:\n                    data = get_data(file_content, file_type=file_type, auto_detect_datetime=False,\n                                    auto_detect_float=False, encoding=encoding if encoding else \"utf-8\")\n                else:\n                    data = get_data(filename, auto_detect_datetime=False,\n                                    auto_detect_float=False, encoding=encoding if encoding else \"utf-8\")\n            except:\n                try:\n                    data = get_data(filename, auto_detect_datetime=False,\n                                    auto_detect_float=False, encoding=\"latin_1\")\n                except:\n                    data = get_data(filename, auto_detect_datetime=False,\n                                    auto_detect_float=False, encoding=\"utf-8-sig\")\n\n            if extension == '.xls' or extension == '.xlsx':\n                if sheet_name is None:\n                    sheet_name = list(data.keys())[0]\n\n                data = data[sheet_name]\n            else:\n                data = data[file_type] if file_type else data[fn.split('/')[-1] + extension]\n\n        table_content, heading = self.content_recognizer(data)\n\n        return self.create_documents(rows=table_content,\n                                     heading=heading,\n                                     file_name=filename,\n                                     dataset=dataset,\n                                     nested_key=nested_key,\n                                     doc_id_field=doc_id_field)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses input text into URIRef", "response": "def parse_uri(self, text: str) -> URIRef:\n        \"\"\"\n        Parse input text into URI\n\n        :param text: can be one of\n              1. URI, directly return\n              2. prefix:name, query namespace for prefix, return expanded URI\n              3. name, use default namespace to expand it and return it\n        :return: URIRef\n        \"\"\"\n        if self.check_uriref(text):\n            return self.check_uriref(text)\n        elif isinstance(text, str):\n            text = text.strip()\n            m = URI_ABBR_PATTERN.match(text)\n            if m:\n                prefix, name = m.groups()\n                base = self.store.namespace(prefix if prefix else '')\n                if not base:\n                    raise PrefixNotFoundException(\"Prefix: %s\", prefix)\n                return URIRef(base + name)\n        raise WrongFormatURIException()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbinds a given prefix to a given namespace.", "response": "def bind(self, prefix: str, namespace: str, override=True, replace=False):\n        \"\"\"\n        bind a given namespace to the prefix, forbids same prefix with different namespace\n\n        :param prefix:\n        :param namespace:\n        :param override: if override, rebind, even if the given namespace is already bound to another prefix.\n        :param replace: if replace, replace any existing prefix with the new namespace\n        \"\"\"\n        namespace = URIRef(str(namespace))\n        # When documenting explain that override only applies in what cases\n        if prefix is None:\n            prefix = ''\n        bound_namespace = self.store.namespace(prefix)\n        # Check if the bound_namespace contains a URI and if so convert it into a URIRef for\n        # comparison. This is to prevent duplicate namespaces with the same URI.\n        if bound_namespace:\n            bound_namespace = URIRef(bound_namespace)\n        if bound_namespace and bound_namespace != namespace:\n\n            if replace:\n                self.store.bind(prefix, namespace)\n            # prefix already in use for different namespace\n            raise PrefixAlreadyUsedException(\"Prefix (%s, %s) already used, instead of (%s, %s).\",\n                                             prefix, self.store.namespace(prefix).toPython(),\n                                             prefix, namespace.toPython())\n        else:\n            bound_prefix = self.store.prefix(namespace)\n            if bound_prefix is None:\n                self.store.bind(prefix, namespace)\n            elif bound_prefix == prefix:\n                pass  # already bound\n            else:\n                if override or bound_prefix.startswith(\"_\"):\n                    self.store.bind(prefix, namespace)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a Document object from a JSON object containing a document in CDR format.", "response": "def create_document(self, doc: Dict, mime_type: str = None, url: str = \"http://ex.com/123\",\n                        doc_id=None, type_=None) -> Document:\n        \"\"\"\n        Factory method to wrap input JSON docs in an ETK Document object.\n\n        Args:\n            doc (object): a JSON object containing a document in CDR format.\n            mime_type (str): if doc is a string, the mime_type tells what it is\n            url (str): if the doc came from the web, specifies the URL for it\n            doc_id\n            type_\n\n        Returns: wrapped Document\n\n        \"\"\"\n        return Document(self, doc, mime_type, url, doc_id=doc_id).with_type(type_)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_json_path(self, jsonpath):\n\n        \"\"\"\n        Parse a jsonpath\n\n        Args:\n            jsonpath: str\n\n        Returns: a parsed json path\n\n        \"\"\"\n        if jsonpath not in self.parsed:\n            try:\n                self.parsed[jsonpath] = self.parser(jsonpath)\n            except Exception:\n                self.log(\"Invalid Json Path: \" + jsonpath, \"error\")\n                raise InvalidJsonPathError(\"Invalid Json Path\")\n\n        return self.parsed[jsonpath]", "response": "Parse a jsonpath and return a parsed json path"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef process_ems(self, doc: Document) -> List[Document]:\n        new_docs = list()\n\n        for a_em in self.em_lst:\n            if a_em.document_selector(doc):\n                self.log(\" processing with \" + str(type(a_em)) + \". Process\", \"info\", doc.doc_id, doc.url)\n                fresh_docs = a_em.process_document(doc)\n                # Allow ETKModules to return nothing in lieu of an empty list (people forget to return empty list)\n                if fresh_docs:\n                    new_docs.extend(fresh_docs)\n            # try:\n            #     if a_em.document_selector(doc):\n            #         self.log(\" processing with \" + str(type(a_em)) + \". Process\", \"info\", doc.doc_id, doc.url)\n            #         new_docs.extend(a_em.process_document(doc))\n            # except Exception as e:\n            #     if self.error_policy == ErrorPolicy.THROW_EXTRACTION:\n            #         self.log(str(e) + \" processing with \" + str(type(a_em)) + \". Continue\", \"error\", doc.doc_id,\n            #                  doc.url)\n            #         continue\n            #     if self.error_policy == ErrorPolicy.THROW_DOCUMENT:\n            #         self.log(str(e) + \" processing with \" + str(type(a_em)) + \". Throw doc\", \"error\", doc.doc_id,\n            #                  doc.url)\n            #         return list()\n            #     if self.error_policy == ErrorPolicy.RAISE:\n            #         self.log(str(e) + \" processing with \" + str(type(a_em)), \"error\", doc.doc_id, doc.url)\n            #         raise e\n\n        # Do house cleaning.\n        doc.insert_kg_into_cdr()\n        if not self.generate_json_ld:\n            if \"knowledge_graph\" in doc.cdr_document:\n                doc.cdr_document[\"knowledge_graph\"].pop(\"@context\", None)\n        Utility.make_json_serializable(doc.cdr_document)\n\n        if self.output_kg_only:\n            doc = doc.kg.value\n        elif not doc.doc_id:\n            doc.doc_id = Utility.create_doc_id_from_json(doc.cdr_document)\n\n        results = [doc]\n        for new_doc in new_docs:\n            results.extend(self.process_ems(new_doc))\n\n        return results", "response": "Process the given document and return a list of Document objects."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_glossary(file_path: str, read_json=False) -> List[str]:\n        if read_json:\n            if file_path.endswith(\".gz\"):\n                return json.load(gzip.open(file_path))\n            return json.load(open(file_path))\n\n        return open(file_path).read().splitlines()", "response": "A glossary is a text file one entry per line."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load_spacy_rule(file_path: str) -> Dict:\n        with open(file_path) as fp:\n            return json.load(fp)", "response": "A spacy rule file is a json file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_ems(self, modules_paths: List[str]):\n        all_em_lst = []\n        if modules_paths:\n            for modules_path in modules_paths:\n                em_lst = []\n                try:\n                    for file_name in os.listdir(modules_path):\n                        if file_name.startswith(\"em_\") and file_name.endswith(\".py\"):\n                            sys.path.append(modules_path)  # append module dir path\n                            this_module = importlib.import_module(file_name[:-3])\n                            for em in self.classes_in_module(this_module):\n                                em_lst.append(em(self))\n                except:\n                    self.log(\"Error when loading etk modules from \" + modules_path, \"error\")\n                    raise NotGetETKModuleError(\"Wrong file path for ETK modules\")\n                all_em_lst += em_lst\n\n        try:\n            all_em_lst = self.topological_sort(all_em_lst)\n        except Exception:\n            self.log(\"Topological sort for ETK modules fails\", \"error\")\n            raise NotGetETKModuleError(\"Topological sort for ETK modules fails\")\n\n        # if not all_em_lst:\n        #     self.log(\"No ETK module in \" + str(modules_paths), \"error\")\n        #     raise NotGetETKModuleError(\"No ETK module in dir, module file should start with em_, end with .py\")\n        return all_em_lst", "response": "Load all extraction modules from the given list of modules paths."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef classes_in_module(module) -> List:\n        md = module.__dict__\n        return [\n            md[c] for c in md if (\n                    isinstance(md[c], type) and\n                    issubclass(md[c], ETKModule\n                               ) and\n                    md[c].__module__ == module.__name__)\n        ]", "response": "Returns all classes in the specified module which have super class ExtractionModule\n clf."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nextracting the last set of entries from the input text.", "response": "def extract(self, text: str, confidence=0.5, filter=['Person', 'Place', 'Organisation']) -> List[Extraction]:\n        \"\"\"\n            Extract with the input text, confidence and fields filter to be used.\n            Args:\n                text (str): text input to be annotated\n                confidence (float): the confidence of the annotation\n                filter (List[str]): the fields that to be extracted\n\n            Returns:\n                List[Extraction]\n        \"\"\"\n\n        filter = ','.join(filter)\n        search_data = [('confidence', confidence),\n                       ('text', text),\n                       ('types', filter)]\n        search_headers = {'Accept': 'application/json'}\n        r = requests.post(self._search_url,\n                          data=search_data,\n                          headers=search_headers)\n        results = r.json()\n\n        last_results = self._combiner(results)\n        return last_results"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run(args):\n    regex_extractor = RegexExtractor(pattern=args.pattern)\n\n    with warnings.catch_warnings():\n        warnings.simplefilter('ignore')\n\n        for line in args.input_file:\n            extractions = regex_extractor.extract(line)\n            for e in extractions:\n                print(e.value)", "response": "A simple command line tool to extract the items from the sequence of items in the sequence."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef summary(self, html_partial=False):\n        try:\n            ruthless = True\n            \n            #Added recall priority flag\n            recallPriority = self.recallPriority\n            if recallPriority:\n                ruthless = False\n                self.TEXT_LENGTH_THRESHOLD = 2\n                self.RETRY_LENGTH = 25\n\n            while True:\n                self._html(True)\n\n                for i in self.tags(self.html, 'script', 'style'):\n                    i.drop_tree()\n                for i in self.tags(self.html, 'body'):\n                    i.set('id', 'readabilityBody')\n\n                if ruthless:\n                    self.remove_unlikely_candidates()\n\n                self.transform_misused_divs_into_paragraphs()\n                candidates = self.score_paragraphs()\n\n                best_candidates = self.select_best_candidates(candidates)\n\n\n                if best_candidates and not recallPriority:\n                    article = self.get_article_from_candidates(candidates,best_candidates,html_partial)\n                else:\n                    if ruthless and not recallPriority:\n                        log.debug(\"ruthless removal did not work. \")\n                        ruthless = False\n                        self.debug(\n                            (\"ended up stripping too much - \"\n                             \"going for a safer _parse\"))\n                        # try again\n                        continue\n                    else:\n\n                        log.debug(\n                            (\"Ruthless and lenient parsing did not work. \"\n                             \"Returning raw html\"))\n                        article = self.html.find('body')\n                        if article is None:\n                            article = self.html\n\n                cleaned_article = self.sanitize(article, candidates)\n                # print(cleaned_article)\n\n                article_length = len(cleaned_article or '')\n                retry_length = self.options.get(\n                    'retry_length',\n                    self.RETRY_LENGTH)\n\n                of_acceptable_length = article_length >= retry_length\n                if ruthless and not of_acceptable_length:\n                    ruthless = False\n                    continue\n                else:\n                    return cleaned_article\n        except Exception as e:\n            print(\"error: %s\", e)\n            log.exception('error getting summary: ')\n            raise Exception(Unparseable(str(e)), None, sys.exc_info()[2])", "response": "Generate the summary of the html document."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts the value of an object to a string using joiner", "response": "def get_string(self, joiner: str = \" \") -> str:\n        \"\"\"\n        Args:\n            joiner(str): if the value of an extractable is not a string, join the elements\n            using this string to separate them.\n\n        Returns: the value of the segment as a string, using a default method to convert\n        objects to strings.\n        \"\"\"\n        if not self._value:\n            return \"\"\n        elif isinstance(self._value, list):\n            return self.list2str(self._value, joiner)\n        elif isinstance(self._value, dict):\n            return self.dict2str(self._value, joiner)\n        elif isinstance(self.value, numbers.Number):\n            return str(self.value)\n        elif isinstance(self._value, datetime.date):\n            return self._value.strftime(\"%Y-%m-%d\")\n        elif isinstance(self._value, datetime.datetime):\n            return self._value.isoformat()\n        else:\n            return self._value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting list to str as input for tokenizer", "response": "def list2str(self, l: List, joiner: str) -> str:\n        \"\"\"\n        Convert list to str as input for tokenizer\n\n        Args:\n            l (list): list for converting\n            joiner (str): join the elements using this string to separate them.\n\n        Returns: the value of the list as a string\n\n        \"\"\"\n        result = str()\n        for item in l:\n            if isinstance(item, list):\n                result = result + self.list2str(item, joiner) + joiner\n            elif isinstance(item, dict):\n                result = result + self.dict2str(item, joiner) + joiner\n            elif item:\n                result = result + str(item) + joiner\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dict2str(self, d: Dict, joiner: str) -> str:\n        result = str()\n        for key in d:\n            result = result + str(key) + \" : \"\n            if isinstance(d[key], list):\n                result = result + self.list2str(d[key], joiner) + joiner\n            elif isinstance(d[key], dict):\n                result = result + self.dict2str(d[key], joiner) + joiner\n            elif d[key]:\n                result = result + str(d[key]) + joiner\n        return result", "response": "Convert dict to str as input for tokenizer\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_tokens(self, tokenizer: Tokenizer) -> List[Token]:\n\n        if (self, tokenizer) in self.tokenize_results:\n            return self.tokenize_results[(self, tokenizer)]\n        else:\n            segment_value_for_tokenize = self.get_string()\n            tokens = tokenizer.tokenize(segment_value_for_tokenize)\n            self.tokenize_results[(self, tokenizer)] = tokens\n            return tokens", "response": "Tokenize this Extractable. get_string method."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd command line arguments to the argument parser.", "response": "def add_arguments(parser):\n    \"\"\"\n    Parse arguments\n    Args:\n        parser (argparse.ArgumentParser)\n    \"\"\"\n    parser.description = 'Triple re-ontologization'\n    parser.add_argument('-i', '--input-file', type=argparse.FileType('r'), dest='input_file')\n    parser.add_argument('--input-type', action='store', dest='input_type', default='nt')\n    parser.add_argument('-o', '--output-file', type=argparse.FileType('wb'), dest='output_file')\n    parser.add_argument('--output-type', action='store', dest='output_type', default='nt')\n    parser.add_argument('-q', '--query-file', type=argparse.FileType('r'), dest='query_file')\n    parser.add_argument('-s', '--chunk-size', action='store', dest='chunk_size', default=0)\n    parser.add_argument('-g', '--graph', type=argparse.FileType('r'), dest='graphs', nargs='+', default=[])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run(args):\n\n    with warnings.catch_warnings():\n        warnings.simplefilter('ignore')\n\n        query = prepareQuery(args.query_file.read())\n        ds = Dataset()\n\n        res_indices_prev = set()  # de-duplication\n        res_indices = set()\n\n        # create sub graphs\n        for f in args.graphs:\n            g = Graph(identifier=os.path.basename(f.name))\n            g.parse(data=f.read(), format='n3')\n            ds.add_graph(g)\n\n        # create and query data graph\n        for data in read_by_chunk(args.input_file, int(args.chunk_size)):\n            g = Graph(identifier='data')\n            g.parse(data=data, format=args.input_type)\n            ds.add_graph(g)\n            res = ds.query(query)\n\n            dedup_res_graph = Graph()\n            if len(res) != 0:\n                for r in res:\n                    tid = generate_index(r)\n                    res_indices.add(tid)\n                    if tid in res_indices_prev:  # duplicated\n                        continue\n                    dedup_res_graph.add(r)\n\n                if len(dedup_res_graph) > 0:\n                    ret = dedup_res_graph.serialize(format=args.output_type, encoding='utf-8')\n                    args.output_file.write(ret)\n\n            ds.remove_graph(g)\n            res_indices_prev = res_indices\n            res_indices = set()", "response": "This function is called by the command line interface to get the next available language code."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_date(date, date_format=None):\n\n        \"\"\"Convert a date to ISO8601 date format\n        input format: YYYY-MM-DD HH:MM:SS GMT (works less reliably for other TZs)\n        or            YYYY-MM-DD HH:MM:SS.0\n        or            YYYY-MM-DD\n        or            epoch (13 digit, indicating ms)\n        or            epoch (10 digit, indicating sec)\n        output format: iso8601\"\"\"\n        date = date.strip()\n\n        if date_format:\n            try:\n                if date_format.find('%Z') != -1:\n                    date_format = date_format.replace('%Z', '')\n                    match_object = re.search('(([-+])(\\d{2})(\\d{2}))', date)\n                    if match_object is None:\n                        match_object = re.search('(([-+])(\\d{2}):(\\d{2}))', date)\n                    tz = match_object.groups()\n\n                    dt = datetime.strptime(date.replace(tz[0], ''), date_format)\n                    delta = timedelta(hours=int(tz[2]), minutes=int(tz[3]))\n                    if tz[1] == '-': delta = delta * -1\n                    dt = dt + delta\n\n                    return dt\n\n                return datetime.strptime(date, date_format)\n            except Exception:\n                pass\n\n        try:\n            return datetime.strptime(date, \"%Y-%m-%d %H:%M:%S\")\n        except:\n            pass\n\n        try:\n            # Friday, October 2, 2015 1:35 AM\n            return datetime.strptime(date, \"%A, %B %d, %Y %I:%M %p\")\n        except:\n            pass\n\n        try:\n            # Friday, 2 October 2015, 18:23\n            return datetime.strptime(date, \"%A, %d %B %Y, %H:%M\")\n        except:\n            pass\n\n        try:\n            # Thu October 01st, 2015\n            return datetime.strptime(date, \"%a %B %dst, %Y\")\n        except:\n            pass\n\n        try:\n            # Thu October 02nd, 2015\n            return datetime.strptime(date, \"%a %B %dnd, %Y\")\n        except:\n            pass\n\n        try:\n            # Thu October 03rd, 2015\n            return datetime.strptime(date, \"%a %B %drd, %Y\")\n        except:\n            pass\n\n        try:\n            # Thu October 04th, 2015\n            return datetime.strptime(date, \"%a %B %dth, %Y\")\n        except:\n            pass\n\n        try:\n            return datetime.strptime(date, \"%Y-%m-%d %H:%M:%S %Z\")\n        except Exception:\n            pass\n\n        try:\n            return datetime.strptime(date, \"%A, %b %d, %Y\")\n        except Exception:\n            pass\n\n        try:\n            return datetime.strptime(date, \"%Y-%m-%d %H:%M:%S.0\")\n        except:\n            pass\n\n        try:\n            return datetime.strptime(date, \"%Y-%m-%d\")\n        except:\n            pass\n\n        try:\n            return datetime.strptime(date, \"%b %d, %Y\")\n        except:\n            pass\n\n        try:\n            return datetime.strptime(date, \"%B %d, %Y\")\n        except:\n            pass\n\n        try:\n            return datetime.strptime(date, \"%B %d, %Y %I:%M %p\")\n        except:\n            pass\n\n        try:\n            return datetime.strptime(date, \"%b %d, %Y at %I:%M %p\")\n        except:\n            pass\n\n        try:\n            return datetime.strptime(date, \"%m-%d-%Y\")\n        except:\n            pass\n\n        try:\n            return datetime.strptime(date, \"%Y-%m-%dT%H:%M:%S\")\n        except:\n            pass\n\n        try:\n            return datetime.strptime(date, \"%Y-%m-%dT%H:%M:%SZ\")\n        except:\n            pass\n\n        try:\n            return datetime.strptime(date, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n        except:\n            pass\n\n        try:\n            return datetime.strptime(date, \"%Y-%m-%dT%H:%M:%S.%f+00:00\")\n        except:\n            pass\n\n        try:\n            date = int(date)\n            if 1000000000000 < date < 9999999999999:\n                # 13 digit epoch\n                return datetime.fromtimestamp(mktime(gmtime(date / 1000)))\n        except:\n            pass\n\n        try:\n            date = int(date)\n            if 1000000000 < date < 9999999999:\n                # 10 digit epoch\n                return datetime.fromtimestamp(mktime(gmtime(date)))\n        except:\n            pass\n        # If all else fails, return empty\n        return None", "response": "Convert a date to ISO8601 date format"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef uri_from_fields(prefix, *fields):\n        string = '_'.join(AlignmentHelper.alpha_numeric(f.strip().lower(), '') for f in fields)\n\n        if len(string) == len(fields) - 1:\n            return ''\n\n        return prefix + string", "response": "Construct a URI out of the fields and prepends it with prefix"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_arguments(parser):\n    parser.description = 'Examples:\\n' \\\n                         'python -m etk html_content_extractor /tmp/input.html\\n' \\\n                         'cat /tmp/input.html | python -m etk html_content_extractor'\n    parser.add_argument('input_file', nargs='?', type=argparse.FileType('r'), default=sys.stdin)", "response": "Add arguments to parser."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse the ethnicity from the Backpage ad.", "response": "def parse_ethnicity(parts):\n  \"\"\"\n    Parse the ethnicity from the Backpage ad. Returns the higher level ethnicities associated with an ethnicity.\n    For example, if \"russian\" is found in the ad, this function will return [\"russian\", \"eastern_european\", \"white_non_hispanic\"].\n    This allows for us to look at ethnicities numerically and uniformally.\n    Note: The code for this function is pretty old and messy, but still works well enough for our purposes.\n      parts -> \n  \"\"\"\n\n  eastern_european = ['russian', 'ukrainian', 'moldova', 'bulgarian', 'slovakian', 'hungarian', 'romanian',\n  'polish', 'latvian', 'lithuanian', 'estonia', 'czech', 'croatian', 'bosnian', 'montenegro', 'macedonian', 'albanian',\n  'slovenian', 'serbian', 'kosovo', 'armenian', 'siberian', 'belarusian']\n  western_european = ['british', 'german', 'france', 'greek', 'italian', 'belgian', 'netherlands', 'swiss', 'irish',\n  'danish', 'sweden', 'finnish', 'norwegian', 'portugese', 'austrian', 'sanmarino', 'turkish', 'liechtenstein', 'australian',\n  'newzealand', 'andorra', 'luxembourg', 'israeli', 'jewish']\n  caribbean = ['bahamian', 'haitian', 'dominican', 'puertorican', 'jamaican', 'cuban', 'caymanislands', 'trinidad', 'caribbean',\n  'guadeloupe', 'martinique', 'barbados', 'saintlucia', 'stlucia', 'curacao', 'aruban', 'saintvincent', 'stvincent', 'creole',\n  'grenadines', 'grenada', 'barbuda', 'saintkitts', 'saintmartin', 'anguilla', 'virginislands', 'montserrat', 'saintbarthelemy']\n  south_central_american = ['guatemalan', 'belizean', 'honduras', 'nicaraguan', 'elsalvador', 'panamanian', 'costarican',\n  'colombian', 'columbian', 'venezuelan', 'ecuadorian', 'peruvian', 'bolivian', 'chilean', 'argentine', 'uruguayan', 'paraguayan',\n  'brazilian', 'guyana', 'suriname']\n  mexican = ['mexican']\n  spanish = ['spanish']\n  east_asian = ['thai', 'vietnamese', 'cambodian', 'malaysian', 'filipino', 'singaporean', 'indonesian', 'japanese',\n  'chinese', 'taiwanese', 'northkorean', 'southkorean', 'korean']\n  korean = ['northkorean', 'southkorean', 'korean']\n  south_asian = ['nepalese', 'bangladeshi', 'bhutanese', 'indian']\n  hawaiian_pacific_islanders = ['hawaiian', 'guamanian', 'newguinea', 'fiji', 'marianaislands', 'solomonislands', 'micronesia',\n  'tuvalu', 'samoan', 'vanuata', 'polynesia', 'cookislands', 'pitcaimislands', 'marshallese']\n  middle_eastern = ['iraqi', 'iranian', 'pakistani', 'afghan', 'kazakhstan', 'uzbekistan', 'tajikistan', 'turkmenistan',\n  'azerbaijan', 'kyrgyzstan', 'syrian', 'lebanese', 'jordanian', 'saudiarabian', 'unitedarabemirates', 'bahrain', 'kuwait',\n  'persian', 'kurdish', 'middleeastern']\n  north_african = ['egyptian', 'libyan', 'algerian', 'tunisian', 'moroccan', 'westernsaharan', 'mauritanian', 'senegal', 'djibouti']\n\n  # Broad, high level ethnicity classes\n  white_non_hispanic = eastern_european + western_european\n  hispanic_latino = caribbean + south_central_american + mexican + spanish\n  # Get tribe names!!!\n  american_indian = ['nativeamerican', 'canadian', 'alaskan', 'apache', 'aztec', 'cherokee', 'chinook', 'comanche',\n  'eskimo', 'incan', 'iroquois', 'kickapoo', 'mayan', 'mohave', 'mojave', 'navaho', 'navajo', 'seminole']\n  asian = east_asian + south_asian + hawaiian_pacific_islanders\n  midEast_nAfrica = middle_eastern + north_african\n  african_american = ['black', 'african american']\n  ss_african = ['gambia', 'bissau', 'guinea', 'sierraleone', 'liberian', 'ghana', 'malian', 'burkinafaso', 'beninese', 'nigerian',\n  'sudanese', 'eritrea', 'ethiopian', 'cameroon', 'centralafricanrepublic', 'somalian', 'gabon', 'congo', 'ugandan', 'kenyan',\n  'tanzanian', 'rwandan', 'burundi', 'angola', 'zambian', 'mozambique', 'malawi', 'zimbabwe', 'namibia', 'botswana',\n  'lesotho', 'southafrican', 'swaziland', 'madagascar', 'comoros', 'mauritius', 'saintdenis', 'seychelles', 'saotome']\n\n  # Add various identifying values to Category lists\n  white_non_hispanic.append('european')\n  white_non_hispanic.append('white')\n  hispanic_latino.extend(['hispanic', 'latina'])\n  asian.extend(['asian', 'oriental'])\n  midEast_nAfrica.extend(['arabian', 'muslim'])\n\n  # \"from ____\" to handle false positives as names\n  from_names = ['malaysia']\n\n  # One massive ethnicities list\n  ethnicities = white_non_hispanic + hispanic_latino + american_indian + asian + midEast_nAfrica + african_american + ss_african\n\n  num = 0\n  found = []\n  # Check each part of the body\n  clean_parts = []\n  for p in parts:\n    part = parser_helpers.clean_part_ethn(p)\n    clean_parts.append(part)\n\n    # handle \"from _____\" ethnicities to avoid false positives in names\n    for name in from_names:\n      if re.compile(r'from +' + re.escape(name)).search(part):\n        found.append(name)\n\n    if any(eth in part for eth in ethnicities):\n      # At least one ethnicity was found\n      for ethn in ethnicities:\n        if ethn in part:\n          index=part.index(ethn)\n          if (' no ' in part and part.index(' no ')+4==index) or ('no ' in part and part.index('no')==0 and part.index('no ') + 3==index) or ('.no ' in part and part.index('.no ') + 4==index): \n            pass\n          else:\n          # Found the current ethnicity\n            if ethn in ['black', 'african american']:\n              ethn = \"african_american\"\n            if ethn == 'white': \n              ethn = 'white_non_hispanic'\n            if ethn not in found:\n              # Add to Found list, check for subsets\n              found.append(ethn)\n              if ethn in eastern_european:\n                found.append(\"eastern_european\")\n              if ethn in western_european:\n                found.append(\"western_european\")\n              if ethn in caribbean:\n                found.append(\"caribbean\")\n              if ethn in south_central_american:\n                found.append(\"south_central_american\")\n              if ethn in east_asian:\n                found.append(\"east_asian\")\n              if ethn in south_asian:\n                found.append(\"south_asian_indian\")\n              if ethn in hawaiian_pacific_islanders:\n                found.append(\"hawaiian_pacific_islanders\")\n              if ethn in middle_eastern:\n                found.append(\"middle_eastern\")\n              if ethn in north_african:\n                found.append(\"north_african\")\n              # Check the most general ethnicity categories\n              if ethn in white_non_hispanic and \"white_non_hispanic\" not in found:\n                found.append(\"white_non_hispanic\")\n                num += 1\n              if ethn in hispanic_latino and \"hispanic_latino\" not in found:\n                found.append(\"hispanic_latino\")\n                num += 1\n              if ethn in american_indian and \"american_indian\" not in found:\n                found.append(\"american_indian\")\n                num += 1\n              if ethn in asian and \"asian\" not in found:\n                if ethn != \"asian\":\n                  found.append(\"asian\")\n                  num += 1\n              if ethn in midEast_nAfrica and \"midEast_nAfrican\" not in found:\n                found.append(\"midEast_nAfrican\")\n                num += 1\n              if ethn in ss_african and \"subsaharan_african\" not in found:\n                found.append(\"subsaharan_african\")\n                num += 1\n              if ethn == \"african_american\":\n                num += 1\n\n  # Remove ethnicity from all parts\n  output_parts = []\n  for p in clean_parts:\n    part = p\n    if any(eth in part for eth in found):\n      # Ethnicity(s) found in this part\n      for eth in found:\n        if eth in part:\n          # Remove ethnicity\n          part = re.sub(eth, \"\", part)\n\n    # Add part to output\n    if len(part) > 2:\n      output_parts.append(part)\n\n  # Check if there was more than one general ethnicity. If so, the ad is multi-racial.\n  if num > 1:\n    found.append(\"multiracial\")\n\n  found = list(set(found))\n\n  return (found, output_parts)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse the indicators of trafficking.", "response": "def parse_indicators(parts, ethnicity):\n  \"\"\"\n    Parses terms that may or may not be \"indicators\" of trafficking. Some terms are used for\n    non-trafficking related purposes (e.g. matching or identification problems). Also good to \n    note is that this list has been growing slowly for about 2 years and some indicators have\n    been removed/combined. Thus, you may notice that some numeric values are non-existent.\n\n    TODO: Move logic from hard-coded into JSON config file(s).\n    \n      parts -> The backpage ad's posting_body, separated into substrings\n      ethnicity -> The ethnicity list that we parsed for the ad\n  \"\"\"\n\n  ret_val=[]\n  for part in parts:\n    part=part.lower()\n    part = part.replace('virginia', '').replace('fresh pot', '')\n    part = re.sub(r'virgin ?island', '', part)\n    part = re.sub(r'no teen', '', part)\n\n    if re.compile(r'new ?to ?the ?(usa?|country)').search(part):\n      ret_val.append(1)\n    if \"natasha\" in part or \"svetlana\" in part:\n      ret_val.append(2)\n    if 'young' in part:\n      ret_val.append(3)\n    if re.compile(r'just *(hit|turned) *18').search(part):\n      ret_val.append(5)\n    if re.compile(r'fresh *meat').search(part):\n      ret_val.append(6)\n    if 'virgin' in part:\n      ret_val.append(7)\n    if 'foreign' in part:\n      ret_val.append(8)\n    if re.compile(r'(just|fresh)( *off *)?( *the *)boat').search(part):\n      ret_val.append(9)\n    if re.compile(r'fresh from over ?sea').search(part):\n      ret_val.append(9)\n    if re.compile(r'easy *sex').search(part):\n      ret_val.append(10)\n    if re.compile(r'come *chat *with *me').search(part):\n      ret_val.append(11)\n    if re.compile(r'\\b(massage|nuru)\\b').search(part):\n      ret_val.append(12)\n    if re.compile(r'escort *agency').search(part):\n      ret_val.append(13)\n    if re.compile(r'((https?)|www)\\.\\w{5,30}?.com').search(part):\n      ret_val.append(14)\n    if (re.compile(r'world( )*series').search(part) or re.compile(r'grand( )*slam').search(part) or\n       re.compile(r'base( )?ball').search(part) or re.compile(r'double( )?play').search(part) or\n       'cws' in part or re.compile(r'home( )?run').search(part) or re.compile(r'batter( )?up').search(part) or\n       re.compile(r'triple( )?play').search(part) or re.compile(r'strike( )?out').search(part) or\n       'sports' in part):\n      ret_val.append(15)\n    if (re.compile(r'new ?girls').search(part) or re.compile(r'new ?arrivals').search(part) or\n       re.compile(r'just ?in ?from ? \\w{3,15}\\W').search(part) or re.compile(r'new \\w{3,9} staff').search(part)):\n      ret_val.append(17)\n    if re.compile(r'brand *new').search(part):\n      ret_val.append(18)\n    if re.compile(r'coll(e|a)ge').search(part) and 15 not in ret_val:\n      ret_val.append(19)\n    if 'teen' in part:\n      ret_val.append(20)\n    if re.compile(r'high ?school').search(part):\n      ret_val.append(21)\n    if re.compile(r'daddy\\'?s? ?little ?girl').search(part):\n      ret_val.append(22)\n    if 'fresh' in part:\n      ret_val.append(23)\n    phrases = [(r'100%' + re.escape(eth)) for eth in ethnicity]\n    if any(re.compile(phrase).search(part) for phrase in phrases):\n      ret_val.append(24)\n    if re.compile(r'speaks? \\d\\d? language').search(part):\n      ret_val.append(25)\n    if re.compile(r'new to the (country|us)').search(part):\n      ret_val.append(26)\n    if re.compile(r'massage ?parlou?r').search(part):\n      ret_val.append(27)\n    if re.compile(r'come see us at ').search(part):\n      ret_val.append(28)\n    if (re.compile(r'420 ?friendly').search(part) or re.compile(r'party ?friendly').search(part) or \n        re.compile(r'420 ?sp').search(part) or ' 420 ' in part):\n      ret_val.append(30)\n    if 'under 35' in part:\n      ret_val.append(31)\n    if re.compile(r'\\b(avail(able)?|open) *24(/|\\\\|-)7\\b').search(part):\n      ret_val.append(33)\n    if re.compile(r'no ?(indian)').search(part) or re.compile(r'indians? not ((allow)|(welcome))'):\n      ret_val.append(36)\n    if re.compile(r'no ?(hispanic|mexican)').search(part) or re.compile(r'(hispanic|mexican)s? not ((allow)|(welcome))'):\n      ret_val.append(37)\n    if 'incall' in part:\n      ret_val.append(38)\n    if 'outcall' in part:\n      ret_val.append(39)\n\n    parts = part.split('from ')\n    parts.pop(0)\n    for p in parts:\n      p = re.sub(r' +', ' ', p)\n      if p.split(' ')[0].lower() in countries:\n        ret_val.append(27)\n        break\n    eastern_euro_countries = ['estonia', 'latvia', 'lithuania', 'armenia', 'russia', 'kazakhstan', 'ukrain', 'belarus',\n                              'moldova', 'czech', 'austria', 'croatia', 'hungary', 'poland', 'slovakia', 'slovenia',\n                              'albania', 'bosnia', 'bulgaria', 'greece', 'macedonia', 'romania']\n    if any(c in part for c in eastern_euro_countries):\n      ret_val.append(28)\n  ret_val = list(set(ret_val))\n  return ret_val"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_name(parts, main_names, common_names, debug=0):\n\n  lowercase_parts = [re.sub(r'(in|out)call', '', p.lower()) for p in parts]\n\n  start = time.time()\n\n  # Intros to common names\n  intros = {\n    'pre': ['my name is', 'i am', 'call me', 'call', 'text', 'my names', 'my name', 'known as', 'go by', 'Intro',\n            'ask for', 'call for', 'ask', 'this is', 'one and only', 'prevphonespothti', 'called'],\n    'post': ['is back', 'is here', 'in town', 'prevphonespothti', 'is ready', 'is available']\n  }\n  spanish_intros = ['hola soy', 'me llamo', 'mi nombre es', 'yo soy', 'pregunta por', 'encantada de conocerlo', 'hola papi']\n  intros['pre'].extend(spanish_intros)\n\n  # Regex intros to common names\n  rgx_intros = {\n    'pre': [r'\\b(?:it\\'s)\\b', r'\\b(?:it s)\\b', r'\\b(?:its)\\b', r'\\b(?:soy)\\b', r'\\b(?:es)\\b', r'\\b(?:hola)\\b', r'\\b(?:y?o?ur girl)\\b', r'\\b(?:i\\'m)\\b',\n            r'\\b(?:im)\\b', r'\\b(?:y?o?ur favorite girl)\\b', r'\\b(?:y?o?ur most favorite girl)\\b', r'\\bmy ?fr(?:i|e)(?:i|e)nd\\b', r'\\bm(?:s|z)\\.',\n            r'\\bmi(?:s{1,2}|z{1,2})'],\n    'post': [r'\\b(?:here)\\b', r'\\b(?:(?:i|\\')s (?:the|my) name)\\b']\n  }\n\n  # These words shouldn't follow common name matched from an intro\n  false_positives = set(['complexion', 'skin', 'hair', 'locks', 'eyes', 'st', 'ave', 'street', 'avenue', 'blvd', 'boulevard',\n                         'highway', 'circle', 'hwy', 'road', 'rd'])\n  vowels_with_y = set(list('aeiouy'))\n\n  uniques = set([])\n\n  for p in lowercase_parts:\n    part = p.lower()\n    part = re.sub(r\"(^| )i ?'? ?m \", \" Intro \", part).strip()\n    part = part.replace('<br>', ' ').replace('&amp;', ' and ')\n    part = re.sub(r'\\.+', ' ', part)\n    part = re.sub(r'x+', 'x', part)\n    part = re.sub(r'y+', 'y', part)\n    # Retain 'part' to be used for separating comma-separated names\n    part = re.sub(r',+', ' ', part)\n    part = re.sub(r' +', ' ', part)\n\n    builder = []\n    for pt in part.split():\n      if len(pt) > 2:\n        lastc = pt[len(pt)-1]\n        # Convert names that have repeated last letters and the last letters aren't \"E\" and aren't two consonants following a vowel\n        if lastc == pt[len(pt)-2] and not (lastc == 'e' or (pt[len(pt)-3] in vowels_with_y and lastc not in vowels_with_y)):\n          builder.append(pt[:len(pt)-1])\n        else:\n          builder.append(pt)\n      else:\n        builder.append(pt)\n    part = ' '.join(builder)\n\n    # Check if the part is entirely just a common word\n    ageless_title = re.sub(r' - \\d\\d', '', part.lower())\n    ageless_title = re.sub(r'\\W+', '', ageless_title)\n    if ageless_title in common_names or ageless_title in main_names:\n      uniques.add(ageless_title)\n      continue;\n\n    # Find common names that come immediately before or after a \"both-side intro\"\n    for k in intros:\n      for intro in intros[k]:\n        if intro in part:\n          pts = part.split(intro)\n          for i in range(1, len(pts)):\n            if k == 'post':\n              # Check left side of intro\n              ptl = re.sub(r'\\W', ' ', pts[i-1])\n              ptl = re.sub(r' +', ' ', ptl)\n              tokenized = ptl.split()\n              if tokenized and tokenized[len(tokenized)-1] and tokenized[len(tokenized)-1] in common_names:\n                uniques.add(tokenized[len(tokenized)-1])\n                break\n            else:\n              # Check right side of intro\n              ptr = re.sub(r'\\W', ' ', pts[i])\n              ptr = re.sub(r' +', ' ', ptr)\n              tokenized = ptr.split()\n              if tokenized and tokenized[0] in common_names:\n                if not (len(tokenized) > 1 and tokenized[1] in false_positives or (len(tokenized) > 2 and tokenized[2] in false_positives)):\n                  # Next 2 words are not false positives\n                  uniques.add(tokenized[0])\n                  break\n\n    # Check intros that include regexes\n    for k in rgx_intros:\n      for intro in rgx_intros[k]:\n        matches = list(re.findall(intro, part))\n        for match in matches:\n          pts = part.split(match)\n          for i in range(1, len(pts)):\n            if k == 'post':\n              # Check left side of intro\n              ptl = re.sub(r'\\W', ' ', pts[i-1])\n              ptl = re.sub(r' +', ' ', ptl)\n              tokenized = ptl.split()\n              if tokenized and tokenized[len(tokenized)-1] and tokenized[len(tokenized)-1] in common_names:\n                uniques.add(tokenized[len(tokenized)-1])\n                break\n            else:\n              # Check right side of intro\n              ptr = re.sub(r'\\W', ' ', pts[i])\n              ptr = re.sub(r' +', ' ', ptr)\n              tokenized = ptr.split()\n              if tokenized and tokenized[0] in common_names:\n                if not (len(tokenized) > 1 and tokenized[1] in false_positives or (len(tokenized) > 2 and tokenized[2] in false_positives)):\n                  # Next 2 words are not false positives\n                  uniques.add(tokenized[0])\n                  break\n\n    # Find regular names\n    tokens = list(re.split(r'\\W+', part))\n    for i in range(len(tokens)):\n      if not tokens[i]:\n        continue\n      curr = tokens[i]\n      # Check if current token has an 's' at the end (ex: \"brittanys beautiful body\")\n      if curr not in main_names and curr[len(curr)-1] == 's' and curr[:-1] in main_names:\n        curr = curr[:-1]\n      if curr in main_names:\n        # Check if name is a two-part name\n        if i > 0 and (''.join([tokens[i-1], curr]) in main_names or ''.join([tokens[i-1], curr]) in common_names):\n          # Prev token was a prefix to current\n          uniques.add(' '.join([tokens[i-1], curr]))\n          uniques.discard(tokens[i-1])\n        elif (i < len(tokens)-1 and tokens[i+1] and (''.join([tokens[i], tokens[i+1]]) in main_names or\n              ''.join([tokens[i], tokens[i+1]]) in common_names)):\n          # Current token has a suffix\n          uniques.add(' '.join([tokens[i], tokens[i+1]]))\n        elif (i < len(tokens)-1 and tokens[i+1] and tokens[i+1][len(tokens[i+1])-1] == 's' and (''.join([tokens[i], tokens[i+1][:-1]]) in main_names or\n              ''.join([tokens[i], tokens[i+1][:-1]]) in common_names)):\n          # Current token has a suffix with plural ending ('s')\n          uniques.add(' '.join([tokens[i], tokens[i+1][:-1]]))\n        else:\n          # Only single-word name\n          uniques.add(curr)\n\n    # Find common words that are part of \"pairing\" phrases, paired with names that we found already\n    pairings = set(['and', 'plus', 'with'])\n    for i in range(len(tokens)):\n      if tokens[i] not in uniques and tokens[i] in common_names:\n        if i > 1 and tokens[i-2] in uniques and tokens[i-1] in pairings:\n          # ex: \"jessica and diamond\"\n          uniques.add(tokens[i])\n        elif i < len(tokens)-2 and tokens[i+2] in uniques and tokens[i+1] in pairings:\n          # ex: \"diamond and jessica\"\n          uniques.add(tokens[i])\n\n  # Odd cases\n  if ('mary' in uniques or 'jane' in uniques or 'mary jane' in uniques) and re.search(r'mary\\W+jane', part):\n    uniques.discard('jane')\n    uniques.discard('mary')\n    uniques.discard('mary jane')\n  if 'crystal' in uniques and re.search(r'crystal ?(blue|spa|massage|parlor|city|stone)', part):\n    uniques.discard('crystal')\n      \n  # Remove names that are substrings of larger names\n  names_final = set([])\n  if isinstance(main_names, set):\n    # Name datasets are raw sets of names\n    for match in uniques:\n      if not any(match in name for name in [v for v in uniques if v != match]) and match:\n        names_final.add(match.strip())\n  else:\n    # Name datasets are misspelled names mapped to properly spelled names\n    for match in uniques:\n      nosp_match = match.replace(' ', '')\n      if not any(nosp_match in name for name in [v for v in uniques if v != nosp_match]) and nosp_match:\n        # add the parsed name, not the converted one (ex: don't change \"mickey\" to \"mikey\")\n        names_final.add(nosp_match)\n\n  if debug == 1:\n    print 'parse_name time taken: {} seconds'.format(time.time() - start)\n  return list(names_final)", "response": "Parse all names from a Backpage ad."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_no_blacks(parts):\n  match_patterns = [r'no ?black', r'no ?african', r'no ?aa', r'white ?(guys|men|clients) ?only',\n                   r'only ?white ?(guys|men|clients)']\n\n  remove_patterns = [r'no ?black ?or ?african', r'no ?african ?or ?black', r'men', r'guys']\n\n  output_parts = []\n  output_val = 0\n\n  # Check each part\n  for part in parts:\n    o_part = part\n\n    # Check all patterns\n    for m in match_patterns:\n      found = re.search(m, part)\n      if found != None:\n        # Found a 'no black allowed' phrase\n        output_val = 1\n\n        # Remove all relevant phrases\n        for p in remove_patterns:\n          o_part = re.sub(p, '', o_part)\n        o_part = re.sub(m, '', o_part)\n\n    # Append part to output (if it's not empty)\n    if len(o_part) > 2:\n      output_parts.append(o_part)\n  \n  return (output_val, output_parts)", "response": "Parse whether or not an ad indicates that there are no black clients allowed."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse the phone number from the backpage ad s posting_body separated into substrings parts - The backpage ad s posting_body separated into substrings allow_multiple - True will choose the most commonly occurring phone number otherwise will choose the most commonly occurring phone number", "response": "def parse_phone(parts, allow_multiple=False):\n  \"\"\"\n    Parse the phone number from the ad's parts\n      parts -> The backpage ad's posting_body, separated into substrings\n      allow_multiple -> If false, arbitrarily chooses the most commonly occurring phone\n  \"\"\"\n  \n  # Get text substitutions (ex: 'three' -> '3')\n  text_subs = misc.phone_text_subs()\n  Small = text_subs['Small']\n  Magnitude = text_subs['Magnitude']\n  Others = text_subs['Others']\n\n  phone_pattern = r'1?(?:[2-9][0-8][0-9])\\s?(?:[2-9][0-9]{2})\\s?(?:[0-9]{2})\\s?(?:[0-9]{2})'\n  phone_pattern_spaces = r'1?\\W?[2-9]\\W?[0-8]\\W?[0-9]\\W?[2-9]\\W?[0-9]\\W?[0-9]\\W?[0-9]\\W?[0-9]\\W?[0-9]\\W?[0-9]'\n\n  found_phones = []\n  return_parts = []\n  # Check each part for phone # and remove from parts if found\n  for part in parts:\n    body = part\n    # remove '420' references to avoid false positives\n    body = re.sub(r'420 ?friendly', '', body)\n    body = body.replace(' 420 ', '')\n    body = body.replace('420 sp', '')\n\n    # Replace all disguising characters in the body\n    for key in Small:\n      body = re.sub(r'-?'+re.escape(key)+r'-?', str(Small[key]), body)\n    for key in Magnitude:\n      body = re.sub(r'-?'+re.escape(key)+r'-?', str(Magnitude[key]), body)\n    for key in Others:\n      body = re.sub(r'-?'+re.escape(key)+r'-?', str(Others[key]), body)\n    body = re.sub(r'\\W', ' ', body)\n    body = re.sub(r' +', ' ', body)\n\n    if len(re.sub(r'\\D', '', body)) < 10:\n      # Less than 10 numeric digits in part - no phone number here\n      return_parts.append(part)\n      continue;\n\n    phones = re.findall(phone_pattern, body)\n\n    if len(phones) == 0:\n      # No phone number in standard format\n      phones = re.findall(phone_pattern_spaces, body)\n      if len(phones) > 0:\n        # Phone number had spaces between digits\n        for found in phones:\n          found_phones.append(re.sub(r'\\D', '', found))\n\n    else:\n      # Found phone in standard format\n      for found in phones:\n        found_phones.append(re.sub(r'\\D', '', found))\n\n    if found_phones:\n      # Phone has been found, remove from part)\n      for found in found_phones:\n        filtered_part = parser_helpers.remove_phone(part, found)\n      if re.sub(r'\\W', '', filtered_part):\n        # get rid of now-empty parts\n        return_parts.append(filtered_part)\n    else:\n      # Phone not found yet, add part to output\n      return_parts.append(part)\n\n  if not allow_multiple:\n    # Get most commonly occurring phone\n    found_phone = ''\n    if len(found_phones) > 0:\n      found_phone = max(set(found_phones), key=found_phones.count)\n\n    # Return the phone along with the original parts (minus any occurrences of the phone number)\n    return (found_phone, return_parts)\n  else:\n    # return all phones\n    return (list(set(found_phones)), return_parts)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_posting_id(text, city):\n  parts = text.split('Post ID: ')\n  if len(parts) == 2:\n    post_id = parts[1].split(' ')[0]\n    if post_id:\n      return post_id + post_id_bp_groups[city]", "response": "Parse the posting ID from the Backpage ad. \n text and city."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse for phrases that indicate a form of trucker friendly.", "response": "def parse_truckers(parts):\n  \"\"\"\n    Parse for phrases that indicate a form of \"trucker friendly\".\n      parts -> The backpage ad's posting_body, separated into substrings\n    Returns a tuple containing:\n      [0]: Binary result of whether or not ad is trucker-friendly\n      [1]: The input strings, minus the sections indicating trucker-friendly\n  \"\"\"\n\n  match_terms = ['truck', 'cabs', 'flying j']\n  match_patterns = [r'exit /d{1,4}', r'interstate /d{0,3}', r'i-/d/d?/d?', r'iowa ?80']\n\n  output_val = 0\n  output_parts = []\n  for p in parts:\n    part = p\n    if any(term in part for term in match_terms):\n      # found trucker string\n      output_val = 1\n      for term in match_terms: \n        # remove matched term\n        part = part.replace(term, '')\n    else:\n      # check for trucker patterns\n      for patt in match_patterns:\n        found = re.search(patt, part)\n        if found != None:\n          # found match pattern\n          output_val = 1\n          part = re.sub(patt, '', part) # remove matched term\n\n    if len(part) > 2:\n      output_parts.append(part)\n\n  return (output_val, output_parts)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef extract(self, text: str = None,\n                extract_first_date_only: bool = False,\n                additional_formats: List[str] = list(),\n                use_default_formats: bool = False,\n                ignore_dates_before: datetime.datetime = None,\n                ignore_dates_after: datetime.datetime = None,\n                detect_relative_dates: bool = False,\n                relative_base: datetime.datetime = None,\n                preferred_date_order: str = \"MDY\",\n                prefer_language_date_order: bool = True,\n                timezone: str = None,\n                to_timezone: str = None,\n                return_as_timezone_aware: bool = True,\n                prefer_day_of_month: str = \"first\",\n                prefer_dates_from: str = \"current\",\n                date_value_resolution: DateResolution = DateResolution.DAY,\n                ) -> List[Extraction]:\n        \"\"\"\n        Args:\n            text (str):  extract dates from this 'text', default to None\n            extract_first_date_only (bool): extract the first valid date only or extract all, default to False\n            additional_formats (List[str]):  user defined formats for extraction, default to empty list\n            use_default_formats (bool): if use default formats together with addtional_formats, default to False\n            ignore_dates_before (datetime.datetime): ignore dates before 'ignore_dates_before', default to None\n            ignore_dates_after (datetime.datetime): ignore dates after 'ignore_dates_after', default to None\n            detect_relative_dates (bool): if detect relative dates like '9 days before', default to False (pass in a ETK instance(with spaCy enabled) when init the DateExtractor is required for relative date extraction)\n            relative_base (datetime.datetime): offset relative dates detected based on 'relative_base', default to None\n            preferred_date_order (enum['MDY', 'DMY', 'YMD']): preferred date order when ambiguous, default to 'MDY'\n            prefer_language_date_order (bool): if use the text language's preferred order, default to True\n            timezone (str): add 'timezone' if there is no timezone information in the extracted date, default to None\n            to_timezone (str): convert all dates extracted to this timezone, default to None\n            return_as_timezone_aware (bool): returned datetime timezone awareness, default to None\n            prefer_day_of_month (enum['first', 'current', 'last']): use which day of the month when there is no 'day', default to 'first'\n            prefer_dates_from (enum['past', 'current', 'future']): use which date when there is few info(e.g. only month), default to 'current'\n            date_value_resolution (enum[DateResolution.SECOND, DateResolution.MINUTE, DateResolution.HOUR, \\\n                DateResolution.DAY, DateResolution.MONTH, DateResolution.YEAR]): specify resolution \\\n                when convert to iso format string, default to DateResolution.DAY\n\n        Returns:\n            List[Extraction]: List of extractions, the information including::\n\n                Extraction._value: iso format string,\n                Extraction._provenance: provenance information including:\n                {\n                    'start_char': int - start_char,\n                    'end_char': int - end_char\n                },\n                Extraction._addition_inf: additional information including:\n                {\n                    'date_object': datetime.datetime - the datetime object,\n                    'original_text': str - the original str extracted from text,\n                    'language': enum['en', 'es'] - language of the date\n                }\n        \"\"\"\n\n        if return_as_timezone_aware:\n            self._default_tz = pytz.timezone(timezone) if timezone else get_localzone()\n            if ignore_dates_before and not ignore_dates_before.tzinfo:\n                ignore_dates_before = ignore_dates_before.astimezone(self._default_tz)\n            if ignore_dates_after and not ignore_dates_after.tzinfo:\n                ignore_dates_after = ignore_dates_after.astimezone(self._default_tz)\n            if relative_base and not relative_base.tzinfo:\n                relative_base = relative_base.astimezone(self._default_tz)\n        else:\n            if ignore_dates_before and ignore_dates_before.tzinfo:\n                ignore_dates_before = ignore_dates_before.replace(tzinfo=None)\n            if ignore_dates_after and ignore_dates_after.tzinfo:\n                ignore_dates_after = ignore_dates_after.replace(tzinfo=None)\n            if relative_base and relative_base.tzinfo:\n                relative_base = relative_base.replace(tzinfo=None)\n\n        if prefer_language_date_order:\n            try:\n                self._lan = detect(text)\n            except Exception as e:\n                warn('DateExtractor: Catch LangDetectException ' + str(e))\n                warn(message='DateExtractor: Catch LangDetectException {}'.format(str(e)))\n\n        self._settings = {\n            EXTRACT_FIRST_DATE_ONLY: extract_first_date_only,\n            ADDITIONAL_FORMATS: additional_formats,\n            USE_DEFAULT_FORMATS: use_default_formats,\n            IGNORE_DATES_BEFORE: ignore_dates_before,\n            IGNORE_DATES_AFTER: ignore_dates_after,\n            DETECT_RELATIVE_DATES: detect_relative_dates,\n            RELATIVE_BASE: relative_base,\n            PREFERRED_DATE_ORDER: preferred_date_order,\n            PREFER_LANGUAGE_DATE_ORDER: prefer_language_date_order,\n            TIMEZONE: timezone,\n            TO_TIMEZONE: to_timezone,\n            RETURN_AS_TIMEZONE_AWARE: return_as_timezone_aware,\n            PREFER_DAY_OF_MONTH: prefer_day_of_month,\n            PREFER_DATES_FROM: prefer_dates_from,\n            DATE_VALUE_RESOLUTION: date_value_resolution\n        }\n\n        results = []\n        additional_regex = []\n        if additional_formats:\n            for date_format in additional_formats:\n                order = ''\n                reg = date_format\n                for key in singleton_regex:\n                    if key[0] == '%':\n                        reg2 = re.sub(key, singleton_regex[key], reg)\n                        if reg != reg2:\n                            if key in units['M']:\n                                order += 'M'\n                            elif key in units['Y']:\n                                order += 'Y'\n                            elif key in units['D']:\n                                order += 'D'\n                            reg = reg2\n                additional_regex.append({\n                    'reg': reg,\n                    'pattern': date_format,\n                    'order': order,\n                })\n            for r in additional_regex:\n                try:\n                    matches = [self._wrap_date_match(r['order'], match, pattern=r['pattern']) for\n                           match in re.finditer(r['reg'], text, re.I) if match]\n                    if matches:\n                        results.append(matches)\n                except:\n                    warn('DateExtractor: Failed to extract with additional format ' + str(r) + '.')\n            if use_default_formats:\n                for order in self._final_regex.keys():\n                    matches = [self._wrap_date_match(order, match) for match\n                               in re.finditer(self._final_regex[order], text, re.I) if match]\n                    if matches:\n                        results.append(matches)\n        else:\n            for order in self._final_regex.keys():\n                matches = [self._wrap_date_match(order, match) for match\n                           in re.finditer(self._final_regex[order], text, re.I) if match]\n                results.append(matches)\n\n        # for absolute dates:\n        ans = self._remove_overlapped_date_str(results)\n        # for relative dates:\n        if detect_relative_dates:\n            ans += self._extract_relative_dates(text)\n\n        return ans", "response": "Extracts dates from the given text."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwraps the final result of a date extraction with an Extraction and return the Extraction object.", "response": "def _wrap_extraction(self, date_object: datetime.datetime,\n                        original_text: str,\n                        start_char: int,\n                        end_char: int\n                        ) -> Extraction or None:\n        \"\"\"\n        wrap the final result as an Extraction and return\n\n        \"\"\"\n        try:\n            resolution = self._settings[MIN_RESOLUTION] \\\n                    if self._settings[DATE_VALUE_RESOLUTION] == DateResolution.ORIGINAL \\\n                    else self._settings[DATE_VALUE_RESOLUTION]\n            e = Extraction(self._convert_to_iso_format(date_object, resolution=resolution),\n                           start_char=start_char,\n                           end_char=end_char,\n                           extractor_name=self._name,\n                           date_object=date_object,\n                           original_date=original_text)\n            return e\n        except Exception as e:\n            warn('DateExtractor: Failed to wrap result ' + str(original_text) + ' with Extraction class.\\n'\n                                                                                'Catch ' + str(e))\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nremove overlapping date strings from the results list.", "response": "def _remove_overlapped_date_str(self, results: List[List[dict]]) -> List[Extraction]:\n        \"\"\"\n        some string may be matched by multiple date templates,\n        deduplicate the results and return a single list\n\n        \"\"\"\n        res = []\n        all_results = []\n        for x in results:\n            all_results = all_results + x\n        if not all_results or len(all_results) == 0:\n            return list()\n        all_results.sort(key=lambda k: k['start'])\n        cur_max = None\n        i = 0\n        while i < len(all_results) and not cur_max:\n            if self._post_check(all_results[i]):\n                cur_max = all_results[i]\n            i += 1\n\n        if not cur_max:\n            return res\n\n        while i < len(all_results):\n            x = all_results[i]\n            i += 1\n            if not self._post_check(x):\n                continue\n            if cur_max['end'] <= x['start']:\n                parsed_date = self._parse_date(cur_max)\n                if parsed_date:\n                    if self._settings[EXTRACT_FIRST_DATE_ONLY]:\n                        return res\n                    res.append(parsed_date)\n                cur_max = x\n            else:\n                if len(x['value']) > len(cur_max['value']):\n                    cur_max = x\n                elif len(x['value']) == len(cur_max['value']):\n                    if x['order'] in ['SINGLE_YEAR']:\n                        cur_max = x\n                    elif len(x['order']) == len(cur_max['order']):\n                        if len(x['groups']) < len(cur_max['groups']):\n                            cur_max = x\n                        elif len(x['groups']) == len(cur_max['groups']):\n                            if sum(ele is not None for ele in x['groups']) < sum(ele is not None for ele in cur_max['groups']):\n                                cur_max = x\n                            elif self._settings[PREFER_LANGUAGE_DATE_ORDER] and self._lan in language_date_order:\n                                if x['order'] == language_date_order[self._lan]:\n                                    cur_max = x\n                                elif x['order'] == self._settings[PREFERRED_DATE_ORDER]:\n                                    cur_max = x\n                            elif x['order'] == self._settings[PREFERRED_DATE_ORDER]:\n                                cur_max = x\n        parsed_date = self._parse_date(cur_max)\n        if parsed_date:\n            if self._settings[EXTRACT_FIRST_DATE_ONLY]:\n                return res\n            res.append(parsed_date)\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _parse_date(self, date_info: dict) -> Extraction or None:\n\n        user_defined_pattern = None\n\n        if date_info['pattern']:\n            user_defined_pattern = re.findall(r'%[a-zA-Z]', date_info['pattern'])\n\n        miss_day = miss_month = miss_year = miss_week = True\n\n        i = 0\n        pattern = list()\n        formatted = list()\n\n        for s in date_info['groups']:\n            if s:\n                p = self._symbol_list[date_info['order']][i] if not user_defined_pattern else user_defined_pattern[i]\n                if p in units['D']:\n                    miss_day = False\n                elif p in units['M']:\n                    miss_month = False\n                elif p in units['Y']:\n                    miss_year = False\n                elif p in units['W']:\n                    miss_week = False\n                formatted_str = s.strip('.').strip().lower()\n                if p in ['%B', '%b', '%A', '%a']:\n                    if formatted_str in foreign_to_english:\n                        # TODO: rearrange language detection in a better way\n                        if self._lan == 'en':\n                            continue\n                        formatted_str = foreign_to_english[formatted_str]\n                if p in ['%b', '%a']:\n                    formatted_str = formatted_str[:3]\n                formatted.append(re.sub(r'[^0-9+\\-]', '', formatted_str) if p == '%z' else formatted_str)\n                pattern.append(p)\n            i += 1\n\n        # TODO: deduplicate in the regex extraction part would be better\n        exist, new_formatted, new_pattern = set(), [], []\n        for i in range(len(pattern)):\n            if pattern[i] not in exist:\n                if re.match(r'[a-zA-Z]', formatted[i]) and pattern[i] == '%a':\n                    miss_week = True\n                else:\n                    new_pattern.append(pattern[i])\n                    new_formatted.append(formatted[i])\n                    exist.add(pattern[i])\n        formatted, pattern = new_formatted, new_pattern\n\n        if formatted and pattern:\n            try:\n                if self._settings[DATE_VALUE_RESOLUTION] == DateResolution.ORIGINAL:\n                    self._settings[MIN_RESOLUTION] = DateResolutionHelper.min_resolution(pattern)\n                date = datetime.datetime.strptime('-'.join(formatted), '-'.join(pattern))\n            except ValueError:\n                try:\n                    date = datetime.datetime.strptime('-'.join(formatted[:-1]), '-'.join(pattern[:-1]))\n                except ValueError:\n                    warn('DateExtractor: Failed to parse string to datetime object. \\n'\n                         'Patterns are not matched with string or the formats are not supported. ' +\n                         '-'.join(formatted) + ' with ' + '-'.join(pattern))\n                    return None\n\n            if miss_year and miss_month and miss_day:\n                today = datetime.datetime.now()\n                if miss_week:\n                    date = date.replace(day=today.day, month=today.month, year=today.year)\n                else:\n                    date = today\n                    week_of_day = formatted[0].strip().lower()\n                    if week_of_day in foreign_to_english:\n                        week_of_day = foreign_to_english[week_of_day]\n                    target = day_of_week_to_number[week_of_day] if week_of_day in day_of_week_to_number \\\n                        else today.weekday()\n                    if self._settings[PREFER_DATES_FROM] == 'past':\n                        date = date + relativedelta(days=-(date.weekday()+7-target)%7)\n                    elif self._settings[PREFER_DATES_FROM] == 'future':\n                        date = date + relativedelta(days=(target+7-date.weekday())%7)\n                    else:\n                        delta = target - date.weekday()\n                        if abs(delta) <= 3:\n                            date = date + relativedelta(days=delta)\n                        else:\n                            date = date + relativedelta(days=delta-7)\n\n            else:\n                if miss_day:\n                    last = calendar.monthrange(date.year, date.month)[1]\n                    if self._settings[PREFER_DAY_OF_MONTH] == 'current':\n                        cur = datetime.datetime.now().day\n                        date = date.replace(day=cur if cur <= last else last)\n                    elif self._settings[PREFER_DAY_OF_MONTH] == 'last':\n                        date = date.replace(day=last)\n\n                if miss_year:\n                    today = datetime.datetime.now()\n                    date = date.replace(year=today.year)\n                    next_year = date.replace(year=today.year+1)\n                    last_year = date.replace(year=today.year-1)\n                    if self._settings[PREFER_DATES_FROM] == 'past':\n                        date = last_year if date > today else date\n                    elif self._settings[PREFER_DATES_FROM] == 'future':\n                        date = next_year if date < today else date\n                    else:\n                        if date > today and (date-today > today-last_year):\n                            date = last_year\n                        elif date < today and (today-date > next_year-today):\n                            date = next_year\n\n            date = self._post_process_date(date)\n\n            if date:\n                return self._wrap_extraction(date, date_info['value'], date_info['start'], date_info['end'])\n        return None", "response": "Parse a date string extracted to a datetime. datetime object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\noverriding encoding when charset declaration or charset determination is a subset of a larger charset.", "response": "def custom_decode(encoding):\n    \"\"\"Overrides encoding when charset declaration\n       or charset determination is a subset of a larger\n       charset.  Created because of issues with Chinese websites\"\"\"\n    encoding = encoding.lower()\n    alternates = {\n        'big5': 'big5hkscs',\n        'gb2312': 'gb18030',\n        'ascii': 'utf-8',\n        'MacCyrillic': 'cp1251',\n    }\n    if encoding in alternates:\n        return alternates[encoding]\n    else:\n        return encoding"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef iso_date(d) -> str:\n        if isinstance(d, datetime):\n            return d.isoformat()\n        elif isinstance(d, date):\n            return datetime.combine(d, datetime.min.time()).isoformat()\n        else:\n            try:\n                datetime.strptime(d, '%Y-%m-%dT%H:%M:%S')\n                return d\n            except ValueError:\n                try:\n                    datetime.strptime(d, '%Y-%m-%d')\n                    return d + \"T00:00:00\"\n                except ValueError:\n                    pass\n        raise ISODateError(\"Can not convert value to ISO format for kg\")", "response": "Returns ISO format of a date"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_valid(self, field_name, value) -> (bool, object):\n\n        if self.has_field(field_name):\n            if self.fields_dict[field_name] == FieldType.KG_ID:\n                return True, value\n\n            if self.fields_dict[field_name] == FieldType.NUMBER:\n                if isinstance(value, numbers.Number):\n                    return True, value\n                else:\n                    converted_number = self.parse_number(value)\n                    return (False, value) if not converted_number else (True, value)\n            if self.fields_dict[field_name] == FieldType.STRING:\n                if isinstance(value, str):\n                    return True, value.strip()\n                else:\n                    return True, str(value).strip()\n\n            if self.fields_dict[field_name] == FieldType.DATE:\n                valid, d = self.is_date(value)\n                if valid:\n                    return True, d.isoformat()\n                else:\n                    return False, value\n\n            if self.fields_dict[field_name] == FieldType.LOCATION:\n                valid, l = self.is_location(value)\n                if valid:\n                    return True, l\n                else:\n                    return False, value\n        else:\n            print('{} not found in KG Schema'.format(field_name))\n            return False, value", "response": "Returns True if the value is valid for the KG schema otherwise False."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_date(v) -> (bool, date):\n        if isinstance(v, date):\n            return True, v\n        try:\n            reg = r'^([0-9]{4})(?:-(0[1-9]|1[0-2])(?:-(0[1-9]|[1-2][0-9]|3[0-1])(?:T' \\\n                  r'([0-5][0-9])(?::([0-5][0-9])(?::([0-5][0-9]))?)?)?)?)?$'\n            match = re.match(reg, v)\n            if match:\n                groups = match.groups()\n                patterns = ['%Y', '%m', '%d', '%H', '%M', '%S']\n                d = datetime.strptime('-'.join([x for x in groups if x]),\n                                      '-'.join([patterns[i] for i in range(len(patterns)) if groups[i]]))\n                return True, d\n        except:\n            pass\n        return False, v", "response": "Boolean function for checking if v is a date"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nselect the segments that contain the specified json_path inside the document and returns the selected elements.", "response": "def select_segments(self, jsonpath: str) -> List[Segment]:\n        \"\"\"\n        Dereferences the json_path inside the document and returns the selected elements.\n        This method should compile and cache the compiled json_path in case the same path\n        is reused by multiple extractors.\n\n        Args:\n            jsonpath (str): a valid JSON path.\n\n        Returns: A list of Segments object that contains the elements selected by the json path.\n        \"\"\"\n        path = self.etk.parse_json_path(jsonpath)\n        matches = path.find(self.cdr_document)\n\n        segments = list()\n        for a_match in matches:\n            this_segment = Segment(str(a_match.full_path), a_match.value, self)\n            segments.append(this_segment)\n\n        return segments"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nextract all the extractions from the given extractable and returns a list of Extraction objects.", "response": "def extract(self, extractor: Extractor, extractable: Extractable = None, tokenizer: Tokenizer = None,\n                joiner: str = \"  \", **options) -> List[Extraction]:\n\n        \"\"\"\n        Invoke the extractor on the given extractable, accumulating all the extractions in a list.\n\n        Args:\n            extractor (Extractor):\n            extractable (extractable):\n            tokenizer: user can pass custom tokenizer if extractor wants token\n            joiner: user can pass joiner if extractor wants text\n            options: user can pass arguments as a dict to the extract() function of different extractors\n\n        Returns: List of Extraction, containing all the extractions.\n\n        \"\"\"\n        if not extractable:\n            extractable = self\n\n        if not tokenizer:\n            tokenizer = self.etk.default_tokenizer\n\n        extracted_results = list()\n\n        if extractor.input_type == InputType.TOKENS:\n            if self.etk.error_policy == ErrorPolicy.PROCESS:\n                if isinstance(extractable.value, list):\n                    self.etk.log(\n                        \"Extractor needs tokens, tokenizer needs string to tokenize, got list, converting to string\",\n                        \"warning\", self.doc_id, self.url)\n                    warnings.warn(\n                        \"Extractor needs tokens, tokenizer needs string to tokenize, got list, converting to string\")\n                elif isinstance(extractable.value, dict):\n                    self.etk.log(\n                        \"Extractor needs tokens, tokenizer needs string to tokenize, got dict, converting to string\",\n                        \"warning\", self.doc_id, self.url)\n                    warnings.warn(\n                        \"Extractor needs tokens, tokenizer needs string to tokenize, got dict, converting to string\")\n                tokens = extractable.get_tokens(tokenizer)\n                if tokens:\n                    extracted_results = extractor.extract(tokens, **options)\n            else:\n                raise ExtractorValueError(\n                    \"Extractor needs string, tokenizer needs string to tokenize, got \" + str(type(extractable.value)))\n\n        elif extractor.input_type == InputType.TEXT:\n            if self.etk.error_policy == ErrorPolicy.PROCESS:\n                if isinstance(extractable.value, list):\n                    self.etk.log(\"Extractor needs string, got extractable value as list, converting to string\",\n                                 \"warning\", self.doc_id, self.url)\n                    warnings.warn(\"Extractor needs string, got extractable value as list, converting to string\")\n                elif isinstance(extractable.value, dict):\n                    self.etk.log(\"Extractor needs string, got extractable value as dict, converting to string\",\n                                 \"warning\", self.doc_id, self.url)\n                    warnings.warn(\"Extractor needs string, got extractable value as dict, converting to string\")\n                text = extractable.get_string(joiner)\n                if text:\n                    extracted_results = extractor.extract(text, **options)\n            else:\n                # raise ExtractorValueError(\"Extractor needs string, got \" + str(type(extractable.value)))\n                # TODO: Yixiang - needs to be handled properly\n                pass\n\n        elif extractor.input_type == InputType.OBJECT:\n            extracted_results = extractor.extract(extractable.value, **options)\n\n        elif extractor.input_type == InputType.HTML:\n            if bool(BeautifulSoup(extractable.value, \"html.parser\").find()):\n                extracted_results = extractor.extract(extractable.value, **options)\n            else:\n                # raise ExtractorValueError(\"Extractor needs HTML, got non HTML string\")\n                # TODO: Yixiang - needs to be handled properly\n                pass\n\n        try:\n            jsonPath = extractable.full_path\n        except AttributeError:\n            jsonPath = None\n\n        for e in extracted_results:\n            # for the purpose of provenance hierarrchy tracking, a parent's id for next generation.\n            e.prov_id = self.provenance_id_index\n            extraction_provenance_record: ExtractionProvenanceRecord = ExtractionProvenanceRecord(\n                e.prov_id, jsonPath, e.provenance[\"extractor_name\"],\n                e.provenance[\"start_char\"], e.provenance[\"end_char\"], e.provenance[\"confidence\"], self,\n                extractable.prov_id)\n            self._provenances[e.prov_id] = extraction_provenance_record\n\n            # for the purpose of provenance hierarchy tracking\n            self.provenance_id_index_incrementer()\n            self.create_provenance(extraction_provenance_record)\n\n        return extracted_results"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef build_knowledge_graph(self, json_ontology: dict) -> List:\n        nested_docs = list()\n        if json_ontology:\n            for key in list(json_ontology):\n                j_values = json_ontology[key]\n                if not isinstance(j_values, list):\n                    j_values = [j_values]\n                for j_value in j_values:\n                    if not isinstance(j_value, dict):\n                        if self.kg:\n                            if key not in ['doc_id', 'uri']:\n                                self.kg.add_value(key, value=j_value)\n                    else:\n                        \"\"\"Now we have to create a nested document, assign it a doc_id and \n                           add the doc_id to parent document's knowledge graph\"\"\"\n                        child_doc_id = None\n                        if 'uri' in j_value:\n                            child_doc_id = j_value['uri']\n                        elif 'doc_id' in j_value:\n                            child_doc_id = j_value['doc_id']\n\n                        child_doc = Document(self.etk, cdr_document=dict(), mime_type='json', url='')\n                        nested_docs.extend(child_doc.build_knowledge_graph(j_value))\n\n                        if not child_doc_id:\n                            child_doc_id = Utility.create_doc_id_from_json(child_doc.kg._kg)\n\n                        if self.kg:\n                            self.kg.add_value(key, value=child_doc_id)\n                        child_doc.cdr_document[\"doc_id\"] = child_doc_id\n\n                        nested_docs.append(child_doc)\n\n        return nested_docs", "response": "Build a knowledge graph from a json - like ontology representation."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nextract the content of the given html text into a list of extraction objects.", "response": "def extract(self, html_text: str,\n                extract_title: bool = False,\n                extract_meta: bool = False,\n                extract_microdata: bool = False,\n                microdata_base_url: str = \"\",\n                extract_json_ld: bool = False,\n                extract_rdfa: bool = False,\n                rdfa_base_url: str = \"\") \\\n            -> List[Extraction]:\n        \"\"\"\n        Args:\n            html_text (str): input html string to be extracted\n            extract_title (bool): True if string of 'title' tag needs to be extracted, return as { \"title\": \"...\" }\n            extract_meta (bool): True if string of 'meta' tags needs to be extracted, return as { \"meta\": { \"author\": \"...\", ...}}\n            extract_microdata (bool): True if microdata needs to be extracted, returns as { \"microdata\": [...] }\n            microdata_base_url (str): base namespace url for microdata, empty string if no base url is specified\n            extract_json_ld (bool): True if json-ld needs to be extracted, return as { \"json-ld\": [...] }\n            extract_rdfa (bool): True if rdfs needs to be extracted, returns as { \"rdfa\": [...] }\n            rdfa_base_url (str): base namespace url for rdfa, empty string if no base url is specified\n\n        Returns:\n            List[Extraction]: the list of extraction or the empty list if there are no matches.\n        \"\"\"\n        res = list()\n        soup = BeautifulSoup(html_text, 'html.parser')\n\n        if soup.title and extract_title:\n            title = self._wrap_data(\"title\", soup.title.string.encode('utf-8').decode('utf-8'))\n            res.append(title)\n\n        if soup.title and extract_meta:\n            meta_content = self._wrap_meta_content(soup.find_all(\"meta\"))\n            meta_data = self._wrap_data(\"meta\", meta_content)\n            res.append(meta_data)\n\n        if extract_microdata:\n            mde = MicrodataExtractor()\n            mde_data = self._wrap_data(\"microdata\", mde.extract(html_text, microdata_base_url))\n            res.append(mde_data)\n\n        if extract_json_ld:\n            jslde = JsonLdExtractor()\n            jslde_data = self._wrap_data(\"json-ld\", jslde.extract(html_text))\n            res.append(jslde_data)\n\n        if extract_rdfa:\n            rdfae = RDFaExtractor()\n            rdfae_data = self._wrap_data(\"rdfa\", rdfae.extract(html_text, rdfa_base_url))\n            res.append(rdfae_data)\n\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef attribute_value(self, doc: Document, attribute_name: str):\n        return doc.cdr_document.get(self.header_translation_table[attribute_name])", "response": "Returns the value for the given attribute name"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd the actors to the list of actors that are part of the event.", "response": "def add_actors(self, doc:Document, event: str, cameo_code: int) -> List[Document]:\n        \"\"\"\n        Each event has two actors. The relationship of the event to the actors depends\n        on the cameo code and is defined by the mapping.\n        Args:\n            doc: the document containing the evence\n            event: one of \"event1\", \"event2\", or \"event3\"\n            cameo_code:\n\n        Returns: the documents created for each actor\n\n        \"\"\"\n        # Actor1\n        actor1_cdr = {\n            \"ActorName\": doc.cdr_document[self.attribute(\"Actor1Name\")],\n            \"ActorCountryCode\": doc.cdr_document[self.attribute(\"Actor1CountryCode\")],\n            \"ActorKnownGroupCode\": doc.cdr_document[self.attribute(\"Actor1KnownGroupCode\")],\n            \"ActorEthnicCode\": doc.cdr_document[self.attribute(\"Actor1EthnicCode\")],\n            \"ActorReligion1Code\": doc.cdr_document[self.attribute(\"Actor1Religion1Code\")],\n            \"ActorReligion2Code\": doc.cdr_document[self.attribute(\"Actor1Religion2Code\")],\n            \"ActorType1Code\": doc.cdr_document[self.attribute(\"Actor1Type1Code\")],\n            \"ActorType2Code\": doc.cdr_document[self.attribute(\"Actor1Type2Code\")],\n            \"ActorType3Code\": doc.cdr_document[self.attribute(\"Actor1Type3Code\")],\n            \"ActorGeo_Type\": doc.cdr_document[self.attribute(\"Actor1Geo_Type\")],\n            \"ActorGeo_FullName\": doc.cdr_document[self.attribute(\"Actor1Geo_FullName\")],\n            \"ActorGeo_CountryCode\": doc.cdr_document[self.attribute(\"Actor1Geo_CountryCode\")],\n            \"ActorGeo_ADM1Code\": doc.cdr_document[self.attribute(\"Actor1Geo_ADM1Code\")],\n            \"ActorGeo_Lat\": doc.cdr_document[self.attribute(\"Actor1Geo_Lat\")],\n            \"ActorGeo_Long\": doc.cdr_document[self.attribute(\"Actor1Geo_Long\")],\n            \"ActorGeo_FeatureID\": doc.cdr_document[self.attribute(\"Actor1Geo_FeatureID\")],\n            \"dataset\": \"gdelt-actor\"\n        }\n        actor1 = etk.create_document(actor1_cdr)\n        actor1.doc_id = doc.doc_id + \"-actor1\"\n\n        # Link actor1 to the event\n        actor_field = \"participant\"\n        actor_prop = self.mapping.actor_property(event, \"actor1\", cameo_code)\n        if actor_prop and self.actor_role.get(actor_prop):\n            actor_field = self.actor_role.get(actor_prop)\n        doc.kg.add_value(actor_field, actor1.doc_id)\n\n        # Actor2\n        actor2_cdr = {\n            \"ActorName\": doc.cdr_document[self.attribute(\"Actor2Name\")],\n            \"ActorCountryCode\": doc.cdr_document[self.attribute(\"Actor2CountryCode\")],\n            \"ActorKnownGroupCode\": doc.cdr_document[self.attribute(\"Actor2KnownGroupCode\")],\n            \"ActorEthnicCode\": doc.cdr_document[self.attribute(\"Actor2EthnicCode\")],\n            \"ActorReligion1Code\": doc.cdr_document[self.attribute(\"Actor2Religion1Code\")],\n            \"ActorReligion2Code\": doc.cdr_document[self.attribute(\"Actor2Religion2Code\")],\n            \"ActorType1Code\": doc.cdr_document[self.attribute(\"Actor2Type1Code\")],\n            \"ActorType2Code\": doc.cdr_document[self.attribute(\"Actor2Type2Code\")],\n            \"ActorType3Code\": doc.cdr_document[self.attribute(\"Actor2Type3Code\")],\n            \"ActorGeo_Type\": doc.cdr_document[self.attribute(\"Actor2Geo_Type\")],\n            \"ActorGeo_FullName\": doc.cdr_document[self.attribute(\"Actor2Geo_FullName\")],\n            \"ActorGeo_CountryCode\": doc.cdr_document[self.attribute(\"Actor2Geo_CountryCode\")],\n            \"ActorGeo_ADM1Code\": doc.cdr_document[self.attribute(\"Actor2Geo_ADM1Code\")],\n            \"ActorGeo_Lat\": doc.cdr_document[self.attribute(\"Actor2Geo_Lat\")],\n            \"ActorGeo_Long\": doc.cdr_document[self.attribute(\"Actor2Geo_Long\")],\n            \"ActorGeo_FeatureID\": doc.cdr_document[self.attribute(\"Actor2Geo_FeatureID\")],\n            \"dataset\": \"gdelt-actor\"\n        }\n        actor2 = etk.create_document(actor2_cdr)\n        actor2.doc_id = doc.doc_id + \"-actor2\"\n\n        # Link actor2 to the event\n        actor_field = \"participant\"\n        actor_prop = self.mapping.actor_property(event, \"actor2\", cameo_code)\n        if actor_prop and self.actor_role.get(actor_prop):\n            actor_field = self.actor_role.get(actor_prop)\n        doc.kg.add_value(actor_field, actor2.doc_id)\n\n        return [actor1, actor2]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef all_cities():\n  cities = []\n  fname = pkg_resources.resource_filename(__name__, 'resources/CityPops.csv')\n  with open(fname, 'rU') as csvfile:\n    reader = csv.reader(csvfile, delimiter = ',')\n    for row in reader:\n      cities.append(row[0])\n  cities.sort()\n  return cities", "response": "Get a list of all Backpage city names."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of all days from 11 - 12 - 2015 to the present.", "response": "def all_days(boo):\n  \"\"\"\n  Return a list of all dates from 11/12/2015 to the present.\n\n  Args:\n    boo: if true, list contains Numbers (20151230); if false, list contains Strings (\"2015-12-30\")\n  Returns:\n    list of either Numbers or Strings\n  \"\"\"\n  earliest = datetime.strptime(('2015-11-12').replace('-', ' '), '%Y %m %d')\n  latest = datetime.strptime(datetime.today().date().isoformat().replace('-', ' '), '%Y %m %d')\n  num_days = (latest - earliest).days + 1\n  all_days = [latest - timedelta(days=x) for x in range(num_days)]\n  all_days.reverse()\n\n  output = []\n\n  if boo:\n    # Return as Integer, yyyymmdd\n    for d in all_days:\n      output.append(int(str(d).replace('-', '')[:8]))\n  else:\n    # Return as String, yyyy-mm-dd\n    for d in all_days:\n      output.append(str(d)[:10])\n  return output"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef city_nums():\n  city_nums = {}\n  first_row = 1\n  num = 0\n  fname = pkg_resources.resource_filename(__name__, 'resources/Distance_Matrix.csv')\n  with open(fname, 'rU') as csvfile:\n    reader = csv.reader(csvfile, delimiter = ',')\n    for row in reader:\n      if first_row == 1:\n        first_row = 0\n      else:\n        city_nums[row[0]] = num\n        num = num + 1\n\n  return city_nums", "response": "Get a dictionary of Backpage city names mapped to their numeric value."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef date_clean(date, dashboard_style=False):\n  if dashboard_style:\n    dt = str(date)\n    out = dt[4:6] + '/' + dt[6:] + '/' + dt[:4]\n  else:\n    dt = str(date)\n    out = dt[:4] + '-' + dt[4:6] + '-' + dt[6:]\n  return out", "response": "Clean the numerical date value in order to present it."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of dates within a specified range inclusive.", "response": "def date_range(start, end, boo):\n  \"\"\"\n  Return list of dates within a specified range, inclusive.\n\n  Args:\n    start: earliest date to include, String (\"2015-11-25\")\n    end: latest date to include, String (\"2015-12-01\")\n    boo: if true, output list contains Numbers (20151230); if false, list contains Strings (\"2015-12-30\")\n  Returns:\n    list of either Numbers or Strings\n  \"\"\"\n  earliest = datetime.strptime(start.replace('-', ' '), '%Y %m %d')\n  latest = datetime.strptime(end.replace('-', ' '), '%Y %m %d')\n  num_days = (latest - earliest).days + 1\n  all_days = [latest - timedelta(days=x) for x in range(num_days)]\n  all_days.reverse()\n\n  output = []\n\n  if boo:\n    # Return as Integer, yyyymmdd\n    for d in all_days:\n      output.append(int(str(d).replace('-', '')[:8]))\n  else:\n    # Return as String, yyyy-mm-dd\n    for d in all_days:\n      output.append(str(d)[:10])\n  return output"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dicts_equal(d1, d2):\n\n  # check for different sizes\n  if len(d1) != len(d2):\n    return False\n  # check for different keys\n  for k in d1:\n    if k not in d2:\n      return False\n  for k in d2:\n    if k not in d1:\n      return False\n\n  # compare each element in dict\n  for k in d1:\n    if type(d1[k]) != type(d2[k]):\n      # different value types\n      return False\n\n    # lists\n    elif isinstance(d1[k], list):\n      if not (sorted(d1[k]) == sorted(d2[k])):\n        return False\n\n    # nested dicts\n    elif isinstance(d1[k], dict):\n      if not dicts_equal(d1[k], d2[k]):\n        return False\n\n    # primitives\n    else:\n      if d1[k] != d2[k]:\n        return False\n\n  return True", "response": "Perform a deep comparison of two dictionaries returning True if the dictionaries are equal False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a dictionary of distances between all Backpage cities in miles.", "response": "def distances():\n  \"\"\"\n  Get all distances between all cities in miles (matrix-style). \n\n  Returns:\n    dictionary of Backpage city names mapped to a list of distances, one for every other city\n  \"\"\"\n  distances = {}\n  # Instantiate a matrix of distances (in miles) between all cities\n  num = 0\n  top_row = 1\n  fname = pkg_resources.resource_filename(__name__, 'resources/Distance_Matrix.csv')\n  with open(fname, 'rU') as csvfile:\n    reader = csv.reader(csvfile, delimiter = ',')\n    for row in reader:\n      if top_row == 1:\n        top_row = 0\n      else:\n        # Map each city to a list of distances to all other cities.\n        vals = []\n        for item in row[1:]:\n          if not item:\n            continue\n          try:\n            vals.append(int(float(item)))\n          except ValueError:\n            print 'Invalid data type for row {} with value {}, column value: {}'.format(num, row[0], item)\n        distances[num] = vals\n        num += 1\n\n  return distances"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ethnicities_clean():\n  eths_clean = {}\n\n  fname = pkg_resources.resource_filename(__name__, 'resources/Ethnicity_Groups.csv')\n  with open(fname, 'rU') as csvfile:\n    reader = csv.reader(csvfile, delimiter = ',')\n    first = []\n    for row in reader:\n      if first:\n        for i in range(len(first)):\n          if first[i] and row[i]:\n            eths_clean[first[i]] = row[i]\n        first = []\n      else:\n        first = deepcopy(row)\n  return eths_clean", "response": "Get dictionary of unformatted ethnicity types mapped to clean corresponding ethnicity strings"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a dictionary that maps all Backpage city names to their presentable formal names.", "response": "def formal_cities(reverse=False):\n  \"\"\"\n  Get a dictionary that maps all Backpage city names to their presentable, formal names.\n\n  Returns:\n    dictionary of Backpage city names mapped to formal city names\n  \"\"\"\n  output = {}\n  fname = pkg_resources.resource_filename(__name__, 'resources/Formal_City_Name_Pairs.csv')\n  with open(fname, 'rU') as csvfile:\n    reader = csv.reader(csvfile, delimiter = ',')\n    for row in reader:\n      if not reverse:\n        # Informal to formal\n        output[row[0]] = row[1]\n      else:\n        # Formal to informal\n        output[row[1]] = row[0]\n  return output"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_lats():\n  lats = {}\n  fname = pkg_resources.resource_filename(__name__, 'resources/Latitudes-Longitudes.csv')\n  with open(fname, 'rb') as csvfile:\n    # Read latitude/longitude coordinates\n    reader = csv.reader(csvfile, delimiter = ',')\n    for row in reader:\n      word = row[0].lower()\n      word = re.sub(' ', '', word)\n      lats[word] = float(row[1])\n\n  return lats", "response": "Returns a dictionary that maps Backpage city names to their respective latitudes."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a dictionary that maps Backpage city names to their respective longitudes.", "response": "def get_longs():\n  \"\"\"\n  Get a dictionary that maps Backpage city names to their respective longitudes.\n\n  Returns:\n    dictionary that maps city names (Strings) to longitudes (Floats)\n  \"\"\"\n  longs = {}\n  fname = pkg_resources.resource_filename(__name__, 'resources/Latitudes-Longitudes.csv')\n  with open(fname, 'rb') as csvfile:\n    # Read latitude/longitude coordinates\n    reader = csv.reader(csvfile, delimiter = ',')\n    for row in reader:\n      word = row[0].lower()\n      word = re.sub(' ', '', word)\n      longs[word] = float(row[2])\n  return longs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_regions():\n  new_england = ['Connecticut', 'Maine', 'Massachusetts', 'New Hampshire', 'Rhode Island', 'Vermont']\n  mid_atlantic = ['New Jersey', 'New York', 'Pennsylvania', 'Delaware', 'Maryland', 'District of Columbia']\n  midwest_east = ['Illinois', 'Indiana', 'Michigan', 'Ohio', 'Wisconsin']\n  midwest_west = ['Iowa', 'Kansas', 'Minnesota', 'Missouri', 'Nebraska', 'North Dakota', 'South Dakota']\n  southeast = ['Florida', 'Georgia', 'South Carolina', 'Virginia', 'Alabama', 'Kentucky', 'Mississippi', 'Tennessee', 'Arkansas', 'Louisiana', 'West Virginia', 'North Carolina']\n  southwest = ['Texas', 'Oklahoma', 'New Mexico', 'Arizona']\n  mtn_west = ['Montana', 'Idaho', 'Wyoming', 'Colorado', 'Nevada', 'Utah']\n  pacific = ['Washington', 'Oregon', 'California']\n  alaska = ['Alaska']\n  hawaii = ['Hawaii']\n\n  regions = []\n  regions.append(new_england)\n  regions.append(mid_atlantic)\n  regions.append(midwest_east)\n  regions.append(midwest_west)\n  regions.append(southeast)\n  regions.append(southwest)\n  regions.append(mtn_west)\n  regions.append(pacific)\n  regions.append(alaska)\n  regions.append(hawaii)\n\n  # Map each state to its region number\n  output = {}\n  for i in range(len(regions)):\n    states = regions[i]\n    for j in range(len(states)):\n      output[states[j]] = i\n  return output", "response": "Returns a list of state names mapped to their respective region numbers."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets a dictionary of Backpage city names mapped to their citizen populations.", "response": "def populations():\n  \"\"\"\n  Get a dictionary of Backpage city names mapped to their citizen populations.\n\n  Returns:\n    dictionary of Backpage city names mapped to their populations (integers)\n  \"\"\"\n  city_pops = {}\n  fname = pkg_resources.resource_filename(__name__, 'resources/CityPops.csv')\n  with open(fname, 'rU') as csvfile:\n    reader = csv.reader(csvfile, delimiter = ',')\n    for row in reader:\n      city_pops[row[0]] = int(row[1])\n  return city_pops"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a dictionary of Backpage city names mapped to their posting ID group", "response": "def post_id_backpage_groups():\n  \"\"\"\n  Get a dictionary of Backpage city names mapped to their posting ID group (ex: groups['buffalo']: 'upstateny')\n\n  Returns:\n    dictionary of Backpage city names mapped to their posting ID group\n  \"\"\"\n  city_bp_groups = {}\n  with open('dataFiles/city_bp_groups.csv', 'rU') as csvfile:\n    reader = csv.reader(csvfile, delimiter = ',')\n    for row in reader:\n      city_bp_groups[row[0]] = row[1]\n\n  return city_bp_groups"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef state_names():\n  \n  names = set()\n  fname = pkg_resources.resource_filename(__name__, 'resources/States.csv')\n  with open(fname, 'rU') as csvfile:\n    reader = csv.reader(csvfile, delimiter = ',')\n    for row in reader:\n      names.add(row[0])\n  return names", "response": "Get the set of all US state names"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget a dictionary of state names mapped to their numeric value.", "response": "def state_nums():\n  \"\"\"\n  Get a dictionary of state names mapped to their 'legend' value.\n\n  Returns:\n    dictionary of state names mapped to their numeric value\n  \"\"\"\n  st_nums = {}\n  fname = pkg_resources.resource_filename(__name__, 'resources/States.csv')\n  with open(fname, 'rU') as csvfile:\n    reader = csv.reader(csvfile, delimiter = ',')\n    i = 0\n    for row in reader:\n      st_nums[row[0]] = i\n      i = i + 1\n  return st_nums"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef states():\n  states = {}\n  fname = pkg_resources.resource_filename(__name__, 'resources/City_State_Pairs.csv')\n  with open(fname, 'rU') as csvfile:\n    reader = csv.reader(csvfile, delimiter = ',')\n    for row in reader:\n      states[row[0]] = row[1]\n\n  return states", "response": "Get a dictionary of Backpage city names mapped to their respective states."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef today(boo):\n  tod = datetime.strptime(datetime.today().date().isoformat().replace('-', ' '), '%Y %m %d')\n  if boo:\n    return int(str(tod).replace('-', '')[:8])\n  else:\n    return str(tod)[:10]", "response": "Return the today s date as either a Number or a string depending upon the user s input\n"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nextract information from a string with the GlossaryExtractor instance.", "response": "def extract(self, tokens: List[Token]) -> List[Extraction]:\n        \"\"\"\n        Extracts information from a string(TEXT) with the GlossaryExtractor instance\n\n        Args:\n            token (List[Token]): list of spaCy token to be processed.\n\n        Returns:\n            List[Extraction]: the list of extraction or the empty list if there are no matches.\n\n        \"\"\"\n        results = list()\n\n        if len(tokens) > 0:\n            if self._case_sensitive:\n                new_tokens = [x.orth_ if isinstance(x, Token) else x for x in tokens]\n            else:\n                new_tokens = [x.lower_ if isinstance(x, Token) else x.lower() for x in tokens]\n        else:\n            return results\n\n        try:\n            ngrams_iter = self._generate_ngrams_with_context(new_tokens)\n            results.extend(map(lambda term: self._wrap_value_with_context(tokens, term[1], term[2]),\n                               filter(lambda term: isinstance(term[0], str),\n                                      map(lambda term: (self._glossary.get(term[0]), term[1], term[2]),\n                                          map(lambda term: (\n                                              self._combine_ngrams(term[0], self._joiner), term[1], term[2]),\n                                              ngrams_iter)))))\n        except Exception as e:\n            raise ExtractorError('GlossaryExtractor: Failed to extract with ' + self.name + '. Catch ' + str(e) + '. ')\n        return results"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating the 1 - gram to n - gram tuples of the list of tokens", "response": "def _generate_ngrams_with_context(self, tokens: List[Token]) -> chain:\n        \"\"\"Generates the 1-gram to n-grams tuples of the list of tokens\"\"\"\n        chained_ngrams_iter = self._generate_ngrams_with_context_helper(iter(tokens), 1)\n        for n in range(2, self._ngrams + 1):\n            ngrams_iter = tee(tokens, n)\n            for j in range(1, n):\n                for k in range(j):\n                    next(ngrams_iter[j], None)\n            ngrams_iter_with_context = self._generate_ngrams_with_context_helper(zip(*ngrams_iter), n)\n            chained_ngrams_iter = chain(chained_ngrams_iter, ngrams_iter_with_context)\n        return chained_ngrams_iter"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _populate_trie(self, values: List[str]) -> CharTrie:\n        if self._default_tokenizer:\n            return reduce(self._populate_trie_reducer, iter(values), CharTrie())\n        return reduce(self._populate_trie_reducer_regex, iter(values), CharTrie())", "response": "Takes a list and inserts its elements into a new trie and returns it"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _populate_trie_reducer(self, trie_accumulator=CharTrie(), value=\"\") -> CharTrie:\n        if self._case_sensitive:\n            key = self._joiner.join([x.orth_ if isinstance(x, Token) else x for x in\n                                     self._default_tokenizer.tokenize(value, disable=disable_spacy)])\n        else:\n            key = self._joiner.join([x.lower_ if isinstance(x, Token) else x.lower() for x in\n                                     self._default_tokenizer.tokenize(value, disable=disable_spacy)])\n        trie_accumulator[key] = value\n        return trie_accumulator", "response": "Populates trie accumulator with the value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _populate_trie_reducer_regex(self, trie_accumulator=CharTrie(), value=\"\") -> CharTrie:\n        regex = re.compile(r\"[A-Za-z0-9]+|[^\\w\\s]|_\")\n        if self._case_sensitive:\n            key = self._joiner.join([x for x in re.findall(regex, value)])\n        else:\n            key = self._joiner.join([x.lower() for x in re.findall(regex, value)])\n        trie_accumulator[key] = value\n        return trie_accumulator", "response": "Populate trie accumulator with the value of the given value."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _wrap_value_with_context(self, tokens: List[Token], start: int, end: int) -> Extraction:\n        return Extraction(' '.join([x.orth_ if isinstance(x, Token) else x for x in tokens[start:end]]),\n                          self.name,\n                          start_token=start,\n                          end_token=end,\n                          start_char=tokens[start].idx if isinstance(tokens[start], Token) else -1,\n                          end_char=tokens[end - 1].idx + len(tokens[end - 1].orth_) if isinstance(tokens[end - 1],\n                                                                                                  Token) else -1\n                          )", "response": "Wraps the final result of the value with the context of the token."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates n - grams with context helper.", "response": "def _generate_ngrams_with_context_helper(ngrams_iter: iter, ngrams_len: int) -> map:\n        \"\"\"Updates the end index\"\"\"\n        return map(lambda term: (term[1], term[0], term[0] + ngrams_len), enumerate(ngrams_iter))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _combine_ngrams(ngrams, joiner) -> str:\n        if isinstance(ngrams, str):\n            return ngrams\n        else:\n            combined = joiner.join(ngrams)\n            return combined", "response": "Combine ngrams into a single string."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef extract(self, text: str) -> List[Extraction]:\n\n        doc = self._tokenizer.tokenize_to_spacy_doc(text)\n        self._load_matcher()\n\n        matches = [x for x in self._matcher(doc) if x[1] != x[2]]\n        pos_filtered_matches = []\n        neg_filtered_matches = []\n        for idx, start, end in matches:\n            span_doc = self._tokenizer.tokenize_to_spacy_doc(doc[start:end].text)\n            this_spacy_rule = self._matcher.get(idx)\n            relations = self._find_relation(span_doc, this_spacy_rule)\n            rule_id, _ = self._hash_map[idx]\n            this_rule = self._rule_lst[rule_id]\n            if self._filter_match(doc[start:end], relations, this_rule.patterns):\n                value = self._form_output(doc[start:end], this_rule.output_format, relations, this_rule.patterns)\n                if this_rule.polarity:\n                    pos_filtered_matches.append((start, end, value, rule_id, relations))\n                else:\n                    neg_filtered_matches.append((start, end, value, rule_id, relations))\n\n        return_lst = []\n        if pos_filtered_matches:\n            longest_lst_pos = self._get_longest(pos_filtered_matches)\n            if neg_filtered_matches:\n                longest_lst_neg = self._get_longest(neg_filtered_matches)\n                return_lst = self._reject_neg(longest_lst_pos, longest_lst_neg)\n            else:\n                return_lst = longest_lst_pos\n\n        extractions = []\n        for (start, end, value, rule_id, relation) in return_lst:\n            this_extraction = Extraction(value=value,\n                                         extractor_name=self.name,\n                                         start_token=start,\n                                         end_token=end,\n                                         start_char=doc[start].idx,\n                                         end_char=doc[end-1].idx+len(doc[end-1]),\n                                         rule_id=rule_id.split(\"rule_id##\")[0],\n                                         match_mapping=relation)\n            extractions.append(this_extraction)\n\n        return extractions", "response": "Extracts the entry - point from the input str."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _load_matcher(self) -> None:\n        for id_key in self._rule_lst:\n            if self._rule_lst[id_key].active:\n                pattern_lst = [a_pattern.spacy_token_lst for a_pattern in self._rule_lst[id_key].patterns]\n\n                for spacy_rule_id, spacy_rule in enumerate(itertools.product(*pattern_lst)):\n                    self._matcher.add(self._construct_key(id_key, spacy_rule_id), None, list(spacy_rule))", "response": "Load the matcher from the rule list."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfiltering the match result according to prefix suffix min max...", "response": "def _filter_match(self, span: span, relations: Dict, patterns: List) -> bool:\n        \"\"\"\n        Filter the match result according to prefix, suffix, min, max ...\n        Args:\n            span: span\n            relations: Dict\n            patterns: List of pattern\n\n        Returns: bool\n        \"\"\"\n\n        for pattern_id, a_pattern in enumerate(patterns):\n            token_range = relations[pattern_id]\n            if token_range:\n                tokens = [x for x in span[token_range[0]:token_range[1]]]\n                if a_pattern.type == \"word\":\n                    if not self._pre_suf_fix_filter(tokens, a_pattern.prefix, a_pattern.suffix):\n                        return False\n                if a_pattern.type == \"shape\":\n                    if not (self._full_shape_filter(tokens, a_pattern.full_shape)\n                            and self._pre_suf_fix_filter(tokens, a_pattern.prefix,a_pattern.suffix)):\n                        return False\n                if a_pattern.type == \"number\":\n                    if not self._min_max_filter(tokens, a_pattern.min, a_pattern.max):\n                        return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the longest match for overlap", "response": "def _get_longest(value_lst: List) -> List:\n        \"\"\"\n        Get the longest match for overlap\n        Args:\n            value_lst: List\n\n        Returns: List\n        \"\"\"\n\n        value_lst.sort()\n        result = []\n        pivot = value_lst[0]\n        start, end = pivot[0], pivot[1]\n        pivot_e = end\n        pivot_s = start\n        for idx, (s, e, v, rule_id, _) in enumerate(value_lst):\n            if s == pivot_s and pivot_e < e:\n                pivot_e = e\n                pivot = value_lst[idx]\n            elif s != pivot_s and pivot_e < e:\n                result.append(pivot)\n                pivot = value_lst[idx]\n                pivot_e = e\n                pivot_s = s\n        result.append(pivot)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _reject_neg(pos_lst: List, neg_lst: List) -> List:\n\n        pos_lst.sort()\n        neg_lst.sort()\n        result = []\n        pivot_pos = pos_lst[0]\n        pivot_neg = neg_lst[0]\n        while pos_lst:\n            if pivot_pos[1] <= pivot_neg[0]:\n                result.append(pivot_pos)\n                pos_lst.pop(0)\n                if pos_lst:\n                    pivot_pos = pos_lst[0]\n            elif pivot_pos[0] >= pivot_neg[1]:\n                neg_lst.pop(0)\n                if not neg_lst:\n                    result += pos_lst\n                    break\n                else:\n                    pivot_neg = neg_lst[0]\n            else:\n                pos_lst.pop(0)\n                if pos_lst:\n                    pivot_pos = pos_lst[0]\n        return result", "response": "Reject some positive matches according to negative matches\n            Returns a list of pos_lst and neg_lst."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprefix and Suffix filter Args: t: List, list of tokens prefix: str suffix: str Returns: bool", "response": "def _pre_suf_fix_filter(t: List, prefix: str, suffix: str) -> bool:\n        \"\"\"\n        Prefix and Suffix filter\n        Args:\n            t: List, list of tokens\n            prefix: str\n            suffix: str\n\n        Returns: bool\n        \"\"\"\n\n        if prefix:\n            for a_token in t:\n                if a_token._.n_prefix(len(prefix)) != prefix:\n                    return False\n        if suffix:\n            for a_token in t:\n                if a_token._.n_suffix(len(suffix)) != suffix:\n                    return False\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn True if the list of tokens in t are between min_v and max_v.", "response": "def _min_max_filter(t: List, min_v: str, max_v: str) -> bool:\n        \"\"\"\n        Min and Max filter\n        Args:\n            t: List, list of tokens\n            min_v: str\n            max_v: str\n\n        Returns: bool\n        \"\"\"\n\n        def tofloat(value):\n            try:\n                float(value)\n                return float(value)\n            except ValueError:\n                return False\n\n        for a_token in t:\n            if not tofloat(a_token.text):\n                return False\n            else:\n                if min_v and tofloat(min_v):\n                    this_v = tofloat(a_token.text)\n                    if this_v < tofloat(min_v):\n                        return False\n                if max_v and tofloat(max_v):\n                    this_v = tofloat(a_token.text)\n                    if this_v > tofloat(max_v):\n                        return False\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _full_shape_filter(t: List, shapes: List) -> bool:\n\n        if shapes:\n            for a_token in t:\n                if a_token._.full_shape not in shapes:\n                    return False\n\n        return True", "response": "Filter out all the full_shape tokens in the list of shapes."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nform an output value according to user input of output_format", "response": "def _form_output(span_doc: span, output_format: str, relations: Dict, patterns: List) -> str:\n        \"\"\"\n        Form an output value according to user input of output_format\n        Args:\n            span_doc: span\n            format: str\n            relations: Dict\n            patterns: List\n\n        Returns: str\n        \"\"\"\n\n        format_value = []\n        output_inf = [a_pattern.in_output for a_pattern in patterns]\n        for i in range(len(output_inf)):\n            token_range = relations[i]\n            if token_range and output_inf[i]:\n                format_value.append(span_doc[token_range[0]:token_range[1]].text)\n\n        if not output_format:\n            return \" \".join(format_value)\n\n        result_str = re.sub(\"{}\", \" \".join(format_value), output_format)\n\n        positions = re.findall(\"{[0-9]+}\", result_str)\n\n        if not positions:\n            return result_str\n\n        position_indices = [int(x[1:-1]) for x in positions]\n        if max(position_indices) < len(format_value):\n            result_str = result_str.format(*format_value)\n        else:\n            try:\n                result_str = result_str.format(\"\", *format_value)\n            except:\n                positions = [x for x in positions if int(x[1:-1]) > len(format_value)-1 or int(x[1:-1]) < 0]\n                for pos in positions:\n                    result_str = result_str.replace(pos, \"\")\n                result_str = result_str.format(*format_value)\n\n        return result_str"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _construct_key(self, rule_id: str, spacy_rule_id:int) -> int:\n\n        hash_key = (rule_id, spacy_rule_id)\n        hash_v = hash(hash_key) + sys.maxsize + 1\n        self._hash_map[hash_v] = hash_key\n        return hash_v", "response": "Construct a key for the cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinds the relations between the each pattern in the spacy rule and the matches ArcGIS doc.", "response": "def _find_relation(self, span_doc: doc, r: List) -> Dict:\n        \"\"\"\n        Get the relations between the each pattern in the spacy rule and the matches\n        Args:\n            span_doc: doc\n            r: List\n\n        Returns: Dict\n        \"\"\"\n\n        rule = r[1][0]\n        span_pivot = 0\n        relation = {}\n        for e_id, element in enumerate(rule):\n            if not span_doc[span_pivot:]:\n                for extra_id, _, in enumerate(rule[e_id:]):\n                    relation[e_id+extra_id] = None\n                break\n            new_doc = self._tokenizer.tokenize_to_spacy_doc(span_doc[span_pivot:].text)\n            if \"OP\" not in element:\n                relation[e_id] = (span_pivot, span_pivot+1)\n                span_pivot += 1\n            else:\n                if e_id < len(rule)-1:\n                    tmp_rule_1 = [rule[e_id]]\n                    tmp_rule_2 = [rule[e_id+1]]\n                    tmp_matcher = Matcher(self._nlp.vocab)\n                    tmp_matcher.add(0, None, tmp_rule_1)\n                    tmp_matcher.add(1, None, tmp_rule_2)\n                    tmp_matches = sorted([x for x in tmp_matcher(new_doc) if x[1] != x[2]], key=lambda a: a[1])\n\n                    if not tmp_matches:\n                        relation[e_id] = None\n                    else:\n                        matches_1 = [x for x in tmp_matches if x[0] == 0 and x[1] == 0]\n                        if not matches_1:\n                            relation[e_id] = None\n                        else:\n                            _, s1, e1 = matches_1[0]\n                            matches_2 = [x for x in tmp_matches if x[0] == 1]\n                            if not matches_2:\n                                relation[e_id] = (span_pivot, span_pivot + e1)\n                                span_pivot += e1\n                            else:\n                                _, s2, e2 = matches_2[0]\n                                if e1 <= s2:\n                                    relation[e_id] = (span_pivot, span_pivot + e1)\n                                    span_pivot += e1\n                                else:\n                                    relation[e_id] = (span_pivot, span_pivot + s2)\n                                    span_pivot += s2\n                else:\n                    relation[e_id] = (span_pivot, len(span_doc))\n\n        return relation"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _construct_word_token(self, d: Dict, nlp) -> List[Dict]:\n\n        result = []\n        if len(d[\"token\"]) == 1:\n            if tf_transfer(d[\"match_all_forms\"]):\n                this_token = {attrs.LEMMA: nlp(d[\"token\"][0])[0].lemma_}\n            else:\n                this_token = {attrs.LOWER: d[\"token\"][0].lower()}\n            result.append(this_token)\n            if d[\"capitalization\"]:\n                result = self._add_capitalization_constrain(result, d[\"capitalization\"], d[\"token\"])\n\n        elif not d[\"token\"]:\n            if tf_transfer(d[\"contain_digit\"]):\n                this_token = {attrs.IS_ASCII: True, attrs.IS_PUNCT: False}\n            else:\n                this_token = {attrs.IS_ALPHA: True}\n            if tf_transfer(d[\"is_out_of_vocabulary\"]) and not tf_transfer(d[\"is_in_vocabulary\"]):\n                this_token[attrs.IS_OOV] = True\n            elif not tf_transfer(d[\"is_out_of_vocabulary\"]) and tf_transfer(d[\"is_in_vocabulary\"]):\n                this_token[attrs.IS_OOV] = False\n            result.append(this_token)\n            if d[\"length\"]:\n                result = self._add_length_constrain(result, d[\"length\"])\n            if d[\"capitalization\"]:\n                result = self._add_capitalization_constrain(result, d[\"capitalization\"], d[\"token\"])\n\n        else:\n            if \"match_all_forms\" in d and not tf_transfer(d[\"match_all_forms\"]):\n                global FLAG_ID\n                token_set = set(d[\"token\"])\n\n                def is_selected_token(x):\n                    return x in token_set\n\n                FLAG_DICT[FLAG_ID] = nlp.vocab.add_flag(is_selected_token)\n                this_token = {FLAG_DICT[FLAG_ID]: True}\n                FLAG_ID += 1\n                result.append(this_token)\n\n            else:\n                token_set = [nlp(x)[0].lemma_ for x in set(d[\"token\"])]\n                for a_lemma in token_set:\n                    this_token = {attrs.LEMMA: a_lemma}\n                    result.append(this_token)\n\n            if d[\"capitalization\"]:\n                result = self._add_capitalization_constrain(result, d[\"capitalization\"], d[\"token\"])\n\n        result = self._add_common_constrain(result, d)\n        if d[\"part_of_speech\"]:\n            result = self._add_pos_constrain(result, d[\"part_of_speech\"])\n\n        return result", "response": "Construct a word token from a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconstructs a shape token from a dictionary.", "response": "def _construct_shape_token(self, d: Dict) -> List[Dict]:\n        \"\"\"\n        Construct a shape token\n        Args:\n            d: Dict\n\n        Returns: List[Dict]\n        \"\"\"\n\n        result = []\n        if not d[\"shapes\"]:\n            this_token = {attrs.IS_ASCII: True}\n            result.append(this_token)\n        else:\n            for shape in d[\"shapes\"]:\n                this_shape = self._generate_shape(shape)\n                this_token = {attrs.SHAPE: this_shape}\n                result.append(copy.deepcopy(this_token))\n\n        result = self._add_common_constrain(result, d)\n        if d[\"part_of_speech\"]:\n            result = self._add_pos_constrain(result, d[\"part_of_speech\"])\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _construct_number_token(self, d: Dict, nlp) -> List[Dict]:\n\n        result = []\n        if not d[\"numbers\"]:\n            this_token = {attrs.LIKE_NUM: True}\n            result.append(this_token)\n            if d[\"length\"]:\n                result = self._add_length_constrain(result, d[\"length\"])\n        elif len(d[\"numbers\"]) == 1:\n            this_token = {attrs.ORTH: str(d[\"numbers\"][0])}\n            result.append(this_token)\n        else:\n            global FLAG_ID\n            number_set = set(d[\"numbers\"])\n\n            def is_selected_number(x):\n                return x in number_set\n\n            FLAG_DICT[FLAG_ID] = nlp.vocab.add_flag(is_selected_number)\n            this_token = {FLAG_DICT[FLAG_ID]: True}\n            FLAG_ID += 1\n            result.append(this_token)\n        result = self._add_common_constrain(result, d)\n        return result", "response": "Construct a shape token\n            from a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _construct_punctuation_token(self, d: Dict, nlp) -> List[Dict]:\n\n        result = []\n        if not d[\"token\"]:\n            this_token = {attrs.IS_PUNCT: True}\n        elif len(d[\"token\"]) == 1:\n            this_token = {attrs.ORTH: d[\"token\"][0]}\n        else:\n            global FLAG_ID\n            punct_set = set(d[\"token\"])\n\n            def is_selected_punct(x):\n                return x in punct_set\n\n            FLAG_DICT[FLAG_ID] = nlp.vocab.add_flag(is_selected_punct)\n            this_token = {FLAG_DICT[FLAG_ID]: True}\n            FLAG_ID += 1\n        result.append(this_token)\n        result = self._add_common_constrain(result, d)\n        return result", "response": "Construct a shape token\n           "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _construct_linebreak_token(self, d: Dict) -> List[Dict]:\n\n        result = []\n        num_break = int(d[\"length\"][0]) if d[\"length\"] else 1\n        if num_break:\n            s = ''\n            for i in range(num_break):\n                s += '\\n'\n            this_token = {attrs.LOWER: s}\n            result.append(this_token)\n            s += ' '\n            this_token = {attrs.LOWER: s}\n            result.append(this_token)\n        result = self._add_common_constrain(result, d)\n\n        return result", "response": "Construct a shape token\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _add_common_constrain(token_lst: List[Dict], d: Dict) -> List[Dict]:\n\n        result = []\n        for a_token in token_lst:\n            if not tf_transfer(d[\"is_required\"]):\n                a_token[\"OP\"] = \"?\"\n            result.append(a_token)\n        return result", "response": "Add common constrain for every token type like is_required"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding length constrain for some token type create cross production", "response": "def _add_length_constrain(token_lst: List[Dict], lengths: List) -> List[Dict]:\n        \"\"\"\n        Add length constrain for some token type, create cross production\n        Args:\n            token_lst: List[Dict]\n            lengths: List\n\n        Returns: List[Dict]\n        \"\"\"\n\n        result = []\n        for a_token in token_lst:\n            for length in lengths:\n                if type(length) == str and length and length.isdigit():\n                    a_token[attrs.LENGTH] = int(length)\n                    result.append(copy.deepcopy(a_token))\n                elif type(length) == int:\n                    a_token[attrs.LENGTH] = int(length)\n                    result.append(copy.deepcopy(a_token))\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _add_pos_constrain(token_lst: List[Dict], pos_tags: List) -> List[Dict]:\n\n        result = []\n        for a_token in token_lst:\n            for pos in pos_tags:\n                a_token[attrs.POS] = POS_MAP[pos]\n                result.append(copy.deepcopy(a_token))\n        return result", "response": "Add pos tag constrain for some token type create cross production\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _add_capitalization_constrain(token_lst: List[Dict], capi_lst: List, word_lst: List) -> List[Dict]:\n\n        result = []\n        for a_token in token_lst:\n            if \"exact\" in capi_lst and word_lst != []:\n                for word in word_lst:\n                    token = copy.deepcopy(a_token)\n                    token[attrs.ORTH] = word\n                    result.append(token)\n            if \"lower\" in capi_lst:\n                token = copy.deepcopy(a_token)\n                token[attrs.IS_LOWER] = True\n                result.append(token)\n            if \"upper\" in capi_lst:\n                token = copy.deepcopy(a_token)\n                token[attrs.IS_UPPER] = True\n                result.append(token)\n            if \"title\" in capi_lst:\n                token = copy.deepcopy(a_token)\n                token[attrs.IS_TITLE] = True\n                result.append(token)\n            if \"mixed\" in capi_lst:\n                token = copy.deepcopy(a_token)\n                token[attrs.IS_UPPER] = False\n                token[attrs.IS_LOWER] = False\n                token[attrs.IS_TITLE] = False\n                result.append(token)\n        return result", "response": "Add capitalization constrain for some token type create cross production\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrecreating a shape from a token input by user", "response": "def _generate_shape(word: str) -> str:\n        \"\"\"\n        Recreate shape from a token input by user\n        Args:\n            word: str\n\n        Returns: str\n        \"\"\"\n\n        def counting_stars(w) -> List[int]:\n            count = [1]\n            for i in range(1, len(w)):\n                if w[i - 1] == w[i]:\n                    count[-1] += 1\n                else:\n                    count.append(1)\n\n            return count\n\n        shape = \"\"\n        p = 0\n        for c in counting_stars(word):\n            if c > 4:\n                shape += word[p:p + 4]\n            else:\n                shape += word[p:p + c]\n            p = p + c\n\n        return shape"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef process_document(self, doc):\n\n        descriptions = doc.select_segments(\"projects[*].description\")\n        projects = doc.select_segments(\"projects[*]\")\n\n        for d, p in zip(descriptions, projects):\n            names = doc.extract(self.rule_extractor, d)\n            p.store(names, \"members\")\n        return list()", "response": "Process the document and store the result in the database."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef process_document(self, doc):\n\n        segment = doc.select_segments(\"target_text\")[0]\n\n        for e in self.e_list:\n            res = doc.extract(e, segment)\n            doc.store(res, e.name)\n        return list()", "response": "Process the document and store the result in the database"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprocess the document and store the result in the database.", "response": "def process_document(self, doc):\n        \"\"\"\n        Add your code for processing the document\n        \"\"\"\n\n        raw = doc.select_segments(\"$.raw_content\")[0]\n\n        doc.store(doc.extract(self.content_extractor, raw, strategy=Strategy.ALL_TEXT), \"etk2_text\")\n        doc.store(doc.extract(self.content_extractor, raw, strategy=Strategy.MAIN_CONTENT_STRICT),\n                              \"etk2_content_strict\")\n        doc.store(doc.extract(self.content_extractor, raw, strategy=Strategy.MAIN_CONTENT_RELAXED),\n                              \"etk2_content_relaxed\")\n        doc.store(doc.extract(self.metadata_extractor,\n                              raw,\n                              extract_title=True,\n                              extract_meta=True,\n                              extract_microdata=True,\n                              extract_rdfa=True,\n                              ), \"etk2_metadata\")\n        return list()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmaking the document JSON serializable. This is a poor man s implementation that handles dates and nothing else.", "response": "def make_json_serializable(doc: Dict):\n        \"\"\"\n        Make the document JSON serializable. This is a poor man's implementation that handles dates and nothing else.\n        This method modifies the given document in place.\n\n        Args:\n            doc: A Python Dictionary, typically a CDR object.\n\n        Returns: None\n\n        \"\"\"\n        for k, v in doc.items():\n            if isinstance(v, datetime.date):\n                doc[k] = v.strftime(\"%Y-%m-%d\")\n            elif isinstance(v, datetime.datetime):\n                doc[k] = v.isoformat()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_doc_id_from_json(doc) -> str:\n        return hashlib.sha256(json.dumps(doc, sort_keys=True).encode('utf-8')).hexdigest()", "response": "Create a unique ID for a given document."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate sha256 has of a string", "response": "def create_doc_id_string(any_string):\n        \"\"\"\n        Creates sha256 has of a string\n        :param any_string: input string\n        :return: sha256 hash of any_string\n        \"\"\"\n        try:\n            return hashlib.sha256(any_string).hexdigest()\n        except:\n            # probably failed because of unicode\n            return hashlib.sha256(any_string.encode('utf-8')).hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the event type of an event in the cache.", "response": "def event_type(self, event, cameo_code) -> List[str]:\n        \"\"\"\n        Look up the event tupe of an event\n        Args:\n            event: one of \"event1\", \"event2\" or \"event3\"\n            cameo_code: one of the cameo codes\n\n        Returns: a list of the event types or None if the event is not relevant.\n\n        \"\"\"\n        key = self.event_name[event]\n        entry = self.mapping.get(cameo_code)\n        result = None\n        if entry:\n            result = entry[key]\n            if result is None or result == \"\":\n                return None\n            elif not isinstance(result, list):\n                result = [result]\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _actor_property(self, event, cameo_code, actor_regex):\n        if cameo_code not in self.mapping:\n            return None\n\n        arguments = self.mapping[cameo_code][event + \"-arguments\"]\n        if not isinstance(arguments, list):\n            arguments = [arguments]\n\n        result = list()\n        for a in arguments:\n            match = re.search(actor_regex, a)\n            if match:\n                result.append(match.group(1))\n        return result[0] if len(result) > 0 else None", "response": "Determines the property to use for modeling an actor in the log."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntest whether there is an event2 or event3 entry for the given event.", "response": "def has_event(self, event, cameo_code):\n        \"\"\"\n        Test whether there is an \"event2\" or \"event3\" entry for the given cameo code\n        Args:\n            event:\n            cameo_code:\n\n        Returns:\n\n        \"\"\"\n        if self.has_cameo_code(cameo_code):\n            entry = self.mapping.get(cameo_code)\n            if entry:\n                return entry[self.event_name[event]]\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef extract(self, text: str, get_attr=['PERSON', 'ORG', 'GPE']) -> List[Extraction]:\n        doc = self.__nlp(text)\n        attr_list = list()\n        for ent in doc.ents:\n            if ent.label_ in get_attr:\n                attr_list.append(Extraction(extractor_name=self.name,\n                                            start_char=int(ent.start_char),\n                                            end_char=int(ent.end_char),\n                                            value=ent.text,\n                                            tag=ent.label_,\n                                            start_token=ent.start,\n                                            end_token=ent.end))\n        return attr_list", "response": "Extracts the spaCy NER attributes from the text."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing a single Backpage ad and returns a dict representing the ad that is scraped as such.", "response": "def parse_single_ad(ad, global_names, common_words, args={}):\n  \"\"\"\n    An example extraction of a Backpage ad, with the following parameters:\n      ad -> A dict representing an ad that is scraped as such:\n        ad = items.BackpageScrapeItem(\n          backpage_id=response.url.split('.')[0].split('/')[2].encode('utf-8'),\n          date = str(self.static_now)[:10],\n          posted_age = response.xpath(\"//p[@class='metaInfoDisplay']/text()\").extract()[0].encode('utf-8'),\n          posted_date = response.xpath(\"//div[@class='adInfo']/text()\").extract()[0].encode('utf-8'),\n          posted_title = response.xpath(\"//div[@id='postingTitle']//h1/text()\").extract()[0].encode('utf-8'),\n          posting_body= response.xpath(\"//div[@class='postingBody']\").extract()[0].encode('utf-8'),\n          text = response.body,\n          url=response.url\n        )\n  \"\"\"\n\n  multiple_phones = False if 'multiple_phones' not in args else args['multiple_phones']\n\n  item = {}\n\n  # Backpage category\n  ## 1 --> FemaleEscorts\n  ## 2 --> BodyRubs\n  ## 3 --> Dating section (after 1/9/17 Backpage shutdown of FemaleEscorts and BodyRubs)\n  ## 4 --> TherapeuticMassage section (1/23/17 partial begin date, 1/24/17 full begin date)\n  if 'therapeuticmassage' in ad['url'].split('.backpage.com/')[1].lower():\n    item['category'] = 4\n  else:\n    item['category'] = 3\n  \n  # parse age\n  if item['category'] == 4:\n    item['age'] = -1\n  else:\n    item['age'] = int(re.sub(r'\\D', '', ad['posted_age'][14:]))\n\n  # Get rid of any posted age under 10 and over 60. Assign to -1 if it is invalid\n  if item['age'] < 10 or item['age'] > 60:\n    item['age'] = -1\n\n  ageless_title = re.sub(r' {1,2}- {1,2}\\d\\d\\Z', '', ad['posted_title'])\n  ad_text = json.dumps(ad['text'].lower())\n\n  # Get filtered, decomposed list of body + title parts.\n  # 'Parts' are separated only when there is a newline (\\n) character in the ad.\n  decoded_body = decode_unicode(ad['posting_body'], replace_boo=False)[1]\n  decoded_title = decode_unicode(ageless_title, replace_boo=False)[1]\n  parts = get_clean_parts(decoded_body, decoded_title)\n  loc_section = get_location_section(ad_text)\n  loc_parts = get_clean_loc_parts(loc_section, is_location=True)\n\n  all_parts = {\n    'body': parts,\n    'loc': [','.join(loc_parts)]\n  }\n  \n  item['time'] = parser.parse_time(repr(ad['posted_date']))\n\n  item['post_id'] = parser.parse_posting_id(ad_text, ad['city'])\n\n  # Find/remove phone number\n  ret_data = parser.parse_phone(all_parts['body'], allow_multiple=multiple_phones)\n  item['phone'] = ret_data[0]\n  all_parts['body'] = ret_data[1]\n  if multiple_phones:\n    # parse phone(s) from the URL \n    url = ad['url'].replace('http://', '')\n    url = re.sub(r'\\w+\\.backpage\\.com/\\w+/', '', url)\n    url = url.split('/')\n    ret_data = parser.parse_phone(url, allow_multiple=multiple_phones)\n    if ret_data[0]:\n      item['phone'].extend(ret_data[0])\n      item['phone'] = list(set(item['phone']))\n\n  # Find/remove \"No blacks allowed\"\n  ret_data = parser.parse_no_blacks(all_parts['body'])\n  item['no_blacks'] = ret_data[0]\n  all_parts['body'] = ret_data[1]\n\n  # Find/remove ethnicity(s)\n  ret_data = parser.parse_ethnicity(all_parts['body'])\n  item['ethnicity'] = ret_data[0]\n  all_parts['body'] = ret_data[1]\n\n  # Find/remove \"trucker friendly\"\n  ret_data = parser.parse_truckers(all_parts['body'])\n  tf = ret_data[0]\n  if not tf:\n    tf = parser.parse_truckers(all_parts['loc'])[0]\n  item['trucker'] = tf\n  all_parts['body'] = ret_data[1]\n\n  # Find names\n  item['name'] = parser.parse_name(all_parts['body'], global_names, common_words)\n\n  # Indicators that may indicate a HT victim. Still in early stages\n  item['indicators'] = parser.parse_indicators(all_parts['body'], item['ethnicity'])\n\n  # Parse whether or not the ad lists 'military' friendly\n  item['military'] = parser.parse_military_friendly(all_parts['body'])\n\n  return item"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfilters out the extraction if extracted value is in the blacklist", "response": "def filter(self, extractions, case_sensitive=False) -> List[Extraction]:\n        \"\"\"filters out the extraction if extracted value is in the blacklist\"\"\"\n        filtered_extractions = []\n        if not isinstance(extractions, list):\n            extractions = [extractions]\n\n        for extraction in extractions:\n            if case_sensitive:\n                try:\n                    if extraction.value.lower() not in self.black_list:\n                        filtered_extractions.append(extraction)\n                except Exception as e:\n                    print('Error in BlackListFilter: {} while filtering out extraction: {}'.format(e, extraction.value))\n                    # most likely it s a unicode character which is messing things up, return it\n                    filtered_extractions.append(extraction)\n            else:\n                if extraction.value not in self.black_list:\n                    filtered_extractions.append(extraction)\n        return filtered_extractions"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting input knowledge graph object into n - triples RDF in str", "response": "def rdf_generation(kg_object) -> str:\n    \"\"\"\n    Convert input knowledge graph object into n-triples RDF\n\n    :param kg_object: str, dict, or json object\n    :return: n-triples RDF in str\n    \"\"\"\n    import json\n\n    if isinstance(kg_object, dict):\n        kg_object = json.dumps(kg_object)\n    g = Graph()\n    g.parse(data=kg_object, format='json-ld')\n    return g.serialize(format='nt').decode('utf-8')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck if a class is a legal subject.", "response": "def is_legal_subject(self, c: OntologyClass) -> bool:\n        \"\"\"\n        is_legal_subject(c) = true if\n        - c in included_domains(self) or\n        - super_classes_closure(c) intersection included_domains(self) is not empty\n\n        There is no need to check the included_domains(super_properties_closure(self)) because\n        included_domains(super_properties_closure(self)) is subset of super_classes_closure(included_domains(self))\n\n        Args:\n            c:\n\n        Returns:\n\n        \"\"\"\n        domains = self.included_domains()\n        return c and (not domains or c in domains or c.super_classes_closure() & domains)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn true if the given OntologyClass is legal object.", "response": "def is_legal_object(self, c: OntologyClass) -> bool:\n        \"\"\"\n        is_legal_object(c) = true if\n        - c in included_ranges(self) or\n        - super_classes_closure(c) intersection included_ranges(self) is not empty\n\n        Args:\n            c:\n\n        Returns:\n\n        \"\"\"\n        ranges = self.included_ranges()\n        return not ranges or c in ranges or c.super_classes_closure() & ranges"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if the data_type is legal object.", "response": "def is_legal_object(self, data_type: str) -> bool:\n        \"\"\"\n        Do data_type validation according to the rules of the XML xsd schema.\n\n        Args:\n            data_type:\n\n        Returns:\n\n        \"\"\"\n        data_type = str(data_type)\n        ranges = self.included_ranges()\n        return not ranges or data_type in ranges or self.super_properties() and \\\n               any(x.is_legal_object(data_type) for x in self.super_properties())"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef __validation_property_domain(self, p):\n        for x in p.included_domains():\n            superclasses = x.super_classes_closure()\n            for q in p.super_properties_closure():\n                if x in q.included_domains():\n                    logging.warning(\"Redundant domain :%s for :%s.\", x.name(), p.name())\n                    continue\n                if not any(d == x or d in superclasses for d in q.included_domains()) and not any(\n                        d == x or d in superclasses for s in q.super_properties_closure() for d in\n                        s.included_domains()):\n                    raise ValidationError(\"Domain {} of property {} isn't a subclass of any domain of \"\n                                          \"superproperty {}\".format(x.name(), p.name(), q.name()))", "response": "Validate that the domain of a property is a subclass of any superproperty."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nvalidating that the property is a subclass of any range of the superproperties of the object.", "response": "def __validation_property_range(self, p):\n        \"\"\"\n        if p.sub_property_of(q) then\n        for every y in included_ranges(p) we have y is subclass of some r in\n        included_ranges(q) -- for object properties\n        \"\"\"\n        for y in p.included_ranges():\n            superclasses = y.super_classes_closure()\n            for q in p.super_properties():\n                if not any(r == y or r in superclasses for r in q.included_ranges()) and not any(\n                        r == y or r in superclasses for s in q.super_properties_closure() for r in\n                        s.included_ranges()):\n                    raise ValidationError(\"Range {} of property {} isn't a subclass of any range of\"\n                                          \" superproperty {}\".format(y.name(), p.name(), q.name()))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_entity(self, uri: str) -> OntologyClass:\n        return self.entities.get(str(uri), None)", "response": "Find an ontology entity based on URIRef"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the set of all classes that don t have a super class.", "response": "def root_classes(self) -> Set[OntologyClass]:\n        \"\"\"\n        Returns: All classes that don't have a super class.\n        \"\"\"\n        return set(filter(lambda e: not e.super_classes(), self.classes))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmerge master config with current ontology.", "response": "def merge_with_master_config(self, config, defaults={}, delete_orphan_fields=False) -> dict:\n        \"\"\"\n        Merge current ontology with input master config.\n\n        :param config: master config, should be str or dict\n        :param defaults: a dict that sets default color and icon\n        :param delete_orphan_fields: if a property doesn't exist in the ontology then delete it\n        :return: merged master config in dict\n        \"\"\"\n        if isinstance(config, str):\n            import json\n            config = json.loads(config)\n        properties = self.all_properties()\n        config['fields'] = config.get('fields', dict())\n        fields = config['fields']\n\n        d_color = defaults.get('color', 'white')\n        d_icon = defaults.get('icon', 'icons:default')\n\n        if delete_orphan_fields:\n            exist = {p.name() for p in properties}\n            unexist = set(fields.keys()) - exist\n            for name in unexist:\n                del fields[name]\n\n        for p in properties:\n            field = fields.get(p.name(), {'show_in_search': False,\n                                          'combine_fields': False,\n                                          'number_of_rules': 0,\n                                          'glossaries': [],\n                                          'use_in_network_search': False,\n                                          'case_sensitive': False,\n                                          'show_as_link': 'text',\n                                          'blacklists': [],\n                                          'show_in_result': 'no',\n                                          'rule_extractor_enabled': False,\n                                          'search_importance': 1,\n                                          'group_name': '',\n                                          'show_in_facets': False,\n                                          'predefined_extractor': 'none',\n                                          'rule_extraction_target': ''})\n            config['fields'][p.name()] = field\n            field['screen_label'] = ' '.join(p.label())\n            field['description'] = '\\n'.join(p.definition())\n            field['name'] = p.name()\n\n            # color\n            if 'color' not in field:\n                color = self.__merge_close_ancestor_color(p, fields, attr='color')\n                field['color'] = color if color else d_color\n            # icon\n            if 'icon' not in field:\n                icon = self.__merge_close_ancestor_color(p, fields, attr='icon')\n                field['icon'] = icon if icon else d_icon\n            # type\n            if isinstance(p, OntologyObjectProperty):\n                field['type'] = 'kg_id'\n            else:\n                try:\n                    field['type'] = self.__merge_xsd_to_type(next(iter(p.included_ranges())))\n                except StopIteration:\n                    field['type'] = None\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if the given value is valid for the given name property according to input knowledge graph and return a dict with key = value for the object property and value for the DatatypeProperty.", "response": "def is_valid(self, field_name: str, value, kg: dict) -> Optional[dict]:\n        \"\"\"\n        Check if this value is valid for the given name property according to input knowledge graph and ontology.\n        If is valid, then return a dict with key @id or @value for ObjectProperty or DatatypeProperty.\n        No schema checked by this function.\n\n        :param field_name: name of the property, if prefix is omitted, then use default namespace\n        :param value: the value that try to add\n        :param kg: the knowledge graph that perform adding action\n        :return: None if the value isn't valid for the property, otherwise return {key: value}, key is @id for\n            ObjectProperty and @value for DatatypeProperty.\n        \"\"\"\n        # property\n        uri = self.__is_valid_uri_resolve(field_name, kg.get(\"@context\"))\n        property_ = self.get_entity(uri)\n        if not isinstance(property_, OntologyProperty):\n            logging.warning(\"Property is not OntologyProperty, ignoring it:  %s\", uri)\n            return None\n        if not self.__is_valid_domain(property_, kg):\n            logging.warning(\"Property does not have valid domain, ignoring it:  %s\", uri)\n            return None\n        # check if is valid range\n        # first determine the input value type\n        if isinstance(property_, OntologyDatatypeProperty):\n            types = self.__is_valid_determine_value_type(value)\n        else:\n            if isinstance(value, dict):\n                try:\n                    types = map(self.get_entity, value['@type'])\n                except KeyError:\n                    return None  # input entity without type\n            elif self.__is_schema_org_datatype(property_):\n                if self.expanded_jsonld:\n                    return {'@value': self.__serialize_type(value)}\n                else:\n                    return value\n            else:\n                return {'@id': self.__serialize_type(value)}\n        # check if is a valid range\n        if any(property_.is_legal_object(type_) for type_ in types):\n            if isinstance(property_, OntologyObjectProperty):\n                return value\n            elif self.expanded_jsonld:\n                return {'@value': self.__serialize_type(value)}\n            else:\n                return self.__serialize_type(value)\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef phone_num_lists():\n\n  all_nums = {}\n  all_nums['2'] = ['2', 'two']\n  all_nums['3'] = ['3', 'three']\n  all_nums['4'] = ['4', 'four', 'fuor']\n  all_nums['5'] = ['5', 'five', 'fith']\n  all_nums['6'] = ['6', 'six']\n  all_nums['7'] = ['7', 'seven', 'sven']\n  all_nums['8'] = ['8', 'eight']\n  all_nums['9'] = ['9', 'nine']\n  all_nums['0'] = ['0', 'zero', 'zer0', 'oh', 'o']\n  all_nums['1'] = ['1', 'one', '!' 'l', 'i']\n\n  return all_nums", "response": "Returns a dictionary of all possible Backpage ad manifestations for a given key."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef phone_text_subs():\n\n  Small = {\n    'zero': 0,\n    'zer0': 0,\n    'one': 1,\n    'two': 2,\n    'three': 3,\n    'four': 4,\n    'fuor': 4,\n    'five': 5,\n    'fith': 5,\n    'six': 6,\n    'seven': 7,\n    'sven': 7,\n    'eight': 8,\n    'nine': 9,\n    'ten': 10,\n    'eleven': 11,\n    'twelve': 12,\n    'thirteen': 13,\n    'fourteen': 14,\n    'fifteen': 15,\n    'sixteen': 16,\n    'seventeen': 17,\n    'eighteen': 18,\n    'nineteen': 19,\n    'twenty': 20,\n    'thirty': 30,\n    'forty': 40,\n    'fifty': 50,\n    'sixty': 60,\n    'seventy': 70,\n    'eighty': 80,\n    'ninety': 90,\n    'oh': 0\n  }\n\n  Magnitude = {\n    'thousand': 000,\n    'million': 000000,\n  }\n\n  Others = {\n    '!': 1,\n    'o': 0,\n    'l': 1,\n    'i': 1\n  }\n\n  output = {}\n  output['Small'] = Small\n  output['Magnitude'] = Magnitude\n  output['Others'] = Others\n\n  return output", "response": "Returns a dictionary of dictionaries that each contains a single string containing the actual alphabetic number manifestations mapped to their actual number value."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsplitting text by sentences and returns a list of Extraction objects.", "response": "def extract(self, text: str) -> List[Extraction]:\n        \"\"\"\n        Splits text by sentences.\n\n        Args:\n            text (str): Input text to be extracted.\n\n        Returns:\n            List[Extraction]: the list of extraction or the empty list if there are no matches.\n        \"\"\"\n\n        doc = self._parser(text)\n\n        extractions = list()\n        for sent in doc.sents:\n            this_extraction = Extraction(value=sent.text,\n                                         extractor_name=self.name,\n                                         start_token=sent[0],\n                                         end_token=sent[-1],\n                                         start_char=sent.text[0],\n                                         end_char=sent.text[-1])\n            extractions.append(this_extraction)\n\n        return extractions"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a glossary for the given attribute name", "response": "def add_glossary(self, glossary: List[str], attr_name: str) -> None:\n        \"\"\"\n        Adds a glossary for the given attribute name\n        :param glossary: a list of possible mentions of the attribute name\n        :param attr_name: the attribute name (field name)\n        \"\"\"\n        self.glossaries[attr_name] = glossary"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nextract all the extractions from the input table.", "response": "def extract(self, table: dict) -> List[Extraction]:\n        \"\"\"\n        :param table: a table extracted by table extractor, as a json object\n        :return: list of all extractions from the input table\n        \"\"\"\n        if table['features']['max_cols_in_a_row'] != 2 and table['features']['no_of_rows'] < 2:\n            return []\n        results = list()\n        for row in table['rows']:\n            if len(row['cells']) != 2:\n                continue\n            text = [row['cells'][0]['text'], row['cells'][1]['text']]\n            for field_name in self.glossaries.keys():\n                if self.cell_matches_dict(text[0], self.glossaries[field_name]):\n                    results.append(self.wrap_value_with_context(text[1], field_name))\n                if self.cell_matches_dict(text[1], self.glossaries[field_name]):\n                    results.append(self.wrap_value_with_context(text[0], field_name))\n        return results"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pstdev(data):\n        n = len(data)\n        if n < 2:\n            return 0\n            # raise ValueError('variance requires at least two data points')\n        ss = TableExtraction._ss(data)\n        pvar = ss/n # the population variance\n        return pvar**0.5", "response": "Calculates the population standard deviation."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn html table string from a list of data rows", "response": "def gen_html(row_list):\n        \"\"\" Return html table string from a list of data rows \"\"\"\n        table = \"<table>\"\n        for row in row_list:\n            table += \"<tr>\"\n            cells = row[\"cells\"]\n            for c in cells:\n                t = c['cell'] if c else ''\n                table += t\n            table += \"</tr>\"\n        table += \"</table>\"\n        return table"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrap the final result of the extraction with the context of the current object.", "response": "def _wrap_value_with_context(self, value: dict or str, field_name: str, start: int=0, end: int=0) -> Extraction:\n        \"\"\"Wraps the final result\"\"\"\n        return Extraction(value, self.name, start_token=start, end_token=end, tag=field_name)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef extract(self, html: str, return_text: bool=False) -> List[Extraction]:\n        results = list()\n        temp_res = TableExtractor.tableExtractorInstance.extract(html)\n        if return_text:\n            results.append(self._wrap_value_with_context(temp_res['html_text'], \"text_without_tables\"))\n        results.extend(map(lambda t: self._wrap_value_with_context(t, \"tables\"), temp_res['tables']))\n        return results", "response": "Extract the page from the given html and return a list of Extractions"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef process_document(self, doc):\n        d = doc.select_segments(\"$.raw_content\")[0]\n\n        tables = doc.extract(self.table_extractor, d)\n        for t in tables:\n            doc.store([t], t.tag, group_by_tags=False)\n\n        table_data_extractor = EntityTableDataExtraction()\n        table_data_extractor.add_glossary(etk.load_glossary(\"./resources/address_dict.txt\"), \"address\")\n        table_data_extractor.add_glossary(etk.load_glossary(\"./resources/calibre_dict.txt\"), \"caliber\")\n        table_data_extractor.add_glossary(etk.load_glossary(\"./resources/capacity_dict.txt\"), \"capacity\")\n        table_data_extractor.add_glossary(etk.load_glossary(\"./resources/manufacturer_dict.txt\"), \"manufacturer\")\n        table_data_extractor.add_glossary(etk.load_glossary(\"./resources/price_dict.txt\"), \"price\")\n\n        tables = doc.select_segments(\"$.tables[*]\")\n\n        for t in tables:\n            extractions = doc.extract(table_data_extractor, t)\n            doc.store(extractions, \"table_data_extraction\")\n        return list()", "response": "Process the document and return a list of the objects that are available in the database."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nextracts text from an HTML page using a variety of strategies.", "response": "def extract(self, html_text: str, strategy: Strategy=Strategy.ALL_TEXT) \\\n            -> List[Extraction]:\n        \"\"\"\n        Extracts text from an HTML page using a variety of strategies\n\n        Args:\n            html_text (str): html page in string\n            strategy (enum[Strategy.ALL_TEXT, Strategy.MAIN_CONTENT_RELAXED, Strategy.MAIN_CONTENT_STRICT]): one of\n            Strategy.ALL_TEXT, Strategy.MAIN_CONTENT_STRICT and Strategy.MAIN_CONTENT_RELAXED\n\n        Returns:\n             List[Extraction]: typically a singleton list with the extracted text\n        \"\"\"\n\n        if html_text:\n            if strategy == Strategy.ALL_TEXT:\n                soup = BeautifulSoup(html_text, 'html.parser')\n                texts = soup.findAll(text=True)\n                visible_texts = filter(self._tag_visible, texts)\n                all_text = u\" \".join(t.strip() for t in visible_texts)\n                return [Extraction(all_text, self.name)]\n            else:\n                relax = strategy == Strategy.MAIN_CONTENT_RELAXED\n                readable = Document(html_text, recallPriority=relax).summary(html_partial=False)\n                clean_text = BeautifulSoup(readable.encode('utf-8'), 'lxml').strings\n                readability_text = ' '.join(clean_text)\n                return [Extraction(readability_text, self.name)]\n        else:\n            return []"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprocesses a single document and store the results in the cache", "response": "def process_document(self, doc):\n        \"\"\"\n        Add your code for processing the document\n        \"\"\"\n\n        raw = doc.select_segments(\"$.raw_content\")[0]\n        extractions = doc.extract(self.inferlink_extractor, raw)\n        doc.store(extractions, \"inferlink_extraction\")\n        return list()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef process_document(self, doc):\n\n        descriptions = doc.select_segments(\"projects[*].description\")\n        projects = doc.select_segments(\"projects[*]\")\n\n        for d, p in zip(descriptions, projects):\n\n            # First phase of extraction\n            names = doc.extract(self.name_extractor, d)\n            p.store(names, \"members\")\n\n            # Second phase of extraction\n            students = []\n            for name_extraction in names:\n                students += doc.extract(self.student_extractor, name_extraction)\n            p.store(students, \"students\")\n        doc.kg.add_value(\"developer\", json_path=\"projects[*].members[*]\")\n        doc.kg.add_value(\"owner\", json_path=\"projects[*].students[*]\")\n        return list()", "response": "Process the document and store the information in the KG field."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncleans a string for ethnicities.", "response": "def clean_part_ethn(body):\n  \"\"\"\n    Prepare a string to be parsed for ethnicities.\n\n    Returns a \"translated\" string (e.g. all instances of \"china\" converted to \"chinese\")\n  \"\"\"\n\n  # patterns that can create false positive situations\n  patterns_to_remove = [r'black ?or ?african', r'african ?or ?black', r'no ?black', r'no ?african', r'no ?aa', r'white ?men', \n  r'white ?gentlemen', r'no ?spanish', r'speak ?spanish', r'black ?(guys|men|hair|client)', r'dark ?hair', \n  r'(dark ?)?brown ?hair', r'white ?tie']\n\n  # indian states to convert the term 'indian'\n  indian_states = ['awadhi', 'badhi', 'bhutia', 'garhwali', 'halbi', 'kamboj', 'bhattarai', 'bhotiya', 'pardeshi',\n  'bengali', 'madra', 'tamil', 'rajasthani', 'adivasi']\n\n  for p in patterns_to_remove:\n    body = re.sub(p, '', body)\n  for i in indian_states:\n    body = body.replace(i, 'indian')\n\n  # regex substitutions\n  body = re.sub(r'hong ?kong', 'chinese', body)\n  body = re.sub(r'snow ?bunn(y|ies)', 'white', body)\n  body = re.sub(r'a\\ss\\si\\sa\\sn', 'asian', body)\n  body = re.sub(r'l\\sa\\st\\si\\sn\\sa', 'latina', body)\n\n  # convert many ethnicity variations into standardized ones (e.g. china -> chinese)\n  for sub in eth_subs:\n    body = body.replace(sub, eth_subs[sub])\n\n  body = re.sub(r' +', ' ', body)\n\n  return body"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef extract(self, file_name: str, sheet_name: str, region: List, variables: Dict) -> List[Extraction]:\n        extractions = []\n\n        book = pyexcel.get_book(file_name=file_name)\n        sheet = book[sheet_name]\n        region = [ExcelExtractor._excel_coord_to_location(coord) for coord in region]\n        r = region[0][0]\n        # per row\n        for row in sheet.region(region[0], region[1]):\n            c = region[0][1]\n            # per col\n            for col in row:\n                var = copy.deepcopy(variables)\n                # per variable\n                for k, v in var.items():\n                    parsed_v = ExcelExtractor._parse_variable(v, r, c)\n                    if len(parsed_v) == 1:  # normal variable\n                        var[k] = parsed_v[0]\n                    else:  # location\n                        rr, cc = parsed_v\n                        var[k] = sheet[rr, cc]\n                    extractions.append(var)\n\n                c += 1\n            r += 1\n\n        return extractions", "response": "Extracts the variables from the file in the specified region and returns a list of extracted variables."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses a variable in the Excel file.", "response": "def _parse_variable(s: str, curr_row: int, curr_col: int) -> Tuple:\n        '''\n        $A,$2 <- constant col and row\n        $row,$2 <- current col, row 2\n        $A+1,$2 <- col A + 1 = 2, row 2\n        $row+1,$2 <- current col + 1, row 2\n        $A,$2-1 <-- col A, row 2 - 1 = 1\n        '''\n\n        def parse_expression(ss, curr_row, curr_col):\n            ss = ss.replace('$row', str(curr_row))\n            ss = ss.replace('$col', str(curr_col))\n            ss = ExcelExtractor._re_row_identifier.sub(\n                lambda x: str(ExcelExtractor._row_name_to_num(x.group()[1:])) if len(x.group()) > 0 else '', ss)\n            ss = ExcelExtractor._re_col_identifier.sub(\n                lambda x: str(ExcelExtractor._col_name_to_num(x.group()[1:])) if len(x.group()) > 0 else '', ss)\n            return eval(ss)\n\n        ss = s.split(',')\n        if len(ss) == 1:\n            return parse_expression(ss[0], curr_row, curr_col),\n        elif len(ss) == 2:\n            rr, cc = (ss[1], ss[0])\n            return parse_expression(rr, curr_row, curr_col), parse_expression(cc, curr_row, curr_col)\n        else:\n            raise ValueError('Invalid variable')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef validate_field(self, field_name: str) -> bool:\n        if field_name in {\"@id\", \"@type\"}:\n            return True\n        result = self.schema.has_field(field_name)\n        if not result:\n            # todo: how to comply with our error handling policies?\n            raise UndefinedFieldError(\"'{}' should be present in the knowledge graph schema.\".format(field_name))\n        return result", "response": "Validate that a field is present in the knowledge graph schema."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _add_value(self, field_name: str, value, provenance_path=None) -> bool:\n        if not isinstance(value, list):\n            value = [value]\n\n        all_valid = True\n        for x in value:\n            valid = self._add_single_value(field_name, x, provenance_path=provenance_path)\n            all_valid = all_valid and valid\n        return all_valid", "response": "Helper function to add values to a knowledge graph"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _add_doc_value(self, field_name: str, jsonpath: str) -> None:\n        path = self.origin_doc.etk.parse_json_path(jsonpath)\n        matches = path.find(self.origin_doc.value)\n        all_valid = True\n        invalid = []\n        for a_match in matches:\n            # If the value is the empty string, we treat is a None.\n            if a_match.value:\n                valid = self._add_value(field_name, a_match.value, provenance_path=str(a_match.full_path))\n                if not valid:\n                    invalid.append(field_name + \":\" + str(a_match.value))\n                all_valid = all_valid and valid\n\n        if not all_valid:\n            raise KgValueError(\"Some kg value type invalid according to schema: \" + json.dumps(invalid))", "response": "Add a value to the knowledge graph by giving a jsonpath"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_value(self, field_name: str, value: object = None, json_path: str = None,\n                  json_path_extraction: str = None, keep_empty: bool = False) -> None:\n        \"\"\"\n        Add a value to knowledge graph.\n        Input can either be a value or a json_path. If the input is json_path, the helper function _add_doc_value is\n        called.\n        If the input is a value, then it is handled\n\n        Args:\n            field_name: str, the field name in the knowledge graph\n            value: the value to be added to the knowledge graph\n            json_path: str, if json_path is provided, then get the value at this path in the doc\n            json_path_extraction: str,\n            discard_empty: bool,\n        Returns:\n        \"\"\"\n\n        def validate(v):\n            if v is not None:\n                if isinstance(v, str):\n                    if v.strip() != \"\" or keep_empty:\n                        return True\n                    else:\n                        return False\n                else:\n                    return True\n            return False\n\n        self.validate_field(field_name)\n        if field_name not in self._kg:\n            self._kg[field_name] = []\n\n        if json_path:\n            self._add_doc_value(field_name, json_path)\n\n        if validate(value):\n            if not isinstance(value, list):\n                value = [value]\n\n            all_valid = True\n            invalid = []\n            for a_value in value:\n                if isinstance(a_value, Extraction):\n                    valid = self._add_single_value(field_name, a_value.value, provenance_path=str(json_path_extraction),\n                                                   keep_empty=keep_empty)\n                elif isinstance(a_value, Segment):\n                    valid = self._add_single_value(field_name, a_value.value, provenance_path=a_value.json_path,\n                                                   keep_empty=keep_empty)\n                else:\n                    valid = self._add_single_value(field_name, a_value, provenance_path=json_path_extraction,\n                                                   reference_type=\"constant\", keep_empty=keep_empty)\n\n                all_valid = all_valid and valid\n                if not valid:\n                    invalid.append(field_name + \":\" + str(a_value))\n            if not all_valid:\n                print(\"Some kg value type invalid according to schema:\" + json.dumps(invalid))\n                # raise KgValueError(\"Some kg value type invalid according to schema\")\n        # IF we did not add any value, remove the empty field we just added to kg\n        if len(self._kg[field_name]) == 0:\n            self._kg.pop(field_name)", "response": "Add a value to the knowledge graph."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a list of all the values of a field.", "response": "def get_values(self, field_name: str) -> List[object]:\n        \"\"\"\n        Get a list of all the values of a field.\n\n        Args:\n            field_name:\n\n        Returns: the list of values (not the keys)\n\n        \"\"\"\n        result = list()\n        if self.validate_field(field_name):\n            for value_key in self._kg.get(field_name):\n                result.append(value_key[\"value\"])\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef context_resolve(self, field_uri: str) -> str:\n        from rdflib.namespace import split_uri\n        context = self._kg[\"@context\"] = self._kg.get(\"@context\", dict())\n        nm = self.ontology.g.namespace_manager\n        space, name = split_uri(field_uri)\n        if \"@vocab\" not in context and None in nm.namespaces():\n            context[\"@vocab\"] = nm.store.prefix(space)\n        if \"@vocab\" in context and space == context[\"@vocab\"]:\n            # case #1, can directly use name\n            return name\n        if self.schema.has_field(name):\n            if name not in context:\n                prefix = [x for x in list(self.ontology.g.namespace_manager.namespaces())]\n                for x, y in prefix:\n                    if space[:-1] == x:\n                        context[name] = str(y) + name\n                        return name\n                context[name] = field_uri\n            return name\n        prefix = nm.store.prefix(space)\n        if prefix:\n            context[prefix] = space\n            return nm.qname(field_uri)\n        return field_uri", "response": "Resolves a field_uri to add corresponding context and returns a resolvable field_name"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprocess the document and return a list of the entries.", "response": "def process_document(self, doc):\n        \"\"\"\n        Add your code for processing the document\n        \"\"\"\n        input_text = doc.select_segments(\"input\")[0]\n        format = doc.select_segments('format')[0]\n\n        ignore_before = datetime.datetime(1890, 1, 1)\n        ignore_after = datetime.datetime(2500, 10, 10)\n        relative_base = datetime.datetime(2018, 1, 1)\n\n        dates = doc.extract(\n            self.date_extractor,\n            input_text,\n            extract_first_date_only=False,  # first valid\n\n            additional_formats=[format._value],\n\n            use_default_formats=False,\n\n            # ignore_dates_before: datetime.datetime = None,\n            ignore_dates_before=ignore_before,\n\n            # ignore_dates_after: datetime.datetime = None,\n            ignore_dates_after=ignore_after,\n\n            detect_relative_dates=not format._value,\n\n            relative_base=relative_base,\n\n            # preferred_date_order: str = \"MDY\",  # used for interpreting ambiguous dates that are missing parts\n            preferred_date_order=\"DMY\",\n\n            prefer_language_date_order=True,\n\n            # timezone: str = None,  # default is local timezone.\n            # timezone='GMT',\n\n            # to_timezone: str = None,  # when not specified, not timezone conversion is done.\n            # to_timezone='UTC',\n\n            # return_as_timezone_aware: bool = True\n            return_as_timezone_aware=False,\n\n            # prefer_day_of_month: str = \"first\",  # can be \"current\", \"first\", \"last\".\n            prefer_day_of_month='first',\n\n            # prefer_dates_from: str = \"current\"  # can be \"current\", \"future\", \"past\".)\n            prefer_dates_from='current',\n\n            # date_value_resolution: DateResolution = DateResolution.DAY\n            date_value_resolution = DateResolution.SECOND if format._value and format._value[1] in ['H','I'] else DateResolution.DAY\n        )\n        doc.select_segments('$')[0].store(dates, \"extracted_date\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef process_document(self, doc):\n        member_descriptions = doc.select_segments(\"members[*].description\")\n        members = doc.select_segments(\"members[*]\")\n\n        ignore_before = datetime.datetime(1890, 1, 1)\n        ignore_after = datetime.datetime(2500, 10, 10)\n        relative_base = datetime.datetime(2018, 1, 1)\n\n        for m_d, m in zip(member_descriptions, members):\n            dates = doc.extract(\n                self.date_extractor,\n                m_d,\n                extract_first_date_only=False,  # first valid\n\n                additional_formats=['%Y@%m@%d', '%a %Y, %b %d'],\n\n                use_default_formats=True,\n\n                # ignore_dates_before: datetime.datetime = None,\n                ignore_dates_before=ignore_before,\n\n                # ignore_dates_after: datetime.datetime = None,\n                ignore_dates_after=ignore_after,\n\n                detect_relative_dates=False,\n\n                relative_base=relative_base,\n\n                # preferred_date_order: str = \"MDY\",  # used for interpreting ambiguous dates that are missing parts\n                preferred_date_order=\"DMY\",\n\n                prefer_language_date_order=True,\n\n                # timezone: str = None,  # default is local timezone.\n                # timezone='GMT',\n\n                # to_timezone: str = None,  # when not specified, not timezone conversion is done.\n                # to_timezone='UTC',\n\n                # return_as_timezone_aware: bool = True\n                return_as_timezone_aware=False,\n\n                # prefer_day_of_month: str = \"first\",  # can be \"current\", \"first\", \"last\".\n                prefer_day_of_month='first',\n\n                # prefer_dates_from: str = \"current\"  # can be \"future\", \"future\", \"past\".)\n                prefer_dates_from='future',\n\n                # date_value_resolution: DateResolution = DateResolution.DAY\n            )\n            m.store(dates, \"related_dates\")\n        return list()", "response": "Process the document and return a list of the entries in the cluster."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _check_domain(tokens) -> bool:\n\n        idx = None\n        for e in tokens:\n            if e.text == \"@\":\n                idx = e.i\n                break\n        if not idx or tokens[idx+1].text in FILTER_PROVIDER:\n            return False\n        else:\n            return True", "response": "Checks if the domain should be filtered by the email provider."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndeals with corner case that there is an email string in text and no space around it", "response": "def _get_non_space_email(self, doc) -> List:\n        \"\"\"\n        Deal with corner case that there is \"email\" string in text and no space around it\n        Args:\n            doc: List[Token]\n\n        Returns: Bool\n        \"\"\"\n        result_lst = []\n        for e in doc:\n            if \"mail:\" in e.text.lower():\n                idx = e.text.lower().index(\"mail:\") + 5\n                value = e.text[idx:]\n                tmp_doc = self._nlp(value)\n                tmp_email_matches = self._like_email_matcher(tmp_doc)\n                for match_id, start, end in tmp_email_matches:\n                    span = tmp_doc[start:end]\n                    if self._check_domain(self._tokenizer.tokenize(span.text)):\n                        result_lst.append((span.text, idx+e.idx, idx+e.idx+len(value)))\n\n        return result_lst"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef tokenize_to_spacy_doc(self, text: str) -> Doc:\n        if not self.keep_multi_space:\n            text = re.sub(' +', ' ', text)\n        doc = self.nlp(text, disable=['parser'])\n        for a_token in doc:\n            self.custom_token(a_token)\n\n        return doc", "response": "Tokenize the given text returning a spacy doc. Used for spacy rule extractor."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef custom_token(spacy_token) -> Token:\n\n        \"\"\"Add custom attributes\"\"\"\n        \"\"\"Add full_shape attribute. Eg. 21.33 => dd.dd, esadDeweD23 => xxxxXxxxXdd\"\"\"\n\n        def get_shape(token):\n            full_shape = \"\"\n            for i in token.text:\n                if i.isdigit():\n                    full_shape += \"d\"\n                elif i.islower():\n                    full_shape += \"x\"\n                elif i.isupper():\n                    full_shape += \"X\"\n                else:\n                    full_shape += i\n            return full_shape\n\n        spacy_token.set_extension(\"full_shape\", getter=get_shape, force=True)\n\n        def is_integer(token):\n            pattern = re.compile('^[-+]?[0-9]+$')\n            return bool(pattern.match(token.text))\n\n        spacy_token.set_extension(\"is_integer\", getter=is_integer, force=True)\n\n        def is_decimal(token):\n            pattern = re.compile('^[-+]?[0-9]+\\.[0-9]+$')\n            return bool(pattern.match(token.text))\n\n        spacy_token.set_extension(\"is_decimal\", getter=is_decimal, force=True)\n\n        def is_ordinal(token):\n            return token.orth_[-2:] in ['rd', 'st', 'th', 'nd']\n\n        spacy_token.set_extension(\"is_ordinal\", getter=is_ordinal, force=True)\n\n        def is_mixed(token):\n            if not token.is_title and not token.is_lower and not token.is_upper:\n                return True\n            else:\n                return False\n\n        spacy_token.set_extension(\"is_mixed\", getter=is_mixed, force=True)\n\n        \"\"\"Add custom methods\"\"\"\n        \"\"\"Add get_prefix method. RETURN length N prefix\"\"\"\n\n        def n_prefix(token, n):\n            return token.text[:n]\n\n        spacy_token.set_extension(\"n_prefix\", method=n_prefix, force=True)\n\n        \"\"\"Add get_suffix method. RETURN length N suffix\"\"\"\n\n        def n_suffix(token, n):\n            return token.text[-n:]\n\n        spacy_token.set_extension(\"n_suffix\", method=n_suffix, force=True)\n\n        return spacy_token", "response": "Function for setting custom attributes of the token."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngives a list of tokens reconstruct the original text with as much fidelity as possible.", "response": "def reconstruct_text(tokens: List[Token]) -> str:\n        \"\"\"\n        Given a list of tokens, reconstruct the original text with as much fidelity as possible.\n\n        Args:\n            [tokens]:\n\n        Returns: a string.\n\n        \"\"\"\n        return \"\".join([x.text_with_ws for x in tokens])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating an HTML report from the current object.", "response": "def generate_html_report(self, include_turtle=False, exclude_warning=False, list_auxiliary_line=False) -> str:\n        \"\"\"\n        Shows links to all classes and properties, a nice hierarchy of the classes, and then a nice\n        description of all the classes with all the properties that apply to it.\n        Example: http://www.cidoc-crm.org/sites/default/files/Documents/cidoc_crm_version_5.0.4.html\n\n        :param include_turtle: include turtle related to this entity.\n        :param exclude_warning: Exclude warning messages in HTML report\n        :return: HTML in raw string\n        \"\"\"\n        import os\n        template = os.path.dirname(os.path.abspath(__file__)) + '/../ontologies/template.html'\n        with open(template) as f:\n            # Lists\n            content = f.read().replace('{{{title}}}', 'Ontology Entities')\n            content = content.replace('{{{class_list}}}', self.__html_entities_hierarchy(self.classes))\n            content = content.replace('{{{dataproperty_list}}}', self.__html_entities_hierarchy(self.data_properties))\n            content = content.replace('{{{objectproperty_list}}}', self.__html_entities_hierarchy(self.object_properties))\n            # Classes\n            content = content.replace('{{{classes}}}', self.__html_classes(include_turtle))\n            # Properties\n            properties = self.__html_properties(include_turtle)\n            content = content.replace('{{{dataproperties}}}', properties[0])\n            content = content.replace('{{{objectproperties}}}', properties[1])\n\n            # Logging\n            content = content.replace('{{{logging-title}}}', '' if exclude_warning else '<h2>Logging</h2>')\n            logs = '' if exclude_warning else self.ontology.log_stream.getvalue()\n            content = content.replace('{{{logging}}}', '<pre><code>{}</code></pre>'.format(logs))\n\n            # Auxiliary line\n            content = content.replace('{{{list_auxiliary_line}}}', self.__show_list_auxiliary_line(list_auxiliary_line))\n        return content"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef clonetopath(self, dest):\n        raise Exception(\n            \"%s.%s needs to implement @classmethod .clonetopath(dest)\" % (\n                self.__class__.__module__, self.__class__.__name__))", "response": "Clone the repo at self. pushablepath into dest."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _removecleaner(self, cleaner):\n        oldlen = len(self._old_cleaners)\n        self._old_cleaners = [\n            oldc for oldc in self._old_cleaners\n            if not oldc.issame(cleaner)\n        ]\n        return len(self._old_cleaners) != oldlen", "response": "Removes the cleaner from the list if it already exists. Returns True if the cleaner was removed."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a git repository to the local clone of it and returns the local clone of it.", "response": "def add(repo_path, dest_path):\n    '''\n    Registers a git repository with homely so that it will run its `HOMELY.py`\n    script on each invocation of `homely update`. `homely add` also immediately\n    executes a `homely update` so that the dotfiles are installed straight\n    away. If the git repository is hosted online, a local clone will be created\n    first.\n\n    REPO_PATH\n        A path to a local git repository, or the URL for a git repository\n        hosted online. If REPO_PATH is a URL, then it should be in a format\n        accepted by `git clone`. If REPO_PATH is a URL, you may also specify\n        DEST_PATH.\n    DEST_PATH\n        If REPO_PATH is a URL, then the local clone will be created at\n        DEST_PATH. If DEST_PATH is omitted then the path to the local clone\n        will be automatically derived from REPO_PATH.\n    '''\n    mkcfgdir()\n    try:\n        repo = getrepohandler(repo_path)\n    except NotARepo as err:\n        echo(\"ERROR: {}: {}\".format(ERR_NOT_A_REPO, err.repo_path))\n        sys.exit(1)\n\n    # if the repo isn't on disk yet, we'll need to make a local clone of it\n    if repo.isremote:\n        localrepo, needpull = addfromremote(repo, dest_path)\n    elif dest_path:\n        raise UsageError(\"DEST_PATH is only for repos hosted online\")\n    else:\n        try:\n            repoid = repo.getrepoid()\n        except RepoHasNoCommitsError as err:\n            echo(\"ERROR: {}\".format(ERR_NO_COMMITS))\n            sys.exit(1)\n        localrepo = RepoInfo(repo, repoid, None)\n        needpull = False\n\n    # if we don't have a local repo, then there is nothing more to do\n    if not localrepo:\n        return\n\n    # remember this new local repo\n    with saveconfig(RepoListConfig()) as cfg:\n        cfg.add_repo(localrepo)\n    success = run_update([localrepo], pullfirst=needpull, cancleanup=True)\n    if not success:\n        sys.exit(1)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef forget(identifier):\n    '''\n    Tells homely to forget about a dotfiles repository that was previously\n    added. You can then run `homely update` to have homely perform automatic\n    cleanup of anything that was installed by that dotfiles repo.\n\n    REPO\n        This should be the path to a local dotfiles repository that has already\n        been registered using `homely add`. You may specify multiple REPOs to\n        remove at once.\n    '''\n    errors = False\n    for one in identifier:\n        cfg = RepoListConfig()\n        info = cfg.find_by_any(one, \"ilc\")\n        if not info:\n            warn(\"No repos matching %r\" % one)\n            errors = True\n            continue\n\n        # update the config ...\n        note(\"Removing record of repo [%s] at %s\" % (\n            info.shortid(), info.localrepo.repo_path))\n        with saveconfig(RepoListConfig()) as cfg:\n            cfg.remove_repo(info.repoid)\n\n    # if there were errors, then don't try and do an update\n    if errors:\n        sys.exit(1)", "response": "Forget about a given set of dotfiles repositories."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nperforms a git pull in each of the repositories registered with homely add and performs automatic cleanup as necessary.", "response": "def update(identifiers, nopull, only):\n    '''\n    Performs a `git pull` in each of the repositories registered with\n    `homely add`, runs all of their HOMELY.py scripts, and then performs\n    automatic cleanup as necessary.\n\n    REPO\n        This should be the path to a local dotfiles repository that has already\n        been registered using `homely add`. If you specify one or more `REPO`s\n        then only the HOMELY.py scripts from those repositories will be run,\n        and automatic cleanup will not be performed (automatic cleanup is only\n        possible when homely has done an update of all repositories in one go).\n        If you do not specify a REPO, all repositories' HOMELY.py scripts will\n        be run.\n\n    The --nopull and --only options are useful when you are working on your\n    HOMELY.py script - the --nopull option stops you from wasting time checking\n    the internet for the same updates on every run, and the --only option\n    allows you to execute only the section you are working on.\n    '''\n    mkcfgdir()\n    setallowpull(not nopull)\n\n    cfg = RepoListConfig()\n    if len(identifiers):\n        updatedict = {}\n        for identifier in identifiers:\n            repo = cfg.find_by_any(identifier, \"ilc\")\n            if repo is None:\n                hint = \"Try running %s add /path/to/this/repo first\" % CMD\n                raise Fatal(\"Unrecognised repo %s (%s)\" % (identifier, hint))\n            updatedict[repo.repoid] = repo\n        updatelist = updatedict.values()\n        cleanup = len(updatelist) == cfg.repo_count()\n    else:\n        updatelist = list(cfg.find_all())\n        cleanup = True\n    success = run_update(updatelist,\n                         pullfirst=not nopull,\n                         only=only,\n                         cancleanup=cleanup)\n    if not success:\n        sys.exit(1)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninstall packages from pip.", "response": "def pipinstall(packagename, pips=None, trypips=[], scripts=None):\n    \"\"\"\n    Install packages from pip.\n\n    The primary advantage of using this function is that homely can\n    automatically remove the package for you when you no longer want it.\n\n    package:\n      The name of the pip package to install\n\n    pips:\n      A list of `pip` executables to install the package with.\n\n      `['pip2.7', 'pip3.4']` would install the package using both the `pip2.7`\n      and `pip3.4` executables. The default is to use `['pip']` as long as you\n      aren't using `trypips`.\n\n    trypips:\n      This is a supplementary list of `pip` executables that homely will use to\n      install the package, but no exception will be raised if the `pip`\n      executables aren't available.\n\n    Note that the `pip install ...` commands are run with the `--user` option\n    so that the packages are installed into your home directory.\n    \"\"\"\n    # `scripts` is an alternate location for bin scripts. Useful for bad\n    # platforms that put pip2/pip3 scripts in the same bin dir such that they\n    # clobber each other.\n    # FIXME: `scripts` still has the following issues\n    # - useless if you're specifying multiple pips at once\n    # - won't do the uninstall/reinstall dance to reinstall something that was\n    #   installed with a different `scripts` path\n    if scripts is None:\n        scripts = {}\n    if pips is None:\n        pips = [] if len(trypips) else ['pip']\n    engine = getengine()\n    for pip in pips:\n        helper = PIPInstall(packagename,\n                            pip,\n                            mustinstall=True,\n                            scripts=scripts.get(pip))\n        engine.run(helper)\n    for pip in trypips:\n        helper = PIPInstall(packagename,\n                            pip,\n                            mustinstall=False,\n                            scripts=scripts.get(pip))\n        engine.run(helper)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn True if the nominated pip executable is >= 9. 0. 0", "response": "def _needs_format(pipcmd):\n    \"\"\"\n    pip >= 9.0.0 needs a --format=freeze argument to avoid a DEPRECATION\n    warning. This function returns True if the nominated pip executable\n    is >= 9.0.0\n    \"\"\"\n    try:\n        return _needs_format_cache[pipcmd]\n    except KeyError:\n        pass\n\n    # grab the version number\n    output = run([pipcmd, '--version'], stdout=True)[1].decode('utf-8')\n    m = re.match(r'^pip (\\S+) from ', output)\n    needs_format = StrictVersion(m.group(1)) >= '9.0.0'\n    _needs_format_cache[pipcmd] = needs_format\n    return needs_format"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run(cmd, stdout=None, stderr=None, **kwargs):\n    devnull = None\n    try:\n        stdoutfilter = None\n        stderrfilter = None\n\n        wantstdout = False\n        wantstderr = False\n        if stdout is False:\n            devnull = open('/dev/null', 'w')\n            stdout = devnull\n        elif stdout is True:\n            stdout = subprocess.PIPE\n            wantstdout = True\n        elif callable(stdout):\n            stdoutfilter = partial(stdout)\n            stdout = subprocess.PIPE\n        else:\n            assert stdout is None, \"Invalid stdout %r\" % stdout\n\n        if stderr is False:\n            if devnull is None:\n                devnull = open('/dev/null', 'w')\n            stderr = devnull\n        elif stderr is True:\n            stderr = subprocess.PIPE\n            wantstderr = True\n        elif stderr == \"STDOUT\":\n            stderr = subprocess.STDOUT\n        elif callable(stderr):\n            stderrfilter = partial(stderr)\n            stderr = subprocess.PIPE\n        else:\n            assert stderr is None, \"Invalid stderr %r\" % stderr\n\n        if (stdoutfilter or stderrfilter) and asyncio:\n            # run background process asynchronously and filter output as\n            # it is running\n            exitcode, out, err, = _runasync(stdoutfilter,\n                                            stderrfilter,\n                                            cmd,\n                                            stdout=stdout,\n                                            stderr=stderr,\n                                            **kwargs)\n            if not wantstdout:\n                out = None\n            if not wantstderr:\n                err = None\n            return exitcode, out, err\n\n        proc = subprocess.Popen(cmd, stdout=stdout, stderr=stderr, **kwargs)\n        out, err = proc.communicate()\n        if not wantstdout:\n            if stdoutfilter:\n                stdoutfilter(out, True)\n            out = None\n        if not wantstderr:\n            if stderrfilter:\n                stderrfilter(err, True)\n            err = None\n        return proc.returncode, out, err\n    finally:\n        if devnull is not None:\n            devnull.close()", "response": "A blocking wrapper around subprocess. Popen and returns a tuple of exitcode stdout and stderr."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn True if the parent and child path are not required to be a valid filesystem entry.", "response": "def isnecessarypath(parent, child):\n    \"\"\"\n    returns True if the file, directory or symlink <parent> is required to\n    exist in order for the path <child> to refer to a valid filesystem entry.\n\n    Examples:\n\n    If <parent> refers to a file, and <child> refers to the same file, then\n    <parent> must exist in order for <child> to be valid.\n\n    If <parent> refers to a directory, and <child> refers to the same directory\n    or anything under that directory, then <parent> must exist in order for\n    <child> to be valid.\n\n    If <parent> is a symlink to a directory, and <child> refers to something in\n    that directory *and contains the symlink <parent> in its path*, then\n    <parent> must continue to exist in order for <child> to be valid.\n    \"\"\"\n    assert parent.startswith('/')\n    assert child.startswith('/')\n    # resolve all symlinks in the parent (except for the final part itself)\n    head, tail = os.path.split(parent)\n    # expand the head part out to its real path\n    head = os.path.realpath(head)\n    fullparent = os.path.realpath(parent)\n    assert len(tail), \"Can't use isancestor() on path ending in /: %s\" % parent\n    prefix = '/'\n    parts = child.split('/')\n    while len(parts):\n        prefix = os.path.realpath(join(prefix, parts.pop(0)))\n        common = os.path.commonprefix([prefix, head])\n\n        # if at any time we stumble upon the parent as we are reconstructing\n        # the path, then we are dependent on the parent\n        if prefix == fullparent and len(parts):\n            return True\n\n        # if they refer to the same thing up to this point, check to see if the\n        # next parts are also the same\n        if len(common) == len(head):\n            # if the next item of child's path is the tail of parent, then they\n            # must refer to the same thing\n            if len(parts) and tail == parts[0]:\n                return True\n\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getstatus():\n    if exists(RUNFILE):\n        mtime = os.stat(RUNFILE).st_mtime\n        with open(SECTIONFILE) as f:\n            section = f.read().strip()\n        # what section?\n        return UpdateStatus.RUNNING, mtime, section\n    if exists(PAUSEFILE):\n        return UpdateStatus.PAUSED, None, None\n\n    mtime = None\n    if exists(TIMEFILE):\n        mtime = os.stat(TIMEFILE).st_mtime\n\n    if exists(FAILFILE):\n        if not mtime:\n            mtime = os.stat(FAILFILE).st_mtime\n        # TODO: return a different error code when the error was inability to\n        # contact one or more remote servers\n        with open(FAILFILE) as f:\n            content = f.read().strip()\n            if content == UpdateStatus.NOCONN:\n                return UpdateStatus.NOCONN, mtime, None\n            elif content == UpdateStatus.DIRTY:\n                return UpdateStatus.DIRTY, mtime, None\n        return UpdateStatus.FAILED, mtime, None\n\n    if mtime is None:\n        return UpdateStatus.NEVER, None, None\n\n    return UpdateStatus.OK, mtime, None", "response": "Get the status of the current homely update or any homely update."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef find_by_id(self, repoid):\n        for row in self.jsondata:\n            if repoid == row[\"repoid\"]:\n                return self._infofromdict(row)", "response": "Returns the repo with the specified repoid"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the repo with the specified local path.", "response": "def find_by_localpath(self, path):\n        \"\"\"\n        Returns the repo with the specified local <path>\n        \"\"\"\n        # note that the paths in self.jsondata were already _homepath2real()'d\n        # in the class' __init__()\n        resolved = _homepath2real(path)\n        for row in self.jsondata:\n            if resolved == os.path.realpath(row[\"localpath\"]):\n                return self._infofromdict(row)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef find_by_any(self, identifier, how):\n        if \"i\" in how:\n            match = self.find_by_id(identifier)\n            if match:\n                return match\n        if \"l\" in how:\n            match = self.find_by_localpath(identifier)\n            if match:\n                return match\n        if \"c\" in how:\n            match = self.find_by_canonical(identifier)\n            if match:\n                return match", "response": "find a single entry in the cache by any of the characters in how"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef isOriginalLocation(attr):\n    sourceModule = inspect.getmodule(attr.load())\n    if sourceModule is None:\n        return False\n\n    currentModule = attr\n    while not isinstance(currentModule, PythonModule):\n        currentModule = currentModule.onObject\n\n    return currentModule.name == sourceModule.__name__", "response": "Returns True if the attribute s load location is the module where that class was passed in."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef wrapFQPN(fqpn):\n    # largely cribbed from t.p.reflect.namedAny\n\n    if not fqpn:\n        raise InvalidFQPN(\"FQPN was empty\")\n\n    components = collections.deque(fqpn.split('.'))\n\n    if '' in components:\n        raise InvalidFQPN(\n            \"name must be a string giving a '.'-separated list of Python \"\n            \"identifiers, not %r\" % (fqpn,))\n\n    component = components.popleft()\n    try:\n        module = getModule(component)\n    except KeyError:\n        raise NoModule(component)\n\n    # find the bottom-most module\n    while components:\n        component = components.popleft()\n        try:\n            module = module[component]\n        except KeyError:\n            components.appendleft(component)\n            break\n        else:\n            module.load()\n    else:\n        return module\n\n    # find the bottom-most attribute\n    attribute = module\n    for component in components:\n        try:\n            attribute = next(child for child in attribute.iterAttributes()\n                             if child.name.rsplit('.', 1)[-1] == component)\n        except StopIteration:\n            raise NoObject('{}.{}'.format(attribute.name, component))\n\n    return attribute", "response": "Given an FQPN retrieve the object via the global Python module\n    namespace and wrap it with a L{PythonAttribute. a\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the initial state of the archive.", "response": "def initialState(self, state):\n        \"\"\"\n        Set this automaton's initial state.  Raises a ValueError if\n        this automaton already has an initial state.\n        \"\"\"\n\n        if self._initialState is not _NO_STATE:\n            raise ValueError(\n                \"initial state already set to {}\".format(self._initialState))\n\n        self._initialState = state"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds the given transition to the outputSymbol. Raise ValueError if there is already a transition with the given inState and inputSymbol.", "response": "def addTransition(self, inState, inputSymbol, outState, outputSymbols):\n        \"\"\"\n        Add the given transition to the outputSymbol. Raise ValueError if\n        there is already a transition with the same inState and inputSymbol.\n        \"\"\"\n        # keeping self._transitions in a flat list makes addTransition\n        # O(n^2), but state machines don't tend to have hundreds of\n        # transitions.\n        for (anInState, anInputSymbol, anOutState, _) in self._transitions:\n            if (anInState == inState and anInputSymbol == inputSymbol):\n                raise ValueError(\n                    \"already have transition from {} via {}\".format(inState, inputSymbol))\n        self._transitions.add(\n            (inState, inputSymbol, outState, tuple(outputSymbols))\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef outputAlphabet(self):\n        return set(\n            chain.from_iterable(\n                outputSymbols for\n                (inState, inputSymbol, outState, outputSymbols)\n                in self._transitions\n            )\n        )", "response": "Returns the set of symbols which can be produced by this automaton."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef states(self):\n        return frozenset(\n            chain.from_iterable(\n                (inState, outState)\n                for\n                (inState, inputSymbol, outState, outputSymbol)\n                in self._transitions\n            )\n        )", "response": "Returns a set of all valid states in the state machine."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef outputForInput(self, inState, inputSymbol):\n        for (anInState, anInputSymbol,\n             outState, outputSymbols) in self._transitions:\n            if (inState, inputSymbol) == (anInState, anInputSymbol):\n                return (outState, list(outputSymbols))\n        raise NoTransition(state=inState, symbol=inputSymbol)", "response": "Returns the output for the input symbol."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef transition(self, inputSymbol):\n        outState, outputSymbols = self._automaton.outputForInput(self._state,\n                                                                 inputSymbol)\n        outTracer = None\n        if self._tracer:\n            outTracer = self._tracer(self._state._name(),\n                                     inputSymbol._name(),\n                                     outState._name())\n        self._state = outState\n        return (outputSymbols, outTracer)", "response": "Transition between states returning any outputs."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nnormalize inspect. ArgSpec across python versions and convert mutable attributes to immutable types.", "response": "def _getArgSpec(func):\n    \"\"\"\n    Normalize inspect.ArgSpec across python versions\n    and convert mutable attributes to immutable types.\n\n    :param Callable func: A function.\n    :return: The function's ArgSpec.\n    :rtype: ArgSpec\n    \"\"\"\n    spec = getArgsSpec(func)\n    return ArgSpec(\n        args=tuple(spec.args),\n        varargs=spec.varargs,\n        varkw=spec.varkw if six.PY3 else spec.keywords,\n        defaults=spec.defaults if spec.defaults else (),\n        kwonlyargs=tuple(spec.kwonlyargs) if six.PY3 else (),\n        kwonlydefaults=(\n            tuple(spec.kwonlydefaults.items())\n            if spec.kwonlydefaults else ()\n        ) if six.PY3 else (),\n        annotations=tuple(spec.annotations.items()) if six.PY3 else (),\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _getArgNames(spec):\n    return set(\n        spec.args\n        + spec.kwonlyargs\n        + (('*args',) if spec.varargs else ())\n        + (('**kwargs',) if spec.varkw else ())\n        + spec.annotations\n    )", "response": "Get the names of all arguments defined in a function signature."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndecorate a function so all its arguments must be passed by keyword. A useful utility for decorators that take arguments so that they don't accidentally get passed the thing they're decorating as their first argument. Only works for methods right now.", "response": "def _keywords_only(f):\n    \"\"\"\n    Decorate a function so all its arguments must be passed by keyword.\n\n    A useful utility for decorators that take arguments so that they don't\n    accidentally get passed the thing they're decorating as their first\n    argument.\n\n    Only works for methods right now.\n    \"\"\"\n    @wraps(f)\n    def g(self, **kw):\n        return f(self, **kw)\n    return g"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a transitioner from an instance of the class", "response": "def _transitionerFromInstance(oself, symbol, automaton):\n    \"\"\"\n    Get a L{Transitioner}\n    \"\"\"\n    transitioner = getattr(oself, symbol, None)\n    if transitioner is None:\n        transitioner = Transitioner(\n            automaton,\n            automaton.initialState,\n        )\n        setattr(oself, symbol, transitioner)\n    return transitioner"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfiltering out arguments that are not passed to the output method.", "response": "def _filterArgs(args, kwargs, inputSpec, outputSpec):\n    \"\"\"\n    Filter out arguments that were passed to input that output won't accept.\n\n    :param tuple args: The *args that input received.\n    :param dict kwargs: The **kwargs that input received.\n    :param ArgSpec inputSpec: The input's arg spec.\n    :param ArgSpec outputSpec: The output's arg spec.\n    :return: The args and kwargs that output will accept.\n    :rtype: Tuple[tuple, dict]\n    \"\"\"\n    named_args = tuple(zip(inputSpec.args[1:], args))\n    if outputSpec.varargs:\n        # Only return all args if the output accepts *args.\n        return_args = args\n    else:\n        # Filter out arguments that don't appear\n        # in the output's method signature.\n        return_args = [v for n, v in named_args if n in outputSpec.args]\n\n    # Get any of input's default arguments that were not passed.\n    passed_arg_names = tuple(kwargs)\n    for name, value in named_args:\n        passed_arg_names += (name, value)\n    defaults = zip(inputSpec.args[::-1], inputSpec.defaults[::-1])\n    full_kwargs = {n: v for n, v in defaults if n not in passed_arg_names}\n    full_kwargs.update(kwargs)\n\n    if outputSpec.varkw:\n        # Only pass all kwargs if the output method accepts **kwargs.\n        return_kwargs = full_kwargs\n    else:\n        # Filter out names that the output method does not accept.\n        all_accepted_names = outputSpec.args[1:] + outputSpec.kwonlyargs\n        return_kwargs = {n: v for n, v in full_kwargs.items()\n                         if n in all_accepted_names}\n\n    return return_args, return_kwargs"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef state(self, initial=False, terminal=False,\n              serialized=None):\n        \"\"\"\n        Declare a state, possibly an initial state or a terminal state.\n\n        This is a decorator for methods, but it will modify the method so as\n        not to be callable any more.\n\n        :param bool initial: is this state the initial state?\n            Only one state on this :class:`automat.MethodicalMachine`\n            may be an initial state; more than one is an error.\n\n        :param bool terminal: Is this state a terminal state?\n            i.e. a state that the machine can end up in?\n            (This is purely informational at this point.)\n\n        :param Hashable serialized: a serializable value\n            to be used to represent this state to external systems.\n            This value should be hashable;\n            :py:func:`unicode` is a good type to use.\n        \"\"\"\n        def decorator(stateMethod):\n            state = MethodicalState(machine=self,\n                                    method=stateMethod,\n                                    serialized=serialized)\n            if initial:\n                self._automaton.initialState = state\n            return state\n        return decorator", "response": "Declare a state for this object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd one transition to the automaton.", "response": "def _oneTransition(self, startState, inputToken, endState, outputTokens,\n                       collector):\n        \"\"\"\n        See L{MethodicalState.upon}.\n        \"\"\"\n        # FIXME: tests for all of this (some of it is wrong)\n        # if not isinstance(startState, MethodicalState):\n        #     raise NotImplementedError(\"start state {} isn't a state\"\n        #                               .format(startState))\n        # if not isinstance(inputToken, MethodicalInput):\n        #     raise NotImplementedError(\"start state {} isn't an input\"\n        #                               .format(inputToken))\n        # if not isinstance(endState, MethodicalState):\n        #     raise NotImplementedError(\"end state {} isn't a state\"\n        #                               .format(startState))\n        # for output in outputTokens:\n        #     if not isinstance(endState, MethodicalState):\n        #         raise NotImplementedError(\"output state {} isn't a state\"\n        #                                   .format(endState))\n        self._automaton.addTransition(startState, inputToken, endState,\n                                      tuple(outputTokens))\n        inputToken.collectors[startState] = collector"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a Digraph representation of the current state of the object.", "response": "def asDigraph(self):\n        \"\"\"\n        Generate a L{graphviz.Digraph} that represents this machine's\n        states and transitions.\n\n        @return: L{graphviz.Digraph} object; for more information, please\n            see the documentation for\n            U{graphviz<https://graphviz.readthedocs.io/>}\n\n        \"\"\"\n        from ._visualize import makeDigraph\n        return makeDigraph(\n            self._automaton,\n            stateAsString=lambda state: state.method.__name__,\n            inputAsString=lambda input: input.method.__name__,\n            outputAsString=lambda output: output.method.__name__,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef elementMaker(name, *children, **attrs):\n    formattedAttrs = ' '.join('{}={}'.format(key, _gvquote(str(value)))\n                              for key, value in sorted(attrs.items()))\n    formattedChildren = ''.join(children)\n    return u'<{name} {attrs}>{children}</{name}>'.format(\n        name=name,\n        attrs=formattedAttrs,\n        children=formattedChildren)", "response": "Construct a string from the HTML element description."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconstructing an HTML table to label a state transition.", "response": "def tableMaker(inputLabel, outputLabels, port, _E=elementMaker):\n    \"\"\"\n    Construct an HTML table to label a state transition.\n    \"\"\"\n    colspan = {}\n    if outputLabels:\n        colspan['colspan'] = str(len(outputLabels))\n\n    inputLabelCell = _E(\"td\",\n                        _E(\"font\",\n                           inputLabel,\n                           face=\"menlo-italic\"),\n                        color=\"purple\",\n                        port=port,\n                        **colspan)\n\n    pointSize = {\"point-size\": \"9\"}\n    outputLabelCells = [_E(\"td\",\n                           _E(\"font\",\n                              outputLabel,\n                              **pointSize),\n                           color=\"pink\")\n                        for outputLabel in outputLabels]\n\n    rows = [_E(\"tr\", inputLabelCell)]\n\n    if outputLabels:\n        rows.append(_E(\"tr\", *outputLabelCells))\n\n    return _E(\"table\", *rows)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nproduces a Digraph object from an automaton.", "response": "def makeDigraph(automaton, inputAsString=repr,\n                outputAsString=repr,\n                stateAsString=repr):\n    \"\"\"\n    Produce a L{graphviz.Digraph} object from an automaton.\n    \"\"\"\n    digraph = graphviz.Digraph(graph_attr={'pack': 'true',\n                                           'dpi': '100'},\n                               node_attr={'fontname': 'Menlo'},\n                               edge_attr={'fontname': 'Menlo'})\n\n    for state in automaton.states():\n        if state is automaton.initialState:\n            stateShape = \"bold\"\n            fontName = \"Menlo-Bold\"\n        else:\n            stateShape = \"\"\n            fontName = \"Menlo\"\n        digraph.node(stateAsString(state),\n                     fontame=fontName,\n                     shape=\"ellipse\",\n                     style=stateShape,\n                     color=\"blue\")\n    for n, eachTransition in enumerate(automaton.allTransitions()):\n        inState, inputSymbol, outState, outputSymbols = eachTransition\n        thisTransition = \"t{}\".format(n)\n        inputLabel = inputAsString(inputSymbol)\n\n        port = \"tableport\"\n        table = tableMaker(inputLabel, [outputAsString(outputSymbol)\n                                        for outputSymbol in outputSymbols],\n                           port=port)\n\n        digraph.node(thisTransition,\n                     label=_gvhtml(table), margin=\"0.2\", shape=\"none\")\n\n        digraph.edge(stateAsString(inState),\n                     '{}:{}:w'.format(thisTransition, port),\n                     arrowhead=\"none\")\n        digraph.edge('{}:{}:e'.format(thisTransition, port),\n                     stateAsString(outState))\n\n    return digraph"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef preserveName(f):\n    def decorator(decorated):\n        return copyfunction(decorated,\n                            dict(name=f.__name__), dict(name=f.__name__))\n    return decorator", "response": "A decorator that preserve the name of the given function on the decorated function."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn an SSL context based on the certificate information in the Kafka config vars.", "response": "def get_kafka_ssl_context():\n    \"\"\"\n    Returns an SSL context based on the certificate information in the Kafka config vars.\n    \"\"\"\n    # NOTE: We assume that Kafka environment variables are present. If using\n    # Apache Kafka on Heroku, they will be available in your app configuration.\n    #\n    # 1. Write the PEM certificates necessary for connecting to the Kafka brokers to physical\n    # files.  The broker connection SSL certs are passed in environment/config variables and\n    # the python and ssl libraries require them in physical files.  The public keys are written\n    # to short lived NamedTemporaryFile files; the client key is encrypted before writing to\n    # the short lived NamedTemporaryFile\n    #\n    # 2. Create and return an SSLContext for connecting to the Kafka brokers referencing the\n    # PEM certificates written above\n    #\n\n    # stash the kafka certs in named temporary files for loading into SSLContext.  Initialize the\n    # SSLContext inside the with so when it goes out of scope the files are removed which has them\n    # existing for the shortest amount of time.  As extra caution password\n    # protect/encrypt the client key\n    with NamedTemporaryFile(suffix='.crt') as cert_file, \\\n         NamedTemporaryFile(suffix='.key') as key_file, \\\n         NamedTemporaryFile(suffix='.crt') as trust_file:\n        cert_file.write(os.environ['KAFKA_CLIENT_CERT'].encode('utf-8'))\n        cert_file.flush()\n\n        # setup cryptography to password encrypt/protect the client key so it's not in the clear on\n        # the filesystem.  Use the generated password in the call to load_cert_chain\n        passwd = standard_b64encode(os.urandom(33))\n        private_key = serialization.load_pem_private_key(\n            os.environ['KAFKA_CLIENT_CERT_KEY'].encode('utf-8'),\n            password=None,\n            backend=default_backend()\n        )\n        pem = private_key.private_bytes(\n            encoding=serialization.Encoding.PEM,\n            format=serialization.PrivateFormat.PKCS8,\n            encryption_algorithm=serialization.BestAvailableEncryption(passwd)\n        )\n        key_file.write(pem)\n        key_file.flush()\n\n        trust_file.write(os.environ['KAFKA_TRUSTED_CERT'].encode('utf-8'))\n        trust_file.flush()\n\n        # create an SSLContext for passing into the kafka provider using the create_default_context\n        # function which creates an SSLContext with protocol set to PROTOCOL_SSLv23, OP_NO_SSLv2,\n        # and OP_NO_SSLv3 when purpose=SERVER_AUTH.\n        ssl_context = ssl.create_default_context(\n            purpose=ssl.Purpose.SERVER_AUTH, cafile=trust_file.name)\n        ssl_context.load_cert_chain(cert_file.name, keyfile=key_file.name, password=passwd)\n\n        # Intentionally disabling hostname checking.  The Kafka cluster runs in the cloud and Apache\n        # Kafka on Heroku doesn't currently provide stable hostnames.  We're pinned to a specific certificate\n        # for this connection even though the certificate doesn't include host information.  We rely\n        # on the ca trust_cert for this purpose.\n        ssl_context.check_hostname = False\n\n    return ssl_context"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_kafka_brokers():\n    # NOTE: The Kafka environment variables need to be present. If using\n    # Apache Kafka on Heroku, they will be available in your app configuration.\n    if not os.environ.get('KAFKA_URL'):\n        raise RuntimeError('The KAFKA_URL config variable is not set.')\n\n    return ['{}:{}'.format(parsedUrl.hostname, parsedUrl.port) for parsedUrl in\n            [urlparse(url) for url in os.environ.get('KAFKA_URL').split(',')]]", "response": "Parses the KAKFA_URL and returns a list of hostname and port pairs in the format\n    that kafka - python expects."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a KafkaProducer that uses the SSLContext created with create_ssl_context.", "response": "def get_kafka_producer(acks='all',\n                       value_serializer=lambda v: json.dumps(v).encode('utf-8')):\n    \"\"\"\n    Return a KafkaProducer that uses the SSLContext created with create_ssl_context.\n    \"\"\"\n\n    producer = KafkaProducer(\n        bootstrap_servers=get_kafka_brokers(),\n        security_protocol='SSL',\n        ssl_context=get_kafka_ssl_context(),\n        value_serializer=value_serializer,\n        acks=acks\n    )\n\n    return producer"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a KafkaConsumer that uses the SSLContext created with create_ssl_context.", "response": "def get_kafka_consumer(topic=None,\n                       value_deserializer=lambda v: json.loads(v.decode('utf-8'))):\n    \"\"\"\n    Return a KafkaConsumer that uses the SSLContext created with create_ssl_context.\n    \"\"\"\n\n    # Create the KafkaConsumer connected to the specified brokers. Use the\n    # SSLContext that is created with create_ssl_context.\n    consumer = KafkaConsumer(\n        topic,\n        bootstrap_servers=get_kafka_brokers(),\n        security_protocol='SSL',\n        ssl_context=get_kafka_ssl_context(),\n        value_deserializer=value_deserializer\n    )\n\n    return consumer"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngive a jobscript instance return a list of lines that encode the resource requirements to be added at the top of the list of resource headers that are added at the top of the list of resource headers.", "response": "def resource_headers(self, jobscript):\n        \"\"\"Given a :class:`~clusterjob.JobScript` instance, return a list of\n        lines that encode the resource requirements, to be added at the top of\n        the rendered job script\n        \"\"\"\n        lines = []\n        for (key, val) in jobscript.resources.items():\n            if key in self.resource_replacements:\n                pbs_key = self.resource_replacements[key]\n                if key == 'mem':\n                    val = str(val) + \"m\"\n            else:\n                pbs_key = key\n            if key in ['nodes', 'threads', 'ppn']:\n                raise ResourcesNotSupportedError(\"The SGE scheduling system \"\n                        \"uses 'parallel environments' to request resources \"\n                        \"for parallelization. SgeBackend should be subclassed \"\n                        \"for a specific cluster configuration in order to \"\n                        \"encode 'nodes', 'threads', and 'ppn'.\")\n            if key in ['-cwd', 'cwd']:\n                continue\n            if val is None:\n                continue\n            if type(val) is bool:\n                if val:\n                    if not pbs_key.startswith('-'):\n                        pbs_key = '-' + pbs_key\n                    lines.append(\"%s %s\" % (self.prefix, pbs_key))\n            else:\n                if not pbs_key.startswith('-'):\n                    pbs_key = '-l %s=' % pbs_key\n                if pbs_key.endswith('='):\n                    lines.append('%s %s%s' % (self.prefix, pbs_key, str(val)))\n                else:\n                    lines.append('%s %s %s' % (self.prefix, pbs_key, str(val)))\n        lines.append(\"%s -cwd\" % self.prefix)\n        return lines"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngive a multiline string that is the body of the job script replace the environment variables with backend - specific values and return the modified body.", "response": "def replace_body_vars(self, body):\n        \"\"\"Given a multiline string that is the body of the job script, replace\n        the placeholders for environment variables with backend-specific\n        realizations, and return the modified body. See the `job_vars`\n        attribute for the mappings that are performed.\n        \"\"\"\n        for key, val in self.job_vars.items():\n            body = body.replace(key, val)\n        return body"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_job_id(self, response):\n        match = re.search('Job <([^>]+)> is submitted', response)\n        if match:\n            return match.group(1)\n        else:\n            return None", "response": "Given the stdout from the command returned by cmd_submit return a job ID"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngiving the stdout from the command returned by cmd_status return one of the status codes defined in clusterjob. status", "response": "def get_status(self, response, finished=False):\n        \"\"\"Given the stdout from the command returned by :meth:`cmd_status`,\n        return one of the status code defined in :mod:`clusterjob.status`\"\"\"\n        status_pos = 0\n        for line in response.split(\"\\n\"):\n            if line.startswith('JOBID'):\n                try:\n                    status_pos = line.find('STAT')\n                except ValueError:\n                    return None\n            else:\n                status = line[status_pos:].split()[0]\n                if status in self.status_mapping:\n                    return self.status_mapping[status]\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of lines that encode the resource requirements for the job script.", "response": "def resource_headers(self, jobscript):\n        \"\"\"Given a :class:`~clusterjob.JobScript` instance, return a list of\n        lines that encode the resource requirements, to be added at the top of\n        the rendered job script\n        \"\"\"\n        resources = jobscript.resources\n        lines = []\n        cores_per_node = 1\n        nodes = 1\n        if 'ppn' in resources:\n            cores_per_node *= resources['ppn']\n        if 'threads' in resources:\n            cores_per_node *= resources['threads']\n        if 'nodes' in resources:\n            nodes = resources['nodes']\n        if len(set(['ppn', 'threads', 'nodes']).intersection(resources)) > 0:\n            line = '%s -n %d' % (self.prefix, nodes*cores_per_node)\n            if cores_per_node > 1:\n                line += ' -R \"span[ptiles=%d]\"' % cores_per_node\n            lines.append(line)\n        for (key, val) in resources.items():\n            if key in ['nodes', 'ppn', 'threads']:\n                continue\n            if key in self.resource_replacements:\n                lsf_key = self.resource_replacements[key]\n                if key == 'time':\n                    val = time_to_minutes(val)\n            else:\n                lsf_key = key\n            if val is None:\n                continue\n            if type(val) is bool:\n                if val:\n                    lines.append(\"%s %s\" % (self.prefix, lsf_key))\n            else:\n                lines.append(\"%s %s %s\" % (self.prefix, lsf_key, str(val)))\n        return lines"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_executable(filename):\n    st = os.stat(filename)\n    os.chmod(filename, st.st_mode | stat.S_IEXEC)", "response": "Set the exectuable bit on the given filename"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef split_seq(seq, n_chunks):\n    newseq = []\n    splitsize = 1.0/n_chunks*len(seq)\n    for i in range(n_chunks):\n        newseq.append(seq[int(round(i*splitsize)):int(round((i+1)*splitsize))])\n    return newseq", "response": "Splits the given sequence into n_chunks."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef upload_file(localfile, remote, remotefile, scp='scp'):\n    sp.check_output(\n        [scp, localfile, remote+':'+remotefile],\n        stderr=sp.STDOUT)", "response": "Uploads a file to a remote host on which to put it."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwrapping the run_cmd function to return a record - replay - or record - record - sequence model.", "response": "def _wrap_run_cmd(jsonfile, mode='replay'):\n    \"\"\"Wrapper around :func:`run_cmd` for the testing using a record-replay\n    model\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    records = []\n    counter = 0\n    json_opts = {'indent': 2, 'separators':(',',': '), 'sort_keys': True}\n    def run_cmd_record(*args, **kwargs):\n        response = run_cmd(*args, **kwargs)\n        records.append({'args': args, 'kwargs': kwargs, 'response': response})\n        with open(jsonfile, 'w') as out_fh:\n            json.dump(records, out_fh, **json_opts)\n        return response\n    def run_cmd_replay(*args, **kwargs):\n        record = records.pop(0)\n        logger.debug(\"cached run_cmd, args=%s, kwargs=%s\"\n                     % (str(args), str(kwargs)) )\n        assert list(record['args']) == list(args), \\\n            \"run_cmd call #%d: Obtained args: '%s'; Expected args: '%s'\" \\\n            % (counter+1, str(args), str(record['args']))\n        assert record['kwargs'] == kwargs, \\\n            \"run_cmd call #%d: Obtained kwargs: '%s'; Expected kwargs: '%s'\" \\\n            % (counter+1, str(kwargs), str(record['kwargs']))\n        response = record['response']\n        if \"\\n\" in response:\n            if len(response.splitlines()) == 1:\n                logger.debug(\"cached response: %s\", response)\n            else:\n                logger.debug(\"cached response: ---\\n%s\\n---\", response)\n        else:\n            logger.debug(\"cached response: '%s'\", response)\n        return response\n    if mode == 'replay':\n        with open(jsonfile) as in_fh:\n            records = json.load(in_fh)\n        return run_cmd_replay\n    elif mode == 'record':\n        return run_cmd_record\n    else:\n        raise ValueError(\"Invalid mode\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting a string describing a time duration into seconds.", "response": "def time_to_seconds(time_str):\n    \"\"\"Convert a string describing a time duration into seconds. The supported\n    formats are::\n\n        minutes\n        minutes:seconds\n        hours:minutes:seconds\n        days-hours\n        days-hours:minutes\n        days-hours:minutes:seconds\n        days:hours:minutes:seconds\n\n    Raises:\n        ValueError: if `time_str` has an invalid format.\n\n    Examples:\n        >>> time_to_seconds('10')\n        600\n        >>> time_to_seconds('10:00')\n        600\n        >>> time_to_seconds('10:30')\n        630\n        >>> time_to_seconds('1:10:30')\n        4230\n        >>> time_to_seconds('1-1:10:30')\n        90630\n        >>> time_to_seconds('1-0')\n        86400\n        >>> time_to_seconds('1-10')\n        122400\n        >>> time_to_seconds('1-1:10')\n        90600\n        >>> time_to_seconds('1-1:10:30')\n        90630\n        >>> time_to_seconds('1:1:10:30')\n        90630\n        >>> time_to_seconds('1 1:10:30')\n        Traceback (most recent call last):\n        ...\n        ValueError: '1 1:10:30' has invalid pattern\n    \"\"\"\n    patterns = [\n        re.compile(r'^(?P<hours>\\d+):(?P<minutes>\\d+):(?P<seconds>\\d+)$'),\n        re.compile(r'^(?P<days>\\d+)-(?P<hours>\\d+)$'),\n        re.compile(r'^(?P<minutes>\\d+)$'),\n        re.compile(r'^(?P<minutes>\\d+):(?P<seconds>\\d+)$'),\n        re.compile(r'^(?P<days>\\d+)-(?P<hours>\\d+):(?P<minutes>\\d+)$'),\n        re.compile(\n          r'^(?P<days>\\d+)-(?P<hours>\\d+):(?P<minutes>\\d+):(?P<seconds>\\d+)$'),\n        re.compile(\n          r'^(?P<days>\\d+):(?P<hours>\\d+):(?P<minutes>\\d+):(?P<seconds>\\d+)$'),\n    ]\n    seconds = 0\n    for pattern in patterns:\n        match = pattern.match(str(time_str).strip())\n        if match:\n            if 'seconds' in match.groupdict():\n                seconds += int(match.group('seconds'))\n            if 'minutes' in match.groupdict():\n                seconds += 60*int(match.group('minutes'))\n            if 'hours' in match.groupdict():\n                seconds += 3600*int(match.group('hours'))\n            if 'days' in match.groupdict():\n                seconds += 86400*int(match.group('days'))\n            return seconds\n    raise ValueError(\"'%s' has invalid pattern\" % time_str)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_job_id(self, response):\n        lines = [line.strip() for line in response.split(\"\\n\")\n                if line.strip() != '']\n        last_line = lines[-1]\n        match = re.match('(\\d+)\\.[\\w.-]+$', last_line)\n        if match:\n            return match.group(1)\n        else:\n            return None", "response": "Given the stdout from the command returned by cmd_submit return a job ID"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_status(self, response, finished=False):\n        lines = [line.strip() for line in response.split(\"\\n\")\n                if line.strip() != '']\n        last_line = lines[-1]\n        if last_line.startswith('qstat: Unknown Job'):\n            return COMPLETED\n        else:\n            try:\n                status = lines[-1].split()[4]\n                return self.status_mapping[status]\n            except (IndexError, KeyError):\n                return None", "response": "Given the stdout from the command returned by cmd_status return one of the status codes defined in clusterjob. status or None if the status cannot be determined"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngiving a jobscript instance return a list of lines that encode the resource requirements to be added at the top of the job script", "response": "def resource_headers(self, jobscript):\n        \"\"\"Given a :class:`~clusterjob.JobScript` instance, return a list of\n        lines that encode the resource requirements, to be added at the top of\n        the rendered job script\n        \"\"\"\n        resources = jobscript.resources\n        lines = []\n        cores_per_node = 1\n        nodes = 1\n        if 'ppn' in resources:\n            cores_per_node *= resources['ppn']\n        if 'threads' in resources:\n            cores_per_node *= resources['threads']\n        if 'nodes' in resources:\n            nodes = resources['nodes']\n        if len(set(['ppn', 'threads', 'nodes']).intersection(resources)) > 0:\n            lines.append('%s -l nodes=%d:ppn=%d'\n                        % (self.prefix, nodes, cores_per_node))\n        for (key, val) in resources.items():\n            if key in ['nodes', 'threads', 'ppn']:\n                continue\n            if key in self.resource_replacements:\n                pbs_key = self.resource_replacements[key]\n                if key == 'mem':\n                    val = str(val) + \"m\"\n            else:\n                pbs_key = key\n            if val is None:\n                continue\n            if type(val) is bool:\n                if val:\n                    if not pbs_key.startswith('-'):\n                        pbs_key = '-' + pbs_key\n                    lines.append(\"%s %s\" % (self.prefix, pbs_key))\n            else:\n                if not pbs_key.startswith('-'):\n                    pbs_key = '-l %s=' % pbs_key\n                if pbs_key.endswith('='):\n                    lines.append('%s %s%s' % (self.prefix, pbs_key, str(val)))\n                else:\n                    lines.append('%s %s %s' % (self.prefix, pbs_key, str(val)))\n        return lines"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cmd_status(self, run, finished=False):\n        if finished:\n            return ['sacct', '--format=state', '-n', '-j',  str(run.job_id)]\n        else:\n            return ['squeue', '-h', '-o', '%T', '-j', str(run.job_id)]", "response": "Returns a list of command line arguments that queries the scheduler for the job status."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngive the stdout from the command returned by cmd_status return one of the status codes defined in clusterjob. status.", "response": "def get_status(self, response, finished=False):\n        \"\"\"Given the stdout from the command returned by :meth:`cmd_status`,\n        return one of the status code defined in :mod:`clusterjob.status`\"\"\"\n        for line in response.split(\"\\n\"):\n            if line.strip() in self.status_mapping:\n                return self.status_mapping[line.strip()]\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef resource_headers(self, jobscript):\n        lines = []\n        for (key, val) in jobscript.resources.items():\n            if key in self.resource_replacements:\n                slurm_key = self.resource_replacements[key]\n                val = str(val).strip()\n            else:\n                slurm_key = key\n            if not slurm_key.startswith('-'):\n                if len(slurm_key) == 1:\n                    slurm_key = '-%s' % slurm_key\n                else:\n                    slurm_key = '--%s' % slurm_key\n            if val is None:\n                continue\n            if type(val) is bool:\n                if val:\n                    lines.append(\"%s %s\" % (self.prefix, slurm_key))\n            else:\n                if slurm_key.startswith('--'):\n                    lines.append('%s %s=%s'\n                                  % (self.prefix, slurm_key, str(val)))\n                else:\n                    lines.append('%s %s %s'\n                                 % (self.prefix, slurm_key, str(val)))\n        return lines", "response": "Given a : class : ~clusterjob. JobScript instance return a list of lines that encode the resource requirements at the top of\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the job s status as one of the codes defined in the alide module.", "response": "def status(self):\n        \"\"\"Return the job status as one of the codes defined in the\n        `clusterjob.status` module.\n        finished, communicate with the cluster to determine the job's status.\n        \"\"\"\n        if self._status >= COMPLETED:\n            return self._status\n        else:\n            cmd = self.backend.cmd_status(self, finished=False)\n            response = self._run_cmd(cmd, self.remote, ignore_exit_code=True,\n                                     ssh=self.ssh)\n            status = self.backend.get_status(response, finished=False)\n            if status is None:\n                cmd = self.backend.cmd_status(self, finished=True)\n                response = self._run_cmd(cmd, self.remote,\n                                         ignore_exit_code=True, ssh=self.ssh)\n                status = self.backend.get_status(response, finished=True)\n            prev_status = self._status\n            self._status = status\n            if self._status not in STATUS_CODES:\n                raise ValueError(\"Invalid status code %s\", self._status)\n            if prev_status != self._status:\n                if self._status >= COMPLETED:\n                    self.run_epilogue()\n                self.dump()\n            return self._status"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the status of the current object.", "response": "def get(self, timeout=None):\n        \"\"\"Return status\"\"\"\n        status = self.status\n        if status >= COMPLETED:\n            return status\n        else:\n            self.wait(timeout)\n            return self.status"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwriting dump out to file cache_file defaulting to self. cache_file", "response": "def dump(self, cache_file=None):\n        \"\"\"Write dump out to file `cache_file`, defaulting to\n        ``self.cache_file``\"\"\"\n        if cache_file is None:\n            cache_file = self.cache_file\n        if cache_file is not None:\n            self.cache_file = cache_file\n            with open(cache_file, 'wb') as pickle_fh:\n                pickle.dump(\n                    (self.remote, self.backend.name, self.max_sleep_interval,\n                     self.job_id, self._status, self.epilogue, self.ssh,\n                     self.scp),\n                    pickle_fh)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwait until the result is available or until roughly timeout seconds pass.", "response": "def wait(self, timeout=None):\n        \"\"\"Wait until the result is available or until roughly timeout seconds\n        pass.\"\"\"\n        logger = logging.getLogger(__name__)\n        if int(self.max_sleep_interval) < int(self._min_sleep_interval):\n            self.max_sleep_interval = int(self._min_sleep_interval)\n        t0 = time.time()\n        sleep_seconds = min(5, self.max_sleep_interval)\n        status = self.status\n        prev_status = status\n        while status < COMPLETED:\n            logger.debug(\"sleep for %d seconds\", sleep_seconds)\n            time.sleep(sleep_seconds)\n            if 2*sleep_seconds <= self.max_sleep_interval:\n                sleep_seconds *= 2\n            if timeout is not None:\n                if int(time.time() - t0) > int(timeout):\n                    return\n            status = self.status\n            if status != prev_status:\n                sleep_seconds = min(5, self.max_sleep_interval)\n                prev_status = status"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn True if the job finished with a COMPLETED status False otherwise. Raise an AssertionError if the job has not completed.", "response": "def successful(self):\n        \"\"\"Return True if the job finished with a COMPLETED status, False if it\n        finished with a CANCELLED or FAILED status. Raise an `AssertionError`\n        if the job has not completed\"\"\"\n        status = self.status\n        assert status >= COMPLETED, \"status is %s\" % status\n        return (self.status == COMPLETED)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cancel(self):\n        if self.status > COMPLETED:\n            return\n        cmd = self.backend.cmd_cancel(self)\n        self._run_cmd(cmd, self.remote, ignore_exit_code=True, ssh=self.ssh)\n        self._status = CANCELLED\n        self.dump()", "response": "Cancel the running job."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nruns the epilogue script in the current working directory.", "response": "def run_epilogue(self):\n        \"\"\"Run the epilogue script in the current working directory.\n\n        raises:\n            subprocess.CalledProcessError: if the script does not finish with\n                exit code zero.\n        \"\"\"\n        logger = logging.getLogger(__name__)\n        if self.epilogue is not None:\n            with tempfile.NamedTemporaryFile('w', delete=False) as epilogue_fh:\n                epilogue_fh.write(self.epilogue)\n                tempfilename = epilogue_fh.name\n            set_executable(tempfilename)\n            try:\n                sp.check_output( [tempfilename, ], stderr=sp.STDOUT)\n            except sp.CalledProcessError as e:\n                logger.error(dedent(r'''\n                Epilogue script did not exit cleanly.\n                CWD: {cwd}\n                epilogue: ---\n                {epilogue}\n                ---\n                response: ---\n                {response}\n                ---\n                ''').format(cwd=os.getcwd(), epilogue=self.epilogue,\n                            response=e.output))\n                raise\n            finally:\n                os.unlink(tempfilename)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmap a string to an iterator over tokens. In other words: [char] -> [token]", "response": "def tokenize(self, string):\n        '''\n        Maps a string to an iterator over tokens. In other words: [char] -> [token]\n        '''\n\n        new_lexer = ply.lex.lex(module=self, debug=self.debug, errorlog=logger)\n        new_lexer.latest_newline = 0\n        new_lexer.string_value = None\n        new_lexer.input(string)\n\n        while True:\n            t = new_lexer.token()\n            if t is None: break\n            t.col = t.lexpos - new_lexer.latest_newline\n            yield t\n\n        if new_lexer.string_value is not None:\n            raise JsonPathLexerError('Unexpected EOF in string literal or identifier')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef t_ID(self, t):\n        r'[a-zA-Z_@][a-zA-Z0-9_@\\-]*'\n        t.type = self.reserved_words.get(t.value, 'ID')\n        return t", "response": "t_ID is a reserved word"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef t_singlequote(self, t):\n        r\"'\"\n        t.lexer.string_start = t.lexer.lexpos\n        t.lexer.string_value = ''\n        t.lexer.push_state('singlequote')", "response": "r Singlequote a tag name."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef t_singlequote_end(self, t):\n        r\"'\"\n        t.value = t.lexer.string_value\n        t.type = 'ID'\n        t.lexer.string_value = None\n        t.lexer.pop_state()\n        return t", "response": "r' end of a singlequote"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef t_doublequote(self, t):\n        r'\"'\n        t.lexer.string_start = t.lexer.lexpos\n        t.lexer.string_value = ''\n        t.lexer.push_state('doublequote')", "response": "r doublequote a tag name"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef t_doublequote_end(self, t):\n        r'\"'\n        t.value = t.lexer.string_value\n        t.type = 'ID'\n        t.lexer.string_value = None\n        t.lexer.pop_state()\n        return t", "response": "r'\"' r'}' r'}'}' r}'}' r'}' r"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef child(self, child):\n        if isinstance(self, This) or isinstance(self, Root):\n            return child\n        elif isinstance(child, This):\n            return self\n        elif isinstance(child, Root):\n            return child\n        else:\n            return Child(self, child)", "response": "Same as Child but with some canonicalization"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the pseudopath of the resource.", "response": "def id_pseudopath(self):\n        \"\"\"\n        Looks like a path, but with ids stuck in when available\n        \"\"\"\n        try:\n            pseudopath = Fields(str(self.value[auto_id_field]))\n        except (TypeError, AttributeError, KeyError): # This may not be all the interesting exceptions\n            pseudopath = self.path\n\n        if self.context:\n            return self.context.id_pseudopath.child(pseudopath)\n        else:\n            return pseudopath"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind all the records that match the given datum.", "response": "def find(self, datum):\n        \"\"\"\n        Extra special case: auto ids do not have children,\n        so cut it off right now rather than auto id the auto id\n        \"\"\"\n        \n        return [submatch\n                for subdata in self.left.find(datum)\n                if not isinstance(subdata, AutoIdForDatum)\n                for submatch in self.right.find(subdata)]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nyielding all the tokens in the given stream that are not variables.", "response": "def filter_stream(self, stream):\n        \"\"\"\n        We convert \n        {{ some.variable | filter1 | filter 2}}\n            to \n        {{ some.variable | filter1 | filter 2 | bind}}\n        \n        ... for all variable declarations in the template\n\n        This function is called by jinja2 immediately \n        after the lexing stage, but before the parser is called. \n        \"\"\"\n        while not stream.eos:\n            token = next(stream)\n            if token.test(\"variable_begin\"):\n                var_expr = []\n                while not token.test(\"variable_end\"):\n                    var_expr.append(token)\n                    token = next(stream)\n                variable_end = token\n\n                last_token = var_expr[-1]\n                if (not last_token.test(\"name\") \n                    or not last_token.value in ('bind', 'inclause', 'sqlsafe')):\n                    param_name = self.extract_param_name(var_expr)\n                    # don't bind twice\n                    var_expr.append(Token(10, 'pipe', u'|'))\n                    var_expr.append(Token(10, 'name', u'bind'))\n                    var_expr.append(Token(2, 'lparen', u'('))\n                    var_expr.append(Token(10, 'string', param_name))\n                    var_expr.append(Token(2, 'rparen', u')'))\n\n                var_expr.append(variable_end)\n                for token in var_expr:\n                    yield token\n            else:\n                yield token"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef inject(function):\n    try:\n        bindings = _infer_injected_bindings(function)\n    except _BindingNotYetAvailable:\n        bindings = 'deferred'\n    return method_wrapper(function, bindings)", "response": "Decorator that injects parameters into the base class."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef noninjectable(*args):\n\n    def decorator(function):\n        argspec = inspect.getfullargspec(inspect.unwrap(function))\n        for arg in args:\n            if arg not in argspec.args and arg not in argspec.kwonlyargs:\n                raise UnknownArgument('Unable to mark unknown argument %s ' 'as non-injectable.' % arg)\n\n        existing = getattr(function, '__noninjectables__', set())\n        merged = existing | set(args)\n        function.__noninjectables__ = merged\n        return function\n\n    return decorator", "response": "Mark some parameters as non - injectable."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef bind(self, interface, to=None, scope=None):\n        if type(interface) is type and issubclass(interface, (BaseMappingKey, BaseSequenceKey)):\n            return self.multibind(interface, to, scope=scope)\n        key = BindingKey.create(interface)\n        self._bindings[key] = self.create_binding(interface, to, scope)", "response": "Bind an interface to an implementation."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates or extends a multi - binding.", "response": "def multibind(self, interface, to, scope=None):\n        \"\"\"Creates or extends a multi-binding.\n\n        A multi-binding maps from a key to a sequence, where each element in\n        the sequence is provided separately.\n\n        :param interface: :func:`MappingKey` or :func:`SequenceKey` to bind to.\n        :param to: Instance, class to bind to, or an explicit :class:`Provider`\n                subclass. Must provide a sequence.\n        :param scope: Optional Scope in which to bind.\n        \"\"\"\n        key = BindingKey.create(interface)\n        if key not in self._bindings:\n            if isinstance(interface, dict) or isinstance(interface, type) and issubclass(interface, dict):\n                provider = MapBindProvider()\n            else:\n                provider = MultiBindProvider()\n            binding = self.create_binding(interface, provider, scope)\n            self._bindings[key] = binding\n        else:\n            binding = self._bindings[key]\n            provider = binding.provider\n            assert isinstance(provider, ListOfProviders)\n        provider.append(self.provider_for(key.interface, to))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninstalls a module into this binder.", "response": "def install(self, module):\n        \"\"\"Install a module into this binder.\n\n        In this context the module is one of the following:\n\n        * function taking the :class:`Binder` as it's only parameter\n\n          ::\n\n            def configure(binder):\n                bind(str, to='s')\n\n            binder.install(configure)\n\n        * instance of :class:`Module` (instance of it's subclass counts)\n\n          ::\n\n            class MyModule(Module):\n                def configure(self, binder):\n                    binder.bind(str, to='s')\n\n            binder.install(MyModule())\n\n        * subclass of :class:`Module` - the subclass needs to be instantiable so if it\n          expects any parameters they need to be injected\n\n          ::\n\n            binder.install(MyModule)\n        \"\"\"\n        if type(module) is type and issubclass(module, Module):\n            instance = module()\n        else:\n            instance = module\n        instance(self)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets an instance of the given interface.", "response": "def get(self, interface: Type[T], scope=None) -> T:\n        \"\"\"Get an instance of the given interface.\n\n        .. note::\n\n            Although this method is part of :class:`Injector`'s public interface\n            it's meant to be used in limited set of circumstances.\n\n            For example, to create some kind of root object (application object)\n            of your application (note that only one `get` call is needed,\n            inside the `Application` class and any of its dependencies\n            :func:`inject` can and should be used):\n\n            .. code-block:: python\n\n                class Application:\n\n                    @inject\n                    def __init__(self, dep1: Dep1, dep2: Dep2):\n                        self.dep1 = dep1\n                        self.dep2 = dep2\n\n                    def run(self):\n                        self.dep1.something()\n\n                injector = Injector(configuration)\n                application = injector.get(Application)\n                application.run()\n\n        :param interface: Interface whose implementation we want.\n        :param scope: Class of the Scope in which to resolve.\n        :returns: An implementation of interface.\n        \"\"\"\n        key = BindingKey.create(interface)\n        binding, binder = self.binder.get_binding(None, key)\n        scope = scope or binding.scope\n        if isinstance(scope, ScopeDecorator):\n            scope = scope.scope\n        # Fetch the corresponding Scope instance from the Binder.\n        scope_key = BindingKey.create(scope)\n        scope_binding, _ = binder.get_binding(None, scope_key)\n        scope_instance = scope_binding.provider.get(self)\n\n        log.debug(\n            '%sInjector.get(%r, scope=%r) using %r', self._log_prefix, interface, scope, binding.provider\n        )\n        result = scope_instance.get(key, binding.provider).get(self)\n        log.debug('%s -> %r', self._log_prefix, result)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a new instance satisfying any dependencies on cls.", "response": "def create_object(self, cls: Type[T], additional_kwargs=None) -> T:\n        \"\"\"Create a new instance, satisfying any dependencies on cls.\"\"\"\n        additional_kwargs = additional_kwargs or {}\n        log.debug('%sCreating %r object with %r', self._log_prefix, cls, additional_kwargs)\n\n        try:\n            instance = cls.__new__(cls)\n        except TypeError as e:\n            reraise(\n                e,\n                CallError(cls, getattr(cls.__new__, '__func__', cls.__new__), (), {}, e, self._stack),\n                maximum_frames=2,\n            )\n        try:\n            init = cls.__init__\n            self.call_with_injection(init, self_=instance, kwargs=additional_kwargs)\n        except TypeError as e:\n            # The reason why getattr() fallback is used here is that\n            # __init__.__func__ apparently doesn't exist for Key-type objects\n            reraise(\n                e,\n                CallError(\n                    instance,\n                    getattr(instance.__init__, '__func__', instance.__init__),\n                    (),\n                    additional_kwargs,\n                    e,\n                    self._stack,\n                ),\n            )\n        return instance"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef call_with_injection(self, callable, self_=None, args=(), kwargs={}):\n\n        def _get_callable_bindings(callable):\n            if not hasattr(callable, '__bindings__'):\n                return {}\n\n            if callable.__bindings__ == 'deferred':\n                read_and_store_bindings(callable, _infer_injected_bindings(callable))\n            return callable.__bindings__\n\n        bindings = _get_callable_bindings(callable)\n        noninjectables = getattr(callable, '__noninjectables__', set())\n        signature = inspect.signature(callable)\n        full_args = args\n        if self_ is not None:\n            full_args = (self_,) + full_args\n        bound_arguments = signature.bind_partial(*full_args)\n\n        needed = dict(\n            (k, v)\n            for (k, v) in bindings.items()\n            if k not in kwargs and k not in noninjectables and k not in bound_arguments.arguments\n        )\n\n        dependencies = self.args_to_inject(\n            function=callable,\n            bindings=needed,\n            owner_key=self_.__class__ if self_ is not None else callable.__module__,\n        )\n\n        dependencies.update(kwargs)\n\n        try:\n            return callable(*full_args, **dependencies)\n        except TypeError as e:\n            reraise(e, CallError(self_, callable, args, dependencies, e, self._stack))", "response": "Call a callable and provide it s dependencies if needed."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninject arguments into a function.", "response": "def args_to_inject(self, function, bindings, owner_key):\n        \"\"\"Inject arguments into a function.\n\n        :param function: The function.\n        :param bindings: Map of argument name to binding key to inject.\n        :param owner_key: A key uniquely identifying the *scope* of this function.\n            For a method this will be the owning class.\n        :returns: Dictionary of resolved arguments.\n        \"\"\"\n        dependencies = {}\n\n        key = (owner_key, function, tuple(sorted(bindings.items())))\n\n        def repr_key(k):\n            owner_key, function, bindings = k\n            return '%s.%s(injecting %s)' % (tuple(map(_describe, k[:2])) + (dict(k[2]),))\n\n        log.debug('%sProviding %r for %r', self._log_prefix, bindings, function)\n\n        if key in self._stack:\n            raise CircularDependency(\n                'circular dependency detected: %s -> %s'\n                % (' -> '.join(map(repr_key, self._stack)), repr_key(key))\n            )\n\n        self._stack += (key,)\n        try:\n            for arg, key in bindings.items():\n                try:\n                    instance = self.get(key.interface)\n                except UnsatisfiedRequirement as e:\n                    if not e.args[0]:\n                        e = UnsatisfiedRequirement(owner_key, e.args[1])\n                    raise e\n                dependencies[arg] = instance\n        finally:\n            self._stack = tuple(self._stack[:-1])\n\n        return dependencies"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nscrub output before saving notebooks", "response": "def scrub_output_pre_save(model, **kwargs):\n    \"\"\"scrub output before saving notebooks\"\"\"\n    # only run on notebooks\n    if model['type'] != 'notebook':\n        return\n    # only run on nbformat v4\n    if model['content']['nbformat'] != 4:\n        return\n\n    for cell in model['content']['cells']:\n        if cell['cell_type'] != 'code':\n            continue\n        cell['outputs'] = []\n        cell['execution_count'] = None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef close(self):\n        if not self.closed:\n            self._ipython.events.unregister('post_run_cell', self._fill)\n            self._box.close()\n            self.closed = True", "response": "Close and remove hooks."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfilling self with variable information.", "response": "def _fill(self):\n        \"\"\"Fill self with variable information.\"\"\"\n        types_to_exclude = ['module', 'function', 'builtin_function_or_method',\n                            'instance', '_Feature', 'type', 'ufunc']\n        values = self.namespace.who_ls()\n\n        def eval(expr):\n            return self.namespace.shell.ev(expr)\n\n        var = [(v,\n                type(eval(v)).__name__,\n                str(_getsizeof(eval(v))),\n                str(_getshapeof(eval(v))) if _getshapeof(eval(v)) else '',\n                str(eval(v))[:200])\n               for v in values if (v not in ['_html', '_nms', 'NamespaceMagics', '_Jupyter']) & (type(eval(v)).__name__ not in types_to_exclude)]\n\n        self._table.value = '<div class=\"rendered_html jp-RenderedHTMLCommon\"><table><thead><tr><th>Name</th><th>Type</th><th>Size</th><th>Shape</th><th>Value</th></tr></thead><tr><td>' + \\\n            '</td></tr><tr><td>'.join(['{0}</td><td>{1}</td><td>{2}</td><td>{3}</td><td>{4}'.format(v1, v2, v3, v4, v5) for v1, v2, v3, v4, v5 in var]) + \\\n            '</td></tr></table></div>'"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting list of cells to notebook form list should be of the form : lst", "response": "def list_to_cells(lst):\n    '''convert list of cells to notebook form\n    list should be of the form:\n    [[list of strings representing python code for cell]]\n    '''\n    cells = '\"cells\": ['\n    for cell in lst:\n        to_add = '{\"cell_type\": \"code\", \"execution_count\": null, \"metadata\": {}, \"outputs\": [], \"source\": [\"' + '\\\\n\",\"'.join(cell) + '\"]},'\n        cells += to_add\n\n    cells = cells[:-1] + '],'\n\n    nb = '{' + cells + '\"metadata\": {\"header\": \"HEADLESS\", \"kernelspec\": {\"display_name\" : \"python\", \"language\": \"\", \"name\": \"python\"}, \"language\":\"python\"'\n\n    return nbformat.writes(nbformat.reads(nb, as_version=4)).encode('utf-8')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_var(var, set_='\"\"'):\n    '''set var outside notebook'''\n    if isinstance(set_, str):\n        to_set = json.dumps(set_)\n    elif isinstance(set_, dict) or isinstance(set_, list):\n        try:\n            to_set = json.dumps(set_)\n        except ValueError:\n            raise Exception('var not jsonable')\n    else:\n        raise Exception('var must be jsonable list or dict')\n\n    os.environ['NBCONVERT_' + var] = to_set", "response": "set var outside notebook"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets var inside notebook", "response": "def get_var(var, default='\"\"'):\n    '''get var inside notebook'''\n    ret = os.environ.get('NBCONVERT_' + var)\n    if ret is None:\n        return default\n    return json.loads(ret)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef align_yaxis_np(axes):\n    axes = np.array(axes)\n    extrema = np.array([ax.get_ylim() for ax in axes])\n\n    # reset for divide by zero issues\n    for i in range(len(extrema)):\n        if np.isclose(extrema[i, 0], 0.0):\n            extrema[i, 0] = -1\n        if np.isclose(extrema[i, 1], 0.0):\n            extrema[i, 1] = 1\n\n    # upper and lower limits\n    lowers = extrema[:, 0]\n    uppers = extrema[:, 1]\n\n    # if all pos or all neg, don't scale\n    all_positive = False\n    all_negative = False\n    if lowers.min() > 0.0:\n        all_positive = True\n\n    if uppers.max() < 0.0:\n        all_negative = True\n\n    if all_negative or all_positive:\n        # don't scale\n        return\n\n    # pick \"most centered\" axis\n    res = abs(uppers+lowers)\n    min_index = np.argmin(res)\n\n    # scale positive or negative part\n    multiplier1 = abs(uppers[min_index]/lowers[min_index])\n    multiplier2 = abs(lowers[min_index]/uppers[min_index])\n\n    for i in range(len(extrema)):\n        # scale positive or negative part based on which induces valid\n        if i != min_index:\n            lower_change = extrema[i, 1] * -1*multiplier2\n            upper_change = extrema[i, 0] * -1*multiplier1\n            if upper_change < extrema[i, 1]:\n                extrema[i, 0] = lower_change\n            else:\n                extrema[i, 1] = upper_change\n\n        # bump by 10% for a margin\n        extrema[i, 0] *= 1.1\n        extrema[i, 1] *= 1.1\n\n    # set axes limits\n    [axes[i].set_ylim(*extrema[i]) for i in range(len(extrema))]", "response": "Align zeros of the two axes zooming them out by same ratio"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a dash definitio of an HTML table for a Pandas dataframe", "response": "def make_dash_table(df):\n    ''' Return a dash definitio of an HTML table for a Pandas dataframe '''\n    table = []\n    for index, row in df.iterrows():\n        html_row = []\n        for i in range(len(row)):\n            html_row.append(html.Td([row[i]]))\n        table.append(html.Tr(html_row))\n    return table"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting notebooks to Python script after save with nbconvert replaces IPython notebook - script", "response": "def script_post_save(model, os_path, contents_manager, **kwargs):\n    \"\"\"convert notebooks to Python script after save with nbconvert\n\n    replaces `ipython notebook --script`\n    \"\"\"\n    from nbconvert.exporters.script import ScriptExporter\n\n    if model['type'] != 'notebook':\n        return\n\n    global _script_exporter\n\n    if _script_exporter is None:\n        _script_exporter = ScriptExporter(parent=contents_manager)\n\n    log = contents_manager.log\n\n    base, ext = os.path.splitext(os_path)\n    # py_fname = base + '.py'\n    script, resources = _script_exporter.from_filename(os_path)\n    script_fname = base + resources.get('output_extension', '.txt')\n    log.info(\"Saving script /%s\", to_api_path(script_fname, contents_manager.root_dir))\n\n    with io.open(script_fname, 'w', encoding='utf-8') as f:\n        f.write(script)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncopies given string into system clipboard.", "response": "def copy(string, **kwargs):\n    \"\"\"Copy given string into system clipboard.\"\"\"\n    try:\n        subprocess.Popen(['pbcopy'], stdin=subprocess.PIPE).communicate(\n                string.encode(\"utf-8\"))\n    except OSError as why:\n        raise XcodeNotFound\n\n    return"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns system clipboard contents.", "response": "def paste(**kwargs):\n    \"\"\"Returns system clipboard contents.\"\"\"\n    try:\n        #Tell the IO system to decode IPC IO with utf-8,\n        #to prevent UnicodeDecodeErrors on python3\n        os.environ['LANG'] = 'en_US.utf-8'\n        return subprocess.Popen(\n            ['pbpaste'], stdout=subprocess.PIPE).communicate()[0].decode('utf-8')\n\n    except OSError as why:\n        raise XcodeNotFound"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncopying given string into system clipboard.", "response": "def copy(string, **kwargs):\n    \"\"\"Copy given string into system clipboard.\"\"\"\n\n    clip.OpenClipboard()\n    clip.EmptyClipboard()\n    clip.SetClipboardData(win32con.CF_UNICODETEXT, string) \n    clip.CloseClipboard()\n\n    return"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\npastes the contents of the system clipboard.", "response": "def paste(**kwargs):\n    \"\"\"Returns system clipboard contents.\"\"\"\n\n    clip.OpenClipboard() \n    d = clip.GetClipboardData(win32con.CF_UNICODETEXT)\n    clip.CloseClipboard() \n    return d"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef copy(string, xsel=False):\n    try:\n        _cmd = [\"xclip\", \"-selection\", \"clipboard\"]\n        subprocess.Popen(_cmd, stdin=subprocess.PIPE).communicate(\n                string.encode('utf-8'))\n        if xsel:\n            _cmd = [\"xclip\", \"-selection\", \"primary\"]\n            subprocess.Popen(_cmd, stdin=subprocess.PIPE).communicate(\n                    string.encode('utf-8'))\n    except OSError as why:\n        raise XclipNotFound", "response": "Copy given string into system clipboard."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef paste(xsel=False):\n    selection = \"primary\" if xsel else \"clipboard\"\n    try:\n        return subprocess.Popen([\"xclip\", \"-selection\", selection, \"-o\"], stdout=subprocess.PIPE).communicate()[0].decode(\"utf-8\")\n    except OSError as why:\n        raise XclipNotFound", "response": "Returns system clipboard contents."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef main():\n    if sys.argv[1:]:  # called with input arguments\n        copy(' '.join(sys.argv[1:]))\n    elif not sys.stdin.isatty():  # piped in input\n        copy(''.join(sys.stdin.readlines()).rstrip('\\n'))\n    else:  # paste output\n        print(paste())", "response": "Entry point for the\n    command line."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncopying given string into system clipboard.", "response": "def copy(string, **kwargs):\n    \"\"\"Copy given string into system clipboard.\"\"\"\n    window = Tk()\n    window.withdraw()\n    window.clipboard_clear()\n    window.clipboard_append(string)\n    window.destroy()\n    return"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns system clipboard contents.", "response": "def paste(**kwargs):\n    \"\"\"Returns system clipboard contents.\"\"\"\n    window = Tk()\n    window.withdraw()\n    d = window.selection_get(selection = 'CLIPBOARD')\n    return d"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn dict object with model s data.", "response": "def to_dict(self, nested=False):\n        \"\"\"Return dict object with model's data.\n\n        :param nested: flag to return nested relationships' data if true\n        :type: bool\n        :return: dict\n        \"\"\"\n        result = dict()\n        for key in self.columns:\n            result[key] = getattr(self, key)\n\n        if nested:\n            for key in self.relations:\n                obj = getattr(self, key)\n\n                if isinstance(obj, SerializeMixin):\n                    result[key] = obj.to_dict()\n                elif isinstance(obj, Iterable):\n                    result[key] = [o.to_dict() for o in obj]\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _flatten_schema(schema):\r\n    def _flatten(schema, parent_path, result):\r\n        \"\"\"\r\n        :type schema: dict\r\n        \"\"\"\r\n        for path, value in schema.items():\r\n            # for supporting schemas like Product.user: {...},\r\n            # we transform, say, Product.user to 'user' string\r\n            if isinstance(path, InstrumentedAttribute):\r\n                path = path.key\r\n\r\n            if isinstance(value, tuple):\r\n                join_method, inner_schema = value[0], value[1]\r\n            elif isinstance(value, dict):\r\n                join_method, inner_schema = JOINED, value\r\n            else:\r\n                join_method, inner_schema = value, None\r\n\r\n            full_path = parent_path + '.' + path if parent_path else path\r\n            result[full_path] = join_method\r\n\r\n            if inner_schema:\r\n                _flatten(inner_schema, full_path, result)\r\n\r\n    result = {}\r\n    _flatten(schema, '', result)\r\n    return result", "response": "Flatten a schema into a single dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns an eager expression that loads the nested list of items from a flat schema.", "response": "def _eager_expr_from_flat_schema(flat_schema):\r\n    \"\"\"\r\n    :type flat_schema: dict\r\n    \"\"\"\r\n    result = []\r\n    for path, join_method in flat_schema.items():\r\n        if join_method == JOINED:\r\n            result.append(joinedload(path))\r\n        elif join_method == SUBQUERY:\r\n            result.append(subqueryload(path))\r\n        else:\r\n            raise ValueError('Bad join method `{}` in `{}`'\r\n                             .format(join_method, path))\r\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a new QuerySet instance with the relations joined.", "response": "def with_joined(cls, *paths):\r\n        \"\"\"\r\n        Eagerload for simple cases where we need to just\r\n         joined load some relations\r\n        In strings syntax, you can split relations with dot \r\n         due to this SQLAlchemy feature: https://goo.gl/yM2DLX\r\n         \r\n        :type paths: *List[str] | *List[InstrumentedAttribute]\r\n\r\n        Example 1:\r\n            Comment.with_joined('user', 'post', 'post.comments').first()\r\n\r\n        Example 2:\r\n            Comment.with_joined(Comment.user, Comment.post).first()\r\n        \"\"\"\r\n        options = [joinedload(path) for path in paths]\r\n        return cls.query.options(*options)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef with_subquery(cls, *paths):\r\n        options = [subqueryload(path) for path in paths]\r\n        return cls.query.options(*options)", "response": "Returns a new QuerySet that will load all relations with the given paths."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of all primary key properties for a SQLAlchemy cls.", "response": "def primary_keys_full(cls):\n        \"\"\"Get primary key properties for a SQLAlchemy cls.\n        Taken from marshmallow_sqlalchemy\n        \"\"\"\n        mapper = cls.__mapper__\n        return [\n            mapper.get_property_by_column(column)\n            for column in mapper.primary_key\n        ]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of relationship names or the given model", "response": "def relations(cls):\n        \"\"\"Return a `list` of relationship names or the given model\n        \"\"\"\n        return [c.key for c in cls.__mapper__.iterate_properties\n                if isinstance(c, RelationshipProperty)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef settable_relations(cls):\n        return [r for r in cls.relations\n                if getattr(cls, r).property.viewonly is False]", "response": "Return a list of relationship names or the given model\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef save(self):\n        self.session.add(self)\n        self.session.flush()\n        return self", "response": "Saves the current model to the current entity db."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef destroy(cls, *ids):\n        for pk in ids:\n            cls.find(pk).delete()\n        cls.session.flush()", "response": "Delete the records with the given ids\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _parse_path_and_make_aliases(entity, entity_path, attrs, aliases):\n    relations = {}\n    # take only attributes that have magic RELATION_SPLITTER\n    for attr in attrs:\n        # from attr (say, 'product__grade__order')  take\n        # relationship name ('product') and nested attribute ('grade__order')\n        if RELATION_SPLITTER in attr:\n            relation_name, nested_attr = attr.split(RELATION_SPLITTER, 1)\n            if relation_name in relations:\n                relations[relation_name].append(nested_attr)\n            else:\n                relations[relation_name] = [nested_attr]\n\n    for relation_name, nested_attrs in relations.items():\n        path = entity_path + RELATION_SPLITTER + relation_name \\\n               if entity_path else relation_name\n        if relation_name not in entity.relations:\n            raise KeyError(\"Incorrect path `{}`: \"\n                           \"{} doesnt have `{}` relationship \"\n                           .format(path, entity, relation_name))\n        relationship = getattr(entity, relation_name)\n        alias = aliased(relationship.property.argument())\n        aliases[path] = alias, relationship\n        _parse_path_and_make_aliases(alias, path, nested_attrs, aliases)", "response": "Parse the path and make aliases for the given entity."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndoes magic Django-ish joins like post___user___name__startswith='Bob' (see https://goo.gl/jAgCyM) Does filtering, sorting and eager loading at the same time. And if, say, filters and sorting need the same joinm it will be done only one. That's why all stuff is combined in single method :param query: sqlalchemy.orm.query.Query :param filters: dict :param sort_attrs: List[basestring] :param schema: dict", "response": "def smart_query(query, filters=None, sort_attrs=None, schema=None):\n    \"\"\"\n    Does magic Django-ish joins like post___user___name__startswith='Bob'\n     (see https://goo.gl/jAgCyM)\n    Does filtering, sorting and eager loading at the same time.\n    And if, say, filters and sorting need the same joinm it will be done\n     only one. That's why all stuff is combined in single method\n\n    :param query: sqlalchemy.orm.query.Query\n    :param filters: dict\n    :param sort_attrs: List[basestring]\n    :param schema: dict\n    \"\"\"\n    if not filters:\n        filters = {}\n    if not sort_attrs:\n        sort_attrs = []\n    if not schema:\n        schema = {}\n\n    # noinspection PyProtectedMember\n    root_cls = query._joinpoint_zero().class_  # for example, User or Post\n    attrs = list(filters.keys()) + \\\n        list(map(lambda s: s.lstrip(DESC_PREFIX), sort_attrs))\n    aliases = OrderedDict({})\n    _parse_path_and_make_aliases(root_cls, '', attrs, aliases)\n\n    loaded_paths = []\n    for path, al in aliases.items():\n        relationship_path = path.replace(RELATION_SPLITTER, '.')\n        query = query.outerjoin(al[0], al[1]) \\\n            .options(contains_eager(relationship_path, alias=al[0]))\n        loaded_paths.append(relationship_path)\n\n    for attr, value in filters.items():\n        if RELATION_SPLITTER in attr:\n            parts = attr.rsplit(RELATION_SPLITTER, 1)\n            entity, attr_name = aliases[parts[0]][0], parts[1]\n        else:\n            entity, attr_name = root_cls, attr\n        try:\n            query = query.filter(*entity.filter_expr(**{attr_name: value}))\n        except KeyError as e:\n            raise KeyError(\"Incorrect filter path `{}`: {}\"\n                           .format(attr, e))\n\n    for attr in sort_attrs:\n        if RELATION_SPLITTER in attr:\n            prefix = ''\n            if attr.startswith(DESC_PREFIX):\n                prefix = DESC_PREFIX\n                attr = attr.lstrip(DESC_PREFIX)\n            parts = attr.rsplit(RELATION_SPLITTER, 1)\n            entity, attr_name = aliases[parts[0]][0], prefix + parts[1]\n        else:\n            entity, attr_name = root_cls, attr\n        try:\n            query = query.order_by(*entity.order_expr(attr_name))\n        except KeyError as e:\n            raise KeyError(\"Incorrect order path `{}`: {}\".format(attr, e))\n\n    if schema:\n        flat_schema = _flatten_schema(schema)\n        not_loaded_part = {path: v for path, v in flat_schema.items()\n                           if path not in loaded_paths}\n        query = query.options(*_eager_expr_from_flat_schema(\n            not_loaded_part))\n\n    return query"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nforming expressions like [Product.age_from = 5, Product.subject_ids.in_([1,2])] from filters like {'age_from': 5, 'subject_ids__in': [1,2]} Example 1: db.query(Product).filter( *Product.filter_expr(age_from = 5, subject_ids__in=[1, 2])) Example 2: filters = {'age_from': 5, 'subject_ids__in': [1,2]} db.query(Product).filter(*Product.filter_expr(**filters)) ### About alias ###: If we will use alias: alias = aliased(Product) # table name will be product_1 we can't just write query like db.query(alias).filter(*Product.filter_expr(age_from=5)) because it will be compiled to SELECT * FROM product_1 WHERE product.age_from=5 which is wrong: we select from 'product_1' but filter on 'product' such filter will not work We need to obtain SELECT * FROM product_1 WHERE product_1.age_from=5 For such case, we can call filter_expr ON ALIAS: alias = aliased(Product) db.query(alias).filter(*alias.filter_expr(age_from=5)) Alias realization details: * we allow to call this method either ON ALIAS (say, alias.filter_expr()) or on class (Product.filter_expr()) * when method is called on alias, we need to generate SQL using aliased table (say, product_1), but we also need to have a real class to call methods on (say, Product.relations) * so, we have 'mapper' that holds table name and 'cls' that holds real class when we call this method ON ALIAS, we will have: mapper = <product_1 table> cls = <Product> when we call this method ON CLASS, we will simply have: mapper = <Product> (or we could write <Product>.__mapper__. It doesn't matter because when we call <Product>.getattr, SA will magically call <Product>.__mapper__.getattr()) cls = <Product>", "response": "def filter_expr(cls_or_alias, **filters):\n        \"\"\"\n        forms expressions like [Product.age_from = 5,\n                                Product.subject_ids.in_([1,2])]\n        from filters like {'age_from': 5, 'subject_ids__in': [1,2]}\n\n        Example 1:\n            db.query(Product).filter(\n                *Product.filter_expr(age_from = 5, subject_ids__in=[1, 2]))\n\n        Example 2:\n            filters = {'age_from': 5, 'subject_ids__in': [1,2]}\n            db.query(Product).filter(*Product.filter_expr(**filters))\n\n\n        ### About alias ###:\n        If we will use alias:\n            alias = aliased(Product) # table name will be product_1\n        we can't just write query like\n            db.query(alias).filter(*Product.filter_expr(age_from=5))\n        because it will be compiled to\n            SELECT * FROM product_1 WHERE product.age_from=5\n        which is wrong: we select from 'product_1' but filter on 'product'\n        such filter will not work\n\n        We need to obtain\n            SELECT * FROM product_1 WHERE product_1.age_from=5\n        For such case, we can call filter_expr ON ALIAS:\n            alias = aliased(Product)\n            db.query(alias).filter(*alias.filter_expr(age_from=5))\n\n        Alias realization details:\n          * we allow to call this method\n            either ON ALIAS (say, alias.filter_expr())\n            or on class (Product.filter_expr())\n          * when method is called on alias, we need to generate SQL using\n            aliased table (say, product_1), but we also need to have a real\n            class to call methods on (say, Product.relations)\n          * so, we have 'mapper' that holds table name\n            and 'cls' that holds real class\n\n            when we call this method ON ALIAS, we will have:\n                mapper = <product_1 table>\n                cls = <Product>\n            when we call this method ON CLASS, we will simply have:\n                mapper = <Product> (or we could write <Product>.__mapper__.\n                                    It doesn't matter because when we call\n                                    <Product>.getattr, SA will magically\n                                    call <Product>.__mapper__.getattr())\n                cls = <Product>\n        \"\"\"\n        if isinstance(cls_or_alias, AliasedClass):\n            mapper, cls = cls_or_alias, inspect(cls_or_alias).mapper.class_\n        else:\n            mapper = cls = cls_or_alias\n\n        expressions = []\n        valid_attributes = cls.filterable_attributes\n        for attr, value in filters.items():\n            # if attribute is filtered by method, call this method\n            if attr in cls.hybrid_methods:\n                method = getattr(cls, attr)\n                expressions.append(method(value, mapper=mapper))\n            # else just add simple condition (== for scalars or IN for lists)\n            else:\n                # determine attrbitute name and operator\n                # if they are explicitly set (say, id___between), take them\n                if OPERATOR_SPLITTER in attr:\n                    attr_name, op_name = attr.rsplit(OPERATOR_SPLITTER, 1)\n                    if op_name not in cls._operators:\n                        raise KeyError('Expression `{}` has incorrect '\n                                       'operator `{}`'.format(attr, op_name))\n                    op = cls._operators[op_name]\n                # assume equality operator for other cases (say, id=1)\n                else:\n                    attr_name, op = attr, operators.eq\n\n                if attr_name not in valid_attributes:\n                    raise KeyError('Expression `{}` '\n                                   'has incorrect attribute `{}`'\n                                   .format(attr, attr_name))\n\n                column = getattr(mapper, attr_name)\n                expressions.append(op(column, value))\n\n        return expressions"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef order_expr(cls_or_alias, *columns):\n        if isinstance(cls_or_alias, AliasedClass):\n            mapper, cls = cls_or_alias, inspect(cls_or_alias).mapper.class_\n        else:\n            mapper = cls = cls_or_alias\n\n        expressions = []\n        for attr in columns:\n            fn, attr = (desc, attr[1:]) if attr.startswith(DESC_PREFIX) \\\n                        else (asc, attr)\n            if attr not in cls.sortable_attributes:\n                raise KeyError('Cant order {} by {}'.format(cls, attr))\n\n            expr = fn(getattr(mapper, attr))\n            expressions.append(expr)\n        return expressions", "response": "Returns a list of expressions that can be used to order the users by the given columns."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef smart_query(cls, filters=None, sort_attrs=None, schema=None):\n        return smart_query(cls.query, filters, sort_attrs, schema)", "response": "This method is used to make a smart query of the object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get(self, name, default=None):\n        '''\n        Retrieves the object with \"name\", like with SessionManager.get(), but\n        removes the object from the database after retrieval, so that it can be\n        retrieved only once\n        '''\n\n        session_object = super(NotificationManager, self).get(name, default)\n        if session_object is not None:\n            self.delete(name)\n        return session_object", "response": "Gets the object with the given name."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set(self, name, value):\n        '''\n        Sets a value for \"name\". It may be any pickable (see \"pickle\" module\n        documentation) object.\n        '''\n\n        def change(session):\n            session[name] = value\n        self.__change_session(change)", "response": "Sets a value for name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get(self, name, default=None):\n        '''\n        Gets the object for \"name\", or None if there's no such object. If\n        \"default\" is provided, return it if no object is found.\n        '''\n\n        session = self.__get_session_from_db()\n\n        return session.get(name, default)", "response": "Gets the object for the given name. If no object is found for the given name None is returned."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delete(self, *names):\n        '''\n        Deletes the object with \"name\" from the session, if exists.\n        '''\n\n        def change(session):\n            keys = session.keys()\n            names_in_common = [name for name in names if name in keys]\n            for name in names_in_common:\n                del session[name]\n        self.__change_session(change)", "response": "Deletes the object with name from the session if exists."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if a thing is a valid date.", "response": "def validate(self, obj, **kwargs):\n        \"\"\"Check if a thing is a valid date.\"\"\"\n        obj = stringify(obj)\n        if obj is None:\n            return False\n        return self.DATE_RE.match(obj) is not None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef clean(self, text, guess=True, format=None, **kwargs):\n        # handle date/datetime before converting to text.\n        date = self._clean_datetime(text)\n        if date is not None:\n            return date\n\n        text = stringify(text)\n        if text is None:\n            return\n\n        if format is not None:\n            # parse with a specified format\n            try:\n                obj = datetime.strptime(text, format)\n                return obj.date().isoformat()\n            except Exception:\n                return None\n\n        if guess and not self.validate(text):\n            # use dateparser to guess the format\n            obj = self.fuzzy_date_parser(text)\n            if obj is not None:\n                return obj.date().isoformat()\n\n        return self._clean_text(text)", "response": "The classic date parsing"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fuzzy_date_parser(self, text):\n        try:\n            parsed = dateparser.parse(text, dayfirst=True)\n            return parsed\n        except (ValueError, TypeError):\n            locales = parsedatetime._locales[:]\n            # Loop through all the locales and try to parse successfully our\n            # string\n            for locale in locales:\n                const = parsedatetime.Constants(locale)\n                const.re_option += re.UNICODE\n                parser = parsedatetime.Calendar(const)\n                parsed, ok = parser.parse(text)\n                if ok:\n                    return datetime(*parsed[:6])", "response": "A wrapper around dateparser. parse that does not parse date or time."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses the result directory to extract the pieces information to be stored in the NIDM - Results.", "response": "def parse(self):\n        \"\"\"\n        Parse a result directory to extract the pieces information to be\n        stored in NIDM-Results.\n        \"\"\"\n\n        try:\n            # Methods: find_software, find_model_fitting, find_contrasts and\n            # find_inferences should be defined in the children classes and\n            # return a list of NIDM Objects as specified in the objects module\n\n            # Object of type Software describing the neuroimaging software\n            # package used for the analysis\n            self.software = self._find_software()\n\n            # List of objects of type ModelFitting describing the\n            # model fitting step in NIDM-Results (main activity: Model\n            # Parameters Estimation)\n            self.model_fittings = self._find_model_fitting()\n\n            # Dictionary of (key, value) pairs where where key is a tuple\n            # containing the identifier of a ModelParametersEstimation object\n            # and a tuple of identifiers of ParameterEstimateMap objects and\n            # value is an object of type Contrast describing the contrast\n            # estimation step in NIDM-Results (main activity: Contrast\n            # Estimation)\n            self.contrasts = self._find_contrasts()\n\n            # Inference activity and entities\n            # Dictionary of (key, value) pairs where key is the identifier of a\n            # ContrastEstimation object and value is an object of type\n            # Inference describing the inference step in NIDM-Results (main\n            # activity: Inference)\n            self.inferences = self._find_inferences()\n        except Exception:\n            self.cleanup()\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd a NIDM object to the NIDM - Results export.", "response": "def add_object(self, nidm_object, export_file=True):\n        \"\"\"\n        Add a NIDMObject to a NIDM-Results export.\n        \"\"\"\n        if not export_file:\n            export_dir = None\n        else:\n            export_dir = self.export_dir\n\n        if not isinstance(nidm_object, NIDMFile):\n            nidm_object.export(self.version, export_dir)\n        else:\n            nidm_object.export(self.version, export_dir, self.prepend_path)\n        # ProvDocument: add object to the bundle\n        if nidm_object.prov_type == PROV['Activity']:\n            self.bundle.activity(nidm_object.id,\n                                 other_attributes=nidm_object.attributes)\n        elif nidm_object.prov_type == PROV['Entity']:\n            self.bundle.entity(nidm_object.id,\n                               other_attributes=nidm_object.attributes)\n        elif nidm_object.prov_type == PROV['Agent']:\n            self.bundle.agent(nidm_object.id,\n                              other_attributes=nidm_object.attributes)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef export(self):\n        try:\n            if not os.path.isdir(self.export_dir):\n                os.mkdir(self.export_dir)\n\n            # Initialise main bundle\n            self._create_bundle(self.version)\n\n            self.add_object(self.software)\n\n            # Add model fitting steps\n            if not isinstance(self.model_fittings, list):\n                self.model_fittings = list(self.model_fittings.values())\n\n            for model_fitting in self.model_fittings:\n                # Design Matrix\n                # model_fitting.activity.used(model_fitting.design_matrix)\n                self.bundle.used(model_fitting.activity.id,\n                                 model_fitting.design_matrix.id)\n                self.add_object(model_fitting.design_matrix)\n                # *** Export visualisation of the design matrix\n                self.add_object(model_fitting.design_matrix.image)\n\n                if model_fitting.design_matrix.image.file is not None:\n                    self.add_object(model_fitting.design_matrix.image.file)\n\n                if model_fitting.design_matrix.hrf_models is not None:\n                    # drift model\n                    self.add_object(model_fitting.design_matrix.drift_model)\n\n                if self.version['major'] > 1 or \\\n                        (self.version['major'] == 1 and\n                         self.version['minor'] >= 3):\n                    # Machine\n                    # model_fitting.data.wasAttributedTo(model_fitting.machine)\n                    self.bundle.wasAttributedTo(model_fitting.data.id,\n                                                model_fitting.machine.id)\n                    self.add_object(model_fitting.machine)\n\n                    # Imaged subject or group(s)\n                    for sub in model_fitting.subjects:\n                        self.add_object(sub)\n                        # model_fitting.data.wasAttributedTo(sub)\n                        self.bundle.wasAttributedTo(model_fitting.data.id,\n                                                    sub.id)\n\n                # Data\n                # model_fitting.activity.used(model_fitting.data)\n                self.bundle.used(model_fitting.activity.id,\n                                 model_fitting.data.id)\n                self.add_object(model_fitting.data)\n\n                # Error Model\n                # model_fitting.activity.used(model_fitting.error_model)\n                self.bundle.used(model_fitting.activity.id,\n                                 model_fitting.error_model.id)\n                self.add_object(model_fitting.error_model)\n\n                # Parameter Estimate Maps\n                for param_estimate in model_fitting.param_estimates:\n                    # param_estimate.wasGeneratedBy(model_fitting.activity)\n                    self.bundle.wasGeneratedBy(param_estimate.id,\n                                               model_fitting.activity.id)\n                    self.add_object(param_estimate)\n                    self.add_object(param_estimate.coord_space)\n                    self.add_object(param_estimate.file)\n\n                    if param_estimate.derfrom is not None:\n                        self.bundle.wasDerivedFrom(param_estimate.id,\n                                                   param_estimate.derfrom.id)\n                        self.add_object(param_estimate.derfrom)\n                        self.add_object(param_estimate.derfrom.file,\n                                        export_file=False)\n\n                # Residual Mean Squares Map\n                # model_fitting.rms_map.wasGeneratedBy(model_fitting.activity)\n                self.add_object(model_fitting.rms_map)\n                self.bundle.wasGeneratedBy(model_fitting.rms_map.id,\n                                           model_fitting.activity.id)\n                self.add_object(model_fitting.rms_map.coord_space)\n                self.add_object(model_fitting.rms_map.file)\n                if model_fitting.rms_map.derfrom is not None:\n                    self.bundle.wasDerivedFrom(\n                        model_fitting.rms_map.id,\n                        model_fitting.rms_map.derfrom.id)\n                    self.add_object(model_fitting.rms_map.derfrom)\n                    self.add_object(model_fitting.rms_map.derfrom.file,\n                                    export_file=False)\n\n                # Resels per Voxel Map\n                if model_fitting.rpv_map is not None:\n                    self.add_object(model_fitting.rpv_map)\n                    self.bundle.wasGeneratedBy(model_fitting.rpv_map.id,\n                                               model_fitting.activity.id)\n                    self.add_object(model_fitting.rpv_map.coord_space)\n                    self.add_object(model_fitting.rpv_map.file)\n                    if model_fitting.rpv_map.inf_id is not None:\n                        self.bundle.used(model_fitting.rpv_map.inf_id,\n                                         model_fitting.rpv_map.id)\n                    if model_fitting.rpv_map.derfrom is not None:\n                        self.bundle.wasDerivedFrom(\n                            model_fitting.rpv_map.id,\n                            model_fitting.rpv_map.derfrom.id)\n                        self.add_object(model_fitting.rpv_map.derfrom)\n                        self.add_object(model_fitting.rpv_map.derfrom.file,\n                                        export_file=False)\n\n                # Mask\n                # model_fitting.mask_map.wasGeneratedBy(model_fitting.activity)\n                self.bundle.wasGeneratedBy(model_fitting.mask_map.id,\n                                           model_fitting.activity.id)\n                self.add_object(model_fitting.mask_map)\n                if model_fitting.mask_map.derfrom is not None:\n                    self.bundle.wasDerivedFrom(\n                        model_fitting.mask_map.id,\n                        model_fitting.mask_map.derfrom.id)\n                    self.add_object(model_fitting.mask_map.derfrom)\n                    self.add_object(model_fitting.mask_map.derfrom.file,\n                                    export_file=False)\n\n                # Create coordinate space export\n                self.add_object(model_fitting.mask_map.coord_space)\n                # Create \"Mask map\" entity\n                self.add_object(model_fitting.mask_map.file)\n\n                # Grand Mean map\n                # model_fitting.grand_mean_map.wasGeneratedBy(model_fitting.activity)\n                self.bundle.wasGeneratedBy(model_fitting.grand_mean_map.id,\n                                           model_fitting.activity.id)\n                self.add_object(model_fitting.grand_mean_map)\n                # Coordinate space entity\n                self.add_object(model_fitting.grand_mean_map.coord_space)\n                # Grand Mean Map entity\n                self.add_object(model_fitting.grand_mean_map.file)\n\n                # Model Parameters Estimation activity\n                self.add_object(model_fitting.activity)\n                self.bundle.wasAssociatedWith(model_fitting.activity.id,\n                                              self.software.id)\n                # model_fitting.activity.wasAssociatedWith(self.software)\n                # self.add_object(model_fitting)\n\n            # Add contrast estimation steps\n            analysis_masks = dict()\n            for (model_fitting_id, pe_ids), contrasts in list(\n                    self.contrasts.items()):\n                for contrast in contrasts:\n                    model_fitting = self._get_model_fitting(model_fitting_id)\n                    # for contrast in contrasts:\n                    # contrast.estimation.used(model_fitting.rms_map)\n                    self.bundle.used(contrast.estimation.id,\n                                     model_fitting.rms_map.id)\n                    # contrast.estimation.used(model_fitting.mask_map)\n                    self.bundle.used(contrast.estimation.id,\n                                     model_fitting.mask_map.id)\n                    analysis_masks[contrast.estimation.id] = \\\n                        model_fitting.mask_map.id\n                    self.bundle.used(contrast.estimation.id,\n                                     contrast.weights.id)\n                    self.bundle.used(contrast.estimation.id,\n                                     model_fitting.design_matrix.id)\n                    # contrast.estimation.wasAssociatedWith(self.software)\n                    self.bundle.wasAssociatedWith(contrast.estimation.id,\n                                                  self.software.id)\n\n                    for pe_id in pe_ids:\n                        # contrast.estimation.used(pe_id)\n                        self.bundle.used(contrast.estimation.id, pe_id)\n\n                    # Create estimation activity\n                    self.add_object(contrast.estimation)\n\n                    # Create contrast weights\n                    self.add_object(contrast.weights)\n\n                    if contrast.contrast_map is not None:\n                        # Create contrast Map\n                        # contrast.contrast_map.wasGeneratedBy(contrast.estimation)\n                        self.bundle.wasGeneratedBy(contrast.contrast_map.id,\n                                                   contrast.estimation.id)\n                        self.add_object(contrast.contrast_map)\n                        self.add_object(contrast.contrast_map.coord_space)\n                        # Copy contrast map in export directory\n                        self.add_object(contrast.contrast_map.file)\n\n                        if contrast.contrast_map.derfrom is not None:\n                            self.bundle.wasDerivedFrom(\n                                contrast.contrast_map.id,\n                                contrast.contrast_map.derfrom.id)\n                            self.add_object(contrast.contrast_map.derfrom)\n                            self.add_object(contrast.contrast_map.derfrom.file,\n                                            export_file=False)\n\n                    # Create Std Err. Map (T-tests) or Explained Mean Sq. Map\n                    # (F-tests)\n                    # contrast.stderr_or_expl_mean_sq_map.wasGeneratedBy\n                    # (contrast.estimation)\n                    stderr_explmeansq_map = (\n                        contrast.stderr_or_expl_mean_sq_map)\n                    self.bundle.wasGeneratedBy(\n                        stderr_explmeansq_map.id,\n                        contrast.estimation.id)\n                    self.add_object(stderr_explmeansq_map)\n                    self.add_object(\n                        stderr_explmeansq_map.coord_space)\n                    if isinstance(stderr_explmeansq_map,\n                                  ContrastStdErrMap) and \\\n                            stderr_explmeansq_map.contrast_var:\n                        self.add_object(\n                            stderr_explmeansq_map.contrast_var)\n                        if stderr_explmeansq_map.var_coord_space:\n                            self.add_object(\n                                stderr_explmeansq_map.var_coord_space)\n                        if stderr_explmeansq_map.contrast_var.coord_space:\n                            self.add_object(\n                                stderr_explmeansq_map.contrast_var.coord_space)\n                        self.add_object(\n                            stderr_explmeansq_map.contrast_var.file,\n                            export_file=False)\n                        self.bundle.wasDerivedFrom(\n                            stderr_explmeansq_map.id,\n                            stderr_explmeansq_map.contrast_var.id)\n                    self.add_object(stderr_explmeansq_map.file)\n\n                    # Create Statistic Map\n                    # contrast.stat_map.wasGeneratedBy(contrast.estimation)\n                    self.bundle.wasGeneratedBy(contrast.stat_map.id,\n                                               contrast.estimation.id)\n                    self.add_object(contrast.stat_map)\n                    self.add_object(contrast.stat_map.coord_space)\n                    # Copy Statistical map in export directory\n                    self.add_object(contrast.stat_map.file)\n\n                    if contrast.stat_map.derfrom is not None:\n                        self.bundle.wasDerivedFrom(\n                            contrast.stat_map.id,\n                            contrast.stat_map.derfrom.id)\n                        self.add_object(contrast.stat_map.derfrom)\n                        self.add_object(contrast.stat_map.derfrom.file,\n                                        export_file=False)\n\n                    # Create Z Statistic Map\n                    if contrast.z_stat_map:\n                        # contrast.z_stat_map.wasGeneratedBy(contrast.estimation)\n                        self.bundle.wasGeneratedBy(contrast.z_stat_map.id,\n                                                   contrast.estimation.id)\n                        self.add_object(contrast.z_stat_map)\n                        self.add_object(contrast.z_stat_map.coord_space)\n                        # Copy Statistical map in export directory\n                        self.add_object(contrast.z_stat_map.file)\n\n                    # self.add_object(contrast)\n\n            # Add inference steps\n            for contrast_id, inferences in list(self.inferences.items()):\n                contrast = self._get_contrast(contrast_id)\n\n                for inference in inferences:\n                    if contrast.z_stat_map:\n                        used_id = contrast.z_stat_map.id\n                    else:\n                        used_id = contrast.stat_map.id\n                    # inference.inference_act.used(used_id)\n                    self.bundle.used(inference.inference_act.id, used_id)\n                    # inference.inference_act.wasAssociatedWith(self.software)\n                    self.bundle.wasAssociatedWith(inference.inference_act.id,\n                                                  self.software.id)\n\n                    # self.add_object(inference)\n                    # Excursion set\n                    # inference.excursion_set.wasGeneratedBy(inference.inference_act)\n                    self.bundle.wasGeneratedBy(inference.excursion_set.id,\n                                               inference.inference_act.id)\n                    self.add_object(inference.excursion_set)\n                    self.add_object(inference.excursion_set.coord_space)\n                    if inference.excursion_set.visu is not None:\n                        self.add_object(inference.excursion_set.visu)\n                        if inference.excursion_set.visu.file is not None:\n                            self.add_object(inference.excursion_set.visu.file)\n                    # Copy \"Excursion set map\" file in export directory\n                    self.add_object(inference.excursion_set.file)\n                    if inference.excursion_set.clust_map is not None:\n                        self.add_object(inference.excursion_set.clust_map)\n                        self.add_object(inference.excursion_set.clust_map.file)\n                        self.add_object(\n                            inference.excursion_set.clust_map.coord_space)\n\n                    if inference.excursion_set.mip is not None:\n                        self.add_object(inference.excursion_set.mip)\n                        self.add_object(inference.excursion_set.mip.file)\n\n                    # Height threshold\n                    if inference.height_thresh.equiv_thresh is not None:\n                        for equiv in inference.height_thresh.equiv_thresh:\n                            self.add_object(equiv)\n                    self.add_object(inference.height_thresh)\n\n                    # Extent threshold\n                    if inference.extent_thresh.equiv_thresh is not None:\n                        for equiv in inference.extent_thresh.equiv_thresh:\n                            self.add_object(equiv)\n                    self.add_object(inference.extent_thresh)\n\n                    # Display Mask (potentially more than 1)\n                    if inference.disp_mask:\n                        for mask in inference.disp_mask:\n                            # inference.inference_act.used(mask)\n                            self.bundle.used(inference.inference_act.id,\n                                             mask.id)\n                            self.add_object(mask)\n                            # Create coordinate space entity\n                            self.add_object(mask.coord_space)\n                            # Create \"Display Mask Map\" entity\n                            self.add_object(mask.file)\n\n                            if mask.derfrom is not None:\n                                self.bundle.wasDerivedFrom(mask.id,\n                                                           mask.derfrom.id)\n                                self.add_object(mask.derfrom)\n                                self.add_object(mask.derfrom.file,\n                                                export_file=False)\n\n                    # Search Space\n                    self.bundle.wasGeneratedBy(inference.search_space.id,\n                                               inference.inference_act.id)\n                    # inference.search_space.wasGeneratedBy(inference.inference_act)\n                    self.add_object(inference.search_space)\n                    self.add_object(inference.search_space.coord_space)\n                    # Copy \"Mask map\" in export directory\n                    self.add_object(inference.search_space.file)\n\n                    # Peak Definition\n                    if inference.peak_criteria:\n                        # inference.inference_act.used(inference.peak_criteria)\n                        self.bundle.used(inference.inference_act.id,\n                                         inference.peak_criteria.id)\n                        self.add_object(inference.peak_criteria)\n\n                    # Cluster Definition\n                    if inference.cluster_criteria:\n                        # inference.inference_act.used(inference.cluster_criteria)\n                        self.bundle.used(inference.inference_act.id,\n                                         inference.cluster_criteria.id)\n                        self.add_object(inference.cluster_criteria)\n\n                    if inference.clusters:\n                        # Clusters and peaks\n                        for cluster in inference.clusters:\n                            # cluster.wasDerivedFrom(inference.excursion_set)\n                            self.bundle.wasDerivedFrom(\n                                cluster.id, inference.excursion_set.id)\n                            self.add_object(cluster)\n                            for peak in cluster.peaks:\n                                self.bundle.wasDerivedFrom(peak.id, cluster.id)\n                                self.add_object(peak)\n                                self.add_object(peak.coordinate)\n\n                            if cluster.cog is not None:\n                                self.bundle.wasDerivedFrom(cluster.cog.id,\n                                                           cluster.id)\n                                self.add_object(cluster.cog)\n                                self.add_object(cluster.cog.coordinate)\n\n                    # Inference activity\n                    # inference.inference_act.wasAssociatedWith(inference.software_id)\n                    # inference.inference_act.used(inference.height_thresh)\n                    self.bundle.used(inference.inference_act.id,\n                                     inference.height_thresh.id)\n                    # inference.inference_act.used(inference.extent_thresh)\n                    self.bundle.used(inference.inference_act.id,\n                                     inference.extent_thresh.id)\n                    self.bundle.used(inference.inference_act.id,\n                                     analysis_masks[contrast.estimation.id])\n                    self.add_object(inference.inference_act)\n\n            # Write-out prov file\n            self.save_prov_to_files()\n\n            return self.out_dir\n        except Exception:\n            self.cleanup()\n            raise", "response": "Generate a NIDM - Results export."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the model fitting with identifier mf_id from the list of model fitting objects stored in self. model_fitting .", "response": "def _get_model_fitting(self, mf_id):\n        \"\"\"\n        Retreive model fitting with identifier 'mf_id' from the list of model\n        fitting objects stored in self.model_fitting\n        \"\"\"\n        for model_fitting in self.model_fittings:\n            if model_fitting.activity.id == mf_id:\n                return model_fitting\n\n        raise Exception(\"Model fitting activity with id: \" + str(mf_id) +\n                        \" not found.\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_contrast(self, con_id):\n        for contrasts in list(self.contrasts.values()):\n            for contrast in contrasts:\n                if contrast.estimation.id == con_id:\n                    return contrast\n        raise Exception(\"Contrast activity with id: \" + str(con_id) +\n                        \" not found.\")", "response": "Get contrast with identifier con_id from the list of contrast objects stored in self. contrasts\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds namespaces to NIDM document.", "response": "def _add_namespaces(self):\n        \"\"\"\n        Add namespaces to NIDM document.\n        \"\"\"\n        self.doc.add_namespace(NIDM)\n        self.doc.add_namespace(NIIRI)\n        self.doc.add_namespace(CRYPTO)\n        self.doc.add_namespace(DCT)\n        self.doc.add_namespace(DC)\n        self.doc.add_namespace(NFO)\n        self.doc.add_namespace(OBO)\n        self.doc.add_namespace(SCR)\n        self.doc.add_namespace(NIF)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _create_bundle(self, version):\n        # *** Bundle entity\n        if not hasattr(self, 'bundle_ent'):\n            self.bundle_ent = NIDMResultsBundle(nidm_version=version['num'])\n\n        self.bundle = ProvBundle(identifier=self.bundle_ent.id)\n\n        self.bundle_ent.export(self.version, self.export_dir)\n\n        # # provn export\n        # self.bundle = ProvBundle(identifier=bundle_id)\n\n        self.doc.entity(self.bundle_ent.id,\n                        other_attributes=self.bundle_ent.attributes)\n\n        # *** NIDM-Results Export Activity\n        if version['num'] not in [\"1.0.0\", \"1.1.0\"]:\n            if not hasattr(self, 'export_act'):\n                self.export_act = NIDMResultsExport()\n            self.export_act.export(self.version, self.export_dir)\n            # self.doc.update(self.export_act.p)\n            self.doc.activity(self.export_act.id,\n                              other_attributes=self.export_act.attributes)\n\n        # *** bundle was Generated by NIDM-Results Export Activity\n        if not hasattr(self, 'export_time'):\n            self.export_time = str(datetime.datetime.now().time())\n\n        if version['num'] in [\"1.0.0\", \"1.1.0\"]:\n            self.doc.wasGeneratedBy(entity=self.bundle_ent.id,\n                                    time=self.export_time)\n        else:\n            # provn\n            self.doc.wasGeneratedBy(\n                entity=self.bundle_ent.id, activity=self.export_act.id,\n                time=self.export_time)\n\n        # *** NIDM-Results Exporter (Software Agent)\n        if version['num'] not in [\"1.0.0\", \"1.1.0\"]:\n            if not hasattr(self, 'exporter'):\n                self.exporter = self._get_exporter()\n            self.exporter.export(self.version, self.export_dir)\n            # self.doc.update(self.exporter.p)\n            self.doc.agent(self.exporter.id,\n                           other_attributes=self.exporter.attributes)\n\n            self.doc.wasAssociatedWith(self.export_act.id, self.exporter.id)", "response": "Create the NIDM - Results bundle."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_model_parameters_estimations(self, error_model):\n        if error_model.dependance == NIDM_INDEPEDENT_ERROR:\n            if error_model.variance_homo:\n                estimation_method = STATO_OLS\n            else:\n                estimation_method = STATO_WLS\n        else:\n            estimation_method = STATO_GLS\n\n        mpe = ModelParametersEstimation(estimation_method, self.software.id)\n\n        return mpe", "response": "Return an object containing the model parameters estimation method."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwriting out the provn serialisation to nidm. provn.", "response": "def save_prov_to_files(self, showattributes=False):\n        \"\"\"\n        Write-out provn serialisation to nidm.provn.\n        \"\"\"\n        self.doc.add_bundle(self.bundle)\n        # provn_file = os.path.join(self.export_dir, 'nidm.provn')\n        # provn_fid = open(provn_file, 'w')\n        # # FIXME None\n        # # provn_fid.write(self.doc.get_provn(4).replace(\"None\", \"-\"))\n        # provn_fid.close()\n\n        ttl_file = os.path.join(self.export_dir, 'nidm.ttl')\n        ttl_txt = self.doc.serialize(format='rdf', rdf_format='turtle')\n        ttl_txt, json_context = self.use_prefixes(ttl_txt)\n\n        # Add namespaces to json-ld context\n        for namespace in self.doc._namespaces.get_registered_namespaces():\n            json_context[namespace._prefix] = namespace._uri\n        for namespace in \\\n                list(self.doc._namespaces._default_namespaces.values()):\n            json_context[namespace._prefix] = namespace._uri\n        json_context[\"xsd\"] = \"http://www.w3.org/2000/01/rdf-schema#\"\n\n        # Work-around to issue with INF value in rdflib (reported in\n        # https://github.com/RDFLib/rdflib/pull/655)\n        ttl_txt = ttl_txt.replace(' inf ', ' \"INF\"^^xsd:float ')\n        with open(ttl_file, 'w') as ttl_fid:\n            ttl_fid.write(ttl_txt)\n\n        # print(json_context)\n        jsonld_file = os.path.join(self.export_dir, 'nidm.json')\n        jsonld_txt = self.doc.serialize(format='rdf', rdf_format='json-ld',\n                                        context=json_context)\n        with open(jsonld_file, 'w') as jsonld_fid:\n            jsonld_fid.write(jsonld_txt)\n\n        # provjsonld_file = os.path.join(self.export_dir, 'nidm.provjsonld')\n        # provjsonld_txt = self.doc.serialize(format='jsonld')\n        # with open(provjsonld_file, 'w') as provjsonld_fid:\n        #     provjsonld_fid.write(provjsonld_txt)\n\n        # provn_file = os.path.join(self.export_dir, 'nidm.provn')\n        # provn_txt = self.doc.serialize(format='provn')\n        # with open(provn_file, 'w') as provn_fid:\n        #     provn_fid.write(provn_txt)\n\n        # Post-processing\n        if not self.zipped:\n            # Just rename temp directory to output_path\n            os.rename(self.export_dir, self.out_dir)\n        else:\n            # Create a zip file that contains the content of the temp directory\n            os.chdir(self.export_dir)\n            zf = zipfile.ZipFile(os.path.join(\"..\", self.out_dir), mode='w')\n            try:\n                for root, dirnames, filenames in os.walk(\".\"):\n                    for filename in filenames:\n                        zf.write(os.path.join(filename))\n            finally:\n                zf.close()\n                # Need to move up before deleting the folder\n                os.chdir(\"..\")\n                shutil.rmtree(os.path.join(\"..\", self.export_dir))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse a phone number and return in international format.", "response": "def clean_text(self, number, countries=None, country=None, **kwargs):\n        \"\"\"Parse a phone number and return in international format.\n\n        If no valid phone number can be detected, None is returned. If\n        a country code is supplied, this will be used to infer the\n        prefix.\n\n        https://github.com/daviddrysdale/python-phonenumbers\n        \"\"\"\n        for code in self._clean_countries(countries, country):\n            try:\n                num = parse_number(number, code)\n                if is_possible_number(num):\n                    if is_valid_number(num):\n                        return format_number(num, PhoneNumberFormat.E164)\n            except NumberParseException:\n                pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks to see if this is a valid ip address.", "response": "def validate(self, ip, **kwargs):\n        \"\"\"Check to see if this is a valid ip address.\"\"\"\n        \n        if ip is None:\n            return False\n\n        ip = stringify(ip)\n\n        if self.IPV4_REGEX.match(ip):\n\n            try:\n                socket.inet_pton(socket.AF_INET, ip)\n                return True\n            except AttributeError:  # no inet_pton here, sorry\n                try:\n                    socket.inet_aton(ip)\n                except socket.error:\n                    return False\n                return ip.count('.') == 3\n            except socket.error:  # not a valid address\n                return False\n\n        if self.IPV6_REGEX.match(ip):\n            try:\n                socket.inet_pton(socket.AF_INET6, ip)\n            except socket.error:  # not a valid address\n                return False\n            return True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef export(self, nidm_version, export_dir):\n        self.add_attributes({\n            PROV['type']: self.type,\n            NIDM_DIMENSIONS_IN_VOXELS: json.dumps(self.dimensions.tolist()),\n            NIDM_NUMBER_OF_DIMENSIONS: self.number_of_dimensions,\n            NIDM_VOXEL_TO_WORLD_MAPPING:\n            json.dumps(self.voxel_to_world.tolist()),\n            NIDM_IN_WORLD_COORDINATE_SYSTEM: self.coordinate_system,\n            NIDM_VOXEL_UNITS: json.dumps(self.units),\n            NIDM_VOXEL_SIZE: json.dumps(self.voxel_size.tolist()),\n            PROV['label']: self.label})", "response": "Export this object to NIDM."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef export(self, nidm_version, export_dir, prepend_path):\n        if self.path is not None:\n            if export_dir is not None:\n                # Copy file only if export_dir is not None\n                new_file = os.path.join(export_dir, self.filename)\n                if not self.path == new_file:\n                    if prepend_path.endswith('.zip'):\n                        with zipfile.ZipFile(prepend_path) as z:\n                            extracted = z.extract(str(self.path), export_dir)\n                            shutil.move(extracted, new_file)\n                    else:\n                        if prepend_path:\n                            file_copied = os.path.join(prepend_path, self.path)\n                        else:\n                            file_copied = self.path\n                        shutil.copy(file_copied, new_file)\n\n                    if self.temporary:\n                        os.remove(self.path)\n            else:\n                new_file = self.path\n\n        if nidm_version['num'] in [\"1.0.0\", \"1.1.0\"]:\n            loc = Identifier(\"file://./\" + self.filename)\n        else:\n            loc = Identifier(self.filename)\n\n        self.add_attributes([(NFO['fileName'], self.filename)])\n\n        if export_dir:\n            self.add_attributes([(PROV['atLocation'], loc)])\n\n        if nidm_version['num'] in (\"1.0.0\", \"1.1.0\"):\n            path, org_filename = os.path.split(self.path)\n            if (org_filename is not self.filename) \\\n                    and (not self.temporary):\n                self.add_attributes([(NFO['fileName'], org_filename)])\n\n        if self.is_nifti():\n            if self.sha is None:\n                self.sha = self.get_sha_sum(new_file)\n            if self.fmt is None:\n                self.fmt = \"image/nifti\"\n\n            self.add_attributes([\n                (CRYPTO['sha512'], self.sha),\n                (DCT['format'], self.fmt)\n            ])", "response": "Export the current object into a new file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nexports the current object to the export_dir.", "response": "def export(self, nidm_version, export_dir):\n        \"\"\"\n        Create prov entity.\n        \"\"\"\n        if self.file is not None:\n            self.add_attributes([\n                (PROV['type'], self.type),\n                (DCT['format'], \"image/png\"),\n            ])"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nexports this entry into the NIDM.", "response": "def export(self, nidm_version, export_dir):\n        \"\"\"\n        Create prov entities and activities.\n        \"\"\"\n        if nidm_version['major'] < 1 or \\\n                (nidm_version['major'] == 1 and nidm_version['minor'] < 3):\n            self.type = NLX_OLD_FSL\n\n        atts = (\n            (PROV['type'], self.type),\n            (PROV['type'], PROV['SoftwareAgent']),\n            (PROV['label'], Literal(self.label, datatype=XSD_STRING)),\n            (NIDM_SOFTWARE_VERSION, self.version)\n            )\n\n        if self.feat_version:\n            atts = atts + ((FSL_FEAT_VERSION, self.feat_version),)\n\n        self.add_attributes(atts)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmaking the address more compareable.", "response": "def normalize(self, address, **kwargs):\n        \"\"\"Make the address more compareable.\"\"\"\n        # TODO: normalize well-known parts like \"Street\", \"Road\", etc.\n        # TODO: consider using https://github.com/openvenues/pypostal\n        addresses = super(AddressType, self).normalize(address, **kwargs)\n        return addresses"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef clean_text(self, url, **kwargs):\n        try:\n            return normalize_url(url)\n        except UnicodeDecodeError:\n            log.warning(\"Invalid URL: %r\", url)", "response": "Perform intensive care on URLs see urlnormalizer."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a more clean version of an anonymized version of the text.", "response": "def clean_text(self, text, **kwargs):\n        \"\"\"Create a more clean, but still user-facing version of an\n        instance of the type.\"\"\"\n        text = text.replace(\" \", \"\")\n        text = text.upper()\n        return text"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef clean_text(self, country, guess=False, **kwargs):\n        code = country.lower().strip()\n        if code in self.names:\n            return code\n        country = countrynames.to_code(country, fuzzy=guess)\n        if country is not None:\n            return country.lower()", "response": "Determine a two - letter country code based on an input."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef validate(self, email, **kwargs):\n        email = stringify(email)\n        if email is None:\n            return\n        if not self.EMAIL_REGEX.match(email):\n            return False\n        mailbox, domain = email.rsplit('@', 1)\n        return self.domains.validate(domain, **kwargs)", "response": "Check to see if this is a valid email address."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef clean_text(self, email, **kwargs):\n        if not self.EMAIL_REGEX.match(email):\n            return None\n        email = strip_quotes(email)\n        mailbox, domain = email.rsplit('@', 1)\n        domain = self.domains.clean(domain, **kwargs)\n        if domain is None or mailbox is None:\n            return\n        return '@'.join((mailbox, domain))", "response": "Parse and normalize an email address. Returns None if this is not an email address."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfixes the RDF data for specific versions of the NIDM - Results NIDM and FSL exporters.", "response": "def fix_for_specific_versions(self, rdf_data, to_replace):\n        \"\"\"\n        Fixes of the RDF before loading the graph. All of these are workaround\n        to circuvent known issues of the SPM and FSL exporters.\n        \"\"\"\n\n        # Load the graph as is so that we can query\n        g = self.parse(rdf_data)\n\n        # Retreive the exporter name and version\n        query = \"\"\"\nprefix nidm_spm_results_nidm: <http://purl.org/nidash/nidm#NIDM_0000168>\nprefix nidm_nidmfsl: <http://purl.org/nidash/nidm#NIDM_0000167>\nprefix nidm_softwareVersion: <http://purl.org/nidash/nidm#NIDM_0000122>\n\nSELECT DISTINCT ?type ?version ?exp_act WHERE {\n    {?exporter a nidm_nidmfsl: .} UNION {?exporter a nidm_spm_results_nidm: .}.\n    ?exporter a ?type ;\n        nidm_softwareVersion: ?version .\n\n    ?exp_act prov:wasAssociatedWith ?exporter .\n\n    FILTER ( ?type NOT IN (prov:SoftwareAgent, prov:Agent))\n}\n        \"\"\"\n\n        sd = g.query(query)\n        objects = dict()\n        if sd:\n            for row in sd:\n                argums = row.asdict()\n                if (argums['type'] == NIDM_SPM_RESULTS_NIDM and\n                        (argums['version'].eq('12.6903') or\n                         argums['version'].eq('12.575ac2c'))):\n                    warnings.warn('Applying fixes for SPM exporter ' +\n                                  str(argums['version']))\n                    # crypto namespace inconsistent with NIDM-Results spec\n                    to_replace[('http://id.loc.gov/vocabulary/preservation/' +\n                                'cryptographicHashFunctions/')] = (\n                                'http://id.loc.gov/vocabulary/preservation/' +\n                                'cryptographicHashFunctions#')\n                    # Missing 'activity' attribute in qualified Generation\n                    to_replace['a prov:Generation .'] = (\n                        'a prov:Generation ; prov:activity <' +\n                        str(argums['exp_act']) + '> .')\n\n        # Avoid confusion between attribute and\n        # class uncorrected p-value\n        # cf. https://github.com/incf-nidash/nidm/issues/421\n        to_replace[('@prefix nidm_PValueUncorrected: ' +\n                    '<http://purl.org/nidash/nidm#NIDM_0000160>')] = (\n                    '@prefix nidm_UncorrectedPValue: ' +\n                    '<http://purl.org/nidash/nidm#NIDM_0000160>')\n        to_replace['nidm_PValueUncorrected'] = 'nidm_UncorrectedPValue'\n\n        if to_replace is not None:\n            for to_rep, replacement in to_replace.items():\n                rdf_data = rdf_data.replace(to_rep, replacement)\n\n        return rdf_data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_model_fitting(self, con_est_id):\n        for (mpe_id, pe_ids), contrasts in self.contrasts.items():\n            for contrast in contrasts:\n                if contrast.estimation.id == con_est_id:\n                    model_fitting_id = mpe_id\n                    pe_map_ids = pe_ids\n                    break\n\n        for model_fitting in self.model_fittings:\n            if model_fitting.activity.id == model_fitting_id:\n                return (model_fitting, pe_map_ids)\n\n        raise Exception(\"Model fitting of contrast : \" + str(con_est_id) +\n                        \" not found.\")", "response": "Return model fitting that corresponds to contrast with identifier con_id."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nexport this object to NIDM.", "response": "def export(self, nidm_version, export_dir):\n        \"\"\"\n        Create prov entities and activities.\n        \"\"\"\n        self.add_attributes((\n            (PROV['type'], self.type),\n            (NIDM_GROUP_NAME, self.group_name),\n            (NIDM_NUMBER_OF_SUBJECTS, self.num_subjects),\n            (PROV['label'], self.label)))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexports the current state of the object into a dictionary.", "response": "def export(self, nidm_version, export_dir):\n        \"\"\"\n        Create prov entities and activities.\n        \"\"\"\n        # Create cvs file containing design matrix\n        np.savetxt(os.path.join(export_dir, self.csv_file),\n                   np.asarray(self.matrix), delimiter=\",\")\n\n        if nidm_version['num'] in [\"1.0.0\", \"1.1.0\"]:\n            csv_location = Identifier(\"file://./\" + self.csv_file)\n        else:\n            csv_location = Identifier(self.csv_file)\n\n        attributes = [(PROV['type'], self.type),\n                      (PROV['label'], self.label),\n                      (NIDM_REGRESSOR_NAMES, json.dumps(self.regressors)),\n                      (DCT['format'], \"text/csv\"),\n                      (NFO['fileName'], self.filename),\n                      (DC['description'], self.image.id),\n                      (PROV['location'], csv_location)]\n\n        if self.hrf_models is not None:\n            if nidm_version['num'] in (\"1.0.0\", \"1.1.0\"):\n                if self.design_type is not None:\n                    attributes.append(\n                        (NIDM_HAS_FMRI_DESIGN, self.design_type))\n                else:\n                    warnings.warn(\"Design type is missing\")\n\n            # hrf model\n            for hrf_model in self.hrf_models:\n                attributes.append((NIDM_HAS_HRF_BASIS, hrf_model))\n            # drift model\n            if self.drift_model is not None:\n                attributes.append((NIDM_HAS_DRIFT_MODEL, self.drift_model.id))\n\n        # Create \"design matrix\" entity\n        self.add_attributes(attributes)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexports the current object to NIDM.", "response": "def export(self, nidm_version, export_dir):\n        \"\"\"\n        Create prov entities and activities.\n        \"\"\"\n        attributes = [(PROV['type'], self.drift_type),\n                      (PROV['label'], self.label)]\n\n        if self.drift_type == FSL_GAUSSIAN_RUNNING_LINE_DRIFT_MODEL:\n            attributes.append((FSL_DRIFT_CUTOFF_PERIOD, self.parameter))\n\n        if self.drift_type == SPM_DCT_DRIFT_MODEL:\n            attributes.append((SPM_SPMS_DRIFT_CUT_OFF_PERIOD, self.parameter))\n\n        # Create \"drift model\" entity\n        self.add_attributes(attributes)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nexports this object into the export_dir directory.", "response": "def export(self, nidm_version, export_dir):\n        \"\"\"\n        Create prov entities and activities.\n        \"\"\"\n        if nidm_version['major'] < 1 or \\\n                (nidm_version['major'] == 1 and nidm_version['minor'] < 3):\n            self.type = NIDM_DATA_SCALING\n    # Create \"Data\" entity\n        # FIXME: grand mean scaling?\n        # FIXME: medianIntensity\n        self.add_attributes((\n            (PROV['type'], self.type),\n            (PROV['type'], PROV['Collection']),\n            (PROV['label'], self.label),\n            (NIDM_GRAND_MEAN_SCALING, self.grand_mean_sc),\n            (NIDM_TARGET_INTENSITY, self.target_intensity)))\n\n        if nidm_version['major'] > 1 or \\\n                (nidm_version['major'] == 1 and nidm_version['minor'] > 2):\n            if self.mri_protocol is not None:\n                self.add_attributes(\n                    [(NIDM_HAS_MRI_PROTOCOL, self.mri_protocol)])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nexporting this error model into a NIDM file.", "response": "def export(self, nidm_version, export_dir):\n        \"\"\"\n        Create prov entities and activities.\n        \"\"\"\n        atts = (\n            (PROV['type'], NIDM_ERROR_MODEL),\n            (NIDM_HAS_ERROR_DISTRIBUTION, self.error_distribution),\n            (NIDM_ERROR_VARIANCE_HOMOGENEOUS, self.variance_homo),\n            (NIDM_VARIANCE_SPATIAL_MODEL, self.variance_spatial),\n            (NIDM_HAS_ERROR_DEPENDENCE, self.dependance))\n\n        # If the error covariance is independent then there is no associated\n        # spatial model\n        if self.dependance_spatial is not None:\n            atts = atts + (\n                ((NIDM_DEPENDENCE_SPATIAL_MODEL, self.dependance_spatial),))\n\n        # Create \"Error Model\" entity\n        self.add_attributes(atts)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef export(self, nidm_version, export_dir):\n        # Create \"Model Parameter estimation\" activity\n        self.add_attributes((\n            (PROV['type'], self.type),\n            (NIDM_WITH_ESTIMATION_METHOD, self.estimation_method),\n            (PROV['label'], self.label)))", "response": "Export this object to NIDM."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef export(self, nidm_version, export_dir):\n\n        if self.masked_median is None:\n            grand_mean_file = self.file.path\n            grand_mean_img = nib.load(grand_mean_file)\n            grand_mean_data = grand_mean_img.get_data()\n            grand_mean_data = np.ndarray.flatten(grand_mean_data)\n\n            mask_img = nib.load(self.mask_file)\n            mask_data = mask_img.get_data()\n            mask_data = np.ndarray.flatten(mask_data)\n\n            grand_mean_data_in_mask = grand_mean_data[mask_data > 0]\n            self.masked_median = np.median(\n                np.array(grand_mean_data_in_mask, dtype=float))\n\n        self.add_attributes((\n            (PROV['type'], self.type),\n            (PROV['label'], self.label),\n            (NIDM_MASKED_MEDIAN, self.masked_median),\n            (NIDM_IN_COORDINATE_SPACE, self.coord_space.id))\n        )", "response": "Export the prov entities and activities."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a boolean to indicate if this is a valid instance of the type.", "response": "def validate(self, text, **kwargs):\n        \"\"\"Returns a boolean to indicate if this is a valid instance of\n        the type.\"\"\"\n        cleaned = self.clean(text, **kwargs)\n        return cleaned is not None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a more clean but still user - facing version of an anonymized anonym", "response": "def clean(self, text, **kwargs):\n        \"\"\"Create a more clean, but still user-facing version of an\n        instance of the type.\"\"\"\n        text = stringify(text)\n        if text is not None:\n            return self.clean_text(text, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef normalize(self, text, cleaned=False, **kwargs):\n        if not cleaned:\n            text = self.clean(text, **kwargs)\n        return ensure_list(text)", "response": "Create a represenation ideal for comparisons but not to be\n            shown to the user."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef validate(self, obj, **kwargs):\n        text = stringify(obj)\n        if text is None:\n            return False\n        if '.' not in text:\n            return False\n        if '@' in text or ':' in text:\n            return False\n        if len(text) < 4:\n            return False\n        return True", "response": "Check if a thing is a valid domain name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntrying to extract only the domain bit from the domain bit from the domain bit.", "response": "def clean_text(self, domain, **kwargs):\n        \"\"\"Try to extract only the domain bit from the \"\"\"\n        try:\n            # handle URLs by extracting the domain name\n            domain = urlparse(domain).hostname or domain\n            domain = domain.lower()\n            # get rid of port specs\n            domain = domain.rsplit(':', 1)[0]\n            domain = domain.rstrip('.')\n            # handle unicode\n            domain = domain.encode(\"idna\").decode('ascii')\n        except ValueError:\n            return None\n        if self.validate(domain):\n            return domain"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nexport the entry to the given directory.", "response": "def export(self, nidm_version, export_dir):\n        \"\"\"\n        Create prov entities and activities.\n        \"\"\"\n\n        # In FSL we have a single thresholding (extent, height) applied to all\n        # contrasts\n        # FIXME: Deal with two-tailed inference?\n        atts = (\n            (PROV['type'], self.type),\n            (PROV['label'], self.label),\n            (NIDM_HAS_ALTERNATIVE_HYPOTHESIS, self.tail))\n\n        if self.partial_degree is not None:\n            atts += (\n                (SPM_PARTIAL_CONJUNCTION_DEGREE, self.partial_degree),)\n\n        self.add_attributes(atts)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef export(self, nidm_version, export_dir):\n        # Create \"Excursion set\" entity\n        self.add_attributes((\n            (PROV['type'], self.type),\n            (NIDM_IN_COORDINATE_SPACE, self.coord_space.id),\n            (PROV['label'], self.label),\n        ))\n\n        if self.visu is not None:\n            self.add_attributes((\n                (DC['description'], self.visu.id),\n            ))\n\n        if self.clust_map is not None:\n            self.add_attributes((\n                (NIDM_HAS_CLUSTER_LABELS_MAP, self.clust_map.id),\n            ))\n\n        if self.mip is not None:\n            self.add_attributes((\n                (NIDM_HAS_MAXIMUM_INTENSITY_PROJECTION, self.mip.id),\n            ))\n\n        if self.num_clusters is not None:\n            self.add_attributes((\n                (NIDM_NUMBER_OF_CLUSTERS, self.num_clusters),\n            ))\n\n        if self.p_value is not None:\n            self.add_attributes((\n                (NIDM_P_VALUE, self.p_value),\n            ))", "response": "Export the current object to NIDM."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nexport the NIDM entity to NIDM.", "response": "def export(self, nidm_version, export_dir):\n        \"\"\"\n        Create prov entities and activities.\n        \"\"\"\n        # Create \"Cluster Labels Map\" entity\n        self.add_attributes((\n            (PROV['type'], self.type),\n            (NIDM_IN_COORDINATE_SPACE, self.coord_space.id),\n            (PROV['label'], self.label)\n        ))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef export(self, version, export_dir):\n\n        atts = [\n            (PROV['type'], self.type),\n            (PROV['label'], self.label),\n        ]\n\n        if version['num'] == \"1.0.0\":\n            atts += [\n                (NIDM_USER_SPECIFIED_THRESHOLD_TYPE, self.user_threshold_type),\n                (PROV['value'], self.stat_threshold),\n                (NIDM_P_VALUE_UNCORRECTED, self.p_uncorr_threshold),\n                (NIDM_P_VALUE_FWER, self.p_corr_threshold)\n                ]\n        else:\n            atts += [\n                (PROV['type'], self.threshold_type),\n                (PROV['value'], self.value)\n                ]\n\n        if self.equiv_thresh is not None:\n            for equiv in self.equiv_thresh:\n                atts += [\n                    (NIDM_EQUIVALENT_THRESHOLD, equiv.id)\n                ]\n\n        self.add_attributes([(k, v) for k, v in atts if v is not None])", "response": "Export the current state of the current prov entity into the export_dir."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef export(self, nidm_version, export_dir):\n        if nidm_version['num'] in [\"1.0.0\", \"1.1.0\"]:\n            self.label = self.label.replace(\"Supra-Threshold\", \"Significant\")\n\n        # FIXME deal with multiple contrasts\n        atts = (\n            (PROV['type'], NIDM_SIGNIFICANT_CLUSTER),\n            (PROV['label'], self.label),\n            (NIDM_CLUSTER_LABEL_ID, self.num),\n            (NIDM_CLUSTER_SIZE_IN_VOXELS, self.size)\n            )\n\n        if self.clust_size_resels is not None:\n            atts = atts + (\n                (NIDM_CLUSTER_SIZE_IN_RESELS, self.clust_size_resels),\n                )\n\n        if self.punc is not None:\n            atts = atts + (\n                (NIDM_P_VALUE_UNCORRECTED,\n                    Literal(self.punc, datatype=XSD_FLOAT)),\n                )\n\n        if self.pFDR is not None:\n            atts = atts + (\n                (NIDM_Q_VALUE_FDR, Literal(self.pFDR, datatype=XSD_FLOAT)),\n                )\n\n        if self.pFWER is not None:\n            atts = atts + (\n                (NIDM_P_VALUE_FWER, Literal(self.pFWER, datatype=XSD_FLOAT)),\n                )\n\n        self.add_attributes(atts)", "response": "Export this entry into the nIDM."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nexporting this object into NIDM entities and activities.", "response": "def export(self, nidm_version, export_dir):\n        \"\"\"\n        Create prov entities and activities.\n        \"\"\"\n        num_peak = ()\n        if self.num_peak:\n            num_peak = ((NIDM_MAX_NUMBER_OF_PEAKS_PER_CLUSTER, self.num_peak),)\n\n        # Create \"Peak definition criteria\" entity\n        self.add_attributes((\n            (PROV['type'], self.type),\n            (PROV['label'], self.label),\n            (NIDM_MIN_DISTANCE_BETWEEN_PEAKS, self.peak_dist)\n            ) + num_peak)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef export(self, nidm_version, export_dir):\n        # Create \"Cluster definition criteria\" entity\n        if isinstance(self.connectivity, int):\n            if self.connectivity == 6:\n                self.connectivity = NIDM_VOXEL6CONNECTED\n            elif self.connectivity == 18:\n                self.connectivity = NIDM_VOXEL18CONNECTED\n            elif self.connectivity == 26:\n                self.connectivity = NIDM_VOXEL26CONNECTED\n\n        # FIXME if connectivity is missing\n        if self.connectivity is not None:\n            atts = (\n                (PROV['type'], self.type),\n                (PROV['label'], self.label),\n                (NIDM_HAS_CONNECTIVITY_CRITERION, self.connectivity))\n        else:\n            atts = (\n                (PROV['type'], NIDM_CLUSTER_DEFINITION_CRITERIA),\n                (PROV['label'], label))\n\n        self.add_attributes(atts)", "response": "Export the current object to NIDM."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef export(self, version, export_dir):\n        atts = (\n            (PROV['label'], self.label),\n            (PROV['type'], NIDM_SEARCH_SPACE_MASK_MAP),\n            (NIDM_RANDOM_FIELD_STATIONARITY, self.rf_stationarity),\n            (NIDM_IN_COORDINATE_SPACE, self.coord_space.id),\n            (NIDM_SEARCH_VOLUME_IN_VOXELS, self.search_volume_in_voxels),\n            (NIDM_SEARCH_VOLUME_IN_UNITS, self.search_volume_in_units),\n            (NIDM_SEARCH_VOLUME_IN_RESELS, self.search_volume_in_resels),\n            (NIDM_RESEL_SIZE_IN_VOXELS, self.resel_size_in_voxels))\n\n        # Noise FWHM was introduced in NIDM-Results 1.1.0\n        if self.noise_fwhm_in_voxels is not None:\n            if (version['major'] > 1) or \\\n               (version['major'] >= 1 and\n                    (version['minor'] > 0 or version['revision'] > 0)):\n                atts = atts + (\n                    (NIDM_NOISE_FWHM_IN_VOXELS, self.noise_fwhm_in_voxels),\n                    (NIDM_NOISE_FWHM_IN_UNITS, self.noise_fwhm_in_units))\n\n        if self.expected_num_voxels is not None:\n            atts = atts + ((NIDM_EXPECTED_NUMBER_OF_VOXELS_PER_CLUSTER,\n                            self.expected_num_voxels),)\n\n        if self.expected_num_clusters is not None:\n            atts = atts + ((NIDM_EXPECTED_NUMBER_OF_CLUSTERS,\n                            self.expected_num_clusters),)\n\n        if self.height_critical_fwe05 is not None:\n            atts = atts + ((NIDM_HEIGHT_CRITICAL_THRESHOLD_FWE_05,\n                            self.height_critical_fwe05),)\n\n        if self.height_critical_fdr05 is not None:\n            atts = atts + ((NIDM_HEIGHT_CRITICAL_THRESHOLD_FDR_05,\n                            self.height_critical_fdr05),)\n\n        if self.extent_critical_fwe05 is not None:\n            atts = atts + ((\n                SPM_SMALLEST_SIGNIFICANT_CLUSTER_SIZE_IN_VOXELS_FWE05,\n                self.extent_critical_fwe05),)\n\n        if self.extent_critical_fdr05 is not None:\n            atts = atts + ((\n                SPM_SMALLEST_SIGNIFICANT_CLUSTER_SIZE_IN_VOXELS_FDR05,\n                self.extent_critical_fdr05),)\n\n        if self.search_vol_geom is not None:\n            atts = atts + ((SPM_SEARCH_VOLUME_RESELS_GEOMETRY,\n                            self.search_vol_geom),)\n\n        if self.noise_roughness:\n            atts = atts + ((NIDM_NOISE_ROUGHNESS_IN_VOXELS,\n                            self.noise_roughness),)\n\n        # Create \"Search Space Mask map\" entity\n        self.add_attributes(atts)", "response": "Export the current prov entities and activities."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef export(self, nidm_version, export_dir):\n        # We can not have this as a dictionnary because we want to keep the\n        # duplicate prov:type attribute\n        atts = (  # (PROV['type'],PROV['Location']),\n            (PROV['type'], NIDM_COORDINATE),\n            (PROV['type'], PROV['Location']),\n            (PROV['label'], self.label)\n            )\n\n        if self.coord_vector is not None:\n            atts = atts +\\\n                ((NIDM_COORDINATE_VECTOR_IN_VOXELS,\n                  json.dumps(self.coord_vector)),)\n\n        # FSL unnormalised subject-level analyses do not provide coordinates in\n        # voxels\n        if self.coord_vector_std is not None:\n            atts = atts +\\\n                ((NIDM_COORDINATE_VECTOR, json.dumps(self.coord_vector_std)),)\n\n        self.add_attributes(atts)", "response": "Export the object to NIDM."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef export(self, nidm_version, export_dir):\n        if self.p_unc is None:\n            norm_cdf_z = (1.0 + erf(self.equiv_z / sqrt(2.0))) / 2.0\n            self.p_unc = 1 - norm_cdf_z\n\n        atts = (\n            (PROV['type'], self.type),\n            (PROV['label'], self.label),\n            (PROV['location'], self.coordinate.id))\n\n        if self.value is not None:\n            atts = atts + (\n                (PROV['value'], self.value),\n                )\n\n        if self.p_unc is not None:\n            atts = atts + (\n                (NIDM_P_VALUE_UNCORRECTED,\n                 Literal(self.p_unc, datatype=XSD_FLOAT)),\n                )\n\n        if self.equiv_z is not None:\n            atts = atts + (\n                (NIDM_EQUIVALENT_ZSTATISTIC,\n                 Literal(self.equiv_z, datatype=XSD_FLOAT)),\n                )\n\n        if self.p_fdr is not None:\n            atts = atts + (\n                (NIDM_Q_VALUE_FDR,\n                 Literal(self.p_fdr, datatype=XSD_FLOAT)),\n                )\n\n        if self.p_fwer is not None:\n            atts = atts + (\n                (NIDM_P_VALUE_FWER,\n                 Literal(self.p_fwer, datatype=XSD_FLOAT)),\n                )\n\n        self.add_attributes(atts)", "response": "Exports the current object to NIDM."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nloading NIDM - Results file given filename guessing if it is a NIDM - Results pack or a JSON file", "response": "def load(filename, to_replace=dict()):\n    ''' Load NIDM-Results file given filename, guessing if it is a\n    NIDM-Results pack or a JSON file\n\n    Parameters\n    ----------\n    filename : string\n       specification of file to load\n    Returns\n    -------\n    nidmres : ``NIDMResults``\n       NIDM-Results object\n    '''\n    if not os.path.exists(filename):\n        raise IOException('File does not exist: %s' % filename)\n\n    if filename.endswith('.json'):\n        raise Exception('Minimal json file: not handled yet')\n    elif filename.endswith('.nidm.zip'):\n        nidm = NIDMResults.load_from_pack(filename, to_replace=to_replace)\n    else:\n        raise Exception('Unhandled format ' + filename)\n\n    return nidm"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef export(self, nidm_version, export_dir):\n        self.stat = None\n        if isinstance(self.stat_type, QualifiedName):\n            stat = self.stat_type\n        elif self.stat_type is not None:\n            if self.stat_type.lower() == \"t\":\n                stat = STATO_TSTATISTIC\n            elif self.stat_type.lower() == \"z\":\n                stat = STATO_ZSTATISTIC\n            elif self.stat_type.lower() == \"f\":\n                stat = STATO_FSTATISTIC\n            elif self.stat_type.startswith('http'):\n                stat = Identifier(self.stat_type)\n\n        self.add_attributes((\n            (PROV['type'], STATO_CONTRAST_WEIGHT_MATRIX),\n            (NIDM_STATISTIC_TYPE, stat),\n            (PROV['label'], self.label),\n            (NIDM_CONTRAST_NAME, self.contrast_name),\n            (PROV['value'], json.dumps(self.contrast_weights))))", "response": "Export the current object to prov graph."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef export(self, nidm_version, export_dir):\n        # Contrast Map entity\n        atts = (\n            (PROV['type'], NIDM_CONTRAST_MAP),\n            (NIDM_CONTRAST_NAME, self.name))\n\n        if not self.isderfrommap:\n            atts = atts + (\n                (NIDM_IN_COORDINATE_SPACE, self.coord_space.id),)\n\n        if self.label is not None:\n            atts = atts + (\n                (PROV['label'], self.label),)\n\n        if self.name is not None:\n            atts = atts + (\n                (NIDM_CONTRAST_NAME, self.name),)\n\n        # Parameter estimate entity\n        self.add_attributes(atts)", "response": "Export this object to NIDM."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef export(self, nidm_version, export_dir):\n        if self.expl_mean_sq_file is None:\n            # Create Contrast Explained Mean Square Map as fstat<num>.nii.gz\n            # multiplied by sigmasquareds.nii.gz and save it in export_dir\n            fstat_img = nib.load(self.stat_file)\n            fstat = fstat_img.get_data()\n\n            sigma_sq_img = nib.load(self.sigma_sq_file)\n            sigma_sq = sigma_sq_img.get_data()\n\n            expl_mean_sq = nib.Nifti1Image(\n                fstat*sigma_sq, fstat_img.get_qform())\n\n            self.filename = (\"ContrastExplainedMeanSquareMap\" +\n                             self.num + \".nii.gz\")\n            self.expl_mean_sq_file = os.path.join(\n                export_dir, self.filename)\n            nib.save(expl_mean_sq, self.expl_mean_sq_file)\n\n        self.file = NIDMFile(self.id, self.expl_mean_sq_file,\n                             filename=self.filename,\n                             sha=self.sha, fmt=self.fmt)\n\n        # Contrast Explained Mean Square Map entity\n        path, filename = os.path.split(self.expl_mean_sq_file)\n        self.add_attributes((\n            (PROV['type'], self.type),\n            (NIDM_IN_COORDINATE_SPACE, self.coord_space.id),\n            (PROV['label'], self.label)))", "response": "Export the Contrast Explained Mean Square Map into a NIDMFile object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef export(self, nidm_version, export_dir):\n        attributes = [(PROV['type'], NIDM_STATISTIC_MAP),\n                      (DCT['format'], self.fmt)]\n\n        if not self.isderfrommap:\n            attributes.insert(0, (\n                NIDM_IN_COORDINATE_SPACE,  self.coord_space.id))\n            attributes.insert(0, (PROV['label'], self.label))\n\n        if not self.stat_type == 'Z':\n            attributes.insert(0, (NIDM_ERROR_DEGREES_OF_FREEDOM, self.dof))\n            attributes.insert(0, (NIDM_EFFECT_DEGREES_OF_FREEDOM, self.effdof))\n        else:\n            # For Z-Statistic error dof is infinity and effect dof is 1\n            attributes.insert(0, (NIDM_ERROR_DEGREES_OF_FREEDOM, float(\"inf\")))\n            attributes.insert(0, (NIDM_EFFECT_DEGREES_OF_FREEDOM, self.effdof))\n\n        if self.stat is not None:\n            attributes.insert(0, (NIDM_STATISTIC_TYPE, self.stat))\n\n        if self.contrast_name is not None:\n            attributes.insert(0, (NIDM_CONTRAST_NAME, self.contrast_name))\n\n        # Create \"Statistic Map\" entity\n        # FIXME: Deal with other than t-contrast maps: dof + statisticType\n        self.add_attributes(attributes)", "response": "Export the current state of the object into the NIDM."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a sequence of Tokens.", "response": "def get_tokens(source):\n    \"\"\"Returns a sequence of Tokens.\n\n    Args:\n      source: string of C++ source code.\n\n    Yields:\n      Token that represents the next token in the source.\n    \"\"\"\n    if not source.endswith('\\n'):\n        source += '\\n'\n\n    # Cache various valid character sets for speed.\n    valid_identifier_first_chars = VALID_IDENTIFIER_FIRST_CHARS\n    valid_identifier_chars = VALID_IDENTIFIER_CHARS\n    hex_digits = HEX_DIGITS\n    int_or_float_digits = INT_OR_FLOAT_DIGITS\n    int_or_float_digits2 = int_or_float_digits | set('.')\n\n    # Ignore tokens while in a #if 0 block.\n    count_ifs = 0\n\n    i = 0\n    end = len(source)\n    while i < end:\n        # Skip whitespace.\n        while i < end and source[i].isspace():\n            i += 1\n        if i >= end:\n            return\n\n        token_type = UNKNOWN\n        start = i\n        c = source[i]\n        # Find a string token.\n        if c in valid_identifier_first_chars or c == '_':\n            token_type = NAME\n            while source[i] in valid_identifier_chars:\n                i += 1\n            # String and character constants can look like a name if\n            # they are something like L\"\".\n            if source[i] == \"'\" and source[start:i] in _STR_PREFIXES:\n                token_type = CONSTANT\n                i = _get_char(source, start, i)\n            elif source[i] == '\"' and source[start:i] in _STR_PREFIXES:\n                token_type = CONSTANT\n                i = _get_string(source, i)\n        elif c == '/' and source[i + 1] == '/':  # Find // comments.\n            i = _find(source, '\\n', i)\n            continue\n        elif c == '/' and source[i + 1] == '*':  # Find /* comments. */\n            i = _find(source, '*/', i) + 2\n            continue\n        elif c in '<>':                          # Handle '<' and '>' tokens.\n            token_type = SYNTAX\n            i += 1\n            new_ch = source[i]\n            # Do not merge '>>' or '>>=' into a single token\n            if new_ch == c and c != '>':\n                i += 1\n                new_ch = source[i]\n            if new_ch == '=':\n                i += 1\n        elif c in ':+-&|=':                      # Handle 'XX' and 'X=' tokens.\n            token_type = SYNTAX\n            i += 1\n            new_ch = source[i]\n            if new_ch == c:\n                i += 1\n            elif c == '-' and new_ch == '>':\n                i += 1\n            elif new_ch == '=':\n                i += 1\n        elif c in '!*^%/':                       # Handle 'X=' tokens.\n            token_type = SYNTAX\n            i += 1\n            new_ch = source[i]\n            if new_ch == '=':\n                i += 1\n        elif c in '()[]{}~?;.,':                 # Handle single char tokens.\n            token_type = SYNTAX\n            i += 1\n            if c == '.' and source[i].isdigit():\n                token_type = CONSTANT\n                i += 1\n                while source[i] in int_or_float_digits:\n                    i += 1\n                # Handle float suffixes.\n                for suffix in ('l', 'f'):\n                    if suffix == source[i:i + 1].lower():\n                        i += 1\n                        break\n        elif c.isdigit():                        # Find integer.\n            token_type = CONSTANT\n            if c == '0' and source[i + 1] in 'xX':\n                # Handle hex digits.\n                i += 2\n                while source[i] in hex_digits:\n                    i += 1\n            else:\n                while source[i] in int_or_float_digits2:\n                    i += 1\n            # Handle integer (and float) suffixes.\n            if source[i].isalpha():\n                for suffix in ('ull', 'll', 'ul', 'l', 'f', 'u'):\n                    size = len(suffix)\n                    if suffix == source[i:i + size].lower():\n                        i += size\n                        break\n        elif c == '\"':                           # Find string.\n            token_type = CONSTANT\n            i = _get_string(source, i)\n        elif c == \"'\":                           # Find char.\n            token_type = CONSTANT\n            i = _get_char(source, start, i)\n        elif c == '#':                           # Find pre-processor command.\n            token_type = PREPROCESSOR\n            got_if = source[i:i + 3] == '#if'\n            if count_ifs and source[i:i + 6] == '#endif':\n                count_ifs -= 1\n                if count_ifs == 0:\n                    source = source[:i].ljust(i + 6) + source[i + 6:]\n                    continue\n\n            # Handle preprocessor statements (\\ continuations).\n            while True:\n                i1 = source.find('\\n', i)\n                i2 = source.find('//', i)\n                i3 = source.find('/*', i)\n                i4 = source.find('\"', i)\n                # Get the first important symbol (newline, comment, EOF/end).\n                i = min([x for x in (i1, i2, i3, i4, end) if x != -1])\n\n                # Handle comments in #define macros.\n                if i == i3:\n                    i = _find(source, '*/', i) + 2\n                    source = source[:i3].ljust(i) + source[i:]\n                    continue\n\n                # Handle #include \"dir//foo.h\" properly.\n                if source[i] == '\"':\n                    i = _find(source, '\"', i + 1) + 1\n                    continue\n\n                # Keep going if end of the line and the line ends with \\.\n                if i == i1 and source[i - 1] == '\\\\':\n                    i += 1\n                    continue\n\n                if got_if:\n                    begin = source.find('(', start, i)\n                    if begin == -1:\n                        begin = source.find(' ', start)\n                    begin = begin + 1\n                    s1 = source.find(' ', begin)\n                    s2 = source.find(')', begin)\n                    s3 = source.find('\\n', begin)\n                    s = min([x for x in (s1, s2, s3, end) if x != -1])\n\n                    condition = source[begin:s]\n                    if (\n                        count_ifs or\n                        condition == '0' or\n                        condition == '__OBJC__'\n                    ):\n                        count_ifs += 1\n                break\n        elif c == '\\\\':                          # Handle \\ in code.\n            # This is different from the pre-processor \\ handling.\n            i += 1\n            continue\n        elif count_ifs:\n            # Ignore bogus code when we are inside an #if block.\n            i += 1\n            continue\n        else:\n            raise TokenError(\"unexpected token '{0}'\".format(c))\n\n        if count_ifs:\n            continue\n\n        assert i > 0\n        yield Token(token_type, source[start:i], start, i)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _find(string, sub_string, start_index):\n    result = string.find(sub_string, start_index)\n    if result == -1:\n        raise TokenError(\"expected '{0}'\".format(sub_string))\n    return result", "response": "Find the index of sub_string in string."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_string(self):\n        suffix = '%s %s' % (self.type, self.name)\n        if self.initial_value:\n            suffix += ' = ' + self.initial_value\n        return suffix", "response": "Return a string that tries to reconstitute the variable decl."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting a list of tokens into a list of types.", "response": "def to_type(self, tokens):\n        \"\"\"Convert [Token,...] to [Class(...), ] useful for base classes.\n\n        For example, code like class Foo : public Bar<x, y> { ... };\n        the \"Bar<x, y>\" portion gets converted to an AST.\n\n        Returns:\n          [Class(...), ...]\n        \"\"\"\n        result = []\n        name_tokens = []\n        reference = pointer = array = False\n        inside_array = False\n        empty_array = True\n        templated_tokens = []\n\n        def add_type():\n            if not name_tokens:\n                return\n\n            # Partition tokens into name and modifier tokens.\n            names = []\n            modifiers = []\n            for t in name_tokens:\n                if keywords.is_keyword(t.name):\n                    modifiers.append(t.name)\n                else:\n                    names.append(t.name)\n            name = ''.join(names)\n\n            templated_types = self.to_type(templated_tokens)\n            result.append(Type(name_tokens[0].start, name_tokens[-1].end,\n                               name, templated_types, modifiers,\n                               reference, pointer, array))\n            del name_tokens[:]\n            del templated_tokens[:]\n\n        i = 0\n        end = len(tokens)\n        while i < end:\n            token = tokens[i]\n            if token.name == ']':\n                inside_array = False\n                if empty_array:\n                    pointer = True\n                else:\n                    array = True\n            elif inside_array:\n                empty_array = False\n            elif token.name == '<':\n                templated_tokens, i = self._get_template_end(tokens, i + 1)\n                continue\n            elif token.name == ',' or token.name == '(':\n                add_type()\n                reference = pointer = array = False\n                empty_array = True\n            elif token.name == '*':\n                pointer = True\n            elif token.name == '&':\n                reference = True\n            elif token.name == '[':\n                inside_array = True\n            elif token.name != ')':\n                name_tokens.append(token)\n            i += 1\n\n        add_type()\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_name(self, seq=None):\n        if seq is not None:\n            it = iter(seq)\n\n            def get_next_token():\n                return next(it)\n        else:\n            get_next_token = self._get_next_token\n\n        next_token = get_next_token()\n        tokens = []\n        last_token_was_name = False\n        while (next_token.token_type == tokenize.NAME or\n               (next_token.token_type == tokenize.SYNTAX and\n                next_token.name in ('::', '<'))):\n            # Two NAMEs in a row means the identifier should terminate.\n            # It's probably some sort of variable declaration.\n            if last_token_was_name and next_token.token_type == tokenize.NAME:\n                break\n            last_token_was_name = next_token.token_type == tokenize.NAME\n            tokens.append(next_token)\n            # Handle templated names.\n            if next_token.name == '<':\n                tokens.extend(self._get_matching_char('<', '>',\n                                                      get_next_token))\n                last_token_was_name = True\n            next_token = get_next_token()\n        return tokens, next_token", "response": "Returns a list of tokens and next_token_info for the name of the current module."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _lookup_namespace(self, symbol, namespace):\n        for namespace_part in symbol.parts:\n            namespace = namespace.get(namespace_part)\n            if namespace is None:\n                break\n            if not isinstance(namespace, dict):\n                return namespace\n        raise Error('%s not found' % symbol.name)", "response": "Helper for lookup_symbol that only looks up variables in a\n ArcGIS namespace."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _lookup_global(self, symbol):\n        assert symbol.parts\n        namespace = self.namespaces\n        if len(symbol.parts) == 1:\n            # If there is only one part, look in globals.\n            namespace = self.namespaces[None]\n        try:\n            # Try to do a normal, global namespace lookup.\n            return self._lookup_namespace(symbol, namespace)\n        except Error as orig_exc:\n            try:\n                # The normal lookup can fail if all of the parts aren't\n                # namespaces. This happens with OuterClass::Inner.\n                namespace = self.namespaces[None]\n                return self._lookup_namespace(symbol, namespace)\n            except Error:\n                raise orig_exc", "response": "Helper for lookup_symbol that only looks up global variables."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef lookup_symbol(self, name, namespace_stack):\n        # TODO(nnorwitz): a convenient API for this depends on the\n        # representation of the name. e.g., does symbol_name contain\n        # ::, is symbol_name a list of colon separated names, how are\n        # names prefixed with :: handled. These have different lookup\n        # semantics (if leading ::) or change the desirable API.\n\n        # For now assume that the symbol_name contains :: and parse it.\n        symbol = Symbol(name, name.split('::'), namespace_stack)\n        assert symbol.parts\n        if symbol.parts[0] == '':\n            # Handle absolute (global) ::symbol_names.\n            symbol.parts = symbol.parts[1:]\n        elif namespace_stack is not None:\n            result = self._lookup_in_all_namespaces(symbol)\n            if result:\n                return result\n\n        return self._lookup_global(symbol)", "response": "Returns the AST node and module for the given symbol name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _add(self, symbol_name, namespace, node, module):\n        result = symbol_name in namespace\n        namespace[symbol_name] = node, module\n        return not result", "response": "Helper function for adding symbols.\n        to the namespace."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_symbol(self, symbol_name, namespace_stack, node, module):\n        # TODO(nnorwitz): verify symbol_name doesn't contain :: ?\n        if namespace_stack:\n            # Handle non-global symbols (ie, in some namespace).\n            last_namespace = self.namespaces\n            for namespace in namespace_stack:\n                last_namespace = last_namespace.setdefault(namespace, {})\n        else:\n            last_namespace = self.namespaces[None]\n        return self._add(symbol_name, last_namespace, node, module)", "response": "Adds a symbol to the symbol table."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the prefix of names from name_seq that are known namespaces.", "response": "def get_namespace(self, name_seq):\n        \"\"\"Returns the prefix of names from name_seq that are known namespaces.\n\n        Args:\n          name_seq: ['names', 'of', 'possible', 'namespace', 'to', 'find']\n\n        Returns:\n          ['names', 'that', 'are', 'namespaces', 'possibly', 'empty', 'list']\n        \"\"\"\n        namespaces = self.namespaces\n        result = []\n        for name in name_seq:\n            namespaces = namespaces.get(name)\n            if not namespaces:\n                break\n            result.append(name)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nverifying that all include files that are unnecessary.", "response": "def _verify_include_files_used(self, file_uses, included_files):\n        \"\"\"Find all #include files that are unnecessary.\"\"\"\n        for include_file, use in file_uses.items():\n            if not use & USES_DECLARATION:\n                node, module = included_files[include_file]\n                if module.ast_list is not None:\n                    msg = \"'{}' does not need to be #included\".format(\n                        node.filename)\n                    if use & USES_REFERENCE:\n                        msg += '; use a forward declaration instead'\n                    self._add_warning(msg, node)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfinds all the forward declarations that are not used.", "response": "def _verify_forward_declarations_used(self, forward_declarations,\n                                          decl_uses, file_uses):\n        \"\"\"Find all the forward declarations that are not used.\"\"\"\n        for cls in forward_declarations:\n            if cls in file_uses:\n                if not decl_uses[cls] & USES_DECLARATION:\n                    node = forward_declarations[cls]\n                    msg = (\"'{}' forward declared, \"\n                           'but needs to be #included'.format(cls))\n                    self._add_warning(msg, node)\n            else:\n                if decl_uses[cls] == UNUSED:\n                    node = forward_declarations[cls]\n                    msg = \"'{}' not used\".format(cls)\n                    self._add_warning(msg, node)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting up the use type of each symbol.", "response": "def _determine_uses(self, included_files, forward_declarations):\n        \"\"\"Set up the use type of each symbol.\"\"\"\n        file_uses = dict.fromkeys(included_files, UNUSED)\n        decl_uses = dict.fromkeys(forward_declarations, UNUSED)\n        symbol_table = self.symbol_table\n\n        for name, node in forward_declarations.items():\n            try:\n                symbol_table.lookup_symbol(node.name, node.namespace)\n                decl_uses[name] |= USES_REFERENCE\n            except symbols.Error:\n                module = Module(name, None)\n                symbol_table.add_symbol(node.name, node.namespace, node,\n                                        module)\n\n        def _do_lookup(name, namespace):\n            try:\n                file_use_node = symbol_table.lookup_symbol(name, namespace)\n            except symbols.Error:\n                return\n            name = file_use_node[1].filename\n            file_uses[name] = file_uses.get(name, 0) | USES_DECLARATION\n\n        def _add_declaration(name, namespace):\n            if not name:\n                # Ignore anonymous struct. It is not standard, but we might as\n                # well avoid crashing if it is easy.\n                return\n\n            names = [n for n in namespace if n is not None]\n            if names:\n                name = '::'.join(names) + '::' + name\n            if name in decl_uses:\n                decl_uses[name] |= USES_DECLARATION\n\n        def _add_reference(name, namespace):\n            try:\n                file_use_node = symbol_table.lookup_symbol(name, namespace)\n            except symbols.Error:\n                return\n\n            name = file_use_node[1].filename\n            if file_use_node[1].ast_list is None:\n                decl_uses[name] |= USES_REFERENCE\n            elif name in file_uses:\n                # enum and typedef can't be forward declared\n                if isinstance(file_use_node[0], (ast.Enum, ast.Typedef)):\n                    file_uses[name] |= USES_DECLARATION\n                else:\n                    file_uses[name] |= USES_REFERENCE\n\n        def _add_use(node, namespace, name=''):\n            if isinstance(node, basestring):\n                name = node\n            elif isinstance(node, list):\n                # name contains a list of tokens.\n                name = '::'.join([n.name for n in name])\n\n            # node is a Type so look for its symbol immediately.\n            if name:\n                _do_lookup(name, namespace)\n                return\n\n            # Try to search for the value of the variable declaration for any\n            # symbols, such as `#define` values or other variable names which\n            # may be included in other files.\n            obj = getattr(node, 'initial_value', None)\n            if obj:\n                _do_lookup(obj, namespace)\n\n            # If node is a VariableDeclaration, check if the variable type is\n            # a symbol used in other includes.\n            obj = getattr(node, 'type', None)\n            if obj and isinstance(obj.name, basestring):\n                _do_lookup(obj.name, namespace)\n\n            if not isinstance(node, basestring):\n                # Happens when variables are defined with inlined types, e.g.:\n                #   enum {...} variable;\n                return\n\n        def _add_variable(node, namespace, reference=False):\n            obj = node.type if isinstance(\n                node, ast.VariableDeclaration) else node\n\n            if obj.reference or obj.pointer or reference:\n                _add_reference(obj.name, namespace)\n            else:\n                # Add a use for the variable declaration type as well as the\n                # variable value.\n                _add_use(obj.name, namespace)\n                _add_use(node, namespace)\n            # This needs to recurse when the node is a templated type.\n            _add_template_use(obj.name,\n                              obj.templated_types,\n                              namespace,\n                              reference)\n\n        def _process_function(function, namespace):\n            reference = function.body is None\n            if function.return_type:\n                return_type = function.return_type\n                _add_variable(return_type, namespace, reference)\n\n            for s in function.specializations:\n                _add_variable(s, namespace, not function.body)\n\n            templated_types = function.templated_types or ()\n            for p in function.parameters:\n                node = p.type\n                if node.name not in templated_types:\n                    if function.body and p.name:\n                        # Assume that if the function has a body and a name\n                        # the parameter type is really used.\n                        # NOTE(nnorwitz): this is over-aggressive. It would be\n                        # better to iterate through the body and determine\n                        # actual uses based on local vars and data members\n                        # used.\n                        _add_use(node.name, namespace)\n                    elif (\n                        p.default and\n                        p.default[0].name != '0' and\n                        p.default[0].name != 'NULL' and\n                        p.default[0].name != 'nullptr'\n                    ):\n                        _add_use(node.name, namespace)\n                    elif node.reference or node.pointer or reference:\n                        _add_reference(node.name, namespace)\n                    else:\n                        _add_use(node.name, namespace)\n                    _add_template_use(node.name,\n                                      node.templated_types,\n                                      namespace,\n                                      reference)\n\n        def _process_function_body(function, namespace):\n            previous = None\n            save = namespace[:]\n            for t in function.body:\n                if t.token_type == tokenize.NAME:\n                    previous = t\n                    if not keywords.is_keyword(t.name):\n                        # TODO(nnorwitz): handle static function calls.\n                        # TODO(nnorwitz): handle using statements in file.\n                        # TODO(nnorwitz): handle using statements in function.\n                        # TODO(nnorwitz): handle namespace assignment in file.\n                        _add_use(t.name, namespace)\n                elif t.name == '::' and previous is not None:\n                    namespace.append(previous.name)\n                elif t.name in (':', ';'):\n                    namespace = save[:]\n\n        def _add_template_use(name, types, namespace, reference=False):\n            for cls in types or ():\n                if cls.pointer or cls.reference or reference:\n                    _add_reference(cls.name, namespace)\n                elif name.endswith('_ptr'):\n                    # Special case templated classes that end w/_ptr.\n                    # These are things like auto_ptr which do\n                    # not require the class definition, only decl.\n                    _add_reference(cls.name, namespace)\n                elif name.startswith('Q') and name.endswith('Pointer'):\n                    # Special case templated classes from the Qt framework.\n                    _add_reference(cls.name, namespace)\n                else:\n                    _add_use(cls.name, namespace)\n                _add_template_use(cls.name, cls.templated_types,\n                                  namespace, reference)\n\n        def _process_types(nodes, namespace):\n            for node in nodes:\n                if isinstance(node, ast.Type):\n                    _add_variable(node, namespace)\n\n        # Iterate through the source AST/tokens, marking each symbols use.\n        ast_seq = [self.ast_list]\n        namespace_stack = []\n        while ast_seq:\n            for node in ast_seq.pop():\n                if isinstance(node, ast.VariableDeclaration):\n                    namespace = namespace_stack + node.namespace\n                    _add_variable(node, namespace)\n                elif isinstance(node, ast.Function):\n                    namespace = namespace_stack + node.namespace\n                    _process_function(node, namespace)\n                    if node.body:\n                        _process_function_body(node, namespace)\n                elif isinstance(node, ast.Typedef):\n                    namespace = namespace_stack + node.namespace\n                    _process_types(node.alias, namespace)\n                elif isinstance(node, ast.Friend):\n                    expr = node.expr\n                    namespace = namespace_stack + node.namespace\n                    if isinstance(expr, ast.Type):\n                        _add_reference(expr.name, namespace)\n                    elif isinstance(expr, ast.Function):\n                        _process_function(expr, namespace)\n                elif isinstance(node, ast.Union) and node.body is not None:\n                    ast_seq.append(node.body)\n                elif isinstance(node, ast.Class) and node.body is not None:\n                    _add_declaration(node.name, node.namespace)\n                    namespace = namespace_stack + node.namespace\n                    _add_template_use('', node.bases, namespace)\n                    ast_seq.append(node.body)\n                elif isinstance(node, ast.Using):\n                    if node.names[0].name == 'namespace':\n                        namespace_stack.append(node.names[1].name)\n\n        return file_uses, decl_uses"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nverify all the public functions are also declared in a header file.", "response": "def _check_public_functions(self, primary_header, all_headers):\n        \"\"\"Verify all the public functions are also declared in a header\n        file.\"\"\"\n        public_symbols = {}\n        declared_only_symbols = {}\n        if primary_header:\n            for name, symbol in primary_header.public_symbols.items():\n                if isinstance(symbol, ast.Function):\n                    public_symbols[name] = symbol\n            declared_only_symbols = dict.fromkeys(public_symbols, True)\n\n        for node in self.ast_list:\n            # Make sure we have a function that should be exported.\n            if not isinstance(node, ast.Function):\n                continue\n            if isinstance(node, ast.Method):\n                # Ensure that for Foo::Bar, Foo is *not* a namespace.\n                # If Foo is a namespace, we have a function and not a method.\n                names = [n.name for n in node.in_class]\n                if names != self.symbol_table.get_namespace(names):\n                    continue\n            if not (node.is_definition() and node.is_exportable()):\n                continue\n\n            # This function should be declared in a header file.\n            name = node.name\n            if name in public_symbols:\n                declared_only_symbols[name] = False\n            else:\n                self._find_public_function_warnings(node,\n                                                    name,\n                                                    primary_header,\n                                                    all_headers)\n\n        for name, declared_only in declared_only_symbols.items():\n            if declared_only:\n                node = public_symbols[name]\n                if node.templated_types is None:\n                    msg = \"'{}' declared but not defined\".format(name)\n                    self._add_warning(msg, node, primary_header.filename)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfinds unused static variables.", "response": "def _find_unused_static_warnings(filename, lines, ast_list):\n    \"\"\"Warn about unused static variables.\"\"\"\n    static_declarations = dict(_get_static_declarations(ast_list))\n\n    def find_variables_use(body):\n        for child in body:\n            if child.name in static_declarations:\n                static_use_counts[child.name] += 1\n\n    static_use_counts = collections.Counter()\n    for node in ast_list:\n        if isinstance(node, ast.Function) and node.body:\n            find_variables_use(node.body)\n        elif isinstance(node, ast.Class) and node.body:\n            for child in node.body:\n                if isinstance(child, ast.Function) and child.body:\n                    find_variables_use(child.body)\n\n    count = 0\n    for (name, _) in sorted(static_declarations.items(),\n                            key=lambda x: x[1].start):\n        if not static_use_counts[name]:\n            print(\"{}:{}: unused variable '{}'\".format(\n                filename,\n                lines.get_line_number(static_declarations[name].start),\n                name))\n            count += 1\n\n    return count"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read_file(filename, print_error=True):\n    try:\n        for encoding in ['utf-8', 'latin1']:\n            try:\n                with io.open(filename, encoding=encoding) as fp:\n                    return fp.read()\n            except UnicodeDecodeError:\n                pass\n    except IOError as exception:\n        if print_error:\n            print(exception, file=sys.stderr)\n        return None", "response": "Reads a file and returns the contents of it."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef map(self, fn: Callable[[Any], Any]) -> 'Reader':\n        def _compose(x: Any) -> Any:\n            try:\n                ret = fn(self.run(x))\n            except TypeError:\n                ret = partial(fn, self.run(x))\n            return ret\n        return Reader(_compose)", "response": "Returns a new Reader that runs the given function over the current Reader."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef bind(self, fn: \"Callable[[Any], Reader]\") -> 'Reader':\n        return Reader(lambda x: fn(self.run(x)).run(x))", "response": "Bind a monadic function to the Reader."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef apply(self, something: 'Reader') -> 'Reader':\n\n        def _compose(x: Any):\n            f = self.run(x)\n            try:\n                ret = f(something.run(x))\n            except TypeError:\n                ret = partial(f, something.run(x))\n            return ret\n\n        return Reader(_compose)", "response": "Apply a function over a set of resources."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns wrapped function. Haskell: runReader :: Reader r a -> r -> a This is the inverse of unit and returns the wrapped function.", "response": "def run(self, *args, **kwargs) -> Callable:\n        \"\"\"Return wrapped function.\n\n        Haskell: runReader :: Reader r a -> r -> a\n\n        This is the inverse of unit and returns the wrapped function.\n        \"\"\"\n        return self.fn(*args, **kwargs) if args or kwargs else self.fn"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngive a function it returns a Reader which evaluates that function returns the result.", "response": "def asks(cls, fn: Callable) -> Reader:\n        \"\"\"\n        Given a function it returns a Reader which evaluates that\n        function and returns the result.\n\n        asks :: (e -> a) -> R e a\n        asks f = do\n            e <- ask\n            return $ f e\n\n        asks sel = ask >>= return . sel\n        \"\"\"\n        return cls.ask().bind(Reader(lambda x: cls.unit(fn(x))))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef concat(cls, xs):\n\n        def reducer(a, b):\n            return a.append(b)\n\n        return reduce(reducer, xs, cls.empty())", "response": "mconcat - Fold a list using the monoid"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef compose(f: Callable[[Any], Monad], g: Callable[[Any], Monad]) -> Callable[[Any], Monad]:\n    return lambda x: g(x).bind(f)", "response": "r Monadic compose function."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomposes multiple functions right to left.", "response": "def compose(*funcs: Callable) -> Callable:\n    \"\"\"Compose multiple functions right to left.\n\n    Composes zero or more functions into a functional composition. The\n    functions are composed right to left. A composition of zero\n    functions gives back the identity function.\n\n    compose()(x) == x\n    compose(f)(x) == f(x)\n    compose(g, f)(x) == g(f(x))\n    compose(h, g, f)(x) == h(g(f(x)))\n    ...\n\n    Returns the composed function.\n    \"\"\"\n    def _compose(*args, **kw):\n        \"\"\"Reduce functions to a single function.\"\"\"\n        ret = reduce(lambda acc, x: lambda f: f(acc(x)),\n                     funcs[::-1],\n                     lambda f: f(*args, **kw))\n        return ret(lambda x: x)\n    return _compose"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbinding a function to the current state.", "response": "def bind(self, fn: Callable[[Any], 'State']) -> 'State':\n        r\"\"\"m >>= k = State $ \\s -> let (a, s') = runState m s\n                         in runState (k a) s'\n        \"\"\"\n\n        def _(result: Any, state: Any) -> Tuple[Any, Any]:\n            return fn(result).run(state)\n\n        return State(lambda state: _(*self.run(state)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run(self, state: Any) -> Tuple[Any, Any]:\n        return self._value(state)", "response": "Return wrapped state computation."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmap a function func over the Writer value.", "response": "def map(self, func: Callable[[Tuple[Any, Log]], Tuple[Any, Log]]) -> 'Writer':\n        \"\"\"Map a function func over the Writer value.\n\n        Haskell:\n        fmap f m = Writer $ let (a, w) = runWriter m in (f a, w)\n\n        Keyword arguments:\n        func -- Mapper function:\n        \"\"\"\n        a, w = self.run()\n        b, _w = func((a, w))\n        return Writer(b, _w)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef bind(self, func: Callable[[Any], 'Writer']) -> 'Writer':\n        a, w = self.run()\n        b, w_ = func(a).run()\n\n        if isinstance(w_, Monoid):\n            w__ = cast(Monoid, w).append(w_)\n        else:\n            w__ = w + w_\n\n        return Writer(b, w__)", "response": "Bind a function to the nested Writer."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef apply_log(a: tuple, func: Callable[[Any], Tuple[Any, Log]]) -> Tuple[Any, Log]:\n        value, log = a\n        new, entry = func(value)\n        return new, log + entry", "response": "Helper function to apply a function to a value with a log tuple."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a new Writer subclass using specified monoid type.", "response": "def create(cls, class_name: str, monoid_type=Union[Monoid, str]):\n        \"\"\"Create Writer subclass using specified monoid type.\n\n        lets us create a Writer that uses a different monoid than str for\n        the log.\n        \"\"\"\n\n        def unit(cls, value):\n            if hasattr(monoid_type, \"empty\"):\n                log = monoid_type.empty()\n            else:\n                log = monoid_type()\n\n            return cls(value, log)\n\n        return type(class_name, (Writer, ), dict(unit=classmethod(unit)))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmap a function over wrapped values.", "response": "def map(self, mapper: Callable[[Any], Any]) -> 'Identity':\n        \"\"\"Map a function over wrapped values.\"\"\"\n        value = self._value\n        try:\n            result = mapper(value)\n        except TypeError:\n            result = partial(mapper, value)\n\n        return Identity(result)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef bind(self, func: Callable[[Any], IO]) -> 'Put':\n\n        text, a = self._value\n        return Put(text, a.bind(func))", "response": "Bind a function to the current object."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbinds a function to the current IO.", "response": "def bind(self, func: Callable[[Any], IO]) -> IO:\n        \"\"\"IO a -> (a -> IO b) -> IO b\"\"\"\n\n        g = self._value\n        return Get(lambda text: g(text).bind(func))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef bind(self, func: Callable[[Any], IO]) -> IO:\n\n        filename, g = self._get_value()\n        return ReadFile(filename, lambda s: g(s).bind(func))", "response": "Bind a function to the current file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmapping a function over a monadic value and returns a new monadic value.", "response": "def lift(self, func):\n        \"\"\"Map function over monadic value.\n\n        Takes a function and a monadic value and maps the function over the\n        monadic value\n\n        Haskell: liftM :: (Monad m) => (a -> b) -> m a -> m b\n\n        This is really the same function as Functor.fmap, but is instead\n        implemented using bind, and does not rely on us inheriting from\n        Functor.\n        \"\"\"\n\n        return self.bind(lambda x: self.unit(func(x)))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmap a function over an observable.", "response": "def map(self, mapper: Callable[[Any], Any]) -> 'Observable':\n        r\"\"\"Map a function over an observable.\n\n        Haskell: fmap f m = Cont $ \\c -> runCont m (c . f)\n        \"\"\"\n        source = self\n        return Observable(lambda on_next: source.subscribe(compose(on_next, mapper)))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef bind(self, fn: Callable[[Any], 'Observable']) -> 'Observable':\n        source = self\n        return Observable(lambda on_next: source.subscribe(lambda a: fn(a).subscribe(on_next)))", "response": "A decorator that binds a function to the next value of the current key."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfilter the on_next continuation functions.", "response": "def filter(self, predicate) -> 'Observable':\n        \"\"\"Filter the on_next continuation functions\"\"\"\n        source = self\n\n        def subscribe(on_next):\n            def _next(x):\n                if predicate(x):\n                    on_next(x)\n\n            return source.subscribe(_next)\n        return Observable(subscribe)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef call_cc(fn: Callable) -> 'Observable':\n        def subscribe(on_next):\n            return fn(lambda a: Observable(lambda _: on_next(a))).subscribe(on_next)\n\n        return Observable(subscribe)", "response": "r Call a function on the current Continuation and return an Observable that emits the result."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef bind(self, func: Callable[[Any], Maybe]) -> Maybe:\n\n        value = self._value\n        return func(value)", "response": "Just x >>= f x."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cons(self, element: Any) -> 'List':\n\n        tail = self._get_value()\n        return List(lambda sel: sel(element, tail))", "response": "Return a new list with the given element added to the front of the list."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the tail of the list.", "response": "def tail(self) -> 'List':\n        \"\"\"Return tail of List.\"\"\"\n\n        lambda_list = self._get_value()\n        return List(lambda_list(lambda _, tail: tail))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmap a function over a List.", "response": "def map(self, mapper: Callable[[Any], Any]) -> 'List':\n        \"\"\"Map a function over a List.\"\"\"\n        try:\n            ret = List.from_iterable([mapper(x) for x in self])\n        except TypeError:\n            ret = List.from_iterable([partial(mapper, x) for x in self])\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nappending other list to this list.", "response": "def append(self, other: 'List') -> 'List':\n        \"\"\"Append other list to this list.\"\"\"\n\n        if self.null():\n            return other\n        return (self.tail().append(other)).cons(self.head())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef bind(self, fn: Callable[[Any], 'List']) -> 'List':\n        return List.concat(self.map(fn))", "response": "Flatten and map the List."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_iterable(cls, iterable: Iterable) -> 'List':\n\n        iterator = iter(iterable)\n\n        def recurse() -> List:\n            try:\n                value = next(iterator)\n            except StopIteration:\n\n                return List.empty()\n            return List.unit(value).append(recurse())\n        return List.empty().append(recurse())", "response": "Create list from iterable."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef map(self, fn: Callable[[Any], Any]) -> 'Cont':\n        return Cont(lambda c: self.run(compose(c, fn)))", "response": "Haskell version of fmap that returns a Cont object that is a copy of the current Cont object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef bind(self, fn: Callable[[Any], 'Cont']) -> 'Cont':\n        return Cont(lambda c: self.run(lambda a: fn(a).run(c)))", "response": "r Chain continuation passing functions."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef call_cc(fn: Callable) -> 'Cont':\n        return Cont(lambda c: fn(lambda a: Cont(lambda _: c(a))).run(c))", "response": "r Call a function on the current Cont object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef compress(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        result = func(*args, **kwargs)\n        if ('gzip' in bottle.request.headers.get('Accept-Encoding', '') and\n                isinstance(result, string_type) and\n                len(result) > 1024):\n            if isinstance(result, unicode):\n                result = result.encode('utf-8')\n            tmp_fo = BytesIO()\n            with gzip.GzipFile(mode='wb', fileobj=tmp_fo) as gzip_fo:\n                gzip_fo.write(result)\n            result = tmp_fo.getvalue()\n            bottle.response.add_header('Content-Encoding', 'gzip')\n        return result\n    return wrapper", "response": "Decorator to compress the data with gzip compression"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_trace(host='', port=5555, patch_stdstreams=False):\n    pdb = WebPdb.active_instance\n    if pdb is None:\n        pdb = WebPdb(host, port, patch_stdstreams)\n    else:\n        # If the debugger is still attached reset trace to a new location\n        pdb.remove_trace()\n    pdb.set_trace(sys._getframe().f_back)", "response": "Start the debugger and set the trace to the current location."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef post_mortem(tb=None, host='', port=5555, patch_stdstreams=False):\n    # handling the default\n    if tb is None:\n        # sys.exc_info() returns (type, value, traceback) if an exception is\n        # being handled, otherwise it returns (None, None, None)\n        t, v, tb = sys.exc_info()\n        exc_data = traceback.format_exception(t, v, tb)\n    else:\n        exc_data = traceback.format_tb(tb)\n    if tb is None:\n        raise ValueError('A valid traceback must be passed if no '\n                         'exception is being handled')\n    pdb = WebPdb.active_instance\n    if pdb is None:\n        pdb = WebPdb(host, port, patch_stdstreams)\n    else:\n        pdb.remove_trace()\n    pdb.console.writeline('*** Web-PDB post-mortem ***\\n')\n    pdb.console.writeline(''.join(exc_data))\n    pdb.reset()\n    pdb.interaction(None, tb)", "response": "Create a new object that will be used to debug the last unhandled exception."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef catch_post_mortem(host='', port=5555, patch_stdstreams=False):\n    try:\n        yield\n    except Exception:\n        post_mortem(None, host, port, patch_stdstreams)", "response": "A context manager that catches unhandled exceptions and returns a new object that can be used to track the current state of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_repr(obj, pretty=False, indent=1):\n        if pretty:\n            repr_value = pformat(obj, indent)\n        else:\n            repr_value = repr(obj)\n        if sys.version_info[0] == 2:\n            # Try to convert Unicode string to human-readable form\n            try:\n                repr_value = repr_value.decode('raw_unicode_escape')\n            except UnicodeError:\n                repr_value = repr_value.decode('utf-8', 'replace')\n        return repr_value", "response": "Get string representation of an object"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget all data about the current execution frame.", "response": "def get_current_frame_data(self):\n        \"\"\"\n        Get all date about the current execution frame\n\n        :return: current frame data\n        :rtype: dict\n        :raises AttributeError: if the debugger does hold any execution frame.\n        :raises IOError: if source code for the current execution frame is not accessible.\n        \"\"\"\n        filename = self.curframe.f_code.co_filename\n        lines, start_line = inspect.findsource(self.curframe)\n        if sys.version_info[0] == 2:\n            lines = [line.decode('utf-8') for line in lines]\n        return {\n            'dirname': os.path.dirname(os.path.abspath(filename)) + os.path.sep,\n            'filename': os.path.basename(filename),\n            'file_listing': ''.join(lines),\n            'current_line': self.curframe.f_lineno,\n            'breakpoints': self.get_file_breaks(filename),\n            'globals': self.get_globals(),\n            'locals': self.get_locals()\n        }"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nformats the raw_vars dict of var_name = value pairs as a unicode string", "response": "def _format_variables(self, raw_vars):\n        \"\"\"\n        :param raw_vars: a `dict` of `var_name: var_object` pairs\n        :type raw_vars: dict\n        :return: sorted list of variables as a unicode string\n        :rtype: unicode\n        \"\"\"\n        f_vars = []\n        for var, value in raw_vars.items():\n            if not (var.startswith('__') and var.endswith('__')):\n                repr_value = self._get_repr(value)\n                f_vars.append('{0} = {1}'.format(var, repr_value))\n        return '\\n'.join(sorted(f_vars))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remove_trace(self, frame=None):\n        sys.settrace(None)\n        if frame is None:\n            frame = self.curframe\n        while frame and frame is not self.botframe:\n            del frame.f_trace\n            frame = frame.f_back", "response": "Removes the debugger from the execution stack."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef flush(self):\n        i = 0\n        while self._frame_data.is_dirty and i < 10:\n            i += 1\n            time.sleep(0.1)", "response": "Flushes the history until no more than 10 cycles."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef solve_spectral(prob, *args, **kwargs):\n\n    # TODO: do this efficiently without SDP lifting\n\n    # lifted variables and semidefinite constraint\n    X = cvx.Semidef(prob.n + 1)\n\n    W = prob.f0.homogeneous_form()\n    rel_obj = cvx.Minimize(cvx.sum_entries(cvx.mul_elemwise(W, X)))\n\n    W1 = sum([f.homogeneous_form() for f in prob.fs if f.relop == '<='])\n    W2 = sum([f.homogeneous_form() for f in prob.fs if f.relop == '=='])\n\n    rel_prob = cvx.Problem(\n        rel_obj,\n        [\n            cvx.sum_entries(cvx.mul_elemwise(W1, X)) <= 0,\n            cvx.sum_entries(cvx.mul_elemwise(W2, X)) == 0,\n            X[-1, -1] == 1\n        ]\n    )\n    rel_prob.solve(*args, **kwargs)\n\n    if rel_prob.status not in [cvx.OPTIMAL, cvx.OPTIMAL_INACCURATE]:\n        raise Exception(\"Relaxation problem status: %s\" % rel_prob.status)\n\n    (w, v) = LA.eig(X.value)\n    return np.sqrt(np.max(w))*np.asarray(v[:-1, np.argmax(w)]).flatten(), rel_prob.value", "response": "Solve the spectral relaxation with lambda = 1."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef solve_sdr(prob, *args, **kwargs):\n\n    # lifted variables and semidefinite constraint\n    X = cvx.Semidef(prob.n + 1)\n\n    W = prob.f0.homogeneous_form()\n    rel_obj = cvx.Minimize(cvx.sum_entries(cvx.mul_elemwise(W, X)))\n    rel_constr = [X[-1, -1] == 1]\n\n    for f in prob.fs:\n        W = f.homogeneous_form()\n        lhs = cvx.sum_entries(cvx.mul_elemwise(W, X))\n        if f.relop == '==':\n            rel_constr.append(lhs == 0)\n        else:\n            rel_constr.append(lhs <= 0)\n\n    rel_prob = cvx.Problem(rel_obj, rel_constr)\n    rel_prob.solve(*args, **kwargs)\n\n    if rel_prob.status not in [cvx.OPTIMAL, cvx.OPTIMAL_INACCURATE]:\n        raise Exception(\"Relaxation problem status: %s\" % rel_prob.status)\n\n    return X.value, rel_prob.value", "response": "Solve the SDP relaxation."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the problem metadata in QCQP class.", "response": "def get_qcqp_form(prob):\n    \"\"\"Returns the problem metadata in QCQP class\n    \"\"\"\n    # Check quadraticity\n    if not prob.objective.args[0].is_quadratic():\n        raise Exception(\"Objective is not quadratic.\")\n    if not all([constr._expr.is_quadratic() for constr in prob.constraints]):\n        raise Exception(\"Not all constraints are quadratic.\")\n    if prob.is_dcp():\n        logging.warning(\"Problem is already convex; specifying solve method is unnecessary.\")\n\n    extractor = QuadCoeffExtractor(*get_id_map(prob.variables()))\n\n    P0, q0, r0 = extractor.get_coeffs(prob.objective.args[0])\n    # unpacking values\n    P0, q0, r0 = (P0[0]+P0[0].T)/2., q0.T.tocsc(), r0[0]\n\n    if prob.objective.NAME == \"maximize\":\n        P0, q0, r0 = -P0, -q0, -r0\n\n    f0 = QuadraticFunction(P0, q0, r0)\n\n    fs = []\n    for constr in prob.constraints:\n        sz = constr._expr.size[0]*constr._expr.size[1]\n        Pc, qc, rc = extractor.get_coeffs(constr._expr)\n        for i in range(sz):\n            fs.append(QuadraticFunction((Pc[i]+Pc[i].T)/2., qc[i, :].T.tocsc(), rc[i], constr.OP_NAME))\n\n    return QCQPForm(f0, fs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef handleNotification(self, handle, data):\n        _LOGGER.debug(\"Got notification from %s: %s\", handle, codecs.encode(data, 'hex'))\n        if handle in self._callbacks:\n            self._callbacks[handle](data)", "response": "Handle a notification from a Bluetooth GATT request."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make_request(self, handle, value, timeout=DEFAULT_TIMEOUT, with_response=True):\n        try:\n            with self:\n                _LOGGER.debug(\"Writing %s to %s with with_response=%s\", codecs.encode(value, 'hex'), handle, with_response)\n                self._conn.writeCharacteristic(handle, value, withResponse=with_response)\n                if timeout:\n                    _LOGGER.debug(\"Waiting for notifications for %s\", timeout)\n                    self._conn.waitForNotifications(timeout)\n        except btle.BTLEException as ex:\n            _LOGGER.debug(\"Got exception from bluepy while making a request: %s\", ex)\n            raise", "response": "Write a GATT Command without callback - not utf - 8."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cli(ctx, mac, debug):\n    if debug:\n        logging.basicConfig(level=logging.DEBUG)\n    else:\n        logging.basicConfig(level=logging.INFO)\n\n    thermostat = Thermostat(mac)\n    thermostat.update()\n    ctx.obj = thermostat\n\n    if ctx.invoked_subcommand is None:\n        ctx.invoke(state)", "response": "A basic command line interface to query and modify the state of EQ3 BT smart thermostat."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef temp(dev, target):\n    click.echo(\"Current target temp: %s\" % dev.target_temperature)\n    if target:\n        click.echo(\"Setting target temp: %s\" % target)\n        dev.target_temperature = target", "response": "Gets or sets the target temperature."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget or sets the active mode.", "response": "def mode(dev, target):\n    \"\"\" Gets or sets the active mode. \"\"\"\n    click.echo(\"Current mode: %s\" % dev.mode_readable)\n    if target:\n        click.echo(\"Setting mode: %s\" % target)\n        dev.mode = target"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef boost(dev, target):\n    click.echo(\"Boost: %s\" % dev.boost)\n    if target is not None:\n        click.echo(\"Setting boost: %s\" % target)\n        dev.boost = target", "response": "Gets or sets the boost mode."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef locked(dev, target):\n    click.echo(\"Locked: %s\" % dev.locked)\n    if target is not None:\n        click.echo(\"Setting lock: %s\" % target)\n        dev.locked = target", "response": "Gets or sets the lock."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget and sets the window open settings.", "response": "def window_open(dev, temp, duration):\n    \"\"\" Gets and sets the window open settings. \"\"\"\n    click.echo(\"Window open: %s\" % dev.window_open)\n    if temp and duration:\n        click.echo(\"Setting window open conf, temp: %s duration: %s\" % (temp, duration))\n        dev.window_open_config(temp, duration)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef presets(dev, comfort, eco):\n    click.echo(\"Setting presets: comfort %s, eco %s\" % (comfort, eco))\n    dev.temperature_presets(comfort, eco)", "response": "Sets the temperatures for auto mode presets."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the schedule from the thermostat.", "response": "def schedule(dev):\n    \"\"\" Gets the schedule from the thermostat. \"\"\"\n    # TODO: expose setting the schedule somehow?\n    for d in range(7):\n        dev.query_schedule(d)\n    for day in dev.schedule.values():\n        click.echo(\"Day %s, base temp: %s\" % (day.day, day.base_temp))\n        current_hour = day.next_change_at\n        for hour in day.hours:\n            if current_hour == 0: continue\n            click.echo(\"\\t[%s-%s] %s\" % (current_hour, hour.next_change_at, hour.target_temp))\n            current_hour = hour.next_change_at"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nenables or disables the away mode.", "response": "def away(dev, away_end, temperature):\n    \"\"\" Enables or disables the away mode. \"\"\"\n    if away_end:\n        click.echo(\"Setting away until %s, temperature: %s\" % (away_end, temperature))\n    else:\n        click.echo(\"Disabling away mode\")\n    dev.set_away(away_end, temperature)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nprinting out all available information.", "response": "def state(ctx):\n    \"\"\" Prints out all available information. \"\"\"\n    dev = ctx.obj\n    click.echo(dev)\n    ctx.forward(locked)\n    ctx.forward(low_battery)\n    ctx.forward(window_open)\n    ctx.forward(boost)\n    ctx.forward(temp)\n    # ctx.forward(presets)\n    ctx.forward(mode)\n    ctx.forward(valve_state)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nverify that the temperature is valid.", "response": "def _verify_temperature(self, temp):\n        \"\"\"Verifies that the temperature is valid.\n            :raises TemperatureException: On invalid temperature.\n        \"\"\"\n        if temp < self.min_temp or temp > self.max_temp:\n            raise TemperatureException('Temperature {} out of range [{}, {}]'\n                                       .format(temp, self.min_temp, self.max_temp))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_schedule(self, data):\n        sched = Schedule.parse(data)\n        _LOGGER.debug(\"Got schedule data for day '%s'\", sched.day)\n\n        return sched", "response": "Parses the device sent schedule."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef handle_notification(self, data):\n        _LOGGER.debug(\"Received notification from the device..\")\n\n        if data[0] == PROP_INFO_RETURN and data[1] == 1:\n            _LOGGER.debug(\"Got status: %s\" % codecs.encode(data, 'hex'))\n            status = Status.parse(data)\n            _LOGGER.debug(\"Parsed status: %s\", status)\n\n            self._raw_mode = status.mode\n            self._valve_state = status.valve\n            self._target_temperature = status.target_temp\n\n            if status.mode.BOOST:\n                self._mode = Mode.Boost\n            elif status.mode.AWAY:\n                self._mode = Mode.Away\n                self._away_end = status.away\n            elif status.mode.MANUAL:\n                if status.target_temp == EQ3BT_OFF_TEMP:\n                    self._mode = Mode.Closed\n                elif status.target_temp == EQ3BT_ON_TEMP:\n                    self._mode = Mode.Open\n                else:\n                    self._mode = Mode.Manual\n            else:\n                self._mode = Mode.Auto\n\n            _LOGGER.debug(\"Valve state: %s\", self._valve_state)\n            _LOGGER.debug(\"Mode:        %s\", self.mode_readable)\n            _LOGGER.debug(\"Target temp: %s\", self._target_temperature)\n            _LOGGER.debug(\"Away end:    %s\", self._away_end)\n\n        elif data[0] == PROP_SCHEDULE_RETURN:\n            parsed = self.parse_schedule(data)\n            self._schedule[parsed.day] = parsed\n\n        else:\n            _LOGGER.debug(\"Unknown notification %s (%s)\", data[0], codecs.encode(data, 'hex'))", "response": "Handle a notification from a Bluetooth device."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update(self):\n        _LOGGER.debug(\"Querying the device..\")\n        time = datetime.now()\n        value = struct.pack('BBBBBBB', PROP_INFO_QUERY,\n                            time.year % 100, time.month, time.day,\n                            time.hour, time.minute, time.second)\n\n        self._conn.make_request(PROP_WRITE_HANDLE, value)", "response": "Update the thermostat. Always sets the current time."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_schedule(self, data):\n        value = Schedule.build(data)\n        self._conn.make_request(PROP_WRITE_HANDLE, value)", "response": "Sets the schedule for the given day."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef target_temperature(self, temperature):\n        dev_temp = int(temperature * 2)\n        if temperature == EQ3BT_OFF_TEMP or temperature == EQ3BT_ON_TEMP:\n            dev_temp |= 0x40\n            value = struct.pack('BB', PROP_MODE_WRITE, dev_temp)\n        else:\n            self._verify_temperature(temperature)\n            value = struct.pack('BB', PROP_TEMPERATURE_WRITE, dev_temp)\n\n        self._conn.make_request(PROP_WRITE_HANDLE, value)", "response": "Set new target temperature."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef mode(self, mode):\n        _LOGGER.debug(\"Setting new mode: %s\", mode)\n\n        if self.mode == Mode.Boost and mode != Mode.Boost:\n            self.boost = False\n\n        if mode == Mode.Boost:\n            self.boost = True\n            return\n        elif mode == Mode.Away:\n            end = datetime.now() + self._away_duration\n            return self.set_away(end, self._away_temp)\n        elif mode == Mode.Closed:\n            return self.set_mode(0x40 | int(EQ3BT_OFF_TEMP * 2))\n        elif mode == Mode.Open:\n            return self.set_mode(0x40 | int(EQ3BT_ON_TEMP * 2))\n\n        if mode == Mode.Manual:\n            temperature = max(min(self._target_temperature, self.max_temp),\n                              self.min_temp)\n            return self.set_mode(0x40 | int(temperature * 2))\n        else:\n            return self.set_mode(0)", "response": "Set the operation mode."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets away mode with target temperature. When called without parameters disables away mode.", "response": "def set_away(self, away_end=None, temperature=EQ3BT_AWAY_TEMP):\n        \"\"\" Sets away mode with target temperature.\n            When called without parameters disables away mode.\"\"\"\n        if not away_end:\n            _LOGGER.debug(\"Disabling away, going to auto mode.\")\n            return self.set_mode(0x00)\n\n        _LOGGER.debug(\"Setting away until %s, temp %s\", away_end, temperature)\n        adapter = AwayDataAdapter(Byte[4])\n        packed = adapter.build(away_end)\n\n        self.set_mode(0x80 | int(temperature * 2), packed)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef mode_readable(self):\n        ret = \"\"\n        mode = self._raw_mode\n\n        if mode.MANUAL:\n            ret = \"manual\"\n            if self.target_temperature < self.min_temp:\n                ret += \" off\"\n            elif self.target_temperature >= self.max_temp:\n                ret += \" on\"\n            else:\n                ret += \" (%sC)\" % self.target_temperature\n        else:\n            ret = \"auto\"\n\n        if mode.AWAY:\n            ret += \" holiday\"\n        if mode.BOOST:\n            ret += \" boost\"\n        if mode.DST:\n            ret += \" dst\"\n        if mode.WINDOW:\n            ret += \" window\"\n        if mode.LOCKED:\n            ret += \" locked\"\n        if mode.LOW_BATTERY:\n            ret += \" low battery\"\n\n        return ret", "response": "Return a readable representation of the mode."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef window_open_config(self, temperature, duration):\n        _LOGGER.debug(\"Window open config, temperature: %s duration: %s\", temperature, duration)\n        self._verify_temperature(temperature)\n        if duration.seconds < 0 and duration.seconds > 3600:\n            raise ValueError\n\n        value = struct.pack('BBB', PROP_WINDOW_OPEN_CONFIG,\n                            int(temperature * 2), int(duration.seconds / 300))\n        self._conn.make_request(PROP_WRITE_HANDLE, value)", "response": "Configures the window open behavior. The duration is specified in\n        5 minute increments."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlocks or unlocks the thermostat.", "response": "def locked(self, lock):\n        \"\"\"Locks or unlocks the thermostat.\"\"\"\n        _LOGGER.debug(\"Setting the lock: %s\", lock)\n        value = struct.pack('BB', PROP_LOCK, bool(lock))\n        self._conn.make_request(PROP_WRITE_HANDLE, value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef temperature_presets(self, comfort, eco):\n        _LOGGER.debug(\"Setting temperature presets, comfort: %s eco: %s\", comfort, eco)\n        self._verify_temperature(comfort)\n        self._verify_temperature(eco)\n        value = struct.pack('BBB', PROP_COMFORT_ECO_CONFIG, int(comfort * 2),\n                            int(eco * 2))\n        self._conn.make_request(PROP_WRITE_HANDLE, value)", "response": "Set the thermostats preset temperatures comfort and eco."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the thermostat s temperature offset.", "response": "def temperature_offset(self, offset):\n        \"\"\"Sets the thermostat's temperature offset.\"\"\"\n        _LOGGER.debug(\"Setting offset: %s\", offset)\n        # [-3,5 .. 0  .. 3,5 ]\n        # [00   .. 07 .. 0e ]\n        if offset < -3.5 or offset > 3.5:\n            raise TemperatureException(\"Invalid value: %s\" % offset)\n\n        current = -3.5\n        values = {}\n        for i in range(15):\n            values[current] = i\n            current += 0.5\n\n        value = struct.pack('BB', PROP_OFFSET, values[offset])\n        self._conn.make_request(PROP_WRITE_HANDLE, value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nactivates the comfort temperature.", "response": "def activate_comfort(self):\n        \"\"\"Activates the comfort temperature.\"\"\"\n        value = struct.pack('B', PROP_COMFORT)\n        self._conn.make_request(PROP_WRITE_HANDLE, value)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nactivate the comfort temperature.", "response": "def activate_eco(self):\n        \"\"\"Activates the comfort temperature.\"\"\"\n        value = struct.pack('B', PROP_ECO)\n        self._conn.make_request(PROP_WRITE_HANDLE, value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read(self, source = None, **options):\n        '''\n        Reads and optionally parses a single message.\n        \n        :Parameters:\n         - `source` - optional data buffer to be read, if not specified data is \n           read from the wrapped stream\n        :Options:\n         - `raw` (`boolean`) - indicates whether read data should parsed or \n           returned in raw byte form\n         - `numpy_temporals` (`boolean`) - if ``False`` temporal vectors are\n           backed by raw q representation (:class:`.QTemporalList`, \n           :class:`.QTemporal`) instances, otherwise are represented as \n           `numpy datetime64`/`timedelta64` arrays and atoms,\n           **Default**: ``False``\n         \n        :returns: :class:`.QMessage` - read data (parsed or raw byte form) along\n                  with meta information\n        '''\n        message = self.read_header(source)\n        message.data = self.read_data(message.size, message.is_compressed, **options)\n\n        return message", "response": "Reads and parses a single message along with meta information."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read_header(self, source = None):\n        '''\n        Reads and parses message header.\n        \n        .. note:: :func:`.read_header` wraps data for further reading in internal\n                  buffer  \n    \n        :Parameters:\n         - `source` - optional data buffer to be read, if not specified data is \n           read from the wrapped stream\n           \n        :returns: :class:`.QMessage` - read meta information\n        '''\n        if self._stream:\n            header = self._read_bytes(8)\n            self._buffer.wrap(header)\n        else:\n            self._buffer.wrap(source)\n\n        self._buffer.endianness = '<' if self._buffer.get_byte() == 1 else '>'\n        self._is_native = self._buffer.endianness == ('<' if sys.byteorder == 'little' else '>')\n        message_type = self._buffer.get_byte()\n        message_compressed = self._buffer.get_byte() == 1\n        # skip 1 byte\n        self._buffer.skip()\n\n        message_size = self._buffer.get_int()\n        return QMessage(None, message_type, message_size, message_compressed)", "response": "Reads and parses the message header."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread and parses the data part of a message.", "response": "def read_data(self, message_size, is_compressed = False, **options):\n        '''\n        Reads and optionally parses data part of a message.\n        \n        .. note:: :func:`.read_header` is required to be called before executing\n                  the :func:`.read_data`\n        \n        :Parameters:\n         - `message_size` (`integer`) - size of the message to be read\n         - `is_compressed` (`boolean`) - indicates whether data is compressed\n        :Options:\n         - `raw` (`boolean`) - indicates whether read data should parsed or \n           returned in raw byte form\n         - `numpy_temporals` (`boolean`) - if ``False`` temporal vectors are\n           backed by raw q representation (:class:`.QTemporalList`, \n           :class:`.QTemporal`) instances, otherwise are represented as \n           `numpy datetime64`/`timedelta64` arrays and atoms,\n           **Default**: ``False``\n         \n        :returns: read data (parsed or raw byte form)\n        '''\n        self._options = MetaData(**CONVERSION_OPTIONS.union_dict(**options))\n\n        if is_compressed:\n            if self._stream:\n                self._buffer.wrap(self._read_bytes(4))\n            uncompressed_size = -8 + self._buffer.get_int()\n            compressed_data = self._read_bytes(message_size - 12) if self._stream else self._buffer.raw(message_size - 12)\n\n            raw_data = numpy.frombuffer(compressed_data, dtype = numpy.uint8)\n            if  uncompressed_size <= 0:\n                raise QReaderException('Error while data decompression.')\n\n            raw_data = uncompress(raw_data, numpy.intc(uncompressed_size))\n            raw_data = numpy.ndarray.tostring(raw_data)\n            self._buffer.wrap(raw_data)\n        elif self._stream:\n            raw_data = self._read_bytes(message_size - 8)\n            self._buffer.wrap(raw_data)\n        if not self._stream and self._options.raw:\n            raw_data = self._buffer.raw(message_size - 8)\n\n        return raw_data if self._options.raw else self._read_object()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef open(self):\n        '''Initialises connection to q service.\n        \n        If the connection hasn't been initialised yet, invoking the \n        :func:`.open` creates a new socket and performs a handshake with a q \n        service.\n        \n        :raises: :class:`.QConnectionException`, :class:`.QAuthenticationException` \n        '''\n        if not self._connection:\n            if not self.host:\n                raise QConnectionException('Host cannot be None')\n\n            self._init_socket()\n            self._initialize()\n\n            self._writer = self._writer_class(self._connection, protocol_version = self._protocol_version, encoding = self._encoding)\n            self._reader = self._reader_class(self._connection_file, encoding = self._encoding)", "response": "Initialises a connection to the q service."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _init_socket(self):\n        '''Initialises the socket used for communicating with a q service,'''\n        try:\n            self._connection = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            self._connection.connect((self.host, self.port))\n            self._connection.settimeout(self.timeout)\n            self._connection_file = self._connection.makefile('b')\n        except:\n            self._connection = None\n            self._connection_file = None\n            raise", "response": "Initialises the socket used for communicating with a q service."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nclose connection with the q service.", "response": "def close(self):\n        '''Closes connection with the q service.'''\n        if self._connection:\n            self._connection_file.close()\n            self._connection_file = None\n            self._connection.close()\n            self._connection = None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _initialize(self):\n        '''Performs a IPC protocol handshake.'''\n        credentials = (self.username if self.username else '') + ':' + (self.password if self.password else '')\n        credentials = credentials.encode(self._encoding)\n        self._connection.send(credentials + b'\\3\\0')\n        response = self._connection.recv(1)\n\n        if len(response) != 1:\n            self.close()\n            self._init_socket()\n\n            self._connection.send(credentials + b'\\0')\n            response = self._connection.recv(1)\n            if len(response) != 1:\n                self.close()\n                raise QAuthenticationException('Connection denied.')\n\n        self._protocol_version = min(struct.unpack('B', response)[0], 3)", "response": "Performs a IPC protocol handshake."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef query(self, msg_type, query, *parameters, **options):\n        '''Performs a query against a q service.\n        \n        In typical use case, `query` is the name of the function to call and \n        `parameters` are its parameters. When `parameters` list is empty, the \n        query can be an arbitrary q expression (e.g. ``0 +/ til 100``).\n        \n        Calls a anonymous function with a single parameter:\n        \n            >>> q.query(qconnection.MessageType.SYNC,'{til x}', 10)\n        \n        Executes a q expression:\n        \n            >>> q.query(qconnection.MessageType.SYNC,'til 10')\n        \n        :Parameters:\n         - `msg_type` (one of the constants defined in :class:`.MessageType`) - \n           type of the query to be executed\n         - `query` (`string`) - query to be executed\n         - `parameters` (`list` or `None`) - parameters for the query\n        :Options:\n         - `single_char_strings` (`boolean`) - if ``True`` single char Python \n           strings are encoded as q strings instead of chars, \n           **Default**: ``False``\n        \n        :raises: :class:`.QConnectionException`, :class:`.QWriterException`\n        '''\n        if not self._connection:\n            raise QConnectionException('Connection is not established.')\n\n        if parameters and len(parameters) > 8:\n            raise QWriterException('Too many parameters.')\n\n        if not parameters or len(parameters) == 0:\n            self._writer.write(query, msg_type, **self._options.union_dict(**options))\n        else:\n            self._writer.write([query] + list(parameters), msg_type, **self._options.union_dict(**options))", "response": "Performs a query against a q service."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sendSync(self, query, *parameters, **options):\n        '''Performs a synchronous query against a q service and returns parsed \n        data.\n        \n        In typical use case, `query` is the name of the function to call and \n        `parameters` are its parameters. When `parameters` list is empty, the \n        query can be an arbitrary q expression (e.g. ``0 +/ til 100``).\n        \n        Executes a q expression:\n        \n            >>> print(q.sendSync('til 10'))\n            [0 1 2 3 4 5 6 7 8 9]\n        \n        Executes an anonymous q function with a single parameter:\n        \n            >>> print(q.sendSync('{til x}', 10))\n            [0 1 2 3 4 5 6 7 8 9]\n            \n        Executes an anonymous q function with two parameters:\n        \n            >>> print(q.sendSync('{y + til x}', 10, 1))\n            [ 1  2  3  4  5  6  7  8  9 10]\n            \n            >>> print(q.sendSync('{y + til x}', *[10, 1]))\n            [ 1  2  3  4  5  6  7  8  9 10]\n        \n        The :func:`.sendSync` is called from the overloaded :func:`.__call__` \n        function. This allows :class:`.QConnection` instance to be called as \n        a function:\n        \n            >>> print(q('{y + til x}', 10, 1))\n            [ 1  2  3  4  5  6  7  8  9 10]\n        \n        \n        :Parameters:\n         - `query` (`string`) - query to be executed\n         - `parameters` (`list` or `None`) - parameters for the query\n        :Options: \n         - `raw` (`boolean`) - if ``True`` returns raw data chunk instead of \n           parsed data, **Default**: ``False``\n         - `numpy_temporals` (`boolean`) - if ``False`` temporal vectors are\n           backed by raw q representation (:class:`.QTemporalList`, \n           :class:`.QTemporal`) instances, otherwise are represented as \n           `numpy datetime64`/`timedelta64` arrays and atoms,\n           **Default**: ``False``\n         - `single_char_strings` (`boolean`) - if ``True`` single char Python \n           strings are encoded as q strings instead of chars, \n           **Default**: ``False``\n\n        :returns: query result parsed to Python data structures\n        \n        :raises: :class:`.QConnectionException`, :class:`.QWriterException`, \n                 :class:`.QReaderException`\n        '''\n        self.query(MessageType.SYNC, query, *parameters, **options)\n        response = self.receive(data_only = False, **options)\n\n        if response.type == MessageType.RESPONSE:\n            return response.data\n        else:\n            self._writer.write(QException('nyi: qPython expected response message'), MessageType.ASYNC if response.type == MessageType.ASYNC else MessageType.RESPONSE)\n            raise QReaderException('Received message of type: %s where response was expected')", "response": "Performs a synchronous query against a q service and returns parsed \n        data."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nperforms an asynchronous query and returns without ** retrieving of the response.", "response": "def sendAsync(self, query, *parameters, **options):\n        '''Performs an asynchronous query and returns **without** retrieving of \n        the response.\n        \n        In typical use case, `query` is the name of the function to call and \n        `parameters` are its parameters. When `parameters` list is empty, the \n        query can be an arbitrary q expression (e.g. ``0 +/ til 100``).\n        \n        Calls a anonymous function with a single parameter:\n        \n            >>> q.sendAsync('{til x}', 10)\n        \n        Executes a q expression:\n        \n            >>> q.sendAsync('til 10')\n        \n        :Parameters:\n         - `query` (`string`) - query to be executed\n         - `parameters` (`list` or `None`) - parameters for the query\n        :Options: \n         - `single_char_strings` (`boolean`) - if ``True`` single char Python \n           strings are encoded as q strings instead of chars, \n           **Default**: ``False``\n        \n        :raises: :class:`.QConnectionException`, :class:`.QWriterException`\n        '''\n        self.query(MessageType.ASYNC, query, *parameters, **options)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading and parses the response from a q service.", "response": "def receive(self, data_only = True, **options):\n        '''Reads and (optionally) parses the response from a q service.\n        \n        Retrieves query result along with meta-information:\n        \n            >>> q.query(qconnection.MessageType.SYNC,'{x}', 10)\n            >>> print(q.receive(data_only = False, raw = False))\n            QMessage: message type: 2, data size: 13, is_compressed: False, data: 10\n\n        Retrieves parsed query result:\n\n            >>> q.query(qconnection.MessageType.SYNC,'{x}', 10)\n            >>> print(q.receive(data_only = True, raw = False))\n            10\n\n        Retrieves not-parsed (raw) query result:\n        \n            >>> from binascii import hexlify\n            >>> q.query(qconnection.MessageType.SYNC,'{x}', 10)\n            >>> print(hexlify(q.receive(data_only = True, raw = True)))\n            fa0a000000\n                \n        :Parameters:\n         - `data_only` (`boolean`) - if ``True`` returns only data part of the \n           message, otherwise returns data and message meta-information \n           encapsulated in :class:`.QMessage` instance \n        :Options:\n         - `raw` (`boolean`) - if ``True`` returns raw data chunk instead of \n           parsed data, **Default**: ``False``\n         - `numpy_temporals` (`boolean`) - if ``False`` temporal vectors are\n           backed by raw q representation (:class:`.QTemporalList`, \n           :class:`.QTemporal`) instances, otherwise are represented as \n           `numpy datetime64`/`timedelta64` arrays and atoms,\n           **Default**: ``False``\n        \n        :returns: depending on parameter flags: :class:`.QMessage` instance, \n                  parsed message, raw data \n        :raises: :class:`.QReaderException`\n        '''\n        result = self._reader.read(**self._options.union_dict(**options))\n        return result.data if data_only else result"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting a numpy. datetime64 or numpy. timedelta64 to", "response": "def qtemporal(dt, **meta):\n    '''Converts a `numpy.datetime64` or `numpy.timedelta64` to \n    :class:`.QTemporal` and enriches object instance with given meta data.\n    \n    Examples:\n    \n       >>> qtemporal(numpy.datetime64('2001-01-01', 'D'), qtype=QDATE)\n       2001-01-01 [metadata(qtype=-14)]\n       >>> qtemporal(numpy.timedelta64(43499123, 'ms'), qtype=QTIME)\n       43499123 milliseconds [metadata(qtype=-19)]\n       >>> qtemporal(qnull(QDATETIME), qtype=QDATETIME)\n       nan [metadata(qtype=-15)]\n    \n    :Parameters:\n     - `dt` (`numpy.datetime64` or `numpy.timedelta64`) - datetime to be wrapped\n    :Kwargs:\n     - `qtype` (`integer`) - qtype indicator\n    \n    :returns: `QTemporal` - wrapped datetime \n    '''\n    result = QTemporal(dt)\n    result._meta_init(**meta)\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef array_from_raw_qtemporal(raw, qtype):\n    '''\n    Converts `numpy.array` containing raw q representation to ``datetime64``/``timedelta64``\n    array.\n    \n    Examples:\n    \n      >>> raw = numpy.array([366, 121, qnull(QDATE)])\n      >>> print(array_from_raw_qtemporal(raw, qtype = QDATE))\n      ['2001-01-01' '2000-05-01' 'NaT']\n    \n    :Parameters:\n     - `raw` (`numpy.array`) - numpy raw array to be converted\n     - `qtype` (`integer`) - qtype indicator\n    \n    :returns: `numpy.array` - numpy array with ``datetime64``/``timedelta64``\n    \n    :raises: `ValueError`\n    '''\n    if not isinstance(raw, numpy.ndarray):\n        raise ValueError('raw parameter is expected to be of type: numpy.ndarray. Was: %s' % type(raw))\n\n    qtype = -abs(qtype)\n    conversion = _FROM_RAW_LIST[qtype]\n\n    mask = raw == qnull(qtype)\n\n    dtype = PY_TYPE[qtype]\n    array = raw.astype(dtype) if dtype != raw.dtype else raw\n\n    array = conversion(array) if conversion else array\n    null = _NUMPY_NULL[qtype]\n    array = numpy.where(mask, null, array)\n    return array", "response": "Converts numpy. array containing raw q representation to datetime64 and timedelta64 array."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert numpy. array containing datetime64 or timedelta64 to raw q representation.", "response": "def array_to_raw_qtemporal(array, qtype):\n    '''\n    Converts `numpy.array` containing ``datetime64``/``timedelta64`` to raw\n    q representation.\n    \n    Examples:\n    \n      >>> na_dt = numpy.arange('1999-01-01', '2005-12-31', dtype='datetime64[D]')\n      >>> print(array_to_raw_qtemporal(na_dt, qtype = QDATE_LIST))\n      [-365 -364 -363 ..., 2188 2189 2190]\n      >>> array_to_raw_qtemporal(numpy.arange(-20, 30, dtype='int32'), qtype = QDATE_LIST)\n      Traceback (most recent call last):\n        ...\n      ValueError: array.dtype is expected to be of type: datetime64 or timedelta64. Was: int32\n    \n    :Parameters:\n     - `array` (`numpy.array`) - numpy datetime/timedelta array to be converted\n     - `qtype` (`integer`) - qtype indicator\n    \n    :returns: `numpy.array` - numpy array with raw values\n    \n    :raises: `ValueError`\n    '''\n    if not isinstance(array, numpy.ndarray):\n        raise ValueError('array parameter is expected to be of type: numpy.ndarray. Was: %s' % type(array))\n\n    if not array.dtype.type in (numpy.datetime64, numpy.timedelta64):\n        raise ValueError('array.dtype is expected to be of type: datetime64 or timedelta64. Was: %s' % array.dtype)\n\n    qtype = -abs(qtype)\n    conversion = _TO_RAW_LIST[qtype]\n    raw = array.view(numpy.int64).view(numpy.ndarray)\n    mask = raw == numpy.int64(-2 ** 63)\n\n    raw = conversion(raw) if conversion else raw\n    null = qnull(qtype)\n    raw = numpy.where(mask, null, raw)\n    return raw"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write(self, data, msg_type, **options):\n        '''Serializes and pushes single data object to a wrapped stream.\n        \n        :Parameters:\n         - `data` - data to be serialized\n         - `msg_type` (one of the constants defined in :class:`.MessageType`) -\n           type of the message\n        :Options:\n         - `single_char_strings` (`boolean`) - if ``True`` single char Python \n           strings are encoded as q strings instead of chars, \n           **Default**: ``False``\n        \n        :returns: if wraped stream is ``None`` serialized data, \n                  otherwise ``None`` \n        '''\n        self._buffer = BytesIO()\n\n        self._options = MetaData(**CONVERSION_OPTIONS.union_dict(**options))\n\n        # header and placeholder for message size\n        self._buffer.write(('%s%s\\0\\0\\0\\0\\0\\0' % (ENDIANESS, chr(msg_type))).encode(self._encoding))\n\n        self._write(data)\n\n        # update message size\n        data_size = self._buffer.tell()\n        self._buffer.seek(4)\n        self._buffer.write(struct.pack('i', data_size))\n\n        # write data to socket\n        if self._stream:\n            self._stream.sendall(self._buffer.getvalue())\n        else:\n            return self._buffer.getvalue()", "response": "Serializes and pushes single data object to a wrapped stream."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfinds out a corresponding qtype for a specified QList or numpy. ndarray instance.", "response": "def get_list_qtype(array):\n    '''Finds out a corresponding qtype for a specified `QList`/`numpy.ndarray` \n    instance.\n    \n    :Parameters:\n     - `array` (`QList` or `numpy.ndarray`) - array to be checked\n    \n    :returns: `integer` - qtype matching the specified array object\n    '''\n    if not isinstance(array, numpy.ndarray):\n        raise ValueError('array parameter is expected to be of type: numpy.ndarray, got: %s' % type(array))\n\n    if isinstance(array, QList):\n        return -abs(array.meta.qtype)\n\n    qtype = None\n\n    if str(array.dtype) in ('|S1', '<U1', '>U1', '|U1') :\n        qtype = QCHAR\n\n    if qtype is None:\n        qtype = Q_TYPE.get(array.dtype.type, None)\n\n    if qtype is None and array.dtype.type in (numpy.datetime64, numpy.timedelta64):\n        qtype = TEMPORAL_PY_TYPE.get(str(array.dtype), None)\n\n    if qtype is None:\n        # determinate type based on first element of the numpy array\n        qtype = Q_TYPE.get(type(array[0]), QGENERAL_LIST)\n\n    return qtype"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts an input array to q vector and enriches object instance with meta data.", "response": "def qlist(array, adjust_dtype = True, **meta):\n    '''Converts an input array to q vector and enriches object instance with \n    meta data.\n\n    Returns a :class:`.QList` instance for non-datetime vectors. For datetime \n    vectors :class:`.QTemporalList` is returned instead.\n\n    If parameter `adjust_dtype` is `True` and q type retrieved via \n    :func:`.get_list_qtype` doesn't match one provided as a `qtype` parameter \n    guessed q type, underlying numpy.array is converted to correct data type.\n    \n    `qPython` internally represents ``(0x01;0x02;0xff)`` q list as:\n    ``<class 'qpython.qcollection.QList'> dtype: int8 qtype: -4: [ 1  2 -1]``.\n    This object can be created by calling the :func:`.qlist` with following \n    arguments:\n    \n    - `byte numpy.array`:\n       \n       >>> v = qlist(numpy.array([0x01, 0x02, 0xff], dtype=numpy.byte))\n       >>> print('%s dtype: %s qtype: %d: %s' % (type(v), v.dtype, v.meta.qtype, v))\n       <class 'qpython.qcollection.QList'> dtype: int8 qtype: -4: [ 1  2 -1]\n    \n    - `int32 numpy.array` with explicit conversion to `QBYTE_LIST`:   \n      \n       >>> v = qlist(numpy.array([1, 2, -1]), qtype = QBYTE_LIST)\n       >>> print('%s dtype: %s qtype: %d: %s' % (type(v), v.dtype, v.meta.qtype, v))\n       <class 'qpython.qcollection.QList'> dtype: int8 qtype: -4: [ 1  2 -1]\n    \n    - plain Python `integer` list with explicit conversion to `QBYTE_LIST`:   \n       \n       >>> v = qlist([1, 2, -1], qtype = QBYTE_LIST)\n       >>> print('%s dtype: %s qtype: %d: %s' % (type(v), v.dtype, v.meta.qtype, v))\n       <class 'qpython.qcollection.QList'> dtype: int8 qtype: -4: [ 1  2 -1]\n\n    - numpy datetime64 array with implicit conversion to `QDATE_LIST`:   \n       \n       >>> v = qlist(numpy.array([numpy.datetime64('2001-01-01'), numpy.datetime64('2000-05-01'), numpy.datetime64('NaT')], dtype='datetime64[D]'))\n       >>> print('%s dtype: %s qtype: %d: %s' % (type(v), v.dtype, v.meta.qtype, v))\n       <class 'qpython.qcollection.QList'> dtype: datetime64[D] qtype: -14: ['2001-01-01' '2000-05-01' 'NaT']\n       \n    - numpy datetime64 array with explicit conversion to `QDATE_LIST`:   \n       \n       >>> v = qlist(numpy.array([numpy.datetime64('2001-01-01'), numpy.datetime64('2000-05-01'), numpy.datetime64('NaT')], dtype='datetime64[D]'), qtype = QDATE_LIST)\n       >>> print('%s dtype: %s qtype: %d: %s' % (type(v), v.dtype, v.meta.qtype, v))\n       <class 'qpython.qcollection.QList'> dtype: datetime64[D] qtype: -14: ['2001-01-01' '2000-05-01' 'NaT']\n\n    \n    :Parameters:\n     - `array` (`tuple`, `list`, `numpy.array`) - input array to be converted\n     - `adjust_dtype` (`boolean`) - determine whether data type of vector should\n       be adjusted if it doesn't match default representation. **Default**: ``True``\n       \n     .. note:: numpy `datetime64` and `timedelta64` arrays are not converted\n               to raw temporal vectors if `adjust_dtype` is ``True``\n    \n    :Kwargs:\n     - `qtype` (`integer` or `None`) - qtype indicator\n     \n    :returns: `QList` or `QTemporalList` - array representation of the list\n    \n    :raises: `ValueError` \n    '''\n    if type(array) in (list, tuple):\n        if meta and 'qtype' in meta and meta['qtype'] == QGENERAL_LIST:\n            # force shape and dtype for generic lists\n            tarray = numpy.ndarray(shape = len(array), dtype = numpy.dtype('O'))\n            for i in range(len(array)):\n                tarray[i] = array[i]\n            array = tarray\n        else:\n            array = numpy.array(array)\n\n    if not isinstance(array, numpy.ndarray):\n        raise ValueError('array parameter is expected to be of type: numpy.ndarray, list or tuple. Was: %s' % type(array))\n\n    qtype = None\n    is_numpy_temporal = array.dtype.type in (numpy.datetime64, numpy.timedelta64)\n\n    if meta and 'qtype' in meta:\n        qtype = -abs(meta['qtype'])\n        dtype = PY_TYPE[qtype]\n        if adjust_dtype and dtype != array.dtype and not is_numpy_temporal:\n            array = array.astype(dtype = dtype)\n\n    qtype = get_list_qtype(array) if qtype is None else qtype\n    meta['qtype'] = qtype\n\n    is_raw_temporal = meta['qtype'] in [QMONTH, QDATE, QDATETIME, QMINUTE, QSECOND, QTIME, QTIMESTAMP, QTIMESPAN] \\\n                      and not is_numpy_temporal\n    vector = array.view(QList) if not is_raw_temporal else array.view(QTemporalList)\n    vector._meta_init(**meta)\n    return vector"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef qtable(columns, data, **meta):\n    '''Creates a QTable out of given column names and data, and initialises the \n    meta data.\n    \n    :class:`.QTable` is represented internally by `numpy.core.records.recarray`.\n    Data for each column is converted to :class:`.QList` via :func:`.qlist` \n    function. If qtype indicator is defined for a column, this information\n    is used for explicit array conversion.\n    \n    Table examples:\n  \n      >>> # q: flip `name`iq!(`Dent`Beeblebrox`Prefect;98 42 126)\n      >>> t = qtable(qlist(numpy.array(['name', 'iq']), qtype = QSYMBOL_LIST), \n      ...     [qlist(numpy.array(['Dent', 'Beeblebrox', 'Prefect'])), \n      ...      qlist(numpy.array([98, 42, 126], dtype=numpy.int64))])\n      >>> print('%s dtype: %s meta: %s: %s' % (type(t), t.dtype, t.meta, t))\n      <class 'qpython.qcollection.QTable'> dtype: [('name', 'S10'), ('iq', '<i8')] meta: metadata(iq=-7, qtype=98, name=-11): [('Dent', 98L) ('Beeblebrox', 42L) ('Prefect', 126L)]\n      \n      >>> # q: flip `name`iq!(`Dent`Beeblebrox`Prefect;98 42 126)\n      >>> t = qtable(qlist(numpy.array(['name', 'iq']), qtype = QSYMBOL_LIST),\n      ...           [qlist(['Dent', 'Beeblebrox', 'Prefect'], qtype = QSYMBOL_LIST), \n      ...            qlist([98, 42, 126], qtype = QLONG_LIST)])\n      >>> print('%s dtype: %s meta: %s: %s' % (type(t), t.dtype, t.meta, t))\n      <class 'qpython.qcollection.QTable'> dtype: [('name', 'S10'), ('iq', '<i8')] meta: metadata(iq=-7, qtype=98, name=-11): [('Dent', 98L) ('Beeblebrox', 42L) ('Prefect', 126L)]\n      \n      >>> # q: flip `name`iq!(`Dent`Beeblebrox`Prefect;98 42 126)\n      >>> t = qtable(['name', 'iq'],\n      ...            [['Dent', 'Beeblebrox', 'Prefect'], \n      ...             [98, 42, 126]],\n      ...            name = QSYMBOL, iq = QLONG)\n      >>> print('%s dtype: %s meta: %s: %s' % (type(t), t.dtype, t.meta, t)) \n      <class 'qpython.qcollection.QTable'> dtype: [('name', 'S10'), ('iq', '<i8')] meta: metadata(iq=-7, qtype=98, name=-11): [('Dent', 98L) ('Beeblebrox', 42L) ('Prefect', 126L)]\n      \n      >>> # q: flip `name`iq`fullname!(`Dent`Beeblebrox`Prefect;98 42 126;(\"Arthur Dent\"; \"Zaphod Beeblebrox\"; \"Ford Prefect\"))\n      >>> t = qtable(('name', 'iq', 'fullname'),\n      ...            [qlist(numpy.array(['Dent', 'Beeblebrox', 'Prefect']), qtype = QSYMBOL_LIST), \n      ...             qlist(numpy.array([98, 42, 126]), qtype = QLONG_LIST),\n      ...             qlist(numpy.array([\"Arthur Dent\", \"Zaphod Beeblebrox\", \"Ford Prefect\"]), qtype = QSTRING_LIST)])\n      <class 'qpython.qcollection.QTable'> dtype: [('name', 'S10'), ('iq', '<i8'), ('fullname', 'O')] meta: metadata(iq=-7, fullname=0, qtype=98, name=-11): [('Dent', 98L, 'Arthur Dent') ('Beeblebrox', 42L, 'Zaphod Beeblebrox') ('Prefect', 126L, 'Ford Prefect')]\n    \n    :Parameters:\n     - `columns` (list of `strings`) - table column names \n     - `data` (list of lists) - list of columns containing table data\n    \n    :Kwargs:\n     - `meta` (`integer`) - qtype for particular column \n    \n    :returns: `QTable` - representation of q table\n    \n    :raises: `ValueError`\n    '''\n    if len(columns) != len(data):\n        raise ValueError('Number of columns doesn`t match the data layout. %s vs %s' % (len(columns), len(data)))\n\n    meta = {} if not meta else meta\n\n    if not 'qtype' in meta:\n        meta['qtype'] = QTABLE\n\n    dtypes = []\n    for i in range(len(columns)):\n        column_name = columns[i] if isinstance(columns[i], str) else columns[i].decode(\"utf-8\")\n        \n        if isinstance(data[i], str):\n            # convert character list (represented as string) to numpy representation\n            data[i] = numpy.array(list(data[i]), dtype = numpy.string_)\n        if isinstance(data[i], bytes):\n            data[i] = numpy.array(list(data[i].decode()), dtype = numpy.string_)\n\n        if column_name in meta:\n            data[i] = qlist(data[i], qtype = meta[column_name])\n        elif not isinstance(data[i], QList):\n            if type(data[i]) in (list, tuple):\n                data[i] = qlist(data[i], qtype = QGENERAL_LIST)\n            else:\n                data[i] = qlist(data[i])\n\n        \n        meta[column_name] = data[i].meta.qtype\n        dtypes.append((column_name, data[i].dtype))\n\n    table = numpy.core.records.fromarrays(data, dtype = dtypes)\n    table = table.view(QTable)\n\n    table._meta_init(**meta)\n    return table", "response": "Create a QTable out of given column names and data and initialises the \n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef items(self):\n        '''Return a copy of the dictionary's list of ``(key, value)`` pairs.'''\n        return [(self.keys[x], self.values[x]) for x in range(len(self.keys))]", "response": "Return a copy of the dictionary s list of key value pairs."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning an iterator over the dictionary s ( key value pairs.", "response": "def iteritems(self):\n        '''Return an iterator over the dictionary's ``(key, value)`` pairs.'''\n        for x in range(len(self.keys)):\n            yield (self.keys[x], self.values[x])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_url(url, warning=True):\n    parsed = urllib_parse.urlparse(url)\n    query = urllib_parse.parse_qs(parsed.query)\n    is_gdrive = parsed.hostname == 'drive.google.com'\n    is_download_link = parsed.path.endswith('/uc')\n\n    file_id = None\n    if is_gdrive and 'id' in query:\n        file_ids = query['id']\n        if len(file_ids) == 1:\n            file_id = file_ids[0]\n    match = re.match(r'^/file/d/(.*?)/view$', parsed.path)\n    if match:\n        file_id = match.groups()[0]\n\n    if is_gdrive and not is_download_link:\n        warnings.warn(\n            'You specified Google Drive Link but it is not the correct link '\n            \"to download the file. Maybe you should try: {url}\"\n            .format(url='https://drive.google.com/uc?id={}'.format(file_id))\n        )\n\n    return file_id, is_download_link", "response": "Parse URL and return file_id is_download_link is True if it is download link of Google Drive."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nextract all the files in the archive file.", "response": "def extractall(path, to=None):\n    \"\"\"Extract archive file.\n\n    Parameters\n    ----------\n    path: str\n        Path of archive file to be extracted.\n    to: str, optional\n        Directory to which the archive file will be extracted.\n        If None, it will be set to the parent directory of the archive file.\n    \"\"\"\n    if to is None:\n        to = osp.dirname(path)\n\n    if path.endswith('.zip'):\n        opener, mode = zipfile.ZipFile, 'r'\n    elif path.endswith('.tar'):\n        opener, mode = tarfile.open, 'r'\n    elif path.endswith('.tar.gz') or path.endswith('.tgz'):\n        opener, mode = tarfile.open, 'r:gz'\n    elif path.endswith('.tar.bz2') or path.endswith('.tbz'):\n        opener, mode = tarfile.open, 'r:bz2'\n    else:\n        raise ValueError(\"Could not extract '%s' as no appropriate \"\n                         \"extractor is found\" % path)\n\n    def namelist(f):\n        if isinstance(f, zipfile.ZipFile):\n            return f.namelist()\n        return [m.path for m in f.members]\n\n    def filelist(f):\n        files = []\n        for fname in namelist(f):\n            fname = osp.join(to, fname)\n            files.append(fname)\n        return files\n\n    with opener(path, mode) as f:\n        f.extractall(path=to)\n\n    return filelist(f)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_addresses(message: list) -> list:\n    # Need the address so that we know who to send the message back to\n    addresses = []\n    for address in message:\n        # if we hit a blank string, then we've got all the addresses\n        if address == b'':\n            break\n        addresses.append(address)\n\n    return addresses", "response": "Parses a raw list from zmq to get back the addresses that are needed for the message"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nhelping helps you figure out what commands do.", "response": "def help(self, *arg, **kwargs):\n        \"\"\"\n        Help helps you figure out what commands do.\n        Example usage: !help code\n        To see all commands: !commands\n        \"\"\"\n        try:\n            name = arg[0]\n        except IndexError:\n            return 'welcome to vexbot! !commands will list all availabe commands'\n        if any([name.startswith(x) for x in self._prompt.shebangs]):\n            name = name[1:]\n        try:\n            callback = self._commands[name]\n        except KeyError:\n            self.logger.info(' !help not found for: %s', name)\n            return self.help.__doc__\n\n        return callback.__doc__"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef subscribe(self, *args, **kwargs):\n        for address in args:\n            try:\n                self.messaging.subscription_socket.connect(address)\n            except Exception:\n                raise RuntimeError('addresses need to be in the form of: tcp://address_here:port_number'\n                                   ' example: tcp://10.2.3.4:80'\n                                   'address tried {}'.format(address))", "response": "Subscribe to a publish port."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _add_word(completer):\n    def inner(word: str):\n        completer.words.add(word)\n    return inner", "response": "Used to add words to the completors\n    Used to add words to the completors\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _remove_word(completer):\n    def inner(word: str):\n        try:\n            completer.words.remove(word)\n        except Exception:\n            pass\n    return inner", "response": "Used to remove words from the completors\n    Used to remove words from the completors\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run(self):\n        self._scheduler_thread.start()\n\n        self.irc_bot.create_connection()\n        self.irc_bot.add_signal_handlers()\n        event_loop = asyncio.get_event_loop()\n\n        \"\"\"\n        handle_close = _handle_close(messaging,\n                                    event_loop)\n        signal.signal(signal.SIGINT, handle_close)\n        signal.signal(signal.SIGTERM, handle_close)\n        \"\"\"\n        event_loop.run_forever()\n        event_loop.close()", "response": "Starts the scheduler thread."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the log level of the current node.", "response": "def log_level(self,\n              level: typing.Union[str, int]=None,\n              *args,\n              **kwargs) -> typing.Union[None, str]:\n    \"\"\"\n    Args:\n        level:\n\n    Returns:\n        The log level if a `level` is passed in\n    \"\"\"\n    if level is None:\n        return self.root_logger.getEffectiveLevel()\n    # NOTE: `setLevel` takes both string and integers. Try to cast to an integer first\n    try:\n        value = int(level)\n    # if we can't cast to an int, it's probably a string\n    except ValueError:\n        pass\n\n    self.root_logger.setLevel(value)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the log level to INFO", "response": "def set_info(self, *args, **kwargs) -> None:\n    \"\"\"\n    Sets the log level to `INFO`\n    \"\"\"\n    self.root_logger.setLevel(logging.INFO)\n    try:\n        self.messaging.pub_handler.setLevel(logging.INFO)\n    except Exception:\n        pass"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef do_REMOTE(self,\n                  target: str,\n                  remote_command: str,\n                  source: list,\n                  *args,\n                  **kwargs) -> None:\n        \"\"\"\n        Send a remote command to a service. Used \n\n        Args:\n            target: The service that the command gets set to\n            remote_command: The command to do remotely.\n            source: the binary source of the zmq_socket. Packed to send to the\n        \"\"\"\n        if target == self.messaging._service_name:\n            info = 'target for remote command is the bot itself! Returning the function'\n            self.logger.info(info)\n            return self._handle_command(remote_command, source, *args, **kwargs)\n\n        try:\n            target = self.messaging._address_map[target]\n        except KeyError:\n            warn = ' Target %s, not found in addresses. Are you sure that %s sent an IDENT message?'\n            self.logger.warn(warn, target, target)\n            # TODO: raise an error instead of returning?\n            # NOTE: Bail here since there's no point in going forward\n            return\n\n        self.logger.info(' REMOTE %s, target: %s | %s, %s',\n                         remote_command, target, args, kwargs)\n\n        # package the binary together\n        source = target + source\n        self.messaging.send_command_response(source,\n                                             remote_command,\n                                             *args, \n                                             **kwargs)", "response": "Send a remote command to a service. Used \n\n            is a list of strings."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef do_IDENT(self, service_name: str, source: list, *args, **kwargs) -> None:\n        self.logger.info(' IDENT %s as %s', service_name, source)\n        self.messaging._address_map[service_name] = source", "response": "This function is called when the service is identified."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _name_helper(name: str):\n    name = name.rstrip()\n    if name.endswith(('.service', '.socket', '.target')):\n        return name\n    return name + '.service'", "response": "Helper function to return a name with. service"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nhelps helps you figure out what commands do.", "response": "def help(self, *arg, **kwargs):\n    \"\"\"\n    Help helps you figure out what commands do.\n    Example usage: !help code\n    To see all commands: !commands\n    \"\"\"\n    name = arg[0]\n    try:\n        callback = self._commands[name]\n    except KeyError:\n        self._logger.info(' !help not found for: %s', name)\n        return self.do_help.__doc__\n\n    return callback.__doc__"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_vexbot_certificate_filepath() -> (str, bool):\n    public_filepath, secret_filepath = _certificate_helper('vexbot.key','vexbot.key_secret')\n    if path.isfile(secret_filepath):\n        return secret_filepath, True\n    if not path.isfile(public_filepath):\n        err = ('certificates not found. Generate certificates from the '\n               'command line using `vexbot_generate_certificates`')\n        raise FileNotFoundError(err)\n    return public_filepath, False", "response": "Returns the vexbot certificate filepath and whether it is the private\n    filepath or False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_code(self, *args, **kwargs):\n    # FIXME: Honestly should allow multiple commands\n    callback = self._commands[args[0]]\n    # TODO: syntax color would be nice\n    source = _inspect.getsourcelines(callback)[0]\n    \"\"\"\n    source_len = len(source)\n    source = PygmentsLexer(CythonLexer).lex_document(source)()\n    \"\"\"\n    # FIXME: formatting sucks\n    return \"\\n\" + \"\".join(source)", "response": "get the python source code from callback\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_state(self) -> None:\n        # Don't need to go any further if we don't have a message\n        if self.last_message is None:\n            return\n\n        # get the latest message UUID\n        uuid = self.last_message.uuid\n        # Check it against the stored UUID\n        if uuid != self._last_bot_uuid:\n            self.logger.info(' UUID message mismatch')\n            self.logger.debug(' Old UUID: %s | New UUID: %s',\n                              self._last_bot_uuid,\n                              uuid)\n\n            self.send_identity()\n            # Store the latest message UUID now that we've sent an IDENT\n            self._last_bot_uuid = uuid", "response": "Internal method that accomplishes checking to make sure that the UUID message has not changed."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef send_identity(self):\n        service_name = {'service_name': self.messaging._service_name}\n        service_name = _json.dumps(service_name).encode('utf8')\n\n        identify_frame = (b'',\n                          b'IDENT',\n                          _json.dumps([]).encode('utf8'),\n                          service_name)\n\n        # NOTE: Have to do this manually since we built the frame\n        if self.messaging._run_control_loop:\n            # pep8 alias\n            send = self.messaging.command_socket.send_multipart\n            self.messaging.add_callback(send, identify_frame)\n        else:\n            self.messaging.command_socket.send_multipart(identify_frame)\n\n        self.logger.info(' Service Identity sent: %s',\n                         self.messaging._service_name)\n\n        if self.identity_callback:\n            self.identity_callback()", "response": "Send the identity of the service."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _config_convert_to_address_helper(self) -> None:\n        to_address = self._socket_factory.to_address\n        for k, v in self.config.items():\n            if k == 'chatter_subscription_port':\n                continue\n            if k.endswith('port'):\n                self.config[k] = to_address(v)", "response": "Convert the config from ports to zmq ip addresses"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_callback(self, callback: _Callable, *args, **kwargs) -> None:\n        self.loop.add_callback(callback, *args, **kwargs)", "response": "Add a callback to the event loop."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstarting the internal control loop.", "response": "def start(self) -> None:\n        \"\"\"\n        Start the internal control loop. Potentially blocking, depending on\n        the value of `_run_control_loop` set by the initializer.\n        \"\"\"\n        self._setup()\n\n        if self._run_control_loop:\n            asyncio.set_event_loop(asyncio.new_event_loop())\n            self._heartbeat_reciever.start()\n            self._logger.info(' Start Loop')\n            return self.loop.start()\n        else:\n            self._logger.debug(' run_control_loop == False')"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nruns the internal control loop.", "response": "def run(self, blocking: bool=True):\n        \"\"\"\n        Run the internal control loop.\n        Args:\n            blocking (bool): Defaults to `True`. If set to `False`, will\n                intialize a thread to run the control loop.\n        Raises:\n            RuntimeError: If called and not using the internal control loop\n                via `self._run_control_loop`, set in the intializer of the\n                class\n        \"\"\"\n        if not self._run_control_loop:\n            err = (\"`run` called, but not using the internal control loop. Use\"\n                   \" `start` instead\")\n\n            raise RuntimeError(err)\n\n        self._setup()\n        self._heartbeat_reciever.start()\n\n        if blocking:\n            return self.loop.start()\n        else:\n            self._run_thread = _threading.Thread(target=self.loop.start,\n                                                 daemon=True)\n\n            self._thread.run()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsending command to bot.", "response": "def send_command(self, command: str, *args, **kwargs):\n        \"\"\"\n        For request bot to perform some action\n        \"\"\"\n        info = 'send command `%s` to bot. Args: %s | Kwargs: %s'\n        self._messaging_logger.command.info(info, command, args, kwargs)\n\n        command = command.encode('utf8')\n        # target = target.encode('ascii')\n        args = _json.dumps(args).encode('utf8')\n        kwargs = _json.dumps(kwargs).encode('utf8')\n        frame = (b'', command, args, kwargs)\n        debug = ' send command run_control_loop: %s'\n        self._messaging_logger.command.debug(debug, self._run_control_loop)\n\n        if self._run_control_loop:\n            self.add_callback(self.command_socket.send_multipart, frame)\n        else:\n            self.command_socket.send_multipart(frame)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsend a command response to the command_socket.", "response": "def send_command_response(self,\n                              source: list,\n                              command: str,\n                              *args,\n                              **kwargs):\n        \"\"\"\n        Used in bot observer `on_next` method\n        \"\"\"\n        args = _json.dumps(args).encode('utf8')\n        kwargs = _json.dumps(kwargs).encode('utf8')\n        if isinstance(source, list):\n            frame = (*source, b'', command.encode('utf8'), args, kwargs)\n        else:\n            frame = (b'', command.encode('utf8'), args, kwargs)\n        if self._run_control_loop:\n            self.add_callback(self.command_socket.send_multipart, frame)\n        else:\n            self.command_socket.send_multipart(frame)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates and connects or binds the sockets with the specified address.", "response": "def create_n_connect(self,\n                         socket_type,\n                         address: str,\n                         bind=False,\n                         on_error='log',\n                         socket_name=''):\n        \"\"\"\n        Creates and connects or binds the sockets\n        on_error:\n            'log': will log error\n            'exit': will exit the program\n        socket_name:\n            used for troubleshooting/logging\n        \"\"\"\n        self.logger.debug('create and connect: %s %s %s',\n                          socket_type, socket_name, address)\n\n        socket = self.context.socket(socket_type)\n        if self.using_auth:\n            self._create_using_auth(socket, address, bind, on_error, socket_name)\n        else:\n            self._create_no_auth(socket, address, bind, on_error, socket_name)\n\n        return socket"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef to_address(self, port: str):\n        # check to see if user passed in a string\n        # if they did, they want to use that instead\n        if isinstance(port, str) and len(port) > 6:\n            return port\n\n        zmq_address = '{}://{}:{}'\n        return zmq_address.format(self.protocol,\n                                  self.address,\n                                  port)", "response": "transforms the ports into addresses. Will fall through if the port is not a string."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntransforming an iterable, or the expectation of an iterable into zmq addresses", "response": "def iterate_multiple_addresses(self, ports: (str, list, tuple)):\n        \"\"\"\n        transforms an iterable, or the expectation of an iterable\n        into zmq addresses\n        \"\"\"\n        if isinstance(ports, (str, int)):\n            # TODO: verify this works.\n            ports = tuple(ports,)\n\n        result = []\n        for port in ports:\n            result.append(self.to_address(port))\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating the client and server CURVE certificates.", "response": "def generate_certificates(base_dir: str, remove_certificates: bool=False):\n    ''' Generate client and server CURVE certificate files'''\n    public_keys_dir = os.path.join(base_dir, 'public_keys')\n    secret_keys_dir = os.path.join(base_dir, 'private_keys')\n\n    # Make the public and private key directories\n    for path in (public_keys_dir, secret_keys_dir):\n        if not os.path.exists(path):\n            os.mkdir(path)\n\n    # create new keys in certificates dir\n    server_public_file, server_secret_file = zmq.auth.create_certificates(base_dir, \"vexbot\")\n    client_public_file, client_secret_file = zmq.auth.create_certificates(base_dir, \"client\")\n\n    vexbot_public = os.path.join(public_keys_dir, 'vexbot.key')\n    client_public = os.path.join(public_keys_dir, 'client.key')\n\n    vexbot_secret = os.path.join(secret_keys_dir, 'vexbot.key_secret')\n    client_secret = os.path.join(secret_keys_dir, 'client.key_secret')\n\n    if os.path.exists(vexbot_public) and remove_certificates:\n        os.unlink(vexbot_public)\n        try:\n            os.unlink(vexbot_secret)\n        except OSError:\n            pass\n    elif os.path.exists(vexbot_public) and not remove_certificates:\n        os.unlink(server_public_file)\n        os.unlink(server_secret_file)\n\n    if os.path.exists(client_public) and remove_certificates:\n        os.unlink(client_public)\n        try:\n            os.unlink(client_secret)\n        except OSError:\n            pass\n    elif os.path.exists(client_public) and not remove_certificates:\n        os.unlink(client_public_file)\n        os.unlink(client_secret_file)\n\n    # move public keys to appropriate directory\n    for key_file in os.listdir(base_dir):\n        if key_file.endswith(\".key\"):\n            shutil.move(os.path.join(base_dir, key_file),\n                        os.path.join(public_keys_dir, '.'))\n\n    # move secret keys to appropriate directory\n    for key_file in os.listdir(base_dir):\n        if key_file.endswith(\".key_secret\"):\n            shutil.move(os.path.join(base_dir, key_file),\n                        os.path.join(secret_keys_dir, '.'))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_command(self, text: str) -> bool:\n        if text[0] in self.shebangs:\n            return True\n\n        return False", "response": "Checks if the given text is a command."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _handle_text(self, text: str):\n        # If first word is command\n        if self.is_command(text):\n            self._logger.debug(' first word is a command')\n            # get the command, args, and kwargs out using `shlex`\n            command, args, kwargs = _get_cmd_args_kwargs(text)\n            self._logger.info(' command: %s, %s %s', command, args, kwargs)\n            # hand off to the `handle_command` method\n            result = self._handle_command(command, args, kwargs)\n\n            if result:\n                if isinstance(result, str):\n                    print(result)\n                else:\n                    _pprint.pprint(result)\n            # Exit the method here if in this block\n            return\n        # Else first word is not a command\n        else:\n            self._logger.debug(' first word is not a command')\n            # get the first word and then the rest of the text.\n            try:\n                first_word, second_word = text.split(' ', 1)\n                self._logger.debug(' first word: %s', first_word)\n            except ValueError:\n                self._logger.debug('No second word in chain!')\n                return self._handle_NLP(text)\n            # check if second word/string is a command\n            if self.is_command(second_word):\n                self._logger.info(' second word is a command')\n                # get the command, args, and kwargs out using `shlex`\n                command, args, kwargs = _get_cmd_args_kwargs(second_word)\n                self._logger.debug(' second word: %s', command)\n                self._logger.debug(' command %s', command)\n                self._logger.debug('args %s ', args)\n                self._logger.debug('kwargs %s', kwargs)\n                return self._first_word_not_cmd(first_word, command, args, kwargs)\n            # if second word is not a command, default to NLP\n            else:\n                self._logger.info(' defaulting to message since second word '\n                                  'isn\\'t a command')\n\n                return self._handle_NLP(text)", "response": "Handles the text in the block of text."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking to see if this is an author or service. This method does high level control handling", "response": "def _first_word_not_cmd(self,\n                            first_word: str,\n                            command: str,\n                            args: tuple,\n                            kwargs: dict) -> None:\n        \"\"\"\n        check to see if this is an author or service.\n        This method does high level control handling\n        \"\"\"\n        if self.service_interface.is_service(first_word):\n            self._logger.debug(' first word is a service')\n            kwargs = self.service_interface.get_metadata(first_word, kwargs)\n            self._logger.debug(' service transform kwargs: %s', kwargs)\n        elif self.author_interface.is_author(first_word):\n            self._logger.debug(' first word is an author')\n            kwargs = self.author_interface.get_metadata(first_word, kwargs)\n            self._logger.debug(' author transform kwargs: %s', kwargs)\n        if not kwargs.get('remote'):\n            kwargs['remote_command'] = command\n            command= 'REMOTE'\n            self.messaging.send_command(command, *args, **kwargs)\n            return\n        else:\n            self.messaging.send_command(command, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get(self, key, default=None):\n        if key not in self.table:\n            return default\n\n        return self[key]", "response": "Get an item - return default if not present"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the spacy entities from the text.", "response": "def get_spacy_entites(self, text: str) -> list:\n        \"\"\"\n        Pretrained (spaCy)\n        Uses averaged preceptron\n        Places, dates, people, organizations\n        \"\"\"\n        # NOTE: spaCy doc\n        doc = self._language_model(text)\n\n        entities = [{\"entity\": ent.label_,\n                     \"value\": ent.text,\n                     \"start\": ent.start_char,\n                     \"end\": ent.end_char} for ent in doc.ents]\n\n        return entities"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert a word into discrete features in self. crf_features including word before and word after.", "response": "def _sentence_to_features(self, sentence: list):\n        # type: (List[Tuple[Text, Text, Text, Text]]) -> List[Dict[Text, Any]]\n        \"\"\"Convert a word into discrete features in self.crf_features,\n        including word before and word after.\"\"\"\n\n        sentence_features = []\n        prefixes = ('-1', '0', '+1')\n\n        for word_idx in range(len(sentence)):\n            # word before(-1), current word(0), next word(+1)\n            word_features = {}\n            for i in range(3):\n                if word_idx == len(sentence) - 1 and i == 2:\n                    word_features['EOS'] = True\n                    # End Of Sentence\n                elif word_idx == 0 and i == 0:\n                    word_features['BOS'] = True\n                    # Beginning Of Sentence\n                else:\n                    word = sentence[word_idx - 1 + i]\n                    prefix = prefixes[i]\n                    features = self.crf_features[i]\n                    for feature in features:\n                        # append each feature to a feature vector\n                        value = self.function_dict[feature](word)\n                        word_features[prefix + \":\" + feature] = value\n            sentence_features.append(word_features)\n        return sentence_features"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _setup(self):\n        self._close_sockets()\n        create_n_conn = self._socket_factory.create_n_connect\n\n        control_address = self.config['control_port']\n        publish_address = self.config['chatter_publish_port']\n        request_address = self.config['request_port']\n        command_address = self.config['command_port']\n\n        self._messaging_logger.control.info(' Address: %s', control_address)\n        # NOTE: These sockets will exit the program to exit if there's an issue\n        self.control_socket = create_n_conn(zmq.ROUTER,\n                                            control_address,\n                                            on_error='exit',\n                                            bind=True,\n                                            socket_name='control socket')\n        self._messaging_logger.publish.info(' Address: %s', publish_address)\n        # publish socket is an XSUB socket\n        self.publish_socket = create_n_conn(zmq.XSUB,\n                                            publish_address,\n                                            bind=True,\n                                            on_error='exit',\n                                            socket_name='publish socket')\n        self._messaging_logger.command.info(' Address: %s', command_address)\n        # NOTE: these sockets will only log an error if there's an issue\n        self.command_socket = create_n_conn(zmq.ROUTER,\n                                            command_address,\n                                            bind=True)\n        self._messaging_logger.request.info(' Address: %s', request_address)\n        self.request_socket = create_n_conn(zmq.DEALER,\n                                            request_address,\n                                            bind=True,\n                                            socket_name='request socket')\n\n        self._setup_subscription_socket()\n        self._setup_scheduler()", "response": "Creates the sockets for the messaging and sets up the appropriate sockets."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsend a MSG command to the device.", "response": "def send_request(self, target: str, *args, **kwargs):\n        \"\"\"\n        address must a list instance. Or a string which will be transformed into a address\n        \"\"\"\n        # TODO: Log error here if not found?\n        address = self._get_address_from_source(target)\n        if address is None:\n            return\n\n        args = json.dumps(args).encode('utf8')\n        kwargs = json.dumps(kwargs).encode('utf8')\n\n        # TODO: test that this works\n        # NOTE: pop out command?\n        frame = (*address, b'', b'MSG', args, kwargs)\n        self.add_callback(self.command_socket.send_multipart, frame)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsend a command response to the server.", "response": "def send_command_response(self, source: list, command: str, *args, **kwargs):\n        \"\"\"\n        Used in bot observer `on_next` method\n        \"\"\"\n        args = json.dumps(args).encode('utf8')\n        kwargs = json.dumps(kwargs).encode('utf8')\n        frame = (*source, b'', command.encode('utf8'), args, kwargs)\n        self.add_callback(self.command_socket.send_multipart, frame)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cli(\n        template_variables, template, output, variables, extensions,\n        encoding, include_path, no_variable_file, no_extension_file,\n        no_trim_blocks, no_lstrip_blocks, remove_trailing_newline,\n        mode, m, md):\n    \"\"\"Reads the given Jinja TEMPLATE and renders its content\n    into a new file. For example, a template called 'foo.c.j2'\n    will be written into 'foo.c' in case the output file is not\n    explicitly given.\n\n    Template variables can be defined in a separate file or\n    given as part of the command-line call, e.g.\n\n        yasha --hello=world -o output.txt template.j2\n\n    defines a variable 'hello' for a template like:\n\n        Hello {{ hello }} !\n    \"\"\"\n\n    # Set the encoding of the template file\n    if encodings.search_function(encoding) is None:\n        msg = \"Unrecognized encoding name '{}'\"\n        raise ClickException(msg.format(encoding))\n    yasha.ENCODING = encoding\n\n    # Append include path of referenced templates\n    include_path = [os.path.dirname(template.name)] + list(include_path)\n\n    if not extensions or not variables:\n        template_companion = yasha.find_template_companion(template.name)\n        template_companion = list(template_companion)\n\n    if not extensions and not no_extension_file:\n        for file in template_companion:\n            if file.endswith(yasha.EXTENSION_FILE_FORMATS):\n                extensions = click.open_file(file, \"rb\")\n                break\n\n    if extensions:\n        load_extensions(extensions)\n\n    if not variables and not no_variable_file:\n        for file in template_companion:\n            if file.endswith(tuple(PARSERS.keys())):\n                variables = (click.open_file(file, \"rb\"),)\n                break\n\n    if not output:\n        if template.name == \"<stdin>\":\n            output = click.open_file(\"-\", \"wb\")\n        else:\n            output = os.path.splitext(template.name)[0]\n            output = click.open_file(output, \"wb\", lazy=True)\n\n    if m or md:\n        deps = [os.path.relpath(template.name)]\n        for file in variables:\n            deps.append(os.path.relpath(file.name))\n        if extensions:\n            deps.append(os.path.relpath(extensions.name))\n        for d in yasha.find_referenced_templates(template, include_path):\n            deps.append(os.path.relpath(d))\n\n        deps = os.path.relpath(output.name) + \": \" + \" \".join(deps)\n        if m:\n            click.echo(deps)\n            return  # Template won't be rendered\n        if md:\n            deps += os.linesep\n            output_d = click.open_file(output.name + \".d\", \"wb\")\n            output_d.write(deps.encode(yasha.ENCODING))\n\n    # Load Jinja\n    jinja = yasha.load_jinja(\n        path=include_path,\n        tests=TESTS,\n        filters=FILTERS,\n        classes=CLASSES,\n        mode=mode,\n        trim_blocks=not no_trim_blocks,\n        lstrip_blocks=not no_lstrip_blocks,\n        keep_trailing_newline=not remove_trailing_newline\n   )\n\n    # Get template\n    if template.name == \"<stdin>\":\n        stdin = template.read()\n        t = jinja.from_string(stdin.decode(yasha.ENCODING))\n    else:\n        t = jinja.get_template(os.path.basename(template.name))\n\n    # Parse variables\n    context = dict()\n    for file in variables:\n        context.update(parse_variable_file(file))\n    context.update(yasha.parse_cli_variables(template_variables))\n\n    # Finally render template and save it\n    try:\n        t_stream = t.stream(context)\n        t_stream.enable_buffering(size=5)\n        t_stream.dump(output, encoding=yasha.ENCODING)\n    except JinjaUndefinedError as e:\n        raise ClickException(\"Variable {}\".format(e))", "response": "This function is used to run the command - line interface for the Yahoo Yahoo! Yasha CLI."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_template_companion(template, extension='', check=True):\n\n    if check and not os.path.isfile(template):\n        yield ''\n        return # May be '<stdin>' (click)\n\n    template = os.path.abspath(template)\n    template_dirname = os.path.dirname(template)\n    template_basename = os.path.basename(template).split('.')\n\n    current_path = template_dirname\n    stop_path = os.path.commonprefix((os.getcwd(), current_path))\n    stop_path = os.path.dirname(stop_path)\n\n    token = template_basename[0] + '.'\n\n    while True:\n\n        for file in sorted(os.listdir(current_path)):\n            if not file.startswith(token):\n                continue\n            if not file.endswith(extension):\n                continue\n\n            file_parts = file.split('.')\n            for i in range(1, len(template_basename)):\n                if template_basename[:-i] != file_parts[:-1]:\n                    continue\n                if current_path == template_dirname:\n                    if file_parts == template_basename:\n                        continue # Do not accept template itself\n\n                yield os.path.join(current_path, file)\n\n        if current_path == stop_path:\n            break\n\n        # cd ..\n        current_path = os.path.split(current_path)[0]", "response": "Find the first template companion file in the directory tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfinds the files that can be imported extended or included within a template.", "response": "def find_referenced_templates(template, search_path):\n    \"\"\"\n    Returns a list of files which can be either {% imported %},\n    {% extended %} or {% included %} within a template.\n    \"\"\"\n    from jinja2 import Environment, meta\n    env = Environment()\n    ast = env.parse(template.read())\n    referenced_templates = list(meta.find_referenced_templates(ast))\n\n    def realpath(tpl):\n        for path in search_path:\n            t = os.path.realpath(os.path.join(path, tpl))\n            if os.path.isfile(t):\n                return t\n        return None\n\n    return [realpath(t) for t in referenced_templates if t is not None]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npopulates object variables from an SVD element", "response": "def from_element(self, element, defaults={}):\n        \"\"\"Populate object variables from SVD element\"\"\"\n        if isinstance(defaults, SvdElement):\n            defaults = vars(defaults)\n        for key in self.props:\n            try:\n                value = element.find(key).text\n            except AttributeError:  # Maybe it's attribute?\n                default = defaults[key] if key in defaults else None\n                value = element.get(key, default)\n            if value is not None:\n                if key in self.props_to_integer:\n                    try:\n                        value = int(value)\n                    except ValueError:  # It has to be hex\n                        value = int(value, 16)\n                elif key in self.props_to_boolean:\n                    value = value.lower() in (\"yes\", \"true\", \"t\", \"1\")\n\n            setattr(self, key, value)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfold the register in accordance with its dimensions.", "response": "def fold(self):\n        \"\"\"Folds the Register in accordance with it's dimensions.\n\n        If the register is dimensionless, the returned list just\n        contains the register itself unchanged. In case the register\n        name looks like a C array, the returned list contains the register\n        itself, where nothing else than the '%s' placeholder in it's name\n        has been replaced with value of the dim element.\n        \"\"\"\n        if self.dim is None:\n            return [self]\n        if self.name.endswith(\"[%s]\"):  # C array like\n            self.name = self.name.replace(\"%s\", str(self.dim))\n            return [self]\n\n        registers = []\n        for offset, index in enumerate(self.dimIndex):\n            reg = self.copy()\n            reg.name = self.name.replace(\"%s\", str(index))\n            reg.addressOffset += offset * reg.dimIncrement\n\n            reg.fields = [field.copy() for field in reg.fields]\n            for field in reg.fields:\n                field.parent = reg\n\n            reg.dim = reg.dimIndex = reg.dimIncrement = None  # Dimensionless\n            registers.append(reg)\n\n        return registers"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a device name that associated network communication direction.", "response": "def get_tc_device(self):\n        \"\"\"\n        Return a device name that associated network communication direction.\n        \"\"\"\n\n        if self.direction == TrafficDirection.OUTGOING:\n            return self.device\n\n        if self.direction == TrafficDirection.INCOMING:\n            return self.ifb_device\n\n        raise ParameterError(\n            \"unknown direction\", expected=TrafficDirection.LIST, value=self.direction\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delete_tc(self):\n\n        rule_finder = TcShapingRuleFinder(logger=logger, tc=self)\n        filter_param = rule_finder.find_filter_param()\n\n        if not filter_param:\n            message = \"shaping rule not found ({}).\".format(rule_finder.get_filter_string())\n            if rule_finder.is_empty_filter_condition():\n                message += \" you can delete all of the shaping rules with --all option.\"\n            logger.error(message)\n\n            return 1\n\n        logger.info(\"delete a shaping rule: {}\".format(dict(filter_param)))\n\n        filter_del_command = (\n            \"{command:s} del dev {dev:s} protocol {protocol:s} \"\n            \"parent {parent:} handle {handle:s} prio {prio:} u32\".format(\n                command=get_tc_base_command(TcSubCommand.FILTER),\n                dev=rule_finder.get_parsed_device(),\n                protocol=filter_param.get(Tc.Param.PROTOCOL),\n                parent=\"{:s}:\".format(rule_finder.find_parent().split(\":\")[0]),\n                handle=filter_param.get(Tc.Param.FILTER_ID),\n                prio=filter_param.get(Tc.Param.PRIORITY),\n            )\n        )\n\n        result = run_command_helper(\n            command=filter_del_command, ignore_error_msg_regexp=None, notice_msg=None\n        )\n\n        rule_finder.clear()\n        if not rule_finder.is_any_filter():\n            logger.debug(\"there are no filters remain. delete qdiscs.\")\n            self.delete_all_tc()\n\n        return result", "response": "Delete a specific shaping rule."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef request(identifier, namespace='cid', domain='compound', operation=None, output='JSON', searchtype=None, **kwargs):\n    if not identifier:\n        raise ValueError('identifier/cid cannot be None')\n    # If identifier is a list, join with commas into string\n    if isinstance(identifier, int):\n        identifier = str(identifier)\n    if not isinstance(identifier, text_types):\n        identifier = ','.join(str(x) for x in identifier)\n    # Filter None values from kwargs\n    kwargs = dict((k, v) for k, v in kwargs.items() if v is not None)\n    # Build API URL\n    urlid, postdata = None, None\n    if namespace == 'sourceid':\n        identifier = identifier.replace('/', '.')\n    if namespace in ['listkey', 'formula', 'sourceid'] \\\n            or searchtype == 'xref' \\\n            or (searchtype and namespace == 'cid') or domain == 'sources':\n        urlid = quote(identifier.encode('utf8'))\n    else:\n        postdata = urlencode([(namespace, identifier)]).encode('utf8')\n    comps = filter(None, [API_BASE, domain, searchtype, namespace, urlid, operation, output])\n    apiurl = '/'.join(comps)\n    if kwargs:\n        apiurl += '?%s' % urlencode(kwargs)\n    # Make request\n    try:\n        log.debug('Request URL: %s', apiurl)\n        log.debug('Request data: %s', postdata)\n        response = urlopen(apiurl, postdata)\n        return response\n    except HTTPError as e:\n        raise PubChemHTTPError(e)", "response": "Construct API request from parameters and return the response."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nrequests wrapper that automatically handles async requests.", "response": "def get(identifier, namespace='cid', domain='compound', operation=None, output='JSON', searchtype=None, **kwargs):\n    \"\"\"Request wrapper that automatically handles async requests.\"\"\"\n    if (searchtype and searchtype != 'xref') or namespace in ['formula']:\n        response = request(identifier, namespace, domain, None, 'JSON', searchtype, **kwargs).read()\n        status = json.loads(response.decode())\n        if 'Waiting' in status and 'ListKey' in status['Waiting']:\n            identifier = status['Waiting']['ListKey']\n            namespace = 'listkey'\n            while 'Waiting' in status and 'ListKey' in status['Waiting']:\n                time.sleep(2)\n                response = request(identifier, namespace, domain, operation, 'JSON', **kwargs).read()\n                status = json.loads(response.decode())\n            if not output == 'JSON':\n                response = request(identifier, namespace, domain, operation, output, searchtype, **kwargs).read()\n    else:\n        response = request(identifier, namespace, domain, operation, output, searchtype, **kwargs).read()\n    return response"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwrapping that automatically parses JSON response and supresses NotFoundError.", "response": "def get_json(identifier, namespace='cid', domain='compound', operation=None, searchtype=None, **kwargs):\n    \"\"\"Request wrapper that automatically parses JSON response and supresses NotFoundError.\"\"\"\n    try:\n        return json.loads(get(identifier, namespace, domain, operation, 'JSON', searchtype, **kwargs).decode())\n    except NotFoundError as e:\n        log.info(e)\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_sdf(identifier, namespace='cid', domain='compound',operation=None, searchtype=None, **kwargs):\n    try:\n        return get(identifier, namespace, domain, operation, 'SDF', searchtype, **kwargs).decode()\n    except NotFoundError as e:\n        log.info(e)\n        return None", "response": "Request wrapper that automatically parses SDF response and supresses NotFoundError."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nretrieve the specified compound records from PubChem.", "response": "def get_compounds(identifier, namespace='cid', searchtype=None, as_dataframe=False, **kwargs):\n    \"\"\"Retrieve the specified compound records from PubChem.\n\n    :param identifier: The compound identifier to use as a search query.\n    :param namespace: (optional) The identifier type, one of cid, name, smiles, sdf, inchi, inchikey or formula.\n    :param searchtype: (optional) The advanced search type, one of substructure, superstructure or similarity.\n    :param as_dataframe: (optional) Automatically extract the :class:`~pubchempy.Compound` properties into a pandas\n                         :class:`~pandas.DataFrame` and return that.\n    \"\"\"\n    results = get_json(identifier, namespace, searchtype=searchtype, **kwargs)\n    compounds = [Compound(r) for r in results['PC_Compounds']] if results else []\n    if as_dataframe:\n        return compounds_to_frame(compounds)\n    return compounds"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_substances(identifier, namespace='sid', as_dataframe=False, **kwargs):\n    results = get_json(identifier, namespace, 'substance', **kwargs)\n    substances = [Substance(r) for r in results['PC_Substances']] if results else []\n    if as_dataframe:\n        return substances_to_frame(substances)\n    return substances", "response": "Retrieve the specified substance records from PubChem."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_assays(identifier, namespace='aid', **kwargs):\n    results = get_json(identifier, namespace, 'assay', 'description', **kwargs)\n    return [Assay(r) for r in results['PC_AssayContainer']] if results else []", "response": "Retrieve the specified assay records from PubChem."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_properties(properties, identifier, namespace='cid', searchtype=None, as_dataframe=False, **kwargs):\n    if isinstance(properties, text_types):\n        properties = properties.split(',')\n    properties = ','.join([PROPERTY_MAP.get(p, p) for p in properties])\n    properties = 'property/%s' % properties\n    results = get_json(identifier, namespace, 'compound', properties, searchtype=searchtype, **kwargs)\n    results = results['PropertyTable']['Properties'] if results else []\n    if as_dataframe:\n        import pandas as pd\n        return pd.DataFrame.from_records(results, index='CID')\n    return results", "response": "Retrieve the specified properties from PubChem."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef download(outformat, path, identifier, namespace='cid', domain='compound', operation=None, searchtype=None,\n             overwrite=False, **kwargs):\n    \"\"\"Format can be  XML, ASNT/B, JSON, SDF, CSV, PNG, TXT.\"\"\"\n    response = get(identifier, namespace, domain, operation, outformat, searchtype, **kwargs)\n    if not overwrite and os.path.isfile(path):\n        raise IOError(\"%s already exists. Use 'overwrite=True' to overwrite it.\" % path)\n    with open(path, 'wb') as f:\n        f.write(response)", "response": "Download the current node from the cluster."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef deprecated(message=None):\n    def deco(func):\n        @functools.wraps(func)\n        def wrapped(*args, **kwargs):\n            warnings.warn(\n                message or 'Call to deprecated function {}'.format(func.__name__),\n                category=PubChemPyDeprecationWarning,\n                stacklevel=2\n            )\n            return func(*args, **kwargs)\n        return wrapped\n    return deco", "response": "Decorator to mark functions as deprecated. A warning will be emitted when the function is used."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _parse_prop(search, proplist):\n    props = [i for i in proplist if all(item in i['urn'].items() for item in search.items())]\n    if len(props) > 0:\n        return props[0]['value'][list(props[0]['value'].keys())[0]]", "response": "Extract property value from record using the given urn search filter."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconstructs a pandas. DataFrame from a list of Pubchem objects.", "response": "def compounds_to_frame(compounds, properties=None):\n    \"\"\"Construct a pandas :class:`~pandas.DataFrame` from a list of :class:`~pubchempy.Compound` objects.\n\n    Optionally specify a list of the desired :class:`~pubchempy.Compound` properties.\n    \"\"\"\n    import pandas as pd\n    if isinstance(compounds, Compound):\n        compounds = [compounds]\n    properties = set(properties) | set(['cid']) if properties else None\n    return pd.DataFrame.from_records([c.to_dict(properties) for c in compounds], index='cid')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef substances_to_frame(substances, properties=None):\n    import pandas as pd\n    if isinstance(substances, Substance):\n        substances = [substances]\n    properties = set(properties) | set(['sid']) if properties else None\n    return pd.DataFrame.from_records([s.to_dict(properties) for s in substances], index='sid')", "response": "Construct a pandas. DataFrame from a list of ~pubchempy. Substance objects."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_dict(self):\n        data = {'aid': self.aid, 'number': self.number, 'element': self.element}\n        for coord in {'x', 'y', 'z'}:\n            if getattr(self, coord) is not None:\n                data[coord] = getattr(self, coord)\n        if self.charge is not 0:\n            data['charge'] = self.charge\n        return data", "response": "Return a dictionary containing Atom data."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_coordinates(self, x, y, z=None):\n        self.x = x\n        self.y = y\n        self.z = z", "response": "Set all coordinate dimensions at once."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a dictionary containing Bond data.", "response": "def to_dict(self):\n        \"\"\"Return a dictionary containing Bond data.\"\"\"\n        data = {'aid1': self.aid1, 'aid2': self.aid2, 'order': self.order}\n        if self.style is not None:\n            data['style'] = self.style\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _setup_atoms(self):\n        # Delete existing atoms\n        self._atoms = {}\n        # Create atoms\n        aids = self.record['atoms']['aid']\n        elements = self.record['atoms']['element']\n        if not len(aids) == len(elements):\n            raise ResponseParseError('Error parsing atom elements')\n        for aid, element in zip(aids, elements):\n            self._atoms[aid] = Atom(aid=aid, number=element)\n        # Add coordinates\n        if 'coords' in self.record:\n            coord_ids = self.record['coords'][0]['aid']\n            xs = self.record['coords'][0]['conformers'][0]['x']\n            ys = self.record['coords'][0]['conformers'][0]['y']\n            zs = self.record['coords'][0]['conformers'][0].get('z', [])\n            if not len(coord_ids) == len(xs) == len(ys) == len(self._atoms) or (zs and not len(zs) == len(coord_ids)):\n                raise ResponseParseError('Error parsing atom coordinates')\n            for aid, x, y, z in zip_longest(coord_ids, xs, ys, zs):\n                self._atoms[aid].set_coordinates(x, y, z)\n        # Add charges\n        if 'charge' in self.record['atoms']:\n            for charge in self.record['atoms']['charge']:\n                self._atoms[charge['aid']].charge = charge['value']", "response": "Derive Atom objects from the record."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nderive Bond objects from the record.", "response": "def _setup_bonds(self):\n        \"\"\"Derive Bond objects from the record.\"\"\"\n        self._bonds = {}\n        if 'bonds' not in self.record:\n            return\n        # Create bonds\n        aid1s = self.record['bonds']['aid1']\n        aid2s = self.record['bonds']['aid2']\n        orders = self.record['bonds']['order']\n        if not len(aid1s) == len(aid2s) == len(orders):\n            raise ResponseParseError('Error parsing bonds')\n        for aid1, aid2, order in zip(aid1s, aid2s, orders):\n            self._bonds[frozenset((aid1, aid2))] = Bond(aid1=aid1, aid2=aid2, order=order)\n        # Add styles\n        if 'coords' in self.record and 'style' in self.record['coords'][0]['conformers'][0]:\n            aid1s = self.record['coords'][0]['conformers'][0]['style']['aid1']\n            aid2s = self.record['coords'][0]['conformers'][0]['style']['aid2']\n            styles = self.record['coords'][0]['conformers'][0]['style']['annotation']\n            for aid1, aid2, style in zip(aid1s, aid2s, styles):\n                self._bonds[frozenset((aid1, aid2))].style = style"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves the Compound record for the specified CID.", "response": "def from_cid(cls, cid, **kwargs):\n        \"\"\"Retrieve the Compound record for the specified CID.\n\n        Usage::\n\n            c = Compound.from_cid(6819)\n\n        :param int cid: The PubChem Compound Identifier (CID).\n        \"\"\"\n        record = json.loads(request(cid, **kwargs).read().decode())['PC_Compounds'][0]\n        return cls(record)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef to_dict(self, properties=None):\n        if not properties:\n            skip = {'aids', 'sids', 'synonyms'}\n            properties = [p for p in dir(Compound) if isinstance(getattr(Compound, p), property) and p not in skip]\n        return {p: [i.to_dict() for i in getattr(self, p)] if p in {'atoms', 'bonds'} else getattr(self, p) for p in properties}", "response": "Return a dictionary containing the data of the current object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_series(self, properties=None):\n        import pandas as pd\n        return pd.Series(self.to_dict(properties))", "response": "Return a pandas. Series containing the Compound data."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cid(self):\n        if 'id' in self.record and 'id' in self.record['id'] and 'cid' in self.record['id']['id']:\n            return self.record['id']['id']['cid']", "response": "The PubChem Compound Identifier."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef atoms(self):\n        return sorted(self._atoms.values(), key=lambda x: x.aid)", "response": "List of Atoms in this Compound."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef bonds(self):\n        return sorted(self._bonds.values(), key=lambda x: (x.aid1, x.aid2))", "response": "List of bonds between atoms in this Compound."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of all the sids in the user s account.", "response": "def sids(self):\n        \"\"\"Requires an extra request. Result is cached.\"\"\"\n        if self.cid:\n            results = get_json(self.cid, operation='sids')\n            return results['InformationList']['Information'][0]['SID'] if results else []"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nrequires an extra request. Result is cached.", "response": "def aids(self):\n        \"\"\"Requires an extra request. Result is cached.\"\"\"\n        if self.cid:\n            results = get_json(self.cid, operation='aids')\n            return results['InformationList']['Information'][0]['AID'] if results else []"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef from_sid(cls, sid):\n        record = json.loads(request(sid, 'sid', 'substance').read().decode())['PC_Substances'][0]\n        return cls(record)", "response": "Retrieve the Substance record for the specified SID."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_dict(self, properties=None):\n        if not properties:\n            skip = {'deposited_compound', 'standardized_compound', 'cids', 'aids'}\n            properties = [p for p in dir(Substance) if isinstance(getattr(Substance, p), property) and p not in skip]\n        return {p: getattr(self, p) for p in properties}", "response": "Return a dictionary containing the data for the current object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef standardized_compound(self):\n        for c in self.record['compound']:\n            if c['id']['type'] == CompoundIdType.STANDARDIZED:\n                return Compound.from_cid(c['id']['id']['cid'])", "response": "Return the standardized compound that was produced when this Substance was standardized."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a Compound produced from the unstandardized Substance record as deposited.", "response": "def deposited_compound(self):\n        \"\"\"Return a :class:`~pubchempy.Compound` produced from the unstandardized Substance record as deposited.\n\n        The resulting :class:`~pubchempy.Compound` will not have a ``cid`` and will be missing most properties.\n        \"\"\"\n        for c in self.record['compound']:\n            if c['id']['type'] == CompoundIdType.DEPOSITED:\n                return Compound(c)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef from_aid(cls, aid):\n        record = json.loads(request(aid, 'aid', 'assay', 'description').read().decode())['PC_AssayContainer'][0]\n        return cls(record)", "response": "Retrieve the Assay record for the specified AID."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_dict(self, properties=None):\n        if not properties:\n            properties = [p for p in dir(Assay) if isinstance(getattr(Assay, p), property)]\n        return {p: getattr(self, p) for p in properties}", "response": "Return a dictionary containing Assay data."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _is_possible_token(token, token_length=6):\n    if not isinstance(token, bytes):\n        token = six.b(str(token))\n    return token.isdigit() and len(token) <= token_length", "response": "Determines if given value is acceptable as a token. Used when validating\n    tokens."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_hotp(\n        secret,\n        intervals_no,\n        as_string=False,\n        casefold=True,\n        digest_method=hashlib.sha1,\n        token_length=6,\n):\n    \"\"\"\n    Get HMAC-based one-time password on the basis of given secret and\n    interval number.\n\n    :param secret: the base32-encoded string acting as secret key\n    :type secret: str or unicode\n    :param intervals_no: interval number used for getting different tokens, it\n        is incremented with each use\n    :type intervals_no: int\n    :param as_string: True if result should be padded string, False otherwise\n    :type as_string: bool\n    :param casefold: True (default), if should accept also lowercase alphabet\n    :type casefold: bool\n    :param digest_method: method of generating digest (hashlib.sha1 by default)\n    :type digest_method: callable\n    :param token_length: length of the token (6 by default)\n    :type token_length: int\n    :return: generated HOTP token\n    :rtype: int or str\n\n    >>> get_hotp(b'MFRGGZDFMZTWQ2LK', intervals_no=1)\n    765705\n    >>> get_hotp(b'MFRGGZDFMZTWQ2LK', intervals_no=2)\n    816065\n    >>> result = get_hotp(b'MFRGGZDFMZTWQ2LK', intervals_no=2, as_string=True)\n    >>> result == b'816065'\n    True\n    \"\"\"\n    if isinstance(secret, six.string_types):\n        # It is unicode, convert it to bytes\n        secret = secret.encode('utf-8')\n    # Get rid of all the spacing:\n    secret = secret.replace(b' ', b'')\n    try:\n        key = base64.b32decode(secret, casefold=casefold)\n    except (TypeError):\n        raise TypeError('Incorrect secret')\n    msg = struct.pack('>Q', intervals_no)\n    hmac_digest = hmac.new(key, msg, digest_method).digest()\n    ob = hmac_digest[19] if six.PY3 else ord(hmac_digest[19])\n    o = ob & 15\n    token_base = struct.unpack('>I', hmac_digest[o:o + 4])[0] & 0x7fffffff\n    token = token_base % (10 ** token_length)\n    if as_string:\n        # TODO: should as_string=True return unicode, not bytes?\n        return six.b('{{:0{}d}}'.format(token_length).format(token))\n    else:\n        return token", "response": "Get one - time password on the basis of given secret and interval number."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_totp(\n        secret,\n        as_string=False,\n        digest_method=hashlib.sha1,\n        token_length=6,\n        interval_length=30,\n        clock=None,\n):\n    \"\"\"Get time-based one-time password on the basis of given secret and time.\n\n    :param secret: the base32-encoded string acting as secret key\n    :type secret: str\n    :param as_string: True if result should be padded string, False otherwise\n    :type as_string: bool\n    :param digest_method: method of generating digest (hashlib.sha1 by default)\n    :type digest_method: callable\n    :param token_length: length of the token (6 by default)\n    :type token_length: int\n    :param interval_length: length of TOTP interval (30 seconds by default)\n    :type interval_length: int\n    :param clock: time in epoch seconds to generate totp for, default is now\n    :type clock: int\n    :return: generated TOTP token\n    :rtype: int or str\n\n    >>> get_hotp(b'MFRGGZDFMZTWQ2LK', int(time.time())//30) == \\\n        get_totp(b'MFRGGZDFMZTWQ2LK')\n    True\n    >>> get_hotp(b'MFRGGZDFMZTWQ2LK', int(time.time())//30) == \\\n        get_totp(b'MFRGGZDFMZTWQ2LK', as_string=True)\n    False\n    \"\"\"\n    if clock is None:\n        clock = time.time()\n    interv_no = int(clock) // interval_length\n    return get_hotp(\n        secret,\n        intervals_no=interv_no,\n        as_string=as_string,\n        digest_method=digest_method,\n        token_length=token_length,\n    )", "response": "Get one - time password on the basis of given secret and time."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef valid_hotp(\n        token,\n        secret,\n        last=1,\n        trials=1000,\n        digest_method=hashlib.sha1,\n        token_length=6,\n):\n    \"\"\"Check if given token is valid for given secret. Return interval number\n    that was successful, or False if not found.\n\n    :param token: token being checked\n    :type token: int or str\n    :param secret: secret for which token is checked\n    :type secret: str\n    :param last: last used interval (start checking with next one)\n    :type last: int\n    :param trials: number of intervals to check after 'last'\n    :type trials: int\n    :param digest_method: method of generating digest (hashlib.sha1 by default)\n    :type digest_method: callable\n    :param token_length: length of the token (6 by default)\n    :type token_length: int\n    :return: interval number, or False if check unsuccessful\n    :rtype: int or bool\n\n    >>> secret = b'MFRGGZDFMZTWQ2LK'\n    >>> valid_hotp(713385, secret, last=1, trials=5)\n    4\n    >>> valid_hotp(865438, secret, last=1, trials=5)\n    False\n    >>> valid_hotp(713385, secret, last=4, trials=5)\n    False\n    \"\"\"\n    if not _is_possible_token(token, token_length=token_length):\n        return False\n    for i in six.moves.xrange(last + 1, last + trials + 1):\n        token_candidate = get_hotp(\n            secret=secret,\n            intervals_no=i,\n            digest_method=digest_method,\n            token_length=token_length,\n        )\n        if token_candidate == int(token):\n            return i\n    return False", "response": "Check if given token is valid for given secret. Return interval number if successful False if not found."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef valid_totp(\n        token,\n        secret,\n        digest_method=hashlib.sha1,\n        token_length=6,\n        interval_length=30,\n        clock=None,\n        window=0,\n):\n    \"\"\"Check if given token is valid time-based one-time password for given\n    secret.\n\n    :param token: token which is being checked\n    :type token: int or str\n    :param secret: secret for which the token is being checked\n    :type secret: str\n    :param digest_method: method of generating digest (hashlib.sha1 by default)\n    :type digest_method: callable\n    :param token_length: length of the token (6 by default)\n    :type token_length: int\n    :param interval_length: length of TOTP interval (30 seconds by default)\n    :type interval_length: int\n    :param clock: time in epoch seconds to generate totp for, default is now\n    :type clock: int\n    :param window: compensate for clock skew, number of intervals to check on\n        each side of the current time. (default is 0 - only check the current\n        clock time)\n    :type window: int (positive)\n    :return: True, if is valid token, False otherwise\n    :rtype: bool\n\n    >>> secret = b'MFRGGZDFMZTWQ2LK'\n    >>> token = get_totp(secret)\n    >>> valid_totp(token, secret)\n    True\n    >>> valid_totp(token+1, secret)\n    False\n    >>> token = get_totp(secret, as_string=True)\n    >>> valid_totp(token, secret)\n    True\n    >>> valid_totp(token + b'1', secret)\n    False\n    \"\"\"\n    if _is_possible_token(token, token_length=token_length):\n        if clock is None:\n            clock = time.time()\n        for w in range(-window, window+1):\n            if int(token) == get_totp(\n                secret,\n                digest_method=digest_method,\n                token_length=token_length,\n                interval_length=interval_length,\n                clock=int(clock)+(w*interval_length)\n            ):\n                return True\n    return False", "response": "Check if given token is valid one - time password for given secret."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn list of params as params.", "response": "def parametrize(params):\n    \"\"\"Return list of params as params.\n\n    >>> parametrize(['a'])\n    'a'\n    >>> parametrize(['a', 'b'])\n    'a[b]'\n    >>> parametrize(['a', 'b', 'c'])\n    'a[b][c]'\n\n    \"\"\"\n    returned = str(params[0])\n    returned += \"\".join(\"[\" + str(p) + \"]\" for p in params[1:])\n    return returned"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef urlencode(params):\n\n    # Not doing duck typing here. Will make debugging easier.\n    if not isinstance(params, dict):\n        raise TypeError(\"Only dicts are supported.\")\n\n    params = flatten(params)\n\n    url_params = OrderedDict()\n    for param in params:\n        value = param.pop()\n\n        name = parametrize(param)\n        if isinstance(value, (list, tuple)):\n            name += \"[]\"\n\n        url_params[name] = value\n\n    return urllib_urlencode(url_params, doseq=True)", "response": "Urlencode a multidimensional dict."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef find_matlab_root():\n    matlab_root = None\n\n    path_dirs = os.environ.get(\"PATH\").split(os.pathsep)\n    for path_dir in path_dirs:\n        candidate = realpath(join(path_dir, 'matlab'))\n        if isfile(candidate) or isfile(candidate + '.exe'):\n            matlab_root = dirname(dirname(candidate))\n            break\n\n    return matlab_root", "response": "Look for matlab binary and return root directory of MATLAB\n    installation."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads and return MATLAB engine and libmx.", "response": "def load_engine_and_libs(matlab_root, options):\n    \"\"\"Load and return `libeng` and `libmx`.  Start and return MATLAB\n    engine.\n\n    Returns\n    -------\n    engine\n    libeng\n    libmx\n\n    \"\"\"\n    if sys.maxsize > 2**32:\n        bits = '64bit'\n    else:\n        bits = '32bit'\n\n    system = platform.system()\n\n    if system == 'Linux':\n        if bits == '64bit':\n            lib_dir = join(matlab_root, \"bin\", \"glnxa64\")\n        else:\n            lib_dir = join(matlab_root, \"bin\", \"glnx86\")\n\n        check_python_matlab_architecture(bits, lib_dir)\n\n        libeng = Library(\n            join(lib_dir, 'libeng.so')\n        )\n        libmx = Library(\n            join(lib_dir, 'libmx.so')\n        )\n\n        command = \"{executable} {options}\".format(\n            executable=join(matlab_root, 'bin', 'matlab'),\n            options=options\n        )\n\n        ### Check for /bin/csh\n        if not os.path.exists(\"/bin/csh\"):\n            warnings.warn(\"MATLAB engine requires /bin/csh.  Please install it on your system or matlab_wrapper will not work properly.\")\n\n    elif system == 'Windows':\n        if bits == '64bit':\n            lib_dir = join(matlab_root, \"bin\", \"win64\")\n        else:\n            lib_dir = join(matlab_root, \"bin\", \"win32\")\n\n        check_python_matlab_architecture(bits, lib_dir)\n\n        ## We need to modify PATH, to find MATLAB libs\n        if lib_dir not in os.environ['PATH']:\n            os.environ['PATH'] = lib_dir + ';' + os.environ['PATH']\n\n        libeng = Library('libeng')\n        libmx = Library('libmx')\n\n        command = None\n\n    elif system == 'Darwin':\n        if bits == '64bit':\n            lib_dir = join(matlab_root, \"bin\", \"maci64\")\n        else:\n            unsupported_platform(system, bits)\n\n        check_python_matlab_architecture(bits, lib_dir)\n\n        libeng = Library(\n            join(lib_dir, 'libeng.dylib')\n        )\n        libmx = Library(\n            join(lib_dir, 'libmx.dylib')\n        )\n\n        command = \"{executable} {options}\".format(\n            executable=join(matlab_root, 'bin', 'matlab'),\n            options=options\n        )\n\n    else:\n        unsupported_platform(system, bits)\n\n    ### Check MATLAB version\n    try:\n        version_str = c_char_p.in_dll(libeng, \"libeng_version\").value\n        version = tuple([int(v) for v in version_str.split('.')[:2]])\n\n    except ValueError:\n        warnings.warn(\"Unable to identify MATLAB (libeng) version.\")\n        version = None\n\n    if (system == 'Linux') and (version == (8, 3)) and (bits == '64bit'):\n        warnings.warn(\"You are using MATLAB version 8.3 (R2014a) on Linux, which appears to have a bug in engGetVariable().  You will only be able to use arrays of type double.\")\n\n    elif (system == 'Darwin') and (version == (8, 3)) and (bits == '64bit'):\n        warnings.warn(\"You are using MATLAB version 8.3 (R2014a) on OS X, which appears to have a bug in engGetVariable().  You will only be able to use arrays of type double.\")\n\n    ### Start the engine\n    engine = libeng.engOpen(command)\n\n    return engine, libeng, libmx, version"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check_python_matlab_architecture(bits, lib_dir):\n    if not os.path.isdir(lib_dir):\n        raise RuntimeError(\"It seem that you are using {bits} version of Python, but there's no matching MATLAB installation in {lib_dir}.\".format(bits=bits, lib_dir=lib_dir))", "response": "Make sure we can find corresponding MATLAB installation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef mxarray_to_ndarray(libmx, pm):\n\n    ndims = libmx.mxGetNumberOfDimensions(pm)\n    dims = libmx.mxGetDimensions(pm)\n    numelems = libmx.mxGetNumberOfElements(pm)\n    elem_size = libmx.mxGetElementSize(pm)\n    class_name = libmx.mxGetClassName(pm)\n    is_numeric = libmx.mxIsNumeric(pm)\n    is_complex = libmx.mxIsComplex(pm)\n    data = libmx.mxGetData(pm)\n    imag_data = libmx.mxGetImagData(pm)\n\n\n    if is_numeric:\n        datasize = numelems*elem_size\n\n        real_buffer = ctypes.create_string_buffer(datasize)\n        ctypes.memmove(real_buffer, data, datasize)\n        pyarray = np.ndarray(\n            buffer=real_buffer,\n            shape=dims[:ndims],\n            dtype=class_name,\n            order='F'\n        )\n\n        if is_complex:\n            imag_buffer = ctypes.create_string_buffer(datasize)\n            ctypes.memmove(imag_buffer, imag_data, datasize)\n            pyarray_imag = np.ndarray(\n                buffer=imag_buffer,\n                shape=dims[:ndims],\n                dtype=class_name,\n                order='F'\n            )\n\n            pyarray = pyarray + pyarray_imag * 1j\n\n        out = pyarray.squeeze()\n\n        if out.ndim == 0:\n            out, = np.atleast_1d(out)\n\n\n    elif class_name == 'char':\n        datasize = numelems + 1\n\n        pystring = ctypes.create_string_buffer(datasize+1)\n        libmx.mxGetString(pm, pystring, datasize)\n\n        out = pystring.value\n\n\n    elif class_name == 'logical':\n        datasize = numelems*elem_size\n\n        buf = ctypes.create_string_buffer(datasize)\n        ctypes.memmove(buf, data, datasize)\n\n        pyarray = np.ndarray(\n            buffer=buf,\n            shape=dims[:ndims],\n            dtype='bool',\n            order='F'\n        )\n\n        out = pyarray.squeeze()\n        if out.ndim == 0:\n            out, = np.atleast_1d(out)\n\n\n    elif class_name == 'cell':\n        out = np.empty(numelems, dtype='O')\n        for i in range(numelems):\n            cell = libmx.mxGetCell(pm, i)\n\n            if bool(cell):\n                out[i] = mxarray_to_ndarray(libmx, cell)\n            else:\n                ### uninitialized cell\n                out[i] = None\n\n        out = out.reshape(dims[:ndims], order='F')\n        out = out.squeeze()\n\n\n    elif class_name == 'struct':\n        field_num = libmx.mxGetNumberOfFields(pm)\n\n        ### Get all field names\n        field_names = []\n        for i in range(field_num):\n            field_name = libmx.mxGetFieldNameByNumber(pm, i)\n            field_names.append(field_name)\n\n        ### Get all fields\n        records = []            # [(x0, y0, z0), (x1, y1, z1), ... (xN, yN, zN)]\n        for i in range(numelems):\n            record = []\n            for field_name in field_names:\n                field = libmx.mxGetField(pm, i, field_name)\n\n                if bool(field):\n                    el = mxarray_to_ndarray(libmx, field)\n                else:\n                    ### uninitialized cell\n                    el = None\n\n                record.append(el)\n            records.append(record)\n\n        ### Set the dtypes right (if there is any ndarray, we want dtype=object)\n        arrays = zip(*records)  # [(x0, x1, ... xN), (y0, y1, ... yN), (z0, z1, ... zN)]\n        new_arrays = []\n\n        ## This loop is necessary, because np.rec.fromarrays() cannot\n        ## handle a list of arrays of the same size well\n        for arr in arrays:\n            contains_ndarray = np.any([isinstance(el, np.ndarray) for el in arr])\n\n            if contains_ndarray:\n                newarr = np.empty(len(arr), dtype='O')\n                for i,a in enumerate(arr):\n                    newarr[i] = a\n            else:\n                newarr = np.array(arr)\n\n            new_arrays.append(newarr)\n\n        if new_arrays:\n            out = np.rec.fromarrays(new_arrays, names=field_names)\n            out = out.reshape(dims[:ndims], order='F')\n            out = out.squeeze()\n        else:\n            out = np.array([])\n\n\n    else:\n        raise NotImplementedError('{}-arrays are not supported'.format(class_name))\n\n\n    return out", "response": "Convert MATLAB object pm to numpy equivalent."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef eval(self, expression):\n        expression_wrapped = wrap_script.format(expression)\n\n\n        ### Evaluate the expression\n        self._libeng.engEvalString(self._ep, expression_wrapped)\n\n        ### Check for exceptions in MATLAB\n        mxresult = self._libeng.engGetVariable(self._ep, 'ERRSTR__')\n\n        error_string = self._libmx.mxArrayToString(mxresult)\n\n        self._libmx.mxDestroyArray(mxresult)\n\n        if error_string != \"\":\n            raise RuntimeError(\"Error from MATLAB\\n{0}\".format(error_string))", "response": "Evaluate expression in MATLAB engine and return the result."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get(self, name):\n        pm = self._libeng.engGetVariable(self._ep, name)\n\n        out = mxarray_to_ndarray(self._libmx, pm)\n\n\n        self._libmx.mxDestroyArray(pm)\n\n        return out", "response": "Get the value of a variable in MATLAB workspace."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nput a variable to MATLAB workspace.", "response": "def put(self, name, value):\n        \"\"\"Put a variable to MATLAB workspace.\n\n        \"\"\"\n\n        pm = ndarray_to_mxarray(self._libmx, value)\n\n        self._libeng.engPutVariable(self._ep, name, pm)\n\n        self._libmx.mxDestroyArray(pm)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nnaming of operations provided by containerizers as a set.", "response": "def methods():\n    \"Names of operations provided by containerizers, as a set.\"\n    pairs = inspect.getmembers(Containerizer, predicate=inspect.ismethod)\n    return set(k for k, _ in pairs if k[0:1] != \"_\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconnects containerizer class to command line args and STDIN returning a new object.", "response": "def stdio(containerizer, *args):\n    \"\"\"Connect containerizer class to command line args and STDIN\n\n    Dispatches to an appropriate containerizer method based on the first\n    argument and parses the input using an appropriate Protobuf type.\n\n        launch < containerizer::Launch\n        update < containerizer::Update\n        usage < containerizer::Usage > mesos::ResourceStatistics\n        wait < containerizer::Wait > containerizer::Termination\n        destroy < containerizer::Destroy\n        containers > containerizer::Containers\n        recover\n\n    Output serialization must be handled by the containerizer method (it\n    doesn't necessarily happen at the end).\n\n    Not really part of the containerizer protocol but exposed by Deimos as a\n    subcommand:\n\n        # Follows a Docker ID, PID, &c and exits with an appropriate, matching\n        # exit code, in a manner specific to the containerizer\n        observe <id>\n\n    \"\"\"\n    try:\n        name = args[0]\n        method, proto = {\"launch\": (containerizer.launch, Launch),\n                          \"update\": (containerizer.update, Update),\n                          \"usage\": (containerizer.usage, Usage),\n                          \"wait\": (containerizer.wait, Wait),\n                          \"destroy\": (containerizer.destroy, Destroy),\n                          \"containers\": (containerizer.containers, None),\n                          \"recover\": (containerizer.recover, None),\n                          \"observe\": (containerizer.observe, None)}[name]\n    except IndexError:\n        raise Err(\"Please choose a subcommand\")\n    except KeyError:\n        raise Err(\"Subcommand %s is not valid for containerizers\" % name)\n    log.debug(\"%r\", (method, proto))\n    if proto is not None:\n        return method(recordio.read(proto), *args[1:])\n    else:\n        return method(*args[1:])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef argv(*args, **opts):\n    spacer = [\"--\"] if opts.get(\"__\") else []\n    args = [arg(_) for _ in args]\n    opts = [_ for k, v in opts.items() for _ in opt(k, v)]\n    return opts + spacer + args", "response": "Returns an argument vector from its array of arguments and keyword arguments."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef construct(path, name=None):\n    \"Selects an appropriate CGroup subclass for the given CGroup path.\"\n    name = name if name else path.split(\"/\")[4]\n    classes = {\"memory\": Memory,\n               \"cpu\": CPU,\n               \"cpuacct\": CPUAcct}\n    constructor = classes.get(name, CGroup)\n    log.debug(\"Chose %s for: %s\", constructor.__name__, path)\n    return constructor(path, name)", "response": "Selects an appropriate CGroup subclass for the given CGroup path."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef serialize(cls, **properties):\n    obj = cls()\n    for k, v in properties.iteritems():\n        log.debug(\"%s.%s = %r\", cls.__name__, k, v)\n        setattr(obj, k, v)\n    return obj.SerializeToString()", "response": "Serialize a sequence of objects into a string."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef logger(height=1):                 # http://stackoverflow.com/a/900404/48251\n    caller = inspect.stack()[height]\n    scope = caller[0].f_globals\n    function = caller[3]\n    path = scope[\"__name__\"]\n    if path == \"__main__\" and scope[\"__package__\"]:\n        path = scope[\"__package__\"]\n    return logging.getLogger(path + \".\" + function + \"()\")", "response": "Returns a logger for the calling function."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load_jupyter_server_extension(nb_server_app):\n  app = nb_server_app.web_app\n  host_pattern = '.*$'\n\n  app.add_handlers(host_pattern, [\n      (utils.url_path_join(app.settings['base_url'], '/http_over_websocket'),\n       handlers.HttpOverWebSocketHandler),\n      (utils.url_path_join(app.settings['base_url'],\n                           '/http_over_websocket/diagnose'),\n       handlers.HttpOverWebSocketDiagnosticHandler),\n  ])\n  print('jupyter_http_over_ws extension initialized. Listening on '\n        '/http_over_websocket')", "response": "Called by Jupyter when this module is loaded as a server extension."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _validate_min_version(min_version):\n  if min_version is not None:\n    try:\n      parsed_min_version = version.StrictVersion(min_version)\n    except ValueError:\n      return ExtensionVersionResult(\n          error_reason=ExtensionValidationError.UNPARSEABLE_REQUESTED_VERSION,\n          requested_extension_version=min_version)\n\n    if parsed_min_version > HANDLER_VERSION:\n      return ExtensionVersionResult(\n          error_reason=ExtensionValidationError.OUTDATED_VERSION,\n          requested_extension_version=str(parsed_min_version))\n\n  return ExtensionVersionResult(\n      error_reason=None, requested_extension_version=min_version)", "response": "Validates the extension version matches the requested version."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nhandle a streaming chunk of the response. The streaming_response callback gives no indication about whether the received chunk is the last in the stream. The \"last_response\" instance variable allows us to keep track of the last received chunk of the response. Each time this is called, the previous chunk is emitted. The done() method is expected to be called after the response completes to ensure that the last piece of data is sent. Args: body_part: A chunk of the streaming response.", "response": "def streaming_callback(self, body_part):\n    \"\"\"Handles a streaming chunk of the response.\n\n    The streaming_response callback gives no indication about whether the\n    received chunk is the last in the stream. The \"last_response\" instance\n    variable allows us to keep track of the last received chunk of the\n    response. Each time this is called, the previous chunk is emitted. The\n    done() method is expected to be called after the response completes to\n    ensure that the last piece of data is sent.\n\n    Args:\n      body_part: A chunk of the streaming response.\n    \"\"\"\n    b64_body_string = base64.b64encode(body_part).decode('utf-8')\n\n    response = {\n        'message_id': self._message_id,\n        'data': b64_body_string,\n    }\n    if self._last_response is None:\n      # This represents the first chunk of data to be streamed to the caller.\n      # Attach status and header information to this item.\n      response.update(self._generate_metadata_body())\n    else:\n      self._last_response['done'] = False\n      self._write_message_func(self._last_response)\n\n    self._last_response = response"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlowering cases string keys in given dict.", "response": "def lower_dict(d):\n    \"\"\"Lower cases string keys in given dict.\"\"\"\n\n    _d = {}\n\n    for k, v in d.items():\n        try:\n            _d[k.lower()] = v\n        except AttributeError:\n            _d[k] = v\n\n    return _d"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef urlparse(d, keys=None):\n\n    d = d.copy()\n\n    if keys is None:\n        keys = d.keys()\n\n    for key in keys:\n        d[key] = _urlparse(d[key])\n\n    return d", "response": "Returns a copy of the given dictionary with url values parsed."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef prefix(prefix):\n\n    d = {}\n    e = lower_dict(environ.copy())\n\n    prefix = prefix.lower()\n\n    for k, v in e.items():\n        try:\n            if k.startswith(prefix):\n                k = k[len(prefix):]\n                d[k] = v\n        except AttributeError:\n            pass\n\n    return d", "response": "Returns a dictionary of all environment variables starting with the given prefix lower cased and stripped."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef map(**kwargs):\n\n    d = {}\n    e = lower_dict(environ.copy())\n\n    for k, v in kwargs.items():\n        d[k] = e.get(v.lower())\n\n    return d", "response": "Returns a dictionary of the given keyword arguments mapped to their\n    values from the environment with input keys lower cased."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef Load(file):\n    with open(file, 'rb') as file:\n        model = dill.load(file)\n        return model", "response": "Loads a model from a file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding one more variable as an input of the block wk", "response": "def _AddInput(self, variable):\n        \"\"\"\n        Add one more variable as an input of the block\n\n        :param variable: variable (or signal as it is also a variable)\n        \"\"\"\n        if isinstance(variable, Variable):\n            self.inputs.append(variable)\n        else:\n            print('Error: ', variable.name, variable,\n                  ' given is not a variable')\n            raise TypeError"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd one more variable as an output of the block", "response": "def _AddOutput(self, variable):\n        \"\"\"\n            Add one more variable as an output of the block\n\n            :param variable: variable (or signal as it is also a variable)\n        \"\"\"\n        if isinstance(variable, Variable):\n            self.outputs.append(variable)\n        else:\n            print(variable)\n            raise TypeError"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the input values at a given iteration for solving the block outputs", "response": "def InputValues(self, it, nsteps=None):\n        \"\"\"\n            Returns the input values at a given iteration for solving the block outputs\n        \"\"\"\n        if nsteps == None:\n            nsteps = self.max_input_order\n#        print(self,it)\n        # Provides values in inputs values for computing at iteration it\n        I = np.zeros((self.n_inputs, nsteps))\n        for iv, variable in enumerate(self.inputs):\n            #            print(it-self.max_input_order+1,it+1)\n            #            print(variable._values[it-self.max_input_order+1:it+1])\n            I[iv, :] = variable._values[it-nsteps+1:it+1]\n        return I"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef AddBlock(self, block):\n        if isinstance(block, Block):\n            self.blocks.append(block)\n            self.max_order = max(self.max_order, block.max_input_order-1)\n            self.max_order = max(self.max_order, block.max_output_order)\n            for variable in block.inputs+block.outputs:\n                self._AddVariable(variable)\n        else:\n            print(block)\n            raise TypeError\n        self._utd_graph = False", "response": "Adds the given block to the model and also its input and output variables."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _ResolutionOrder(self, variables_to_solve):\n#    Gp=nx.DiGraph()\n#\n#    for i in range(nvar):\n#        Gp.add_node('v'+str(i),bipartite=0)\n#\n#    for i in range(neq):\n#        Gp.add_node('e'+str(i),bipartite=1)\n#        for j in range(nvar):\n#            if Mo[i,j]==1:\n#                Gp.add_edge('e'+str(i),'v'+str(j))\n\n        Gp = nx.DiGraph()\n        for variable in self.variables:\n            Gp.add_node(variable, bipartite=0)\n        for block in self.blocks:\n            for iov, output_variable in enumerate(block.outputs):\n                Gp.add_node((block, iov), bipartite=1)\n                Gp.add_edge((block, iov), output_variable)\n                Gp.add_edge(output_variable, (block, iov))\n                for input_variable in block.inputs:\n                    if not isinstance(input_variable, Signal):\n                        Gp.add_edge(input_variable, (block, iov))\n\n    #    for n1,n2 in M.items():\n    #        Gp.add_edge(n1,n2)\n\n        sinks = []\n        sources = []\n        for node in Gp.nodes():\n            if Gp.out_degree(node) == 0:\n                sinks.append(node)\n            elif Gp.in_degree(node) == 0:\n                sources.append(node)\n\n        G2 = sources[:]\n        for node in sources:\n            for node2 in nx.descendants(Gp, node):\n                if node2 not in G2:\n                    G2.append(node2)\n\n        if G2 != []:\n            print(G2)\n            raise ModelError('Overconstrained variables')\n\n        G3 = sinks[:]\n        for node in sinks:\n            for node2 in nx.ancestors(Gp, node):\n                if node2 not in G3:\n                    G3.append(node2)\n\n        if G3 != []:\n            raise ModelError('Underconstrained variables')\n\n#        vars_resolvables=[]\n#        for var in vars_resoudre:\n#            if not 'v'+str(var) in G2+G3:\n#                vars_resolvables.append(var)\n\n\n#        G1=Gp.copy()\n#        G1.remove_nodes_from(G2+G3)\n#\n#        M1=nx.bipartite.maximum_matching(G1)\n#        G1p=nx.DiGraph()\n#\n#        G1p.add_nodes_from(G1.nodes())\n#        for e in G1.edges():\n#            # equation vers variable\n#            if e[0][0]=='v':\n#                G1p.add_edge(e[0],e[1])\n#            else:\n#                G1p.add_edge(e[1],e[0])\n#    #    print(len(M))\n#        for n1,n2 in M1.items():\n#    #        print(n1,n2)\n#            if n1[0]=='e':\n#                G1p.add_edge(n1,n2)\n#            else:\n#                G1p.add_edge(n2,n1)\n\n        scc = list(nx.strongly_connected_components(Gp))\n    #    pos=nx.spring_layout(G1p)\n    #    plt.figure()\n    #    nx.draw(G1p,pos)\n    #    nx.draw_networkx_labels(G1p,pos)\n#        print(scc)\n        if scc != []:\n            C = nx.condensation(Gp, scc)\n            isc_vars = []\n            for isc, sc in enumerate(scc):\n                for var in variables_to_solve:\n                    if var in sc:\n                        isc_vars.append(isc)\n                        break\n            ancestors_vars = isc_vars[:]\n\n            for isc_var in isc_vars:\n                for ancetre in nx.ancestors(C, isc_var):\n                    if ancetre not in ancestors_vars:\n                        ancestors_vars.append(ancetre)\n\n            order_sc = [sc for sc in nx.topological_sort(\n                C) if sc in ancestors_vars]\n            order_ev = []\n            for isc in order_sc:\n                # liste d'\u00e9quations et de variables tri\u00e9es pour \u00eatre s\u00e9par\u00e9es\n                evs = list(scc[isc])\n#                print(evs)\n#                levs=int(len(evs)/2)\n                eqs = []\n                var = []\n                for element in evs:\n                    if type(element) == tuple:\n                        eqs.append(element)\n                    else:\n                        var.append(element)\n                order_ev.append((len(eqs), eqs, var))\n\n            return order_ev\n\n        raise ModelError", "response": "This function determines the order of the variables in the internal memory."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef VariablesValues(self, variables, t):\n        # TODO: put interpolation in variables\n        if (t < self.te) | (t > 0):\n            i = t//self.ts  # time step\n            ti = self.ts*i\n            if type(variables) == list:\n                values = []\n                for variable in variables:\n                    # interpolation\n                    values.append(\n                        variables.values[i]*((ti-t)/self.ts+1)+variables.values[i+1]*(t-ti)/self.ts)\n                return values\n\n            else:\n                # interpolation\n                return variables.values[i]*((ti-t)/self.ts+1)+variables.values[i+1]*(t-ti)/self.ts\n        else:\n            raise ValueError", "response": "Returns the value of given variables at time t."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsave the current object to a file.", "response": "def Save(self, name_file):\n        \"\"\"\n            name_file: name of the file without extension.\n            The extension .bms is added by function\n        \"\"\"\n        with open(name_file+'.bms', 'wb') as file:\n            model = dill.dump(self, file)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning dynamical system blocks associated to output variable", "response": "def PartialDynamicSystem(self, ieq, variable):\n        \"\"\"\n        returns dynamical system blocks associated to output variable\n        \"\"\"\n        if ieq == 0:\n            # U1=0\n            if variable == self.variables[0]:\n                b1 = FunctionBlock(\n                    self.physical_nodes[0].variable, self.max_torque, self.Tmax)\n                b2 = Saturation(self.commands[0], self.throttle, 0, 1)\n                b3 = Product(self.max_torque, self.throttle, variable)\n                return[b1, b2, b3]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning dynamical system blocks associated to output variable", "response": "def PartialDynamicSystem(self, ieq, variable):\n        \"\"\"\n        returns dynamical system blocks associated to output variable\n        \"\"\"\n        if ieq == 0:\n            # U1=0\n            if variable == self.variables[0]:\n                return[Gain(self.commands[0], variable, -self.Tmax)]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning dynamical system blocks associated to output variable", "response": "def PartialDynamicSystem(self, ieq, variable):\n        \"\"\"\n        returns dynamical system blocks associated to output variable\n        \"\"\"\n        if ieq == 0:\n            # C1=-f*Tmax*sign(w1-w2)\n            if variable == self.variables[0]:\n                ut = Variable('unsigned clutch friction torque', hidden=True)\n                b1 = Gain(self.commands[0], ut, self.Tmax)\n                dw = Variable('Delta rotationnal speed', hidden=True)\n                sdw = Variable('Sign of delta rotationnal speed')\n                b2 = WeightedSum(\n                    [self.physical_nodes[0].variable, self.physical_nodes[1].variable], dw, [-1, 1])\n                b3 = Sign(dw, sdw)\n                b4 = Product(ut, sdw, variable)\n                return[b1, b2, b3, b4]\n        elif ieq == 1:\n            # C1=-C2\n            if variable == self.variables[0]:\n                return [Gain(self.variables[1], variable, -1)]\n            if variable == self.variables[1]:\n                return [Gain(self.variables[0], variable, -1)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef PartialDynamicSystem(self, ieq, variable):\n        if ieq == 0:\n            # w2=Rw1\n            if variable == self.physical_nodes[0].variable:\n                # w1=w2/R\n                return[Gain(self.physical_nodes[1].variable, variable, 1/self.ratio)]\n            elif variable == self.physical_nodes[1].variable:\n                # w2=Rw1\n                return[Gain(self.physical_nodes[0].variable, variable, self.ratio)]\n        elif ieq == 1:\n            # C1=-RC2\n            if variable == self.variables[0]:\n                # C1=-RC2\n                return[Gain(self.variables[1], variable, -self.ratio)]\n            elif variable == self.variables[1]:\n                # C2=-C1/R\n                return[Gain(self.variables[0], variable, -1/self.ratio)]", "response": "returns dynamical system blocks associated to output variable"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef PartialDynamicSystem(self, ieq, variable):\n        if ieq == 0:\n            # v=Rw\n            if variable == self.physical_nodes[0].variable:\n                # W=V/r\n                return[Gain(self.physical_nodes[1].variable, variable, 1/self.wheels_radius)]\n            elif variable == self.physical_nodes[1].variable:\n                # V=rw\n                return[Gain(self.physical_nodes[0].variable, variable, self.wheels_radius)]\n        elif ieq == 1:\n            # C=-FR\n            if variable == self.variables[0]:\n                # C=-FR\n                return[Gain(self.variables[1], variable, -self.wheels_radius)]\n            elif variable == self.variables[1]:\n                # F=-C/R\n                return[Gain(self.variables[0], variable, -1/self.wheels_radius)]", "response": "returns dynamical system blocks associated to output variable"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef PartialDynamicSystem(self, ieq, variable):\n        if ieq == 0:\n            # U1=0\n            if variable == self.physical_nodes[0].variable:\n                v = Step('Ground', 0)\n                return[Gain(v, variable, 1)]", "response": "Partial dynamic system blocks."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns dynamical system blocks associated to output variable", "response": "def PartialDynamicSystem(self, ieq, variable):\n        \"\"\"\n        returns dynamical system blocks associated to output variable\n        \"\"\"\n#        print(ieq,variable.name)\n        if ieq == 0:\n            # U1-U2=R(i1)\n            if variable == self.physical_nodes[0].variable:\n                # U1 is output\n                # U1=R(i1)+U2\n                return [WeightedSum([self.physical_nodes[1].variable, self.variables[0]], variable, [1, self.R])]\n            elif variable == self.physical_nodes[1].variable:\n                # U2 is output\n                # U2=-R(i1)+U2\n                return [WeightedSum([self.physical_nodes[0].variable, self.variables[0]], variable, [1, -self.R])]\n            elif variable == self.variables[0]:\n                # i1 is output\n                # i1=(U1-U2)/R\n                return [WeightedSum([self.physical_nodes[0].variable, self.physical_nodes[1].variable], variable, [1/self.R, -1/self.R])]\n        elif ieq == 1:\n            # i1=-i2\n            if variable == self.variables[0]:\n                # i1 as output\n                return [Gain(self.variables[1], self.variables[0], -1)]\n            elif variable == self.variables[1]:\n                # i2 as output\n                return [Gain(self.variables[0], self.variables[1], -1)]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn dynamical system blocks associated to output variable", "response": "def PartialDynamicSystem(self, ieq, variable):\n        \"\"\"\n        returns dynamical system blocks associated to output variable\n        \"\"\"\n        if ieq == 0:\n            # U2-U1=signal\n            if variable == self.physical_nodes[0].variable:\n                # U1 is output\n                # U1=U2-signal\n                return [WeightedSum([self.physical_nodes[1].variable, self.voltage_signal], variable, [1, -1])]\n            elif variable == self.physical_nodes[1].variable:\n                # U2 is output\n                # U2=U1+signal\n                return [WeightedSum([self.physical_nodes[0].variable, self.voltage_signal], variable, [1, 1])]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn dynamical system blocks associated to output variable", "response": "def PartialDynamicSystem(self, ieq, variable):\n        \"\"\"\n        returns dynamical system blocks associated to output variable\n        \"\"\"\n\n        if ieq == 0:\n\n            if variable == self.physical_nodes[0].variable:\n                print('1')\n                # U1 is output\n                # U1=i1/pC+U2\n                Uc = Variable(hidden=True)\n                block1 = ODE(self.variables[0], Uc, [1], [0, self.C])\n                sub1 = Sum([self.physical_nodes[1].variable, Uc], variable)\n                return [block1, sub1]\n            elif variable == self.physical_nodes[1].variable:\n                print('2')\n                # U2 is output\n                # U2=U1-i1/pC\n                Uc = Variable(hidden=True)\n                block1 = ODE(self.variables[0], Uc, [-1], [0, self.C])\n                sum1 = Sum([self.physical_nodes[0].variable, Uc], variable)\n                return [block1, sum1]\n#            elif variable==self.variables[0]:\n#                print('3')\n#                # i1 is output\n#                # i1=pC(U1-U2)\n#                ic=Variable(hidden=True)\n#                subs1=Subtraction(self.physical_nodes[0].variable,self.physical_nodes[1].variable,ic)\n#                block1=ODE(ic,variable,[0,self.C],[1])\n#                return [block1,subs1]\n        elif ieq == 1:\n            # i1=-i2\n            if variable == self.variables[0]:\n                # i1 as output\n                #                print('Bat1#0')\n                return [Gain(self.variables[1], self.variables[0], -1)]\n            elif variable == self.variables[1]:\n                # i2 as output\n                #                print('Bat1#1')\n                return [Gain(self.variables[0], self.variables[1], -1)]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning dynamical system blocks associated to output variable", "response": "def PartialDynamicSystem(self, ieq, variable):\n        \"\"\"\n        returns dynamical system blocks associated to output variable\n        \"\"\"\n\n        if ieq == 0:\n\n            #            if variable==self.physical_nodes[0].variable:\n            # print('1')\n            #                # U1 is output\n            #                # U1=i1/pC+U2\n            #                Uc=Variable(hidden=True)\n            #                block1=ODE(self.variables[0],Uc,[1],[0,self.C])\n            #                sub1=Sum([self.physical_nodes[1].variable,Uc],variable)\n            #                return [block1,sub1]\n            #            elif variable==self.physical_nodes[1].variable:\n            #                print('2')\n            #                # U2 is output\n            #                # U2=U1-i1/pC\n            #                Uc=Variable(hidden=True)\n            #                block1=ODE(self.variables[0],Uc,[-1],[0,self.C])\n            #                sum1=Sum([self.physical_nodes[0].variable,Uc],variable)\n            #                return [block1,sum1]\n            if variable == self.variables[0]:  # i1=(u1-u2)/Lp\n                print('3')\n                # i1 is output\n                # i1=pC(U1-U2)\n                Uc = Variable(hidden=True)\n                subs1 = Subtraction(\n                    self.physical_nodes[0].variable, self.physical_nodes[1].variable, Uc)\n                block1 = ODE(Uc, variable, [1], [0, self.L])\n                return [block1, subs1]\n        elif ieq == 1:\n            # i1=-i2\n            if variable == self.variables[0]:\n                # i1 as output\n                return [Gain(self.variables[1], self.variables[0], -1)]\n            elif variable == self.variables[1]:\n                # i2 as output\n                return [Gain(self.variables[0], self.variables[1], -1)]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating text position and redraw", "response": "def on_release_event(self, event):\n        \" Update text position and redraw\"\n        element = self.element_from_artist[self.selected_patch]\n        self.position[element] = [event.xdata, event.ydata]\n        # Signal is a variable! therefore must be tested before\n        if isinstance(element, bms.Signal):\n            artists = self.artists_from_element[element]\n            points = np.array((5, 2))\n            xp, yp = self.position[element]\n            points = np.array([[xp-1.5*self.l, yp-0.5*self.l], [xp-0.5*self.l, yp-0.5*self.l], [\n                              xp, yp], [xp-0.5*self.l, yp+0.5*self.l], [xp-1.5*self.l, yp+0.5*self.l]])\n            artists[0].set_xy(xy=points)\n            artists[1].set(x=xp-1*self.l, y=yp)\n            for artist in artists[3]:  # update out arrows\n                artist.set_positions((xp, yp), None)\n\n        elif isinstance(element, bms.Variable):\n            artists = self.artists_from_element[element]\n            pos = self.position[element]\n            artists[0].set(x=pos[0], y=pos[1])\n            for artist in artists[2]:  # update in arrows\n                artist.set_positions(None, pos)\n            for artist in artists[3]:  # update out arrows\n                artist.set_positions(pos, None)\n\n        elif isinstance(element, bms.Block):\n            artists = self.artists_from_element[element]\n            pb = self.position[element][:]\n            hb = 0.5*(1+max(len(element.inputs), len(element.outputs)))*self.l\n            artists[0].set(xy=(pb[0]-0.5*self.l, pb[1]-hb/2))\n            artists[1].set(x=pb[0], y=pb[1])\n            li = len(element.inputs)\n#            lo=len(element.outputs)\n            for i in range(2, li+2):\n                artists[i].set_positions(\n                    None, (pb[0]-0.5*self.l, pb[1]+hb/2-0.5*(i+1-2)*self.l))\n            for i in range(2+li, len(artists)):\n                artists[i].set_positions(\n                    (pb[0]+0.5*self.l, pb[1]+hb/2-0.5*(i+1-2-li)*self.l), None)\n\n        plt.draw()\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nuses get to return an object or None if the object does not exist.", "response": "def get_object_or_None(klass, *args, **kwargs):\n    \"\"\"\n    Uses get() to return an object or None if the object does not exist.\n\n    klass may be a Model, Manager, or QuerySet object. All other passed\n    arguments and keyword arguments are used in the get() query.\n\n    Note: Like with get(), a MultipleObjectsReturned will be raised if more than one\n    object is found.\n    \"\"\"\n    queryset = _get_queryset(klass)\n    try:\n        return queryset.get(*args, **kwargs)\n    except queryset.model.DoesNotExist:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nuse get to return an object or the value of this argument.", "response": "def get_object_or_this(model, this=None, *args, **kwargs):\n    \"\"\"\n    Uses get() to return an object or the value of <this> argument\n    if object does not exist.\n\n    If the <this> argument if not provided None would be returned.\n    <model> can be either a QuerySet instance or a class.\n    \"\"\"\n\n    return get_object_or_None(model, *args, **kwargs) or this"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_python(self, value):\n        if value == \"\":\n            return None\n\n        try:\n            if isinstance(value, six.string_types):\n                return self.deserializer(value)\n            elif isinstance(value, bytes):\n                return self.deserializer(value.decode('utf8'))\n        except ValueError:\n            pass\n        return value", "response": "Convert a string from the database to a Python value."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert the value to a string so it can be stored in the database.", "response": "def get_prep_value(self, value):\n        \"\"\"\n        Convert the value to a string so it can be stored in the database.\n        \"\"\"\n        if value == \"\":\n            return None\n        if isinstance(value, (dict, list)):\n            return self.serializer(value)\n        return super(JSONField, self).get_prep_value(value)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ajax_request(func):\n    @wraps(func)\n    def wrapper(request, *args, **kwargs):\n        for accepted_type in request.META.get('HTTP_ACCEPT', '').split(','):\n            if accepted_type in FORMAT_TYPES.keys():\n                format_type = accepted_type\n                break\n        else:\n            format_type = 'application/json'\n        response = func(request, *args, **kwargs)\n        if not isinstance(response, HttpResponse):\n            if hasattr(settings, 'FORMAT_TYPES'):\n                format_type_handler = settings.FORMAT_TYPES[format_type]\n                if hasattr(format_type_handler, '__call__'):\n                    data = format_type_handler(response)\n                elif isinstance(format_type_handler, six.string_types):\n                    mod_name, func_name = format_type_handler.rsplit('.', 1)\n                    module = __import__(mod_name, fromlist=[func_name])\n                    function = getattr(module, func_name)\n                    data = function(response)\n            else:\n                data = FORMAT_TYPES[format_type](response)\n            response = HttpResponse(data, content_type=format_type)\n            response['content-length'] = len(data)\n        return response\n    return wrapper", "response": "A decorator that returns a view that returns a serializable dict that can be extended by ajax_request."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef autostrip(cls):\n    warnings.warn(\n        \"django-annoying autostrip is deprecated and will be removed in a \"\n        \"future version. Django now has native support for stripping form \"\n        \"fields. \"\n        \"https://docs.djangoproject.com/en/stable/ref/forms/fields/#django.forms.CharField.strip\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    fields = [(key, value) for key, value in cls.base_fields.items() if isinstance(value, forms.CharField)]\n    for field_name, field_object in fields:\n        def get_clean_func(original_clean):\n            return lambda value: original_clean(value and value.strip())\n        clean_func = get_clean_func(getattr(field_object, 'clean'))\n        setattr(field_object, 'clean', clean_func)\n    return cls", "response": "Auto - trip the base fields before validation."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _patch_file(path, content):\n    f = open(path)\n    existing_content = f.read()\n    f.close()\n    if existing_content == content:\n        # already patched\n        log.warn('Already patched.')\n        return False\n    log.warn('Patching...')\n    _rename_path(path)\n    f = open(path, 'w')\n    try:\n        f.write(content)\n    finally:\n        f.close()\n    return True", "response": "Will backup the file then patch it"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _set_overlay_verify(name, overlay_path, config_path):\n    global DEBUG\n    # VERIFY PATH IS NOT THERE\n    if os.path.exists(config_path):\n        print(\"Config path already exists! Not moving forward\")\n        print(\"config_path: {0}\".format(config_path))\n        return -1\n\n    # MAKE THE CONFIGURATION PATH\n    os.makedirs(config_path)\n\n    # CAT THE OVERLAY INTO THE CONFIG FILESYSTEM\n    with open(config_path + \"/dtbo\", 'wb') as outfile:\n        with open(overlay_path, 'rb') as infile:\n            shutil.copyfileobj(infile, outfile)\n\n    # SLEEP TO ENABLE THE KERNEL TO DO ITS JOB\n    time.sleep(0.2)\n\n    # VERIFY\n    if name == \"CUST\":\n        # BLINDLY ACCEPT THAT IT LOADED\n        return 0\n    elif name == \"PWM0\":\n        if os.path.exists(PWMSYSFSPATH):\n            if DEBUG:\n                print(\"PWM IS LOADED!\")\n            return 0\n        else:\n            if DEBUG:\n                print(\"ERROR LOAIDNG PWM0\")\n            return 1\n    elif name == \"SPI2\":\n        if os.listdir(SPI2SYSFSPATH) != \"\":\n            if DEBUG:\n                print(\"SPI2 IS LOADED!\")\n            return 0\n        else:\n            if DEBUG:\n                print(\"ERROR LOADING SPI2\")\n            return 0", "response": "Function to load the overlay and verify it was setup properly"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load(overlay, path=\"\"):\n    global DEBUG\n    global _LOADED\n    if DEBUG:\n        print(\"LOAD OVERLAY: {0} @ {1}\".format(overlay,path))\n    # SEE IF OUR OVERLAY NAME IS IN THE KEYS\n    if overlay.upper() in _OVERLAYS.keys():\n        cpath = OVERLAYCONFIGPATH + \"/\" + _FOLDERS[overlay.upper()]\n        if DEBUG:\n            print(\"VALID OVERLAY\")\n            print(\"CONFIG PATH:  {0}\".format(cpath))\n        # CHECK TO SEE IF WE HAVE A PATH FOR CUSTOM OVERLAY\n        if overlay.upper() == \"CUST\" and path == \"\":\n            raise ValueError(\"Path must be specified for Custom Overlay Choice\")\n        elif overlay.upper() == \"CUST\" and _LOADED[overlay.upper()]:\n            print(\"Custom Overlay already loaded\")\n            return 2\n        elif overlay.upper() == \"CUST\" and not os.path.exists(path):\n            print(\"Custom Overlay path does not exist\")\n            return 1\n\n        # DETERMINE IF WE ARE A CHIP PRO AND WE ARE COMMANDED TO LOAD PWM0\n        if is_chip_pro() and overlay.upper() == \"PWM0\":\n            print(\"CHIP Pro supports PWM0 in base DTB, exiting\")\n            return 1\n\n        # SET UP THE OVERLAY PATH FOR OUR USE\n        if overlay.upper() != \"CUST\":\n            opath = OVERLAYINSTALLPATH\n            opath += \"/\" + _OVERLAYS[overlay.upper()]\n        else:\n            opath = path\n        if DEBUG:\n            print(\"OVERLAY PATH: {0}\".format(opath))\n\n        if overlay.upper() == \"PWM0\" and _LOADED[overlay.upper()]:\n            print(\"PWM0 Overlay already loaded\")\n            return 2\n\n        if overlay.upper() == \"SPI2\" and _LOADED[overlay.upper()]:\n            print(\"SPI2 Overlay already loaded\")\n            return 2\n\n        # LOAD THE OVERLAY\n        errc = _set_overlay_verify(overlay.upper(), opath, cpath)\n        if DEBUG:\n            print(\"_SET_OVERLAY_VERIFY ERRC: {0}\".format(errc))\n        if errc == 0:\n            _LOADED[overlay.upper()] = True\n\n    else:\n        raise ValueError(\"Invalid Overlay name specified! Choose between: SPI2, PWM0, CUST\")", "response": "Loads a custom overlay into the internal list of base classes."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreconstructs this Dagobah instance from the backend.", "response": "def from_backend(self, dagobah_id):\n        \"\"\" Reconstruct this Dagobah instance from the backend. \"\"\"\n        logger.debug('Reconstructing Dagobah instance from backend with ID {0}'.format(dagobah_id))\n        rec = self.backend.get_dagobah_json(dagobah_id)\n        if not rec:\n            raise DagobahError('dagobah with id %s does not exist '\n                               'in backend' % dagobah_id)\n        self._construct_from_json(rec)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconstruct this Dagobah instance from a JSON document.", "response": "def _construct_from_json(self, rec):\n        \"\"\" Construct this Dagobah instance from a JSON document. \"\"\"\n\n        self.delete()\n\n        for required_key in ['dagobah_id', 'created_jobs']:\n            setattr(self, required_key, rec[required_key])\n\n        for job_json in rec.get('jobs', []):\n            self._add_job_from_spec(job_json)\n\n        self.commit(cascade=True)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_job_from_json(self, job_json, destructive=False):\n        logger.debug('Importing job from JSON document: {0}'.format(job_json))\n        rec = self.backend.decode_import_json(job_json)\n        if destructive:\n            try:\n                self.delete_job(rec['name'])\n            except DagobahError:  # expected if no job with this name\n                pass\n        self._add_job_from_spec(rec, use_job_id=False)\n\n        self.commit(cascade=True)", "response": "Construct a new Job from an imported JSON document."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a single job from a Dagobah spec.", "response": "def _add_job_from_spec(self, job_json, use_job_id=True):\n        \"\"\" Add a single job to the Dagobah from a spec. \"\"\"\n\n        job_id = (job_json['job_id']\n                  if use_job_id\n                  else self.backend.get_new_job_id())\n        self.add_job(str(job_json['name']), job_id)\n        job = self.get_job(job_json['name'])\n        if job_json.get('cron_schedule', None):\n            job.schedule(job_json['cron_schedule'])\n\n        for task in job_json.get('tasks', []):\n            self.add_task_to_job(job,\n                                 str(task['command']),\n                                 str(task['name']),\n                                 soft_timeout=task.get('soft_timeout', 0),\n                                 hard_timeout=task.get('hard_timeout', 0),\n                                 hostname=task.get('hostname', None))\n\n        dependencies = job_json.get('dependencies', {})\n        for from_node, to_nodes in dependencies.iteritems():\n            for to_node in to_nodes:\n                job.add_dependency(from_node, to_node)\n\n        if job_json.get('notes', None):\n            job.update_job_notes(job_json['notes'])"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncommits this Dagobah instance to the backend.", "response": "def commit(self, cascade=False):\n        \"\"\" Commit this Dagobah instance to the backend.\n\n        If cascade is True, all child Jobs are commited as well.\n        \"\"\"\n        logger.debug('Committing Dagobah instance with cascade={0}'.format(cascade))\n        self.backend.commit_dagobah(self._serialize())\n        if cascade:\n            [job.commit() for job in self.jobs]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef delete(self):\n        logger.debug('Deleting Dagobah instance with ID {0}'.format(self.dagobah_id))\n        self.jobs = []\n        self.created_jobs = 0\n        self.backend.delete_dagobah(self.dagobah_id)", "response": "Delete this Dagobah instance from the backend."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_job(self, job_name, job_id=None):\n        logger.debug('Creating a new job named {0}'.format(job_name))\n        if not self._name_is_available(job_name):\n            raise DagobahError('name %s is not available' % job_name)\n\n        if not job_id:\n            job_id = self.backend.get_new_job_id()\n            self.created_jobs += 1\n\n        self.jobs.append(Job(self,\n                             self.backend,\n                             job_id,\n                             job_name))\n\n        job = self.get_job(job_name)\n        job.commit()", "response": "Add a new job to the queue."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a Host dict with config options or None if no such host exists.", "response": "def get_host(self, hostname):\n        \"\"\" Returns a Host dict with config options, or None if none exists\"\"\"\n        if hostname in self.get_hosts():\n            return self.load_ssh_conf().lookup(hostname)\n        logger.warn('Tried to find host with name {0}, but host not found'.format(hostname))\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a Job object by name.", "response": "def get_job(self, job_name):\n        \"\"\" Returns a Job by name, or None if none exists. \"\"\"\n        for job in self.jobs:\n            if job.name == job_name:\n                return job\n        logger.warn('Tried to find job with name {0}, but job not found'.format(job_name))\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef delete_job(self, job_name):\n        logger.debug('Deleting job {0}'.format(job_name))\n        for idx, job in enumerate(self.jobs):\n            if job.name == job_name:\n                self.backend.delete_job(job.job_id)\n                del self.jobs[idx]\n                self.commit()\n                return\n        raise DagobahError('no job with name %s exists' % job_name)", "response": "Delete a job by name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd a task to a job.", "response": "def add_task_to_job(self, job_or_job_name, task_command, task_name=None,\n                        **kwargs):\n        \"\"\" Add a task to a job owned by the Dagobah instance. \"\"\"\n\n        if isinstance(job_or_job_name, Job):\n            job = job_or_job_name\n        else:\n            job = self.get_job(job_or_job_name)\n\n        if not job:\n            raise DagobahError('job %s does not exist' % job_or_job_name)\n\n        logger.debug('Adding task with command {0} to job {1}'.format(task_command, job.name))\n\n        if not job.state.allow_change_graph:\n            raise DagobahError(\"job's graph is immutable in its current \" +\n                               \"state: %s\"\n                               % job.state.status)\n\n        job.add_task(task_command, task_name, **kwargs)\n        job.commit()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _name_is_available(self, job_name):\n        return (False\n                if [job for job in self.jobs if job.name == job_name]\n                else True)", "response": "Returns True if the specified name is already in use."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _serialize(self, include_run_logs=False, strict_json=False):\n        result = {'dagobah_id': self.dagobah_id,\n                  'created_jobs': self.created_jobs,\n                  'jobs': [job._serialize(include_run_logs=include_run_logs,\n                                          strict_json=strict_json)\n                           for job in self.jobs]}\n        if strict_json:\n            result = json.loads(json.dumps(result, cls=StrictJSONEncoder))\n        return result", "response": "Serialize a representation of this Dagobah object to JSON."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef commit(self):\n        logger.debug('Committing job {0}'.format(self.name))\n        self.backend.commit_job(self._serialize())\n        self.parent.commit()", "response": "Commit the metadata of this Job to the backend."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd a new Task to the job with no edges.", "response": "def add_task(self, command, name=None, **kwargs):\n        \"\"\" Adds a new Task to the graph with no edges. \"\"\"\n\n        logger.debug('Adding task with command {0} to job {1}'.format(command, self.name))\n        if not self.state.allow_change_graph:\n            raise DagobahError(\"job's graph is immutable in its current state: %s\"\n                               % self.state.status)\n\n        if name is None:\n            name = command\n        new_task = Task(self, command, name, **kwargs)\n        self.tasks[name] = new_task\n        self.add_node(name)\n        self.commit()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_dependency(self, from_task_name, to_task_name):\n\n        logger.debug('Adding dependency from {0} to {1}'.format(from_task_name, to_task_name))\n        if not self.state.allow_change_graph:\n            raise DagobahError(\"job's graph is immutable in its current state: %s\"\n                               % self.state.status)\n\n        self.add_edge(from_task_name, to_task_name)\n        self.commit()", "response": "Add a dependency between two tasks."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndeleting the named Task in this Job.", "response": "def delete_task(self, task_name):\n        \"\"\" Deletes the named Task in this Job. \"\"\"\n\n        logger.debug('Deleting task {0}'.format(task_name))\n        if not self.state.allow_change_graph:\n            raise DagobahError(\"job's graph is immutable in its current state: %s\"\n                               % self.state.status)\n\n        if task_name not in self.tasks:\n            raise DagobahError('task %s does not exist' % task_name)\n\n        self.tasks.pop(task_name)\n        self.delete_node(task_name)\n        self.commit()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndelete a dependency between two tasks.", "response": "def delete_dependency(self, from_task_name, to_task_name):\n        \"\"\" Delete a dependency between two tasks. \"\"\"\n\n        logger.debug('Deleting dependency from {0} to {1}'.format(from_task_name, to_task_name))\n        if not self.state.allow_change_graph:\n            raise DagobahError(\"job's graph is immutable in its current state: %s\"\n                               % self.state.status)\n\n        self.delete_edge(from_task_name, to_task_name)\n        self.commit()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef schedule(self, cron_schedule, base_datetime=None):\n\n        logger.debug('Scheduling job {0} with cron schedule {1}'.format(self.name, cron_schedule))\n        if not self.state.allow_change_schedule:\n            raise DagobahError(\"job's schedule cannot be changed in state: %s\"\n                               % self.state.status)\n\n        if cron_schedule is None:\n            self.cron_schedule = None\n            self.cron_iter = None\n            self.next_run = None\n\n        else:\n            if base_datetime is None:\n                base_datetime = datetime.utcnow()\n            self.cron_schedule = cron_schedule\n            self.cron_iter = croniter(cron_schedule, base_datetime)\n            self.next_run = self.cron_iter.get_next(datetime)\n\n        logger.debug('Determined job {0} next run of {1}'.format(self.name, self.next_run))\n        self.commit()", "response": "Schedules the job to run periodically using the cron syntax."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nstarts the job by kicking off all tasks with no dependencies.", "response": "def start(self):\n        \"\"\" Begins the job by kicking off all tasks with no dependencies. \"\"\"\n\n        logger.info('Job {0} starting job run'.format(self.name))\n        if not self.state.allow_start:\n            raise DagobahError('job cannot be started in its current state; ' +\n                               'it is probably already running')\n\n        self.initialize_snapshot()\n\n        # don't increment if the job was run manually\n        if self.cron_iter and datetime.utcnow() > self.next_run:\n            self.next_run = self.cron_iter.get_next(datetime)\n\n        self.run_log = {'job_id': self.job_id,\n                        'name': self.name,\n                        'parent_id': self.parent.dagobah_id,\n                        'log_id': self.backend.get_new_log_id(),\n                        'start_time': datetime.utcnow(),\n                        'tasks': {}}\n        self._set_status('running')\n\n        logger.debug('Job {0} resetting all tasks prior to start'.format(self.name))\n        for task in self.tasks.itervalues():\n            task.reset()\n\n        logger.debug('Job {0} seeding run logs'.format(self.name))\n        for task_name in self.ind_nodes(self.snapshot):\n            self._put_task_in_run_log(task_name)\n            self.tasks[task_name].start()\n\n        self._commit_run_log()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef retry(self):\n\n        logger.info('Job {0} retrying all failed tasks'.format(self.name))\n        self.initialize_snapshot()\n\n        failed_task_names = []\n        for task_name, log in self.run_log['tasks'].items():\n            if log.get('success', True) == False:\n                failed_task_names.append(task_name)\n\n        if len(failed_task_names) == 0:\n            raise DagobahError('no failed tasks to retry')\n\n        self._set_status('running')\n        self.run_log['last_retry_time'] = datetime.utcnow()\n\n        logger.debug('Job {0} seeding run logs'.format(self.name))\n        for task_name in failed_task_names:\n            self._put_task_in_run_log(task_name)\n            self.tasks[task_name].start()\n\n        self._commit_run_log()", "response": "Restarts all failed tasks of a job."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nterminating all currently running tasks.", "response": "def terminate_all(self):\n        \"\"\" Terminate all currently running tasks. \"\"\"\n        logger.info('Job {0} terminating all currently running tasks'.format(self.name))\n        for task in self.tasks.itervalues():\n            if task.started_at and not task.completed_at:\n                task.terminate()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef kill_all(self):\n        logger.info('Job {0} killing all currently running tasks'.format(self.name))\n        for task in self.tasks.itervalues():\n            if task.started_at and not task.completed_at:\n                task.kill()", "response": "Kill all currently running jobs."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef edit(self, **kwargs):\n\n        logger.debug('Job {0} changing name to {1}'.format(self.name, kwargs.get('name')))\n        if not self.state.allow_edit_job:\n            raise DagobahError('job cannot be edited in its current state')\n\n        if 'name' in kwargs and isinstance(kwargs['name'], str):\n            if not self.parent._name_is_available(kwargs['name']):\n                raise DagobahError('new job name %s is not available' %\n                                   kwargs['name'])\n\n        for key in ['name']:\n            if key in kwargs and isinstance(kwargs[key], str):\n                setattr(self, key, kwargs[key])\n\n        self.parent.commit(cascade=True)", "response": "Edit this Job s name."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nediting the name of a Task owned by this Job.", "response": "def edit_task(self, task_name, **kwargs):\n        \"\"\" Change the name of a Task owned by this Job.\n\n        This will affect the historical data available for this\n        Task, e.g. past run logs will no longer be accessible.\n        \"\"\"\n\n        logger.debug('Job {0} editing task {1}'.format(self.name, task_name))\n        if not self.state.allow_edit_task:\n            raise DagobahError(\"tasks cannot be edited in this job's \" +\n                               \"current state\")\n\n        if task_name not in self.tasks:\n            raise DagobahError('task %s not found' % task_name)\n\n        if 'name' in kwargs and isinstance(kwargs['name'], str):\n            if kwargs['name'] in self.tasks:\n                raise DagobahError('task name %s is unavailable' %\n                                   kwargs['name'])\n\n        task = self.tasks[task_name]\n        for key in ['name', 'command']:\n            if key in kwargs and isinstance(kwargs[key], str):\n                setattr(task, key, kwargs[key])\n\n        if 'soft_timeout' in kwargs:\n            task.set_soft_timeout(kwargs['soft_timeout'])\n\n        if 'hard_timeout' in kwargs:\n            task.set_hard_timeout(kwargs['hard_timeout'])\n\n        if 'hostname' in kwargs:\n            task.set_hostname(kwargs['hostname'])\n\n        if 'name' in kwargs and isinstance(kwargs['name'], str):\n            self.rename_edges(task_name, kwargs['name'])\n            self.tasks[kwargs['name']] = task\n            del self.tasks[task_name]\n\n        self.parent.commit(cascade=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmarks this task as completed.", "response": "def _complete_task(self, task_name, **kwargs):\n        \"\"\" Marks this task as completed. Kwargs are stored in the run log. \"\"\"\n\n        logger.debug('Job {0} marking task {1} as completed'.format(self.name, task_name))\n        self.run_log['tasks'][task_name] = kwargs\n\n        for node in self.downstream(task_name, self.snapshot):\n            self._start_if_ready(node)\n\n        try:\n            self.backend.acquire_lock()\n            self._commit_run_log()\n        except:\n            logger.exception(\"Error in handling events.\")\n        finally:\n            self.backend.release_lock()\n\n        if kwargs.get('success', None) == False:\n            task = self.tasks[task_name]\n            try:\n                self.backend.acquire_lock()\n                if self.event_handler:\n                    self.event_handler.emit('task_failed',\n                                            task._serialize(include_run_logs=True))\n            except:\n                logger.exception(\"Error in handling events.\")\n            finally:\n                self.backend.release_lock()\n\n        self._on_completion()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _put_task_in_run_log(self, task_name):\n        logger.debug('Job {0} initializing run log entry for task {1}'.format(self.name, task_name))\n        data = {'start_time': datetime.utcnow(),\n                'command': self.tasks[task_name].command}\n        self.run_log['tasks'][task_name] = data", "response": "Stores the task in the run log entry for this task."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks to see if the job has completed and cleans up if it has.", "response": "def _on_completion(self):\n        \"\"\" Checks to see if the Job has completed, and cleans up if it has. \"\"\"\n\n        logger.debug('Job {0} running _on_completion check'.format(self.name))\n        if self.state.status != 'running' or (not self._is_complete()):\n            return\n\n        for job, results in self.run_log['tasks'].iteritems():\n            if results.get('success', False) == False:\n                self._set_status('failed')\n                try:\n                    self.backend.acquire_lock()\n                    if self.event_handler:\n                        self.event_handler.emit('job_failed',\n                                                self._serialize(include_run_logs=True))\n                except:\n                    logger.exception(\"Error in handling events.\")\n                finally:\n                    self.backend.release_lock()\n                break\n\n        if self.state.status != 'failed':\n            self._set_status('waiting')\n            self.run_log = {}\n            try:\n                self.backend.acquire_lock()\n                if self.event_handler:\n                    self.event_handler.emit('job_complete',\n                                            self._serialize(include_run_logs=True))\n            except:\n                logger.exception(\"Error in handling events.\")\n            finally:\n                self.backend.release_lock()\n\n        self.destroy_snapshot()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _start_if_ready(self, task_name):\n        logger.debug('Job {0} running _start_if_ready for task {1}'.format(self.name, task_name))\n        task = self.tasks[task_name]\n        dependencies = self._dependencies(task_name, self.snapshot)\n        for dependency in dependencies:\n            if self.run_log['tasks'].get(dependency, {}).get('success', False) == True:\n                continue\n            return\n        self._put_task_in_run_log(task_name)\n        task.start()", "response": "Start this task if all its dependencies finished successfully."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _commit_run_log(self):\n        logger.debug('Committing run log for job {0}'.format(self.name))\n        self.backend.commit_log(self.run_log)", "response": "Commits the current run log to the backend."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _serialize(self, include_run_logs=False, strict_json=False):\n\n        # return tasks in sorted order if graph is in a valid state\n        try:\n            topo_sorted = self.topological_sort()\n            t = [self.tasks[task]._serialize(include_run_logs=include_run_logs,\n                                             strict_json=strict_json)\n                 for task in topo_sorted]\n        except:\n            t = [task._serialize(include_run_logs=include_run_logs,\n                                 strict_json=strict_json)\n                 for task in self.tasks.itervalues()]\n\n        dependencies = {}\n        for k, v in self.graph.iteritems():\n            dependencies[k] = list(v)\n\n        result = {'job_id': self.job_id,\n                  'name': self.name,\n                  'parent_id': self.parent.dagobah_id,\n                  'tasks': t,\n                  'dependencies': dependencies,\n                  'status': self.state.status,\n                  'cron_schedule': self.cron_schedule,\n                  'next_run': self.next_run,\n                  'notes': self.notes}\n\n        if strict_json:\n            result = json.loads(json.dumps(result, cls=StrictJSONEncoder))\n        return result", "response": "Serialize a representation of this Job to a Python dict object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncopies the DAG and validate the job.", "response": "def initialize_snapshot(self):\n        \"\"\" Copy the DAG and validate \"\"\"\n        logger.debug('Initializing DAG snapshot for job {0}'.format(self.name))\n        if self.snapshot is not None:\n            logging.warn(\"Attempting to initialize DAG snapshot without \" +\n                         \"first destroying old snapshot.\")\n\n        snapshot_to_validate = deepcopy(self.graph)\n\n        is_valid, reason = self.validate(snapshot_to_validate)\n        if not is_valid:\n            raise DagobahError(reason)\n\n        self.snapshot = snapshot_to_validate"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreset this Task to a clean state prior to execution.", "response": "def reset(self):\n        \"\"\" Reset this Task to a clean state prior to execution. \"\"\"\n\n        logger.debug('Resetting task {0}'.format(self.name))\n\n        self.stdout_file = os.tmpfile()\n        self.stderr_file = os.tmpfile()\n\n        self.stdout = \"\"\n        self.stderr = \"\"\n\n        self.started_at = None\n        self.completed_at = None\n        self.successful = None\n\n        self.terminate_sent = False\n        self.kill_sent = False\n        self.remote_failure = False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nstart execution of this task.", "response": "def start(self):\n        \"\"\" Begin execution of this task. \"\"\"\n        logger.info('Starting task {0}'.format(self.name))\n        self.reset()\n        if self.hostname:\n            host = self.parent_job.parent.get_host(self.hostname)\n            if host:\n                self.remote_ssh(host)\n            else:\n                self.remote_failure = True\n        else:\n            self.process = subprocess.Popen(self.command,\n                                            shell=True,\n                                            env=os.environ.copy(),\n                                            stdout=self.stdout_file,\n                                            stderr=self.stderr_file)\n\n        self.started_at = datetime.utcnow()\n        self._start_check_timer()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef remote_ssh(self, host):\n        logger.info('Starting remote execution of task {0} on host {1}'.format(self.name, host['hostname']))\n        try:\n            self.remote_client = paramiko.SSHClient()\n            self.remote_client.load_system_host_keys()\n            self.remote_client.set_missing_host_key_policy(\n                paramiko.AutoAddPolicy())\n            self.remote_client.connect(host['hostname'], username=host['user'],\n                                       key_filename=host['identityfile'][0],\n                                       timeout=82800)\n            transport = self.remote_client.get_transport()\n            transport.set_keepalive(10)\n\n            self.remote_channel = transport.open_session()\n            self.remote_channel.get_pty()\n            self.remote_channel.exec_command(self.command)\n        except Exception as e:\n            logger.warn('Exception encountered in remote task execution')\n            self.remote_failure = True\n            self.stderr += 'Exception when trying to SSH related to: '\n            self.stderr += '{0}: {1}\\n\"'.format(type(e).__name__, str(e))\n            self.stderr += 'Was looking for host \"{0}\"\\n'.format(str(host))\n            self.stderr += 'Found in config:\\n'\n            self.stderr += 'host: \"{0}\"\\n'.format(str(host))\n            self.stderr += 'hostname: \"{0}\"\\n'.format(str(host.get('hostname')))\n            self.stderr += 'user: \"{0}\"\\n'.format(str(host.get('user')))\n            self.stderr += 'identityfile: \"{0}\"\\n'.format(str(host.get('identityfile')))\n            self.remote_client.close()", "response": "Execute a command on SSH. Takes a paramiko host dict."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_complete(self):\n        logger.debug('Running check_complete for task {0}'.format(self.name))\n\n        # Tasks not completed\n        if self.remote_not_complete() or self.local_not_complete():\n            self._start_check_timer()\n            return\n\n        return_code = self.completed_task()\n\n        # Handle task errors\n        if self.terminate_sent:\n            self.stderr += '\\nDAGOBAH SENT SIGTERM TO THIS PROCESS\\n'\n        if self.kill_sent:\n            self.stderr += '\\nDAGOBAH SENT SIGKILL TO THIS PROCESS\\n'\n        if self.remote_failure:\n            return_code = -1\n            self.stderr += '\\nAn error occurred with the remote machine.\\n'\n\n        self.stdout_file = None\n        self.stderr_file = None\n\n        self._task_complete(success=True if return_code == 0 else False,\n                            return_code=return_code,\n                            stdout=self.stdout,\n                            stderr=self.stderr,\n                            start_time=self.started_at,\n                            complete_time=datetime.utcnow())", "response": "Runs the completion flow for this task if it s finished."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns True if this task is on a remote channel and False if it is on a remote machine and not completed.", "response": "def remote_not_complete(self):\n        \"\"\"\n        Returns True if this task is on a remote channel, and on a remote\n        machine, False if it is either not remote or not completed\n        \"\"\"\n        if self.remote_channel and not self.remote_channel.exit_status_ready():\n            self._timeout_check()\n            # Get some stdout/std error\n            if self.remote_channel.recv_ready():\n                self.stdout += self.remote_channel.recv(1024)\n            if self.remote_channel.recv_stderr_ready():\n                self.stderr += self.remote_channel.recv_stderr(1024)\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef local_not_complete(self):\n        if self.process and self.process.poll() is None:\n            self._timeout_check()\n            return True\n        return False", "response": "Returns True if task is local and not completed"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef completed_task(self):\n        # If its remote and finished running\n        if self.remote_channel and self.remote_channel.exit_status_ready():\n            # Collect all remaining stdout/stderr\n            while self.remote_channel.recv_ready():\n                self.stdout += self.remote_channel.recv(1024)\n            while self.remote_channel.recv_stderr_ready():\n                self.stderr += self.remote_channel.recv_stderr(1024)\n            return self.remote_channel.recv_exit_status()\n        # Otherwise check for finished local command\n        elif self.process:\n            self.stdout, self.stderr = (self._read_temp_file(self.stdout_file),\n                                        self._read_temp_file(self.stderr_file))\n            for temp_file in [self.stdout_file, self.stderr_file]:\n                temp_file.close()\n            return self.process.returncode", "response": "Handle wrapping up a completed task."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsending SIGTERM to the task s process.", "response": "def terminate(self):\n        \"\"\" Send SIGTERM to the task's process. \"\"\"\n        logger.info('Sending SIGTERM to task {0}'.format(self.name))\n        if hasattr(self, 'remote_client') and self.remote_client is not None:\n            self.terminate_sent = True\n            self.remote_client.close()\n            return\n        if not self.process:\n            raise DagobahError('task does not have a running process')\n        self.terminate_sent = True\n        self.process.terminate()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef kill(self):\n        logger.info('Sending SIGKILL to task {0}'.format(self.name))\n        if hasattr(self, 'remote_client') and self.remote_client is not None:\n            self.kill_sent = True\n            self.remote_client.close()\n            return\n        if not self.process:\n            raise DagobahError('task does not have a running process')\n        self.kill_sent = True\n        self.process.kill()", "response": "Send SIGKILL to the task s process."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nheads a specified stream by num_lines.", "response": "def head(self, stream='stdout', num_lines=10):\n        \"\"\" Head a specified stream (stdout or stderr) by num_lines. \"\"\"\n        target = self._map_string_to_file(stream)\n        if not target:  # no current temp file\n            last_run = self.backend.get_latest_run_log(self.parent_job.job_id,\n                                                       self.name)\n            if not last_run:\n                return None\n            return self._head_string(last_run['tasks'][self.name][stream],\n                                     num_lines)\n        else:\n            return self._head_temp_file(target, num_lines)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntail a specified stream by num_lines.", "response": "def tail(self, stream='stdout', num_lines=10):\n        \"\"\" Tail a specified stream (stdout or stderr) by num_lines. \"\"\"\n        target = self._map_string_to_file(stream)\n        if not target:  # no current temp file\n            last_run = self.backend.get_latest_run_log(self.parent_job.job_id,\n                                                       self.name)\n            if not last_run:\n                return None\n            return self._tail_string(last_run['tasks'][self.name][stream],\n                                     num_lines)\n        else:\n            return self._tail_temp_file(target, num_lines)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nstarting a timer that checks to see if the task has completed.", "response": "def _start_check_timer(self):\n        \"\"\" Periodically checks to see if the task has completed. \"\"\"\n        if self.timer:\n            self.timer.cancel()\n        self.timer = threading.Timer(2.5, self.check_complete)\n        self.timer.daemon = True\n        self.timer.start()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _head_temp_file(self, temp_file, num_lines):\n        if not isinstance(num_lines, int):\n            raise DagobahError('num_lines must be an integer')\n        temp_file.seek(0)\n        result, curr_line = [], 0\n        for line in temp_file:\n            curr_line += 1\n            result.append(line.strip())\n            if curr_line >= num_lines:\n                break\n        return result", "response": "Returns a list of the first num_lines lines from a temp file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _tail_temp_file(self, temp_file, num_lines, seek_offset=10000):\n\n        if not isinstance(num_lines, int):\n            raise DagobahError('num_lines must be an integer')\n\n        temp_file.seek(0, os.SEEK_END)\n        size = temp_file.tell()\n        temp_file.seek(-1 * min(size, seek_offset), os.SEEK_END)\n\n        result = []\n        while True:\n            this_line = temp_file.readline()\n            if this_line == '':\n                break\n            result.append(this_line.strip())\n            if len(result) > num_lines:\n                result.pop(0)\n        return result", "response": "Returns a list of the last num_lines lines from a temp file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _task_complete(self, **kwargs):\n        logger.debug('Running _task_complete for task {0}'.format(self.name))\n        with self.parent_job.completion_lock:\n            self.completed_at = datetime.utcnow()\n            self.successful = kwargs.get('success', None)\n            self.parent_job._complete_task(self.name, **kwargs)", "response": "Cleans up the tasks and notifies the parent job that the Task has finished."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _serialize(self, include_run_logs=False, strict_json=False):\n\n        result = {'command': self.command,\n                  'name': self.name,\n                  'started_at': self.started_at,\n                  'completed_at': self.completed_at,\n                  'success': self.successful,\n                  'soft_timeout': self.soft_timeout,\n                  'hard_timeout': self.hard_timeout,\n                  'hostname': self.hostname}\n\n        if include_run_logs:\n            last_run = self.backend.get_latest_run_log(self.parent_job.job_id,\n                                                       self.name)\n            if last_run:\n                run_log = last_run.get('tasks', {}).get(self.name, {})\n                if run_log:\n                    result['run_log'] = run_log\n\n        if strict_json:\n            result = json.loads(json.dumps(result, cls=StrictJSONEncoder))\n        return result", "response": "Serialize a representation of this Task to a Python dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef do_login():\n\n    dt_filter = lambda x: x >= datetime.utcnow() - timedelta(seconds=60)\n    app.config['AUTH_ATTEMPTS'] = filter(dt_filter, app.config['AUTH_ATTEMPTS'])\n\n    if len(app.config['AUTH_ATTEMPTS']) > app.config['AUTH_RATE_LIMIT']:\n        return redirect(url_for('login',\n                                alert=\"Rate limit exceeded. Try again in 60 seconds.\"))\n\n    if request.form.get('password') == app.config['APP_PASSWORD']:\n        login_user(SingleAuthUser)\n        return redirect('/')\n\n    app.config['AUTH_ATTEMPTS'].append(datetime.utcnow())\n    return redirect(url_for('login', alert=\"Incorrect password.\"))", "response": "Attempt to login using single login. Rate limited at the site level."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef return_standard_conf():\n    result = resource_string(__name__, 'daemon/dagobahd.yml')\n    result = result % {'app_secret': os.urandom(24).encode('hex')}\n    return result", "response": "Return the sample config file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef job_detail(job_id=None):\n    jobs = [job for job in get_jobs() if str(job['job_id']) == job_id]\n    if not jobs:\n        abort(404)\n    return render_template('job_detail.html', job=jobs[0], hosts=dagobah.get_hosts())", "response": "Show a detailed description of a Job."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nshow a detailed description of a specific task.", "response": "def task_detail(job_id=None, task_name=None):\n    \"\"\" Show a detailed description of a specific task. \"\"\"\n    jobs = get_jobs()\n    job = [job for job in jobs if str(job['job_id']) == job_id][0]\n    return render_template('task_detail.html',\n                           job=job,\n                           task_name=task_name,\n                           task=[task for task in job['tasks']\n                                 if task['name'] == task_name][0])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nshowing a detailed description of a specific log.", "response": "def log_detail(job_id=None, task_name=None, log_id=None):\n        \"\"\" Show a detailed description of a specific log. \"\"\"\n        jobs = get_jobs()\n        job = [job for job in jobs if str(job['job_id']) == job_id][0]\n        return render_template('log_detail.html',\n                               job=job,\n                               task_name=task_name,\n                               task=[task for task in job['tasks']\n                                     if task['name'] == task_name][0],\n                               log_id=log_id)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a standard formatting of a Task serialization.", "response": "def _task_to_text(self, task):\n        \"\"\" Return a standard formatting of a Task serialization. \"\"\"\n\n        started = self._format_date(task.get('started_at', None))\n        completed = self._format_date(task.get('completed_at', None))\n\n        success = task.get('success', None)\n        success_lu = {None: 'Not executed', True: 'Success',\n                      False: 'Failed'}\n\n        run_log = task.get('run_log', {})\n        return '\\n'.join(['Task: %s' % task.get('name', None),\n                          'Command: %s' % task.get('command', None),\n                          'Result: %s' % success_lu[success],\n                          'Started at: %s' % started,\n                          'Completed at: %s' % completed,\n                          'Return Code: %s' % run_log.get('return_code', None),\n                          'Stdout: %s' % run_log.get('stdout', None),\n                          'Stderr: %s' % run_log.get('stderr', None)])"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a standard formatting of a Job object.", "response": "def _job_to_text(self, job):\n        \"\"\" Return a standard formatting of a Job serialization. \"\"\"\n\n        next_run = self._format_date(job.get('next_run', None))\n\n        tasks = ''\n        for task in job.get('tasks', []):\n            tasks += self._task_to_text(task)\n            tasks += '\\n\\n'\n\n        return '\\n'.join(['Job name: %s' % job.get('name', None),\n                          'Cron schedule: %s' % job.get('cron_schedule', None),\n                          'Next run: %s' % next_run,\n                          '',\n                          'Parent ID: %s' % job.get('parent_id', None),\n                          'Job ID: %s' % job.get('job_id', None),\n                          '',\n                          'Tasks Detail',\n                          '',\n                          tasks])"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a Jinja2 template of the specified file.", "response": "def _get_template(self, template_name, template_file):\n        \"\"\" Returns a Jinja2 template of the specified file. \"\"\"\n        template = os.path.join(self.location, 'templates',\n                                template_name, template_file)\n        return jinja2.Template(open(template).read())"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nupdates a dict or list in place to replace none string values with Python None.", "response": "def replace_nones(dict_or_list):\n    \"\"\"Update a dict or list in place to replace\n    'none' string values with Python None.\"\"\"\n\n    def replace_none_in_value(value):\n        if isinstance(value, basestring) and value.lower() == \"none\":\n            return None\n        return value\n\n    items = dict_or_list.iteritems() if isinstance(dict_or_list, dict) else enumerate(dict_or_list)\n\n    for accessor, value in items:\n        if isinstance(value, (dict, list)):\n            replace_nones(value)\n        else:\n            dict_or_list[accessor] = replace_none_in_value(value)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the loaded config file if one exists.", "response": "def get_config_file():\n    \"\"\" Return the loaded config file if one exists. \"\"\"\n\n    # config will be created here if we can't find one\n    new_config_path = os.path.expanduser('~/.dagobahd.yml')\n\n    config_dirs = ['/etc',\n                   os.path.expanduser('~')]\n    config_filenames = ['dagobahd.yml',\n                        'dagobahd.yaml',\n                        '.dagobahd.yml',\n                        '.dagobahd.yaml']\n\n    for directory in config_dirs:\n        for filename in config_filenames:\n            try:\n                if os.path.isfile(os.path.join(directory, filename)):\n                    to_load = open(os.path.join(directory, filename))\n                    config = yaml.load(to_load.read())\n                    to_load.close()\n                    replace_nones(config)\n                    return config\n            except:\n                pass\n\n    # if we made it to here, need to create a config file\n    # double up on notifications here to make sure first-time user sees it\n    print 'Creating new config file in home directory'\n    logging.info('Creating new config file in home directory')\n    new_config = open(new_config_path, 'w')\n    new_config.write(return_standard_conf())\n    new_config.close()\n\n    new_config = open(new_config_path, 'r')\n    config = yaml.load(new_config.read())\n    new_config.close()\n    replace_nones(config)\n    return config"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef configure_event_hooks(config):\n\n    def print_event_info(**kwargs):\n        print kwargs.get('event_params', {})\n\n    def job_complete_email(email_handler, **kwargs):\n        email_handler.send_job_completed(kwargs['event_params'])\n\n    def job_failed_email(email_handler, **kwargs):\n        email_handler.send_job_failed(kwargs['event_params'])\n\n    def task_failed_email(email_handler, **kwargs):\n        email_handler.send_task_failed(kwargs['event_params'])\n\n    handler = EventHandler()\n\n    email_handler = get_email_handler(get_conf(config, 'Dagobahd.email', None),\n                                      get_conf(config, 'Email', {}))\n\n    if (email_handler and\n        get_conf(config, 'Email.send_on_success', False) == True):\n        handler.register('job_complete', job_complete_email, email_handler)\n\n    if (email_handler and\n        get_conf(config, 'Email.send_on_failure', False) == True):\n        handler.register('job_failed', job_failed_email, email_handler)\n        handler.register('task_failed', task_failed_email, email_handler)\n\n    return handler", "response": "Returns an EventHandler instance with registered hooks."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef init_core_logger(location, config):\n\n    logger = logging.getLogger('dagobah')\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\n    if get_conf(config, 'Logging.Core.enabled', False) == False:\n        logger.addHandler(NullHandler())\n        return\n\n    config_filepath = get_conf(config, 'Logging.Core.logfile', 'default')\n    if config_filepath == 'default':\n        config_filepath = os.path.join(location, 'dagobah.log')\n    config_filepath = os.path.expanduser(config_filepath) if config_filepath else None\n\n    level_string = get_conf(config, 'Logging.Core.loglevel', 'info').upper()\n    numeric_level = getattr(logging, level_string, None)\n\n    basic_config_kwargs = {'level': numeric_level,\n                           'format': '%(asctime)s - %(name)s - %(levelname)s - %(message)s'}\n    if config_filepath:\n        basic_config_kwargs['filename'] = config_filepath\n    else:\n        basic_config_kwargs['stream'] = open(os.devnull, 'w')\n    logging.basicConfig(**basic_config_kwargs)\n\n    if get_conf(config, 'Logging.Core.log_to_stdout'):\n        root = logging.getLogger()\n        stdout_logger = logging.StreamHandler(sys.stdout)\n        stdout_logger.setLevel(logging.DEBUG)\n        stdout_logger.setFormatter(formatter)\n        root.addHandler(stdout_logger)\n\n    if config_filepath:\n        print 'Logging output to %s' % config_filepath\n    logging.info('Core logger initialized at level %s' % level_string)", "response": "Initialize the core logger with the given configuration."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a new instance of the backend based on the Daemon config file.", "response": "def get_backend(config):\n    \"\"\" Returns a backend instance based on the Daemon config file. \"\"\"\n\n    backend_string = get_conf(config, 'Dagobahd.backend', None)\n\n    if backend_string is None:\n        from ..backend.base import BaseBackend\n        return BaseBackend()\n\n    elif backend_string.lower() == 'mongo':\n        backend_kwargs = {}\n        for conf_kwarg in ['host', 'port', 'db',\n                           'dagobah_collection', 'job_collection',\n                           'log_collection']:\n            backend_kwargs[conf_kwarg] = get_conf(config,\n                                                  'MongoBackend.%s' % conf_kwarg)\n        backend_kwargs['port'] = int(backend_kwargs['port'])\n\n        try:\n            from ..backend.mongo import MongoBackend\n        except:\n            raise ImportError('Could not initialize the MongoDB Backend. Are you sure' +\n                              ' the optional drivers are installed? If not, try running ' +\n                              '\"pip install pymongo\" to install them.')\n        return MongoBackend(**backend_kwargs)\n\n    raise ValueError('unknown backend type specified in conf')"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a function result in API format if requested from an API WorkItem endpoint", "response": "def api_call(fn):\n    \"\"\" Returns function result in API format if requested from an API\n    endpoint \"\"\"\n\n    @wraps(fn)\n    def wrapper(*args, **kwargs):\n        try:\n            result = fn(*args, **kwargs)\n        except (DagobahError, DAGValidationError) as e:\n            if request and request.endpoint == fn.__name__:\n                return jsonify(error_type=type(e).__name__, message=e.message), 400\n            raise e\n        except Exception as e:\n            logging.exception(e)\n            raise e\n\n        if request and request.endpoint == fn.__name__:\n            status_code = None\n            try:\n                if result and '_status' in result:\n                    status_code = result['_status']\n                    del result['_status']\n            except TypeError:\n                pass\n\n            if isinstance(result, dict):\n                if 'result' in result:\n                    return jsonify(status=status_code if status_code else 200,\n                                   **result)\n                else:\n                    return jsonify(status=status_code if status_code else 200,\n                                   result=result)\n            else:\n                return jsonify(status=status_code if status_code else 200,\n                               result=result)\n\n        else:\n            return result\n\n    return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef validate_dict(in_dict, **kwargs):\n\n    if not isinstance(in_dict, dict):\n        raise ValueError('requires a dictionary')\n\n    for key, value in kwargs.iteritems():\n\n        if key == 'required':\n            for required_key in value:\n                if required_key not in in_dict:\n                    return False\n\n        elif key not in in_dict:\n            continue\n\n        elif value == bool:\n\n            in_dict[key] = (True\n                            if str(in_dict[key]).lower() == 'true'\n                            else False)\n\n        else:\n\n            if (isinstance(in_dict[key], list) and\n                len(in_dict[key]) == 1 and\n                value != list):\n                in_dict[key] = in_dict[key][0]\n\n            try:\n                if key in in_dict:\n                    in_dict[key] = value(in_dict[key])\n            except ValueError:\n                return False\n\n    return True", "response": "Validates that given dict conforms to type specifications\n    given in kwargs. Returns Boolean of whether given dict conforms to type specifications\n    given in kwargs."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndeletes the Dagobah and all child Jobs from the database.", "response": "def delete_dagobah(self, dagobah_id):\n        \"\"\" Deletes the Dagobah and all child Jobs from the database.\n\n        Related run logs are deleted as well.\n        \"\"\"\n\n        rec = self.dagobah_coll.find_one({'_id': dagobah_id})\n        for job in rec.get('jobs', []):\n            if 'job_id' in job:\n                self.delete_job(job['job_id'])\n        self.log_coll.remove({'parent_id': dagobah_id})\n        self.dagobah_coll.remove({'_id': dagobah_id})"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncommits a run log to the Mongo backend.", "response": "def commit_log(self, log_json):\n        \"\"\" Commits a run log to the Mongo backend.\n\n        Due to limitations of maximum document size in Mongo,\n        stdout and stderr logs are truncated to a maximum size for\n        each task.\n        \"\"\"\n\n        log_json['_id'] = log_json['log_id']\n        append = {'save_date': datetime.utcnow()}\n\n        for task_name, values in log_json.get('tasks', {}).items():\n            for key, size in TRUNCATE_LOG_SIZES_CHAR.iteritems():\n                if isinstance(values.get(key, None), str):\n                    if len(values[key]) > size:\n                        values[key] = '\\n'.join([values[key][:size/2],\n                                                 'DAGOBAH STREAM SPLIT',\n                                                 values[key][-1 * (size/2):]])\n        self.log_coll.save(dict(log_json.items() + append.items()))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndecode a JSON string based on a list of transformers.", "response": "def decode_import_json(self, json_doc, transformers=None):\n        \"\"\" Decode a JSON string based on a list of transformers.\n\n        Each transformer is a pair of ([conditional], transformer). If\n        all conditionals are met on each non-list, non-dict object,\n        the transformer tries to apply itself.\n\n        conditional: Callable that returns a Bool.\n        transformer: Callable transformer on non-dict, non-list objects.\n        \"\"\"\n\n        def custom_decoder(dct):\n\n            def transform(o):\n\n                if not transformers:\n                    return o\n\n                for conditionals, transformer in transformers:\n\n                    conditions_met = True\n                    for conditional in conditionals:\n                        try:\n                            condition_met = conditional(o)\n                        except:\n                            condition_met = False\n                        if not condition_met:\n                            conditions_met = False\n                            break\n\n                    if not conditions_met:\n                        continue\n\n                    try:\n                        return transformer(o)\n                    except:\n                        pass\n\n                return o\n\n            for key in dct.iterkeys():\n                if isinstance(key, dict):\n                    custom_decoder(dct[key])\n                elif isinstance(key, list):\n                    [custom_decoder[elem] for elem in dct[key]]\n                else:\n                    dct[key] = transform(dct[key])\n\n            return dct\n\n        return json.loads(json_doc, object_hook=custom_decoder)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef redis(self):\n        redis_url = current_app.config.get(\"SSE_REDIS_URL\")\n        if not redis_url:\n            redis_url = current_app.config.get(\"REDIS_URL\")\n        if not redis_url:\n            raise KeyError(\"Must set a redis connection URL in app config.\")\n        return StrictRedis.from_url(redis_url)", "response": "A redis instance configured to connect to the current application s Redis server."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\npublishes data as a server - sent event.", "response": "def publish(self, data, type=None, id=None, retry=None, channel='sse'):\n        \"\"\"\n        Publish data as a server-sent event.\n\n        :param data: The event data. If it is not a string, it will be\n            serialized to JSON using the Flask application's\n            :class:`~flask.json.JSONEncoder`.\n        :param type: An optional event type.\n        :param id: An optional event ID.\n        :param retry: An optional integer, to specify the reconnect time for\n            disconnected clients of this stream.\n        :param channel: If you want to direct different events to different\n            clients, you may specify a channel for this event to go to.\n            Only clients listening to the same channel will receive this event.\n            Defaults to \"sse\".\n        \"\"\"\n        message = Message(data, type=type, id=id, retry=retry)\n        msg_json = json.dumps(message.to_dict())\n        return self.redis.publish(channel=channel, message=msg_json)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef messages(self, channel='sse'):\n        pubsub = self.redis.pubsub()\n        pubsub.subscribe(channel)\n        for pubsub_message in pubsub.listen():\n            if pubsub_message['type'] == 'message':\n                msg_dict = json.loads(pubsub_message['data'])\n                yield Message(**msg_dict)", "response": "A generator of all messages from the given channel."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse and cast provided value.", "response": "def cast(cls, value, cast=str, subcast=None):\n        \"\"\"\n        Parse and cast provided value.\n\n        :param value: Stringed value.\n        :param cast: Type or callable to cast return value as.\n        :param subcast: Subtype or callable to cast return values as (used for\n                        nested structures).\n\n        :returns: Value of type `cast`.\n        \"\"\"\n        if cast is bool:\n            value = value.lower() in cls.BOOLEAN_TRUE_STRINGS\n        elif cast is float:\n            # Clean string\n            float_str = re.sub(r'[^\\d,\\.]', '', value)\n            # Split to handle thousand separator for different locales, i.e.\n            # comma or dot being the placeholder.\n            parts = re.split(r'[,\\.]', float_str)\n            if len(parts) == 1:\n                float_str = parts[0]\n            else:\n                float_str = \"{0}.{1}\".format(''.join(parts[0:-1]), parts[-1])\n            value = float(float_str)\n        elif type(cast) is type and (issubclass(cast, list) or\n                                     issubclass(cast, tuple)):\n            value = (subcast(i.strip()) if subcast else i.strip() for i in\n                     value.split(',') if i)\n        elif cast is dict:\n            value = {k.strip(): subcast(v.strip()) if subcast else v.strip()\n                     for k, v in (i.split('=') for i in value.split(',') if\n                     value)}\n        try:\n            return cast(value)\n        except ValueError as error:\n            raise ConfigurationError(*error.args)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread a. env file and sets the environment variables in the environment.", "response": "def read_envfile(path=None, **overrides):\n        \"\"\"\n        Read a .env file (line delimited KEY=VALUE) into os.environ.\n\n        If not given a path to the file, recurses up the directory tree until\n        found.\n\n        Uses code from Honcho (github.com/nickstenning/honcho) for parsing the\n        file.\n        \"\"\"\n        if path is None:\n            frame = inspect.currentframe().f_back\n            caller_dir = os.path.dirname(frame.f_code.co_filename)\n            path = os.path.join(os.path.abspath(caller_dir), '.env')\n\n        try:\n            with open(path, 'r') as f:\n                content = f.read()\n        except getattr(__builtins__, 'FileNotFoundError', IOError):\n            logger.debug('envfile not found at %s, looking in parent dir.',\n                         path)\n            filedir, filename = os.path.split(path)\n            pardir = os.path.abspath(os.path.join(filedir, os.pardir))\n            path = os.path.join(pardir, filename)\n            if filedir != pardir:\n                Env.read_envfile(path, **overrides)\n            else:\n                # Reached top level directory.\n                warnings.warn('Could not any envfile.')\n            return\n\n        logger.debug('Reading environment variables from: %s', path)\n        for line in content.splitlines():\n            tokens = list(shlex.shlex(line, posix=True))\n            # parses the assignment statement\n            if len(tokens) < 3:\n                continue\n            name, op = tokens[:2]\n            value = ''.join(tokens[2:])\n            if op != '=':\n                continue\n            if not re.match(r'[A-Za-z_][A-Za-z_0-9]*', name):\n                continue\n            value = value.replace(r'\\n', '\\n').replace(r'\\t', '\\t')\n            os.environ.setdefault(name, value)\n\n        for name, value in overrides.items():\n            os.environ.setdefault(name, value)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npressing R in home view to retrieve quiz list", "response": "def reload_list(self):\n        '''Press R in home view to retrieve quiz list'''\n        self.leetcode.load()\n        if self.leetcode.quizzes and len(self.leetcode.quizzes) > 0:\n            self.home_view = self.make_listview(self.leetcode.quizzes)\n            self.view_stack = []\n            self.goto_view(self.home_view)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the simplices of the triangulation.", "response": "def simplices(self):\n        \"\"\"\n        Returns the simplices of the triangulation.\n        \"\"\"\n        return [Simplex([self.points[i] for i in v]) for v in self.vertices]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef qhull_cmd(cmd, options, points):\n    prep_str = [str(len(points[0])), str(len(points))]\n    prep_str.extend([' '.join(map(repr, row)) for row in points])\n    output = getattr(hull, cmd)(options, \"\\n\".join(prep_str))\n    return list(map(str.strip, output.strip().split(\"\\n\")))", "response": "Generalized helper method to perform a qhull based command."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nvolume of the simplex.", "response": "def volume(self):\n        \"\"\"\n        Volume of the simplex.\n        \"\"\"\n        return abs(np.linalg.det(self.T)) / math.factorial(self.space_dim)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_hyperplane(basis, origin, point, internal = True):\n        basis = np.array(basis)\n        assert basis.shape[0] + 1 == basis.shape[1]\n\n        big_basis = np.zeros((basis.shape[1], basis.shape[1]))\n        big_basis[:basis.shape[0],:basis.shape[1]] = basis\n\n        u, s, vh = np.linalg.svd(big_basis)\n        null_mask = (s <= 1e-8)\n        normal = np.compress(null_mask, vh, axis=0)[0]\n\n        if np.inner(np.array(point)-np.array(origin), normal) > 0:\n            if internal:\n                normal *= -1\n        else:\n            if not internal:\n                normal *= -1\n        offset = -np.dot(origin, normal)\n        return Halfspace(normal, offset)", "response": "Returns a Halfspace defined by a list of vectors parallel to the bounding hyperplane."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the vertices of the halfspace intersection with the current instance.", "response": "def vertices(self):\n        \"\"\"\n        Returns the vertices of the halfspace intersection\n        \"\"\"\n        if self._v_out is None:\n            output = qhalf('Fp', self.halfspaces, self.interior_point)\n            pts = []\n            for l in output[2:]:\n                pt = []\n                for c in l.split():\n                    c = float(c)\n                    if c != 10.101 and c != -10.101:\n                        pt.append(c)\n                    else:\n                        pt.append(np.inf)\n                pts.append(pt)\n            self._v_out = np.array(pts)\n        return self._v_out"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of non - redundant halfspace indices for each vertex in the set.", "response": "def facets_by_vertex(self):\n        \"\"\"\n        Returns a list of non-redundant halfspace indices for each vertex\n        e.g: facets_by_vertex[0] is the list of indices of halfspaces\n        incident to vertex 0\n        \"\"\"\n        if self._fbv_out is None:\n            output = qhalf('Fv', self.halfspaces, self.interior_point)\n            facets = []\n            for l in output[1:]:\n                facets.append([int(i) for i in l.split()[1:]])\n            self._fbv_out = facets\n        return self._fbv_out"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of vertex indices for each halfspace in the set.", "response": "def facets_by_halfspace(self):\n        \"\"\"\n        Returns a list of vertex indices for each halfspace\n        e.g: facets_by_halfspace[0] is the list of indices ov vertices\n        incident to halfspace 0\n        \"\"\"\n        if self._fbh_out is None:\n            output = qhalf('FN', self.halfspaces, self.interior_point)\n            facets = []\n            for l in output[1:]:\n                facets.append([int(i) for i in l.split()[1:]])\n            self._fbh_out = facets\n        return self._fbh_out"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a pandas DataFrame containing all relevant class properties and values for the specified game.", "response": "def dataframe(self):\n        \"\"\"\n        Returns a ``pandas DataFrame`` containing all other relevant class\n        properties and values for the specified game.\n        \"\"\"\n        fields_to_include = {\n            'assists': self.assists,\n            'at_bats': self.at_bats,\n            'average_leverage_index': self.average_leverage_index,\n            'average_leverage_index_pitcher':\n            self.average_leverage_index_pitcher,\n            'bases_on_balls': self.bases_on_balls,\n            'bases_on_balls_given': self.bases_on_balls_given,\n            'base_out_runs_added': self.base_out_runs_added,\n            'base_out_runs_saved': self.base_out_runs_saved,\n            'batters_faced': self.batters_faced,\n            'batting_average': self.batting_average,\n            'earned_runs_allowed': self.earned_runs_allowed,\n            'earned_runs_against': self.earned_runs_against,\n            'fly_balls': self.fly_balls,\n            'game_score': self.game_score,\n            'grounded_balls': self.grounded_balls,\n            'hits': self.hits,\n            'hits_allowed': self.hits_allowed,\n            'home_runs_thrown': self.home_runs_thrown,\n            'inherited_runners': self.inherited_runners,\n            'inherited_score': self.inherited_score,\n            'innings_pitched': self.innings_pitched,\n            'line_drives': self.line_drives,\n            'name': self.name,\n            'on_base_percentage': self.on_base_percentage,\n            'on_base_plus_slugging_percentage':\n            self.on_base_plus_slugging_percentage,\n            'pitches_thrown': self.pitches_thrown,\n            'plate_appearances': self.plate_appearances,\n            'putouts': self.putouts,\n            'runs': self.runs,\n            'runs_allowed': self.runs_allowed,\n            'runs_batted_in': self.runs_batted_in,\n            'slugging_percentage': self.slugging_percentage,\n            'strikes': self.strikes,\n            'strikes_contact': self.strikes_contact,\n            'strikes_looking': self.strikes_looking,\n            'strikes_swinging': self.strikes_swinging,\n            'strikes_thrown': self.strikes_thrown,\n            'strikeouts': self.strikeouts,\n            'times_struck_out': self.times_struck_out,\n            'unknown_bat_types': self.unknown_bat_types,\n            'win_probability_added': self.win_probability_added,\n            'win_probability_added_pitcher':\n            self.win_probability_added_pitcher,\n            'win_probability_for_offensive_player':\n            self.win_probability_for_offensive_player,\n            'win_probability_subtracted': self.win_probability_subtracted\n        }\n        return pd.DataFrame([fields_to_include], index=[self._player_id])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _extract_player_stats(self, table, player_dict, home_or_away):\n        for row in table('tbody tr').items():\n            player_id = self._find_player_id(row)\n            # Occurs when a header row is identified instead of a player.\n            if not player_id:\n                continue\n            name = self._find_player_name(row)\n            try:\n                player_dict[player_id]['data'] += str(row).strip()\n            except KeyError:\n                player_dict[player_id] = {\n                    'name': name,\n                    'data': str(row).strip(),\n                    'team': home_or_away\n                }\n        return player_dict", "response": "Extract all player stats from a single boxscore table."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _instantiate_players(self, player_dict):\n        home_players = []\n        away_players = []\n        for player_id, details in player_dict.items():\n            player = BoxscorePlayer(player_id,\n                                    details['name'],\n                                    details['data'])\n            if details['team'] == HOME:\n                home_players.append(player)\n            else:\n                away_players.append(player)\n        return away_players, home_players", "response": "Instantiates the players for both the home and away teams."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _find_players(self, boxscore):\n        player_dict = {}\n        table_count = 0\n\n        tables = self._find_boxscore_tables(boxscore)\n        for table in tables:\n            home_or_away = HOME\n            # There are two tables per team with the odd tables belonging to\n            # the away team.\n            if table_count % 2 == 1:\n                home_or_away = AWAY\n            player_dict = self._extract_player_stats(table,\n                                                     player_dict,\n                                                     home_or_away)\n            table_count += 1\n        away_players, home_players = self._instantiate_players(player_dict)\n        return away_players, home_players", "response": "Find all players for each team and create a list of instances of BoxscorePlayer class for each team and each player."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a pandas DataFrame containing all other class properties and values.", "response": "def dataframe(self):\n        \"\"\"\n        Returns a pandas DataFrame containing all other class properties and\n        values. The index for the DataFrame is the string URI that is used to\n        instantiate the class, such as 'BOS201806070'.\n        \"\"\"\n        if self._away_runs is None and self._home_runs is None:\n            return None\n        fields_to_include = {\n            'date': self.date,\n            'time': self.time,\n            'venue': self.venue,\n            'attendance': self.attendance,\n            'duration': self.duration,\n            'time_of_day': self.time_of_day,\n            'winner': self.winner,\n            'winning_name': self.winning_name,\n            'winning_abbr': self.winning_abbr,\n            'losing_name': self.losing_name,\n            'losing_abbr': self.losing_abbr,\n            'away_at_bats': self.away_at_bats,\n            'away_runs': self.away_runs,\n            'away_hits': self.away_hits,\n            'away_rbi': self.away_rbi,\n            'away_earned_runs': self.away_earned_runs,\n            'away_bases_on_balls': self.away_bases_on_balls,\n            'away_strikeouts': self.away_strikeouts,\n            'away_plate_appearances': self.away_plate_appearances,\n            'away_batting_average': self.away_batting_average,\n            'away_on_base_percentage': self.away_on_base_percentage,\n            'away_slugging_percentage': self.away_slugging_percentage,\n            'away_on_base_plus': self.away_on_base_plus,\n            'away_pitches': self.away_pitches,\n            'away_strikes': self.away_strikes,\n            'away_win_probability_for_offensive_player':\n            self.away_win_probability_for_offensive_player,\n            'away_average_leverage_index': self.away_average_leverage_index,\n            'away_win_probability_added': self.away_win_probability_added,\n            'away_win_probability_subtracted':\n            self.away_win_probability_subtracted,\n            'away_base_out_runs_added': self.away_base_out_runs_added,\n            'away_putouts': self.away_putouts,\n            'away_assists': self.away_assists,\n            'away_innings_pitched': self.away_innings_pitched,\n            'away_home_runs': self.away_home_runs,\n            'away_strikes_by_contact': self.away_strikes_by_contact,\n            'away_strikes_swinging': self.away_strikes_swinging,\n            'away_strikes_looking': self.away_strikes_looking,\n            'away_grounded_balls': self.away_grounded_balls,\n            'away_fly_balls': self.away_fly_balls,\n            'away_line_drives': self.away_line_drives,\n            'away_unknown_bat_type': self.away_unknown_bat_type,\n            'away_game_score': self.away_game_score,\n            'away_inherited_runners': self.away_inherited_runners,\n            'away_inherited_score': self.away_inherited_score,\n            'away_win_probability_by_pitcher':\n            self.away_win_probability_by_pitcher,\n            'away_base_out_runs_saved': self.away_base_out_runs_saved,\n            'home_at_bats': self.home_at_bats,\n            'home_runs': self.home_runs,\n            'home_hits': self.home_hits,\n            'home_rbi': self.home_rbi,\n            'home_earned_runs': self.home_earned_runs,\n            'home_bases_on_balls': self.home_bases_on_balls,\n            'home_strikeouts': self.home_strikeouts,\n            'home_plate_appearances': self.home_plate_appearances,\n            'home_batting_average': self.home_batting_average,\n            'home_on_base_percentage': self.home_on_base_percentage,\n            'home_slugging_percentage': self.home_slugging_percentage,\n            'home_on_base_plus': self.home_on_base_plus,\n            'home_pitches': self.home_pitches,\n            'home_strikes': self.home_strikes,\n            'home_win_probability_for_offensive_player':\n            self.home_win_probability_for_offensive_player,\n            'home_average_leverage_index': self.home_average_leverage_index,\n            'home_win_probability_added': self.home_win_probability_added,\n            'home_win_probability_subtracted':\n            self.home_win_probability_subtracted,\n            'home_base_out_runs_added': self.home_base_out_runs_added,\n            'home_putouts': self.home_putouts,\n            'home_assists': self.home_assists,\n            'home_innings_pitched': self.home_innings_pitched,\n            'home_home_runs': self.home_home_runs,\n            'home_strikes_by_contact': self.home_strikes_by_contact,\n            'home_strikes_swinging': self.home_strikes_swinging,\n            'home_strikes_looking': self.home_strikes_looking,\n            'home_grounded_balls': self.home_grounded_balls,\n            'home_fly_balls': self.home_fly_balls,\n            'home_line_drives': self.home_line_drives,\n            'home_unknown_bat_type': self.home_unknown_bat_type,\n            'home_game_score': self.home_game_score,\n            'home_inherited_runners': self.home_inherited_runners,\n            'home_inherited_score': self.home_inherited_score,\n            'home_win_probability_by_pitcher':\n            self.home_win_probability_by_pitcher,\n            'home_base_out_runs_saved': self.home_base_out_runs_saved\n        }\n        return pd.DataFrame([fields_to_include], index=[self._uri])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef winning_name(self):\n        if self.winner == HOME:\n            return self._home_name.text()\n        return self._away_name.text()", "response": "Returns a string of the winning team s name such as Houston\n        Astros."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a string of the winning team s abbreviation such as HOU or Houston Astros.", "response": "def winning_abbr(self):\n        \"\"\"\n        Returns a ``string`` of the winning team's abbreviation, such as 'HOU'\n        for the Houston Astros.\n        \"\"\"\n        if self.winner == HOME:\n            return utils._parse_abbreviation(self._home_name)\n        return utils._parse_abbreviation(self._away_name)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef losing_name(self):\n        if self.winner == HOME:\n            return self._away_name.text()\n        return self._home_name.text()", "response": "Returns a string of the losing team s name such as Los Angeles\n        Dodgers."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a string of the losing team s abbreviation such as LAD or LAD.", "response": "def losing_abbr(self):\n        \"\"\"\n        Returns a ``string`` of the losing team's abbreviation, such as 'LAD'\n        for the Los Angeles Dodgers.\n        \"\"\"\n        if self.winner == HOME:\n            return utils._parse_abbreviation(self._away_name)\n        return utils._parse_abbreviation(self._home_name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _create_url(self, date):\n        return BOXSCORES_URL % (date.year, date.month, date.day)", "response": "Build the URL based on the passed datetime object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_boxscore_uri(self, url):\n        uri = re.sub(r'.*/boxes/', '', str(url))\n        uri = re.sub(r'\\.shtml.*', '', uri).strip()\n        return uri", "response": "Find the boxscore URI for a game."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _parse_abbreviation(self, abbr):\n        abbr = re.sub(r'.*/teams/', '', str(abbr))\n        abbr = re.sub(r'/.*', '', abbr)\n        return abbr", "response": "Parse a team s abbreviation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_name(self, name):\n        team_name = name.text()\n        abbr = self._parse_abbreviation(name)\n        return team_name, abbr", "response": "Given a team s HTML name tag determine their name and abbreviation."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_score(self, score_link):\n        score = score_link.replace('<td class=\"right\">', '')\n        score = score.replace('</td>', '')\n        return int(score)", "response": "Given an HTML string of a team s boxscore extract the integer and return the number."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a tuple containing the names and abbreviations of both teams in a game.", "response": "def _get_team_details(self, game):\n        \"\"\"\n        Find the names and abbreviations for both teams in a game.\n\n        Using the HTML contents in a boxscore, find the name and abbreviation\n        for both teams.\n\n        Parameters\n        ----------\n        game : PyQuery object\n            A PyQuery object of a single boxscore containing information about\n            both teams.\n\n        Returns\n        -------\n        tuple\n            Returns a tuple containing the names and abbreviations of both\n            teams in the following order: Away Name, Away Abbreviation, Away\n            Score, Home Name, Home Abbreviation, Home Score.\n        \"\"\"\n        links = [i for i in game('td a').items()]\n        # The away team is the first link in the boxscore\n        away = links[0]\n        # The home team is the last (3rd) link in the boxscore\n        home = links[-1]\n        scores = re.findall(r'<td class=\"right\">\\d+</td>', str(game))\n        away_score = None\n        home_score = None\n        # If the game hasn't started or hasn't been updated on sports-reference\n        # yet, no score will be shown and therefore can't be parsed.\n        if len(scores) == 2:\n            away_score = self._get_score(scores[0])\n            home_score = self._get_score(scores[1])\n        away_name, away_abbr = self._get_name(away)\n        home_name, home_abbr = self._get_name(home)\n        return (away_name, away_abbr, away_score, home_name, home_abbr,\n                home_score)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_team_results(self, team_result_html):\n        link = [i for i in team_result_html('td a').items()]\n        # If there are no links, the boxscore is likely misformed and can't be\n        # parsed. In this case, the boxscore should be skipped.\n        if len(link) < 1:\n            return None\n        name, abbreviation = self._get_name(link[0])\n        return name, abbreviation", "response": "Extract the winning or losing team s name and abbreviation."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _extract_game_info(self, games):\n        all_boxscores = []\n\n        for game in games:\n            details = self._get_team_details(game)\n            away_name, away_abbr, away_score, home_name, home_abbr, \\\n                home_score = details\n            boxscore_url = game('td[class=\"right gamelink\"] a')\n            boxscore_uri = self._get_boxscore_uri(boxscore_url)\n            losers = [l for l in game('tr[class=\"loser\"]').items()]\n            winner = self._get_team_results(game('tr[class=\"winner\"]'))\n            loser = self._get_team_results(game('tr[class=\"loser\"]'))\n            # Occurs when the boxscore format is invalid and the game should be\n            # skipped to avoid conflicts populating the game information.\n            if (len(losers) != 2 and loser and not winner) or \\\n               (len(losers) != 2 and winner and not loser):\n                continue\n            # Occurs when information couldn't be parsed from the boxscore or\n            # the game hasn't occurred yet. In this case, the winner should be\n            # None to avoid conflicts.\n            if not winner or len(losers) == 2:\n                winning_name = None\n                winning_abbreviation = None\n            else:\n                winning_name, winning_abbreviation = winner\n            # Occurs when information couldn't be parsed from the boxscore or\n            # the game hasn't occurred yet. In this case, the winner should be\n            # None to avoid conflicts.\n            if not loser or len(losers) == 2:\n                losing_name = None\n                losing_abbreviation = None\n            else:\n                losing_name, losing_abbreviation = loser\n            game_info = {\n                'boxscore': boxscore_uri,\n                'away_name': away_name,\n                'away_abbr': away_abbr,\n                'away_score': away_score,\n                'home_name': home_name,\n                'home_abbr': home_abbr,\n                'home_score': home_score,\n                'winning_name': winning_name,\n                'winning_abbr': winning_abbreviation,\n                'losing_name': losing_name,\n                'losing_abbr': losing_abbreviation\n            }\n            all_boxscores.append(game_info)\n        return all_boxscores", "response": "Parse game information from all boxscores and return the results in a list."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsearches the HTML page for all games played on a given date.", "response": "def _find_games(self, date, end_date):\n        \"\"\"\n        Retrieve all major games played on a given day.\n\n        Builds a URL based on the requested date and downloads the HTML\n        contents before parsing any and all games played during that day. Any\n        games that are found are added to the boxscores dictionary with\n        high-level game information such as the home and away team names and a\n        link to the boxscore page.\n\n        Parameters\n        ----------\n        date : datetime object\n            The date to search for any matches. The month, day, and year are\n            required for the search, but time is not factored into the search.\n        end_date : datetime object (optional)\n            Optionally specify an end date to iterate until. All boxscores\n            starting from the date specified in the 'date' parameter up to and\n            including the boxscores specified in the 'end_date' parameter will\n            be pulled. If left empty, or if 'end_date' is prior to 'date', only\n            the games from the day specified in the 'date' parameter will be\n            saved.\n        \"\"\"\n        # Set the end date to the start date if the end date is before the\n        # start date.\n        if not end_date or date > end_date:\n            end_date = date\n        date_step = date\n        while date_step <= end_date:\n            url = self._create_url(date_step)\n            page = self._get_requested_page(url)\n            games = page('table[class=\"teams\"]').items()\n            boxscores = self._extract_game_info(games)\n            timestamp = '%s-%s-%s' % (date_step.month, date_step.day,\n                                      date_step.year)\n            self._boxscores[timestamp] = boxscores\n            date_step += timedelta(days=1)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing the season string from the stats table row.", "response": "def _parse_season(self, row):\n        \"\"\"\n        Parse the season string from the table.\n\n        The season is generally located in the first column of the stats tables\n        and should be parsed to detonate which season metrics are being pulled\n        from.\n\n        Parameters\n        ----------\n        row : PyQuery object\n            A PyQuery object of a single row in a stats table.\n\n        Returns\n        -------\n        string\n            A string representation of the season in the format 'YYYY', such as\n            '2017'.\n        \"\"\"\n        season = utils._parse_field(PLAYER_SCHEME, row, 'season')\n        return season.replace('*', '').replace('+', '')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _combine_season_stats(self, table_rows, career_stats, all_stats_dict):\n        most_recent_season = self._most_recent_season\n        if not table_rows:\n            table_rows = []\n        for row in table_rows:\n            season = self._parse_season(row)\n            try:\n                all_stats_dict[season]['data'] += str(row)\n            except KeyError:\n                all_stats_dict[season] = {'data': str(row)}\n            most_recent_season = season\n        self._most_recent_season = most_recent_season\n        if not career_stats:\n            return all_stats_dict\n        try:\n            all_stats_dict['Career']['data'] += str(next(career_stats))\n        except KeyError:\n            try:\n                all_stats_dict['Career'] = {'data': str(next(career_stats))}\n            # Occurs when the player doesn't have any career stats listed on\n            # their page in error.\n            except StopIteration:\n                return all_stats_dict\n        return all_stats_dict", "response": "Combine all stats for each season into a single dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses the player s stats page.", "response": "def _parse_player_information(self, player_info):\n        \"\"\"\n        Parse general player information.\n\n        Parse general player information such as height, weight, and name. The\n        attribute for the requested field will be set with the value prior to\n        returning.\n\n        Parameters\n        ----------\n        player_info : PyQuery object\n            A PyQuery object containing the HTML from the player's stats page.\n        \"\"\"\n        for field in ['_height', '_weight', '_name']:\n            short_field = str(field)[1:]\n            value = utils._parse_field(PLAYER_SCHEME, player_info, short_field)\n            setattr(self, field, value)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a dictionary of all the fields to include with DataFrame.", "response": "def _dataframe_fields(self):\n        \"\"\"\n        Creates a dictionary of all fields to include with DataFrame.\n\n        With the result of the calls to class properties changing based on the\n        class index value, the dictionary should be regenerated every time the\n        index is changed when the dataframe property is requested.\n\n        Returns\n        -------\n        dictionary\n            Returns a dictionary where the keys are the shortened ``string``\n            attribute names and the values are the actual value for each\n            attribute for the specified index.\n        \"\"\"\n        fields_to_include = {\n            'adjusted_net_yards_per_attempt_index':\n            self.adjusted_net_yards_per_attempt_index,\n            'adjusted_net_yards_per_pass_attempt':\n            self.adjusted_net_yards_per_pass_attempt,\n            'adjusted_yards_per_attempt': self.adjusted_yards_per_attempt,\n            'adjusted_yards_per_attempt_index':\n            self.adjusted_yards_per_attempt_index,\n            'all_purpose_yards': self.all_purpose_yards,\n            'approximate_value': self.approximate_value,\n            'assists_on_tackles': self.assists_on_tackles,\n            'attempted_passes': self.attempted_passes,\n            'birth_date': self.birth_date,\n            'blocked_punts': self.blocked_punts,\n            'catch_percentage': self.catch_percentage,\n            'completed_passes': self.completed_passes,\n            'completion_percentage_index': self.completion_percentage_index,\n            'espn_qbr': self.espn_qbr,\n            'extra_point_percentage': self.extra_point_percentage,\n            'extra_points_attempted': self.extra_points_attempted,\n            'extra_points_made': self.extra_points_made,\n            'field_goal_percentage': self.field_goal_percentage,\n            'field_goals_attempted': self.field_goals_attempted,\n            'field_goals_made': self.field_goals_made,\n            'fifty_plus_yard_field_goal_attempts':\n            self.fifty_plus_yard_field_goal_attempts,\n            'fifty_plus_yard_field_goals_made':\n            self.fifty_plus_yard_field_goals_made,\n            'fourth_quarter_comebacks': self.fourth_quarter_comebacks,\n            'fourty_to_fourty_nine_yard_field_goal_attempts':\n            self.fourty_to_fourty_nine_yard_field_goal_attempts,\n            'fourty_to_fourty_nine_yard_field_goals_made':\n            self.fourty_to_fourty_nine_yard_field_goals_made,\n            'fumbles': self.fumbles,\n            'fumbles_forced': self.fumbles_forced,\n            'fumbles_recovered': self.fumbles_recovered,\n            'fumbles_recovered_for_touchdown':\n            self.fumbles_recovered_for_touchdown,\n            'game_winning_drives': self.game_winning_drives,\n            'games': self.games,\n            'games_started': self.games_started,\n            'height': self.height,\n            'interception_percentage': self.interception_percentage,\n            'interception_percentage_index':\n            self.interception_percentage_index,\n            'interceptions': self.interceptions,\n            'interceptions_returned_for_touchdown':\n            self.interceptions_returned_for_touchdown,\n            'interceptions_thrown': self.interceptions_thrown,\n            'kickoff_return_touchdown': self.kickoff_return_touchdown,\n            'kickoff_return_yards': self.kickoff_return_yards,\n            'kickoff_returns': self.kickoff_returns,\n            'less_than_nineteen_yards_field_goal_attempts':\n            self.less_than_nineteen_yards_field_goal_attempts,\n            'less_than_nineteen_yards_field_goals_made':\n            self.less_than_nineteen_yards_field_goals_made,\n            'longest_field_goal_made': self.longest_field_goal_made,\n            'longest_interception_return': self.longest_interception_return,\n            'longest_kickoff_return': self.longest_kickoff_return,\n            'longest_pass': self.longest_pass,\n            'longest_punt': self.longest_punt,\n            'longest_punt_return': self.longest_punt_return,\n            'longest_reception': self.longest_reception,\n            'longest_rush': self.longest_rush,\n            'name': self.name,\n            'net_yards_per_attempt_index': self.net_yards_per_attempt_index,\n            'net_yards_per_pass_attempt': self.net_yards_per_pass_attempt,\n            'passer_rating_index': self.passer_rating_index,\n            'passes_defended': self.passes_defended,\n            'passing_completion': self.passing_completion,\n            'passing_touchdown_percentage': self.passing_touchdown_percentage,\n            'passing_touchdowns': self.passing_touchdowns,\n            'passing_yards': self.passing_yards,\n            'passing_yards_per_attempt': self.passing_yards_per_attempt,\n            'player_id': self.player_id,\n            'position': self.position,\n            'punt_return_touchdown': self.punt_return_touchdown,\n            'punt_return_yards': self.punt_return_yards,\n            'punt_returns': self.punt_returns,\n            'punts': self.punts,\n            'qb_record': self.qb_record,\n            'quarterback_rating': self.quarterback_rating,\n            'receiving_touchdowns': self.receiving_touchdowns,\n            'receiving_yards': self.receiving_yards,\n            'receiving_yards_per_game': self.receiving_yards_per_game,\n            'receiving_yards_per_reception':\n            self.receiving_yards_per_reception,\n            'receptions': self.receptions,\n            'receptions_per_game': self.receptions_per_game,\n            'rush_attempts': self.rush_attempts,\n            'rush_attempts_per_game': self.rush_attempts_per_game,\n            'rush_touchdowns': self.rush_touchdowns,\n            'rush_yards': self.rush_yards,\n            'rush_yards_per_attempt': self.rush_yards_per_attempt,\n            'rush_yards_per_game': self.rush_yards_per_game,\n            'rushing_and_receiving_touchdowns':\n            self.rushing_and_receiving_touchdowns,\n            'sack_percentage': self.sack_percentage,\n            'sack_percentage_index': self.sack_percentage_index,\n            'sacks': self.sacks,\n            'safeties': self.safeties,\n            'season': self.season,\n            'tackles': self.tackles,\n            'team_abbreviation': self.team_abbreviation,\n            'thirty_to_thirty_nine_yard_field_goal_attempts':\n            self.thirty_to_thirty_nine_yard_field_goal_attempts,\n            'thirty_to_thirty_nine_yard_field_goals_made':\n            self.thirty_to_thirty_nine_yard_field_goals_made,\n            'times_pass_target': self.times_pass_target,\n            'times_sacked': self.times_sacked,\n            'total_punt_yards': self.total_punt_yards,\n            'touchdown_percentage_index': self.touchdown_percentage_index,\n            'touches': self.touches,\n            'twenty_to_twenty_nine_yard_field_goal_attempts':\n            self.twenty_to_twenty_nine_yard_field_goal_attempts,\n            'twenty_to_twenty_nine_yard_field_goals_made':\n            self.twenty_to_twenty_nine_yard_field_goals_made,\n            'weight': self.weight,\n            'yards_from_scrimmage': self.yards_from_scrimmage,\n            'yards_lost_to_sacks': self.yards_lost_to_sacks,\n            'yards_per_attempt_index': self.yards_per_attempt_index,\n            'yards_per_completed_pass': self.yards_per_completed_pass,\n            'yards_per_game_played': self.yards_per_game_played,\n            'yards_per_kickoff_return': self.yards_per_kickoff_return,\n            'yards_per_punt': self.yards_per_punt,\n            'yards_per_punt_return': self.yards_per_punt_return,\n            'yards_per_touch': self.yards_per_touch,\n            'yards_recovered_from_fumble': self.yards_recovered_from_fumble,\n            'yards_returned_from_interception':\n            self.yards_returned_from_interception\n        }\n        return fields_to_include"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dataframe(self):\n        temp_index = self._index\n        rows = []\n        indices = []\n        if not self._season:\n            return None\n        for season in self._season:\n            self._index = self._season.index(season)\n            rows.append(self._dataframe_fields())\n            indices.append(season)\n        self._index = temp_index\n        return pd.DataFrame(rows, index=[indices])", "response": "Returns a pandas DataFrame containing all other relevant class\n        properties and values where each index is a different season plus the\n        career stats."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the year for a given season.", "response": "def _find_year_for_season(league):\n    \"\"\"\n    Return the necessary seaons's year based on the current date.\n\n    Since all sports start and end at different times throughout the year,\n    simply using the current year is not sufficient to describe a season. For\n    example, the NCAA Men's Basketball season begins in November every year.\n    However, for the November and December months, the following year is used\n    to denote the season (ie. November 2017 marks the start of the '2018'\n    season) on sports-reference.com. This rule does not apply to all sports.\n    Baseball begins and ends in one single calendar year, so the year never\n    needs to be incremented.\n\n    Additionally, since information for future seasons is generally not\n    finalized until a month before the season begins, the year will default to\n    the most recent season until the month prior to the season start date. For\n    example, the 2018 MLB season begins in April. In January 2018, however, not\n    all of the season's information is present in the system, so the default\n    year will be '2017'.\n\n    Parameters\n    ----------\n    league : string\n        A string pertaining to the league start information as listed in\n        SEASON_START_MONTH (ie. 'mlb', 'nba', 'nfl', etc.). League must be\n        present in SEASON_START_MONTH.\n\n    Returns\n    -------\n    int\n        The respective season's year.\n\n    Raises\n    ------\n    ValueError\n        If the passed 'league' is not a key in SEASON_START_MONTH.\n    \"\"\"\n    today = _todays_date()\n    if league not in SEASON_START_MONTH:\n        raise ValueError('\"%s\" league cannot be found!')\n    start = SEASON_START_MONTH[league]['start']\n    wrap = SEASON_START_MONTH[league]['wrap']\n    if wrap and start - 1 <= today.month <= 12:\n        return today.year + 1\n    elif not wrap and start == 1 and today.month == 12:\n        return today.year + 1\n    elif not wrap and not start - 1 <= today.month <= 12:\n        return today.year - 1\n    else:\n        return today.year"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _parse_abbreviation(uri_link):\n    abbr = re.sub(r'/[0-9]+\\..*htm.*', '', uri_link('a').attr('href'))\n    abbr = re.sub(r'/.*/schools/', '', abbr)\n    abbr = re.sub(r'/teams/', '', abbr)\n    return abbr.upper()", "response": "Parses a URI link which contains a team s abbreviation."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing an HTML table to find the requested value.", "response": "def _parse_field(parsing_scheme, html_data, field, index=0):\n    \"\"\"\n    Parse an HTML table to find the requested field's value.\n\n    All of the values are passed in an HTML table row instead of as individual\n    items. The values need to be parsed by matching the requested attribute\n    with a parsing scheme that sports-reference uses to differentiate stats.\n    This function returns a single value for the given attribute.\n\n    Parameters\n    ----------\n    parsing_scheme : dict\n        A dictionary of the parsing scheme to be used to find the desired\n        field. The key corresponds to the attribute name to parse, and the\n        value is a PyQuery-readable parsing scheme as a string (such as\n        'td[data-stat=\"wins\"]').\n    html_data : string\n        A string containing all of the rows of stats for a given team. If\n        multiple tables are being referenced, this will be comprised of\n        multiple rows in a single string.\n    field : string\n        The name of the attribute to match. Field must be a key in\n        parsing_scheme.\n    index : int (optional)\n        An optional index if multiple fields have the same attribute name. For\n        example, 'HR' may stand for the number of home runs a baseball team has\n        hit, or the number of home runs a pitcher has given up. The index\n        aligns with the order in which the attributes are recevied in the\n        html_data parameter.\n\n    Returns\n    -------\n    string\n        The value at the specified index for the requested field. If no value\n        could be found, returns None.\n    \"\"\"\n    if field == 'abbreviation':\n        return _parse_abbreviation(html_data)\n    scheme = parsing_scheme[field]\n    items = [i.text() for i in html_data(scheme).items()]\n    # Stats can be added and removed on a yearly basis. If not stats are found,\n    # return None and have the be the value.\n    if len(items) == 0:\n        return None\n    # Default to returning the first element. Optionally return another element\n    # if multiple fields have the same tag attribute.\n    try:\n        return items[index]\n    except IndexError:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a generator of all rows in a given table.", "response": "def _get_stats_table(html_page, div, footer=False):\n    \"\"\"\n    Returns a generator of all rows in a requested table.\n\n    When given a PyQuery HTML object and a requested div, this function creates\n    a generator where every item is a PyQuery object pertaining to every row in\n    the table. Generally, each row will contain multiple stats for a given\n    player or team.\n\n    Parameters\n    ----------\n    html_page : PyQuery object\n        A PyQuery object which contains the requested HTML page contents.\n    div : string\n        The requested tag type and id string in the format \"<tag>#<id name>\"\n        which aligns to the desired table in the passed HTML page. For example,\n        \"div#all_stats_table\" or \"table#conference_standings\".\n    footer : boolean (optional)\n        Optionally return the table footer rows instead of the table header.\n\n    Returns\n    -------\n    generator\n        A generator of all row items in a given table.\n    \"\"\"\n    stats_html = html_page(div)\n    try:\n        stats_table = pq(_remove_html_comment_tags(stats_html))\n    except (ParserError, XMLSyntaxError):\n        return None\n    if footer:\n        teams_list = stats_table('tfoot tr').items()\n    else:\n        teams_list = stats_table('tbody tr').items()\n    return teams_list"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dataframe(self):\n        fields_to_include = {\n            'abbreviation': self.abbreviation,\n            'average_age': self.average_age,\n            'games_played': self.games_played,\n            'goals_against': self.goals_against,\n            'goals_for': self.goals_for,\n            'losses': self.losses,\n            'name': self.name,\n            'overtime_losses': self.overtime_losses,\n            'pdo_at_even_strength': self.pdo_at_even_strength,\n            'penalty_killing_percentage': self.penalty_killing_percentage,\n            'points': self.points,\n            'points_percentage': self.points_percentage,\n            'power_play_goals': self.power_play_goals,\n            'power_play_goals_against': self.power_play_goals_against,\n            'power_play_opportunities': self.power_play_opportunities,\n            'power_play_opportunities_against':\n            self.power_play_opportunities_against,\n            'power_play_percentage': self.power_play_percentage,\n            'rank': self.rank,\n            'save_percentage': self.save_percentage,\n            'shooting_percentage': self.shooting_percentage,\n            'short_handed_goals': self.short_handed_goals,\n            'short_handed_goals_against': self.short_handed_goals_against,\n            'shots_against': self.shots_against,\n            'shots_on_goal': self.shots_on_goal,\n            'simple_rating_system': self.simple_rating_system,\n            'strength_of_schedule': self.strength_of_schedule,\n            'total_goals_per_game': self.total_goals_per_game,\n            'wins': self.wins\n        }\n        return pd.DataFrame([fields_to_include], index=[self._abbreviation])", "response": "Returns a pandas DataFrame containing all other class properties and values."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _retrieve_all_teams(self, year):\n        if not year:\n            year = utils._find_year_for_season('nhl')\n        doc = pq(SEASON_PAGE_URL % year)\n        teams_list = utils._get_stats_table(doc, 'div#all_stats')\n        # Teams are listed in terms of rank with the first team being #1\n        rank = 1\n        for team_data in teams_list:\n            team = Team(team_data, rank, year)\n            self._teams.append(team)\n            rank += 1", "response": "Retrieves all teams in a given season."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dataframes(self):\n        frames = []\n        for team in self.__iter__():\n            frames.append(team.dataframe)\n        return pd.concat(frames)", "response": "Returns a pandas DataFrame where each row is a representation of the\n            Team class. Rows are indexed by the team abbreviation. Rows are indexed by the team class. Rows are indexed by the team abbreviation. Rows are indexed by the team class. Rows are indexed by the team abbreviation."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nretrieves the team s name and abbreviation.", "response": "def _get_team(self, team):\n        \"\"\"\n        Retrieve team's name and abbreviation.\n\n        The team's name and abbreviation are embedded within the 'school_name'\n        tag and, in the case of the abbreviation, require special parsing as it\n        is located in the middle of a URI. The name and abbreviation are\n        returned for the requested school.\n\n        Parameters\n        ----------\n        team : PyQuery object\n            A PyQuery object representing a single row in a table on the\n            rankings page.\n\n        Returns\n        -------\n        tuple (string, string)\n            Returns a tuple of two strings where the first string is the team's\n            abbreviation, such as 'PURDUE' and the second string is the team's\n            name, such as 'Purdue'.\n        \"\"\"\n        name_tag = team('td[data-stat=\"school_name\"]')\n        abbreviation = re.sub(r'.*/cfb/schools/', '', str(name_tag('a')))\n        abbreviation = re.sub(r'/.*', '', abbreviation)\n        name = team('td[data-stat=\"school_name\"] a').text()\n        return abbreviation, name"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _find_rankings(self, year):\n        if not year:\n            year = utils._find_year_for_season('ncaaf')\n        page = self._pull_rankings_page(year)\n        if not page:\n            output = (\"Can't pull rankings page. Ensure the following URL \"\n                      \"exists: %s\" % RANKINGS_URL)\n            raise ValueError(output)\n        rankings = page('table#ap tbody tr').items()\n        weekly_rankings = []\n        week = 0\n        for team in rankings:\n            if 'class=\"thead\"' in str(team):\n                self._rankings[int(week)] = weekly_rankings\n                weekly_rankings = []\n                continue\n            abbreviation, name = self._get_team(team)\n            rank = utils._parse_field(RANKINGS_SCHEME, team, 'rank')\n            week = utils._parse_field(RANKINGS_SCHEME, team, 'week')\n            date = utils._parse_field(RANKINGS_SCHEME, team, 'date')\n            previous = utils._parse_field(RANKINGS_SCHEME, team, 'previous')\n            change = utils._parse_field(RANKINGS_SCHEME, team, 'change')\n            if 'decrease' in str(team(RANKINGS_SCHEME['change'])):\n                change = int(change) * -1\n            elif 'increase' in str(team(RANKINGS_SCHEME['change'])):\n                try:\n                    change = int(change)\n                except ValueError:\n                    change = 0\n            else:\n                change = 0\n            rank_details = {\n                'abbreviation': abbreviation,\n                'name': name,\n                'rank': int(rank),\n                'week': int(week),\n                'date': date,\n                'previous': previous,\n                'change': change\n            }\n            weekly_rankings.append(rank_details)\n        # Add the final rankings which is not terminated with another header\n        # row and hence will not hit the first if statement in the loop above.\n        self._rankings[int(week)] = weekly_rankings", "response": "Retrieve the rankings for the requested year and combine them with the information about the team and rank."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef current(self):\n        rankings_dict = {}\n\n        for team in self.current_extended:\n            rankings_dict[team['abbreviation']] = team['rank']\n        return rankings_dict", "response": "Returns a dictionary of the most recent rankings from the\n        associated Press where each key is a string of the team s alphabetical name and each value is an integer of the team s rank for the current week."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a pandas DataFrame containing all relevant class properties and values for the specified game.", "response": "def dataframe(self):\n        \"\"\"\n        Returns a ``pandas DataFrame`` containing all other relevant class\n        properties and values for the specified game.\n        \"\"\"\n        fields_to_include = {\n            'completed_passes': self.completed_passes,\n            'attempted_passes': self.attempted_passes,\n            'passing_yards': self.passing_yards,\n            'passing_touchdowns': self.passing_touchdowns,\n            'interceptions_thrown': self.interceptions_thrown,\n            'times_sacked': self.times_sacked,\n            'yards_lost_from_sacks': self.yards_lost_from_sacks,\n            'longest_pass': self.longest_pass,\n            'quarterback_rating': self.quarterback_rating,\n            'rush_attempts': self.rush_attempts,\n            'rush_yards': self.rush_yards,\n            'rush_touchdowns': self.rush_touchdowns,\n            'longest_rush': self.longest_rush,\n            'times_pass_target': self.times_pass_target,\n            'receptions': self.receptions,\n            'receiving_yards': self.receiving_yards,\n            'receiving_touchdowns': self.receiving_touchdowns,\n            'longest_reception': self.longest_reception,\n            'fumbles': self.fumbles,\n            'fumbles_lost': self.fumbles_lost,\n            'interceptions': self.interceptions,\n            'yards_returned_from_interception':\n            self.yards_returned_from_interception,\n            'interceptions_returned_for_touchdown':\n            self.interceptions_returned_for_touchdown,\n            'longest_interception_return': self.longest_interception_return,\n            'passes_defended': self.passes_defended,\n            'sacks': self.sacks,\n            'combined_tackles': self.combined_tackles,\n            'solo_tackles': self.solo_tackles,\n            'assists_on_tackles': self.assists_on_tackles,\n            'tackles_for_loss': self.tackles_for_loss,\n            'quarterback_hits': self.quarterback_hits,\n            'fumbles_recovered': self.fumbles_recovered,\n            'yards_recovered_from_fumble': self.yards_recovered_from_fumble,\n            'fumbles_recovered_for_touchdown':\n            self.fumbles_recovered_for_touchdown,\n            'fumbles_forced': self.fumbles_forced,\n            'kickoff_returns': self.kickoff_returns,\n            'kickoff_return_yards': self.kickoff_return_yards,\n            'average_kickoff_return_yards': self.average_kickoff_return_yards,\n            'kickoff_return_touchdown': self.kickoff_return_touchdown,\n            'longest_kickoff_return': self.longest_kickoff_return,\n            'punt_returns': self.punt_returns,\n            'punt_return_yards': self.punt_return_yards,\n            'yards_per_punt_return': self.yards_per_punt_return,\n            'punt_return_touchdown': self.punt_return_touchdown,\n            'longest_punt_return': self.longest_punt_return,\n            'extra_points_made': self.extra_points_made,\n            'extra_points_attempted': self.extra_points_attempted,\n            'field_goals_made': self.field_goals_made,\n            'field_goals_attempted': self.field_goals_attempted,\n            'punts': self.punts,\n            'total_punt_yards': self.total_punt_yards,\n            'yards_per_punt': self.yards_per_punt,\n            'longest_punt': self.longest_punt\n        }\n        return pd.DataFrame([fields_to_include], index=[self._player_id])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _retrieve_html_page(self, uri):\n        url = BOXSCORE_URL % uri\n        try:\n            url_data = pq(url)\n        except HTTPError:\n            return None\n        # For NFL, a 404 page doesn't actually raise a 404 error, so it needs\n        # to be manually checked.\n        if '404 error' in str(url_data):\n            return None\n        return pq(utils._remove_html_comment_tags(url_data))", "response": "Download the requested HTML page and return a pyquery object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse the game s date and location.", "response": "def _parse_game_date_and_location(self, boxscore):\n        \"\"\"\n        Retrieve the game's date and location.\n\n        The games' meta information, such as date, location, attendance, and\n        duration, follow a complex parsing scheme that changes based on the\n        layout of the page. The information should be able to be parsed and set\n        regardless of the order and how much information is included. To do\n        this, the meta information should be iterated through line-by-line and\n        fields should be determined by the values that are found in each line.\n\n        Parameters\n        ----------\n        boxscore : PyQuery object\n            A PyQuery object containing all of the HTML data from the boxscore.\n        \"\"\"\n        scheme = BOXSCORE_SCHEME[\"game_info\"]\n        items = [i.text() for i in boxscore(scheme).items()]\n        game_info = items[0].split('\\n')\n        attendance = None\n        date = None\n        duration = None\n        stadium = None\n        time = None\n        date = game_info[0]\n        for line in game_info:\n            if 'Attendance' in line:\n                attendance = line.replace('Attendance: ', '').replace(',', '')\n            if 'Time of Game' in line:\n                duration = line.replace('Time of Game: ', '')\n            if 'Stadium' in line:\n                stadium = line.replace('Stadium: ', '')\n            if 'Start Time' in line:\n                time = line.replace('Start Time: ', '')\n        setattr(self, '_attendance', attendance)\n        setattr(self, '_date', date)\n        setattr(self, '_duration', duration)\n        setattr(self, '_stadium', stadium)\n        setattr(self, '_time', time)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing the name of the requested tag.", "response": "def _parse_name(self, field, boxscore):\n        \"\"\"\n        Retrieve the team's complete name tag.\n\n        Both the team's full name (embedded in the tag's text) and the team's\n        abbreviation are stored in the name tag which can be used to parse\n        the winning and losing team's information.\n\n        Parameters\n        ----------\n        field : string\n            The name of the attribute to parse\n        boxscore : PyQuery object\n            A PyQuery object containing all of the HTML data from the boxscore.\n\n        Returns\n        -------\n        PyQuery object\n            The complete text for the requested tag.\n        \"\"\"\n        scheme = BOXSCORE_SCHEME[field]\n        return pq(str(boxscore(scheme)).strip())"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _find_boxscore_tables(self, boxscore):\n        tables = []\n        valid_tables = ['player_offense', 'player_defense', 'returns',\n                        'kicking']\n\n        for table in boxscore('table').items():\n            if table.attr['id'] in valid_tables:\n                tables.append(table)\n        return tables", "response": "Find all tables with boxscore information on the page and return the final list."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _find_home_or_away(self, row):\n        name = row('td[data-stat=\"team\"]').text().upper()\n        if name == self.home_abbreviation.upper():\n            return HOME\n        else:\n            return AWAY", "response": "Returns a string constant denoting whether the player is on the home or away team."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _find_players(self, boxscore):\n        player_dict = {}\n\n        tables = self._find_boxscore_tables(boxscore)\n        for table in tables:\n            player_dict = self._extract_player_stats(table, player_dict)\n        away_players, home_players = self._instantiate_players(player_dict)\n        return away_players, home_players", "response": "Find all players for each team and create a list of instances of BoxscorePlayer class for each team."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a pandas DataFrame containing all other class properties and values. The index for the DataFrame is the string URI that is used to instantiate the class such as 201702040nwe.", "response": "def dataframe(self):\n        \"\"\"\n        Returns a pandas DataFrame containing all other class properties and\n        values. The index for the DataFrame is the string URI that is used to\n        instantiate the class, such as '201802040nwe'.\n        \"\"\"\n        if self._away_points is None and self._home_points is None:\n            return None\n        fields_to_include = {\n            'attendance': self.attendance,\n            'away_first_downs': self.away_first_downs,\n            'away_fourth_down_attempts': self.away_fourth_down_attempts,\n            'away_fourth_down_conversions': self.away_fourth_down_conversions,\n            'away_fumbles': self.away_fumbles,\n            'away_fumbles_lost': self.away_fumbles_lost,\n            'away_interceptions': self.away_interceptions,\n            'away_net_pass_yards': self.away_net_pass_yards,\n            'away_pass_attempts': self.away_pass_attempts,\n            'away_pass_completions': self.away_pass_completions,\n            'away_pass_touchdowns': self.away_pass_touchdowns,\n            'away_pass_yards': self.away_pass_yards,\n            'away_penalties': self.away_penalties,\n            'away_points': self.away_points,\n            'away_rush_attempts': self.away_rush_attempts,\n            'away_rush_touchdowns': self.away_rush_touchdowns,\n            'away_rush_yards': self.away_rush_yards,\n            'away_third_down_attempts': self.away_third_down_attempts,\n            'away_third_down_conversions': self.away_third_down_conversions,\n            'away_time_of_possession': self.away_time_of_possession,\n            'away_times_sacked': self.away_times_sacked,\n            'away_total_yards': self.away_total_yards,\n            'away_turnovers': self.away_turnovers,\n            'away_yards_from_penalties': self.away_yards_from_penalties,\n            'away_yards_lost_from_sacks': self.away_yards_lost_from_sacks,\n            'date': self.date,\n            'duration': self.duration,\n            'home_first_downs': self.home_first_downs,\n            'home_fourth_down_attempts': self.home_fourth_down_attempts,\n            'home_fourth_down_conversions': self.home_fourth_down_conversions,\n            'home_fumbles': self.home_fumbles,\n            'home_fumbles_lost': self.home_fumbles_lost,\n            'home_interceptions': self.home_interceptions,\n            'home_net_pass_yards': self.home_net_pass_yards,\n            'home_pass_attempts': self.home_pass_attempts,\n            'home_pass_completions': self.home_pass_completions,\n            'home_pass_touchdowns': self.home_pass_touchdowns,\n            'home_pass_yards': self.home_pass_yards,\n            'home_penalties': self.home_penalties,\n            'home_points': self.home_points,\n            'home_rush_attempts': self.home_rush_attempts,\n            'home_rush_touchdowns': self.home_rush_touchdowns,\n            'home_rush_yards': self.home_rush_yards,\n            'home_third_down_attempts': self.home_third_down_attempts,\n            'home_third_down_conversions': self.home_third_down_conversions,\n            'home_time_of_possession': self.home_time_of_possession,\n            'home_times_sacked': self.home_times_sacked,\n            'home_total_yards': self.home_total_yards,\n            'home_turnovers': self.home_turnovers,\n            'home_yards_from_penalties': self.home_yards_from_penalties,\n            'home_yards_lost_from_sacks': self.home_yards_lost_from_sacks,\n            'losing_abbr': self.losing_abbr,\n            'losing_name': self.losing_name,\n            'stadium': self.stadium,\n            'time': self.time,\n            'winner': self.winner,\n            'winning_abbr': self.winning_abbr,\n            'winning_name': self.winning_name\n        }\n        return pd.DataFrame([fields_to_include], index=[self._uri])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef away_abbreviation(self):\n        abbr = re.sub(r'.*/teams/', '', str(self._away_name))\n        abbr = re.sub(r'/.*', '', abbr)\n        return abbr", "response": "Returns a string of the away team s abbreviation such as NWE."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a string of the home team s abbreviation such as KAN.", "response": "def home_abbreviation(self):\n        \"\"\"\n        Returns a ``string`` of the home team's abbreviation, such as 'KAN'.\n        \"\"\"\n        abbr = re.sub(r'.*/teams/', '', str(self._home_name))\n        abbr = re.sub(r'/.*', '', abbr)\n        return abbr"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_boxscore_uri(self, url):\n        uri = re.sub(r'.*/boxscores/', '', str(url))\n        uri = re.sub(r'\\.htm.*', '', uri).strip()\n        return uri", "response": "Find the boxscore URI for a game."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _find_games(self, week, year, end_week):\n        if not end_week or week > end_week:\n            end_week = week\n        while week <= end_week:\n            url = self._create_url(week, year)\n            page = self._get_requested_page(url)\n            games = page('table[class=\"teams\"]').items()\n            boxscores = self._extract_game_info(games)\n            timestamp = '%s-%s' % (week, year)\n            self._boxscores[timestamp] = boxscores\n            week += 1", "response": "Retrieve all major games played for a given week and year."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _parse_abbreviation(self, game_data):\n        name = game_data('td[data-stat=\"opp_name\"]:first')\n        name = re.sub(r'.*/teams/', '', str(name))\n        name = re.sub('/.*', '', name)\n        setattr(self, '_opponent_abbr', name)", "response": "Parses the opponent s abbreviation from their name."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a pandas DataFrame containing all other class properties and values. The index for the DataFrame is the boxscore string. The index for the DataFrame is the boxscore string.", "response": "def dataframe(self):\n        \"\"\"\n        Returns a pandas DataFrame containing all other class properties and\n        values. The index for the DataFrame is the boxscore string.\n        \"\"\"\n        if self._goals_scored is None and self._goals_allowed is None:\n            return None\n        fields_to_include = {\n            'boxscore_index': self.boxscore_index,\n            'date': self.date,\n            'datetime': self.datetime,\n            'game': self.game,\n            'goals_allowed': self.goals_allowed,\n            'goals_scored': self.goals_scored,\n            'location': self.location,\n            'opponent_abbr': self.opponent_abbr,\n            'opponent_name': self.opponent_name,\n            'overtime': self.overtime,\n            'penalties_in_minutes': self.penalties_in_minutes,\n            'power_play_goals': self.power_play_goals,\n            'power_play_opportunities': self.power_play_opportunities,\n            'result': self.result,\n            'short_handed_goals': self.short_handed_goals,\n            'shots_on_goal': self.shots_on_goal,\n            'opp_shots_on_goal': self.opp_shots_on_goal,\n            'opp_penalties_in_minutes': self.opp_penalties_in_minutes,\n            'opp_power_play_goals': self.opp_power_play_goals,\n            'opp_power_play_opportunities': self.opp_power_play_opportunities,\n            'opp_short_handed_goals': self.opp_short_handed_goals,\n            'corsi_for': self.corsi_for,\n            'corsi_against': self.corsi_against,\n            'corsi_for_percentage': self.corsi_for_percentage,\n            'fenwick_for': self.fenwick_for,\n            'fenwick_against': self.fenwick_against,\n            'fenwick_for_percentage': self.fenwick_for_percentage,\n            'faceoff_wins': self.faceoff_wins,\n            'faceoff_losses': self.faceoff_losses,\n            'faceoff_win_percentage': self.faceoff_win_percentage,\n            'offensive_zone_start_percentage':\n            self.offensive_zone_start_percentage,\n            'pdo': self.pdo\n        }\n        return pd.DataFrame([fields_to_include], index=[self._boxscore])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef result(self):\n        if self._result.lower() == 'w':\n            return WIN\n        if self._result.lower() == 'l' and \\\n           self.overtime != 0:\n            return OVERTIME_LOSS\n        return LOSS", "response": "Returns a string constant to indicate whether the team lost in the innermost in n."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn an integer of the number of overtimes that were played during the game.", "response": "def overtime(self):\n        \"\"\"\n        Returns an ``int`` of the number of overtimes that were played during\n        the game, or an int constant if the game went to a shootout.\n        \"\"\"\n        if self._overtime.lower() == 'ot':\n            return 1\n        if self._overtime.lower() == 'so':\n            return SHOOTOUT\n        if self._overtime == '':\n            return 0\n        num = re.findall(r'\\d+', self._overtime)\n        if len(num) > 0:\n            return num[0]\n        return 0"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dataframe(self):\n        frames = []\n        for game in self.__iter__():\n            df = game.dataframe\n            if df is not None:\n                frames.append(df)\n        if frames == []:\n            return None\n        return pd.concat(frames)", "response": "Returns a pandas DataFrame where each row is a representation of the\n            Game class. Rows are indexed by the boxscore string."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a pandas DataFrame containing all other class properties and their values. The index for the DataFrame is the boxscore string.", "response": "def dataframe(self):\n        \"\"\"\n        Returns a pandas DataFrame containing all other class properties and\n        values. The index for the DataFrame is the boxscore string.\n        \"\"\"\n        if self._points_for is None and self._points_against is None:\n            return None\n        fields_to_include = {\n            'arena': self.arena,\n            'boxscore_index': self.boxscore_index,\n            'date': self.date,\n            'datetime': self.datetime,\n            'game': self.game,\n            'location': self.location,\n            'opponent_abbr': self.opponent_abbr,\n            'opponent_conference': self.opponent_conference,\n            'opponent_name': self.opponent_name,\n            'opponent_rank': self.opponent_rank,\n            'overtimes': self.overtimes,\n            'points_against': self.points_against,\n            'points_for': self.points_for,\n            'result': self.result,\n            'season_losses': self.season_losses,\n            'season_wins': self.season_wins,\n            'streak': self.streak,\n            'time': self.time,\n            'type': self.type\n        }\n        return pd.DataFrame([fields_to_include], index=[self._boxscore])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a datetime object to indicate the month day year and time the requested game took place.", "response": "def datetime(self):\n        \"\"\"\n        Returns a datetime object to indicate the month, day, year, and time\n        the requested game took place.\n        \"\"\"\n        date_string = '%s %s' % (self._date, self._time.upper())\n        date_string = re.sub(r'/.*', '', date_string)\n        date_string = re.sub(r' ET', '', date_string)\n        date_string += 'M'\n        date_string = re.sub(r'PMM', 'PM', date_string, flags=re.IGNORECASE)\n        date_string = re.sub(r'AMM', 'AM', date_string, flags=re.IGNORECASE)\n        date_string = re.sub(r' PM', 'PM', date_string, flags=re.IGNORECASE)\n        date_string = re.sub(r' AM', 'AM', date_string, flags=re.IGNORECASE)\n        return datetime.strptime(date_string, '%a, %b %d, %Y %I:%M%p')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef type(self):\n        if self._type.lower() == 'reg':\n            return REGULAR_SEASON\n        if self._type.lower() == 'ctourn':\n            return CONFERENCE_TOURNAMENT\n        if self._type.lower() == 'ncaa':\n            return NCAA_TOURNAMENT\n        if self._type.lower() == 'nit':\n            return NIT_TOURNAMENT\n        if self._type.lower() == 'cbi':\n            return CBI_TOURNAMENT\n        if self._type.lower() == 'cit':\n            return CIT_TOURNAMENT", "response": "Returns a string constant to indicate whether the game was played at the given time."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a string constant to indicate whether the game was played at the home venue the opponent s venue the neutral site.", "response": "def location(self):\n        \"\"\"\n        Returns a ``string`` constant to indicate whether the game was played\n        at the team's home venue, the opponent's venue, or at a neutral site.\n        \"\"\"\n        if self._location == '':\n            return HOME\n        if self._location == 'N':\n            return NEUTRAL\n        if self._location == '@':\n            return AWAY"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef opponent_name(self):\n        name = re.sub(r'\\(\\d+\\)', '', self._opponent_name)\n        name = name.replace(u'\\xa0', '')\n        return name", "response": "Returns a string of the opponent s name such as Purdue\n        Boilermakers."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef opponent_rank(self):\n        rank = re.findall(r'\\d+', self._opponent_name)\n        if len(rank) > 0:\n            return int(rank[0])\n        return None", "response": "Returns a string of the opponent s rank when the game was played\n        and None if the team was unranked."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns an integer of the number of overtimes that were played during the game and 0 if the game finished at the end of regulation time.", "response": "def overtimes(self):\n        \"\"\"\n        Returns an ``int`` of the number of overtimes that were played during\n        the game and 0 if the game finished at the end of regulation time.\n        \"\"\"\n        if self._overtimes == '' or self._overtimes is None:\n            return 0\n        if self._overtimes.lower() == 'ot':\n            return 1\n        num_overtimes = re.findall(r'\\d+', self._overtimes)\n        try:\n            return int(num_overtimes[0])\n        except (ValueError, IndexError):\n            return 0"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _retrieve_html_page(self):\n        url = self._build_url()\n        try:\n            url_data = pq(url)\n        except HTTPError:\n            return None\n        return pq(utils._remove_html_comment_tags(url_data))", "response": "Download the requested page and return a pyquery object with the comment tags removed."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse the player s nationality.", "response": "def _parse_nationality(self, player_info):\n        \"\"\"\n        Parse the player's nationality.\n\n        The player's nationality is denoted by a flag in the information\n        section with a country code for each nation. The country code needs to\n        pulled and then matched to find the player's home country. Once found,\n        the '_nationality' attribute is set for the player.\n\n        Parameters\n        ----------\n        player_info : PyQuery object\n            A PyQuery object containing the HTML from the player's stats page.\n        \"\"\"\n        for span in player_info('span').items():\n            if 'class=\"f-i' in str(span):\n                nationality = span.text()\n                nationality = NATIONALITY[nationality]\n                setattr(self, '_nationality', nationality)\n                break"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses the years on the contract.", "response": "def _parse_contract_headers(self, table):\n        \"\"\"\n        Parse the years on the contract.\n\n        The years are listed as the headers on the contract. The first header\n        contains 'Team' which specifies the player's current team and should\n        not be included in the years.\n\n        Parameters\n        ----------\n        table : PyQuery object\n            A PyQuery object containing the contract table.\n\n        Returns\n        -------\n        list\n            Returns a list where each element is a string denoting the season,\n            such as '2017-18'.\n        \"\"\"\n        years = [i.text() for i in table('th').items()]\n        years.remove('Team')\n        return years"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses the wages on the contract table.", "response": "def _parse_contract_wages(self, table):\n        \"\"\"\n        Parse the wages on the contract.\n\n        The wages are listed as the data points in the contract table. Any\n        values that don't have a value which starts with a '$' sign are likely\n        not valid and should be dropped.\n\n        Parameters\n        ----------\n        table : PyQuery object\n            A PyQuery object containing the contract table.\n\n        Returns\n        -------\n        list\n            Returns a list of all wages where each element is a string denoting\n            the dollar amount, such as '$40,000,000'.\n        \"\"\"\n        wages = [i.text() if i.text().startswith('$') else ''\n                 for i in table('td').items()]\n        wages.remove('')\n        return wages"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _combine_contract(self, years, wages):\n        contract = {}\n\n        for i in range(len(years)):\n            contract[years[i]] = wages[i]\n        return contract", "response": "Combine the contract wages and year and add to a dictionary representing the player s contract."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _parse_contract(self, player_info):\n        tables = player_info('table').items()\n        for table in tables:\n            id_attr = table.attr('id')\n            if id_attr:\n                if id_attr.startswith('contracts_'):\n                    years = self._parse_contract_headers(table)\n                    wages = self._parse_contract_wages(table)\n                    contract = self._combine_contract(years, wages)\n                    # If the contract is empty, the player likely doesn't have\n                    # a contract and should have a value of None instead.\n                    if contract == {}:\n                        contract = None\n                    setattr(self, '_contract', contract)\n                    break", "response": "Parse the player s contract."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _find_initial_index(self):\n        index = 0\n        for season in self._season:\n            if season == 'Career':\n                self._index = index\n                break\n            index += 1", "response": "Find the index of the initial career stats."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _dataframe_fields(self):\n        fields_to_include = {\n            'and_ones': self.and_ones,\n            'assist_percentage': self.assist_percentage,\n            'assists': self.assists,\n            'block_percentage': self.block_percentage,\n            'blocking_fouls': self.blocking_fouls,\n            'blocks': self.blocks,\n            'box_plus_minus': self.box_plus_minus,\n            'center_percentage': self.center_percentage,\n            'defensive_box_plus_minus': self.defensive_box_plus_minus,\n            'defensive_rebound_percentage': self.defensive_rebound_percentage,\n            'defensive_rebounds': self.defensive_rebounds,\n            'defensive_win_shares': self.defensive_win_shares,\n            'dunks': self.dunks,\n            'effective_field_goal_percentage':\n            self.effective_field_goal_percentage,\n            'field_goal_attempts': self.field_goal_attempts,\n            'field_goal_perc_sixteen_foot_plus_two_pointers':\n            self.field_goal_perc_sixteen_foot_plus_two_pointers,\n            'field_goal_perc_ten_to_sixteen_feet':\n            self.field_goal_perc_ten_to_sixteen_feet,\n            'field_goal_perc_three_to_ten_feet':\n            self.field_goal_perc_three_to_ten_feet,\n            'field_goal_perc_zero_to_three_feet':\n            self.field_goal_perc_zero_to_three_feet,\n            'field_goal_percentage': self.field_goal_percentage,\n            'field_goals': self.field_goals,\n            'free_throw_attempt_rate': self.free_throw_attempt_rate,\n            'free_throw_attempts': self.free_throw_attempts,\n            'free_throw_percentage': self.free_throw_percentage,\n            'free_throws': self.free_throws,\n            'games_played': self.games_played,\n            'games_started': self.games_started,\n            'half_court_heaves': self.half_court_heaves,\n            'half_court_heaves_made': self.half_court_heaves_made,\n            'height': self.height,\n            'lost_ball_turnovers': self.lost_ball_turnovers,\n            'minutes_played': self.minutes_played,\n            'nationality': self.nationality,\n            'net_plus_minus': self.net_plus_minus,\n            'offensive_box_plus_minus': self.offensive_box_plus_minus,\n            'offensive_fouls': self.offensive_fouls,\n            'offensive_rebound_percentage': self.offensive_rebound_percentage,\n            'offensive_rebounds': self.offensive_rebounds,\n            'offensive_win_shares': self.offensive_win_shares,\n            'on_court_plus_minus': self.on_court_plus_minus,\n            'other_turnovers': self.other_turnovers,\n            'passing_turnovers': self.passing_turnovers,\n            'percentage_field_goals_as_dunks':\n            self.percentage_field_goals_as_dunks,\n            'percentage_of_three_pointers_from_corner':\n            self.percentage_of_three_pointers_from_corner,\n            'percentage_shots_three_pointers':\n            self.percentage_shots_three_pointers,\n            'percentage_shots_two_pointers':\n            self.percentage_shots_two_pointers,\n            'percentage_sixteen_foot_plus_two_pointers':\n            self.percentage_sixteen_foot_plus_two_pointers,\n            'percentage_ten_to_sixteen_footers':\n            self.percentage_ten_to_sixteen_footers,\n            'percentage_three_to_ten_footers':\n            self.percentage_three_to_ten_footers,\n            'percentage_zero_to_three_footers':\n            self.percentage_zero_to_three_footers,\n            'personal_fouls': self.personal_fouls,\n            'player_efficiency_rating': self.player_efficiency_rating,\n            'player_id': self.player_id,\n            'point_guard_percentage': self.point_guard_percentage,\n            'points': self.points,\n            'points_generated_by_assists': self.points_generated_by_assists,\n            'position': self.position,\n            'power_forward_percentage': self.power_forward_percentage,\n            'salary': self.salary,\n            'shooting_distance': self.shooting_distance,\n            'shooting_fouls': self.shooting_fouls,\n            'shooting_fouls_drawn': self.shooting_fouls_drawn,\n            'shooting_guard_percentage': self.shooting_guard_percentage,\n            'shots_blocked': self.shots_blocked,\n            'small_forward_percentage': self.small_forward_percentage,\n            'steal_percentage': self.steal_percentage,\n            'steals': self.steals,\n            'take_fouls': self.take_fouls,\n            'team_abbreviation': self.team_abbreviation,\n            'three_point_attempt_rate': self.three_point_attempt_rate,\n            'three_point_attempts': self.three_point_attempts,\n            'three_point_percentage': self.three_point_percentage,\n            'three_point_shot_percentage_from_corner':\n            self.three_point_shot_percentage_from_corner,\n            'three_pointers': self.three_pointers,\n            'three_pointers_assisted_percentage':\n            self.three_pointers_assisted_percentage,\n            'total_rebound_percentage': self.total_rebound_percentage,\n            'total_rebounds': self.total_rebounds,\n            'true_shooting_percentage': self.true_shooting_percentage,\n            'turnover_percentage': self.turnover_percentage,\n            'turnovers': self.turnovers,\n            'two_point_attempts': self.two_point_attempts,\n            'two_point_percentage': self.two_point_percentage,\n            'two_pointers': self.two_pointers,\n            'two_pointers_assisted_percentage':\n            self.two_pointers_assisted_percentage,\n            'usage_percentage': self.usage_percentage,\n            'value_over_replacement_player':\n            self.value_over_replacement_player,\n            'weight': self.weight,\n            'win_shares': self.win_shares,\n            'win_shares_per_48_minutes': self.win_shares_per_48_minutes\n        }\n        return fields_to_include", "response": "Returns a dictionary of all the fields to include with DataFrame."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a pandas DataFrame containing all other class properties and values.", "response": "def dataframe(self):\n        \"\"\"\n        Returns a pandas DataFrame containing all other class properties and\n        values. The index for the DataFrame is the string abbreviation of the\n        team, such as 'PURDUE'.\n        \"\"\"\n        fields_to_include = {\n            'abbreviation': self.abbreviation,\n            'conference': self.conference,\n            'conference_losses': self.conference_losses,\n            'conference_win_percentage': self.conference_win_percentage,\n            'conference_wins': self.conference_wins,\n            'first_downs': self.first_downs,\n            'first_downs_from_penalties': self.first_downs_from_penalties,\n            'fumbles_lost': self.fumbles_lost,\n            'games': self.games,\n            'interceptions': self.interceptions,\n            'losses': self.losses,\n            'name': self.name,\n            'pass_attempts': self.pass_attempts,\n            'pass_completion_percentage': self.pass_completion_percentage,\n            'pass_completions': self.pass_completions,\n            'pass_first_downs': self.pass_first_downs,\n            'pass_touchdowns': self.pass_touchdowns,\n            'pass_yards': self.pass_yards,\n            'penalties': self.penalties,\n            'plays': self.plays,\n            'points_against_per_game': self.points_against_per_game,\n            'points_per_game': self.points_per_game,\n            'rush_attempts': self.rush_attempts,\n            'rush_first_downs': self.rush_first_downs,\n            'rush_touchdowns': self.rush_touchdowns,\n            'rush_yards': self.rush_yards,\n            'rush_yards_per_attempt': self.rush_yards_per_attempt,\n            'simple_rating_system': self.simple_rating_system,\n            'strength_of_schedule': self.strength_of_schedule,\n            'turnovers': self.turnovers,\n            'win_percentage': self.win_percentage,\n            'wins': self.wins,\n            'yards': self.yards,\n            'yards_from_penalties': self.yards_from_penalties,\n            'yards_per_play': self.yards_per_play\n        }\n        return pd.DataFrame([fields_to_include], index=[self._abbreviation])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _retrieve_all_teams(self, year):\n        team_data_dict = {}\n\n        if not year:\n            year = utils._find_year_for_season('ncaaf')\n        doc = pq(SEASON_PAGE_URL % year)\n        teams_list = utils._get_stats_table(doc, 'div#div_standings')\n        offense_doc = pq(OFFENSIVE_STATS_URL % year)\n        offense_list = utils._get_stats_table(offense_doc, 'table#offense')\n        for stats_list in [teams_list, offense_list]:\n            team_data_dict = self._add_stats_data(stats_list, team_data_dict)\n\n        for team_name, team_data in team_data_dict.items():\n            team = Team(team_data['data'],\n                        self._conferences_dict[team_name.lower()],\n                        year)\n            self._teams.append(team)", "response": "Retrieves all teams in a given season."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _retrieve_html_page(self):\n        url = PLAYER_URL % self._player_id\n        try:\n            url_data = pq(url)\n        except HTTPError:\n            return None\n        return pq(utils._remove_html_comment_tags(url_data))", "response": "Download the requested player s stats page and return a pyquery object with the comment tags removed."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses the player s position.", "response": "def _parse_player_position(self, player_info):\n        \"\"\"\n        Parse the player's position.\n\n        The player's position isn't contained within a unique tag and the\n        player's meta information should be iterated through until 'Position'\n        is found as it contains the desired text.\n\n        Parameters\n        ----------\n        player_info : PyQuery object\n            A PyQuery object of the player's information on the HTML stats\n            page.\n        \"\"\"\n        for section in player_info('div#meta p').items():\n            if 'Position' in str(section):\n                position = section.text().replace('Position: ', '')\n                setattr(self, '_position', position)\n                break"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses the conference abbreviation for the player s team.", "response": "def _parse_conference(self, stats):\n        \"\"\"\n        Parse the conference abbreviation for the player's team.\n\n        The conference abbreviation is embedded within the conference name tag\n        and should be special-parsed to extract it.\n\n        Parameters\n        ----------\n        stats : PyQuery object\n            A PyQuery object containing the HTML from the player's stats page.\n\n        Returns\n        -------\n        string\n            Returns a string of the conference abbreviation, such as 'big-12'.\n        \"\"\"\n        conference_tag = stats(PLAYER_SCHEME['conference'])\n        conference = re.sub(r'.*/cbb/conferences/',\n                            '',\n                            str(conference_tag('a')))\n        conference = re.sub(r'/.*', '', conference)\n        return conference"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _parse_team_abbreviation(self, stats):\n        team_tag = stats(PLAYER_SCHEME['team_abbreviation'])\n        team = re.sub(r'.*/cbb/schools/', '', str(team_tag('a')))\n        team = re.sub(r'/.*', '', team)\n        return team", "response": "Parse the team abbreviation."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _dataframe_fields(self):\n        fields_to_include = {\n            'assist_percentage': self.assist_percentage,\n            'assists': self.assists,\n            'block_percentage': self.block_percentage,\n            'blocks': self.blocks,\n            'box_plus_minus': self.box_plus_minus,\n            'conference': self.conference,\n            'defensive_box_plus_minus': self.defensive_box_plus_minus,\n            'defensive_rebound_percentage': self.defensive_rebound_percentage,\n            'defensive_rebounds': self.defensive_rebounds,\n            'defensive_win_shares': self.defensive_win_shares,\n            'effective_field_goal_percentage':\n            self.effective_field_goal_percentage,\n            'field_goal_attempts': self.field_goal_attempts,\n            'field_goal_percentage': self.field_goal_percentage,\n            'field_goals': self.field_goals,\n            'free_throw_attempt_rate': self.free_throw_attempt_rate,\n            'free_throw_attempts': self.free_throw_attempts,\n            'free_throw_percentage': self.free_throw_percentage,\n            'free_throws': self.free_throws,\n            'games_played': self.games_played,\n            'games_started': self.games_started,\n            'height': self.height,\n            'minutes_played': self.minutes_played,\n            'offensive_box_plus_minus': self.offensive_box_plus_minus,\n            'offensive_rebound_percentage': self.offensive_rebound_percentage,\n            'offensive_rebounds': self.offensive_rebounds,\n            'offensive_win_shares': self.offensive_win_shares,\n            'personal_fouls': self.personal_fouls,\n            'player_efficiency_rating': self.player_efficiency_rating,\n            'player_id': self.player_id,\n            'points': self.points,\n            'points_produced': self.points_produced,\n            'position': self.position,\n            'steal_percentage': self.steal_percentage,\n            'steals': self.steals,\n            'team_abbreviation': self.team_abbreviation,\n            'three_point_attempt_rate': self.three_point_attempt_rate,\n            'three_point_attempts': self.three_point_attempts,\n            'three_point_percentage': self.three_point_percentage,\n            'three_pointers': self.three_pointers,\n            'total_rebound_percentage': self.total_rebound_percentage,\n            'total_rebounds': self.total_rebounds,\n            'true_shooting_percentage': self.true_shooting_percentage,\n            'turnover_percentage': self.turnover_percentage,\n            'turnovers': self.turnovers,\n            'two_point_attempts': self.two_point_attempts,\n            'two_point_percentage': self.two_point_percentage,\n            'two_pointers': self.two_pointers,\n            'usage_percentage': self.usage_percentage,\n            'weight': self.weight,\n            'win_shares': self.win_shares,\n            'win_shares_per_40_minutes': self.win_shares_per_40_minutes,\n        }\n        return fields_to_include", "response": "Returns a dictionary of all the fields to include with DataFrame."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\npulling the specified value from the HTML contents.", "response": "def _parse_value(self, stats, field):\n        \"\"\"\n        Pull the specified value from the HTML contents.\n\n        Given a field, find the corresponding HTML tag for that field and parse\n        its value before returning the value as a string.\n\n        Parameters\n        ----------\n        stats : PyQuery object\n            A PyQuery object containing all stats in HTML format for a\n            particular player.\n        field : string\n            A string of the field to parse from the HTML.\n\n        Returns\n        -------\n        string\n            Returns the desired value as a string.\n        \"\"\"\n        value = utils._parse_field(PLAYER_SCHEME, stats, field)\n        if not value and field in BOXSCORE_RETRY:\n            value = utils._parse_field(BOXSCORE_RETRY, stats, field)\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing all player information and set attributes.", "response": "def _parse_player_data(self, player_data):\n        \"\"\"\n        Parse all player information and set attributes.\n\n        Iterate through each class attribute to parse the data from the HTML\n        page and set the attribute value with the result.\n\n        Parameters\n        ----------\n        player_data : dictionary or string\n            If this class is inherited from the ``Player`` class, player_data\n            will be a dictionary where each key is a string representing the\n            season and each value contains the HTML data as a string. If this\n            class is inherited from the ``BoxscorePlayer`` class, player_data\n            will be a string representing the player's game statistics in HTML\n            format.\n        \"\"\"\n        for field in self.__dict__:\n            short_field = str(field)[1:]\n            if short_field == 'player_id' or \\\n               short_field == 'index' or \\\n               short_field == 'most_recent_season' or \\\n               short_field == 'name' or \\\n               short_field == 'weight' or \\\n               short_field == 'height' or \\\n               short_field == 'season':\n                continue\n            field_stats = []\n            if type(player_data) == dict:\n                for year, data in player_data.items():\n                    stats = pq(data['data'])\n                    value = self._parse_value(stats, short_field)\n                    field_stats.append(value)\n            else:\n                stats = pq(player_data)\n                value = self._parse_value(stats, short_field)\n                field_stats.append(value)\n            setattr(self, field, field_stats)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _parse_game_date_and_location(self, field, boxscore):\n        scheme = BOXSCORE_SCHEME[field]\n        items = [i.text() for i in boxscore(scheme).items()]\n        game_info = items[0].split('\\n')\n        if len(game_info) < 3 and field == 'location':\n            return None\n        return game_info[BOXSCORE_ELEMENT_INDEX[field]]", "response": "Parse the game s date and location."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses the team s full name tag.", "response": "def _parse_name(self, field, boxscore):\n        \"\"\"\n        Retrieve the team's complete name tag.\n\n        Both the team's full name (embedded in the tag's text) and the team's\n        abbreviation are stored in the name tag which can be used to parse\n        the winning and losing team's information.\n\n        Parameters\n        ----------\n        field : string\n            The name of the attribute to parse\n        boxscore : PyQuery object\n            A PyQuery object containing all of the HTML data from the boxscore.\n\n        Returns\n        -------\n        PyQuery object\n            The complete text for the requested tag.\n        \"\"\"\n        scheme = BOXSCORE_SCHEME[field]\n        name = boxscore(scheme)\n        if 'cbb/schools' not in str(name):\n            name = re.sub(r'.*name\">', '', str(name))\n            name = re.sub(r'<.*', '', str(name))\n        return name"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse each team s rank if applicable.", "response": "def _parse_ranking(self, field, boxscore):\n        \"\"\"\n        Parse each team's rank if applicable.\n\n        Retrieve the team's rank according to the rankings published each week.\n        The ranking for the week is only located in the scores section at\n        the top of the page and not in the actual boxscore information. The\n        rank is after the team name inside a parenthesis with a special\n        'pollrank' attribute. If this is not in the team's boxscore\n        information, the team is assumed to not have a rank and will return a\n        value of None.\n\n        Parameters\n        ----------\n        field : string\n            The name of the attribute to parse.\n        boxscore : PyQuery object\n            A PyQuery obejct containing all of the HTML data from the boxscore.\n\n        Returns\n        -------\n        int\n            An int representing the team's ranking or None if the team is not\n            ranked.\n        \"\"\"\n        ranking = None\n        index = BOXSCORE_ELEMENT_INDEX[field]\n        teams_boxscore = boxscore(BOXSCORE_SCHEME[field])\n        # Occasionally, the list of boxscores for the day won't be saved on the\n        # page. If that's the case, return the default ranking.\n        if str(teams_boxscore) == '':\n            return ranking\n        team = pq(teams_boxscore[index])\n        if 'pollrank' in str(team):\n            rank_str = re.findall(r'\\(\\d+\\)', str(team))\n            if len(rank_str) == 1:\n                ranking = int(rank_str[0].replace('(', '').replace(')', ''))\n        return ranking"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse each team s record.", "response": "def _parse_record(self, field, boxscore, index):\n        \"\"\"\n        Parse each team's record.\n\n        Find the record for both the home and away teams which are listed above\n        the basic boxscore stats tables. Depending on whether or not the\n        advanced stats table is included on the page (generally only for more\n        recent matchups), a blank header is added to the list which should be\n        removed. With all blank headers removed, the home and away team records\n        can be easily parsed by specifying which team is desired.\n\n        Parameters\n        ----------\n        field : string\n            The name of the attribute to parse.\n        boxscore : PyQuery object\n            A PyQuery object containing all of the HTML data from the boxscore.\n        index : int\n            An int of the index to pull the record from, as specified in the\n            BOXSCORE_ELEMENT_INDEX dictionary.\n\n        Returns\n        -------\n        string\n            A string of the team's record in the format 'Team Name (W-L)'.\n        \"\"\"\n        records = boxscore(BOXSCORE_SCHEME[field]).items()\n        records = [x.text() for x in records if x.text() != '']\n        return records[index]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dataframe(self):\n        if self._away_points is None and self._home_points is None:\n            return None\n        fields_to_include = {\n            'away_assist_percentage': self.away_assist_percentage,\n            'away_assists': self.away_assists,\n            'away_block_percentage': self.away_block_percentage,\n            'away_blocks': self.away_blocks,\n            'away_defensive_rating': self.away_defensive_rating,\n            'away_defensive_rebound_percentage':\n            self.away_defensive_rebound_percentage,\n            'away_defensive_rebounds': self.away_defensive_rebounds,\n            'away_effective_field_goal_percentage':\n            self.away_effective_field_goal_percentage,\n            'away_field_goal_attempts': self.away_field_goal_attempts,\n            'away_field_goal_percentage': self.away_field_goal_percentage,\n            'away_field_goals': self.away_field_goals,\n            'away_free_throw_attempt_rate': self.away_free_throw_attempt_rate,\n            'away_free_throw_attempts': self.away_free_throw_attempts,\n            'away_free_throw_percentage': self.away_free_throw_percentage,\n            'away_free_throws': self.away_free_throws,\n            'away_losses': self.away_losses,\n            'away_minutes_played': self.away_minutes_played,\n            'away_offensive_rating': self.away_offensive_rating,\n            'away_offensive_rebound_percentage':\n            self.away_offensive_rebound_percentage,\n            'away_offensive_rebounds': self.away_offensive_rebounds,\n            'away_personal_fouls': self.away_personal_fouls,\n            'away_points': self.away_points,\n            'away_ranking': self.away_ranking,\n            'away_steal_percentage': self.away_steal_percentage,\n            'away_steals': self.away_steals,\n            'away_three_point_attempt_rate':\n            self.away_three_point_attempt_rate,\n            'away_three_point_field_goal_attempts':\n            self.away_three_point_field_goal_attempts,\n            'away_three_point_field_goal_percentage':\n            self.away_three_point_field_goal_percentage,\n            'away_three_point_field_goals': self.away_three_point_field_goals,\n            'away_total_rebound_percentage':\n            self.away_total_rebound_percentage,\n            'away_total_rebounds': self.away_total_rebounds,\n            'away_true_shooting_percentage':\n            self.away_true_shooting_percentage,\n            'away_turnover_percentage': self.away_turnover_percentage,\n            'away_turnovers': self.away_turnovers,\n            'away_two_point_field_goal_attempts':\n            self.away_two_point_field_goal_attempts,\n            'away_two_point_field_goal_percentage':\n            self.away_two_point_field_goal_percentage,\n            'away_two_point_field_goals': self.away_two_point_field_goals,\n            'away_win_percentage': self.away_win_percentage,\n            'away_wins': self.away_wins,\n            'date': self.date,\n            'home_assist_percentage': self.home_assist_percentage,\n            'home_assists': self.home_assists,\n            'home_block_percentage': self.home_block_percentage,\n            'home_blocks': self.home_blocks,\n            'home_defensive_rating': self.home_defensive_rating,\n            'home_defensive_rebound_percentage':\n            self.home_defensive_rebound_percentage,\n            'home_defensive_rebounds': self.home_defensive_rebounds,\n            'home_effective_field_goal_percentage':\n            self.home_effective_field_goal_percentage,\n            'home_field_goal_attempts': self.home_field_goal_attempts,\n            'home_field_goal_percentage': self.home_field_goal_percentage,\n            'home_field_goals': self.home_field_goals,\n            'home_free_throw_attempt_rate': self.home_free_throw_attempt_rate,\n            'home_free_throw_attempts': self.home_free_throw_attempts,\n            'home_free_throw_percentage': self.home_free_throw_percentage,\n            'home_free_throws': self.home_free_throws,\n            'home_losses': self.home_losses,\n            'home_minutes_played': self.home_minutes_played,\n            'home_offensive_rating': self.home_offensive_rating,\n            'home_offensive_rebound_percentage':\n            self.home_offensive_rebound_percentage,\n            'home_offensive_rebounds': self.home_offensive_rebounds,\n            'home_personal_fouls': self.home_personal_fouls,\n            'home_points': self.home_points,\n            'home_ranking': self.home_ranking,\n            'home_steal_percentage': self.home_steal_percentage,\n            'home_steals': self.home_steals,\n            'home_three_point_attempt_rate':\n            self.home_three_point_attempt_rate,\n            'home_three_point_field_goal_attempts':\n            self.home_three_point_field_goal_attempts,\n            'home_three_point_field_goal_percentage':\n            self.home_three_point_field_goal_percentage,\n            'home_three_point_field_goals': self.home_three_point_field_goals,\n            'home_total_rebound_percentage':\n            self.home_total_rebound_percentage,\n            'home_total_rebounds': self.home_total_rebounds,\n            'home_true_shooting_percentage':\n            self.home_true_shooting_percentage,\n            'home_turnover_percentage': self.home_turnover_percentage,\n            'home_turnovers': self.home_turnovers,\n            'home_two_point_field_goal_attempts':\n            self.home_two_point_field_goal_attempts,\n            'home_two_point_field_goal_percentage':\n            self.home_two_point_field_goal_percentage,\n            'home_two_point_field_goals': self.home_two_point_field_goals,\n            'home_win_percentage': self.home_win_percentage,\n            'home_wins': self.home_wins,\n            'location': self.location,\n            'losing_abbr': self.losing_abbr,\n            'losing_name': self.losing_name,\n            'pace': self.pace,\n            'winner': self.winner,\n            'winning_abbr': self.winning_abbr,\n            'winning_name': self.winning_name\n        }\n        return pd.DataFrame([fields_to_include], index=[self._uri])", "response": "Returns a pandas DataFrame containing all other class properties and values."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a string of the winning team s name such as Purdue Boilermakers.", "response": "def winning_name(self):\n        \"\"\"\n        Returns a ``string`` of the winning team's name, such as 'Purdue\n        Boilermakers'.\n        \"\"\"\n        if self.winner == HOME:\n            if 'cbb/schools' not in str(self._home_name):\n                return str(self._home_name)\n            return self._home_name.text()\n        if 'cbb/schools' not in str(self._away_name):\n            return str(self._away_name)\n        return self._away_name.text()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a string of the losing team s name such as Indiana Hoosiers.", "response": "def losing_name(self):\n        \"\"\"\n        Returns a ``string`` of the losing team's name, such as 'Indiana'\n        Hoosiers'.\n        \"\"\"\n        if self.winner == HOME:\n            if 'cbb/schools' not in str(self._away_name):\n                return str(self._away_name)\n            return self._away_name.text()\n        if 'cbb/schools' not in str(self._home_name):\n            return str(self._home_name)\n        return self._home_name.text()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a float of the percentage of games the away team has won after the conclusion of the game. Percentage ranges from 0 - 1.", "response": "def away_win_percentage(self):\n        \"\"\"\n        Returns a ``float`` of the percentage of games the away team has won\n        after the conclusion of the game. Percentage ranges from 0-1.\n        \"\"\"\n        try:\n            result = float(self.away_wins) / \\\n                     float(self.away_wins + self.away_losses)\n            return round(result, 3)\n        except ZeroDivisionError:\n            return 0.0"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns an integer of the number of games the team has won after the conclusion of the game.", "response": "def away_wins(self):\n        \"\"\"\n        Returns an ``int`` of the number of games the team has won after the\n        conclusion of the game.\n        \"\"\"\n        try:\n            wins, losses = re.findall(r'\\d+', self._away_record)\n            return wins\n        except (ValueError, TypeError):\n            return 0"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef home_win_percentage(self):\n        try:\n            result = float(self.home_wins) / \\\n                     float(self.home_wins + self.home_losses)\n            return round(result, 3)\n        except ZeroDivisionError:\n            return 0.0", "response": "Returns a float of the percentage of games the home team has won\n            after the conclusion of the game. Percentage ranges from 0 - 1."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _parse_abbreviation(self, abbr):\n        if 'cbb/schools' not in str(abbr):\n            return None\n        abbr = re.sub(r'.*/schools/', '', str(abbr))\n        abbr = re.sub(r'/.*', '', abbr)\n        return abbr", "response": "Parse a team s abbreviation."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_name(self, name):\n        team_name = name.text()\n        abbr = self._parse_abbreviation(name)\n        non_di = False\n        if not abbr:\n            abbr = team_name\n            non_di = True\n        return team_name, abbr, non_di", "response": "Given a team s HTML name tag find its name abbreviation and whether or not they compete in Division - I."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the rank of the team when applicable.", "response": "def _get_rank(self, team):\n        \"\"\"\n        Find the team's rank when applicable.\n\n        If a team is ranked, it will showup in a separate <span> tag with the\n        actual rank embedded between parentheses. When a team is ranked, the\n        integer value representing their ranking should be returned. For teams\n        that are not ranked, None should be returned.\n\n        Parameters\n        ----------\n        team : PyQuery object\n            A PyQuery object of a team's HTML tag in the boxscore.\n\n        Returns\n        -------\n        int\n            Returns an integer representing the team's ranking when applicable,\n            or None if the team is not ranked.\n        \"\"\"\n        rank = None\n        rank_field = team('span[class=\"pollrank\"]')\n        if len(rank_field) > 0:\n            rank = re.findall(r'\\(\\d+\\)', str(rank_field))[0]\n            rank = int(rank.replace('(', '').replace(')', ''))\n        return rank"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind the names and abbreviations of both teams in a game.", "response": "def _get_team_names(self, game):\n        \"\"\"\n        Find the names and abbreviations for both teams in a game.\n\n        Using the HTML contents in a boxscore, find the name and abbreviation\n        for both teams and determine wether or not this is a matchup between\n        two Division-I teams.\n\n        Parameters\n        ----------\n        game : PyQuery object\n            A PyQuery object of a single boxscore containing information about\n            both teams.\n\n        Returns\n        -------\n        tuple\n            Returns a tuple containing the names and abbreviations of both\n            teams in the following order: Away Name, Away Abbreviation, Away\n            Score, Away Ranking, Home Name, Home Abbreviation, Home Score, Home\n            Ranking, a boolean which evaluates to True if either team does not\n            participate in Division-I athletics, and a boolean which evalutes\n            to True if either team is currently ranked.\n        \"\"\"\n        # Grab the first <td...> tag for each <tr> row in the boxscore,\n        # representing the name for each participating team.\n        links = [g('td:first') for g in game('tr').items()]\n        # The away team is the first link in the boxscore\n        away = links[0]\n        # The home team is the last (3rd) link in the boxscore\n        home = links[-1]\n        non_di = False\n        scores = re.findall(r'<td class=\"right\">\\d+</td>', str(game))\n        away_score = None\n        home_score = None\n        # If the game hasn't started or hasn't been updated on sports-reference\n        # yet, no score will be shown and therefore can't be parsed.\n        if len(scores) == 2:\n            away_score = self._get_score(scores[0])\n            home_score = self._get_score(scores[1])\n        away_name, away_abbr, away_non_di = self._get_name(away('a'))\n        home_name, home_abbr, home_non_di = self._get_name(home('a'))\n        non_di = away_non_di or home_non_di\n        away_rank = self._get_rank(away)\n        home_rank = self._get_rank(home)\n        top_25 = bool(away_rank or home_rank)\n        return (away_name, away_abbr, away_score, away_rank, home_name,\n                home_abbr, home_score, home_rank, non_di, top_25)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_team_results(self, away_name, away_abbr, away_score, home_name,\n                          home_abbr, home_score):\n        \"\"\"\n        Determine the winner and loser of the game.\n\n        If the game has been completed and sports-reference has been updated\n        with the score, determine the winner and loser and return their\n        respective names and abbreviations.\n\n        Parameters\n        ----------\n        away_name : string\n            The name of the away team, such as 'Indiana'.\n        away_abbr : string\n            The abbreviation of the away team, such as 'indiana'.\n        away_score : int\n            The number of points the away team scored, or None if the game\n            hasn't completed yet.\n        home_score : string\n            The name of the home team, such as 'Purdue'.\n        home_abbr : string\n            The abbreviation of the home team, such as 'purdue'.\n        home_score : int\n            The number of points the home team scored, or None if the game\n            hasn't completed yet.\n\n        Returns\n        -------\n        tuple, tuple\n            Returns two tuples, each containing the name followed by the\n            abbreviation of the winning and losing team, respectively. If the\n            game doesn't have a score associated with it yet, both tuples will\n            be None.\n        \"\"\"\n        if not away_score or not home_score:\n            return None, None\n        if away_score > home_score:\n            return (away_name, away_abbr), (home_name, home_abbr)\n        else:\n            return (home_name, home_abbr), (away_name, away_abbr)", "response": "Returns a tuple of the winner and loser team names and abbreviations."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing the game information from all boxscores and return the results in a list.", "response": "def _extract_game_info(self, games):\n        \"\"\"\n        Parse game information from all boxscores.\n\n        Find the major game information for all boxscores listed on a\n        particular boxscores webpage and return the results in a list.\n\n        Parameters\n        ----------\n        games : generator\n            A generator where each element points to a boxscore on the parsed\n            boxscores webpage.\n\n        Returns\n        -------\n        list\n            Returns a ``list`` of dictionaries where each dictionary contains\n            the name and abbreviations for both the home and away teams, a\n            boolean value indicating whether or not both teams compete in\n            Division-I, and a link to the boxscore.\n        \"\"\"\n        all_boxscores = []\n\n        for game in games:\n            names = self._get_team_names(game)\n            away_name, away_abbr, away_score, away_rank, home_name, \\\n                home_abbr, home_score, home_rank, non_di, top_25 = names\n            boxscore_url = game('td[class=\"right gamelink\"] a')\n            boxscore_uri = self._get_boxscore_uri(boxscore_url)\n            winning_name = None\n            winning_abbr = None\n            losing_name = None\n            losing_abbr = None\n            winner, loser = self._get_team_results(away_name, away_abbr,\n                                                   away_score, home_name,\n                                                   home_abbr, home_score)\n            if winner and loser:\n                winning_name, winning_abbr = winner\n                losing_name, losing_abbr = loser\n            game_info = {\n                'boxscore': boxscore_uri,\n                'away_name': away_name,\n                'away_abbr': away_abbr,\n                'away_score': away_score,\n                'away_rank': away_rank,\n                'home_name': home_name,\n                'home_abbr': home_abbr,\n                'home_score': home_score,\n                'home_rank': home_rank,\n                'non_di': non_di,\n                'top_25': top_25,\n                'winning_name': winning_name,\n                'winning_abbr': winning_abbr,\n                'losing_name': losing_name,\n                'losing_abbr': losing_abbr\n            }\n            all_boxscores.append(game_info)\n        return all_boxscores"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses a value for every attribute in the object.", "response": "def _parse_team_data(self, team_data):\n        \"\"\"\n        Parses a value for every attribute.\n\n        This function looks through every attribute and retrieves the value\n        according to the parsing scheme and index of the attribute from the\n        passed HTML data. Once the value is retrieved, the attribute's value is\n        updated with the returned result.\n\n        Note that this method is called directly once Team is invoked and does\n        not need to be called manually.\n\n        Parameters\n        ----------\n        team_data : string\n            A string containing all of the rows of stats for a given team. If\n            multiple tables are being referenced, this will be comprised of\n            multiple rows in a single string.\n        \"\"\"\n        for field in self.__dict__:\n            if field == '_year' or \\\n               field == '_team_conference':\n                continue\n            value = utils._parse_field(PARSING_SCHEME,\n                                       team_data,\n                                       # Remove the '_' from the name\n                                       str(field)[1:])\n            setattr(self, field, value)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dataframe(self):\n        fields_to_include = {\n            'abbreviation': self.abbreviation,\n            'assist_percentage': self.assist_percentage,\n            'assists': self.assists,\n            'away_losses': self.away_losses,\n            'away_wins': self.away_wins,\n            'block_percentage': self.block_percentage,\n            'blocks': self.blocks,\n            'conference': self.conference,\n            'conference_losses': self.conference_losses,\n            'conference_wins': self.conference_wins,\n            'defensive_rebounds': self.defensive_rebounds,\n            'effective_field_goal_percentage':\n            self.effective_field_goal_percentage,\n            'field_goal_attempts': self.field_goal_attempts,\n            'field_goal_percentage': self.field_goal_percentage,\n            'field_goals': self.field_goals,\n            'free_throw_attempt_rate': self.free_throw_attempt_rate,\n            'free_throw_attempts': self.free_throw_attempts,\n            'free_throw_percentage': self.free_throw_percentage,\n            'free_throws': self.free_throws,\n            'free_throws_per_field_goal_attempt':\n            self.free_throws_per_field_goal_attempt,\n            'games_played': self.games_played,\n            'home_losses': self.home_losses,\n            'home_wins': self.home_wins,\n            'losses': self.losses,\n            'minutes_played': self.minutes_played,\n            'name': self.name,\n            'net_rating': self.net_rating,\n            'offensive_rating': self.offensive_rating,\n            'offensive_rebound_percentage': self.offensive_rebound_percentage,\n            'offensive_rebounds': self.offensive_rebounds,\n            'opp_assist_percentage': self.opp_assist_percentage,\n            'opp_assists': self.opp_assists,\n            'opp_block_percentage': self.opp_block_percentage,\n            'opp_blocks': self.opp_blocks,\n            'opp_defensive_rebounds': self.opp_defensive_rebounds,\n            'opp_effective_field_goal_percentage':\n            self.opp_effective_field_goal_percentage,\n            'opp_field_goal_attempts': self.opp_field_goal_attempts,\n            'opp_field_goal_percentage': self.opp_field_goal_percentage,\n            'opp_field_goals': self.opp_field_goals,\n            'opp_free_throw_attempt_rate': self.opp_free_throw_attempt_rate,\n            'opp_free_throw_attempts': self.opp_free_throw_attempts,\n            'opp_free_throw_percentage': self.opp_free_throw_percentage,\n            'opp_free_throws': self.opp_free_throws,\n            'opp_free_throws_per_field_goal_attempt':\n            self.opp_free_throws_per_field_goal_attempt,\n            'opp_offensive_rating': self.opp_offensive_rating,\n            'opp_offensive_rebound_percentage':\n            self.opp_offensive_rebound_percentage,\n            'opp_offensive_rebounds': self.opp_offensive_rebounds,\n            'opp_personal_fouls': self.opp_personal_fouls,\n            'opp_points': self.opp_points,\n            'opp_steal_percentage': self.opp_steal_percentage,\n            'opp_steals': self.opp_steals,\n            'opp_three_point_attempt_rate': self.opp_three_point_attempt_rate,\n            'opp_three_point_field_goal_attempts':\n            self.opp_three_point_field_goal_attempts,\n            'opp_three_point_field_goal_percentage':\n            self.opp_three_point_field_goal_percentage,\n            'opp_three_point_field_goals': self.opp_three_point_field_goals,\n            'opp_two_point_field_goal_attempts':\n            self.opp_two_point_field_goal_attempts,\n            'opp_two_point_field_goal_percentage':\n            self.opp_two_point_field_goal_percentage,\n            'opp_two_point_field_goals': self.opp_two_point_field_goals,\n            'opp_total_rebound_percentage': self.opp_total_rebound_percentage,\n            'opp_total_rebounds': self.opp_total_rebounds,\n            'opp_true_shooting_percentage': self.opp_true_shooting_percentage,\n            'opp_turnover_percentage': self.opp_turnover_percentage,\n            'opp_turnovers': self.opp_turnovers,\n            'pace': self.pace,\n            'personal_fouls': self.personal_fouls,\n            'points': self.points,\n            'simple_rating_system': self.simple_rating_system,\n            'steal_percentage': self.steal_percentage,\n            'steals': self.steals,\n            'strength_of_schedule': self.strength_of_schedule,\n            'three_point_attempt_rate': self.three_point_attempt_rate,\n            'three_point_field_goal_attempts':\n            self.three_point_field_goal_attempts,\n            'three_point_field_goal_percentage':\n            self.three_point_field_goal_percentage,\n            'three_point_field_goals': self.three_point_field_goals,\n            'two_point_field_goal_attempts':\n            self.two_point_field_goal_attempts,\n            'two_point_field_goal_percentage':\n            self.two_point_field_goal_percentage,\n            'two_point_field_goals': self.two_point_field_goals,\n            'total_rebound_percentage': self.total_rebound_percentage,\n            'total_rebounds': self.total_rebounds,\n            'true_shooting_percentage': self.true_shooting_percentage,\n            'turnover_percentage': self.turnover_percentage,\n            'turnovers': self.turnovers,\n            'win_percentage': self.win_percentage,\n            'wins': self.wins\n        }\n        return pd.DataFrame([fields_to_include], index=[self._abbreviation])", "response": "Returns a pandas DataFrame containing all other class properties and values."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a float of the number of two point field goals made divided by the number of two point field goal attempts. Percentage ranges from 0 - 1.", "response": "def two_point_field_goal_percentage(self):\n        \"\"\"\n        Returns a ``float`` of the number of two point field goals made divided\n        by the number of two point field goal attempts. Percentage ranges from\n        0-1.\n        \"\"\"\n        try:\n            result = float(self.two_point_field_goals) / \\\n                float(self.two_point_field_goal_attempts)\n            return round(result, 3)\n        except ZeroDivisionError:\n            return 0.0"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a float of the percentage of two point field goals made divided by the number of two point field goal attempts by opponents. Percentage ranges from 0 - 1.", "response": "def opp_two_point_field_goal_percentage(self):\n        \"\"\"\n        Returns a ``float`` of the number of two point field goals made divided\n        by the number of two point field goal attempts by opponents. Percentage\n        ranges from 0-1.\n        \"\"\"\n        try:\n            result = float(self.opp_two_point_field_goals) / \\\n                float(self.opp_two_point_field_goal_attempts)\n            return round(result, 3)\n        except ZeroDivisionError:\n            return 0.0"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a team s stats row to a dictionary.", "response": "def _add_stats_data(self, teams_list, team_data_dict):\n        \"\"\"\n        Add a team's stats row to a dictionary.\n\n        Pass table contents and a stats dictionary of all teams to accumulate\n        all stats for each team in a single variable.\n\n        Parameters\n        ----------\n        teams_list : generator\n            A generator of all row items in a given table.\n        team_data_dict : {str: {'data': str}} dictionary\n            A dictionary where every key is the team's abbreviation and every\n            value is another dictionary with a 'data' key which contains the\n            string version of the row data for the matched team.\n\n        Returns\n        -------\n        dictionary\n            An updated version of the team_data_dict with the passed table row\n            information included.\n        \"\"\"\n        for team_data in teams_list:\n            if 'class=\"over_header thead\"' in str(team_data) or\\\n               'class=\"thead\"' in str(team_data):\n                continue\n            abbr = utils._parse_field(PARSING_SCHEME,\n                                      team_data,\n                                      'abbreviation')\n            try:\n                team_data_dict[abbr]['data'] += team_data\n            except KeyError:\n                team_data_dict[abbr] = {'data': team_data}\n        return team_data_dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _retrieve_all_teams(self, year):\n        team_data_dict = {}\n\n        if not year:\n            year = utils._find_year_for_season('ncaab')\n        doc = pq(BASIC_STATS_URL % year)\n        teams_list = utils._get_stats_table(doc, 'table#basic_school_stats')\n        doc = pq(BASIC_OPPONENT_STATS_URL % year)\n        opp_list = utils._get_stats_table(doc, 'table#basic_opp_stats')\n        doc = pq(ADVANCED_STATS_URL % year)\n        adv_teams_list = utils._get_stats_table(doc, 'table#adv_school_stats')\n        doc = pq(ADVANCED_OPPONENT_STATS_URL % year)\n        adv_opp_list = utils._get_stats_table(doc, 'table#adv_opp_stats')\n\n        for stats_list in [teams_list, opp_list, adv_teams_list, adv_opp_list]:\n            team_data_dict = self._add_stats_data(stats_list, team_data_dict)\n\n        for team_name, team_data in team_data_dict.items():\n            team = Team(team_data['data'],\n                        self._conferences_dict[team_name.lower()],\n                        year)\n            self._teams.append(team)", "response": "Retrieves all teams in a given season."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_team_abbreviation(self, team):\n        name_tag = team('th[data-stat=\"school_name\"] a')\n        team_abbreviation = re.sub(r'.*/cfb/schools/', '', str(name_tag))\n        team_abbreviation = re.sub(r'/.*', '', team_abbreviation)\n        return team_abbreviation", "response": "Retrieve team s abbreviation."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _find_conference_teams(self, conference_abbreviation, year):\n        if not year:\n            year = utils._find_year_for_season('ncaaf')\n        page = self._pull_conference_page(conference_abbreviation, year)\n        if not page:\n            url = CONFERENCE_URL % (conference_abbreviation, year)\n            output = (\"Can't pull requested conference page. Ensure the \"\n                      \"following URL exists: %s\" % url)\n            raise ValueError(output)\n        conference = page('table#standings tbody tr').items()\n        for team in conference:\n            team_abbreviation = self._get_team_abbreviation(team)\n            if team_abbreviation == '':\n                continue\n            team_name = team('th[data-stat=\"school_name\"]').text()\n            self._teams[team_abbreviation] = team_name", "response": "Find and retrieve the teams in a conference for the requested season."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_conference_id(self, conference):\n        name_tag = conference('td[data-stat=\"conf_name\"] a')\n        conference_id = re.sub(r'.*/cfb/conferences/', '', str(name_tag))\n        conference_id = re.sub(r'/.*', '', conference_id)\n        return conference_id", "response": "Get the conference id from the conference name tag."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _find_conferences(self, year):\n        if not year:\n            year = utils._find_year_for_season('ncaaf')\n        page = self._pull_conference_page(year)\n        if not page:\n            output = (\"Can't pull requested conference page. Ensure the \"\n                      \"following URL exists: %s\" % (CONFERENCES_URL % year))\n            raise ValueError(output)\n        conferences = page('table#conferences tbody tr').items()\n        for conference in conferences:\n            conference_abbreviation = self._get_conference_id(conference)\n            conference_name = conference('td[data-stat=\"conf_name\"]').text()\n            teams_dict = Conference(conference_abbreviation, year).teams\n            conference_dict = {\n                    'name': conference_name,\n                    'teams': teams_dict\n                }\n            for team in teams_dict.keys():\n                self._team_conference[team] = conference_abbreviation\n            self._conferences[conference_abbreviation] = conference_dict", "response": "Find and retrieve all of the conferences and teams for a given season."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dataframe(self):\n        if self._points_scored is None and self._points_allowed is None:\n            return None\n        fields_to_include = {\n            'boxscore_index': self.boxscore_index,\n            'date': self.date,\n            'datetime': self.datetime,\n            'day': self.day,\n            'extra_points_attempted': self.extra_points_attempted,\n            'extra_points_made': self.extra_points_made,\n            'field_goals_attempted': self.field_goals_attempted,\n            'field_goals_made': self.field_goals_made,\n            'fourth_down_attempts': self.fourth_down_attempts,\n            'fourth_down_conversions': self.fourth_down_conversions,\n            'interceptions': self.interceptions,\n            'location': self.location,\n            'opponent_abbr': self.opponent_abbr,\n            'opponent_name': self.opponent_name,\n            'overtime': self.overtime,\n            'pass_attempts': self.pass_attempts,\n            'pass_completion_rate': self.pass_completion_rate,\n            'pass_completions': self.pass_completions,\n            'pass_touchdowns': self.pass_touchdowns,\n            'pass_yards': self.pass_yards,\n            'pass_yards_per_attempt': self.pass_yards_per_attempt,\n            'points_allowed': self.points_allowed,\n            'points_scored': self.points_scored,\n            'punt_yards': self.punt_yards,\n            'punts': self.punts,\n            'quarterback_rating': self.quarterback_rating,\n            'result': self.result,\n            'rush_attempts': self.rush_attempts,\n            'rush_touchdowns': self.rush_touchdowns,\n            'rush_yards': self.rush_yards,\n            'rush_yards_per_attempt': self.rush_yards_per_attempt,\n            'third_down_attempts': self.third_down_attempts,\n            'third_down_conversions': self.third_down_conversions,\n            'time_of_possession': self.time_of_possession,\n            'times_sacked': self.times_sacked,\n            'type': self.type,\n            'week': self.week,\n            'yards_lost_from_sacks': self.yards_lost_from_sacks\n        }\n        return pd.DataFrame([fields_to_include], index=[self._boxscore])", "response": "Returns a pandas DataFrame containing all other class properties and values."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn an integer that represents the week of the assessment.", "response": "def week(self):\n        \"\"\"\n        Returns an ``int`` of the week number in the season, such as 1 for the\n        first week of the regular season.\n        \"\"\"\n        if self._week.lower() == 'wild card':\n            return WILD_CARD\n        if self._week.lower() == 'division':\n            return DIVISION\n        if self._week.lower() == 'conf. champ.':\n            return CONF_CHAMPIONSHIP\n        if self._week.lower() == 'superbowl':\n            return SUPER_BOWL\n        return self._week"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a datetime object representing the date the game was played.", "response": "def datetime(self):\n        \"\"\"\n        Returns a datetime object representing the date the game was played.\n        \"\"\"\n        date_string = '%s %s %s' % (self._day,\n                                    self._date,\n                                    self._year)\n        return datetime.strptime(date_string, '%a %B %d %Y')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _add_games_to_schedule(self, schedule, game_type, year):\n        for item in schedule:\n            game = Game(item, game_type, year)\n            self._games.append(game)", "response": "Add games instances to the schedule."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _pull_schedule(self, abbreviation, year):\n        if not year:\n            year = utils._find_year_for_season('nfl')\n        doc = pq(SCHEDULE_URL % (abbreviation.lower(), year))\n        schedule = utils._get_stats_table(doc, 'table#gamelog%s' % year)\n        self._add_games_to_schedule(schedule, REGULAR_SEASON, year)\n        if 'playoff_gamelog%s' % year in str(doc):\n            playoffs = utils._get_stats_table(doc,\n                                              'table#playoff_gamelog%s' % year)\n            self._add_games_to_schedule(playoffs, POST_SEASON, year)", "response": "Download and create objects for the team s schedule page and add them to the _games property."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a pandas DataFrame containing all relevant class properties and values for the specified game.", "response": "def dataframe(self):\n        \"\"\"\n        Returns a ``pandas DataFrame`` containing all other relevant class\n        properties and values for the specified game.\n        \"\"\"\n        fields_to_include = {\n            'assist_percentage': self.assist_percentage,\n            'assists': self.assists,\n            'block_percentage': self.block_percentage,\n            'blocks': self.blocks,\n            'box_plus_minus': self.box_plus_minus,\n            'defensive_rating': self.defensive_rating,\n            'defensive_rebound_percentage': self.defensive_rebound_percentage,\n            'defensive_rebounds': self.defensive_rebounds,\n            'effective_field_goal_percentage':\n            self.effective_field_goal_percentage,\n            'field_goal_attempts': self.field_goal_attempts,\n            'field_goal_percentage': self.field_goal_percentage,\n            'field_goals': self.field_goals,\n            'free_throw_attempt_rate': self.free_throw_attempt_rate,\n            'free_throw_attempts': self.free_throw_attempts,\n            'free_throw_percentage': self.free_throw_percentage,\n            'free_throws': self.free_throws,\n            'minutes_played': self.minutes_played,\n            'offensive_rating': self.offensive_rating,\n            'offensive_rebound_percentage': self.offensive_rebound_percentage,\n            'offensive_rebounds': self.offensive_rebounds,\n            'personal_fouls': self.personal_fouls,\n            'points': self.points,\n            'steal_percentage': self.steal_percentage,\n            'steals': self.steals,\n            'three_point_attempt_rate': self.three_point_attempt_rate,\n            'three_point_attempts': self.three_point_attempts,\n            'three_point_percentage': self.three_point_percentage,\n            'three_pointers': self.three_pointers,\n            'total_rebound_percentage': self.total_rebound_percentage,\n            'total_rebounds': self.total_rebounds,\n            'true_shooting_percentage': self.true_shooting_percentage,\n            'turnover_percentage': self.turnover_percentage,\n            'turnovers': self.turnovers,\n            'two_point_attempts': self.two_point_attempts,\n            'two_point_percentage': self.two_point_percentage,\n            'two_pointers': self.two_pointers,\n            'usage_percentage': self.usage_percentage\n        }\n        return pd.DataFrame([fields_to_include], index=[self._player_id])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the number of game minutes the player was on the court for.", "response": "def minutes_played(self):\n        \"\"\"\n        Returns a ``float`` of the number of game minutes the player was on the\n        court for.\n        \"\"\"\n        if self._minutes_played[self._index]:\n            minutes, seconds = self._minutes_played[self._index].split(':')\n            minutes = float(minutes) + float(seconds) / 60\n            return float(minutes)\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef two_pointers(self):\n        if self.field_goals and self.three_pointers:\n            return int(self.field_goals - self.three_pointers)\n        # Occurs when the player didn't make any three pointers, so the number\n        # of two pointers the player made is equal to the total number of field\n        # goals the player made.\n        if self.field_goals:\n            return int(self.field_goals)\n        # If the player didn't make any shots, they didn't make any two point\n        # field goals.\n        return None", "response": "Returns an integer of the total number of two point field goals the player made."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef two_point_attempts(self):\n        if self.field_goal_attempts and self.three_point_attempts:\n            return int(self.field_goal_attempts - self.three_point_attempts)\n        # Occurs when the player didn't take any three pointers, so the number\n        # of two pointers the player took is equal to the total number of field\n        # goals the player took.\n        if self.field_goal_attempts:\n            return int(self.field_goal_attempts)\n        # If the player didn't take any shots, they didn't take any two point\n        # field goals.\n        return None", "response": "Returns an integer of the total number of two point field goals the player attempted during the season."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef two_point_percentage(self):\n        if self.two_pointers and self.two_point_attempts:\n            perc = float(self.two_pointers) / float(self.two_point_attempts)\n            return round(perc, 3)\n        # Occurs when the player didn't make and two pointers.\n        if self.two_point_attempts:\n            return 0.0\n        return None", "response": "Returns a float of the player s two point field goal percentage during the season. Percentage ranges from 0 - 1."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfind all tables with boxscore information on the page.", "response": "def _find_boxscore_tables(self, boxscore):\n        \"\"\"\n        Find all tables with boxscore information on the page.\n\n        Iterate through all tables on the page and see if any of them are\n        boxscore pages by checking if the ID is prefixed with 'box_'. If so,\n        add it to a list and return the final list at the end.\n\n        Parameters\n        ----------\n        boxscore : PyQuery object\n            A PyQuery object containing all of the HTML data from the boxscore.\n\n        Returns\n        -------\n        list\n            Returns a ``list`` of the PyQuery objects where each object\n            represents a boxscore table.\n        \"\"\"\n        tables = []\n\n        for table in boxscore('table').items():\n            try:\n                if 'box_' in table.attr['id']:\n                    tables.append(table)\n            except (KeyError, TypeError):\n                continue\n        return tables"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse a value for every attribute. This function looks through every attribute and retrieves the value according to the parsing scheme and index of the attribute from the passed HTML data. Once the value is retrieved, the attribute's value is updated with the returned result. Note that this method is called directly once Boxscore is invoked and does not need to be called manually. Parameters ---------- uri : string The relative link to the boxscore HTML page, such as '201710310LAL'.", "response": "def _parse_game_data(self, uri):\n        \"\"\"\n        Parses a value for every attribute.\n\n        This function looks through every attribute and retrieves the value\n        according to the parsing scheme and index of the attribute from the\n        passed HTML data. Once the value is retrieved, the attribute's value is\n        updated with the returned result.\n\n        Note that this method is called directly once Boxscore is invoked and\n        does not need to be called manually.\n\n        Parameters\n        ----------\n        uri : string\n            The relative link to the boxscore HTML page, such as\n            '201710310LAL'.\n        \"\"\"\n        boxscore = self._retrieve_html_page(uri)\n        # If the boxscore is None, the game likely hasn't been played yet and\n        # no information can be gathered. As there is nothing to grab, the\n        # class instance should just be empty.\n        if not boxscore:\n            return\n\n        for field in self.__dict__:\n            # Remove the '_' from the name\n            short_field = str(field)[1:]\n            if short_field == 'winner' or \\\n               short_field == 'uri':\n                continue\n            if short_field == 'location' or \\\n               short_field == 'date':\n                value = self._parse_game_date_and_location(short_field,\n                                                           boxscore)\n                setattr(self, field, value)\n                continue\n            if short_field == 'away_name' or \\\n               short_field == 'home_name':\n                value = self._parse_name(short_field, boxscore)\n                setattr(self, field, value)\n                continue\n            index = 0\n            if short_field in BOXSCORE_ELEMENT_INDEX.keys():\n                index = BOXSCORE_ELEMENT_INDEX[short_field]\n            value = utils._parse_field(BOXSCORE_SCHEME,\n                                       boxscore,\n                                       short_field,\n                                       index)\n            setattr(self, field, value)\n        self._away_players, self._home_players = self._find_players(boxscore)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a float of the number of two point field goals made divided by the number of two point field goal attempts by the away team. Percentage ranges from 0 - 1.", "response": "def away_two_point_field_goal_percentage(self):\n        \"\"\"\n        Returns a ``float`` of the number of two point field goals made divided\n        by the number of two point field goal attempts by the away team.\n        Percentage ranges from 0-1.\n        \"\"\"\n        result = float(self.away_two_point_field_goals) / \\\n            float(self.away_two_point_field_goal_attempts)\n        return round(float(result), 3)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn an integer of the number of games the home team won after the game conclusion of the game.", "response": "def home_wins(self):\n        \"\"\"\n        Returns an ``int`` of the number of games the home team won after the\n        conclusion of the game.\n        \"\"\"\n        try:\n            wins, losses = re.findall(r'\\d+', self._home_record)\n            return wins\n        except ValueError:\n            return 0"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef home_two_point_field_goal_percentage(self):\n        result = float(self.home_two_point_field_goals) / \\\n            float(self.home_two_point_field_goal_attempts)\n        return round(float(result), 3)", "response": "Returns a float of the number of two point field goals made divided by the number of two point field goal attempts by the home team. Percentage ranges from 0 - 1."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a pandas DataFrame containing all other class properties and values.", "response": "def dataframe(self):\n        \"\"\"\n        Returns a pandas DataFrame containing all other class properties and\n        values. The index for the DataFrame is the string abbreviation of the\n        team, such as 'KAN'.\n        \"\"\"\n        fields_to_include = {\n            'abbreviation': self.abbreviation,\n            'defensive_simple_rating_system':\n            self.defensive_simple_rating_system,\n            'first_downs': self.first_downs,\n            'first_downs_from_penalties': self.first_downs_from_penalties,\n            'fumbles': self.fumbles,\n            'games_played': self.games_played,\n            'interceptions': self.interceptions,\n            'losses': self.losses,\n            'margin_of_victory': self.margin_of_victory,\n            'name': self.name,\n            'offensive_simple_rating_system':\n            self.offensive_simple_rating_system,\n            'pass_attempts': self.pass_attempts,\n            'pass_completions': self.pass_completions,\n            'pass_first_downs': self.pass_first_downs,\n            'pass_net_yards_per_attempt': self.pass_net_yards_per_attempt,\n            'pass_touchdowns': self.pass_touchdowns,\n            'pass_yards': self.pass_yards,\n            'penalties': self.penalties,\n            'percent_drives_with_points': self.percent_drives_with_points,\n            'percent_drives_with_turnovers':\n            self.percent_drives_with_turnovers,\n            'plays': self.plays,\n            'points_against': self.points_against,\n            'points_contributed_by_offense':\n            self.points_contributed_by_offense,\n            'points_difference': self.points_difference,\n            'points_for': self.points_for,\n            'rank': self.rank,\n            'rush_attempts': self.rush_attempts,\n            'rush_first_downs': self.rush_first_downs,\n            'rush_touchdowns': self.rush_touchdowns,\n            'rush_yards': self.rush_yards,\n            'rush_yards_per_attempt': self.rush_yards_per_attempt,\n            'simple_rating_system': self.simple_rating_system,\n            'strength_of_schedule': self.strength_of_schedule,\n            'turnovers': self.turnovers,\n            'win_percentage': self.win_percentage,\n            'wins': self.wins,\n            'yards': self.yards,\n            'yards_from_penalties': self.yards_from_penalties,\n            'yards_per_play': self.yards_per_play\n        }\n        return pd.DataFrame([fields_to_include], index=[self._abbreviation])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing the team name in the contract table.", "response": "def _parse_team_name(self, team):\n        \"\"\"\n        Parse the team name in the contract table.\n\n        The team names in the contract table contain special encoded characters\n        that are not supported by Python 2.7. These characters should be\n        filtered out to get the proper team name.\n\n        Parameters\n        ----------\n        team : string\n            A string representing the team_name tag in a row in the player's\n            contract table.\n\n        Returns\n        -------\n        string\n            A string of the team's name, such as 'Houston Astros'.\n        \"\"\"\n        team = team.replace('&#160;', ' ')\n        team = team.replace('\\xa0', ' ')\n        team_html = pq(team)\n        return team_html.text()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _parse_contract(self, player_info):\n        contract = {}\n\n        salary_table = player_info('table#br-salaries')\n        for row in salary_table('tbody tr').items():\n            if 'class=\"spacer partial_table\"' in str(row):\n                continue\n            year = row('th[data-stat=\"year_ID\"]').text()\n            if year.strip() == '':\n                continue\n            age = row('td[data-stat=\"age\"]').text()\n            team = self._parse_team_name(str(row('td[data-stat=\"team_name\"]')))\n            salary = row('td[data-stat=\"Salary\"]').text()\n            contract[year] = {\n                'age': age,\n                'team': team,\n                'salary': salary\n            }\n        setattr(self, '_contract', contract)", "response": "Parse the player s contract."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing the HTML table to find the requested field s value.", "response": "def _parse_value(self, html_data, field):\n        \"\"\"\n        Parse the HTML table to find the requested field's value.\n\n        All of the values are passed in an HTML table row instead of as\n        individual items. The values need to be parsed by matching the\n        requested attribute with a parsing scheme that sports-reference uses\n        to differentiate stats. This function returns a single value for the\n        given attribute.\n\n        Parameters\n        ----------\n        html_data : string\n            A string containing all of the rows of stats for a given team. If\n            multiple tables are being referenced, this will be comprised of\n            multiple rows in a single string.\n        field : string\n            The name of the attribute to match. Field must be a key in the\n            PLAYER_SCHEME dictionary.\n\n        Returns\n        -------\n        list\n            A list of all values that match the requested field. If no value\n            could be found, returns None.\n        \"\"\"\n        scheme = PLAYER_SCHEME[field]\n        items = [i.text() for i in html_data(scheme).items()]\n        # Stats can be added and removed on a yearly basis. If no stats are\n        # found, return None and have that be the value.\n        if len(items) == 0:\n            return None\n        return items"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a dictionary of all the fields to include with DataFrame.", "response": "def _dataframe_fields(self):\n        \"\"\"\n        Creates a dictionary of all fields to include with DataFrame.\n\n        With the result of the calls to class properties changing based on the\n        class index value, the dictionary should be regenerated every time the\n        index is changed when the dataframe property is requested.\n\n        Returns\n        -------\n        dictionary\n            Returns a dictionary where the keys are the shortened ``string``\n            attribute names and the values are the actual value for each\n            attribute for the specified index.\n        \"\"\"\n        fields_to_include = {\n            'assists': self.assists,\n            'at_bats': self.at_bats,\n            'bases_on_balls': self.bases_on_balls,\n            'batting_average': self.batting_average,\n            'birth_date': self.birth_date,\n            'complete_games': self.complete_games,\n            'defensive_chances': self.defensive_chances,\n            'defensive_runs_saved_above_average':\n            self.defensive_runs_saved_above_average,\n            'defensive_runs_saved_above_average_per_innings':\n            self.defensive_runs_saved_above_average_per_innings,\n            'double_plays_turned': self.double_plays_turned,\n            'doubles': self.doubles,\n            'errors': self.errors,\n            'fielding_percentage': self.fielding_percentage,\n            'games': self.games,\n            'games_catcher': self.games_catcher,\n            'games_center_fielder': self.games_center_fielder,\n            'games_designated_hitter': self.games_designated_hitter,\n            'games_first_baseman': self.games_first_baseman,\n            'games_in_batting_order': self.games_in_batting_order,\n            'games_in_defensive_lineup': self.games_in_defensive_lineup,\n            'games_left_fielder': self.games_left_fielder,\n            'games_outfielder': self.games_outfielder,\n            'games_pinch_hitter': self.games_pinch_hitter,\n            'games_pinch_runner': self.games_pinch_runner,\n            'games_pitcher': self.games_pitcher,\n            'games_right_fielder': self.games_right_fielder,\n            'games_second_baseman': self.games_second_baseman,\n            'games_shortstop': self.games_shortstop,\n            'games_started': self.games_started,\n            'games_third_baseman': self.games_third_baseman,\n            'grounded_into_double_plays': self.grounded_into_double_plays,\n            'height': self.height,\n            'hits': self.hits,\n            'home_runs': self.home_runs,\n            'innings_played': self.innings_played,\n            'intentional_bases_on_balls': self.intentional_bases_on_balls,\n            'league_fielding_percentage': self.league_fielding_percentage,\n            'league_range_factor_per_game': self.league_range_factor_per_game,\n            'league_range_factor_per_nine_innings':\n            self.league_range_factor_per_nine_innings,\n            'name': self.name,\n            'nationality': self.nationality,\n            'on_base_percentage': self.on_base_percentage,\n            'on_base_plus_slugging_percentage':\n            self.on_base_plus_slugging_percentage,\n            'on_base_plus_slugging_percentage_plus':\n            self.on_base_plus_slugging_percentage_plus,\n            'plate_appearances': self.plate_appearances,\n            'player_id': self.player_id,\n            'position': self.position,\n            'putouts': self.putouts,\n            'range_factor_per_game': self.range_factor_per_game,\n            'range_factor_per_nine_innings':\n            self.range_factor_per_nine_innings,\n            'runs': self.runs,\n            'runs_batted_in': self.runs_batted_in,\n            'sacrifice_flies': self.sacrifice_flies,\n            'sacrifice_hits': self.sacrifice_hits,\n            'season': self.season,\n            'slugging_percentage': self.slugging_percentage,\n            'stolen_bases': self.stolen_bases,\n            'team_abbreviation': self.team_abbreviation,\n            'times_caught_stealing': self.times_caught_stealing,\n            'times_hit_by_pitch': self.times_hit_by_pitch,\n            'times_struck_out': self.times_struck_out,\n            'total_bases': self.total_bases,\n            'total_fielding_runs_above_average':\n            self.total_fielding_runs_above_average,\n            'total_fielding_runs_above_average_per_innings':\n            self.total_fielding_runs_above_average_per_innings,\n            'triples': self.triples,\n            'weight': self.weight,\n            # Properties specific to pitchers\n            'balks': self.balks,\n            'bases_on_balls_given': self.bases_on_balls_given,\n            'bases_on_balls_given_per_nine_innings':\n            self.bases_on_balls_given_per_nine_innings,\n            'batters_faced': self.batters_faced,\n            'batters_struckout_per_nine_innings':\n            self.batters_struckout_per_nine_innings,\n            'earned_runs_allowed': self.earned_runs_allowed,\n            'era': self.era,\n            'era_plus': self.era_plus,\n            'fielding_independent_pitching':\n            self.fielding_independent_pitching,\n            'games_finished': self.games_finished,\n            'hits_against_per_nine_innings':\n            self.hits_against_per_nine_innings,\n            'hits_allowed': self.hits_allowed,\n            'home_runs_against_per_nine_innings':\n            self.home_runs_against_per_nine_innings,\n            'home_runs_allowed': self.home_runs_allowed,\n            'intentional_bases_on_balls_given':\n            self.intentional_bases_on_balls_given,\n            'losses': self.losses,\n            'runs_allowed': self.runs_allowed,\n            'saves': self.saves,\n            'shutouts': self.shutouts,\n            'strikeouts': self.strikeouts,\n            'strikeouts_thrown_per_walk': self.strikeouts_thrown_per_walk,\n            'times_hit_player': self.times_hit_player,\n            'whip': self.whip,\n            'wild_pitches': self.wild_pitches,\n            'win_percentage': self.win_percentage,\n            'wins': self.wins\n        }\n        return fields_to_include"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse the player ID and return it as a string.", "response": "def _get_id(self, player):\n        \"\"\"\n        Parse the player ID.\n\n        Given a PyQuery object representing a single player on the team roster,\n        parse the player ID and return it as a string.\n\n        Parameters\n        ----------\n        player : PyQuery object\n            A PyQuery object representing the player information from the\n            roster table.\n\n        Returns\n        -------\n        string\n            Returns a string of the player ID.\n        \"\"\"\n        name_tag = player('td[data-stat=\"player\"] a')\n        name = re.sub(r'.*/players/./', '', str(name_tag))\n        return re.sub(r'\\.shtml.*', '', name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _find_players(self, year):\n        if not year:\n            year = utils._find_year_for_season('mlb')\n        url = self._create_url(year)\n        page = self._pull_team_page(url)\n        if not page:\n            output = (\"Can't pull requested team page. Ensure the following \"\n                      \"URL exists: %s\" % url)\n            raise ValueError(output)\n        players = page('table#team_batting tbody tr').items()\n        players_parsed = []\n        for player in players:\n            if 'class=\"thead\"' in str(player):\n                continue\n            player_id = self._get_id(player)\n            if self._slim:\n                name = self._get_name(player)\n                self._players[player_id] = name\n            else:\n                player_instance = Player(player_id)\n                self._players.append(player_instance)\n            players_parsed.append(player_id)\n        for player in page('table#team_pitching tbody tr').items():\n            if 'class=\"thead\"' in str(player):\n                continue\n            player_id = self._get_id(player)\n            # Skip players that showup in both batting and pitching tables, as\n            # is often the case with National League pitchers.\n            if player_id in players_parsed:\n                continue\n            if self._slim:\n                name = self._get_name(player)\n                self._players[player_id] = name\n            else:\n                player_instance = Player(player_id)\n                self._players.append(player_instance)", "response": "Find all players on the requested team and year."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a pandas DataFrame containing all relevant properties and values for the specified game.", "response": "def dataframe(self):\n        \"\"\"\n        Returns a ``pandas DataFrame`` containing all other relevant\n        properties and values for the specified game.\n        \"\"\"\n        fields_to_include = {\n            'assists': self.assists,\n            'blocks_at_even_strength': self.blocks_at_even_strength,\n            'corsi_for_percentage': self.corsi_for_percentage,\n            'decision': self.decision,\n            'defensive_zone_starts': self.defensive_zone_starts,\n            'defensive_zone_start_percentage':\n            self.defensive_zone_start_percentage,\n            'even_strength_assists': self.even_strength_assists,\n            'even_strength_goals': self.even_strength_goals,\n            'game_winning_goals': self.game_winning_goals,\n            'goals': self.goals,\n            'goals_against': self.goals_against,\n            'hits_at_even_strength': self.hits_at_even_strength,\n            'invidual_corsi_for_events': self.individual_corsi_for_events,\n            'name': self.name,\n            'offensive_zone_start_percentage':\n            self.offensive_zone_start_percentage,\n            'offensive_zone_starts': self.offensive_zone_starts,\n            'on_ice_shot_attempts_against': self.on_ice_shot_attempts_against,\n            'on_ice_shot_attempts_for': self.on_ice_shot_attempts_for,\n            'penalties_in_minutes': self.penalties_in_minutes,\n            'player_id': self.player_id,\n            'plus_minus': self.plus_minus,\n            'points': self.points,\n            'power_play_assists': self.power_play_assists,\n            'power_play_goals': self.power_play_goals,\n            'relative_corsi_for_percentage':\n            self.relative_corsi_for_percentage,\n            'save_percentage': self.save_percentage,\n            'saves': self.saves,\n            'shifts': self.shifts,\n            'shooting_percentage': self.shooting_percentage,\n            'short_handed_assists': self.short_handed_assists,\n            'short_handed_goals': self.short_handed_goals,\n            'shots_against': self.shots_against,\n            'shots_on_goal': self.shots_on_goal,\n            'shutouts': self.shutouts,\n            'time_on_ice': self.time_on_ice\n        }\n        return pd.DataFrame([fields_to_include], index=[self._player_id])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse the game s date and location.", "response": "def _parse_game_date_and_location(self, boxscore):\n        \"\"\"\n        Retrieve the game's date and location.\n\n        The game's meta information, such as date, location, attendance, and\n        duration, follow a complex parsing scheme that changes based on the\n        layout of the page. The information should be able to be parsed and set\n        regardless of the order and how much information is included. To do\n        this, the meta information should be iterated through line-by-line and\n        fields should be determined by the values that are found in each line.\n\n        Parameters\n        ----------\n        boxscore : PyQuery object\n            A PyQuery object containing all of the HTML data from the boxscore.\n        \"\"\"\n        scheme = BOXSCORE_SCHEME[\"game_info\"]\n        items = [i.text() for i in boxscore(scheme).items()]\n        game_info = items[0].split('\\n')\n        arena = None\n        attendance = None\n        date = None\n        duration = None\n        playoff_round = None\n        time = None\n        if game_info[0].count(',') == 2:\n            date = ','.join(game_info[0].split(',')[0:2]).strip()\n            time = game_info[0].split(',')[-1].strip()\n        else:\n            date = game_info[0]\n        for line in game_info:\n            if 'Arena: ' in line:\n                arena = line.replace('Arena: ', '')\n            if 'Attendance: ' in line:\n                attendance = line.replace('Attendance: ', '').replace(',', '')\n            if 'Game Duration: ' in line:\n                duration = line.replace('Game Duration: ', '')\n            if 'eastern first round' in line.lower() or \\\n               'western first round' in line.lower() or \\\n               'eastern second round' in line.lower() or \\\n               'western second round' in line.lower() or \\\n               'eastern conference finals' in line.lower() or \\\n               'western conference finals' in line.lower() or \\\n               'stanley cup final' in line.lower():\n                playoff_round = line\n        setattr(self, '_arena', arena)\n        setattr(self, '_attendance', attendance)\n        setattr(self, '_date', date)\n        setattr(self, '_duration', duration)\n        setattr(self, '_playoff_round', playoff_round)\n        setattr(self, '_time', time)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfinding the player s ID.", "response": "def _find_player_id(self, row):\n        \"\"\"\n        Find the player's ID.\n\n        Find the player's ID as embedded in the 'data-append-csv' attribute,\n        such as 'zettehe01' for Henrik Zetterberg.\n\n        Parameters\n        ----------\n        row : PyQuery object\n            A PyQuery object representing a single row in a boxscore table for\n            a single player.\n\n        Returns\n        -------\n        str\n            Returns a ``string`` of the player's ID, such as 'zettehe01' for\n            Henrik Zetterberg.\n        \"\"\"\n        player_id = row('th').attr('data-append-csv')\n        if not player_id:\n            player_id = row('td').attr('data-append-csv')\n        return player_id"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses a value for every attribute. This function looks through every attribute and retrieves the value according to the parsing scheme and index of the attribute from the passed HTML data. Once the value is retrieved, the attribute's value is updated with the returned result. Note that this method is called directly once Boxscore is invoked and does not need to be called manually. Parameters ---------- uri : string The relative link to the boxscore HTML page, such as '201802040nwe'.", "response": "def _parse_game_data(self, uri):\n        \"\"\"\n        Parses a value for every attribute.\n\n        This function looks through every attribute and retrieves the value\n        according to the parsing scheme and index of the attribute from the\n        passed HTML data. Once the value is retrieved, the attribute's value is\n        updated with the returned result.\n\n        Note that this method is called directly once Boxscore is invoked and\n        does not need to be called manually.\n\n        Parameters\n        ----------\n        uri : string\n            The relative link to the boxscore HTML page, such as\n            '201802040nwe'.\n        \"\"\"\n        boxscore = self._retrieve_html_page(uri)\n        # If the boxscore is None, the game likely hasn't been played yet and\n        # no information can be gathered. As there is nothing to grab, the\n        # class instance should just be empty.\n        if not boxscore:\n            return\n\n        fields_to_special_parse = [\n            'away_even_strength_assists',\n            'away_power_play_assists',\n            'away_short_handed_assists',\n            'away_game_winning_goals',\n            'away_saves',\n            'away_save_percentage',\n            'away_shutout',\n            'home_even_strength_assists',\n            'home_power_play_assists',\n            'home_short_handed_assists',\n            'home_game_winning_goals',\n            'home_saves',\n            'home_save_percentage',\n            'home_shutout'\n        ]\n\n        for field in self.__dict__:\n            # Remove the '_' from the name\n            short_field = str(field)[1:]\n            if short_field == 'winner' or \\\n               short_field == 'winning_name' or \\\n               short_field == 'winning_abbr' or \\\n               short_field == 'losing_name' or \\\n               short_field == 'losing_abbr' or \\\n               short_field == 'uri' or \\\n               short_field == 'date' or \\\n               short_field == 'time' or \\\n               short_field == 'arena' or \\\n               short_field == 'attendance' or \\\n               short_field == 'time_of_day' or \\\n               short_field == 'duration':\n                continue\n            if short_field == 'away_name' or \\\n               short_field == 'home_name':\n                value = self._parse_name(short_field, boxscore)\n                setattr(self, field, value)\n                continue\n            if short_field in fields_to_special_parse:\n                scheme = BOXSCORE_SCHEME[short_field]\n                value = [i.text() for i in boxscore(scheme).items()]\n                setattr(self, field, value)\n                continue\n            index = 0\n            if short_field in BOXSCORE_ELEMENT_INDEX.keys():\n                index = BOXSCORE_ELEMENT_INDEX[short_field]\n            value = utils._parse_field(BOXSCORE_SCHEME,\n                                       boxscore,\n                                       short_field,\n                                       index)\n            setattr(self, field, value)\n\n        self._away_skaters = len(boxscore(BOXSCORE_SCHEME['away_skaters']))\n        num_away_goalies = boxscore(BOXSCORE_SCHEME['away_goalies']).items()\n        # Skip the first element as it is dedicated to skaters and not goalies.\n        next(num_away_goalies)\n        self._away_goalies = len(next(num_away_goalies)('tbody tr'))\n        self._parse_game_date_and_location(boxscore)\n        self._away_players, self._home_players = self._find_players(boxscore)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dataframe(self):\n        if self._away_goals is None and self._home_goals is None:\n            return None\n        fields_to_include = {\n            'arena': self.arena,\n            'attendance': self.attendance,\n            'away_assists': self.away_assists,\n            'away_even_strength_assists': self.away_even_strength_assists,\n            'away_even_strength_goals': self.away_even_strength_goals,\n            'away_game_winning_goals': self.away_game_winning_goals,\n            'away_goals': self.away_goals,\n            'away_penalties_in_minutes': self.away_penalties_in_minutes,\n            'away_points': self.away_points,\n            'away_power_play_assists': self.away_power_play_assists,\n            'away_power_play_goals': self.away_power_play_goals,\n            'away_save_percentage': self.away_save_percentage,\n            'away_saves': self.away_saves,\n            'away_shooting_percentage': self.away_shooting_percentage,\n            'away_short_handed_assists': self.away_short_handed_assists,\n            'away_short_handed_goals': self.away_short_handed_goals,\n            'away_shots_on_goal': self.away_shots_on_goal,\n            'away_shutout': self.away_shutout,\n            'date': self.date,\n            'duration': self.duration,\n            'home_assists': self.home_assists,\n            'home_even_strength_assists': self.home_even_strength_assists,\n            'home_even_strength_goals': self.home_even_strength_goals,\n            'home_game_winning_goals': self.home_game_winning_goals,\n            'home_goals': self.home_goals,\n            'home_penalties_in_minutes': self.home_penalties_in_minutes,\n            'home_points': self.home_points,\n            'home_power_play_assists': self.home_power_play_assists,\n            'home_power_play_goals': self.home_power_play_goals,\n            'home_save_percentage': self.home_save_percentage,\n            'home_saves': self.home_saves,\n            'home_shooting_percentage': self.home_shooting_percentage,\n            'home_short_handed_assists': self.home_short_handed_assists,\n            'home_short_handed_goals': self.home_short_handed_goals,\n            'home_shots_on_goal': self.home_shots_on_goal,\n            'home_shutout': self.home_shutout,\n            'losing_abbr': self.losing_abbr,\n            'losing_name': self.losing_name,\n            'time': self.time,\n            'winner': self.winner,\n            'winning_abbr': self.winning_abbr,\n            'winning_name': self.winning_name\n        }\n        return pd.DataFrame([fields_to_include], index=[self._uri])", "response": "Returns a pandas DataFrame containing all other class properties and values. The index for the DataFrame is the URI that is used to instantiate the class such as PN6070VEG."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a float of the percentage of shots the away team saved. Percentage ranges from 0 - 1.", "response": "def away_save_percentage(self):\n        \"\"\"\n        Returns a ``float`` of the percentage of shots the away team saved.\n        Percentage ranges from 0-1.\n        \"\"\"\n        try:\n            save_pct = float(self.away_saves) / float(self.home_shots_on_goal)\n            return round(save_pct, 3)\n        except ZeroDivisionError:\n            return 0.0"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a float of the percentage of shots the home team saved. Percentage ranges from 0 - 1.", "response": "def home_save_percentage(self):\n        \"\"\"\n        Returns a ``float`` of the percentage of shots the home team saved.\n        Percentage ranges from 0-1.\n        \"\"\"\n        try:\n            save_pct = float(self.home_saves) / float(self.away_shots_on_goal)\n            return round(save_pct, 3)\n        except ZeroDivisionError:\n            return 0.0"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse the opponent s abbreviation from their name.", "response": "def _parse_abbreviation(self, game_data):\n        \"\"\"\n        Parses the opponent's abbreviation from their name.\n\n        The opponent's abbreviation is embedded within the HTML tag and needs\n        a special parsing scheme in order to be extracted. For non-DI schools,\n        the team's name should be used as the abbreviation.\n\n        Parameters\n        ----------\n        game_data : PyQuery object\n            A PyQuery object containing the information specific to a game.\n        \"\"\"\n        name = game_data('td[data-stat=\"opp_name\"]:first')\n        # Non-DI schools do not have abbreviations and should be handled\n        # differently by just using the team's name as the abbreviation.\n        if 'cfb/schools' not in str(name):\n            setattr(self, '_opponent_abbr', name.text())\n            return\n        name = re.sub(r'.*/cfb/schools/', '', str(name))\n        name = re.sub('/.*', '', name)\n        setattr(self, '_opponent_abbr', name)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a pandas DataFrame containing all other class properties and their values.", "response": "def dataframe(self):\n        \"\"\"\n        Returns a pandas DataFrame containing all other class properties and\n        values. The index for the DataFrame is the boxscore string.\n        \"\"\"\n        if self._points_for is None and self._points_against is None:\n            return None\n        fields_to_include = {\n            'boxscore_index': self.boxscore_index,\n            'date': self.date,\n            'datetime': self.datetime,\n            'day_of_week': self.day_of_week,\n            'game': self.game,\n            'location': self.location,\n            'losses': self.losses,\n            'opponent_abbr': self.opponent_abbr,\n            'opponent_conference': self.opponent_conference,\n            'opponent_name': self.opponent_name,\n            'opponent_rank': self.opponent_rank,\n            'points_against': self.points_against,\n            'points_for': self.points_for,\n            'rank': self.rank,\n            'result': self.result,\n            'streak': self.streak,\n            'time': self.time,\n            'wins': self.wins\n        }\n        return pd.DataFrame([fields_to_include], index=[self._boxscore])"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a datetime object of the month day year and time of the game.", "response": "def datetime(self):\n        \"\"\"\n        Returns a datetime object of the month, day, year, and time the game\n        was played. If the game doesn't include a time, the default value of\n        '00:00' will be used.\n        \"\"\"\n        if self._time == '':\n            return datetime.strptime(self._date, '%b %d, %Y')\n        date_string = '%s %s' % (self._date, self._time)\n        return datetime.strptime(date_string, '%b %d, %Y %I:%M %p')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a string constant to indicate whether the game was played at home away or in a neutral location.", "response": "def location(self):\n        \"\"\"\n        Returns a ``string`` constant to indicate whether the game was played\n        at home, away, or in a neutral location.\n        \"\"\"\n        if self._location.lower() == 'n':\n            return NEUTRAL\n        if self._location.lower() == '@':\n            return AWAY\n        return HOME"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rank(self):\n        rank = re.findall(r'\\d+', self._rank)\n        if len(rank) == 0:\n            return None\n        return rank[0]", "response": "Returns an integer of the team s rank at the time the game was played."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndownload and create Game objects for the team s schedule page and append them to the self. _games property.", "response": "def _pull_schedule(self, abbreviation, year):\n        \"\"\"\n        Download and create objects for the team's schedule.\n\n        Given a team abbreviation and season, first download the team's\n        schedule page and convert to a PyQuery object, then create a Game\n        instance for every game in the team's schedule and append it to the\n        '_games' property.\n\n        Parameters\n        ----------\n        abbreviation : string\n            A team's short name, such as 'MICHIGAN' for the Michigan\n            Wolverines.\n        year : string\n            The requested year to pull stats from.\n        \"\"\"\n        if not year:\n            year = utils._find_year_for_season('ncaaf')\n        doc = pq(SCHEDULE_URL % (abbreviation.lower(), year))\n        schedule = utils._get_stats_table(doc, 'table#schedule')\n\n        for item in schedule:\n            game = Game(item)\n            self._games.append(game)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _parse_boxscore(self, game_data):\n        boxscore = game_data('td[data-stat=\"boxscore\"]:first')\n        boxscore = re.sub(r'.*/boxes/', '', str(boxscore))\n        boxscore = re.sub(r'\\.shtml.*', '', boxscore)\n        setattr(self, '_boxscore', boxscore)", "response": "Parses the boxscore URI for the game."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a pandas DataFrame containing all other class properties and values. The index for the DataFrame is the boxscore string.", "response": "def dataframe(self):\n        \"\"\"\n        Returns a pandas DataFrame containing all other class properties and\n        values. The index for the DataFrame is the boxscore string.\n        \"\"\"\n        # If both the runs scored and allowed are None, the game hasn't been\n        # played yet, and the DataFrame should be None.\n        if self._runs_allowed is None and self._runs_scored is None:\n            return None\n        fields_to_include = {\n            'attendance': self.attendance,\n            'boxscore_index': self.boxscore_index,\n            'date': self.date,\n            'datetime': self.datetime,\n            'game_number_for_day': self.game_number_for_day,\n            'day_or_night': self.day_or_night,\n            'game': self.game,\n            'game_duration': self.game_duration,\n            'games_behind': self.games_behind,\n            'innings': self.innings,\n            'location': self.location,\n            'loser': self.loser,\n            'opponent_abbr': self.opponent_abbr,\n            'rank': self.rank,\n            'record': self.record,\n            'result': self.result,\n            'runs_allowed': self.runs_allowed,\n            'runs_scored': self.runs_scored,\n            'save': self.save,\n            'streak': self.streak,\n            'winner': self.winner\n        }\n        return pd.DataFrame([fields_to_include], index=[self._boxscore])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef datetime(self):\n        date_string = '%s %s' % (self._date, self._year)\n        date_string = re.sub(r' \\(\\d+\\)', '', date_string)\n        return datetime.strptime(date_string, '%A, %b %d %Y')", "response": "Returns a datetime object of the month day year and time the game\n        was played."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef game_number_for_day(self):\n        game_number = re.findall(r'\\(\\d+\\)', self._date)\n        if len(game_number) == 0:\n            return 1\n        game_number = re.findall(r'\\d+', game_number[0])\n        return int(game_number[0])", "response": "Returns an integer denoting which game is played for the team during the given day."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef games_behind(self):\n        if 'up' in self._games_behind.lower():\n            games_behind = re.sub('up *', '', self._games_behind.lower())\n            try:\n                return float(games_behind) * -1.0\n            except ValueError:\n                return None\n        if 'tied' in self._games_behind.lower():\n            return 0.0\n        try:\n            return float(self._games_behind)\n        except ValueError:\n            return None", "response": "Returns the number of games behind the leader."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dataframe(self):\n        frames = []\n        for game in self.__iter__():\n            # If both the runs scored and allowed are None, the game hasn't\n            # been played yet, and the data should not be included in the\n            # DataFrame.\n            if game._runs_scored is None and game._runs_allowed is None:\n                continue\n            frames.append(game.dataframe)\n        if frames == []:\n            return None\n        return pd.concat(frames)", "response": "Returns a pandas DataFrame where each row is a representation of the\n            Game class. Rows are indexed by the boxscore string. Rows are indexed by the boxscore string."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npulls the specified value from the HTML contents of a particular player s stats field and returns it as a string.", "response": "def _parse_value(self, stats, field):\n        \"\"\"\n        Pull the specified value from the HTML contents.\n\n        Given a field, find the corresponding HTML tag for that field and parse\n        its value before returning the value as a string. A couple fields, such\n        as 'conference' and 'team_abbreviation' don't follow a standard parsing\n        scheme and need to be handled differently to get the correct value.\n\n        Parameters\n        ----------\n        stats : PyQuery object\n            A PyQuery object containing all stats in HTML format for a\n            particular player.\n        field : string\n            A string of the field to parse from the HTML.\n\n        Returns\n        -------\n        string\n            Returns the desired value as a string.\n        \"\"\"\n        if field == 'conference':\n            value = self._parse_conference(stats)\n        elif field == 'team_abbreviation':\n            value = self._parse_team_abbreviation(stats)\n        else:\n            value = utils._parse_field(PLAYER_SCHEME, stats, field)\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _parse_name(self, team_data):\n        name = team_data('td[data-stat=\"team_ID\"]:first')\n        name = re.sub(r'.*title=\"', '', str(name))\n        name = re.sub(r'\".*', '', name)\n        setattr(self, '_name', name)", "response": "Parses the team s name."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse a value for every attribute in the specified team_data.", "response": "def _parse_team_data(self, team_data):\n        \"\"\"\n        Parses a value for every attribute.\n\n        This function looks through every attribute with the exception of\n        '_rank' and retrieves the value according to the parsing scheme and\n        index of the attribute from the passed HTML data. Once the value is\n        retrieved, the attribute's value is updated with the returned result.\n\n        Note that this method is called directly once Team is invoked and does\n        not need to be called manually.\n\n        Parameters\n        ----------\n        team_data : string\n            A string containing all of the rows of stats for a given team. If\n            multiple tables are being referenced, this will be comprised of\n            multiple rows in a single string.\n        \"\"\"\n        for field in self.__dict__:\n            # The short field truncates the leading '_' in the attribute name.\n            short_field = str(field)[1:]\n            # The rank attribute is passed directly to the class during\n            # instantiation.\n            if field == '_rank' or \\\n               field == '_year':\n                continue\n            elif field == '_name':\n                self._parse_name(team_data)\n                continue\n            # Default to returning the first element returned unless a\n            # subsequent element is desired. For example, total runs and\n            # runs per game are two different fields, but they both share\n            # the same attribute of 'R' in the HTML tables.\n            index = 0\n            if short_field in ELEMENT_INDEX.keys():\n                index = ELEMENT_INDEX[short_field]\n            value = utils._parse_field(PARSING_SCHEME,\n                                       team_data,\n                                       short_field,\n                                       index)\n            setattr(self, field, value)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dataframe(self):\n        fields_to_include = {\n            'abbreviation': self.abbreviation,\n            'at_bats': self.at_bats,\n            'average_batter_age': self.average_batter_age,\n            'average_pitcher_age': self.average_pitcher_age,\n            'away_losses': self.away_losses,\n            'away_record': self.away_record,\n            'away_wins': self.away_wins,\n            'balks': self.balks,\n            'bases_on_balls': self.bases_on_balls,\n            'bases_on_walks_given': self.bases_on_walks_given,\n            'bases_on_walks_given_per_nine_innings':\n            self.bases_on_walks_given_per_nine_innings,\n            'batters_faced': self.batters_faced,\n            'batting_average': self.batting_average,\n            'complete_game_shutouts': self.complete_game_shutouts,\n            'complete_games': self.complete_games,\n            'doubles': self.doubles,\n            'earned_runs_against': self.earned_runs_against,\n            'earned_runs_against_plus': self.earned_runs_against_plus,\n            'extra_inning_losses': self.extra_inning_losses,\n            'extra_inning_record': self.extra_inning_record,\n            'extra_inning_wins': self.extra_inning_wins,\n            'fielding_independent_pitching':\n            self.fielding_independent_pitching,\n            'games': self.games,\n            'games_finished': self.games_finished,\n            'grounded_into_double_plays': self.grounded_into_double_plays,\n            'hit_pitcher': self.hit_pitcher,\n            'hits': self.hits,\n            'hits_allowed': self.hits_allowed,\n            'hits_per_nine_innings': self.hits_per_nine_innings,\n            'home_losses': self.home_losses,\n            'home_record': self.home_record,\n            'home_runs': self.home_runs,\n            'home_runs_against': self.home_runs_against,\n            'home_runs_per_nine_innings': self.home_runs_per_nine_innings,\n            'home_wins': self.home_wins,\n            'innings_pitched': self.innings_pitched,\n            'intentional_bases_on_balls': self.intentional_bases_on_balls,\n            'interleague_record': self.interleague_record,\n            'last_ten_games_record': self.last_ten_games_record,\n            'last_thirty_games_record': self.last_thirty_games_record,\n            'last_twenty_games_record': self.last_twenty_games_record,\n            'league': self.league,\n            'losses': self.losses,\n            'losses_last_ten_games': self.losses_last_ten_games,\n            'losses_last_thirty_games': self.losses_last_thirty_games,\n            'losses_last_twenty_games': self.losses_last_twenty_games,\n            'losses_vs_left_handed_pitchers':\n            self.losses_vs_left_handed_pitchers,\n            'losses_vs_right_handed_pitchers':\n            self.losses_vs_right_handed_pitchers,\n            'losses_vs_teams_over_500': self.losses_vs_teams_over_500,\n            'losses_vs_teams_under_500': self.losses_vs_teams_under_500,\n            'luck': self.luck,\n            'name': self.name,\n            'number_of_pitchers': self.number_of_pitchers,\n            'number_players_used': self.number_players_used,\n            'on_base_percentage': self.on_base_percentage,\n            'on_base_plus_slugging_percentage':\n            self.on_base_plus_slugging_percentage,\n            'on_base_plus_slugging_percentage_plus':\n            self.on_base_plus_slugging_percentage_plus,\n            'opposing_runners_left_on_base':\n            self.opposing_runners_left_on_base,\n            'plate_appearances': self.plate_appearances,\n            'pythagorean_win_loss': self.pythagorean_win_loss,\n            'rank': self.rank,\n            'record_vs_left_handed_pitchers':\n            self.record_vs_left_handed_pitchers,\n            'record_vs_right_handed_pitchers':\n            self.record_vs_right_handed_pitchers,\n            'record_vs_teams_over_500': self.record_vs_teams_over_500,\n            'record_vs_teams_under_500': self.record_vs_teams_under_500,\n            'run_difference': self.run_difference,\n            'runners_left_on_base': self.runners_left_on_base,\n            'runs': self.runs,\n            'runs_against': self.runs_against,\n            'runs_allowed_per_game': self.runs_allowed_per_game,\n            'runs_batted_in': self.runs_batted_in,\n            'sacrifice_flies': self.sacrifice_flies,\n            'sacrifice_hits': self.sacrifice_hits,\n            'saves': self.saves,\n            'shutouts': self.shutouts,\n            'simple_rating_system': self.simple_rating_system,\n            'single_run_losses': self.single_run_losses,\n            'single_run_record': self.single_run_record,\n            'single_run_wins': self.single_run_wins,\n            'slugging_percentage': self.slugging_percentage,\n            'stolen_bases': self.stolen_bases,\n            'streak': self.streak,\n            'strength_of_schedule': self.strength_of_schedule,\n            'strikeouts': self.strikeouts,\n            'strikeouts_per_base_on_balls': self.strikeouts_per_base_on_balls,\n            'strikeouts_per_nine_innings': self.strikeouts_per_nine_innings,\n            'times_caught_stealing': self.times_caught_stealing,\n            'times_hit_by_pitch': self.times_hit_by_pitch,\n            'times_struck_out': self.times_struck_out,\n            'total_bases': self.total_bases,\n            'total_runs': self.total_runs,\n            'triples': self.triples,\n            'whip': self.whip,\n            'wild_pitches': self.wild_pitches,\n            'win_percentage': self.win_percentage,\n            'wins': self.wins,\n            'wins_last_ten_games': self.wins_last_ten_games,\n            'wins_last_thirty_games': self.wins_last_thirty_games,\n            'wins_last_twenty_games': self.wins_last_twenty_games,\n            'wins_vs_left_handed_pitchers': self.wins_vs_left_handed_pitchers,\n            'wins_vs_right_handed_pitchers':\n            self.wins_vs_right_handed_pitchers,\n            'wins_vs_teams_over_500': self.wins_vs_teams_over_500,\n            'wins_vs_teams_under_500': self.wins_vs_teams_under_500\n        }\n        return pd.DataFrame([fields_to_include], index=[self._abbreviation])", "response": "Returns a pandas DataFrame containing all other class properties and values."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _retrieve_all_teams(self, year):\n        team_data_dict = {}\n\n        if not year:\n            year = utils._find_year_for_season('mlb')\n        doc = pq(STANDINGS_URL % year)\n        div_prefix = 'div#all_expanded_standings_overall'\n        standings = utils._get_stats_table(doc, div_prefix)\n        doc = pq(TEAM_STATS_URL % year)\n        div_prefix = 'div#all_teams_standard_%s'\n        batting_stats = utils._get_stats_table(doc, div_prefix % 'batting')\n        pitching_stats = utils._get_stats_table(doc, div_prefix % 'pitching')\n        for stats_list in [standings, batting_stats, pitching_stats]:\n            team_data_dict = self._add_stats_data(stats_list, team_data_dict)\n\n        for team_data in team_data_dict.values():\n            team = Team(team_data['data'], team_data['rank'], year)\n            self._teams.append(team)", "response": "Retrieves all teams in a given season."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _parse_boxscore(self, game_data):\n        boxscore = game_data('td[data-stat=\"box_score_text\"]:first')\n        boxscore = re.sub(r'.*/boxscores/', '', str(boxscore))\n        boxscore = re.sub(r'\\.html.*', '', boxscore)\n        setattr(self, '_boxscore', boxscore)", "response": "Parses the boxscore URI for the game."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _parse_opponent_abbr(self, game_data):\n        opponent = game_data('td[data-stat=\"opp_name\"]:first')\n        opponent = re.sub(r'.*/teams/', '', str(opponent))\n        opponent = re.sub(r'\\/.*.html.*', '', opponent)\n        setattr(self, '_opponent_abbr', opponent)", "response": "Parses the opponent s 3 - letter abbreviation for the game."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing a game data string and sets the attributes of the object with the value of the game s attribute.", "response": "def _parse_game_data(self, game_data):\n        \"\"\"\n        Parses a value for every attribute.\n\n        The function looks through every attribute with the exception of those\n        listed below and retrieves the value according to the parsing scheme\n        and index of the attribute from the passed HTML data. Once the value\n        is retrieved, the attribute's value is updated with the returned\n        result.\n\n        Note that this method is called directory once Game is invoked and does\n        not need to be called manually.\n\n        Parameters\n        ----------\n        game_data : string\n            A string containing all of the rows of stats for a given game.\n        \"\"\"\n        for field in self.__dict__:\n            # Remove the leading '_' from the name\n            short_name = str(field)[1:]\n            if short_name == 'datetime':\n                continue\n            elif short_name == 'boxscore':\n                self._parse_boxscore(game_data)\n                continue\n            elif short_name == 'opponent_abbr':\n                self._parse_opponent_abbr(game_data)\n                continue\n            value = utils._parse_field(SCHEDULE_SCHEME, game_data, short_name)\n            setattr(self, field, value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dataframe(self):\n        if self._points_allowed is None and self._points_scored is None:\n            return None\n        fields_to_include = {\n            'boxscore_index': self.boxscore_index,\n            'date': self.date,\n            'datetime': self.datetime,\n            'game': self.game,\n            'location': self.location,\n            'losses': self.losses,\n            'opponent_abbr': self.opponent_abbr,\n            'opponent_name': self.opponent_name,\n            'points_allowed': self.points_allowed,\n            'points_scored': self.points_scored,\n            'result': self.result,\n            'streak': self.streak,\n            'time': self.time,\n            'wins': self.wins\n        }\n        return pd.DataFrame([fields_to_include], index=[self._boxscore])", "response": "Returns a pandas DataFrame containing all other class properties and their values."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _add_games_to_schedule(self, schedule):\n        for item in schedule:\n            if 'class=\"thead\"' in str(item) or \\\n               'class=\"over_header thead\"' in str(item):\n                continue  # pragma: no cover\n            game = Game(item)\n            self._games.append(game)", "response": "Add games to the list of games."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a pandas DataFrame containing all other class properties and values. The index for the DataFrame is the abbreviation of the base class.", "response": "def dataframe(self):\n        \"\"\"\n        Returns a pandas DataFrame containing all other class properties and\n        values. The index for the DataFrame is the string abbreviation of the\n        team, such as 'DET'.\n        \"\"\"\n        fields_to_include = {\n            'abbreviation': self.abbreviation,\n            'assists': self.assists,\n            'blocks': self.blocks,\n            'defensive_rebounds': self.defensive_rebounds,\n            'field_goal_attempts': self.field_goal_attempts,\n            'field_goal_percentage': self.field_goal_percentage,\n            'field_goals': self.field_goals,\n            'free_throw_attempts': self.free_throw_attempts,\n            'free_throw_percentage': self.free_throw_percentage,\n            'free_throws': self.free_throws,\n            'games_played': self.games_played,\n            'minutes_played': self.minutes_played,\n            'name': self.name,\n            'offensive_rebounds': self.offensive_rebounds,\n            'opp_assists': self.opp_assists,\n            'opp_blocks': self.opp_blocks,\n            'opp_defensive_rebounds': self.opp_defensive_rebounds,\n            'opp_field_goal_attempts': self.opp_field_goal_attempts,\n            'opp_field_goal_percentage': self.opp_field_goal_percentage,\n            'opp_field_goals': self.opp_field_goals,\n            'opp_free_throw_attempts': self.opp_free_throw_attempts,\n            'opp_free_throw_percentage': self.opp_free_throw_percentage,\n            'opp_free_throws': self.opp_free_throws,\n            'opp_offensive_rebounds': self.opp_offensive_rebounds,\n            'opp_personal_fouls': self.opp_personal_fouls,\n            'opp_points': self.opp_points,\n            'opp_steals': self.opp_steals,\n            'opp_three_point_field_goal_attempts':\n            self.opp_three_point_field_goal_attempts,\n            'opp_three_point_field_goal_percentage':\n            self.opp_three_point_field_goal_percentage,\n            'opp_three_point_field_goals': self.opp_three_point_field_goals,\n            'opp_total_rebounds': self.opp_total_rebounds,\n            'opp_turnovers': self.opp_turnovers,\n            'opp_two_point_field_goal_attempts':\n            self.opp_two_point_field_goal_attempts,\n            'opp_two_point_field_goal_percentage':\n            self.opp_two_point_field_goal_percentage,\n            'opp_two_point_field_goals': self.opp_two_point_field_goals,\n            'personal_fouls': self.personal_fouls,\n            'points': self.points,\n            'rank': self.rank,\n            'steals': self.steals,\n            'three_point_field_goal_attempts':\n            self.three_point_field_goal_attempts,\n            'three_point_field_goal_percentage':\n            self.three_point_field_goal_percentage,\n            'three_point_field_goals': self.three_point_field_goals,\n            'total_rebounds': self.total_rebounds,\n            'turnovers': self.turnovers,\n            'two_point_field_goal_attempts':\n            self.two_point_field_goal_attempts,\n            'two_point_field_goal_percentage':\n            self.two_point_field_goal_percentage,\n            'two_point_field_goals': self.two_point_field_goals\n        }\n        return pd.DataFrame([fields_to_include], index=[self._abbreviation])"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves all teams in a given season.", "response": "def _retrieve_all_teams(self, year):\n        \"\"\"\n        Find and create Team instances for all teams in the given season.\n\n        For a given season, parses the specified NBA stats table and finds all\n        requested stats. Each team then has a Team instance created which\n        includes all requested stats and a few identifiers, such as the team's\n        name and abbreviation. All of the individual Team instances are added\n        to a list.\n\n        Note that this method is called directly once Teams is invoked and does\n        not need to be called manually.\n\n        Parameters\n        ----------\n        year : string\n            The requested year to pull stats from.\n        \"\"\"\n        team_data_dict = {}\n\n        if not year:\n            year = utils._find_year_for_season('nba')\n        doc = pq(SEASON_PAGE_URL % year)\n        teams_list = utils._get_stats_table(doc, 'div#all_team-stats-base')\n        opp_teams_list = utils._get_stats_table(doc,\n                                                'div#all_opponent-stats-base')\n        for stats_list in [teams_list, opp_teams_list]:\n            team_data_dict = self._add_stats_data(stats_list, team_data_dict)\n\n        for team_data in team_data_dict.values():\n            team = Team(team_data['data'], team_data['rank'], year)\n            self._teams.append(team)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dataframe(self):\n        fields_to_include = {\n            'completed_passes': self.completed_passes,\n            'attempted_passes': self.attempted_passes,\n            'passing_completion': self.passing_completion,\n            'passing_yards': self.passing_yards,\n            'pass_yards_per_attempt': self.pass_yards_per_attempt,\n            'adjusted_yards_per_attempt': self.adjusted_yards_per_attempt,\n            'passing_touchdowns': self.passing_touchdowns,\n            'interceptions_thrown': self.interceptions_thrown,\n            'quarterback_rating': self.quarterback_rating,\n            'rush_attempts': self.rush_attempts,\n            'rush_yards': self.rush_yards,\n            'rush_yards_per_attempt': self.rush_yards_per_attempt,\n            'rush_touchdowns': self.rush_touchdowns,\n            'receptions': self.receptions,\n            'receiving_yards': self.receiving_yards,\n            'receiving_yards_per_reception':\n            self.receiving_yards_per_reception,\n            'receiving_touchdowns': self.receiving_touchdowns,\n            'plays_from_scrimmage': self.plays_from_scrimmage,\n            'yards_from_scrimmage': self.yards_from_scrimmage,\n            'yards_from_scrimmage_per_play':\n            self.yards_from_scrimmage_per_play,\n            'rushing_and_receiving_touchdowns':\n            self.rushing_and_receiving_touchdowns,\n            'solo_tackles': self.solo_tackles,\n            'assists_on_tackles': self.assists_on_tackles,\n            'total_tackles': self.total_tackles,\n            'tackles_for_loss': self.tackles_for_loss,\n            'sacks': self.sacks,\n            'interceptions': self.interceptions,\n            'yards_returned_from_interceptions':\n            self.yards_returned_from_interceptions,\n            'yards_returned_per_interception':\n            self.yards_returned_per_interception,\n            'interceptions_returned_for_touchdown':\n            self.interceptions_returned_for_touchdown,\n            'passes_defended': self.passes_defended,\n            'fumbles_recovered': self.fumbles_recovered,\n            'yards_recovered_from_fumble': self.yards_recovered_from_fumble,\n            'fumbles_recovered_for_touchdown':\n            self.fumbles_recovered_for_touchdown,\n            'fumbles_forced': self.fumbles_forced,\n            'kickoff_returns': self.kickoff_returns,\n            'kickoff_return_yards': self.kickoff_return_yards,\n            'average_kickoff_return_yards': self.average_kickoff_return_yards,\n            'kickoff_return_touchdowns': self.kickoff_return_touchdowns,\n            'punt_returns': self.punt_returns,\n            'punt_return_yards': self.punt_return_yards,\n            'average_punt_return_yards': self.average_punt_return_yards,\n            'punt_return_touchdowns': self.punt_return_touchdowns,\n            'extra_points_made': self.extra_points_made,\n            'extra_points_attempted': self.extra_points_attempted,\n            'extra_point_percentage': self.extra_point_percentage,\n            'field_goals_made': self.field_goals_made,\n            'field_goals_attempted': self.field_goals_attempted,\n            'field_goal_percentage': self.field_goal_percentage,\n            'points_kicking': self.points_kicking,\n            'punts': self.punts,\n            'punting_yards': self.punting_yards,\n            'punting_yards_per_punt': self.punting_yards_per_attempt\n        }\n        return pd.DataFrame([fields_to_include], index=[self._player_id])", "response": "Returns a pandas DataFrame containing all relevant class properties and values for the specified game."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _parse_game_date_and_location(self, boxscore):\n        scheme = BOXSCORE_SCHEME['time']\n        items = [i.text() for i in boxscore(scheme).items()]\n        game_info = items[0].split('\\n')\n        time = ''\n        date = ''\n        stadium = ''\n        for line in game_info:\n            time_match = re.findall(r'(\\d:\\d\\d|\\d\\d:\\d\\d)', line.lower())\n            if len(time_match) > 0:\n                time = line\n            for day in ['monday', 'tuesday', 'wednesday', 'thursday', 'friday',\n                        'saturday', 'sunday']:\n                if day in line.lower():\n                    date = line\n            # In general, locations are in the format 'Stadium Name - City,\n            # State'. Since the ' - ' characters seem to be unique to the\n            # location line, it should be safe to use this as a matcher.\n            if ' - ' in line:\n                stadium = line\n        setattr(self, '_time', time)\n        setattr(self, '_date', date)\n        setattr(self, '_stadium', stadium)", "response": "Parse the game s date and location."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a string that indicates whether the player is on the home or away team.", "response": "def _find_home_or_away(self, row):\n        \"\"\"\n        Determine whether the player is on the home or away team.\n\n        Next to every player is their school's name. This name can be matched\n        with the previously parsed home team's name to determine if the player\n        is a member of the home or away team.\n\n        Parameters\n        ----------\n        row : PyQuery object\n            A PyQuery object representing a single row in a boxscore table for\n            a single player.\n\n        Returns\n        -------\n        str\n            Returns a ``string`` constant denoting whether the team plays for\n            the home or away team.\n        \"\"\"\n        name = row('a:last').text()\n        if name == self._home_name.text():\n            return HOME\n        else:\n            return AWAY"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a pandas DataFrame containing all other class properties and values. The index for the DataFrame is the string URI that is used to instantiate the class such as 2017 - 01 - 08 - georgia.", "response": "def dataframe(self):\n        \"\"\"\n        Returns a pandas DataFrame containing all other class properties and\n        values. The index for the DataFrame is the string URI that is used to\n        instantiate the class, such as '2018-01-08-georgia'.\n        \"\"\"\n        if self._away_points is None and self._home_points is None:\n            return None\n        fields_to_include = {\n            'away_first_downs': self.away_first_downs,\n            'away_fumbles': self.away_fumbles,\n            'away_fumbles_lost': self.away_fumbles_lost,\n            'away_interceptions': self.away_interceptions,\n            'away_pass_attempts': self.away_pass_attempts,\n            'away_pass_completions': self.away_pass_completions,\n            'away_pass_touchdowns': self.away_pass_touchdowns,\n            'away_pass_yards': self.away_pass_yards,\n            'away_penalties': self.away_penalties,\n            'away_points': self.away_points,\n            'away_rush_attempts': self.away_rush_attempts,\n            'away_rush_touchdowns': self.away_rush_touchdowns,\n            'away_rush_yards': self.away_rush_yards,\n            'away_total_yards': self.away_total_yards,\n            'away_turnovers': self.away_turnovers,\n            'away_yards_from_penalties': self.away_yards_from_penalties,\n            'date': self.date,\n            'home_first_downs': self.home_first_downs,\n            'home_fumbles': self.home_fumbles,\n            'home_fumbles_lost': self.home_fumbles_lost,\n            'home_interceptions': self.home_interceptions,\n            'home_pass_attempts': self.home_pass_attempts,\n            'home_pass_completions': self.home_pass_completions,\n            'home_pass_touchdowns': self.home_pass_touchdowns,\n            'home_pass_yards': self.home_pass_yards,\n            'home_penalties': self.home_penalties,\n            'home_points': self.home_points,\n            'home_rush_attempts': self.home_rush_attempts,\n            'home_rush_touchdowns': self.home_rush_touchdowns,\n            'home_rush_yards': self.home_rush_yards,\n            'home_total_yards': self.home_total_yards,\n            'home_turnovers': self.home_turnovers,\n            'home_yards_from_penalties': self.home_yards_from_penalties,\n            'losing_abbr': self.losing_abbr,\n            'losing_name': self.losing_name,\n            'stadium': self.stadium,\n            'time': self.time,\n            'winner': self.winner,\n            'winning_abbr': self.winning_abbr,\n            'winning_name': self.winning_name\n        }\n        return pd.DataFrame([fields_to_include], index=[self._uri])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef winning_abbr(self):\n        if self.winner == HOME:\n            if 'cfb/schools' not in str(self._home_name):\n                return self._home_name.text()\n            return utils._parse_abbreviation(self._home_name)\n        if 'cfb/schools' not in str(self._away_name):\n            return self._away_name.text()\n        return utils._parse_abbreviation(self._away_name)", "response": "Returns a string of the winning team s abbreviation such as Alabama Crimson Tide."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncombine all stats from all tables into a single dictionary.", "response": "def _combine_all_stats(self, player_info):\n        \"\"\"\n        Pull stats from all tables into a single data structure.\n\n        Pull the stats from all of the requested tables into a dictionary that\n        is separated by season to allow easy queries of the player's stats for\n        each season.\n\n        Parameters\n        ----------\n        player_info : PyQuery object\n            A PyQuery object containing all of the stats information for the\n            requested player.\n\n        Returns\n        -------\n        dictionary\n            Returns a dictionary where all stats from each table are combined\n            by season to allow easy queries by year.\n        \"\"\"\n        all_stats_dict = {}\n\n        for table_id in ['passing', 'rushing', 'defense', 'scoring']:\n            table_items = utils._get_stats_table(player_info,\n                                                 'table#%s' % table_id)\n            career_items = utils._get_stats_table(player_info,\n                                                  'table#%s' % table_id,\n                                                  footer=True)\n            all_stats_dict = self._combine_season_stats(table_items,\n                                                        career_items,\n                                                        all_stats_dict)\n        return all_stats_dict"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\npull and parses the player s HTML stats page and returns a dictionary of the combined stats.", "response": "def _pull_player_data(self):\n        \"\"\"\n        Pull and aggregate all player information.\n\n        Pull the player's HTML stats page and parse unique properties, such as\n        the player's height, weight, and name. Next, combine all stats for all\n        seasons plus the player's career stats into a single object which can\n        easily be iterated upon.\n\n        Returns\n        -------\n        dictionary\n            Returns a dictionary of the player's combined stats where each key\n            is a string of the season and the value is the season's associated\n            stats.\n        \"\"\"\n        player_info = self._retrieve_html_page()\n        if not player_info:\n            return\n        self._parse_player_information(player_info)\n        all_stats = self._combine_all_stats(player_info)\n        setattr(self, '_season', list(all_stats.keys()))\n        return all_stats"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _dataframe_fields(self):\n        fields_to_include = {\n            'adjusted_yards_per_attempt': self.adjusted_yards_per_attempt,\n            'assists_on_tackles': self.assists_on_tackles,\n            'attempted_passes': self.attempted_passes,\n            'completed_passes': self.completed_passes,\n            'extra_points_made': self.extra_points_made,\n            'field_goals_made': self.field_goals_made,\n            'fumbles_forced': self.fumbles_forced,\n            'fumbles_recovered': self.fumbles_recovered,\n            'fumbles_recovered_for_touchdown':\n            self.fumbles_recovered_for_touchdown,\n            'games': self.games,\n            'height': self.height,\n            'interceptions': self.interceptions,\n            'interceptions_returned_for_touchdown':\n            self.interceptions_returned_for_touchdown,\n            'interceptions_thrown': self.interceptions_thrown,\n            'kickoff_return_touchdowns': self.kickoff_return_touchdowns,\n            'name': self.name,\n            'other_touchdowns': self.other_touchdowns,\n            'passes_defended': self.passes_defended,\n            'passing_completion': self.passing_completion,\n            'passing_touchdowns': self.passing_touchdowns,\n            'passing_yards_per_attempt': self.passing_yards_per_attempt,\n            'player_id': self.player_id,\n            'plays_from_scrimmage': self.plays_from_scrimmage,\n            'points': self.points,\n            'position': self.position,\n            'punt_return_touchdowns': self.punt_return_touchdowns,\n            'quarterback_rating': self.quarterback_rating,\n            'receiving_touchdowns': self.receiving_touchdowns,\n            'receiving_yards': self.receiving_yards,\n            'receiving_yards_per_reception':\n            self.receiving_yards_per_reception,\n            'receptions': self.receptions,\n            'rush_attempts': self.rush_attempts,\n            'rush_touchdowns': self.rush_touchdowns,\n            'rush_yards': self.rush_yards,\n            'rush_yards_per_attempt': self.rush_yards_per_attempt,\n            'rushing_and_receiving_touchdowns':\n            self.rushing_and_receiving_touchdowns,\n            'sacks': self.sacks,\n            'safeties': self.safeties,\n            'season': self.season,\n            'solo_tackles': self.solo_tackles,\n            'tackles_for_loss': self.tackles_for_loss,\n            'team_abbreviation': self.team_abbreviation,\n            'total_tackles': self.total_tackles,\n            'total_touchdowns': self.total_touchdowns,\n            'two_point_conversions': self.two_point_conversions,\n            'weight': self.weight,\n            'yards_from_scrimmage': self.yards_from_scrimmage,\n            'yards_from_scrimmage_per_play':\n            self.yards_from_scrimmage_per_play,\n            'yards_recovered_from_fumble': self.yards_recovered_from_fumble,\n            'yards_returned_from_interceptions':\n            self.yards_returned_from_interceptions,\n            'yards_returned_per_interception':\n            self.yards_returned_per_interception,\n            'year': self.year\n        }\n        return fields_to_include", "response": "Returns a dictionary of all the fields to include with DataFrame."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _dataframe_fields(self):\n        fields_to_include = {\n            'adjusted_assists': self.adjusted_assists,\n            'adjusted_goals': self.adjusted_goals,\n            'adjusted_goals_against_average':\n            self.adjusted_goals_against_average,\n            'adjusted_goals_created': self.adjusted_goals_created,\n            'adjusted_points': self.adjusted_points,\n            'age': self.age,\n            'assists': self.assists,\n            'average_time_on_ice': self.average_time_on_ice,\n            'blocks_at_even_strength': self.blocks_at_even_strength,\n            'corsi_against': self.corsi_against,\n            'corsi_for': self.corsi_for,\n            'corsi_for_percentage': self.corsi_for_percentage,\n            'defensive_point_shares': self.defensive_point_shares,\n            'defensive_zone_start_percentage':\n            self.defensive_zone_start_percentage,\n            'even_strength_assists': self.even_strength_assists,\n            'even_strength_goals': self.even_strength_goals,\n            'even_strength_goals_allowed': self.even_strength_goals_allowed,\n            'even_strength_save_percentage':\n            self.even_strength_save_percentage,\n            'even_strength_shots_faced': self.even_strength_shots_faced,\n            'faceoff_losses': self.faceoff_losses,\n            'faceoff_percentage': self.faceoff_percentage,\n            'faceoff_wins': self.faceoff_wins,\n            'fenwick_against': self.fenwick_against,\n            'fenwick_for': self.fenwick_for,\n            'fenwick_for_percentage': self.fenwick_for_percentage,\n            'game_winning_goals': self.game_winning_goals,\n            'games_played': self.games_played,\n            'giveaways': self.giveaways,\n            'goal_against_percentage_relative':\n            self.goal_against_percentage_relative,\n            'goalie_point_shares': self.goalie_point_shares,\n            'goals': self.goals,\n            'goals_against': self.goals_against,\n            'goals_against_average': self.goals_against_average,\n            'goals_against_on_ice': self.goals_against_on_ice,\n            'goals_created': self.goals_created,\n            'goals_for_on_ice': self.goals_for_on_ice,\n            'goals_saved_above_average': self.goals_saved_above_average,\n            'height': self.height,\n            'hits_at_even_strength': self.hits_at_even_strength,\n            'league': self.league,\n            'losses': self.losses,\n            'minutes': self.minutes,\n            'name': self.name,\n            'offensive_point_shares': self.offensive_point_shares,\n            'offensive_zone_start_percentage':\n            self.offensive_zone_start_percentage,\n            'pdo': self.pdo,\n            'penalties_in_minutes': self.penalties_in_minutes,\n            'player_id': self.player_id,\n            'plus_minus': self.plus_minus,\n            'point_shares': self.point_shares,\n            'points': self.points,\n            'power_play_assists': self.power_play_assists,\n            'power_play_goals': self.power_play_goals,\n            'power_play_goals_against_on_ice':\n            self.power_play_goals_against_on_ice,\n            'power_play_goals_allowed': self.power_play_goals_allowed,\n            'power_play_goals_for_on_ice': self.power_play_goals_for_on_ice,\n            'power_play_save_percentage': self.power_play_save_percentage,\n            'power_play_shots_faced': self.power_play_shots_faced,\n            'quality_start_percentage': self.quality_start_percentage,\n            'quality_starts': self.quality_starts,\n            'really_bad_starts': self.really_bad_starts,\n            'relative_corsi_for_percentage':\n            self.relative_corsi_for_percentage,\n            'relative_fenwick_for_percentage':\n            self.relative_fenwick_for_percentage,\n            'save_percentage': self.save_percentage,\n            'save_percentage_on_ice': self.save_percentage_on_ice,\n            'saves': self.saves,\n            'season': self.season,\n            'shooting_percentage': self.shooting_percentage,\n            'shooting_percentage_on_ice': self.shooting_percentage_on_ice,\n            'shootout_attempts': self.shootout_attempts,\n            'shootout_goals': self.shootout_goals,\n            'shootout_misses': self.shootout_misses,\n            'shootout_percentage': self.shootout_percentage,\n            'short_handed_assists': self.short_handed_assists,\n            'short_handed_goals': self.short_handed_goals,\n            'short_handed_goals_allowed': self.short_handed_goals_allowed,\n            'short_handed_save_percentage': self.short_handed_save_percentage,\n            'short_handed_shots_faced': self.short_handed_shots_faced,\n            'shots_against': self.shots_against,\n            'shots_on_goal': self.shots_on_goal,\n            'shutouts': self.shutouts,\n            'takeaways': self.takeaways,\n            'team_abbreviation': self.team_abbreviation,\n            'ties_plus_overtime_loss': self.ties_plus_overtime_loss,\n            'time_on_ice': self.time_on_ice,\n            'time_on_ice_even_strength': self.time_on_ice_even_strength,\n            'total_goals_against_on_ice': self.total_goals_against_on_ice,\n            'total_goals_for_on_ice': self.total_goals_for_on_ice,\n            'total_shots': self.total_shots,\n            'weight': self.weight,\n            'wins': self.wins\n        }\n        return fields_to_include", "response": "Returns a dictionary of all the fields to include with DataFrame."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef team_abbreviation(self):\n        # For career stats, skip the team abbreviation.\n        if self._season[self._index].lower() == 'career':\n            return None\n        return self._team_abbreviation[self._index]", "response": "Returns a string of the team s abbreviation such as DET for the career stats."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding all players for the requested team and year.", "response": "def _find_players(self, year):\n        \"\"\"\n        Find all player IDs for the requested team.\n\n        For the requested team and year (if applicable), pull the roster table\n        and parse the player ID for all players on the roster and create an\n        instance of the Player class for the player. All player instances are\n        added to the 'players' property to get all stats for all players on a\n        team.\n\n        Parameters\n        ----------\n        year : string\n            The 6-digit string representing the year to pull the team's roster\n            from.\n        \"\"\"\n        if not year:\n            year = utils._find_year_for_season('nhl')\n        url = self._create_url(year)\n        page = self._pull_team_page(url)\n        if not page:\n            output = (\"Can't pull requested team page. Ensure the following \"\n                      \"URL exists: %s\" % url)\n            raise ValueError(output)\n        for player in page('table#roster tbody tr').items():\n            player_id = self._get_id(player)\n            if self._slim:\n                name = self._get_name(player)\n                self._players[player_id] = name\n            else:\n                player_instance = Player(player_id)\n                self._players.append(player_instance)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nloads training vector from rasterstats.", "response": "def load_training_vector(response_shapes, explanatory_rasters, response_field, metric='mean'):\n    \"\"\"\n    Parameters\n    ----------\n    response_shapes : Source of vector features for raster_stats;\n                      can be OGR file path or iterable of geojson-like features\n    response_field : Field name containing the known response category (must be numeric)\n    explanatory_rasters : List of Paths to GDAL rasters containing explanatory variables\n    metric : Statistic to aggregate explanatory data across line and polygon vector features\n             Defaults to 'mean' (optional)\n\n    Returns\n    -------\n    train_xs : Array of explanatory variables\n    train_y : 1xN array of known responses\n    \"\"\"\n    from rasterstats import zonal_stats\n    all_means = []\n    all_zones = None\n\n    for i, raster in enumerate(explanatory_rasters):\n        logger.debug(\"Rasters stats on %s\" % raster)\n\n        stats = zonal_stats(response_shapes, raster, stats=metric, prefix=\"pyimpute_\", geojson_out=True)\n\n        zones = [x['properties'][response_field] for x in stats]\n        if all_zones:\n            assert zones == all_zones\n        else:\n            all_zones = zones\n\n        means = [x['properties']['pyimpute_' + metric] for x in stats]\n        all_means.append(means)\n\n    train_y = np.array(all_zones)\n    train_xs = np.array(all_means).T\n\n    return train_xs, train_y"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_training_rasters(response_raster, explanatory_rasters, selected=None):\n\n    with rasterio.open(response_raster) as src:\n        response_data = src.read().flatten()\n\n    if selected is None:\n        train_y = response_data\n    else:\n        train_y = response_data[selected]\n\n    selected_data = []\n    for rast in explanatory_rasters:\n        with rasterio.open(rast) as src:\n            explanatory_data = src.read().flatten()\n        assert explanatory_data.size == response_data.size\n        if selected is None:\n            selected_data.append(explanatory_data)\n        else:\n            selected_data.append(explanatory_data[selected])\n\n    train_xs = np.asarray(selected_data).T\n    return train_xs, train_y", "response": "Loads training rasters from the response raster and explanatory rasters."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nloading the targets of the specified rasters into a single array of explanatory variables.", "response": "def load_targets(explanatory_rasters):\n    \"\"\"\n    Parameters\n    ----------\n    explanatory_rasters : List of Paths to GDAL rasters containing explanatory variables\n\n    Returns\n    -------\n    expl : Array of explanatory variables\n    raster_info : dict of raster info\n    \"\"\"\n\n    explanatory_raster_arrays = []\n    aff = None\n    shape = None\n    crs = None\n\n    for raster in explanatory_rasters:\n        logger.debug(raster)\n        with rasterio.open(raster) as src:\n            ar = src.read(1)  # TODO band num? \n\n            # Save or check the geotransform\n            if not aff:\n                aff = src.affine\n            else:\n                assert aff == src.affine\n\n            # Save or check the shape\n            if not shape:\n                shape = ar.shape\n            else:\n                assert shape == ar.shape\n\n            # Save or check the geotransform\n            if not crs:\n                crs = src.crs\n            else:\n                assert crs == src.crs\n\n        # Flatten in one dimension\n        arf = ar.flatten()\n        explanatory_raster_arrays.append(arf)\n\n    expl = np.array(explanatory_raster_arrays).T\n\n    raster_info = {\n        'affine': aff,\n        'shape': shape,\n        'crs': crs\n    }\n    return expl, raster_info"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef impute(target_xs, clf, raster_info, outdir=\"output\", linechunk=1000, class_prob=True, certainty=True):\n    if not os.path.exists(outdir):\n        os.makedirs(outdir)\n\n    shape = raster_info['shape']\n\n    profile = {\n        'affine': raster_info['affine'],\n        'blockxsize': shape[1],\n        'height': shape[0],\n        'blockysize': 1,\n        'count': 1,\n        'crs': raster_info['crs'],\n        'driver': u'GTiff',\n        'dtype': 'int16',\n        'nodata': -32768,\n        'tiled': False,\n        'transform': raster_info['affine'].to_gdal(),\n        'width': shape[1]}\n\n    try:\n        response_path = os.path.join(outdir, \"responses.tif\")\n        response_ds = rasterio.open(response_path, 'w', **profile)\n\n        profile['dtype'] = 'float32'\n        if certainty:\n            certainty_path = os.path.join(outdir, \"certainty.tif\")\n            certainty_ds = rasterio.open(certainty_path, 'w', **profile)\n\n        class_dss = []\n        if class_prob:\n            classes = list(clf.classes_)\n            class_paths = []\n            for i, c in enumerate(classes):\n                ods = os.path.join(outdir, \"probability_%s.tif\" % c)\n                class_paths.append(ods)\n            for p in class_paths:\n                class_dss.append(rasterio.open(p, 'w', **profile))\n\n        # Chunky logic\n        if not linechunk:\n            linechunk = shape[0]\n        chunks = int(math.ceil(shape[0] / float(linechunk)))\n\n        for chunk in range(chunks):\n            logger.debug(\"Writing chunk %d of %d\" % (chunk+1, chunks))\n            row = chunk * linechunk\n            if row + linechunk > shape[0]:\n                linechunk = shape[0] - row\n            # in 1D space\n            start = shape[1] * row\n            end = start + shape[1] * linechunk\n            line = target_xs[start:end, :]\n\n            window = ((row, row + linechunk), (0, shape[1]))\n\n            # Predict\n            responses = clf.predict(line)\n            responses2D = responses.reshape((linechunk, shape[1])).astype('int16')\n            response_ds.write_band(1, responses2D, window=window)\n\n            if certainty or class_prob:\n                proba = clf.predict_proba(line)\n\n            # Certainty\n            if certainty:\n                certaintymax = proba.max(axis=1)\n                certainty2D = certaintymax.reshape((linechunk, shape[1])).astype('float32')\n                certainty_ds.write_band(1, certainty2D, window=window)\n\n            # write out probabilities for each class as a separate raster\n            for i, class_ds in enumerate(class_dss):\n                proba_class = proba[:, i]\n                classcert2D = proba_class.reshape((linechunk, shape[1])).astype('float32')\n                class_ds.write_band(1, classcert2D, window=window)\n\n    finally:\n        response_ds.close()\n        if certainty:\n            certainty_ds.close()\n        for class_ds in class_dss:\n            class_ds.close()", "response": "Impute a set of target_xs and clf."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef stratified_sample_raster(strata_data, target_sample_size=30, min_sample_proportion=0.1):\n    with rasterio.open(strata_data) as src:\n        strata = src.read().flatten()\n    index_array = np.arange(strata.size)\n\n    # construct a dictionary of lists,\n    # keys are stratum ids\n    # values are list of indices\n    sample = dict([(int(s),[]) for s in np.unique(strata)])\n    satisfied = []\n\n    # counts for proportion-based constraints\n    bins = np.bincount(strata)\n    ii = np.nonzero(bins)[0]\n    stratum_count = dict(zip(ii,bins[ii]))\n\n    # shuffle the indices and loop until the sample satisfied our constraints\n    np.random.shuffle(index_array)\n    for idx in index_array:\n        stratum = strata[index_array[idx]]\n        if stratum in satisfied:\n            continue\n        sample[stratum].append(idx)\n        nsamples = len(sample[stratum])\n        # constraints -> hit the target sample size OR proportion of total\n        # (whichever is highest)\n        target = stratum_count[stratum] * min_sample_proportion\n        if target < target_sample_size:\n            target = target_sample_size\n        if nsamples >= target:\n            satisfied.append(stratum)\n        if len(satisfied) == len(sample.keys()):\n            break\n\n    # convert sampled indicies into a list of indicies\n    selected = []\n    for k, v in sample.items():\n        # check for stratum with < target sample size\n        if len(v) < target_sample_size:\n            # if we have too few samples, drop them\n            #warnings.warn(\"Stratum %s has only %d samples, dropped\" % (k, len(v)))\n            pass\n        else:\n            selected.extend(v)\n\n    return np.array(selected)", "response": "This function samples from the strata file into a list of indices and returns the indices of the selected straties."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef evaluate_clf(clf, X, y, k=None, test_size=0.5, scoring=\"f1_weighted\", feature_names=None):\n    X_train, X_test, y_train, y_true = cross_validation.train_test_split(\n        X, y, test_size=test_size)\n\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n\n    print(\"Accuracy Score: %f\" % metrics.accuracy_score(y_true, y_pred))\n    print()\n\n    print(\"Classification report\")\n    print(metrics.classification_report(y_true, y_pred))\n    print()\n\n    print(\"Confussion matrix\")\n    print(metrics.confusion_matrix(y_true, y_pred))\n    print()\n\n    print(\"Feature importances\")\n    if not feature_names:\n        feature_names = [\"%d\" % i for i in xrange(X.shape[1])]\n    for f, imp in zip(feature_names, clf.feature_importances_):\n        print(\"%20s: %s\" % (f, round(imp * 100, 1)))\n    print()\n\n    if k:\n        print(\"Cross validation\")\n        kf = cross_validation.KFold(len(y), n_folds=k)\n        scores = cross_validation.cross_val_score(clf, X, y, cv=kf, scoring=scoring)\n        print(scores)\n        print(\"%d-fold Cross Validation Accuracy: %0.2f (+/- %0.2f)\" % (k, scores.mean() * 100, scores.std() * 200))", "response": "Evaluate the classifier on the FULL training dataset"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cells():\n    '''\n    # Slow\n    '''\n\n    import time\n\n    def query_db():\n        time.sleep(5)\n        return [1, 2, 3, 4, 5]\n\n    def clean(data):\n        time.sleep(2)\n        return [x for x in data if x % 2 == 0]\n\n    def myfilter(data):\n        time.sleep(2)\n        return [x for x in data if x >= 3]\n\n    '''\n    '''\n\n    rows = query_db()\n\n    '''\n    '''\n\n    data = clean(rows)\n\n    '''\n    '''\n\n    data, len(data)", "response": "Return a list of cells in the alphabetical order."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_func(func_name, module_pathname):\n\n    if sys.version_info[0] >= 3:\n        if sys.version_info[1] >= 6:\n            import importlib.util\n            spec = importlib.util.spec_from_file_location('module_name', module_pathname)\n            cells_module = importlib.util.module_from_spec(spec)\n            spec.loader.exec_module(cells_module)\n            return getattr(cells_module, func_name)\n        elif sys.version_info[1] >= 4:\n            import importlib.machinery\n            module = importlib.machinery.SourceFileLoader('module_name', module_pathname).load_module()\n            return getattr(module, func_name)\n\n    fatal('Python version {} not supported'.format(sys.version))", "response": "Get function from module\nridge"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cell_hash(self, cell, cell_index):\n        s = '{uid} {cell} {index}'.format(uid=self.uid,\n                                          cell=str(cell.source),\n                                          index=cell_index).encode('utf-8')\n\n        hash = hashlib.sha1(s).hexdigest()[:8]\n        return hash", "response": "Compute cell hash based on cell index and cell content\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun cell with caching", "response": "def run_cell(self, cell, cell_index=0):\n        \"\"\"\n        Run cell with caching\n        :param cell: cell to run\n        :param cell_index: cell index (optional)\n        :return:\n        \"\"\"\n\n        hash = self.cell_hash(cell, cell_index)\n        fname_session = '/tmp/pynb-cache-{}-session.dill'.format(hash)\n        fname_value = '/tmp/pynb-cache-{}-value.dill'.format(hash)\n        cell_snippet = str(\" \".join(cell.source.split())).strip()[:40]\n\n        if self.disable_cache:\n            logging.info('Cell {}: Running: \"{}..\"'.format(hash, cell_snippet))\n            return super().run_cell(cell, cell_index)\n\n        if not self.ignore_cache:\n            if self.cache_valid and os.path.isfile(fname_session) and os.path.isfile(fname_value):\n                logging.info('Cell {}: Loading: \"{}..\"'.format(hash, cell_snippet))\n                self.prev_fname_session = fname_session\n                with open(fname_value, 'rb') as f:\n                    value = dill.load(f)\n                    return value\n\n        # If cache does not exist or not valid:\n        #\n        # 1) Invalidate subsequent cell caches\n        # 2) Load session from previous cached cell (if existing)\n        # 3) Run cell\n        # 4) Cache cell session\n        # 5) Cache cell value\n\n        logging.info('Cell {}: Running: \"{}..\"'.format(hash, cell_snippet))\n\n        # 1) Invalidate subsequent cell caches\n        self.cache_valid = False\n\n        # 2) Load session from previous cached cell (if existing and required)\n        if self.prev_fname_session:\n            if self.prev_fname_session_loaded != self.prev_fname_session:\n                self.session_load(hash, self.prev_fname_session)\n\n        # 2) Run cell\n        value = super().run_cell(cell, cell_index)\n\n        # We make sure that injected cells do not interfere with the cell index...\n        # value[0]['content']['execution_count'] = cell_index\n\n        # 3) Cache cell session\n        cached = self.session_dump(cell, hash, fname_session)\n\n        # 4) Cache cell value, if no errors while dumping the cell session in 3).\n\n        if cached:\n            self.prev_fname_session_loaded = fname_session\n            self.prev_fname_session = fname_session\n\n            logging.debug('Cell {}: dumping value to {}'.format(hash, fname_value))\n\n            with open(fname_value, 'wb') as f:\n                dill.dump(value, f)\n\n            logging.debug('Cell {}: cached'.format(hash))\n\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef session_load(self, hash, fname_session):\n\n        logging.debug('Cell {}: loading session from {}'.format(hash, fname_session))\n\n        # 'dill.settings[\"recurse\"] = True',\n        # 'dill.settings[\"byref\"] = True',\n\n        inject_code = ['import dill',\n                       'dill.load_session(filename=\"{}\")'.format(fname_session),\n                       ]\n\n        inject_cell = nbf.v4.new_code_cell('\\n'.join(inject_code))\n        super().run_cell(inject_cell)", "response": "Load ipython session from file"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef session_dump(self, cell, hash, fname_session):\n\n        logging.debug('Cell {}: Dumping session to {}'.format(hash, fname_session))\n\n        inject_code = ['import dill',\n                       'dill.dump_session(filename=\"{}\")'.format(fname_session),\n                       ]\n\n        inject_cell = nbf.v4.new_code_cell('\\n'.join(inject_code))\n        reply, outputs = super().run_cell(inject_cell)\n\n        errors = list(filter(lambda out: out.output_type == 'error', outputs))\n        if len(errors):\n            logging.info('Cell {}: Warning: serialization failed, cache disabled'.format(hash))\n            logging.debug(\n                'Cell {}: Serialization error: {}'.format(hash, CellExecutionError.from_cell_and_msg(cell, errors[0])))\n\n            # disable attempts to retrieve cache for subsequent cells\n            self.disable_cache = True\n\n            # remove partial cache for current cell\n            os.remove(fname_session)\n\n            return False\n\n        return True", "response": "Dump ipython session to file"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses the source code of a function and add it to the internal notebook.", "response": "def add(self, func, **kwargs):\n        \"\"\"\n        Parse func's function source code as Python and Markdown cells.\n        :param func: Python function to parse\n        :param kwargs: variables to inject as first Python cell\n        :return:\n        \"\"\"\n\n        params = set(kwargs.keys())\n        func_params = set(inspect.getargspec(func).args)\n\n        # ignore self, which is present when extending Notebook.\n        if 'self' in func_params:\n            func_params.remove('self')\n\n        if params != func_params:\n            fatal('Params {} not matching cells function params {}'.format(list(params), list(func_params)))\n\n        lines = inspect.getsourcelines(func)[0][1:]\n\n        buffer = \"\"\n        indent_count = None\n        inside_markdown = False\n        return_found = False\n\n        for line in lines:\n\n            # remove base indentation of function 'func'\n            if len(line.strip()) > 0:\n                if not indent_count:\n                    indent_count = 0\n                    for c in line:\n                        if c not in [' ', '\\t']:\n                            break\n                        else:\n                            indent_count += 1\n                line = line[indent_count:]\n\n            if not inside_markdown and line.strip() == \"return\":\n                logging.info('Encountered \"return\" statement, ignoring the rest of the notebook.')\n                break\n\n            if line.strip() == \"'''\":  # if md block begin/end, or new cell...\n                if len(buffer.strip()) > 0:\n                    if not inside_markdown:  # if md block begin: new markdown block! flush buffer\n                        self.add_cell_code(buffer)\n                    else:  # if md block end: markdown block completed! flush buffer\n                        self.add_cell_markdown(buffer)\n                buffer = \"\"\n                inside_markdown = not inside_markdown\n            else:\n                buffer += line\n\n        if len(buffer.strip()) > 0:\n            if not inside_markdown:\n                self.add_cell_code(buffer)\n            else:\n                self.add_cell_markdown(buffer)\n\n        if len(kwargs) > 0:\n            # We have parameters to inject into the notebook.\n            # If the first cell is Markdown, assume that is the title and\n            # insert parameters as 2nd cell. Otherwise, as 1st cell.\n            if len(self.nb['cells']) > 0 and self.nb['cells'][0].cell_type == 'markdown':\n                self.add_cell_params(kwargs, 1)\n            else:\n                self.add_cell_params(kwargs, 0)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding cell of Python parameters", "response": "def add_cell_params(self, params, pos=None):\n        \"\"\"\n        Add cell of Python parameters\n        :param params: parameters to add\n        :return:\n        \"\"\"\n\n        self.params = params\n        cell_str = '# Parameters:\\n'\n        for k, v in params.items():\n            cell_str += \"{} = {}\\n\".format(k, repr(v))\n        self.add_cell_code(cell_str, pos)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_cell_footer(self):\n\n        # check if there's already a cell footer... if true, do not add a second cell footer.\n        # this situation happens when exporting to ipynb and then importing from ipynb.\n\n        logging.info('Adding footer cell')\n\n        for cell in self.nb['cells']:\n            if cell.cell_type == 'markdown':\n                if 'pynb_footer_tag' in cell.source:\n                    logging.debug('Footer cell already present')\n                    return\n\n        m = \"\"\"\n\n            ---\n            * **Notebook class name**: {class_name}\n            * **Notebook cells name**: {cells_name}\n            * **Execution time**: {exec_begin}\n            * **Execution duration**: {exec_time:.2f}s\n            * **Command line**: {argv}\n            [//]: # (pynb_footer_tag)\n            \"\"\"\n        self.add_cell_markdown(\n            m.format(exec_time=self.exec_time, exec_begin=self.exec_begin_dt, class_name=self.__class__.__name__,\n                     argv=str(sys.argv), cells_name=self.cells_name))", "response": "Add footer cell to notebook"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd a markdown cell to the notebook.", "response": "def add_cell_markdown(self, cell_str):\n        \"\"\"\n        Add a markdown cell\n        :param cell_str: markdown text\n        :return:\n        \"\"\"\n\n        logging.debug(\"add_cell_markdown: {}\".format(cell_str))\n        # drop spaces and taps at beginning and end of all lines\n        #cell = '\\n'.join(map(lambda x: x.strip(), cell_str.split('\\n')))\n        cell = '\\n'.join(cell_str.split('\\n'))\n        cell = nbf.v4.new_markdown_cell(cell)\n\n        self.nb['cells'].append(cell)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding Python cell to the list of code cells.", "response": "def add_cell_code(self, cell_str, pos=None):\n        \"\"\"\n        Add Python cell\n        :param cell_str: cell content\n        :return:\n        \"\"\"\n\n        cell_str = cell_str.strip()\n\n        logging.debug(\"add_cell_code: {}\".format(cell_str))\n        cell = nbf.v4.new_code_cell(cell_str)\n\n        if pos is None:\n            self.nb['cells'].append(cell)\n        else:\n            self.nb['cells'].insert(pos, cell)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nexecute the notebook with the given uid.", "response": "def process(self, uid, add_footer=False, no_exec=False, disable_cache=False, ignore_cache=False):\n        \"\"\"\n        Execute notebook\n        :return: self\n        \"\"\"\n\n        self.exec_begin = time.perf_counter()\n        self.exec_begin_dt = datetime.datetime.now()\n\n        ep = CachedExecutePreprocessor(timeout=None, kernel_name='python3')\n        ep.disable_cache = disable_cache\n        ep.ignore_cache = ignore_cache\n        ep.uid = uid\n\n        # Execute the notebook\n\n        if not no_exec:\n            with warnings.catch_warnings():\n                # On MacOS, annoying warning \"RuntimeWarning: Failed to set sticky bit on\"\n                # Let's suppress it.\n                warnings.simplefilter(\"ignore\")\n                ep.preprocess(self.nb, {'metadata': {'path': '.'}})\n\n        self.exec_time = time.perf_counter() - self.exec_begin\n\n        if add_footer:\n            self.add_cell_footer()\n\n        if not no_exec:\n            logging.info('Execution time: {0:.2f}s'.format(self.exec_time))\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nexporting notebook to. ipynb file.", "response": "def export_ipynb(self, pathname):\n        \"\"\"\n        Export notebook to .ipynb file\n        :param pathname: output filename\n        :return:\n        \"\"\"\n\n        if pathname == '-':\n            nbf.write(self.nb, sys.__stdout__)\n        else:\n            with codecs.open(pathname, 'w', encoding='utf-8') as f:\n                ret = nbf.write(self.nb, f)\n                pass\n\n        logging.info(\"Jupyter notebook exported to '{}'\".format(pathname))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nexports notebook to. html file", "response": "def export_html(self, pathname):\n        \"\"\"\n        Export notebook to .html file\n        :param pathname: output filename\n        :return:\n        \"\"\"\n\n        html_exporter = HTMLExporter()\n\n        (body, resources) = html_exporter.from_notebook_node(self.nb)\n\n        if pathname == '-':\n            sys.__stdout__.write(body)\n        else:\n            with open(pathname, 'w') as f:\n                f.write(body)\n\n        logging.info(\"HTML notebook exported to '{}'\".format(pathname))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting self. cells to function cells in file pathname. py", "response": "def set_cells(self, cells_location):\n        \"\"\"\n        Set self.cells to function :cells in file pathname.py\n        :param cells_location: cells location, format 'pathname.py:cells'\n        :return:\n        \"\"\"\n\n        if ':' in cells_location:\n            pathname, func_name = cells_location.split(':')\n        else:\n            pathname = cells_location\n            func_name = 'cells'\n\n        check_isfile(pathname)\n\n        try:\n            self.cells = get_func(func_name, pathname)\n        except SyntaxError as e:\n            fatal(traceback.format_exc(limit=1))\n\n        return pathname, func_name"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses arguments for the notebook.", "response": "def parse_args(self, **kwargs):\n        \"\"\"\n        Parse arguments\n        :param kwargs: optional params\n        :return:\n        \"\"\"\n\n        self.parser.add_argument('cells', help='path to cells function. Format: PATHNAME.PY[:FUNCTION_NAME]', nargs='?')\n        self.parser.add_argument('--disable-cache', action=\"store_true\", default=False, help='disable execution cache')\n        self.parser.add_argument('--ignore-cache', action=\"store_true\", default=False, help='ignore existing cache')\n        self.parser.add_argument('--no-exec', action=\"store_true\", default=False, help='do not execute notebook')\n        self.parser.add_argument('--param', action='append', help='notebook parameter. Format: NAME=VALUE')\n        self.add_argument('--import-ipynb', help='import from Jupyter notebook')\n        self.add_argument('--export-html', help='export to HTML format')\n        self.add_argument('--export-ipynb', help='export to Jupyter notebook')\n        self.add_argument('--export-pynb', help='export to Python notebook')\n        self.add_argument('--kernel', default=None, help='set kernel')\n        self.add_argument('--log-level', help='set log level')\n        self.add_argument('--check-syntax', action=\"store_true\", default=False, help='check Python syntax')\n        self.add_argument('--disable-footer', action=\"store_true\", default=False,\n                          help='do not append Markdown footer to Jupyter notebook')\n\n        if len(sys.argv) == 1 and self.__class__ == Notebook:\n            # no parameters and Notebook class not extended:\n            # print help and exit.\n            self.parser.print_help()\n            print()\n            sys.exit(1)\n\n        self.args = self.parser.parse_args()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget a kernel specification dictionary given a kernel name", "response": "def get_kernelspec(self, name):\n        \"\"\"Get a kernel specification dictionary given a kernel name\n        \"\"\"\n        ksm = KernelSpecManager()\n        kernelspec = ksm.get_kernel_spec(name).to_dict()\n        kernelspec['name'] = name\n        kernelspec.pop('argv')\n        return kernelspec"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run(self):\n\n        if not self.args:\n            self.parse_args()\n\n        if self.args.log_level:\n            logging.getLogger().setLevel(logging.getLevelName(self.args.log_level))\n            logging.debug('Enabled {} logging level'.format(self.args.log_level))\n\n        if self.args.import_ipynb:\n            check_isfile(self.args.import_ipynb)\n            logging.info('Loading Jupyter notebook {}'.format(self.args.import_ipynb))\n            self.nb = nbf.read(self.args.import_ipynb, as_version=4)\n            uid = self.args.import_ipynb\n        else:\n            uid = self.load_cells_params()\n\n        logging.debug(\"Unique id: '{}'\".format(uid))\n        logging.info('Disable cache: {}'.format(self.args.disable_cache))\n        logging.info('Ignore cache: {}'.format(self.args.ignore_cache))\n\n        if self.args.export_pynb and not self.args.no_exec:\n            fatal('--export-pynb requires --no-exec')\n\n        if self.args.kernel:\n            self.set_kernel(self.args.kernel)\n\n        self.process(uid=uid,\n                     add_footer=not self.args.disable_footer,\n                     no_exec=self.args.no_exec,\n                     disable_cache=self.args.disable_cache,\n                     ignore_cache=self.args.ignore_cache)\n\n        if self.args.export_html:\n            self.export_html(self.args.export_html)\n\n        if self.args.export_ipynb:\n            self.export_ipynb(self.args.export_ipynb)\n\n        if self.args.export_pynb:\n            self.export_pynb(self.args.export_pynb)", "response": "Run the notebook as an application."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef inc_version():\n\n    new_version = version.__version__\n\n    values = list(map(lambda x: int(x), new_version.split('.')))\n    values[2] += 1\n\n    with open('version.py', 'w') as f:\n        f.write('__version__ = \"{}.{}.{}\"\\n'.format(values[0], values[1], values[2]))\n    with open('pynb/version.py', 'w') as f:\n        f.write('__version__ = \"{}.{}.{}\"\\n'.format(values[0], values[1], values[2]))\n\n    importlib.reload(version)\n\n    print('Current version: {}'.format(version.__version__))\n\n    return values", "response": "Increment the micro release version in version. py and re - import it."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck that all changes are committed to origin", "response": "def git_check():\n    \"\"\"\n    Check that all changes , besides versioning files, are committed\n    :return:\n    \"\"\"\n\n    # check that changes staged for commit are pushed to origin\n    output = local('git diff --name-only | egrep -v \"^(pynb/version.py)|(version.py)$\" | tr \"\\\\n\" \" \"',\n                   capture=True).strip()\n    if output:\n        fatal('Stage for commit and commit all changes first: {}'.format(output))\n\n    output = local('git diff --cached --name-only | egrep -v \"^(pynb/version.py)|(version.py)$\" | tr \"\\\\n\" \" \"',\n                   capture=True).strip()\n    if output:\n        fatal('Commit all changes first: {}'.format(output))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npushes new version and corresponding tag to origin", "response": "def git_push():\n    \"\"\"\n    Push new version and corresponding tag to origin\n    :return:\n    \"\"\"\n\n    # get current version\n    new_version = version.__version__\n    values = list(map(lambda x: int(x), new_version.split('.')))\n\n    # Push to origin new version and corresponding tag:\n    # * commit new version\n    # * create tag\n    # * push version,tag to origin\n    local('git add pynb/version.py version.py')\n\n    local('git commit -m \"updated version\"')\n    local('git tag {}.{}.{}'.format(values[0], values[1], values[2]))\n    local('git push origin --tags')\n    local('git push')"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrelease new version to pypi", "response": "def release():\n    \"\"\"\n    Release new package version to pypi\n    :return:\n    \"\"\"\n\n    from secrets import pypi_auth\n\n    # Check that all changes are committed before creating a new version\n    git_check()\n\n    # Test package\n    test()\n\n    # Increment version\n    inc_version()\n\n    # Commit new version, create tag for version and push everything to origin\n    git_push()\n\n    # Build and publish package\n    build()\n    pathname = 'dist/pynb-{}.tar.gz'.format(version.__version__)\n    docker_exec('twine upload -u {user} -p {pass} {pathname}'.format(pathname=pathname, **pypi_auth))\n\n    # Remove temporary files\n    clean()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cells(a, b):\n    '''\n    # Sum\n    '''\n\n    a, b = int(a), int(b)\n\n    '''\n    '''\n\n    a + b", "response": "Sum the cells of two sequences."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read(self, file):\n        content = self._read_content(file)\n        self._validate(content)\n        self._parse(content)\n\n        return self", "response": "Reads the captions file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _parse_timeframe_line(self, line):\n        tf = self._validate_timeframe_line(line)\n        if not tf:\n            raise MalformedCaptionError('Invalid time format')\n\n        return tf.group(1), tf.group(2)", "response": "Parse a timeframe line and return start and end timestamps."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef from_srt(cls, file):\n        parser = SRTParser().read(file)\n        return cls(file=file, captions=parser.captions)", "response": "Reads captions from a file in SubRip format."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread captions from a file in YouTube SBV format.", "response": "def from_sbv(cls, file):\n        \"\"\"Reads captions from a file in YouTube SBV format.\"\"\"\n        parser = SBVParser().read(file)\n        return cls(file=file, captions=parser.captions)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read(cls, file):\n        parser = WebVTTParser().read(file)\n        return cls(file=file, captions=parser.captions, styles=parser.styles)", "response": "Reads a WebVTT captions file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef save(self, output=''):\n        self.file = self._get_output_file(output)\n        with open(self.file, 'w', encoding='utf-8') as f:\n            self.write(f)", "response": "Save the current object to a file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef total_length(self):\n        if not self._captions:\n            return 0\n        return int(self._captions[-1].end_in_seconds) - int(self._captions[0].start_in_seconds)", "response": "Returns the total length of the captions."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsegments the captions based on a number of seconds.", "response": "def segment(self, webvtt, output='', seconds=SECONDS, mpegts=MPEGTS):\n        \"\"\"Segments the captions based on a number of seconds.\"\"\"\n        if isinstance(webvtt, str):\n            # if a string is supplied we parse the file\n            captions = WebVTT().read(webvtt).captions\n        elif not self._validate_webvtt(webvtt):\n            raise InvalidCaptionsError('The captions provided are invalid')\n        else:\n            # we expect to have a webvtt object\n            captions = webvtt.captions\n\n        self._total_segments = 0 if not captions else int(ceil(captions[-1].end_in_seconds / seconds))\n        self._output_folder = output\n        self._seconds = seconds\n        self._mpegts = mpegts\n\n        output_folder = os.path.join(os.getcwd(), output)\n        if not os.path.exists(output_folder):\n            os.makedirs(output_folder)\n\n        self._slice_segments(captions)\n        self._write_segments()\n        self._write_manifest()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cli(\n    paths,\n    dbname,\n    separator,\n    quoting,\n    skip_errors,\n    replace_tables,\n    table,\n    extract_column,\n    date,\n    datetime,\n    datetime_format,\n    primary_key,\n    fts,\n    index,\n    shape,\n    filename_column,\n    no_index_fks,\n    no_fulltext_fks,\n):\n    \"\"\"\n    PATHS: paths to individual .csv files or to directories containing .csvs\n\n    DBNAME: name of the SQLite database file to create\n    \"\"\"\n    # make plural for more readable code:\n    extract_columns = extract_column\n    del extract_column\n\n    if extract_columns:\n        click.echo(\"extract_columns={}\".format(extract_columns))\n    if dbname.endswith(\".csv\"):\n        raise click.BadParameter(\"dbname must not end with .csv\")\n    if \".\" not in dbname:\n        dbname += \".db\"\n\n    db_existed = os.path.exists(dbname)\n\n    conn = sqlite3.connect(dbname)\n\n    dataframes = []\n    csvs = csvs_from_paths(paths)\n    sql_type_overrides = None\n    for name, path in csvs.items():\n        try:\n            df = load_csv(path, separator, skip_errors, quoting, shape)\n            df.table_name = table or name\n            if filename_column:\n                df[filename_column] = name\n                if shape:\n                    shape += \",{}\".format(filename_column)\n            sql_type_overrides = apply_shape(df, shape)\n            apply_dates_and_datetimes(df, date, datetime, datetime_format)\n            dataframes.append(df)\n        except LoadCsvError as e:\n            click.echo(\"Could not load {}: {}\".format(path, e), err=True)\n\n    click.echo(\"Loaded {} dataframes\".format(len(dataframes)))\n\n    # Use extract_columns to build a column:(table,label) dictionary\n    foreign_keys = {}\n    for col in extract_columns:\n        bits = col.split(\":\")\n        if len(bits) == 3:\n            foreign_keys[bits[0]] = (bits[1], bits[2])\n        elif len(bits) == 2:\n            foreign_keys[bits[0]] = (bits[1], \"value\")\n        else:\n            foreign_keys[bits[0]] = (bits[0], \"value\")\n\n    # Now we have loaded the dataframes, we can refactor them\n    created_tables = {}\n    refactored = refactor_dataframes(\n        conn, dataframes, foreign_keys, not no_fulltext_fks\n    )\n    for df in refactored:\n        # This is a bit trickier because we need to\n        # create the table with extra SQL for foreign keys\n        if replace_tables and table_exists(conn, df.table_name):\n            drop_table(conn, df.table_name)\n        if table_exists(conn, df.table_name):\n            df.to_sql(df.table_name, conn, if_exists=\"append\", index=False)\n        else:\n            to_sql_with_foreign_keys(\n                conn,\n                df,\n                df.table_name,\n                foreign_keys,\n                sql_type_overrides,\n                primary_keys=primary_key,\n                index_fks=not no_index_fks,\n            )\n            created_tables[df.table_name] = df\n        if index:\n            for index_defn in index:\n                add_index(conn, df.table_name, index_defn)\n\n    # Create FTS tables\n    if fts:\n        fts_version = best_fts_version()\n        if not fts_version:\n            conn.close()\n            raise click.BadParameter(\n                \"Your SQLite version does not support any variant of FTS\"\n            )\n        # Check that columns make sense\n        for table, df in created_tables.items():\n            for fts_column in fts:\n                if fts_column not in df.columns:\n                    raise click.BadParameter(\n                        'FTS column \"{}\" does not exist'.format(fts_column)\n                    )\n\n        generate_and_populate_fts(conn, created_tables.keys(), fts, foreign_keys)\n\n    conn.close()\n\n    if db_existed:\n        click.echo(\n            \"Added {} CSV file{} to {}\".format(\n                len(csvs), \"\" if len(csvs) == 1 else \"s\", dbname\n            )\n        )\n    else:\n        click.echo(\n            \"Created {} from {} CSV file{}\".format(\n                dbname, len(csvs), \"\" if len(csvs) == 1 else \"s\"\n            )\n        )", "response": "Create a new n - item dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndiscover the most advanced supported SQLite FTS version", "response": "def best_fts_version():\n    \"Discovers the most advanced supported SQLite FTS version\"\n    conn = sqlite3.connect(\":memory:\")\n    for fts in (\"FTS5\", \"FTS4\", \"FTS3\"):\n        try:\n            conn.execute(\"CREATE VIRTUAL TABLE v USING {} (t TEXT);\".format(fts))\n            return fts\n        except sqlite3.OperationalError:\n            continue\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_click_commands(module, cli, command_dict, namespaced):\n    module_commands = [\n        item for item in getmembers(module)\n        if isinstance(item[1], BaseCommand)\n    ]\n    options = command_dict.get('config', {})\n    namespace = command_dict.get('namespace')\n    for name, function in module_commands:\n        f_options = options.get(name, {})\n        command_name = f_options.get('name', getattr(function, 'name', name))\n        if namespace:\n            command_name = '{}_{}'.format(namespace, command_name)\n        elif namespaced:\n            module_namespace = module.__name__.split('.')[-1]\n            command_name = '{}_{}'.format(module_namespace, command_name)\n        function.short_help = f_options.get('help_text', function.short_help)\n        cli.add_command(function, name=command_name)", "response": "Loads all click commands from a module"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload the commands defined in manage file", "response": "def load_commands(cli, manage_dict):\n    \"\"\"Loads the commands defined in manage file\"\"\"\n    namespaced = manage_dict.get('namespaced')\n\n    # get click commands\n    commands = manage_dict.get('click_commands', [])\n    for command_dict in commands:\n        root_module = import_string(command_dict['module'])\n        group = cli.manage_groups.get(command_dict.get('group'), cli)\n        if getattr(root_module, '__path__', None):\n            # This is a package\n            iter_modules = pkgutil.iter_modules(\n                root_module.__path__, prefix=root_module.__name__ + '.'\n            )\n            submodules_names = [item[1] for item in iter_modules]\n            submodules = [import_string(name) for name in submodules_names]\n            for module in submodules:\n                add_click_commands(module, group, command_dict, namespaced)\n        else:\n            # a single file module\n            add_click_commands(root_module, group, command_dict, namespaced)\n\n    # get inline commands\n    commands = manage_dict.get('inline_commands', [])\n    for command_dict in commands:\n        name = command_dict['name']\n        help_text = command_dict.get('help_text')\n        options = command_dict.get('options', {})\n        arguments = command_dict.get('arguments', {})\n        context = command_dict.get('context', [])\n        code = command_dict['code']\n        group = cli.manage_groups.get(command_dict.get('group'), cli)\n        group.add_command(\n            make_command_from_string(\n                code=code,\n                cmd_context=get_context(context),\n                options=options,\n                arguments=arguments,\n                help_text=help_text\n            ),\n            name=name\n        )\n\n    # get function commands\n    commands = manage_dict.get('function_commands', [])\n    for command_dict in commands:\n        name = command_dict['name']\n        help_text = command_dict.get('help_text')\n        options = command_dict.get('options', {})\n        arguments = command_dict.get('arguments', {})\n        function = import_string(command_dict['function'])\n        group = cli.manage_groups.get(command_dict.get('group'), cli)\n        group.add_command(\n            make_command_from_function(\n                function=function,\n                options=options,\n                arguments=arguments,\n                help_text=help_text\n            ),\n            name=name\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nimport all submodules for a package or module name", "response": "def import_submodules(name, submodules=None):\n    \"\"\"Import all submodules for a package/module name\"\"\"\n    sys.path.insert(0, name)\n    if submodules:\n        for submodule in submodules:\n            import_string('{0}.{1}'.format(name, submodule))\n    else:\n        for item in pkgutil.walk_packages([name]):\n            import_string('{0}.{1}'.format(name, item[1]))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef init(banner, hidden, backup):\n    manage_file = HIDDEN_MANAGE_FILE if hidden else MANAGE_FILE\n    if os.path.exists(manage_file):\n        if not click.confirm('Rewrite {0}?'.format(manage_file)):\n            return\n\n        if backup:\n            bck = '.bck_{0}'.format(manage_file)\n            with open(manage_file, 'r') as source, open(bck, 'w') as bck_file:\n                bck_file.write(source.read())\n\n    with open(manage_file, 'w') as output:\n        data = default_manage_dict\n        if banner:\n            data['shell']['banner']['message'] = banner\n        output.write(yaml.dump(data, default_flow_style=False))", "response": "Initialize a manage shell in current directory"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nshow the parsed manage file - V shows version", "response": "def debug(version=False):\n    \"\"\"Shows the parsed manage file -V shows version\"\"\"\n    if version:\n        print(__version__)\n        return\n    print(json.dumps(MANAGE_DICT, indent=2))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfind files that do not exist are not in the repo or are directories.", "response": "def find_invalid_filenames(filenames, repository_root):\n    \"\"\"Find files that does not exist, are not in the repo or are directories.\n\n    Args:\n      filenames: list of filenames to check\n      repository_root: the absolute path of the repository's root.\n\n    Returns: A list of errors.\n    \"\"\"\n    errors = []\n    for filename in filenames:\n        if not os.path.abspath(filename).startswith(repository_root):\n            errors.append((filename, 'Error: File %s does not belong to '\n                           'repository %s' % (filename, repository_root)))\n        if not os.path.exists(filename):\n            errors.append((filename,\n                           'Error: File %s does not exist' % (filename, )))\n        if os.path.isdir(filename):\n            errors.append((filename,\n                           'Error: %s is a directory. Directories are'\n                           ' not yet supported' % (filename, )))\n\n    return errors"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the configuration file either from the repository or the default.", "response": "def get_config(repo_root):\n    \"\"\"Gets the configuration file either from the repository or the default.\"\"\"\n    config = os.path.join(os.path.dirname(__file__), 'configs', 'config.yaml')\n\n    if repo_root:\n        repo_config = os.path.join(repo_root, '.gitlint.yaml')\n        if os.path.exists(repo_config):\n            config = repo_config\n\n    with open(config) as f:\n        # We have to read the content first as yaml hangs up when reading from\n        # MockOpen\n        content = f.read()\n        # Yaml.load will return None when the input is empty.\n        if not content:\n            yaml_config = {}\n        else:\n            yaml_config = yaml.load(content)\n\n    return linters.parse_yaml_config(yaml_config, repo_root)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef format_comment(comment_data):\n    format_pieces = []\n    # Line and column information\n    if 'line' in comment_data:\n        format_pieces.append('line {line}')\n    if 'column' in comment_data:\n        if format_pieces:\n            format_pieces.append(', ')\n        format_pieces.append('col {column}')\n    if format_pieces:\n        format_pieces.append(': ')\n\n    # Severity and Id information\n    if 'severity' in comment_data:\n        format_pieces.append('{severity}: ')\n\n    if 'message_id' in comment_data:\n        format_pieces.append('[{message_id}]: ')\n\n    # The message\n    if 'message' in comment_data:\n        format_pieces.append('{message}')\n\n    return ''.join(format_pieces).format(**comment_data)", "response": "Formats the data returned by the linters."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_vcs_root():\n    for vcs in (git, hg):\n        repo_root = vcs.repository_root()\n        if repo_root:\n            return vcs, repo_root\n\n    return (None, None)", "response": "Returns the vcs module and the root of the repository."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef process_file(vcs, commit, force, gitlint_config, file_data):\n    filename, extra_data = file_data\n\n    if force:\n        modified_lines = None\n    else:\n        modified_lines = vcs.modified_lines(\n            filename, extra_data, commit=commit)\n    result = linters.lint(filename, modified_lines, gitlint_config)\n    result = result[filename]\n\n    return filename, result", "response": "Lint the file and return the filename and the results from the linters."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef last_commit():\n    try:\n        root = subprocess.check_output(\n            ['hg', 'parent', '--template={node}'],\n            stderr=subprocess.STDOUT).strip()\n        # Convert to unicode first\n        return root.decode('utf-8')\n    except subprocess.CalledProcessError:\n        return None", "response": "Returns the SHA1 of the last commit."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of files that have been modified since the last commit.", "response": "def modified_files(root, tracked_only=False, commit=None):\n    \"\"\"Returns a list of files that has been modified since the last commit.\n\n    Args:\n      root: the root of the repository, it has to be an absolute path.\n      tracked_only: exclude untracked files when True.\n      commit: SHA1 of the commit. If None, it will get the modified files in the\n        working copy.\n\n    Returns: a dictionary with the modified files as keys, and additional\n      information as value. In this case it adds the status returned by\n      hg status.\n    \"\"\"\n    assert os.path.isabs(root), \"Root has to be absolute, got: %s\" % root\n\n    command = ['hg', 'status']\n    if commit:\n        command.append('--change=%s' % commit)\n\n    # Convert to unicode and split\n    status_lines = subprocess.check_output(command).decode('utf-8').split(\n        os.linesep)\n\n    modes = ['M', 'A']\n    if not tracked_only:\n        modes.append(r'\\?')\n    modes_str = '|'.join(modes)\n\n    modified_file_status = utils.filter_lines(\n        status_lines,\n        r'(?P<mode>%s) (?P<filename>.+)' % modes_str,\n        groups=('filename', 'mode'))\n\n    return dict((os.path.join(root, filename), mode)\n                for filename, mode in modified_file_status)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef modified_lines(filename, extra_data, commit=None):\n    if extra_data is None:\n        return []\n    if extra_data != 'M':\n        return None\n\n    command = ['hg', 'diff', '-U', '0']\n    if commit:\n        command.append('--change=%s' % commit)\n    command.append(filename)\n\n    # Split as bytes, as the output may have some non unicode characters.\n    diff_lines = subprocess.check_output(command).split(\n        os.linesep.encode('utf-8'))\n    diff_line_numbers = utils.filter_lines(\n        diff_lines,\n        br'@@ -\\d+,\\d+ \\+(?P<start_line>\\d+),(?P<lines>\\d+) @@',\n        groups=('start_line', 'lines'))\n    modified_line_numbers = []\n    for start_line, lines in diff_line_numbers:\n        start_line = int(start_line)\n        lines = int(lines)\n        modified_line_numbers.extend(range(start_line, start_line + lines))\n\n    return modified_line_numbers", "response": "Returns the lines that have been modified for this file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef lint(filename):\n    config = ConfigParser.ConfigParser()\n    try:\n        config.read(filename)\n        return 0\n    except ConfigParser.Error as error:\n        print('Error: %s' % error)\n        return 1\n    except:\n        print('Unexpected Error')\n        return 2", "response": "Lints an INI file returning 0 in case of success."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef missing_requirements_command(missing_programs, installation_string,\n                                 filename, unused_lines):\n    \"\"\"Pseudo-command to be used when requirements are missing.\"\"\"\n    verb = 'is'\n    if len(missing_programs) > 1:\n        verb = 'are'\n    return {\n        filename: {\n            'skipped': [\n                '%s %s not installed. %s' % (', '.join(missing_programs), verb,\n                                             installation_string)\n            ]\n        }\n    }", "response": "Pseudo - command to be used when requirements are missing."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexecutes a lint tool and filters the output of the file with the given regular expression.", "response": "def lint_command(name, program, arguments, filter_regex, filename, lines):\n    \"\"\"Executes a lint program and filter the output.\n\n    Executes the lint tool 'program' with arguments 'arguments' over the file\n    'filename' returning only those lines matching the regular expression\n    'filter_regex'.\n\n    Args:\n      name: string: the name of the linter.\n      program: string: lint program.\n      arguments: list[string]: extra arguments for the program.\n      filter_regex: string: regular expression to filter lines.\n      filename: string: filename to lint.\n      lines: list[int]|None: list of lines that we want to capture. If None,\n        then all lines will be captured.\n\n    Returns: dict: a dict with the extracted info from the message.\n    \"\"\"\n    output = utils.get_output_from_cache(name, filename)\n\n    if output is None:\n        call_arguments = [program] + arguments + [filename]\n        try:\n            output = subprocess.check_output(\n                call_arguments, stderr=subprocess.STDOUT)\n        except subprocess.CalledProcessError as error:\n            output = error.output\n        except OSError:\n            return {\n                filename: {\n                    'error': [('Could not execute \"%s\".%sMake sure all ' +\n                               'required programs are installed') %\n                              (' '.join(call_arguments), os.linesep)]\n                }\n            }\n        output = output.decode('utf-8')\n        utils.save_output_in_cache(name, filename, output)\n\n    output_lines = output.split(os.linesep)\n\n    if lines is None:\n        lines_regex = r'\\d+'\n    else:\n        lines_regex = '|'.join(map(str, lines))\n    lines_regex = '(%s)' % lines_regex\n\n    groups = ('line', 'column', 'message', 'severity', 'message_id')\n    filtered_lines = utils.filter_lines(\n        output_lines,\n        filter_regex.format(lines=lines_regex, filename=re.escape(filename)),\n        groups=groups)\n\n    result = []\n    for data in filtered_lines:\n        comment = dict(p for p in zip(groups, data) if p[1] is not None)\n        if 'line' in comment:\n            comment['line'] = int(comment['line'])\n        if 'column' in comment:\n            comment['column'] = int(comment['column'])\n        if 'severity' in comment:\n            comment['severity'] = comment['severity'].title()\n        result.append(comment)\n\n    return {filename: {'comments': result}}"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _replace_variables(data, variables):\n    formatter = string.Formatter()\n    return [formatter.vformat(item, [], variables) for item in data]", "response": "Replace the format variables in all items of data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert a dictionary ( parsed Yaml ) to the internal representation.", "response": "def parse_yaml_config(yaml_config, repo_home):\n    \"\"\"Converts a dictionary (parsed Yaml) to the internal representation.\"\"\"\n    config = collections.defaultdict(list)\n\n    variables = {\n        'DEFAULT_CONFIGS': os.path.join(os.path.dirname(__file__), 'configs'),\n        'REPO_HOME': repo_home,\n    }\n\n    for name, data in yaml_config.items():\n        command = _replace_variables([data['command']], variables)[0]\n        requirements = _replace_variables(\n            data.get('requirements', []), variables)\n        arguments = _replace_variables(data.get('arguments', []), variables)\n\n        not_found_programs = utils.programs_not_in_path([command] +\n                                                        requirements)\n        if not_found_programs:\n            linter_command = Partial(missing_requirements_command,\n                                     not_found_programs, data['installation'])\n        else:\n            linter_command = Partial(lint_command, name, command, arguments,\n                                     data['filter'])\n        for extension in data['extensions']:\n            config[extension].append(linter_command)\n\n    return config"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlint a file. Args: filename: string: filename to lint. lines: list[int]|None: list of lines that we want to capture. If None, then all lines will be captured. config: dict[string: linter]: mapping from extension to a linter function. Returns: dict: if there were errors running the command then the field 'error' will have the reasons in a list. if the lint process was skipped, then a field 'skipped' will be set with the reasons. Otherwise, the field 'comments' will have the messages.", "response": "def lint(filename, lines, config):\n    \"\"\"Lints a file.\n\n    Args:\n        filename: string: filename to lint.\n        lines: list[int]|None: list of lines that we want to capture. If None,\n          then all lines will be captured.\n        config: dict[string: linter]: mapping from extension to a linter\n          function.\n\n    Returns: dict: if there were errors running the command then the field\n      'error' will have the reasons in a list. if the lint process was skipped,\n      then a field 'skipped' will be set with the reasons. Otherwise, the field\n      'comments' will have the messages.\n    \"\"\"\n    _, ext = os.path.splitext(filename)\n    if ext in config:\n        output = collections.defaultdict(list)\n        for linter in config[ext]:\n            linter_output = linter(filename, lines)\n            for category, values in linter_output[filename].items():\n                output[category].extend(values)\n\n        if 'comments' in output:\n            output['comments'] = sorted(\n                output['comments'],\n                key=lambda x: (x.get('line', -1), x.get('column', -1)))\n\n        return {filename: dict(output)}\n    else:\n        return {\n            filename: {\n                'skipped': [\n                    'no linter is defined or enabled for files'\n                    ' with extension \"%s\"' % ext\n                ]\n            }\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef modified_lines(filename, extra_data, commit=None):\n    if extra_data is None:\n        return []\n    if extra_data not in ('M ', ' M', 'MM'):\n        return None\n\n    if commit is None:\n        commit = '0' * 40\n    commit = commit.encode('utf-8')\n\n    # Split as bytes, as the output may have some non unicode characters.\n    blame_lines = subprocess.check_output(\n        ['git', 'blame', '--porcelain', filename]).split(\n            os.linesep.encode('utf-8'))\n    modified_line_numbers = utils.filter_lines(\n        blame_lines, commit + br' (?P<line>\\d+) (\\d+)', groups=('line', ))\n\n    return list(map(int, modified_line_numbers))", "response": "Returns the lines that have been modified for this file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef filter_lines(lines, filter_regex, groups=None):\n    pattern = re.compile(filter_regex)\n    for line in lines:\n        match = pattern.search(line)\n        if match:\n            if groups is None:\n                yield line\n            elif len(groups) == 1:\n                yield match.group(groups[0])\n            else:\n                matched_groups = match.groupdict()\n                yield tuple(matched_groups.get(group) for group in groups)", "response": "Filters out the lines not matching the regular expression."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef which(program):\n    if (os.path.isabs(program) and os.path.isfile(program)\n            and os.access(program, os.X_OK)):\n        return [program]\n\n    candidates = []\n    locations = os.environ.get(\"PATH\").split(os.pathsep)\n    for location in locations:\n        candidate = os.path.join(location, program)\n        if os.path.isfile(candidate) and os.access(candidate, os.X_OK):\n            candidates.append(candidate)\n    return candidates", "response": "Returns a list of paths where the program is found."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nopening filename for writing creating the directories if needed.", "response": "def _open_for_write(filename):\n    \"\"\"Opens filename for writing, creating the directories if needed.\"\"\"\n    dirname = os.path.dirname(filename)\n    pathlib.Path(dirname).mkdir(parents=True, exist_ok=True)\n\n    return io.open(filename, 'w')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_cache_filename(name, filename):\n    filename = os.path.abspath(filename)[1:]\n    home_folder = os.path.expanduser('~')\n    base_cache_dir = os.path.join(home_folder, '.git-lint', 'cache')\n\n    return os.path.join(base_cache_dir, name, filename)", "response": "Returns the cache location for filename and linter name."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_output_from_cache(name, filename):\n    cache_filename = _get_cache_filename(name, filename)\n    if (os.path.exists(cache_filename)\n            and os.path.getmtime(filename) < os.path.getmtime(cache_filename)):\n        with io.open(cache_filename) as f:\n            return f.read()\n\n    return None", "response": "Returns the output from the cache if still valid."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsaves output in the cache location.", "response": "def save_output_in_cache(name, filename, output):\n    \"\"\"Saves output in the cache location.\n\n    Args:\n      name: string: name of the linter.\n      filename: string: path of the filename for which we are saving the output.\n      output: string: full output (not yet filetered) of the lint command.\n    \"\"\"\n    cache_filename = _get_cache_filename(name, filename)\n    with _open_for_write(cache_filename) as f:\n        f.write(output)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the md5 hash of a string.", "response": "def md5(text):\n    \"\"\"Returns the md5 hash of a string.\"\"\"\n\n    h = hashlib.md5()\n    h.update(_unicode(text).encode(\"utf-8\"))\n\n    return h.hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cleanup_nodes(doc):\n    for node in doc.documentElement.childNodes:\n        if node.nodeType == Node.TEXT_NODE and node.nodeValue.isspace():\n            doc.documentElement.removeChild(node)\n    return doc", "response": "Removes text nodes containing only whitespace."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncollect nodes from xml. dom. Node objects about as close to limit as possible.", "response": "def _collect_nodes(limit, sender, method_name, cacheable, params=None):\n    \"\"\"\n    Returns a sequence of dom.Node objects about as close to limit as possible\n    \"\"\"\n\n    if not params:\n        params = sender._get_params()\n\n    nodes = []\n    page = 1\n    end_of_pages = False\n\n    while not end_of_pages and (not limit or (limit and len(nodes) < limit)):\n        params[\"page\"] = str(page)\n\n        tries = 1\n        while True:\n            try:\n                doc = sender._request(method_name, cacheable, params)\n                break  # success\n            except Exception as e:\n                if tries >= 3:\n                    raise e\n                # Wait and try again\n                time.sleep(1)\n                tries += 1\n\n        doc = cleanup_nodes(doc)\n\n        # break if there are no child nodes\n        if not doc.documentElement.childNodes:\n            break\n        main = doc.documentElement.childNodes[0]\n\n        if main.hasAttribute(\"totalPages\"):\n            total_pages = _number(main.getAttribute(\"totalPages\"))\n        elif main.hasAttribute(\"totalpages\"):\n            total_pages = _number(main.getAttribute(\"totalpages\"))\n        else:\n            raise Exception(\"No total pages attribute\")\n\n        for node in main.childNodes:\n            if not node.nodeType == xml.dom.Node.TEXT_NODE and (\n                not limit or (len(nodes) < limit)\n            ):\n                nodes.append(node)\n\n        if page >= total_pages:\n            end_of_pages = True\n\n        page += 1\n\n    return nodes"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _extract(node, name, index=0):\n\n    nodes = node.getElementsByTagName(name)\n\n    if len(nodes):\n        if nodes[index].firstChild:\n            return _unescape_htmlentity(nodes[index].firstChild.data.strip())\n    else:\n        return None", "response": "Extracts a value from the xml string"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nextracting all the values from the xml string. returning a list.", "response": "def _extract_all(node, name, limit_count=None):\n    \"\"\"Extracts all the values from the xml string. returning a list.\"\"\"\n\n    seq = []\n\n    for i in range(0, len(node.getElementsByTagName(name))):\n        if len(seq) == limit_count:\n            break\n\n        seq.append(_extract(node, name, i))\n\n    return seq"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _delay_call(self):\n        now = time.time()\n\n        time_since_last = now - self.last_call_time\n\n        if time_since_last < DELAY_TIME:\n            time.sleep(DELAY_TIME - time_since_last)\n\n        self.last_call_time = now", "response": "Makes sure that web service calls are at least 0. 2 seconds apart."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_top_artists(self, limit=None, cacheable=True):\n\n        params = {}\n        if limit:\n            params[\"limit\"] = limit\n\n        doc = _Request(self, \"chart.getTopArtists\", params).execute(cacheable)\n\n        return _extract_top_artists(doc, self)", "response": "Returns the most played artists as a sequence of TopItem objects."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the most played tracks as a sequence of TopItem objects.", "response": "def get_top_tracks(self, limit=None, cacheable=True):\n        \"\"\"Returns the most played tracks as a sequence of TopItem objects.\"\"\"\n\n        params = {}\n        if limit:\n            params[\"limit\"] = limit\n\n        doc = _Request(self, \"chart.getTopTracks\", params).execute(cacheable)\n\n        seq = []\n        for node in doc.getElementsByTagName(\"track\"):\n            title = _extract(node, \"name\")\n            artist = _extract(node, \"name\", 1)\n            track = Track(artist, title, self)\n            weight = _number(_extract(node, \"playcount\"))\n            seq.append(TopItem(track, weight))\n\n        return seq"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the most used tags as a sequence of TopItem objects.", "response": "def get_top_tags(self, limit=None, cacheable=True):\n        \"\"\"Returns the most used tags as a sequence of TopItem objects.\"\"\"\n\n        # Last.fm has no \"limit\" parameter for tag.getTopTags\n        # so we need to get all (250) and then limit locally\n        doc = _Request(self, \"tag.getTopTags\").execute(cacheable)\n\n        seq = []\n        for node in doc.getElementsByTagName(\"tag\"):\n            if limit and len(seq) >= limit:\n                break\n            tag = Tag(_extract(node, \"name\"), self)\n            weight = _number(_extract(node, \"count\"))\n            seq.append(TopItem(tag, weight))\n\n        return seq"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the most popular artists on Last. fm by country.", "response": "def get_geo_top_artists(self, country, limit=None, cacheable=True):\n        \"\"\"Get the most popular artists on Last.fm by country.\n        Parameters:\n        country (Required) : A country name, as defined by the ISO 3166-1\n            country names standard.\n        limit (Optional) : The number of results to fetch per page.\n            Defaults to 50.\n        \"\"\"\n        params = {\"country\": country}\n\n        if limit:\n            params[\"limit\"] = limit\n\n        doc = _Request(self, \"geo.getTopArtists\", params).execute(cacheable)\n\n        return _extract_top_artists(doc, self)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the most popular tracks on Last. fm last week by country.", "response": "def get_geo_top_tracks(self, country, location=None, limit=None, cacheable=True):\n        \"\"\"Get the most popular tracks on Last.fm last week by country.\n        Parameters:\n        country (Required) : A country name, as defined by the ISO 3166-1\n            country names standard\n        location (Optional) : A metro name, to fetch the charts for\n            (must be within the country specified)\n        limit (Optional) : The number of results to fetch per page.\n            Defaults to 50.\n        \"\"\"\n        params = {\"country\": country}\n\n        if location:\n            params[\"location\"] = location\n        if limit:\n            params[\"limit\"] = limit\n\n        doc = _Request(self, \"geo.getTopTracks\", params).execute(cacheable)\n\n        tracks = doc.getElementsByTagName(\"track\")\n        seq = []\n\n        for track in tracks:\n            title = _extract(track, \"name\")\n            artist = _extract(track, \"name\", 1)\n            listeners = _extract(track, \"listeners\")\n\n            seq.append(TopItem(Track(artist, title, self), listeners))\n\n        return seq"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nenable a default web proxy", "response": "def enable_proxy(self, host, port):\n        \"\"\"Enable a default web proxy\"\"\"\n\n        self.proxy = [host, _number(port)]\n        self.proxy_enabled = True"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nenables caching for all cacheable calls.", "response": "def enable_caching(self, file_path=None):\n        \"\"\"Enables caching request-wide for all cacheable calls.\n\n        * file_path: A file path for the backend storage file. If\n        None set, a temp file would probably be created, according the backend.\n        \"\"\"\n\n        if not file_path:\n            file_path = tempfile.mktemp(prefix=\"pylast_tmp_\")\n\n        self.cache_backend = _ShelfCacheBackend(file_path)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_track_by_mbid(self, mbid):\n\n        params = {\"mbid\": mbid}\n\n        doc = _Request(self, \"track.getInfo\", params).execute(True)\n\n        return Track(_extract(doc, \"name\", 1), _extract(doc, \"name\"), self)", "response": "Looks up a track by its MusicBrainz ID"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_artist_by_mbid(self, mbid):\n\n        params = {\"mbid\": mbid}\n\n        doc = _Request(self, \"artist.getInfo\", params).execute(True)\n\n        return Artist(_extract(doc, \"name\"), self)", "response": "Looks up an artist by its MusicBrainz ID"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlook up an album by its MusicBrainz ID", "response": "def get_album_by_mbid(self, mbid):\n        \"\"\"Looks up an album by its MusicBrainz ID\"\"\"\n\n        params = {\"mbid\": mbid}\n\n        doc = _Request(self, \"album.getInfo\", params).execute(True)\n\n        return Album(_extract(doc, \"artist\"), _extract(doc, \"name\"), self)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate the current state of the current track.", "response": "def update_now_playing(\n        self,\n        artist,\n        title,\n        album=None,\n        album_artist=None,\n        duration=None,\n        track_number=None,\n        mbid=None,\n        context=None,\n    ):\n        \"\"\"\n        Used to notify Last.fm that a user has started listening to a track.\n\n            Parameters:\n                artist (Required) : The artist name\n                title (Required) : The track title\n                album (Optional) : The album name.\n                album_artist (Optional) : The album artist - if this differs\n                    from the track artist.\n                duration (Optional) : The length of the track in seconds.\n                track_number (Optional) : The track number of the track on the\n                    album.\n                mbid (Optional) : The MusicBrainz Track ID.\n                context (Optional) : Sub-client version\n                    (not public, only enabled for certain API keys)\n        \"\"\"\n\n        params = {\"track\": title, \"artist\": artist}\n\n        if album:\n            params[\"album\"] = album\n        if album_artist:\n            params[\"albumArtist\"] = album_artist\n        if context:\n            params[\"context\"] = context\n        if track_number:\n            params[\"trackNumber\"] = track_number\n        if mbid:\n            params[\"mbid\"] = mbid\n        if duration:\n            params[\"duration\"] = duration\n\n        _Request(self, \"track.updateNowPlaying\", params).execute()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef scrobble(\n        self,\n        artist,\n        title,\n        timestamp,\n        album=None,\n        album_artist=None,\n        track_number=None,\n        duration=None,\n        stream_id=None,\n        context=None,\n        mbid=None,\n    ):\n\n        \"\"\"Used to add a track-play to a user's profile.\n\n        Parameters:\n            artist (Required) : The artist name.\n            title (Required) : The track name.\n            timestamp (Required) : The time the track started playing, in UNIX\n                timestamp format (integer number of seconds since 00:00:00,\n                January 1st 1970 UTC). This must be in the UTC time zone.\n            album (Optional) : The album name.\n            album_artist (Optional) : The album artist - if this differs from\n                the track artist.\n            context (Optional) : Sub-client version (not public, only enabled\n                for certain API keys)\n            stream_id (Optional) : The stream id for this track received from\n                the radio.getPlaylist service.\n            track_number (Optional) : The track number of the track on the\n                album.\n            mbid (Optional) : The MusicBrainz Track ID.\n            duration (Optional) : The length of the track in seconds.\n        \"\"\"\n\n        return self.scrobble_many(\n            (\n                {\n                    \"artist\": artist,\n                    \"title\": title,\n                    \"timestamp\": timestamp,\n                    \"album\": album,\n                    \"album_artist\": album_artist,\n                    \"track_number\": track_number,\n                    \"duration\": duration,\n                    \"stream_id\": stream_id,\n                    \"context\": context,\n                    \"mbid\": mbid,\n                },\n            )\n        )", "response": "Scrobble a user s profile."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nuses to scrobble a batch of tracks at once. The parameter tracks is a sequence of dicts per track containing the keyword arguments as if passed to the scrobble() method.", "response": "def scrobble_many(self, tracks):\n        \"\"\"\n        Used to scrobble a batch of tracks at once. The parameter tracks is a\n        sequence of dicts per track containing the keyword arguments as if\n        passed to the scrobble() method.\n        \"\"\"\n\n        tracks_to_scrobble = tracks[:50]\n        if len(tracks) > 50:\n            remaining_tracks = tracks[50:]\n        else:\n            remaining_tracks = None\n\n        params = {}\n        for i in range(len(tracks_to_scrobble)):\n\n            params[\"artist[%d]\" % i] = tracks_to_scrobble[i][\"artist\"]\n            params[\"track[%d]\" % i] = tracks_to_scrobble[i][\"title\"]\n\n            additional_args = (\n                \"timestamp\",\n                \"album\",\n                \"album_artist\",\n                \"context\",\n                \"stream_id\",\n                \"track_number\",\n                \"mbid\",\n                \"duration\",\n            )\n            args_map_to = {  # so friggin lazy\n                \"album_artist\": \"albumArtist\",\n                \"track_number\": \"trackNumber\",\n                \"stream_id\": \"streamID\",\n            }\n\n            for arg in additional_args:\n\n                if arg in tracks_to_scrobble[i] and tracks_to_scrobble[i][arg]:\n                    if arg in args_map_to:\n                        maps_to = args_map_to[arg]\n                    else:\n                        maps_to = arg\n\n                    params[\"%s[%d]\" % (maps_to, i)] = tracks_to_scrobble[i][arg]\n\n        _Request(self, \"track.scrobble\", params).execute()\n\n        if remaining_tracks:\n            self.scrobble_many(remaining_tracks)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a 32 - character hexadecimal md5 hash of the signature string.", "response": "def _get_signature(self):\n        \"\"\"\n        Returns a 32-character hexadecimal md5 hash of the signature string.\n        \"\"\"\n\n        keys = list(self.params.keys())\n\n        keys.sort()\n\n        string = \"\"\n\n        for name in keys:\n            string += name\n            string += self.params[name]\n\n        string += self.api_secret\n\n        return md5(string)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the cache key for the current user.", "response": "def _get_cache_key(self):\n        \"\"\"\n        The cache key is a string of concatenated sorted names and values.\n        \"\"\"\n\n        keys = list(self.params.keys())\n        keys.sort()\n\n        cache_key = str()\n\n        for key in keys:\n            if key != \"api_sig\" and key != \"api_key\" and key != \"sk\":\n                cache_key += key + self.params[key]\n\n        return hashlib.sha1(cache_key.encode(\"utf-8\")).hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a file object of the cached response.", "response": "def _get_cached_response(self):\n        \"\"\"Returns a file object of the cached response.\"\"\"\n\n        if not self._is_cached():\n            response = self._download_response()\n            self.cache.set_xml(self._get_cache_key(), response)\n\n        return self.cache.get_xml(self._get_cache_key())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _download_response(self):\n\n        if self.network.limit_rate:\n            self.network._delay_call()\n\n        data = []\n        for name in self.params.keys():\n            data.append(\"=\".join((name, url_quote_plus(_string(self.params[name])))))\n        data = \"&\".join(data)\n\n        headers = {\n            \"Content-type\": \"application/x-www-form-urlencoded\",\n            \"Accept-Charset\": \"utf-8\",\n            \"User-Agent\": \"pylast\" + \"/\" + __version__,\n        }\n\n        (host_name, host_subdir) = self.network.ws_server\n\n        if self.network.is_proxy_enabled():\n            conn = HTTPSConnection(\n                context=SSL_CONTEXT,\n                host=self.network._get_proxy()[0],\n                port=self.network._get_proxy()[1],\n            )\n\n            try:\n                conn.request(\n                    method=\"POST\",\n                    url=\"https://\" + host_name + host_subdir,\n                    body=data,\n                    headers=headers,\n                )\n            except Exception as e:\n                raise NetworkError(self.network, e)\n\n        else:\n            conn = HTTPSConnection(context=SSL_CONTEXT, host=host_name)\n\n            try:\n                conn.request(method=\"POST\", url=host_subdir, body=data, headers=headers)\n            except Exception as e:\n                raise NetworkError(self.network, e)\n\n        try:\n            response_text = _unicode(conn.getresponse().read())\n        except Exception as e:\n            raise MalformedResponseError(self.network, e)\n\n        try:\n            self._check_response_for_errors(response_text)\n        finally:\n            conn.close()\n        return response_text", "response": "Returns a response body string from the server."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the XML DOM response of the POST request from the server", "response": "def execute(self, cacheable=False):\n        \"\"\"Returns the XML DOM response of the POST Request from the server\"\"\"\n\n        if self.network.is_caching_enabled() and cacheable:\n            response = self._get_cached_response()\n        else:\n            response = self._download_response()\n\n        return minidom.parseString(_string(response).replace(\"opensearch:\", \"\"))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck the response for errors and raises one if any exists.", "response": "def _check_response_for_errors(self, response):\n        \"\"\"Checks the response for errors and raises one if any exists.\"\"\"\n\n        try:\n            doc = minidom.parseString(_string(response).replace(\"opensearch:\", \"\"))\n        except Exception as e:\n            raise MalformedResponseError(self.network, e)\n\n        e = doc.getElementsByTagName(\"lfm\")[0]\n        # logger.debug(doc.toprettyxml())\n\n        if e.getAttribute(\"status\") != \"ok\":\n            e = doc.getElementsByTagName(\"error\")[0]\n            status = e.getAttribute(\"code\")\n            details = e.firstChild.data.strip()\n            raise WSError(self.network, status, details)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a token from the network for web authentication.", "response": "def _get_web_auth_token(self):\n        \"\"\"\n        Retrieves a token from the network for web authentication.\n        The token then has to be authorized from getAuthURL before creating\n        session.\n        \"\"\"\n\n        request = _Request(self.network, \"auth.getToken\")\n\n        # default action is that a request is signed only when\n        # a session key is provided.\n        request.sign_it()\n\n        doc = request.execute()\n\n        e = doc.getElementsByTagName(\"token\")[0]\n        return e.firstChild.data"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_web_auth_url(self):\n\n        token = self._get_web_auth_token()\n\n        url = \"{homepage}/api/auth/?api_key={api}&token={token}\".format(\n            homepage=self.network.homepage, api=self.network.api_key, token=token\n        )\n\n        self.web_auth_tokens[url] = token\n\n        return url", "response": "Get the URL for the web authentication."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_web_auth_session_key_username(self, url, token=\"\"):\n\n        if url in self.web_auth_tokens.keys():\n            token = self.web_auth_tokens[url]\n        else:\n            # This will raise a WSError if token is blank or unauthorized\n            token = token\n\n        request = _Request(self.network, \"auth.getSession\", {\"token\": token})\n\n        # default action is that a request is signed only when\n        # a session key is provided.\n        request.sign_it()\n\n        doc = request.execute()\n\n        session_key = doc.getElementsByTagName(\"key\")[0].firstChild.data\n        username = doc.getElementsByTagName(\"name\")[0].firstChild.data\n        return session_key, username", "response": "Retrieves the session key and username of a web authorization process by its URL."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_web_auth_session_key(self, url, token=\"\"):\n        session_key, _username = self.get_web_auth_session_key_username(url, token)\n        return session_key", "response": "Get the session key of a web authorization process by its URL and token."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_session_key(self, username, password_hash):\n\n        params = {\"username\": username, \"authToken\": md5(username + password_hash)}\n        request = _Request(self.network, \"auth.getMobileSession\", params)\n\n        # default action is that a request is signed only when\n        # a session key is provided.\n        request.sign_it()\n\n        doc = request.execute()\n\n        return _extract(doc, \"key\")", "response": "Retrieve a session key with a username and a md5 hash of the user s\n        password."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of the most played thing_types by this thing.", "response": "def _get_things(self, method, thing, thing_type, params=None, cacheable=True):\n        \"\"\"Returns a list of the most played thing_types by this thing.\"\"\"\n\n        limit = params.get(\"limit\", 1)\n        seq = []\n        for node in _collect_nodes(\n            limit, self, self.ws_prefix + \".\" + method, cacheable, params\n        ):\n            title = _extract(node, \"name\")\n            artist = _extract(node, \"name\", 1)\n            playcount = _number(_extract(node, \"playcount\"))\n\n            seq.append(TopItem(thing_type(artist, title, self.network), playcount))\n\n        return seq"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_wiki(self, section):\n\n        doc = self._request(self.ws_prefix + \".getInfo\", True)\n\n        if len(doc.getElementsByTagName(\"wiki\")) == 0:\n            return\n\n        node = doc.getElementsByTagName(\"wiki\")[0]\n\n        return _extract(node, section)", "response": "Returns a section of the wiki."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_weekly_chart_dates(self):\n\n        doc = self._request(self.ws_prefix + \".getWeeklyChartList\", True)\n\n        seq = []\n        for node in doc.getElementsByTagName(\"chart\"):\n            seq.append((node.getAttribute(\"from\"), node.getAttribute(\"to\")))\n\n        return seq", "response": "Returns a list of From and To tuples for the available charts."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the weekly charts for the week starting from the from_date value to the to_date value.", "response": "def get_weekly_charts(self, chart_kind, from_date=None, to_date=None):\n        \"\"\"\n        Returns the weekly charts for the week starting from the\n        from_date value to the to_date value.\n        chart_kind should be one of \"album\", \"artist\" or \"track\"\n        \"\"\"\n        method = \".getWeekly\" + chart_kind.title() + \"Chart\"\n        chart_type = eval(chart_kind.title())  # string to type\n\n        params = self._get_params()\n        if from_date and to_date:\n            params[\"from\"] = from_date\n            params[\"to\"] = to_date\n\n        doc = self._request(self.ws_prefix + method, True, params)\n\n        seq = []\n        for node in doc.getElementsByTagName(chart_kind.lower()):\n            if chart_kind == \"artist\":\n                item = chart_type(_extract(node, \"name\"), self.network)\n            else:\n                item = chart_type(\n                    _extract(node, \"artist\"), _extract(node, \"name\"), self.network\n                )\n            weight = _number(_extract(node, \"playcount\"))\n            seq.append(TopItem(item, weight))\n\n        return seq"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef remove_tag(self, tag):\n\n        if isinstance(tag, Tag):\n            tag = tag.get_name()\n\n        params = self._get_params()\n        params[\"tag\"] = tag\n\n        self._request(self.ws_prefix + \".removeTag\", False, params)", "response": "Remove a user s tag from this object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_tags(self):\n\n        # Uncacheable because it can be dynamically changed by the user.\n        params = self._get_params()\n\n        doc = self._request(self.ws_prefix + \".getTags\", False, params)\n        tag_names = _extract_all(doc, \"name\")\n        tags = []\n        for tag in tag_names:\n            tags.append(Tag(tag, self.network))\n\n        return tags", "response": "Returns a list of the tags set by the user to this object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset this object s tags to only those tags.", "response": "def set_tags(self, tags):\n        \"\"\"Sets this object's tags to only those tags.\n        * tags: a sequence of tag names or Tag objects.\n        \"\"\"\n\n        c_old_tags = []\n        old_tags = []\n        c_new_tags = []\n        new_tags = []\n\n        to_remove = []\n        to_add = []\n\n        tags_on_server = self.get_tags()\n\n        for tag in tags_on_server:\n            c_old_tags.append(tag.get_name().lower())\n            old_tags.append(tag.get_name())\n\n        for tag in tags:\n            c_new_tags.append(tag.lower())\n            new_tags.append(tag)\n\n        for i in range(0, len(old_tags)):\n            if not c_old_tags[i] in c_new_tags:\n                to_remove.append(old_tags[i])\n\n        for i in range(0, len(new_tags)):\n            if not c_new_tags[i] in c_old_tags:\n                to_add.append(new_tags[i])\n\n        self.remove_tags(to_remove)\n        self.add_tags(to_add)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of the most frequently used Tags on this object.", "response": "def get_top_tags(self, limit=None):\n        \"\"\"Returns a list of the most frequently used Tags on this object.\"\"\"\n\n        doc = self._request(self.ws_prefix + \".getTopTags\", True)\n\n        elements = doc.getElementsByTagName(\"tag\")\n        seq = []\n\n        for element in elements:\n            tag_name = _extract(element, \"name\")\n            tagcount = _extract(element, \"count\")\n\n            seq.append(TopItem(Tag(tag_name, self.network), tagcount))\n\n        if limit:\n            seq = seq[:limit]\n\n        return seq"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a URI to the cover image", "response": "def get_cover_image(self, size=SIZE_EXTRA_LARGE):\n        \"\"\"\n        Returns a URI to the cover image\n        size can be one of:\n            SIZE_EXTRA_LARGE\n            SIZE_LARGE\n            SIZE_MEDIUM\n            SIZE_SMALL\n        \"\"\"\n        if \"image\" not in self.info:\n            self.info[\"image\"] = _extract_all(\n                self._request(self.ws_prefix + \".getInfo\", cacheable=True), \"image\"\n            )\n        return self.info[\"image\"][size]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the artist or track title.", "response": "def get_title(self, properly_capitalized=False):\n        \"\"\"Returns the artist or track title.\"\"\"\n        if properly_capitalized:\n            self.title = _extract(\n                self._request(self.ws_prefix + \".getInfo\", True), \"name\"\n            )\n\n        return self.title"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the number of plays on the network", "response": "def get_playcount(self):\n        \"\"\"Returns the number of plays on the network\"\"\"\n\n        return _number(\n            _extract(\n                self._request(self.ws_prefix + \".getInfo\", cacheable=True), \"playcount\"\n            )\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_userplaycount(self):\n\n        if not self.username:\n            return\n\n        params = self._get_params()\n        params[\"username\"] = self.username\n\n        doc = self._request(self.ws_prefix + \".getInfo\", True, params)\n        return _number(_extract(doc, \"userplaycount\"))", "response": "Returns the number of plays by a given username"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_listener_count(self):\n\n        return _number(\n            _extract(\n                self._request(self.ws_prefix + \".getInfo\", cacheable=True), \"listeners\"\n            )\n        )", "response": "Returns the number of listeners on the network"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the MusicBrainz ID of the album or track or None if no MusicBrainz ID is available.", "response": "def get_mbid(self):\n        \"\"\"Returns the MusicBrainz ID of the album or track.\"\"\"\n\n        doc = self._request(self.ws_prefix + \".getInfo\", cacheable=True)\n\n        try:\n            lfm = doc.getElementsByTagName(\"lfm\")[0]\n            opus = next(self._get_children_by_tag_name(lfm, self.ws_prefix))\n            mbid = next(self._get_children_by_tag_name(opus, \"mbid\"))\n            return mbid.firstChild.nodeValue\n        except StopIteration:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the list of Tracks on this album.", "response": "def get_tracks(self):\n        \"\"\"Returns the list of Tracks on this album.\"\"\"\n\n        return _extract_tracks(\n            self._request(self.ws_prefix + \".getInfo\", cacheable=True), self.network\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the name of the artist.", "response": "def get_name(self, properly_capitalized=False):\n        \"\"\"Returns the name of the artist.\n        If properly_capitalized was asserted then the name would be downloaded\n        overwriting the given one.\"\"\"\n\n        if properly_capitalized:\n            self.name = _extract(\n                self._request(self.ws_prefix + \".getInfo\", True), \"name\"\n            )\n\n        return self.name"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the MusicBrainz ID of this artist.", "response": "def get_mbid(self):\n        \"\"\"Returns the MusicBrainz ID of this artist.\"\"\"\n\n        doc = self._request(self.ws_prefix + \".getInfo\", True)\n\n        return _extract(doc, \"mbid\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the number of listeners on the network.", "response": "def get_listener_count(self):\n        \"\"\"Returns the number of listeners on the network.\"\"\"\n\n        if hasattr(self, \"listener_count\"):\n            return self.listener_count\n        else:\n            self.listener_count = _number(\n                _extract(self._request(self.ws_prefix + \".getInfo\", True), \"listeners\")\n            )\n            return self.listener_count"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning True if the artist is streamable.", "response": "def is_streamable(self):\n        \"\"\"Returns True if the artist is streamable.\"\"\"\n\n        return bool(\n            _number(\n                _extract(self._request(self.ws_prefix + \".getInfo\", True), \"streamable\")\n            )\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_bio(self, section, language=None):\n        if language:\n            params = self._get_params()\n            params[\"lang\"] = language\n        else:\n            params = None\n\n        return self._extract_cdata_from_request(\n            self.ws_prefix + \".getInfo\", section, params\n        )", "response": "Returns a section of the bio.\n        section can be \"content\", \"summary\" or\n            \"published\" (for published date)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_similar(self, limit=None):\n\n        params = self._get_params()\n        if limit:\n            params[\"limit\"] = limit\n\n        doc = self._request(self.ws_prefix + \".getSimilar\", True, params)\n\n        names = _extract_all(doc, \"name\")\n        matches = _extract_all(doc, \"match\")\n\n        artists = []\n        for i in range(0, len(names)):\n            artists.append(\n                SimilarItem(Artist(names[i], self.network), _number(matches[i]))\n            )\n\n        return artists", "response": "Returns the similar artists on the network."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of the top albums.", "response": "def get_top_albums(self, limit=None, cacheable=True):\n        \"\"\"Returns a list of the top albums.\"\"\"\n        params = self._get_params()\n        if limit:\n            params[\"limit\"] = limit\n\n        return self._get_things(\"getTopAlbums\", \"album\", Album, params, cacheable)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of the most played Tracks by this artist.", "response": "def get_top_tracks(self, limit=None, cacheable=True):\n        \"\"\"Returns a list of the most played Tracks by this artist.\"\"\"\n        params = self._get_params()\n        if limit:\n            params[\"limit\"] = limit\n\n        return self._get_things(\"getTopTracks\", \"track\", Track, params, cacheable)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_top_artists(self, limit=None, cacheable=True):\n        params = self._get_params()\n        if limit:\n            params[\"limit\"] = limit\n\n        doc = self._request(\"geo.getTopArtists\", cacheable, params)\n\n        return _extract_top_artists(doc, self)", "response": "Returns a sequence of the most played artists."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_artists(self, limit=50, cacheable=True):\n\n        seq = []\n        for node in _collect_nodes(\n            limit, self, self.ws_prefix + \".getArtists\", cacheable\n        ):\n            name = _extract(node, \"name\")\n\n            playcount = _number(_extract(node, \"playcount\"))\n            tagcount = _number(_extract(node, \"tagcount\"))\n\n            seq.append(LibraryItem(Artist(name, self.network), playcount, tagcount))\n\n        return seq", "response": "Returns a sequence ofLibraryItem objects representing the available Artist objects."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_url(self, domain_name=DOMAIN_ENGLISH):\n\n        name = _url_safe(self.get_name())\n\n        return self.network._get_url(domain_name, \"tag\") % {\"name\": name}", "response": "Returns the URL of the tag page on the network."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_duration(self):\n\n        doc = self._request(self.ws_prefix + \".getInfo\", True)\n\n        return _number(_extract(doc, \"duration\"))", "response": "Returns the track duration in seconds."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_userloved(self):\n\n        if not self.username:\n            return\n\n        params = self._get_params()\n        params[\"username\"] = self.username\n\n        doc = self._request(self.ws_prefix + \".getInfo\", True, params)\n        loved = _number(_extract(doc, \"userloved\"))\n        return bool(loved)", "response": "Whether the user loved this track"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_streamable(self):\n\n        doc = self._request(self.ws_prefix + \".getInfo\", True)\n        return _extract(doc, \"streamable\") == \"1\"", "response": "Returns True if the track is available at Last. fm."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_fulltrack_available(self):\n\n        doc = self._request(self.ws_prefix + \".getInfo\", True)\n        return (\n            doc.getElementsByTagName(\"streamable\")[0].getAttribute(\"fulltrack\") == \"1\"\n        )", "response": "Returns True if the full track is available for streaming."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_album(self):\n\n        doc = self._request(self.ws_prefix + \".getInfo\", True)\n\n        albums = doc.getElementsByTagName(\"album\")\n\n        if len(albums) == 0:\n            return\n\n        node = doc.getElementsByTagName(\"album\")[0]\n        return Album(_extract(node, \"artist\"), _extract(node, \"title\"), self.network)", "response": "Returns the album object of this track."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of similar items for this track on the network", "response": "def get_similar(self, limit=None):\n        \"\"\"\n        Returns similar tracks for this track on the network,\n        based on listening data.\n        \"\"\"\n\n        params = self._get_params()\n        if limit:\n            params[\"limit\"] = limit\n\n        doc = self._request(self.ws_prefix + \".getSimilar\", True, params)\n\n        seq = []\n        for node in doc.getElementsByTagName(self.ws_prefix):\n            title = _extract(node, \"name\")\n            artist = _extract(node, \"name\", 1)\n            match = _number(_extract(node, \"match\"))\n\n            seq.append(SimilarItem(Track(artist, title, self.network), match))\n\n        return seq"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_url(self, domain_name=DOMAIN_ENGLISH):\n\n        artist = _url_safe(self.get_artist().get_name())\n        title = _url_safe(self.get_title())\n\n        return self.network._get_url(domain_name, self.ws_prefix) % {\n            \"artist\": artist,\n            \"title\": title,\n        }", "response": "Returns the URL of the album or track page on the network."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a list of tracks by a given artist.", "response": "def get_artist_tracks(self, artist, cacheable=False):\n        \"\"\"\n        Get a list of tracks by a given artist scrobbled by this user,\n        including scrobble time.\n        \"\"\"\n        # Not implemented:\n        # \"Can be limited to specific timeranges, defaults to all time.\"\n\n        warnings.warn(\n            \"User.get_artist_tracks is deprecated and will be removed in a future \"\n            \"version. User.get_track_scrobbles is a partial replacement. \"\n            \"See https://github.com/pylast/pylast/issues/298\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n\n        params = self._get_params()\n        params[\"artist\"] = artist\n\n        seq = []\n        for track in _collect_nodes(\n            None, self, self.ws_prefix + \".getArtistTracks\", cacheable, params\n        ):\n            title = _extract(track, \"name\")\n            artist = _extract(track, \"artist\")\n            date = _extract(track, \"date\")\n            album = _extract(track, \"album\")\n            timestamp = track.getElementsByTagName(\"date\")[0].getAttribute(\"uts\")\n\n            seq.append(\n                PlayedTrack(Track(artist, title, self.network), album, date, timestamp)\n            )\n\n        return seq"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_friends(self, limit=50, cacheable=False):\n\n        seq = []\n        for node in _collect_nodes(\n            limit, self, self.ws_prefix + \".getFriends\", cacheable\n        ):\n            seq.append(User(_extract(node, \"name\"), self.network))\n\n        return seq", "response": "Returns a list of the user s friends."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_loved_tracks(self, limit=50, cacheable=True):\n\n        params = self._get_params()\n        if limit:\n            params[\"limit\"] = limit\n\n        seq = []\n        for track in _collect_nodes(\n            limit, self, self.ws_prefix + \".getLovedTracks\", cacheable, params\n        ):\n            try:\n                artist = _extract(track, \"name\", 1)\n            except IndexError:  # pragma: no cover\n                continue\n            title = _extract(track, \"name\")\n            date = _extract(track, \"date\")\n            timestamp = track.getElementsByTagName(\"date\")[0].getAttribute(\"uts\")\n\n            seq.append(LovedTrack(Track(artist, title, self.network), date, timestamp))\n\n        return seq", "response": "Returns this user s loved tracks as a sequence of LovedTrack objects in reverse order of their timestamp."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_now_playing(self):\n\n        params = self._get_params()\n        params[\"limit\"] = \"1\"\n\n        doc = self._request(self.ws_prefix + \".getRecentTracks\", False, params)\n\n        tracks = doc.getElementsByTagName(\"track\")\n\n        if len(tracks) == 0:\n            return None\n\n        e = tracks[0]\n\n        if not e.hasAttribute(\"nowplaying\"):\n            return None\n\n        artist = _extract(e, \"artist\")\n        title = _extract(e, \"name\")\n\n        return Track(artist, title, self.network, self.name)", "response": "Returns the currently playing track or None if nothing is playing."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn this user s played tracks as a sequence of PlayedTrack objects.", "response": "def get_recent_tracks(self, limit=10, cacheable=True, time_from=None, time_to=None):\n        \"\"\"\n        Returns this user's played track as a sequence of PlayedTrack objects\n        in reverse order of playtime, all the way back to the first track.\n\n        Parameters:\n        limit : If None, it will try to pull all the available data.\n        from (Optional) : Beginning timestamp of a range - only display\n        scrobbles after this time, in UNIX timestamp format (integer\n        number of seconds since 00:00:00, January 1st 1970 UTC). This\n        must be in the UTC time zone.\n        to (Optional) : End timestamp of a range - only display scrobbles\n        before this time, in UNIX timestamp format (integer number of\n        seconds since 00:00:00, January 1st 1970 UTC). This must be in\n        the UTC time zone.\n\n        This method uses caching. Enable caching only if you're pulling a\n        large amount of data.\n        \"\"\"\n\n        params = self._get_params()\n        if limit:\n            params[\"limit\"] = limit\n        if time_from:\n            params[\"from\"] = time_from\n        if time_to:\n            params[\"to\"] = time_to\n\n        seq = []\n        for track in _collect_nodes(\n            limit, self, self.ws_prefix + \".getRecentTracks\", cacheable, params\n        ):\n\n            if track.hasAttribute(\"nowplaying\"):\n                continue  # to prevent the now playing track from sneaking in\n\n            title = _extract(track, \"name\")\n            artist = _extract(track, \"artist\")\n            date = _extract(track, \"date\")\n            album = _extract(track, \"album\")\n            timestamp = track.getElementsByTagName(\"date\")[0].getAttribute(\"uts\")\n\n            seq.append(\n                PlayedTrack(Track(artist, title, self.network), album, date, timestamp)\n            )\n\n        return seq"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_country(self):\n\n        doc = self._request(self.ws_prefix + \".getInfo\", True)\n\n        country = _extract(doc, \"country\")\n\n        if country is None or country == \"None\":\n            return None\n        else:\n            return Country(country, self.network)", "response": "Returns the name of the country of the user."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_subscriber(self):\n\n        doc = self._request(self.ws_prefix + \".getInfo\", True)\n\n        return _extract(doc, \"subscriber\") == \"1\"", "response": "Returns whether the user is a subscriber or not. True or False."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_playcount(self):\n\n        doc = self._request(self.ws_prefix + \".getInfo\", True)\n\n        return _number(_extract(doc, \"playcount\"))", "response": "Returns the user s playcount so far."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the user s registration date.", "response": "def get_registered(self):\n        \"\"\"Returns the user's registration date.\"\"\"\n\n        doc = self._request(self.ws_prefix + \".getInfo\", True)\n\n        return _extract(doc, \"registered\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_unixtime_registered(self):\n\n        doc = self._request(self.ws_prefix + \".getInfo\", True)\n\n        return int(doc.getElementsByTagName(\"registered\")[0].getAttribute(\"unixtime\"))", "response": "Returns the user s registration date as a UNIX timestamp."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_tagged_albums(self, tag, limit=None, cacheable=True):\n\n        params = self._get_params()\n        params[\"tag\"] = tag\n        params[\"taggingtype\"] = \"album\"\n        if limit:\n            params[\"limit\"] = limit\n        doc = self._request(self.ws_prefix + \".getpersonaltags\", cacheable, params)\n        return _extract_albums(doc, self.network)", "response": "Returns the albums tagged by a user."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_tagged_artists(self, tag, limit=None):\n\n        params = self._get_params()\n        params[\"tag\"] = tag\n        params[\"taggingtype\"] = \"artist\"\n        if limit:\n            params[\"limit\"] = limit\n        doc = self._request(self.ws_prefix + \".getpersonaltags\", True, params)\n        return _extract_artists(doc, self.network)", "response": "Returns the artists tagged by a user."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_tagged_tracks(self, tag, limit=None, cacheable=True):\n\n        params = self._get_params()\n        params[\"tag\"] = tag\n        params[\"taggingtype\"] = \"track\"\n        if limit:\n            params[\"limit\"] = limit\n        doc = self._request(self.ws_prefix + \".getpersonaltags\", cacheable, params)\n        return _extract_tracks(doc, self.network)", "response": "Returns the tracks tagged by a user."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the top albums played by a user.", "response": "def get_top_albums(self, period=PERIOD_OVERALL, limit=None, cacheable=True):\n        \"\"\"Returns the top albums played by a user.\n        * period: The period of time. Possible values:\n          o PERIOD_OVERALL\n          o PERIOD_7DAYS\n          o PERIOD_1MONTH\n          o PERIOD_3MONTHS\n          o PERIOD_6MONTHS\n          o PERIOD_12MONTHS\n        \"\"\"\n\n        params = self._get_params()\n        params[\"period\"] = period\n        if limit:\n            params[\"limit\"] = limit\n\n        doc = self._request(self.ws_prefix + \".getTopAlbums\", cacheable, params)\n\n        return _extract_top_albums(doc, self.network)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the top artists played by a user.", "response": "def get_top_artists(self, period=PERIOD_OVERALL, limit=None):\n        \"\"\"Returns the top artists played by a user.\n        * period: The period of time. Possible values:\n          o PERIOD_OVERALL\n          o PERIOD_7DAYS\n          o PERIOD_1MONTH\n          o PERIOD_3MONTHS\n          o PERIOD_6MONTHS\n          o PERIOD_12MONTHS\n        \"\"\"\n\n        params = self._get_params()\n        params[\"period\"] = period\n        if limit:\n            params[\"limit\"] = limit\n\n        doc = self._request(self.ws_prefix + \".getTopArtists\", True, params)\n\n        return _extract_top_artists(doc, self.network)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_top_tags(self, limit=None, cacheable=True):\n\n        params = self._get_params()\n        if limit:\n            params[\"limit\"] = limit\n\n        doc = self._request(self.ws_prefix + \".getTopTags\", cacheable, params)\n\n        seq = []\n        for node in doc.getElementsByTagName(\"tag\"):\n            seq.append(\n                TopItem(\n                    Tag(_extract(node, \"name\"), self.network), _extract(node, \"count\")\n                )\n            )\n\n        return seq", "response": "Returns a sequence of the top tags used by this user with their counts\n        as TopItem objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_track_scrobbles(self, artist, track, cacheable=False):\n\n        params = self._get_params()\n        params[\"artist\"] = artist\n        params[\"track\"] = track\n\n        seq = []\n        for track in _collect_nodes(\n            None, self, self.ws_prefix + \".getTrackScrobbles\", cacheable, params\n        ):\n            title = _extract(track, \"name\")\n            artist = _extract(track, \"artist\")\n            date = _extract(track, \"date\")\n            album = _extract(track, \"album\")\n            timestamp = track.getElementsByTagName(\"date\")[0].getAttribute(\"uts\")\n\n            seq.append(\n                PlayedTrack(Track(artist, title, self.network), album, date, timestamp)\n            )\n\n        return seq", "response": "Get a list of this user s scrobbles of this artist s track including scrobble time."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the user s avatar image", "response": "def get_image(self, size=SIZE_EXTRA_LARGE):\n        \"\"\"\n        Returns the user's avatar\n        size can be one of:\n            SIZE_EXTRA_LARGE\n            SIZE_LARGE\n            SIZE_MEDIUM\n            SIZE_SMALL\n        \"\"\"\n\n        doc = self._request(self.ws_prefix + \".getInfo\", True)\n\n        return _extract_all(doc, \"image\")[size]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the total count of all the results.", "response": "def get_total_result_count(self):\n        \"\"\"Returns the total count of all the results.\"\"\"\n\n        doc = self._request(self._ws_prefix + \".search\", True)\n\n        return _extract(doc, \"totalResults\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve the node of matches to be processed", "response": "def _retrieve_page(self, page_index):\n        \"\"\"Returns the node of matches to be processed\"\"\"\n\n        params = self._get_params()\n        params[\"page\"] = str(page_index)\n        doc = self._request(self._ws_prefix + \".search\", True, params)\n\n        return doc.getElementsByTagName(self._ws_prefix + \"matches\")[0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the next page of results as a sequence of Album objects.", "response": "def get_next_page(self):\n        \"\"\"Returns the next page of results as a sequence of Album objects.\"\"\"\n\n        master_node = self._retrieve_next_page()\n\n        seq = []\n        for node in master_node.getElementsByTagName(\"album\"):\n            seq.append(\n                Album(\n                    _extract(node, \"artist\"),\n                    _extract(node, \"name\"),\n                    self.network,\n                    info={\"image\": _extract_all(node, \"image\")},\n                )\n            )\n\n        return seq"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_next_page(self):\n\n        master_node = self._retrieve_next_page()\n\n        seq = []\n        for node in master_node.getElementsByTagName(\"artist\"):\n            artist = Artist(\n                _extract(node, \"name\"),\n                self.network,\n                info={\"image\": _extract_all(node, \"image\")},\n            )\n            artist.listener_count = _number(_extract(node, \"listeners\"))\n            seq.append(artist)\n\n        return seq", "response": "Returns the next page of results as a sequence of Artist objects."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the next page of results as a sequence of Track objects.", "response": "def get_next_page(self):\n        \"\"\"Returns the next page of results as a sequence of Track objects.\"\"\"\n\n        master_node = self._retrieve_next_page()\n\n        seq = []\n        for node in master_node.getElementsByTagName(\"track\"):\n            track = Track(\n                _extract(node, \"artist\"),\n                _extract(node, \"name\"),\n                self.network,\n                info={\"image\": _extract_all(node, \"image\")},\n            )\n            track.listener_count = _number(_extract(node, \"listeners\"))\n            seq.append(track)\n\n        return seq"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget version info from __init__.", "response": "def get_version(file_name='beautysh/__init__.py'):\n    \"\"\"Get version info from __init__.\"\"\"\n    with open(file_name) as v_file:\n        for line in v_file:\n            if \"__version__\" in line:\n                return eval(line.split('=')[-1])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write_file(self, fp, data):\n        with open(fp, 'w') as f:\n            f.write(data)", "response": "Write output to a file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the index for the function declaration style detected in the given string or None if no function declarations are detected.", "response": "def detect_function_style(self, test_record):\n        \"\"\"Returns the index for the function declaration style detected in the given string\n           or None if no function declarations are detected.\"\"\"\n        index = 0\n        # IMPORTANT: apply regex sequentially and stop on the first match:\n        for regex in FUNCTION_STYLE_REGEX:\n            if re.search(regex, test_record):\n                return index\n            index+=1\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef change_function_style(self, stripped_record, func_decl_style):\n        if func_decl_style is None:\n            return stripped_record\n        if self.apply_function_style is None:\n            # user does not want to enforce any specific function style\n            return stripped_record\n        regex = FUNCTION_STYLE_REGEX[func_decl_style]\n        replacement = FUNCTION_STYLE_REPLACEMENT[self.apply_function_style]\n        changed_record = re.sub(regex, replacement, stripped_record)\n        return changed_record.strip()", "response": "Converts a function definition syntax from the func_decl_style and returns the string with the converted syntax."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbeautifies string (file part).", "response": "def beautify_string(self, data, path=''):\n        \"\"\"Beautify string (file part).\"\"\"\n        tab = 0\n        case_level = 0\n        prev_line_had_continue = False\n        continue_line = False\n        started_multiline_quoted_string = False\n        ended_multiline_quoted_string = False\n        open_brackets = 0\n        in_here_doc = False\n        defer_ext_quote = False\n        in_ext_quote = False\n        ext_quote_string = ''\n        here_string = ''\n        output = []\n        line = 1\n        formatter = True\n        for record in re.split('\\n', data):\n            record = record.rstrip()\n            stripped_record = record.strip()\n\n            # preserve blank lines\n            if not stripped_record:\n                output.append(stripped_record)\n                continue\n\n            # ensure space before ;; terminators in case statements\n            if case_level:\n                stripped_record = re.sub(r'(\\S);;', r'\\1 ;;', stripped_record)\n\n            test_record = self.get_test_record(stripped_record)\n\n            # detect whether this line ends with line continuation character:\n            prev_line_had_continue = continue_line\n            continue_line = True if (re.search(r'\\\\$', stripped_record)!=None) else False\n            inside_multiline_quoted_string = prev_line_had_continue and continue_line and started_multiline_quoted_string\n\n            if not continue_line and prev_line_had_continue and started_multiline_quoted_string:\n                # remove contents of strings initiated on previous lines and that are ending on this line:\n                [test_record, num_subs] = re.subn(r'^[^\"]*\"', '', test_record)\n                ended_multiline_quoted_string = True if num_subs>0 else False\n            else:\n                ended_multiline_quoted_string = False\n\n            if(in_here_doc) or (inside_multiline_quoted_string) or (ended_multiline_quoted_string):  # pass on with no changes\n                output.append(record)\n                # now test for here-doc termination string\n                if(re.search(here_string, test_record) and not\n                   re.search(r'<<', test_record)):\n                    in_here_doc = False\n            else:  # not in here doc or inside multiline-quoted\n\n                if continue_line:\n                    if prev_line_had_continue:\n                        # this line is not STARTING a multiline-quoted string... we may be in the middle\n                        # of such a multiline string though\n                        started_multiline_quoted_string = False\n                    else:\n                        # remove contents of strings initiated on current line but that continue on next line\n                        # (in particular we need to ignore brackets they may contain!)\n                        [test_record, num_subs] = re.subn(r'\"[^\"]*?\\\\$', '', test_record)\n                        started_multiline_quoted_string = True if num_subs>0 else False\n                else:\n                    # this line is not STARTING a multiline-quoted string\n                    started_multiline_quoted_string = False\n\n                if(re.search(r'<<-?', test_record)) and not (re.search(r'.*<<<', test_record)):\n                    here_string = re.sub(\n                        r'.*<<-?\\s*[\\'|\"]?([_|\\w]+)[\\'|\"]?.*', r'\\1',\n                        stripped_record, 1)\n                    in_here_doc = (len(here_string) > 0)\n\n                if(in_ext_quote):\n                    if(re.search(ext_quote_string, test_record)):\n                        # provide line after quotes\n                        test_record = re.sub(\n                            r'.*%s(.*)' % ext_quote_string, r'\\1',\n                            test_record, 1)\n                        in_ext_quote = False\n                else:  # not in ext quote\n                    if(re.search(r'(\\A|\\s)(\\'|\")', test_record)):\n                        # apply only after this line has been processed\n                        defer_ext_quote = True\n                        ext_quote_string = re.sub(\n                            r'.*([\\'\"]).*', r'\\1', test_record, 1)\n                        # provide line before quote\n                        test_record = re.sub(\n                            r'(.*)%s.*' % ext_quote_string, r'\\1',\n                            test_record, 1)\n                if(in_ext_quote or not formatter):\n                    # pass on unchanged\n                    output.append(record)\n                    if(re.search(r'#\\s*@formatter:on', stripped_record)):\n                        formatter = True\n                        continue\n                else:  # not in ext quote\n                    if(re.search(r'#\\s*@formatter:off', stripped_record)):\n                        formatter = False\n                        output.append(record)\n                        continue\n\n                    # multi-line conditions are often meticulously formatted\n                    if open_brackets:\n                        output.append(record)\n                    else:\n                        inc = len(re.findall(\n                            r'(\\s|\\A|;)(case|then|do)(;|\\Z|\\s)', test_record))\n                        inc += len(re.findall(r'(\\{|\\(|\\[)', test_record))\n                        outc = len(re.findall(\n                            r'(\\s|\\A|;)(esac|fi|done|elif)(;|\\)|\\||\\Z|\\s)',\n                            test_record))\n                        outc += len(re.findall(r'(\\}|\\)|\\])', test_record))\n                        if(re.search(r'\\besac\\b', test_record)):\n                            if(case_level == 0):\n                                sys.stderr.write(\n                                    'File %s: error: \"esac\" before \"case\" in '\n                                    'line %d.\\n' % (path, line))\n                            else:\n                                outc += 1\n                                case_level -= 1\n\n                        # special handling for bad syntax within case ... esac\n                        if re.search(r'\\bcase\\b', test_record):\n                            inc += 1\n                            case_level += 1\n\n                        choice_case = 0\n                        if case_level:\n                            if(re.search(r'\\A[^(]*\\)', test_record)):\n                                inc += 1\n                                choice_case = -1\n\n                        # detect functions\n                        func_decl_style = self.detect_function_style(test_record)\n                        if func_decl_style != None:\n                             stripped_record = self.change_function_style(stripped_record, func_decl_style)\n\n                        # an ad-hoc solution for the \"else\" or \"elif\" keyword\n                        else_case = (0, -1)[re.search(r'^(else|elif)',\n                                            test_record) is not None]\n                        net = inc - outc\n                        tab += min(net, 0)\n\n                        # while 'tab' is preserved across multiple lines, 'extab' is not and is used for\n                        # some adjustments:\n                        extab = tab + else_case + choice_case\n                        if prev_line_had_continue and not open_brackets and not ended_multiline_quoted_string:\n                            extab+=1\n                        extab = max(0, extab)\n                        output.append((self.tab_str * self.tab_size * extab) +\n                                      stripped_record)\n                        tab += max(net, 0)\n                if(defer_ext_quote):\n                    in_ext_quote = True\n                    defer_ext_quote = False\n\n                # count open brackets for line continuation\n                open_brackets += len(re.findall(r'\\[', test_record))\n                open_brackets -= len(re.findall(r'\\]', test_record))\n            line += 1\n        error = (tab != 0)\n        if(error):\n            sys.stderr.write(\n                'File %s: error: indent/outdent mismatch: %d.\\n' % (path, tab))\n        return '\\n'.join(output), error"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbeautify bash script file.", "response": "def beautify_file(self, path):\n        \"\"\"Beautify bash script file.\"\"\"\n        error = False\n        if(path == '-'):\n            data = sys.stdin.read()\n            result, error = self.beautify_string(data, '(stdin)')\n            sys.stdout.write(result)\n        else:  # named file\n            data = self.read_file(path)\n            result, error = self.beautify_string(data, path)\n            if(data != result):\n                if(self.check_only):\n                    if not error:\n                        # we want to return 0 (success) only if the given file is already\n                        # well formatted:\n                        error = (result != data)\n                else:\n                    if(self.backup):\n                        self.write_file(path+'.bak', data)\n                    self.write_file(path, result)\n        return error"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef equalizer(self, frequency, q=1.0, db=-3.0):\n        self.command.append('equalizer')\n        self.command.append(frequency)\n        self.command.append(str(q) + 'q')\n        self.command.append(db)\n        return self", "response": "set equalizer to filter center frequency in Hz q or band - width db"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef bandpass(self, frequency, q=1.0):\n        self.command.append('bandpass')\n        self.command.append(frequency)\n        self.command.append(str(q) + 'q')\n        return self", "response": "This method is used to set the frequency of the log entry in Hz and q for the band - width."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef lowshelf(self, gain=-20.0, frequency=100, slope=0.5):\n        self.command.append('bass')\n        self.command.append(gain)\n        self.command.append(frequency)\n        self.command.append(slope)\n        return self", "response": "This method is used to set the lowshelf of the log files."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef highshelf(self, gain=-20.0, frequency=3000, slope=0.5):\n        self.command.append('treble')\n        self.command.append(gain)\n        self.command.append(frequency)\n        self.command.append(slope)\n        return self", "response": "highshelf takes 3 parameters"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef highpass(self, frequency, q=0.707):\n        self.command.append('highpass')\n        self.command.append(frequency)\n        self.command.append(str(q) + 'q')\n        return self", "response": "This method is used to set the frequency of the entry in the log file below which we will be attenuated and q values will be clipped."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef lowpass(self, frequency, q=0.707):\n        self.command.append('lowpass')\n        self.command.append(frequency)\n        self.command.append(str(q) + 'q')\n        return self", "response": "This command is used to filter the log entries in the log file above which the frequency is attenuated and the q value is high."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef limiter(self, gain=3.0):\n        self.command.append('gain')\n        self.command.append('-l')\n        self.command.append(gain)\n        return self", "response": "limit the number of entries in the cache"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds bend command to set command", "response": "def bend(self, bends, frame_rate=None, over_sample=None):\n        \"\"\"TODO Add docstring.\"\"\"\n        self.command.append(\"bend\")\n        if frame_rate is not None and isinstance(frame_rate, int):\n            self.command.append('-f %s' % frame_rate)\n        if over_sample is not None and isinstance(over_sample, int):\n            self.command.append('-o %s' % over_sample)\n        for bend in bends:\n            self.command.append(','.join(bend))\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef chorus(self, gain_in, gain_out, decays):\n        self.command.append(\"chorus\")\n        self.command.append(gain_in)\n        self.command.append(gain_out)\n        for decay in decays:\n            modulation = decay.pop()\n            numerical = decay\n            self.command.append(' '.join(map(str, numerical)) + ' -' + modulation)\n        return self", "response": "Add chorus command to set the gain in and gain out of a set of decays."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndelay takes 4 parameters input gain output gain and then two lists delays and decays.", "response": "def delay(self,\n              gain_in=0.8,\n              gain_out=0.5,\n              delays=list((1000, 1800)),\n              decays=list((0.3, 0.25)),\n              parallel=False):\n        \"\"\"delay takes 4 parameters: input gain (max 1), output gain \n        and then two lists, delays and decays.\n\n        Each list is a pair of comma seperated values within\n        parenthesis.\n        \"\"\"\n        self.command.append('echo' + ('s' if parallel else ''))\n        self.command.append(gain_in)\n        self.command.append(gain_out)\n        self.command.extend(list(sum(zip(delays, decays), ())))\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef gain(self, db):\n        self.command.append('gain')\n        self.command.append(db)\n        return self", "response": "gain takes one paramter : gain in dB"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\noverdrives takes 2 parameters gain and colour which effects the character of the distortion effet effet.", "response": "def overdrive(self, gain=20, colour=20):\n        \"\"\"overdrive takes 2 parameters: gain in dB and colour which effects\n        the character of the distortion effet.\n\n        Both have a default value of 20. TODO - changing color does not seem to have an audible effect\n        \"\"\"\n        self.command.append('overdrive')\n        self.command.append(gain)\n        self.command.append(colour)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npitches takes 4 parameters : user_tree segment search and overlap", "response": "def pitch(self, shift,\n              use_tree=False,\n              segment=82,\n              search=14.68,\n              overlap=12):\n        \"\"\"pitch takes 4 parameters: user_tree (True or False), segment, search\n        and overlap.\"\"\"\n        self.command.append(\"pitch\")\n        if use_tree:\n            self.command.append('-q')\n        self.command.append(shift)\n        self.command.append(segment)\n        self.command.append(search)\n        self.command.append(overlap)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef reverb(self,\n               reverberance=50,\n               hf_damping=50,\n               room_scale=100,\n               stereo_depth=100,\n               pre_delay=20,\n               wet_gain=0,\n               wet_only=False):\n        \"\"\"reverb takes 7 parameters: reverberance, high-freqnency damping,\n        room scale, stereo depth, pre-delay, wet gain and wet only (Truce or\n        False)\"\"\"\n        self.command.append('reverb')\n        if wet_only:\n            self.command.append('-w')\n        self.command.append(reverberance)\n        self.command.append(hf_damping)\n        self.command.append(room_scale)\n        self.command.append(stereo_depth)\n        self.command.append(pre_delay)\n        self.command.append(wet_gain)\n        return self", "response": "This method is used to set the reverb command for the current locale."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef speed(self, factor, use_semitones=False):\n        self.command.append(\"speed\")\n        self.command.append(factor if not use_semitones else str(factor) + \"c\")\n        return self", "response": "Sets the speed of the current process."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef tremolo(self, freq, depth=40):\n        self.command.append(\"tremolo\")\n        self.command.append(freq)\n        self.command.append(depth)\n        return self", "response": "tremolo takes two parameters frequency and depth"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves all the elements in positions from the end of the list.", "response": "def trim(self, positions):\n        \"\"\"TODO Add docstring.\"\"\"\n        self.command.append(\"trim\")\n        for position in positions:\n            # TODO: check if the position means something\n            self.command.append(position)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef upsample(self, factor):\n        self.command.append(\"upsample\")\n        self.command.append(factor)\n        return self", "response": "Add an upsample command to the log."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset vol to gain or gain - type dB or amplitude or power or dB", "response": "def vol(self, gain, type=\"amplitude\", limiter_gain=None):\n        \"\"\"vol takes three parameters: gain, gain-type (amplitude, power or dB)\n        and limiter gain.\"\"\"\n        self.command.append(\"vol\")\n        if type in [\"amplitude\", \"power\", \"dB\"]:\n            self.command.append(type)\n        else:\n            raise ValueError(\"Type has to be dB, amplitude or power.\")\n        if limiter_gain is not None:\n            self.command.append(str(limiter_gain))\n        print(self.command)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninitialize the default value for the given field.", "response": "def init_default(required, default, optional_default):\n    \"\"\"\n    Returns optional default if field is not required and\n    default was not provided.\n\n    :param bool required: whether the field is required in a given model.\n    :param default: default provided by creator of field.\n    :param optional_default: default for the data type if none provided.\n    :return: default or optional default based on inputs\n    \"\"\"\n    if not required and default == NOTHING:\n        default = optional_default\n\n    return default"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating an attrs validator based on the cls provided and required setting.", "response": "def init_validator(required, cls, *additional_validators):\n    \"\"\"\n    Create an attrs validator based on the cls provided and required setting.\n    :param bool required: whether the field is required in a given model.\n    :param cls: the expected class type of object value.\n    :return: attrs validator chained correctly (e.g. optional(instance_of))\n    \"\"\"\n    validator = validators.instance_of(cls)\n\n    if additional_validators:\n        additional_validators = list(additional_validators)\n        additional_validators.append(validator)\n        validator = composite(*additional_validators)\n\n    return validator if required else validators.optional(validator)"}
